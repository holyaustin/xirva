[{"id": "2102.00027", "submitter": "Beatrice van Amsterdam", "authors": "Beatrice van Amsterdam, Matthew J. Clarkson, Danail Stoyanov", "title": "Gesture Recognition in Robotic Surgery: a Review", "comments": "in IEEE Transactions on Biomedical Engineering, 2021", "journal-ref": null, "doi": "10.1109/TBME.2021.3054828", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Surgical activity recognition is a fundamental step in\ncomputer-assisted interventions. This paper reviews the state-of-the-art in\nmethods for automatic recognition of fine-grained gestures in robotic surgery\nfocusing on recent data-driven approaches and outlines the open questions and\nfuture research directions. Methods: An article search was performed on 5\nbibliographic databases with the following search terms: robotic,\nrobot-assisted, JIGSAWS, surgery, surgical, gesture, fine-grained, surgeme,\naction, trajectory, segmentation, recognition, parsing. Selected articles were\nclassified based on the level of supervision required for training and divided\ninto different groups representing major frameworks for time series analysis\nand data modelling. Results: A total of 52 articles were reviewed. The research\nfield is showing rapid expansion, with the majority of articles published in\nthe last 4 years. Deep-learning-based temporal models with discriminative\nfeature extraction and multi-modal data integration have demonstrated promising\nresults on small surgical datasets. Currently, unsupervised methods perform\nsignificantly less well than the supervised approaches. Conclusion: The\ndevelopment of large and diverse open-source datasets of annotated\ndemonstrations is essential for development and validation of robust solutions\nfor surgical gesture recognition. While new strategies for discriminative\nfeature extraction and knowledge transfer, or unsupervised and semi-supervised\napproaches, can mitigate the need for data and labels, they have not yet been\ndemonstrated to achieve comparable performance. Important future research\ndirections include detection and forecast of gesture-specific errors and\nanomalies. Significance: This paper is a comprehensive and structured analysis\nof surgical gesture recognition methods aiming to summarize the status of this\nrapidly evolving field.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 19:13:13 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["van Amsterdam", "Beatrice", ""], ["Clarkson", "Matthew J.", ""], ["Stoyanov", "Danail", ""]]}, {"id": "2102.00062", "submitter": "Jae Shin Yoon", "authors": "Jae Shin Yoon, Kihwan Kim, Jan Kautz, and Hyun Soo Park", "title": "Neural 3D Clothes Retargeting from a Single Image", "comments": "20 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a method of clothes retargeting; generating the\npotential poses and deformations of a given 3D clothing template model to fit\nonto a person in a single RGB image. The problem is fundamentally ill-posed as\nattaining the ground truth data is impossible, i.e., images of people wearing\nthe different 3D clothing template model at exact same pose. We address this\nchallenge by utilizing large-scale synthetic data generated from physical\nsimulation, allowing us to map 2D dense body pose to 3D clothing deformation.\nWith the simulated data, we propose a semi-supervised learning framework that\nvalidates the physical plausibility of the 3D deformation by matching with the\nprescribed body-to-cloth contact points and clothing silhouette to fit onto the\nunlabeled real images. A new neural clothes retargeting network (CRNet) is\ndesigned to integrate the semi-supervised retargeting task in an end-to-end\nfashion. In our evaluation, we show that our method can predict the realistic\n3D pose and deformation field needed for retargeting clothes models in\nreal-world examples.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 20:50:34 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Yoon", "Jae Shin", ""], ["Kim", "Kihwan", ""], ["Kautz", "Jan", ""], ["Park", "Hyun Soo", ""]]}, {"id": "2102.00079", "submitter": "Pulkit Tandon", "authors": "Pulkit Tandon, Mariana Afonso, Joel Sole, Luk\\'a\\v{s} Krasula", "title": "CAMBI: Contrast-aware Multiscale Banding Index", "comments": "5 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Banding artifacts are artificially-introduced contours arising from the\nquantization of a smooth region in a video. Despite the advent of recent higher\nquality video systems with more efficient codecs, these artifacts remain\nconspicuous, especially on larger displays. In this work, a comprehensive\nsubjective study is performed to understand the dependence of the banding\nvisibility on encoding parameters and dithering. We subsequently develop a\nsimple and intuitive no-reference banding index called CAMBI (Contrast-aware\nMultiscale Banding Index) which uses insights from Contrast Sensitivity\nFunction in the Human Visual System to predict banding visibility. CAMBI\ncorrelates well with subjective perception of banding while using only a few\nvisually-motivated hyperparameters.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 21:36:41 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Tandon", "Pulkit", ""], ["Afonso", "Mariana", ""], ["Sole", "Joel", ""], ["Krasula", "Luk\u00e1\u0161", ""]]}, {"id": "2102.00084", "submitter": "Aditya Deshpande", "authors": "Aditya Deshpande, Alessandro Achille, Avinash Ravichandran, Hao Li,\n  Luca Zancato, Charless Fowlkes, Rahul Bhotika, Stefano Soatto, Pietro Perona", "title": "A linearized framework and a new benchmark for model selection for\n  fine-tuning", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-tuning from a collection of models pre-trained on different domains (a\n\"model zoo\") is emerging as a technique to improve test accuracy in the\nlow-data regime. However, model selection, i.e. how to pre-select the right\nmodel to fine-tune from a model zoo without performing any training, remains an\nopen topic. We use a linearized framework to approximate fine-tuning, and\nintroduce two new baselines for model selection -- Label-Gradient and\nLabel-Feature Correlation. Since all model selection algorithms in the\nliterature have been tested on different use-cases and never compared directly,\nwe introduce a new comprehensive benchmark for model selection comprising of:\ni) A model zoo of single and multi-domain models, and ii) Many target tasks.\nOur benchmark highlights accuracy gain with model zoo compared to fine-tuning\nImagenet models. We show our model selection baseline can select optimal models\nto fine-tune in few selections and has the highest ranking correlation to\nfine-tuning accuracy compared to existing algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 21:57:15 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Deshpande", "Aditya", ""], ["Achille", "Alessandro", ""], ["Ravichandran", "Avinash", ""], ["Li", "Hao", ""], ["Zancato", "Luca", ""], ["Fowlkes", "Charless", ""], ["Bhotika", "Rahul", ""], ["Soatto", "Stefano", ""], ["Perona", "Pietro", ""]]}, {"id": "2102.00097", "submitter": "Huang Ling", "authors": "Ling Huang, Su Ruan, Thierry Denoeux", "title": "Belief function-based semi-supervised learning for brain tumor\n  segmentation", "comments": "5 pages, 4 figures, ISBI2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise segmentation of a lesion area is important for optimizing its\ntreatment. Deep learning makes it possible to detect and segment a lesion field\nusing annotated data. However, obtaining precisely annotated data is very\nchallenging in the medical domain. Moreover, labeling uncertainty and\nimprecision make segmentation results unreliable. In this paper, we address the\nuncertain boundary problem by a new evidential neural network with an\ninformation fusion strategy, and the scarcity of annotated data by\nsemi-supervised learning. Experimental results show that our proposal has\nbetter performance than state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 22:39:16 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Huang", "Ling", ""], ["Ruan", "Su", ""], ["Denoeux", "Thierry", ""]]}, {"id": "2102.00103", "submitter": "Nathan Clement", "authors": "Nathan Clement, Alan Schoen, Arnold Boedihardjo, and Andrew Jenkins", "title": "Synthetic Data and Hierarchical Object Detection in Overhead Imagery", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of neural network models is often limited by the availability\nof big data sets. To treat this problem, we survey and develop novel synthetic\ndata generation and augmentation techniques for enhancing low/zero-sample\nlearning in satellite imagery. In addition to extending synthetic data\ngeneration approaches, we propose a hierarchical detection approach to improve\nthe utility of synthetic training samples. We consider existing techniques for\nproducing synthetic imagery--3D models and neural style transfer--as well as\nintroducing our own adversarially trained reskinning network, the\nGAN-Reskinner, to blend 3D models. Additionally, we test the value of synthetic\ndata in a two-stage, hierarchical detection/classification model of our own\nconstruction. To test the effectiveness of synthetic imagery, we employ it in\nthe training of detection models and our two stage model, and evaluate the\nresulting models on real satellite images. All modalities of synthetic data are\ntested extensively on practical, geospatial analysis problems. Our experiments\nshow that synthetic data developed using our approach can often enhance\ndetection performance, particularly when combined with some real training\nimages. When the only source of data is synthetic, our GAN-Reskinner often\nboosts performance over conventionally rendered 3D models and in all cases the\nhierarchical model outperforms the baseline end-to-end detection architecture.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 22:52:47 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Clement", "Nathan", ""], ["Schoen", "Alan", ""], ["Boedihardjo", "Arnold", ""], ["Jenkins", "Andrew", ""]]}, {"id": "2102.00109", "submitter": "Jasmine Sekhon", "authors": "Jasmine Sekhon, Cody Fleming", "title": "SCAN: A Spatial Context Attentive Network for Joint Multi-Agent Intent\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safe navigation of autonomous agents in human centric environments requires\nthe ability to understand and predict motion of neighboring pedestrians.\nHowever, predicting pedestrian intent is a complex problem. Pedestrian motion\nis governed by complex social navigation norms, is dependent on neighbors'\ntrajectories, and is multimodal in nature. In this work, we propose SCAN, a\nSpatial Context Attentive Network that can jointly predict socially-acceptable\nmultiple future trajectories for all pedestrians in a scene. SCAN encodes the\ninfluence of spatially close neighbors using a novel spatial attention\nmechanism in a manner that relies on fewer assumptions, is parameter efficient,\nand is more interpretable compared to state-of-the-art spatial attention\napproaches. Through experiments on several datasets we demonstrate that our\napproach can also quantitatively outperform state of the art trajectory\nprediction methods in terms of accuracy of predicted intent.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 23:35:00 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 20:54:39 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Sekhon", "Jasmine", ""], ["Fleming", "Cody", ""]]}, {"id": "2102.00142", "submitter": "Ivan Bajic", "authors": "Ivan V. Baji\\'c", "title": "Latent-Space Inpainting for Packet Loss Concealment in Collaborative\n  Object Detection", "comments": "Extended version of the paper \"Latent Space Inpainting for\n  Loss-Resilient Collaborative Object Detection,\" to be presented at the IEEE\n  International Conference on Communications (ICC), Montreal, Canada, June\n  14-23, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Edge devices, such as cameras and mobile units, are increasingly capable of\nperforming sophisticated computation in addition to their traditional roles in\nsensing and communicating signals. The focus of this paper is on collaborative\nobject detection, where deep features computed on the edge device from input\nimages are transmitted to the cloud for further processing. We consider the\nimpact of packet loss on the transmitted features and examine several ways for\nrecovering the missing data. In particular, through theory and experiments, we\nshow that methods for image inpainting based on partial differential equations\nwork well for the recovery of missing features in the latent space. The\nobtained results represent the new state of the art for missing data recovery\nin collaborative object detection.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 03:32:19 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Baji\u0107", "Ivan V.", ""]]}, {"id": "2102.00150", "submitter": "Haruya Sakashita", "authors": "Haruya Sakashita, Christoph Flothow, Noriko Takemura, Yusuke Sugano", "title": "DRIV100: In-The-Wild Multi-Domain Dataset and Evaluation for Real-World\n  Domain Adaptation of Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Together with the recent advances in semantic segmentation, many domain\nadaptation methods have been proposed to overcome the domain gap between\ntraining and deployment environments. However, most previous studies use\nlimited combinations of source/target datasets, and domain adaptation\ntechniques have never been thoroughly evaluated in a more challenging and\ndiverse set of target domains. This work presents a new multi-domain dataset\nDRIV100 for benchmarking domain adaptation techniques on in-the-wild road-scene\nvideos collected from the Internet. The dataset consists of pixel-level\nannotations for 100 videos selected to cover diverse scenes/domains based on\ntwo criteria; human subjective judgment and an anomaly score judged using an\nexisting road-scene dataset. We provide multiple manually labeled ground-truth\nframes for each video, enabling a thorough evaluation of video-level domain\nadaptation where each video independently serves as the target domain. Using\nthe dataset, we quantify domain adaptation performances of state-of-the-art\nmethods and clarify the potential and novel challenges of domain adaptation\ntechniques. The dataset is available at https://doi.org/10.5281/zenodo.4389243.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 04:43:22 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 05:19:42 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Sakashita", "Haruya", ""], ["Flothow", "Christoph", ""], ["Takemura", "Noriko", ""], ["Sugano", "Yusuke", ""]]}, {"id": "2102.00155", "submitter": "Zhengzhong Tu", "authors": "Zhengzhong Tu, Chia-Ju Chen, Li-Heng Chen, Yilin Wang, Neil Birkbeck,\n  Balu Adsumilli, and Alan C. Bovik", "title": "Regression or Classification? New Methods to Evaluate No-Reference\n  Picture and Video Quality Models", "comments": "ICASSP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Video and image quality assessment has long been projected as a regression\nproblem, which requires predicting a continuous quality score given an input\nstimulus. However, recent efforts have shown that accurate quality score\nregression on real-world user-generated content (UGC) is a very challenging\ntask. To make the problem more tractable, we propose two new methods - binary,\nand ordinal classification - as alternatives to evaluate and compare\nno-reference quality models at coarser levels. Moreover, the proposed new tasks\nconvey more practical meaning on perceptually optimized UGC transcoding, or for\npreprocessing on media processing platforms. We conduct a comprehensive\nbenchmark experiment of popular no-reference quality models on recent\nin-the-wild picture and video quality datasets, providing reliable baselines\nfor both evaluation methods to support further studies. We hope this work\npromotes coarse-grained perceptual modeling and its applications to efficient\nUGC processing.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 05:40:14 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Tu", "Zhengzhong", ""], ["Chen", "Chia-Ju", ""], ["Chen", "Li-Heng", ""], ["Wang", "Yilin", ""], ["Birkbeck", "Neil", ""], ["Adsumilli", "Balu", ""], ["Bovik", "Alan C.", ""]]}, {"id": "2102.00160", "submitter": "S.H.Shabbeer Basha", "authors": "S.H.Shabbeer Basha, Mohammad Farazuddin, Viswanath Pulabaigari, Shiv\n  Ram Dubey, Snehasis Mukherjee", "title": "Deep Model Compression based on the Training History", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (DCNNs) have shown promising results in\nseveral visual recognition problems which motivated the researchers to propose\npopular architectures such as LeNet, AlexNet, VGGNet, ResNet, and many more.\nThese architectures come at a cost of high computational complexity and\nparameter storage. To get rid of storage and computational complexity, deep\nmodel compression methods have been evolved. We propose a novel History Based\nFilter Pruning (HBFP) method that utilizes network training history for filter\npruning. Specifically, we prune the redundant filters by observing similar\npatterns in the L1-norms of filters (absolute sum of weights) over the training\nepochs. We iteratively prune the redundant filters of a CNN in three steps.\nFirst, we train the model and select the filter pairs with redundant filters in\neach pair. Next, we optimize the network to increase the similarity between the\nfilters in a pair. It facilitates us to prune one filter from each pair based\non its importance without much information loss. Finally, we retrain the\nnetwork to regain the performance, which is dropped due to filter pruning. We\ntest our approach on popular architectures such as LeNet-5 on MNIST dataset and\nVGG-16, ResNet-56, and ResNet-110 on CIFAR-10 dataset. The proposed pruning\nmethod outperforms the state-of-the-art in terms of FLOPs reduction\n(floating-point operations) by 97.98%, 83.42%, 78.43%, and 74.95% for LeNet-5,\nVGG-16, ResNet-56, and ResNet-110 models, respectively, while maintaining the\nless error rate.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 06:04:21 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Basha", "S. H. Shabbeer", ""], ["Farazuddin", "Mohammad", ""], ["Pulabaigari", "Viswanath", ""], ["Dubey", "Shiv Ram", ""], ["Mukherjee", "Snehasis", ""]]}, {"id": "2102.00169", "submitter": "Cristian Lazo", "authors": "Cristian Lazo", "title": "Segmentation of skin lesions and their attributes using Generative\n  Adversarial Networks", "comments": "LatinX in AI Research at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is about the semantic segmentation of skin lesion boundary and\ntheir attributes using Image-to-Image Translation with Conditional Adversarial\nNets. Melanoma is a type of skin cancer that can be cured if detected in time.\nSegmentation into dermoscopic images is an essential procedure for\ncomputer-assisted diagnosis due to its existing artifacts typical of skin\nimages. To alleviate the image annotation process, we propose to use a modified\nPix2Pix network. The discriminator network learns the mapping from a dermal\nimage as an input and a mask image of six channels as an output. Likewise, the\ndiscriminative network output called PatchGAN is varied for one channel and six\noutput channels. The photos used come from the 2018 ISIC Challenge, where 500\nphotographs are used with their respective semantic map, divided into 75% for\ntraining and 35% for testing. Obtaining for 100 training epochs high Jaccard\nindices for all attributes of the segmentation map.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 06:57:54 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Lazo", "Cristian", ""]]}, {"id": "2102.00193", "submitter": "Zizhe Wang", "authors": "Zizhe Wang, Shaomeng Shen, Jiabei Mu", "title": "Coupling innovation method and feasibility analysis of garbage\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In order to solve the recent defect in garbage classification - including low\nlevel of intelligence, low accuracy and high cost of equipment, this paper\npresents a series of methods in identification and judgment in intelligent\ngarbage classification, including a material identification based on thermal\nprinciple and non-destructive laser irradiation, another material\nidentification based on optical diffraction and phase analysis, a profile\nidentification which utilizes a scenery thermal image after PCA and histogram\ncorrection, another profile identification which utilizes computer vision with\ninnovated data sets and algorithms. Combining AHP and Bayesian formula, the\npaper innovates a coupling algorithm which helps to make a comprehensive\njudgment of the garbage sort, based on the material and profile identification.\nThis paper also proposes a method for real-time space measurement of garbage\ncans, which based on the characteristics of air as fluid, and analyses the\nfunctions of air cleaning and particle disposing. Instead of the single use of\ngarbage image recognition, this paper provides a comprehensive method to judge\nthe garbage sort by material and profile identifications, which greatly\nenhancing the accuracy and intelligence in garbage classification.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 09:15:46 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 03:18:51 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Wang", "Zizhe", ""], ["Shen", "Shaomeng", ""], ["Mu", "Jiabei", ""]]}, {"id": "2102.00195", "submitter": "Zhengfang Duanmu", "authors": "Zhengfang Duanmu, Wentao Liu, Zhongling Wang, Zhou Wang", "title": "Quantifying Visual Image Quality: A Bayesian View", "comments": "24 pages, 6 figures. Annual Review of Vision Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image quality assessment (IQA) models aim to establish a quantitative\nrelationship between visual images and their perceptual quality by human\nobservers. IQA modeling plays a special bridging role between vision science\nand engineering practice, both as a test-bed for vision theories and\ncomputational biovision models, and as a powerful tool that could potentially\nmake profound impact on a broad range of image processing, computer vision, and\ncomputer graphics applications, for design, optimization, and evaluation\npurposes. IQA research has enjoyed an accelerated growth in the past two\ndecades. Here we present an overview of IQA methods from a Bayesian\nperspective, with the goals of unifying a wide spectrum of IQA approaches under\na common framework and providing useful references to fundamental concepts\naccessible to vision scientists and image processing practitioners. We discuss\nthe implications of the successes and limitations of modern IQA methods for\nbiological vision and the prospect for vision science to inform the design of\nfuture artificial vision systems.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 09:34:23 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 04:03:20 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Duanmu", "Zhengfang", ""], ["Liu", "Wentao", ""], ["Wang", "Zhongling", ""], ["Wang", "Zhou", ""]]}, {"id": "2102.00209", "submitter": "Anthony Bourached", "authors": "George Cann, Anthony Bourached, Ryan-Rhys Griffiths, and David Stork", "title": "Resolution enhancement in the recovery of underdrawings via style\n  transfer by generative adversarial deep neural networks", "comments": "Accepted for Publication at Computer Vision and Art Analysis, IS&T,\n  Springfield, VA, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply generative adversarial convolutional neural networks to the problem\nof style transfer to underdrawings and ghost-images in x-rays of fine art\npaintings with a special focus on enhancing their spatial resolution. We build\nupon a neural architecture developed for the related problem of synthesizing\nhigh-resolution photo-realistic image from semantic label maps. Our neural\narchitecture achieves high resolution through a hierarchy of generators and\ndiscriminator sub-networks, working throughout a range of spatial resolutions.\nThis coarse-to-fine generator architecture can increase the effective\nresolution by a factor of eight in each spatial direction, or an overall\nincrease in number of pixels by a factor of 64. We also show that even just a\nfew examples of human-generated image segmentations can greatly improve --\nqualitatively and quantitatively -- the generated images. We demonstrate our\nmethod on works such as Leonardo's Madonna of the carnation and the\nunderdrawing in his Virgin of the rocks, which pose several special problems in\nstyle transfer, including the paucity of representative works from which to\nlearn and transfer style information.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 11:11:59 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Cann", "George", ""], ["Bourached", "Anthony", ""], ["Griffiths", "Ryan-Rhys", ""], ["Stork", "David", ""]]}, {"id": "2102.00212", "submitter": "Jaime Moraga", "authors": "Jaime Moraga (1), Gurbet Gurkan (1), Sebnem Duzgun (1) ((1) Colorado\n  School of Mines, Golden, Colorado)", "title": "Monitoring the Impacts of a Tailings Dam Failure Using Satellite Images", "comments": "11 pages, 7 figures, Submitted to USSD 2020 Annual Conference\n  Preprint, Denver, CO, 2020-04-09", "journal-ref": null, "doi": "10.13140/RG.2.2.18504.75529", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Monitoring dam failures using satellite images provides first responders with\nefficient management of early interventions. It is also equally important to\nmonitor spatial and temporal changes in the inundation area to track the\npost-disaster recovery. On January 25th, 2019, the tailings dam of the\nC\\'orrego do Feij\\~ao iron ore mine, located in Brumadinho, Brazil, collapsed.\nThis disaster caused more than 230 fatalities and 30 missing people leading to\ndamage on the order of multiple billions of dollars. This study uses Sentinel-2\nsatellite images to map the inundation area and assess and delineate the land\nuse and land cover impacted by the dam failure. The images correspond to data\ncaptures from January 22nd (3 days before), and February 02 (7 days after the\ncollapse). Satellite images of the region were classified for before and\naftermath of the disaster implementing a machine learning algorithm. In order\nto have sufficient land cover types to validate the quality and accuracy of the\nalgorithm, 7 classes were defined: mine, forest, build-up, river, agricultural,\nclear water, and grassland. The developed classification algorithm yielded a\nhigh accuracy (99%) for the image before the collapse. This paper determines\nland cover impact using two different models, 1) by using the trained network\nin the \"after\" image, and 2) by creating a second network, trained in a subset\nof points of the \"after\" image, and then comparing the land cover results of\nthe two trained networks. In the first model, applying the trained network to\nthe \"after\" image, the accuracy is still high (86%), but lower than using the\nsecond model (98%). This strategy can be applied at a low cost for monitoring\nand assessment by using openly available satellite information and, in case of\ndam collapse or with a larger budget, higher resolution and faster data can be\nobtained by fly-overs on the area of concern.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 11:35:47 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Moraga", "Jaime", ""], ["Gurkan", "Gurbet", ""], ["Duzgun", "Sebnem", ""]]}, {"id": "2102.00221", "submitter": "Jiawei Zhang", "authors": "Jiawei Zhang, Yanchun Zhang, Xiaowei Xu", "title": "ObjectAug: Object-level Data Augmentation for Semantic Image\n  Segmentation", "comments": "8 pages, 7 figures, 9 tables, Accepted by IJCNN2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic image segmentation aims to obtain object labels with precise\nboundaries, which usually suffers from overfitting. Recently, various data\naugmentation strategies like regional dropout and mix strategies have been\nproposed to address the problem. These strategies have proved to be effective\nfor guiding the model to attend on less discriminative parts. However, current\nstrategies operate at the image level, and objects and the background are\ncoupled. Thus, the boundaries are not well augmented due to the fixed semantic\nscenario. In this paper, we propose ObjectAug to perform object-level\naugmentation for semantic image segmentation. ObjectAug first decouples the\nimage into individual objects and the background using the semantic labels.\nNext, each object is augmented individually with commonly used augmentation\nmethods (e.g., scaling, shifting, and rotation). Then, the black area brought\nby object augmentation is further restored using image inpainting. Finally, the\naugmented objects and background are assembled as an augmented image. In this\nway, the boundaries can be fully explored in the various semantic scenarios. In\naddition, ObjectAug can support category-aware augmentation that gives various\npossibilities to objects in each category, and can be easily combined with\nexisting image-level augmentation methods to further boost performance.\nComprehensive experiments are conducted on both natural image and medical image\ndatasets. Experiment results demonstrate that our ObjectAug can evidently\nimprove segmentation performance.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 12:46:20 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 09:48:49 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Zhang", "Jiawei", ""], ["Zhang", "Yanchun", ""], ["Xu", "Xiaowei", ""]]}, {"id": "2102.00227", "submitter": "Radu Dogaru", "authors": "Radu Dogaru and Ioana Dogaru", "title": "NL-CNN: A Resources-Constrained Deep Learning Model based on Nonlinear\n  Convolution", "comments": "4 pages, reprint submitted to ATEE 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A novel convolution neural network model, abbreviated NL-CNN is proposed,\nwhere nonlinear convolution is emulated in a cascade of convolution +\nnonlinearity layers. The code for its implementation and some trained models\nare made publicly available. Performance evaluation for several widely known\ndatasets is provided, showing several relevant features: i) for small / medium\ninput image sizes the proposed network gives very good testing accuracy, given\na low implementation complexity and model size; ii) compares favorably with\nother widely known resources-constrained models, for instance in comparison to\nMobileNetv2 provides better accuracy with several times less training times and\nup to ten times less parameters (memory occupied by the model); iii) has a\nrelevant set of hyper-parameters which can be easily and rapidly tuned due to\nthe fast training specific to it. All these features make NL-CNN suitable for\nIoT, smart sensing, bio-medical portable instrumentation and other applications\nwhere artificial intelligence must be deployed in energy-constrained\nenvironments.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 13:38:42 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Dogaru", "Radu", ""], ["Dogaru", "Ioana", ""]]}, {"id": "2102.00230", "submitter": "Raviraj Joshi", "authors": "Pranali Bora, Tulika Awalgaonkar, Himanshu Palve, Raviraj Joshi, Purvi\n  Goel", "title": "ICodeNet -- A Hierarchical Neural Network Approach for Source Code\n  Author Identification", "comments": "Accepted at ICMLC 2021", "journal-ref": null, "doi": "10.1145/3457682.3457709", "report-no": null, "categories": "cs.LG cs.CV cs.IR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the open-source revolution, source codes are now more easily accessible\nthan ever. This has, however, made it easier for malicious users and\ninstitutions to copy the code without giving regards to the license, or credit\nto the original author. Therefore, source code author identification is a\ncritical task with paramount importance. In this paper, we propose ICodeNet - a\nhierarchical neural network that can be used for source code file-level tasks.\nThe ICodeNet processes source code in image format and is employed for the task\nof per file author identification. The ICodeNet consists of an ImageNet trained\nVGG encoder followed by a shallow neural network. The shallow network is based\neither on CNN or LSTM. Different variations of models are evaluated on a source\ncode author classification dataset. We have also compared our image-based\nhierarchical neural network model with simple image-based CNN architecture and\ntext-based CNN and LSTM models to highlight its novelty and efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 14:05:22 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Bora", "Pranali", ""], ["Awalgaonkar", "Tulika", ""], ["Palve", "Himanshu", ""], ["Joshi", "Raviraj", ""], ["Goel", "Purvi", ""]]}, {"id": "2102.00240", "submitter": "Qing-Long Zhang", "authors": "Qing-Long Zhang Yu-Bin Yang", "title": "SA-Net: Shuffle Attention for Deep Convolutional Neural Networks", "comments": "ICASSP 2021 paper: SA-Net: Shuffle Attention for Deep Convolutional\n  Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Attention mechanisms, which enable a neural network to accurately focus on\nall the relevant elements of the input, have become an essential component to\nimprove the performance of deep neural networks. There are mainly two attention\nmechanisms widely used in computer vision studies, \\textit{spatial attention}\nand \\textit{channel attention}, which aim to capture the pixel-level pairwise\nrelationship and channel dependency, respectively. Although fusing them\ntogether may achieve better performance than their individual implementations,\nit will inevitably increase the computational overhead. In this paper, we\npropose an efficient Shuffle Attention (SA) module to address this issue, which\nadopts Shuffle Units to combine two types of attention mechanisms effectively.\nSpecifically, SA first groups channel dimensions into multiple sub-features\nbefore processing them in parallel. Then, for each sub-feature, SA utilizes a\nShuffle Unit to depict feature dependencies in both spatial and channel\ndimensions. After that, all sub-features are aggregated and a \"channel shuffle\"\noperator is adopted to enable information communication between different\nsub-features. The proposed SA module is efficient yet effective, e.g., the\nparameters and computations of SA against the backbone ResNet50 are 300 vs.\n25.56M and 2.76e-3 GFLOPs vs. 4.12 GFLOPs, respectively, and the performance\nboost is more than 1.34% in terms of Top-1 accuracy. Extensive experimental\nresults on common-used benchmarks, including ImageNet-1k for classification, MS\nCOCO for object detection, and instance segmentation, demonstrate that the\nproposed SA outperforms the current SOTA methods significantly by achieving\nhigher accuracy while having lower model complexity. The code and models are\navailable at https://github.com/wofmanaf/SA-Net.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 15:23:17 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Yang", "Qing-Long Zhang Yu-Bin", ""]]}, {"id": "2102.00250", "submitter": "Shi Yan", "authors": "Yiqiu Dong and Chunlin Wu and Shi Yan", "title": "A fast method for simultaneous reconstruction and segmentation in X-ray\n  CT application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we propose a fast method for simultaneous reconstruction and\nsegmentation (SRS) in X-ray computed tomography (CT). Our work is based on the\nSRS model where Bayes' rule and the maximum a posteriori (MAP) are used on\nhidden Markov measure field model (HMMFM). The original method leads to a\nlogarithmic-summation (log-sum) term, which is non-separable to the\nclassification index. The minimization problem in the model was solved by using\nconstrained gradient descend method, Frank-Wolfe algorithm, which is very\ntime-consuming especially when dealing with large-scale CT problems. The\nstarting point of this paper is the commutativity of log-sum operations, where\nthe log-sum problem could be transformed into a sum-log problem by introducing\nan auxiliary variable. The corresponding sum-log problem for the SRS model is\nseparable. After applying alternating minimization method, this problem turns\ninto several easy-to-solve convex sub-problems. In the paper, we also study an\nimproved model by adding Tikhonov regularization, and give some convergence\nresults. Experimental results demonstrate that the proposed algorithms could\nproduce comparable results with the original SRS method with much less CPU\ntime.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 15:46:22 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Dong", "Yiqiu", ""], ["Wu", "Chunlin", ""], ["Yan", "Shi", ""]]}, {"id": "2102.00264", "submitter": "Jakob Stolberg-Larsen", "authors": "Jakob Stolberg-Larsen, Stefan Sommer", "title": "Atlas Generative Models and Geodesic Interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative neural networks have a well recognized ability to estimate\nunderlying manifold structure of high dimensional data. However, if a simply\nconnected latent space is used, it is not possible to faithfully represent a\nmanifold with non-trivial homotopy type. In this work we define the general\nclass of Atlas Generative Models (AGMs), models with hybrid discrete-continuous\nlatent space that estimate an atlas on the underlying data manifold together\nwith a partition of unity on the data space. We identify existing examples of\nmodels from various popular generative paradigms that fit into this class. Due\nto the atlas interpretation, ideas from non-linear latent space analysis and\nstatistics, e.g. geodesic interpolation, which has previously only been\ninvestigated for models with simply connected latent spaces, may be extended to\nthe entire class of AGMs in a natural way. We exemplify this by generalizing an\nalgorithm for graph based geodesic interpolation to the setting of AGMs, and\nverify its performance experimentally.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 16:35:25 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Stolberg-Larsen", "Jakob", ""], ["Sommer", "Stefan", ""]]}, {"id": "2102.00266", "submitter": "Joanna Grzyb", "authors": "Joanna Grzyb, Jakub Klikowski, Micha{\\l} Wo\\'zniak", "title": "Hellinger Distance Weighted Ensemble for Imbalanced Data Stream\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The imbalanced data classification remains a vital problem. The key is to\nfind such methods that classify both the minority and majority class correctly.\nThe paper presents the classifier ensemble for classifying binary,\nnon-stationary and imbalanced data streams where the Hellinger Distance is used\nto prune the ensemble. The paper includes an experimental evaluation of the\nmethod based on the conducted experiments. The first one checks the impact of\nthe base classifier type on the quality of the classification. In the second\nexperiment, the Hellinger Distance Weighted Ensemble (HDWE) method is compared\nto selected state-of-the-art methods using a statistical test with two base\nclassifiers. The method was profoundly tested based on many imbalanced data\nstreams and obtained results proved the HDWE method's usefulness.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 16:38:42 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Grzyb", "Joanna", ""], ["Klikowski", "Jakub", ""], ["Wo\u017aniak", "Micha\u0142", ""]]}, {"id": "2102.00277", "submitter": "Rajvir Kaur", "authors": "Rajvir Kaur, Kenji Bekki, Ghulam Mubashar Hassan, Amitava Datta", "title": "Estimating galaxy masses from kinematics of globular cluster systems: a\n  new method based on deep learning", "comments": "Accepted by MNRAS", "journal-ref": null, "doi": "10.1093/mnras/stab1460", "report-no": null, "categories": "astro-ph.CO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method by which the total masses of galaxies including dark\nmatter can be estimated from the kinematics of their globular cluster systems\n(GCSs). In the proposed method, we apply the convolutional neural networks\n(CNNs) to the two-dimensional (2D) maps of line-of-sight-velocities ($V$) and\nvelocity dispersions ($\\sigma$) of GCSs predicted from numerical simulations of\ndisk and elliptical galaxies. In this method, we first train the CNN using\neither only a larger number ($\\sim 200,000$) of the synthesized 2D maps of\n$\\sigma$ (\"one-channel\") or those of both $\\sigma$ and $V$ (\"two-channel\").\nThen we use the CNN to predict the total masses of galaxies (i.e., test the\nCNN) for the totally unknown dataset that is not used in training the CNN. The\nprincipal results show that overall accuracy for one-channel and two-channel\ndata is 97.6\\% and 97.8\\% respectively, which suggests that the new method is\npromising. The mean absolute errors (MAEs) for one-channel and two-channel data\nare 0.288 and 0.275 respectively, and the value of root mean square errors\n(RMSEs) are 0.539 and 0.51 for one-channel and two-channel respectively. These\nsmaller MAEs and RMSEs for two-channel data (i.e., better performance) suggest\nthat the new method can properly consider the global rotation of GCSs in the\nmass estimation. We also applied our proposed method to real data collected\nfrom observations of NGC 3115 to compare the total mass predicted by our\nproposed method and other popular methods from the literature.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 17:56:40 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 13:00:30 GMT"}, {"version": "v3", "created": "Sun, 16 May 2021 05:12:53 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Kaur", "Rajvir", ""], ["Bekki", "Kenji", ""], ["Hassan", "Ghulam Mubashar", ""], ["Datta", "Amitava", ""]]}, {"id": "2102.00280", "submitter": "Jordan Ott", "authors": "Jordan Ott, David Bruyette, Cody Arbuckle, Dylan Balsz, Silke Hecht,\n  Lisa Shubitz, Pierre Baldi", "title": "Detecting Pulmonary Coccidioidomycosis (Valley fever) with Deep\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coccidioidomycosis is the most common systemic mycosis in dogs in the\nsouthwestern United States. With warming climates, affected areas and number of\ncases are expected to increase in the coming years, escalating also the chances\nof transmission to humans. As a result, developing methods for automating the\ndetection of the disease is important, as this will help doctors and\nveterinarians more easily identify and diagnose positive cases. We apply\nmachine learning models to provide accurate and interpretable predictions of\nCoccidioidomycosis. We assemble a set of radiographic images and use it to\ntrain and test state-of-the-art convolutional neural networks to detect\nCoccidioidomycosis. These methods are relatively inexpensive to train and very\nfast at inference time. We demonstrate the successful application of this\napproach to detect the disease with an Area Under the Curve (AUC) above 0.99\nusing 10-fold cross validation. We also use the classification model to\nidentify regions of interest and localize the disease in the radiographic\nimages, as illustrated through visual heatmaps. This proof-of-concept study\nestablishes the feasibility of very accurate and rapid automated detection of\nValley Fever in radiographic images.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 18:06:40 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Ott", "Jordan", ""], ["Bruyette", "David", ""], ["Arbuckle", "Cody", ""], ["Balsz", "Dylan", ""], ["Hecht", "Silke", ""], ["Shubitz", "Lisa", ""], ["Baldi", "Pierre", ""]]}, {"id": "2102.00281", "submitter": "Weimin Zhou", "authors": "Weimin Zhou, Sayantan Bhadra, Frank J. Brooks, Jason L. Granstedt, Hua\n  Li, Mark A. Anastasio", "title": "Advancing the AmbientGAN for learning stochastic object models", "comments": "SPIE Medical Imaging 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical imaging systems are commonly assessed and optimized by use of\nobjective-measures of image quality (IQ) that quantify the performance of an\nobserver at specific tasks. Variation in the objects to-be-imaged is an\nimportant source of variability that can significantly limit observer\nperformance. This object variability can be described by stochastic object\nmodels (SOMs). In order to establish SOMs that can accurately model realistic\nobject variability, it is desirable to use experimental data. To achieve this,\nan augmented generative adversarial network (GAN) architecture called\nAmbientGAN has been developed and investigated. However, AmbientGANs cannot be\nimmediately trained by use of advanced GAN training methods such as the\nprogressive growing of GANs (ProGANs). Therefore, the ability of AmbientGANs to\nestablish realistic object models is limited. To circumvent this, a\nprogressively-growing AmbientGAN (ProAmGAN) has been proposed. However,\nProAmGANs are designed for generating two-dimensional (2D) images while medical\nimaging modalities are commonly employed for imaging three-dimensional (3D)\nobjects. Moreover, ProAmGANs that employ traditional generator architectures\nlack the ability to control specific image features such as fine-scale textures\nthat are frequently considered when optimizing imaging systems. In this study,\nwe address these limitations by proposing two advanced AmbientGAN\narchitectures: 3D ProAmGANs and Style-AmbientGANs (StyAmGANs). Stylized\nnumerical studies involving magnetic resonance (MR) imaging systems are\nconducted. The ability of 3D ProAmGANs to learn 3D SOMs from imaging\nmeasurements and the ability of StyAmGANs to control fine-scale textures of\nsynthesized objects are demonstrated.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 18:08:23 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Zhou", "Weimin", ""], ["Bhadra", "Sayantan", ""], ["Brooks", "Frank J.", ""], ["Granstedt", "Jason L.", ""], ["Li", "Hua", ""], ["Anastasio", "Mark A.", ""]]}, {"id": "2102.00297", "submitter": "Nicole Han", "authors": "Nicole Han (1), Sudhanshu Srivastava (1), Aiwen Xu (1), Devi Klein\n  (1), Michael Beyeler (1) ((1) University of California, Santa Barbara)", "title": "Deep Learning--Based Scene Simplification for Bionic Vision", "comments": "10 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Retinal degenerative diseases cause profound visual impairment in more than\n10 million people worldwide, and retinal prostheses are being developed to\nrestore vision to these individuals. Analogous to cochlear implants, these\ndevices electrically stimulate surviving retinal cells to evoke visual percepts\n(phosphenes). However, the quality of current prosthetic vision is still\nrudimentary. Rather than aiming to restore \"natural\" vision, there is potential\nmerit in borrowing state-of-the-art computer vision algorithms as image\nprocessing techniques to maximize the usefulness of prosthetic vision. Here we\ncombine deep learning--based scene simplification strategies with a\npsychophysically validated computational model of the retina to generate\nrealistic predictions of simulated prosthetic vision, and measure their ability\nto support scene understanding of sighted subjects (virtual patients) in a\nvariety of outdoor scenarios. We show that object segmentation may better\nsupport scene understanding than models based on visual saliency and monocular\ndepth estimation. In addition, we highlight the importance of basing\ntheoretical predictions on biologically realistic models of phosphene shape.\nOverall, this work has the potential to drastically improve the utility of\nprosthetic vision for people blinded from retinal degenerative diseases.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 19:35:33 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Han", "Nicole", "", "University of California, Santa Barbara"], ["Srivastava", "Sudhanshu", "", "University of California, Santa Barbara"], ["Xu", "Aiwen", "", "University of California, Santa Barbara"], ["Klein", "Devi", "", "University of California, Santa Barbara"], ["Beyeler", "Michael", "", "University of California, Santa Barbara"]]}, {"id": "2102.00322", "submitter": "Vaneet Aggarwal", "authors": "Mayank Gupta and Lingjun Chen and Denny Yu and Vaneet Aggarwal", "title": "A Supervised Learning Approach for Robust Health Monitoring using Face\n  Videos", "comments": "The main part of the paper appeared in DFHS'20: Proceedings of the\n  2nd ACM Workshop on Device-Free Human Sensing; while the Supplementary did\n  not appear in the proceedings", "journal-ref": "Proceedings of the 2nd ACM Workshop on Device-Free Human Sensing\n  (DFHS 2020) Nov. 2020 pp. 6-10", "doi": "10.1145/3427772.3429392", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring of cardiovascular activity is highly desired and can enable novel\napplications in diagnosing potential cardiovascular diseases and maintaining an\nindividual's well-being. Currently, such vital signs are measured using\nintrusive contact devices such as an electrocardiogram (ECG), chest straps, and\npulse oximeters that require the patient or the health provider to manually\nimplement. Non-contact, device-free human sensing methods can eliminate the\nneed for specialized heart and blood pressure monitoring equipment. Non-contact\nmethods can have additional advantages since they are scalable with any\nenvironment where video can be captured, can be used for continuous\nmeasurements, and can be used on patients with varying levels of dexterity and\nindependence, from people with physical impairments to infants (e.g., baby\ncamera). In this paper, we used a non-contact method that only requires face\nvideos recorded using commercially-available webcams. These videos were\nexploited to predict the health attributes like pulse rate and variance in\npulse rate. The proposed approach used facial recognition to detect the face in\neach frame of the video using facial landmarks, followed by supervised learning\nusing deep neural networks to train the machine learning model. The videos\ncaptured subjects performing different physical activities that result in\nvarying cardiovascular responses. The proposed method did not require training\ndata from every individual and thus the prediction can be obtained for the new\nindividuals for which there is no prior data; critical in approach\ngeneralization. The approach was also evaluated on a dataset of people with\ndifferent ethnicity. The proposed approach had less than a 4.6\\% error in\npredicting the pulse rate.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 22:03:16 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Gupta", "Mayank", ""], ["Chen", "Lingjun", ""], ["Yu", "Denny", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "2102.00324", "submitter": "Ad\\'in Ram\\'irez Rivera", "authors": "Juan F. Hern\\'andez Albarrac\\'in and Ad\\'in Ram\\'irez Rivera", "title": "Video Reenactment as Inductive Bias for Content-Motion Disentanglement", "comments": "Project page and source code at https://mipl.gitlab.io/mtc-vae/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a self-supervised motion-transfer VAE model to disentangle\nmotion and content from video. Unlike previous work regarding content-motion\ndisentanglement in videos, we adopt a chunk-wise modeling approach and take\nadvantage of the motion information contained in spatiotemporal neighborhoods.\nOur model yields per-chunk representations that can be modeled independently\nand preserve temporal consistency. Hence, we reconstruct whole videos in a\nsingle forward-pass. We extend the ELBO's log-likelihood term and include a\nBlind Reenactment Loss as inductive bias to leverage motion disentanglement,\nunder the assumption that swapping motion features yields reenactment between\ntwo videos. We test our model on recently-proposed disentanglement metrics, and\nshow that it outperforms a variety of methods for video motion-content\ndisentanglement. Experiments on video reenactment show the effectiveness of our\ndisentanglement in the input space where our model outperforms the baselines in\nreconstruction quality and motion alignment.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 22:07:43 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 20:26:05 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Albarrac\u00edn", "Juan F. Hern\u00e1ndez", ""], ["Rivera", "Ad\u00edn Ram\u00edrez", ""]]}, {"id": "2102.00348", "submitter": "Ziyi Liu", "authors": "Jie Yang, Ziyi Liu, Mengchen Lin, Svetlana Yanushkevich, Orly\n  Yadid-Pecht", "title": "Deep Reformulated Laplacian Tone Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Wide dynamic range (WDR) images contain more scene details and contrast when\ncompared to common images. However, it requires tone mapping to process the\npixel values in order to display properly. The details of WDR images can\ndiminish during the tone mapping process. In this work, we address the problem\nby combining a novel reformulated Laplacian pyramid and deep learning. The\nreformulated Laplacian pyramid always decompose a WDR image into two frequency\nbands where the low-frequency band is global feature-oriented, and the\nhigh-frequency band is local feature-oriented. The reformulation preserves the\nlocal features in its original resolution and condenses the global features\ninto a low-resolution image. The generated frequency bands are reconstructed\nand fine-tuned to output the final tone mapped image that can display on the\nscreen with minimum detail and contrast loss. The experimental results\ndemonstrate that the proposed method outperforms state-of-the-art WDR image\ntone mapping methods. The code is made publicly available at\nhttps://github.com/linmc86/Deep-Reformulated-Laplacian-Tone-Mapping.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 01:18:20 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Yang", "Jie", ""], ["Liu", "Ziyi", ""], ["Lin", "Mengchen", ""], ["Yanushkevich", "Svetlana", ""], ["Yadid-Pecht", "Orly", ""]]}, {"id": "2102.00364", "submitter": "Lingtong Kong", "authors": "Lingtong Kong, Xiaohang Yang, Jie Yang", "title": "OAS-Net: Occlusion Aware Sampling Network for Accurate Optical Flow", "comments": "Accepted by ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical flow estimation is an essential step for many real-world computer\nvision tasks. Existing deep networks have achieved satisfactory results by\nmostly employing a pyramidal coarse-to-fine paradigm, where a key process is to\nadopt warped target feature based on previous flow prediction to correlate with\nsource feature for building 3D matching cost volume. However, the warping\noperation can lead to troublesome ghosting problem that results in ambiguity.\nMoreover, occluded areas are treated equally with non occluded regions in most\nexisting works, which may cause performance degradation. To deal with these\nchallenges, we propose a lightweight yet efficient optical flow network, named\nOAS-Net (occlusion aware sampling network) for accurate optical flow. First, a\nnew sampling based correlation layer is employed without noisy warping\noperation. Second, a novel occlusion aware module is presented to make raw cost\nvolume conscious of occluded regions. Third, a shared flow and occlusion\nawareness decoder is adopted for structure compactness. Experiments on Sintel\nand KITTI datasets demonstrate the effectiveness of proposed approaches.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 03:30:31 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Kong", "Lingtong", ""], ["Yang", "Xiaohang", ""], ["Yang", "Jie", ""]]}, {"id": "2102.00367", "submitter": "Dongliang Chang", "authors": "Dongliang Chang, Yixiao Zheng, Zhanyu Ma, Ruoyi Du, Kongming Liang", "title": "Fine-Grained Visual Classification via Simultaneously Learning of\n  Multi-regional Multi-grained Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fine-grained visual classification is a challenging task that recognizes the\nsub-classes belonging to the same meta-class. Large inter-class similarity and\nintra-class variance is the main challenge of this task. Most exiting methods\ntry to solve this problem by designing complex model structures to explore more\nminute and discriminative regions. In this paper, we argue that mining\nmulti-regional multi-grained features is precisely the key to this task.\nSpecifically, we introduce a new loss function, termed top-down spatial\nattention loss (TDSA-Loss), which contains a multi-stage channel constrained\nmodule and a top-down spatial attention module. The multi-stage channel\nconstrained module aims to make the feature channels in different stages\ncategory-aligned. Meanwhile, the top-down spatial attention module uses the\nattention map generated by high-level aligned feature channels to make\nmiddle-level aligned feature channels to focus on particular regions. Finally,\nwe can obtain multiple discriminative regions on high-level feature channels\nand obtain multiple more minute regions within these discriminative regions on\nmiddle-level feature channels. In summary, we obtain multi-regional\nmulti-grained features. Experimental results over four widely used fine-grained\nimage classification datasets demonstrate the effectiveness of the proposed\nmethod. Ablative studies further show the superiority of two modules in the\nproposed method. Codes are available at:\nhttps://github.com/dongliangchang/Top-Down-Spatial-Attention-Loss.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 03:46:10 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Chang", "Dongliang", ""], ["Zheng", "Yixiao", ""], ["Ma", "Zhanyu", ""], ["Du", "Ruoyi", ""], ["Liang", "Kongming", ""]]}, {"id": "2102.00369", "submitter": "Yuyang You", "authors": "Yunkai Yu, Zhihong Yang, Yuyang You, Guozheng Liu, Peiyao Li, Zhicheng\n  Yang, Wenjing Shan", "title": "Spectral Roll-off Points: Estimating Useful Information Under the Basis\n  of Low-frequency Data Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Useful information is the basis for model decisions. Estimating useful\ninformation in feature maps promotes the understanding of the mechanisms of\nneural networks. Low frequency is a prerequisite for useful information in data\nrepresentations, because downscaling operations reduce the communication\nbandwidth. This study proposes the use of spectral roll-off points (SROPs) to\nintegrate the low-frequency condition when estimating useful information. The\ncomputation of an SROP is extended from a 1-D signal to a 2-D image by the\nrequired rotation invariance in image classification tasks. SROP statistics\nacross feature maps are implemented for layer-wise useful information\nestimation. Sanity checks demonstrate that the variation of layer-wise SROP\ndistributions among model input can be used to recognize useful components that\nsupport model decisions. Moreover, the variations of SROPs and accuracy, the\nground truth of useful information of models, are synchronous when adopting\nsufficient training in various model structures. Therefore, SROP is an accurate\nand convenient estimation of useful information. It promotes the explainability\nof artificial intelligence with respect to frequency-domain knowledge.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 03:49:51 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Yu", "Yunkai", ""], ["Yang", "Zhihong", ""], ["You", "Yuyang", ""], ["Liu", "Guozheng", ""], ["Li", "Peiyao", ""], ["Yang", "Zhicheng", ""], ["Shan", "Wenjing", ""]]}, {"id": "2102.00376", "submitter": "Bing Wei", "authors": "Bing Wei (Student Member, IEEE), Kuangrong Hao (Member, IEEE), Lei Gao\n  (Member, IEEE)", "title": "MLMA-Net: multi-level multi-attentional learning for multi-label object\n  detection in textile defect images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the sake of recognizing and classifying textile defects, deep\nlearning-based methods have been proposed and achieved remarkable success in\nsingle-label textile images. However, detecting multi-label defects in a\ntextile image remains challenging due to the coexistence of multiple defects\nand small-size defects. To address these challenges, a multi-level,\nmulti-attentional deep learning network (MLMA-Net) is proposed and built to 1)\nincrease the feature representation ability to detect small-size defects; 2)\ngenerate a discriminative representation that maximizes the capability of\nattending the defect status, which leverages higher-resolution feature maps for\nmultiple defects. Moreover, a multi-label object detection dataset (DHU-ML1000)\nin textile defect images is built to verify the performance of the proposed\nmodel. The results demonstrate that the network extracts more distinctive\nfeatures and has better performance than the state-of-the-art approaches on the\nreal-world industrial dataset.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 04:50:40 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Wei", "Bing", "", "Student Member, IEEE"], ["Hao", "Kuangrong", "", "Member, IEEE"], ["Gao", "Lei", "", "Member, IEEE"]]}, {"id": "2102.00381", "submitter": "Yang Zhang", "authors": "Yang Zhang, Moyun Liu, Yang Yang, Yanwen Guo, Huiming Zhang", "title": "A Unified Light Framework for Real-time Fault Detection of Freight Train\n  Images", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Real-time fault detection for freight trains plays a vital role in\nguaranteeing the security and optimal operation of railway transportation under\nstringent resource requirements. Despite the promising results for deep\nlearning based approaches, the performance of these fault detectors on freight\ntrain images, are far from satisfactory in both accuracy and efficiency. This\npaper proposes a unified light framework to improve detection accuracy while\nsupporting a real-time operation with a low resource requirement. We firstly\ndesign a novel lightweight backbone (RFDNet) to improve the accuracy and reduce\ncomputational cost. Then, we propose a multi region proposal network using\nmulti-scale feature maps generated from RFDNet to improve the detection\nperformance. Finally, we present multi level position-sensitive score maps and\nregion of interest pooling to further improve accuracy with few redundant\ncomputations. Extensive experimental results on public benchmark datasets\nsuggest that our RFDNet can significantly improve the performance of baseline\nnetwork with higher accuracy and efficiency. Experiments on six fault datasets\nshow that our method is capable of real-time detection at over 38 frames per\nsecond and achieves competitive accuracy and lower computation than the\nstate-of-the-art detectors.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 05:10:20 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Zhang", "Yang", ""], ["Liu", "Moyun", ""], ["Yang", "Yang", ""], ["Guo", "Yanwen", ""], ["Zhang", "Huiming", ""]]}, {"id": "2102.00390", "submitter": "Lin Lanbo", "authors": "Lanbo Lin, Yujiu Yang, Zhenhua Guo", "title": "AACP: Model Compression by Accurate and Automatic Channel Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Channel pruning is formulated as a neural architecture search (NAS) problem\nrecently. However, existing NAS-based methods are challenged by huge\ncomputational cost and inflexibility of applications. How to deal with multiple\nsparsity constraints simultaneously and speed up NAS-based channel pruning are\nstill open challenges. In this paper, we propose a novel Accurate and Automatic\nChannel Pruning (AACP) method to address these problems. Firstly, AACP\nrepresents the structure of a model as a structure vector and introduces a\npruning step vector to control the compressing granularity of each layer.\nSecondly, AACP utilizes Pruned Structure Accuracy Estimator (PSAE) to speed up\nthe performance estimation process. Thirdly, AACP proposes Improved\nDifferential Evolution (IDE) algorithm to search the optimal structure vector\neffectively. Because of IDE, AACP can deal with FLOPs constraint and model size\nconstraint simultaneously and efficiently. Our method can be easily applied to\nvarious tasks and achieve state of the art performance. On CIFAR10, our method\nreduces $65\\%$ FLOPs of ResNet110 with an improvement of $0.26\\%$ top-1\naccuracy. On ImageNet, we reduce $42\\%$ FLOPs of ResNet50 with a small loss of\n$0.18\\%$ top-1 accuracy and reduce $30\\%$ FLOPs of MobileNetV2 with a small\nloss of $0.7\\%$ top-1 accuracy. The source code will be released after\npublication.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 06:19:29 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Lin", "Lanbo", ""], ["Yang", "Yujiu", ""], ["Guo", "Zhenhua", ""]]}, {"id": "2102.00408", "submitter": "Ziyi Liu", "authors": "Jie Yang, Ziyi Liu, Ulian Shahnovich, Orly Yadid-Pecht", "title": "Tone Mapping Based on Multi-scale Histogram Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present a novel tone mapping algorithm that can be used for\ndisplaying wide dynamic range (WDR) images on low dynamic range (LDR) devices.\nThe proposed algorithm is mainly motivated by the logarithmic response and\nlocal adaptation features of the human visual system (HVS). HVS perceives\nluminance differently when under different adaptation levels, and therefore our\nalgorithm uses functions built upon different scales to tone map pixels to\ndifferent values. Functions of large scales are used to maintain image\nbrightness consistency and functions of small scales are used to preserve local\ndetail and contrast. An efficient method using local variance has been proposed\nto fuse the values of different scales and to remove artifacts. The algorithm\nutilizes integral images and integral histograms to reduce computation\ncomplexity and processing time. Experimental results show that the proposed\nalgorithm can generate high brightness, good contrast, and appealing images\nthat surpass the performance of many state-of-the-art tone mapping algorithms.\nThis project is available at\nhttps://github.com/jieyang1987/ToneMapping-Based-on-Multi-scale-Histogram-Synthesis.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 08:11:48 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Yang", "Jie", ""], ["Liu", "Ziyi", ""], ["Shahnovich", "Ulian", ""], ["Yadid-Pecht", "Orly", ""]]}, {"id": "2102.00411", "submitter": "Zhi Chen", "authors": "Zhi Chen, Fan Yang, Wenbing Tao", "title": "Cascade Network with Guided Loss and Hybrid Attention for Finding Good\n  Correspondences", "comments": "Accepted by AAAI2021. arXiv admin note: substantial text overlap with\n  arXiv:2007.05706", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding good correspondences is a critical prerequisite in many feature based\ntasks. Given a putative correspondence set of an image pair, we propose a\nneural network which finds correct correspondences by a binary-class classifier\nand estimates relative pose through classified correspondences. First, we\nanalyze that due to the imbalance in the number of correct and wrong\ncorrespondences, the loss function has a great impact on the classification\nresults. Thus, we propose a new Guided Loss that can directly use evaluation\ncriterion (Fn-measure) as guidance to dynamically adjust the objective function\nduring training. We theoretically prove that the perfect negative correlation\nbetween the Guided Loss and Fn-measure, so that the network is always trained\ntowards the direction of increasing Fn-measure to maximize it. We then propose\na hybrid attention block to extract feature, which integrates the Bayesian\nattentive context normalization (BACN) and channel-wise attention (CA). BACN\ncan mine the prior information to better exploit global context and CA can\ncapture complex channel context to enhance the channel awareness of the\nnetwork. Finally, based on our Guided Loss and hybrid attention block, a\ncascade network is designed to gradually optimize the result for more superior\nperformance. Experiments have shown that our network achieves the\nstate-of-the-art performance on benchmark datasets. Our code will be available\nin https://github.com/wenbingtao/GLHA.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 08:33:20 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Chen", "Zhi", ""], ["Yang", "Fan", ""], ["Tao", "Wenbing", ""]]}, {"id": "2102.00424", "submitter": "Alessandro Suglia", "authors": "Alessandro Suglia, Yonatan Bisk, Ioannis Konstas, Antonio Vergari,\n  Emanuele Bastianelli, Andrea Vanzo, Oliver Lemon", "title": "An Empirical Study on the Generalization Power of Neural Representations\n  Learned via Visual Guessing Games", "comments": "Accepted paper for the 16th Conference of the European Chapter of the\n  Association for Computational Linguistics (EACL 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Guessing games are a prototypical instance of the \"learning by interacting\"\nparadigm. This work investigates how well an artificial agent can benefit from\nplaying guessing games when later asked to perform on novel NLP downstream\ntasks such as Visual Question Answering (VQA). We propose two ways to exploit\nplaying guessing games: 1) a supervised learning scenario in which the agent\nlearns to mimic successful guessing games and 2) a novel way for an agent to\nplay by itself, called Self-play via Iterated Experience Learning (SPIEL).\n  We evaluate the ability of both procedures to generalize: an in-domain\nevaluation shows an increased accuracy (+7.79) compared with competitors on the\nevaluation suite CompGuessWhat?!; a transfer evaluation shows improved\nperformance for VQA on the TDIUC dataset in terms of harmonic average accuracy\n(+5.31) thanks to more fine-grained object representations learned via SPIEL.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 10:30:48 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Suglia", "Alessandro", ""], ["Bisk", "Yonatan", ""], ["Konstas", "Ioannis", ""], ["Vergari", "Antonio", ""], ["Bastianelli", "Emanuele", ""], ["Vanzo", "Andrea", ""], ["Lemon", "Oliver", ""]]}, {"id": "2102.00428", "submitter": "Jules Talloen", "authors": "Jules Talloen, Joni Dambre, Alexander Vandesompele", "title": "PyTorch-Hebbian: facilitating local learning in a deep learning\n  framework", "comments": "Presented as a poster at the NeurIPS 2020 Beyond Backpropagation\n  workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, unsupervised local learning, based on Hebb's idea that change in\nsynaptic efficacy depends on the activity of the pre- and postsynaptic neuron\nonly, has shown potential as an alternative training mechanism to\nbackpropagation. Unfortunately, Hebbian learning remains experimental and\nrarely makes it way into standard deep learning frameworks. In this work, we\ninvestigate the potential of Hebbian learning in the context of standard deep\nlearning workflows. To this end, a framework for thorough and systematic\nevaluation of local learning rules in existing deep learning pipelines is\nproposed. Using this framework, the potential of Hebbian learned feature\nextractors for image classification is illustrated. In particular, the\nframework is used to expand the Krotov-Hopfield learning rule to standard\nconvolutional neural networks without sacrificing accuracy compared to\nend-to-end backpropagation. The source code is available at\nhttps://github.com/Joxis/pytorch-hebbian.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 10:53:08 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Talloen", "Jules", ""], ["Dambre", "Joni", ""], ["Vandesompele", "Alexander", ""]]}, {"id": "2102.00436", "submitter": "Xiaosen Wang", "authors": "Xiaosen Wang, Xuanran He, Jingdong Wang, Kun He", "title": "Admix: Enhancing the Transferability of Adversarial Attacks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are known to be extremely vulnerable to adversarial\nexamples under white-box setting. Moreover, the malicious adversaries crafted\non the surrogate (source) model often exhibit black-box transferability on\nother models with the same learning task but having different architectures.\nRecently, various methods have been proposed to boost the adversarial\ntransferability, among which the input transformation is one of the most\neffective approaches. We investigate in this direction and observe that\nexisting transformations are all applied on a single image, which might limit\nthe adversarial transferability. To this end, we propose a new input\ntransformation based attack method called Admix that considers the input image\nand a set of images randomly sampled from other categories. Instead of directly\ncalculating the gradient on the original input, Admix calculates the gradient\non the input image admixed with a small portion of each add-in image while\nusing the original label of the input, to craft more transferable adversaries.\nEmpirical evaluations on standard ImageNet dataset demonstrate that Admix could\nachieve significantly better transferability than existing input transformation\nmethods under both single model setting and ensemble-model setting. By\nincorporating with existing input transformations, our method could further\nimprove the transferability and outperforms the state-of-the-art combination of\ninput transformations by a clear margin when attacking nine advanced defense\nmodels under ensemble-model setting.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 11:40:50 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 03:18:19 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Wang", "Xiaosen", ""], ["He", "Xuanran", ""], ["Wang", "Jingdong", ""], ["He", "Kun", ""]]}, {"id": "2102.00441", "submitter": "Kyung-Wha Park", "authors": "Kyung-Wha Park (1), Jung-Woo Ha (2), JungHoon Lee (3), Sunyoung Kwon\n  (4), Kyung-Min Kim (2), Byoung-Tak Zhang (1 and 5 and 6) ((1)\n  Interdisciplinary Program in Neuroscience, Seoul National University., (2)\n  NAVER AI LAB, NAVER CLOVA., (3) Statistics and Actuarial Science, Soongsil\n  University., (4) School of Biomedical Convergence Engineering, Pusan National\n  University., (5) Department of Computer Science and Engineering, Seoul\n  National University., (6) Surromind Robotics.)", "title": "M2FN: Multi-step Modality Fusion for Advertisement Image Assessment", "comments": "published in Applied Soft Computing", "journal-ref": null, "doi": "10.1016/j.asoc.2021.107116", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Assessing advertisements, specifically on the basis of user preferences and\nad quality, is crucial to the marketing industry. Although recent studies have\nattempted to use deep neural networks for this purpose, these studies have not\nutilized image-related auxiliary attributes, which include embedded text\nfrequently found in ad images. We, therefore, investigated the influence of\nthese attributes on ad image preferences. First, we analyzed large-scale\nreal-world ad log data and, based on our findings, proposed a novel multi-step\nmodality fusion network (M2FN) that determines advertising images likely to\nappeal to user preferences. Our method utilizes auxiliary attributes through\nmultiple steps in the network, which include conditional batch\nnormalization-based low-level fusion and attention-based high-level fusion. We\nverified M2FN on the AVA dataset, which is widely used for aesthetic image\nassessment, and then demonstrated that M2FN can achieve state-of-the-art\nperformance in preference prediction using a real-world ad dataset with rich\nauxiliary attributes.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 12:44:37 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 04:11:47 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2021 13:05:45 GMT"}, {"version": "v4", "created": "Tue, 9 Feb 2021 08:42:25 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Park", "Kyung-Wha", "", "1 and 5 and 6"], ["Ha", "Jung-Woo", "", "1 and 5 and 6"], ["Lee", "JungHoon", "", "1 and 5 and 6"], ["Kwon", "Sunyoung", "", "1 and 5 and 6"], ["Kim", "Kyung-Min", "", "1 and 5 and 6"], ["Zhang", "Byoung-Tak", "", "1 and 5 and 6"]]}, {"id": "2102.00449", "submitter": "Pengrui Quan", "authors": "Pengrui Quan, Ruiming Guo, Mani Srivastava", "title": "Towards Imperceptible Query-limited Adversarial Attacks with Perceptual\n  Feature Fidelity Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, there has been a large amount of work towards fooling\ndeep-learning-based classifiers, particularly for images, via adversarial\ninputs that are visually similar to the benign examples. However, researchers\nusually use Lp-norm minimization as a proxy for imperceptibility, which\noversimplifies the diversity and richness of real-world images and human visual\nperception. In this work, we propose a novel perceptual metric utilizing the\nwell-established connection between the low-level image feature fidelity and\nhuman visual sensitivity, where we call it Perceptual Feature Fidelity Loss. We\nshow that our metric can robustly reflect and describe the imperceptibility of\nthe generated adversarial images validated in various conditions. Moreover, we\ndemonstrate that this metric is highly flexible, which can be conveniently\nintegrated into different existing optimization frameworks to guide the noise\ndistribution for better imperceptibility. The metric is particularly useful in\nthe challenging black-box attack with limited queries, where the\nimperceptibility is hard to achieve due to the non-trivial perturbation power.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 13:32:55 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Quan", "Pengrui", ""], ["Guo", "Ruiming", ""], ["Srivastava", "Mani", ""]]}, {"id": "2102.00456", "submitter": "Yiming Lei", "authors": "Yiming Lei, Hongming Shan, Junping Zhang", "title": "Meta ordinal weighting net for improving lung nodule classification", "comments": "Accepted by ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The progression of lung cancer implies the intrinsic ordinal relationship of\nlung nodules at different stages-from benign to unsure then to malignant. This\nproblem can be solved by ordinal regression methods, which is between\nclassification and regression due to its ordinal label. However, existing\nconvolutional neural network (CNN)-based ordinal regression methods only focus\non modifying classification head based on a randomly sampled mini-batch of\ndata, ignoring the ordinal relationship resided in the data itself. In this\npaper, we propose a Meta Ordinal Weighting Network (MOW-Net) to explicitly\nalign each training sample with a meta ordinal set (MOS) containing a few\nsamples from all classes. During the training process, the MOW-Net learns a\nmapping from samples in MOS to the corresponding class-specific weight. In\naddition, we further propose a meta cross-entropy (MCE) loss to optimize the\nnetwork in a meta-learning scheme. The experimental results demonstrate that\nthe MOW-Net achieves better accuracy than the state-of-the-art ordinal\nregression methods, especially for the unsure class.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 14:00:20 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Lei", "Yiming", ""], ["Shan", "Hongming", ""], ["Zhang", "Junping", ""]]}, {"id": "2102.00463", "submitter": "Shaoshuai Shi", "authors": "Shaoshuai Shi, Li Jiang, Jiajun Deng, Zhe Wang, Chaoxu Guo, Jianping\n  Shi, Xiaogang Wang, Hongsheng Li", "title": "PV-RCNN++: Point-Voxel Feature Set Abstraction With Local Vector\n  Representation for 3D Object Detection", "comments": "Code will be available at https://github.com/open-mmlab/OpenPCDet.\n  arXiv admin note: text overlap with arXiv:1912.13192", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  3D object detection is receiving increasing attention from both industry and\nacademia thanks to its wide applications in various fields. In this paper, we\npropose the Point-Voxel Region based Convolution Neural Networks (PV-RCNNs) for\naccurate 3D detection from point clouds. First, we propose a novel 3D object\ndetector, PV-RCNN-v1, which employs the voxel-to-keypoint scene encoding and\nkeypoint-to-grid RoI feature abstraction two novel steps. These two steps\ndeeply incorporate both 3D voxel CNN and PointNet-based set abstraction for\nlearning discriminative point-cloud features. Second, we propose a more\nadvanced framework, PV-RCNN-v2, for more efficient and accurate 3D detection.\nIt consists of two major improvements, where the first one is the sectorized\nproposal-centric strategy for efficiently producing more representative and\nuniformly distributed keypoints, and the second one is the VectorPool\naggregation to replace set abstraction for better aggregating local point-cloud\nfeatures with much less resource consumption. With these two major\nmodifications, our PV-RCNN-v2 runs more than twice as fast as the v1 version\nwhile still achieving better performance on the large-scale Waymo Open Dataset\nwith 150m * 150m detection range. Extensive experiments demonstrate that our\nproposed PV-RCNNs significantly outperform previous state-of-the-art 3D\ndetection methods on both the Waymo Open Dataset and the highly-competitive\nKITTI benchmark.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 14:51:49 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Shi", "Shaoshuai", ""], ["Jiang", "Li", ""], ["Deng", "Jiajun", ""], ["Wang", "Zhe", ""], ["Guo", "Chaoxu", ""], ["Shi", "Jianping", ""], ["Wang", "Xiaogang", ""], ["Li", "Hongsheng", ""]]}, {"id": "2102.00487", "submitter": "Nori Uday Kiran", "authors": "Hirak Doshi, N. Uday Kiran", "title": "Nonlinear Evolutionary PDE-Based Refinement of Optical Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of this paper is propose a mathematical framework for optical flow\nrefinement with non-quadratic regularization using variational techniques. We\ndemonstrate how the model can be suitably adapted for both rigid and fluid\nmotion estimation. We study the problem as an abstract IVP using an\nevolutionary PDE approach. We show that for a particular choice of constraint\nour model approximates the continuity model with non-quadratic regularization\nusing augmented Lagrangian techniques. We subsequently show the results of our\nalgorithm on different datasets.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 16:35:26 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Doshi", "Hirak", ""], ["Kiran", "N. Uday", ""]]}, {"id": "2102.00500", "submitter": "Sam Polk", "authors": "James M. Murphy and Sam L. Polk", "title": "A Multiscale Environment for Learning by Diffusion", "comments": "35 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.PR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clustering algorithms partition a dataset into groups of similar points. The\nclustering problem is very general, and different partitions of the same\ndataset could be considered correct and useful. To fully understand such data,\nit must be considered at a variety of scales, ranging from coarse to fine. We\nintroduce the Multiscale Environment for Learning by Diffusion (MELD) data\nmodel, which is a family of clusterings parameterized by nonlinear diffusion on\nthe dataset. We show that the MELD data model precisely captures latent\nmultiscale structure in data and facilitates its analysis. To efficiently learn\nthe multiscale structure observed in many real datasets, we introduce the\nMultiscale Learning by Unsupervised Nonlinear Diffusion (M-LUND) clustering\nalgorithm, which is derived from a diffusion process at a range of temporal\nscales. We provide theoretical guarantees for the algorithm's performance and\nestablish its computational efficiency. Finally, we show that the M-LUND\nclustering algorithm detects the latent structure in a range of synthetic and\nreal datasets.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 17:46:19 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Murphy", "James M.", ""], ["Polk", "Sam L.", ""]]}, {"id": "2102.00501", "submitter": "Peyman Setoodeh", "authors": "Farnoosh Heidary, Mehran Yazdi, Maryam Dehghani, and Peyman Setoodeh", "title": "Urban Change Detection by Fully Convolutional Siamese Concatenate\n  Network with Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection (CD) is an important problem in remote sensing, especially\nin disaster time for urban management. Most existing traditional methods for\nchange detection are categorized based on pixel or objects. Object-based models\nare preferred to pixel-based methods for handling very high-resolution remote\nsensing (VHR RS) images. Such methods can benefit from the ongoing research on\ndeep learning. In this paper, a fully automatic change-detection algorithm on\nVHR RS images is proposed that deploys Fully Convolutional Siamese Concatenate\nnetworks (FC-Siam-Conc). The proposed method uses preprocessing and an\nattention gate layer to improve accuracy. Gaussian attention (GA) as a soft\nvisual attention mechanism is used for preprocessing. GA helps the network to\nhandle feature maps like biological visual systems. Since the GA parameters\ncannot be adjusted during network training, an attention gate layer is\nintroduced to play the role of GA with parameters that can be tuned among other\nnetwork parameters. Experimental results obtained on Onera Satellite Change\nDetection (OSCD) and RIVER-CD datasets confirm the superiority of the proposed\narchitecture over the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 17:47:16 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Heidary", "Farnoosh", ""], ["Yazdi", "Mehran", ""], ["Dehghani", "Maryam", ""], ["Setoodeh", "Peyman", ""]]}, {"id": "2102.00508", "submitter": "Merlin Nau", "authors": "Merlin A. Nau, Florian Schiffers, Yunhao Li, Bingjie Xu, Andreas\n  Maier, Jack Tumblin, Marc Walton, Aggelos K. Katsaggelos, Florian\n  Willomitzer, Oliver Cossairt", "title": "SkinScan: Low-Cost 3D-Scanning for Dermatologic Diagnosis and\n  Documentation", "comments": "5 pages, 4 Figures, Submitted at ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The utilization of computational photography becomes increasingly essential\nin the medical field. Today, imaging techniques for dermatology range from\ntwo-dimensional (2D) color imagery with a mobile device to professional\nclinical imaging systems measuring additional detailed three-dimensional (3D)\ndata. The latter are commonly expensive and not accessible to a broad audience.\nIn this work, we propose a novel system and software framework that relies only\non low-cost (and even mobile) commodity devices present in every household to\nmeasure detailed 3D information of the human skin with a\n3D-gradient-illumination-based method. We believe that our system has great\npotential for early-stage diagnosis and monitoring of skin diseases, especially\nin vastly populated or underdeveloped areas.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 18:21:21 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Nau", "Merlin A.", ""], ["Schiffers", "Florian", ""], ["Li", "Yunhao", ""], ["Xu", "Bingjie", ""], ["Maier", "Andreas", ""], ["Tumblin", "Jack", ""], ["Walton", "Marc", ""], ["Katsaggelos", "Aggelos K.", ""], ["Willomitzer", "Florian", ""], ["Cossairt", "Oliver", ""]]}, {"id": "2102.00511", "submitter": "Edgard Chammas", "authors": "Edgard Chammas, Chafic Mokbel", "title": "Fine-tuning Handwriting Recognition systems with Temporal Dropout", "comments": "5 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper introduces a novel method to fine-tune handwriting recognition\nsystems based on Recurrent Neural Networks (RNN). Long Short-Term Memory (LSTM)\nnetworks are good at modeling long sequences but they tend to overfit over\ntime. To improve the system's ability to model sequences, we propose to drop\ninformation at random positions in the sequence. We call our approach Temporal\nDropout (TD). We apply TD at the image level as well to internal network\nrepresentation. We show that TD improves the results on two different datasets.\nOur method outperforms previous state-of-the-art on Rodrigo dataset.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 18:27:08 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Chammas", "Edgard", ""], ["Mokbel", "Chafic", ""]]}, {"id": "2102.00515", "submitter": "Fatih Uysal", "authors": "Fatih Uysal, F{\\i}rat Hardala\\c{c}, Ozan Peker, Tolga Tolunay and Nil\n  Tokg\\\"oz", "title": "Classification of Shoulder X-Ray Images with Deep Learning Ensemble\n  Models", "comments": "This paper is accepted at Applied Sciences, MDPI, 2021, 11(6), 2723.\n  Section: \"Applied Biosciences and Bioengineering\". Special Issue: \"Advancing\n  Biomedical Image Retrieval and Classification for Computer Aided Diagnosis\"", "journal-ref": "Applied Sciences, MDPI, 2021, 11(6), 2723. Section: \"Applied\n  Biosciences and Bioengineering\". Special Issue: \"Advancing Biomedical Image\n  Retrieval and Classification for Computer Aided Diagnosis\"", "doi": "10.3390/app11062723", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fractures occur in the shoulder area, which has a wider range of motion than\nother joints in the body, for various reasons. To diagnose these fractures,\ndata gathered from Xradiation (X-ray), magnetic resonance imaging (MRI), or\ncomputed tomography (CT) are used. This study aims to help physicians by\nclassifying shoulder images taken from X-ray devices as fracture / non-fracture\nwith artificial intelligence. For this purpose, the performances of 26 deep\nlearning-based pretrained models in the detection of shoulder fractures were\nevaluated on the musculoskeletal radiographs (MURA) dataset, and two ensemble\nlearning models (EL1 and EL2) were developed. The pretrained models used are\nResNet, ResNeXt, DenseNet, VGG, Inception, MobileNet, and their spinal fully\nconnected (Spinal FC) versions. In the EL1 and EL2 models developed using\npretrained models with the best performance, test accuracy was 0.8455,0.8472,\nCohens kappa was 0.6907, 0.6942 and the area that was related with fracture\nclass under the receiver operating characteristic (ROC) curve (AUC) was\n0.8862,0.8695. As a result of 28 different classifications in total, the\nhighest test accuracy and Cohens kappa values were obtained in the EL2 model,\nand the highest AUC value was obtained in the EL1 model.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 19:20:04 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 12:09:24 GMT"}, {"version": "v3", "created": "Sat, 20 Mar 2021 18:28:30 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Uysal", "Fatih", ""], ["Hardala\u00e7", "F\u0131rat", ""], ["Peker", "Ozan", ""], ["Tolunay", "Tolga", ""], ["Tokg\u00f6z", "Nil", ""]]}, {"id": "2102.00523", "submitter": "Haofeng Zhang", "authors": "Ziyi Huang, Haofeng Zhang, Andrew Laine, Elsa Angelini, Christine\n  Hendon, Yu Gan", "title": "Co-Seg: An Image Segmentation Framework Against Label Corruption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised deep learning performance is heavily tied to the availability of\nhigh-quality labels for training. Neural networks can gradually overfit\ncorrupted labels if directly trained on noisy datasets, leading to severe\nperformance degradation at test time. In this paper, we propose a novel deep\nlearning framework, namely Co-Seg, to collaboratively train segmentation\nnetworks on datasets which include low-quality noisy labels. Our approach first\ntrains two networks simultaneously to sift through all samples and obtain a\nsubset with reliable labels. Then, an efficient yet easily-implemented label\ncorrection strategy is applied to enrich the reliable subset. Finally, using\nthe updated dataset, we retrain the segmentation network to finalize its\nparameters. Experiments in two noisy labels scenarios demonstrate that our\nproposed model can achieve results comparable to those obtained from supervised\nlearning trained on the noise-free labels. In addition, our framework can be\neasily implemented in any segmentation algorithm to increase its robustness to\nnoisy labels.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 20:01:40 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Huang", "Ziyi", ""], ["Zhang", "Haofeng", ""], ["Laine", "Andrew", ""], ["Angelini", "Elsa", ""], ["Hendon", "Christine", ""], ["Gan", "Yu", ""]]}, {"id": "2102.00529", "submitter": "Lisa Anne Hendricks", "authors": "Lisa Anne Hendricks, John Mellor, Rosalia Schneider, Jean-Baptiste\n  Alayrac, Aida Nematzadeh", "title": "Decoupling the Role of Data, Attention, and Losses in Multimodal\n  Transformers", "comments": "pre-print of MIT Press Publication version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently multimodal transformer models have gained popularity because their\nperformance on language and vision tasks suggest they learn rich\nvisual-linguistic representations. Focusing on zero-shot image retrieval tasks,\nwe study three important factors which can impact the quality of learned\nrepresentations: pretraining data, the attention mechanism, and loss functions.\nBy pretraining models on six datasets, we observe that dataset noise and\nlanguage similarity to our downstream task are important indicators of model\nperformance. Through architectural analysis, we learn that models with a\nmultimodal attention mechanism can outperform deeper models with modality\nspecific attention mechanisms. Finally, we show that successful contrastive\nlosses used in the self-supervised learning literature do not yield similar\nperformance gains when used in multimodal transformers\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 20:36:41 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Hendricks", "Lisa Anne", ""], ["Mellor", "John", ""], ["Schneider", "Rosalia", ""], ["Alayrac", "Jean-Baptiste", ""], ["Nematzadeh", "Aida", ""]]}, {"id": "2102.00534", "submitter": "Siqiao Ruan", "authors": "Siqiao Ruan, Ian Colbert, Ken Kreutz-Delgado, and Srinjoy Das", "title": "Generative and Discriminative Deep Belief Network Classifiers:\n  Comparisons Under an Approximate Computing Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of Deep Learning hardware algorithms for embedded applications is\ncharacterized by challenges such as constraints on device power consumption,\navailability of labeled data, and limited internet bandwidth for frequent\ntraining on cloud servers. To enable low power implementations, we consider\nefficient bitwidth reduction and pruning for the class of Deep Learning\nalgorithms known as Discriminative Deep Belief Networks (DDBNs) for\nembedded-device classification tasks. We train DDBNs with both generative and\ndiscriminative objectives under an approximate computing framework and analyze\ntheir power-at-performance for supervised and semi-supervised applications. We\nalso investigate the out-of-distribution performance of DDBNs when the\ninference data has the same class structure yet is statistically different from\nthe training data owing to dynamic real-time operating environments. Based on\nour analysis, we provide novel insights and recommendations for choice of\ntraining objectives, bitwidth values, and accuracy sensitivity with respect to\nthe amount of labeled data for implementing DDBN inference with minimum power\nconsumption on embedded hardware platforms subject to accuracy tolerances.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 20:57:33 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Ruan", "Siqiao", ""], ["Colbert", "Ian", ""], ["Kreutz-Delgado", "Ken", ""], ["Das", "Srinjoy", ""]]}, {"id": "2102.00554", "submitter": "Torsten Hoefler", "authors": "Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, Alexandra\n  Peste", "title": "Sparsity in Deep Learning: Pruning and growth for efficient inference\n  and training in neural networks", "comments": "90 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.AR cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing energy and performance costs of deep learning have driven the\ncommunity to reduce the size of neural networks by selectively pruning\ncomponents. Similarly to their biological counterparts, sparse networks\ngeneralize just as well, if not better than, the original dense networks.\nSparsity can reduce the memory footprint of regular networks to fit mobile\ndevices, as well as shorten training time for ever growing networks. In this\npaper, we survey prior work on sparsity in deep learning and provide an\nextensive tutorial of sparsification for both inference and training. We\ndescribe approaches to remove and add elements of neural networks, different\ntraining strategies to achieve model sparsity, and mechanisms to exploit\nsparsity in practice. Our work distills ideas from more than 300 research\npapers and provides guidance to practitioners who wish to utilize sparsity\ntoday, as well as to researchers whose goal is to push the frontier forward. We\ninclude the necessary background on mathematical methods in sparsification,\ndescribe phenomena such as early structure adaptation, the intricate relations\nbetween sparsity and the training process, and show techniques for achieving\nacceleration on real hardware. We also define a metric of pruned parameter\nefficiency that could serve as a baseline for comparison of different sparse\nnetworks. We close by speculating on how sparsity can improve future workloads\nand outline major open problems in the field.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 22:48:50 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Hoefler", "Torsten", ""], ["Alistarh", "Dan", ""], ["Ben-Nun", "Tal", ""], ["Dryden", "Nikoli", ""], ["Peste", "Alexandra", ""]]}, {"id": "2102.00565", "submitter": "Mohamed Ibrahim", "authors": "Mohamed R. Ibrahim, James Haworth, Nicola Christie and Tao Cheng", "title": "CyclingNet: Detecting cycling near misses from video streams in complex\n  urban scenes with deep learning", "comments": "13 Pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Cycling is a promising sustainable mode for commuting and leisure in cities,\nhowever, the fear of getting hit or fall reduces its wide expansion as a\ncommuting mode. In this paper, we introduce a novel method called CyclingNet\nfor detecting cycling near misses from video streams generated by a mounted\nfrontal camera on a bike regardless of the camera position, the conditions of\nthe built, the visual conditions and without any restrictions on the riding\nbehaviour. CyclingNet is a deep computer vision model based on convolutional\nstructure embedded with self-attention bidirectional long-short term memory\n(LSTM) blocks that aim to understand near misses from both sequential images of\nscenes and their optical flows. The model is trained on scenes of both safe\nrides and near misses. After 42 hours of training on a single GPU, the model\nshows high accuracy on the training, testing and validation sets. The model is\nintended to be used for generating information that can draw significant\nconclusions regarding cycling behaviour in cities and elsewhere, which could\nhelp planners and policy-makers to better understand the requirement of safety\nmeasures when designing infrastructure or drawing policies. As for future work,\nthe model can be pipelined with other state-of-the-art classifiers and object\ndetectors simultaneously to understand the causality of near misses based on\nfactors related to interactions of road-users, the built and the natural\nenvironments.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 23:59:28 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Ibrahim", "Mohamed R.", ""], ["Haworth", "James", ""], ["Christie", "Nicola", ""], ["Cheng", "Tao", ""]]}, {"id": "2102.00595", "submitter": "Di Xie", "authors": "Weijie Chen and Yilu Guo and Shicai Yang and Zhaoyang Li and Zhenxin\n  Ma and Binbin Chen and Long Zhao and Di Xie and Shiliang Pu and Yueting\n  Zhuang", "title": "Box Re-Ranking: Unsupervised False Positive Suppression for Domain\n  Adaptive Pedestrian Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  False positive is one of the most serious problems brought by agnostic domain\nshift in domain adaptive pedestrian detection. However, it is impossible to\nlabel each box in countless target domains. Therefore, it yields our attention\nto suppress false positive in each target domain in an unsupervised way. In\nthis paper, we model an object detection task into a ranking task among\npositive and negative boxes innovatively, and thus transform a false positive\nsuppression problem into a box re-ranking problem elegantly, which makes it\nfeasible to solve without manual annotation. An attached problem during box\nre-ranking appears that no labeled validation data is available for\ncherrypicking. Considering we aim to keep the detection of true positive\nunchanged, we propose box number alignment, a self-supervised evaluation\nmetric, to prevent the optimized model from capacity degeneration. Extensive\nexperiments conducted on cross-domain pedestrian detection datasets have\ndemonstrated the effectiveness of our proposed framework. Furthermore, the\nextension to two general unsupervised domain adaptive object detection\nbenchmarks also supports our superiority to other state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 02:31:11 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Chen", "Weijie", ""], ["Guo", "Yilu", ""], ["Yang", "Shicai", ""], ["Li", "Zhaoyang", ""], ["Ma", "Zhenxin", ""], ["Chen", "Binbin", ""], ["Zhao", "Long", ""], ["Xie", "Di", ""], ["Pu", "Shiliang", ""], ["Zhuang", "Yueting", ""]]}, {"id": "2102.00596", "submitter": "Yifan Jiang", "authors": "Yifan Jiang, Han Chen, David K. Han, Hanseok Ko", "title": "Few-shot Learning for CT Scan based COVID-19 Diagnosis", "comments": "Accepted to ICASSP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Coronavirus disease 2019 (COVID-19) is a Public Health Emergency of\nInternational Concern infecting more than 40 million people across 188\ncountries and territories. Chest computed tomography (CT) imaging technique\nbenefits from its high diagnostic accuracy and robustness, it has become an\nindispensable way for COVID-19 mass testing. Recently, deep learning approaches\nhave become an effective tool for automatic screening of medical images, and it\nis also being considered for COVID-19 diagnosis. However, the high infection\nrisk involved with COVID-19 leads to relative sparseness of collected labeled\ndata limiting the performance of such methodologies. Moreover, accurately\nlabeling CT images require expertise of radiologists making the process\nexpensive and time-consuming. In order to tackle the above issues, we propose a\nsupervised domain adaption based COVID-19 CT diagnostic method which can\nperform effectively when only a small samples of labeled CT scans are\navailable. To compensate for the sparseness of labeled data, the proposed\nmethod utilizes a large amount of synthetic COVID-19 CT images and adjusts the\nnetworks from the source domain (synthetic data) to the target domain (real\ndata) with a cross-domain training mechanism. Experimental results show that\nthe proposed method achieves state-of-the-art performance on few-shot COVID-19\nCT imaging based diagnostic tasks.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 02:37:49 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Jiang", "Yifan", ""], ["Chen", "Han", ""], ["Han", "David K.", ""], ["Ko", "Hanseok", ""]]}, {"id": "2102.00599", "submitter": "Ti Bai", "authors": "Ti Bai, Dan Nguyen, Biling Wang and Steve Jiang", "title": "Deep High-Resolution Network for Low Dose X-ray CT Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low Dose Computed Tomography (LDCT) is clinically desirable due to the\nreduced radiation to patients. However, the quality of LDCT images is often\nsub-optimal because of the inevitable strong quantum noise. Inspired by their\nunprecedent success in computer vision, deep learning (DL)-based techniques\nhave been used for LDCT denoising. Despite the promising noise removal ability\nof DL models, people have observed that the resolution of the DL-denoised\nimages is compromised, decreasing their clinical value. Aiming at relieving\nthis problem, in this work, we developed a more effective denoiser by\nintroducing a high-resolution network (HRNet). Since HRNet consists of multiple\nbranches of subnetworks to extract multiscale features which are later fused\ntogether, the quality of the generated features can be substantially enhanced,\nleading to improved denoising performance. Experimental results demonstrated\nthat the introduced HRNet-based denoiser outperforms the benchmarked UNet-based\ndenoiser in terms of superior image resolution preservation ability while\ncomparable, if not better, noise suppression ability. Quantitative metrics in\nterms of root-mean-squared-errors (RMSE)/structure similarity index (SSIM)\nshowed that the HRNet-based denoiser can improve the values from 113.80/0.550\n(LDCT) to 55.24/0.745 (HRNet), in comparison to 59.87/0.712 for the UNet-based\ndenoiser.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 02:54:29 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Bai", "Ti", ""], ["Nguyen", "Dan", ""], ["Wang", "Biling", ""], ["Jiang", "Steve", ""]]}, {"id": "2102.00601", "submitter": "Zhizhong Huang", "authors": "Zhizhong Huang and Junping Zhang and Hongming Shan", "title": "RoutingGAN: Routing Age Progression and Regression with Disentangled\n  Learning", "comments": "Accepted by ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although impressive results have been achieved for age progression and\nregression, there remain two major issues in generative adversarial networks\n(GANs)-based methods: 1) conditional GANs (cGANs)-based methods can learn\nvarious effects between any two age groups in a single model, but are\ninsufficient to characterize some specific patterns due to completely shared\nconvolutions filters; and 2) GANs-based methods can, by utilizing several\nmodels to learn effects independently, learn some specific patterns, however,\nthey are cumbersome and require age label in advance. To address these\ndeficiencies and have the best of both worlds, this paper introduces a\ndropout-like method based on GAN~(RoutingGAN) to route different effects in a\nhigh-level semantic feature space. Specifically, we first disentangle the\nage-invariant features from the input face, and then gradually add the effects\nto the features by residual routers that assign the convolution filters to\ndifferent age groups by dropping out the outputs of others. As a result, the\nproposed RoutingGAN can simultaneously learn various effects in a single model,\nwith convolution filters being shared in part to learn some specific effects.\nExperimental results on two benchmarked datasets demonstrate superior\nperformance over existing methods both qualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 02:57:32 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Huang", "Zhizhong", ""], ["Zhang", "Junping", ""], ["Shan", "Hongming", ""]]}, {"id": "2102.00632", "submitter": "Scott H. Hawley", "authors": "Scott H. Hawley and Andrew C. Morrison", "title": "ConvNets for Counting: Object Detection of Transient Phenomena in\n  Steelpan Drums", "comments": "11 pages, 7 figures, submitted to J. Acous. Soc. Am. (JASA) Special\n  Issue on Machine Learning in Acoustics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG physics.app-ph physics.ins-det", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We train an object detector built from convolutional neural networks to count\ninterference fringes in elliptical antinode regions visible in frames of\nhigh-speed video recordings of transient oscillations in Caribbean steelpan\ndrums illuminated by electronic speckle pattern interferometry (ESPI). The\nannotations provided by our model, \"SPNet\" are intended to contribute to the\nunderstanding of time-dependent behavior in such drums by tracking the\ndevelopment of sympathetic vibration modes. The system is trained on a dataset\nof crowdsourced human-annotated images obtained from the Zooniverse Steelpan\nVibrations Project. Due to the relatively small number of human-annotated\nimages, we also train on a large corpus of synthetic images whose visual\nproperties have been matched to those of the real images by using a Generative\nAdversarial Network to perform style transfer. Applying the model to predict\nannotations of thousands of unlabeled video frames, we can track features and\nmeasure oscillations consistent with audio recordings of the same drum strikes.\nOne surprising result is that the machine-annotated video frames reveal\ntransitions between the first and second harmonics of drum notes that\nsignificantly precede such transitions present in the audio recordings. As this\npaper primarily concerns the development of the model, deeper physical insights\nawait its further application.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 04:41:47 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Hawley", "Scott H.", ""], ["Morrison", "Andrew C.", ""]]}, {"id": "2102.00635", "submitter": "Fei Gao", "authors": "Meimei Shang, Fei Gao, Xiang Li, Jingjie Zhu, Lingna Dai", "title": "Bridging Unpaired Facial Photos And Sketches By Line-drawings", "comments": "accepted by ICASSP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a novel method to learn face sketch synthesis\nmodels by using unpaired data. Our main idea is bridging the photo domain\n$\\mathcal{X}$ and the sketch domain $Y$ by using the line-drawing domain\n$\\mathcal{Z}$. Specially, we map both photos and sketches to line-drawings by\nusing a neural style transfer method, i.e. $F: \\mathcal{X}/\\mathcal{Y} \\mapsto\n\\mathcal{Z}$. Consequently, we obtain \\textit{pseudo paired data}\n$(\\mathcal{Z}, \\mathcal{Y})$, and can learn the mapping $G:\\mathcal{Z} \\mapsto\n\\mathcal{Y}$ in a supervised learning manner. In the inference stage, given a\nfacial photo, we can first transfer it to a line-drawing and then to a sketch\nby $G \\circ F$. Additionally, we propose a novel stroke loss for generating\ndifferent types of strokes. Our method, termed sRender, accords well with human\nartists' rendering process. Experimental results demonstrate that sRender can\ngenerate multi-style sketches, and significantly outperforms existing unpaired\nimage-to-image translation methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 04:51:46 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 03:53:42 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 07:47:27 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Shang", "Meimei", ""], ["Gao", "Fei", ""], ["Li", "Xiang", ""], ["Zhu", "Jingjie", ""], ["Dai", "Lingna", ""]]}, {"id": "2102.00645", "submitter": "Jiangpeng He", "authors": "Jiangpeng He, Runyu Mao, Zeman Shao, Janine L. Wright, Deborah A.\n  Kerr, Carol J. Boushey and Fengqing Zhu", "title": "An End-to-End Food Image Analysis System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep learning techniques have enabled advances in image-based dietary\nassessment such as food recognition and food portion size estimation. Valuable\ninformation on the types of foods and the amount consumed are crucial for\nprevention of many chronic diseases. However, existing methods for automated\nimage-based food analysis are neither end-to-end nor are capable of processing\nmultiple tasks (e.g., recognition and portion estimation) together, making it\ndifficult to apply to real life applications. In this paper, we propose an\nimage-based food analysis framework that integrates food localization,\nclassification and portion size estimation. Our proposed framework is\nend-to-end, i.e., the input can be an arbitrary food image containing multiple\nfood items and our system can localize each single food item with its\ncorresponding predicted food type and portion size. We also improve the single\nfood portion estimation by consolidating localization results with a food\nenergy distribution map obtained by conditional GAN to generate a four-channel\nRGB-Distribution image. Our end-to-end framework is evaluated on a real life\nfood image dataset collected from a nutrition feeding study.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 05:36:20 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["He", "Jiangpeng", ""], ["Mao", "Runyu", ""], ["Shao", "Zeman", ""], ["Wright", "Janine L.", ""], ["Kerr", "Deborah A.", ""], ["Boushey", "Carol J.", ""], ["Zhu", "Fengqing", ""]]}, {"id": "2102.00648", "submitter": "Shu Zhao", "authors": "Shu Zhao, Dayan Wu, Yucan Zhou, Bo Li and Weiping Wang", "title": "Rescuing Deep Hashing from Dead Bits Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep hashing methods have shown great retrieval accuracy and efficiency in\nlarge-scale image retrieval. How to optimize discrete hash bits is always the\nfocus in deep hashing methods. A common strategy in these methods is to adopt\nan activation function, e.g. $\\operatorname{sigmoid}(\\cdot)$ or\n$\\operatorname{tanh}(\\cdot)$, and minimize a quantization loss to approximate\ndiscrete values. However, this paradigm may make more and more hash bits stuck\ninto the wrong saturated area of the activation functions and never escaped. We\ncall this problem \"Dead Bits Problem~(DBP)\". Besides, the existing quantization\nloss will aggravate DBP as well. In this paper, we propose a simple but\neffective gradient amplifier which acts before activation functions to\nalleviate DBP. Moreover, we devise an error-aware quantization loss to further\nalleviate DBP. It avoids the negative effect of quantization loss based on the\nsimilarity between two images. The proposed gradient amplifier and error-aware\nquantization loss are compatible with a variety of deep hashing methods.\nExperimental results on three datasets demonstrate the efficiency of the\nproposed gradient amplifier and the error-aware quantization loss.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 05:51:40 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Zhao", "Shu", ""], ["Wu", "Dayan", ""], ["Zhou", "Yucan", ""], ["Li", "Bo", ""], ["Wang", "Weiping", ""]]}, {"id": "2102.00649", "submitter": "Eadom Dessalene", "authors": "Eadom Dessalene, Chinmaya Devaraj, Michael Maynord, Cornelia\n  Fermuller, and Yiannis Aloimonos", "title": "Forecasting Action through Contact Representations from First Person\n  Video", "comments": "12 pages, 5 figures. in IEEE Transactions on Pattern Analysis and\n  Machine Intelligence, 2021", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3055233", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human actions involving hand manipulations are structured according to the\nmaking and breaking of hand-object contact, and human visual understanding of\naction is reliant on anticipation of contact as is demonstrated by pioneering\nwork in cognitive science. Taking inspiration from this, we introduce\nrepresentations and models centered on contact, which we then use in action\nprediction and anticipation. We annotate a subset of the EPIC Kitchens dataset\nto include time-to-contact between hands and objects, as well as segmentations\nof hands and objects. Using these annotations we train the Anticipation Module,\na module producing Contact Anticipation Maps and Next Active Object\nSegmentations - novel low-level representations providing temporal and spatial\ncharacteristics of anticipated near future action. On top of the Anticipation\nModule we apply Egocentric Object Manipulation Graphs (Ego-OMG), a framework\nfor action anticipation and prediction. Ego-OMG models longer term temporal\nsemantic relations through the use of a graph modeling transitions between\ncontact delineated action states. Use of the Anticipation Module within Ego-OMG\nproduces state-of-the-art results, achieving 1st and 2nd place on the unseen\nand seen test sets, respectively, of the EPIC Kitchens Action Anticipation\nChallenge, and achieving state-of-the-art results on the tasks of action\nanticipation and action prediction over EPIC Kitchens. We perform ablation\nstudies over characteristics of the Anticipation Module to evaluate their\nutility.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 05:52:57 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Dessalene", "Eadom", ""], ["Devaraj", "Chinmaya", ""], ["Maynord", "Michael", ""], ["Fermuller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "2102.00650", "submitter": "Liangchen Song", "authors": "Helong Zhou, Liangchen Song, Jiajie Chen, Ye Zhou, Guoli Wang, Junsong\n  Yuan, Qian Zhang", "title": "Rethinking Soft Labels for Knowledge Distillation: A Bias-Variance\n  Tradeoff Perspective", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge distillation is an effective approach to leverage a well-trained\nnetwork or an ensemble of them, named as the teacher, to guide the training of\na student network. The outputs from the teacher network are used as soft labels\nfor supervising the training of a new network. Recent studies\n\\citep{muller2019does,yuan2020revisiting} revealed an intriguing property of\nthe soft labels that making labels soft serves as a good regularization to the\nstudent network. From the perspective of statistical learning, regularization\naims to reduce the variance, however how bias and variance change is not clear\nfor training with soft labels. In this paper, we investigate the bias-variance\ntradeoff brought by distillation with soft labels. Specifically, we observe\nthat during training the bias-variance tradeoff varies sample-wisely. Further,\nunder the same distillation temperature setting, we observe that the\ndistillation performance is negatively associated with the number of some\nspecific samples, which are named as regularization samples since these samples\nlead to bias increasing and variance decreasing. Nevertheless, we empirically\nfind that completely filtering out regularization samples also deteriorates\ndistillation performance. Our discoveries inspired us to propose the novel\nweighted soft labels to help the network adaptively handle the sample-wise\nbias-variance tradeoff. Experiments on standard evaluation benchmarks validate\nthe effectiveness of our method. Our code is available at\n\\url{https://github.com/bellymonster/Weighted-Soft-Label-Distillation}.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 05:53:04 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Zhou", "Helong", ""], ["Song", "Liangchen", ""], ["Chen", "Jiajie", ""], ["Zhou", "Ye", ""], ["Wang", "Guoli", ""], ["Yuan", "Junsong", ""], ["Zhang", "Qian", ""]]}, {"id": "2102.00663", "submitter": "Kaushik Dutta", "authors": "Kaushik Dutta", "title": "Densely Connected Recurrent Residual (Dense R2UNet) Convolutional Neural\n  Network for Segmentation of Lung CT Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Learning networks have established themselves as providing state of art\nperformance for semantic segmentation. These techniques are widely applied\nspecifically to medical detection, segmentation and classification. The advent\nof the U-Net based architecture has become particularly popular for this\napplication. In this paper we present the Dense Recurrent Residual\nConvolutional Neural Network(Dense R2U CNN) which is a synthesis of Recurrent\nCNN, Residual Network and Dense Convolutional Network based on the U-Net model\narchitecture. The residual unit helps training deeper network, while the dense\nrecurrent layers enhances feature propagation needed for segmentation. The\nproposed model tested on the benchmark Lung Lesion dataset showed better\nperformance on segmentation tasks than its equivalent models.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 06:34:10 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Dutta", "Kaushik", ""]]}, {"id": "2102.00670", "submitter": "Zhenqi Fu", "authors": "Zhenqi Fu, Xueyang Fu, Yue Huang, and Xinghao Ding", "title": "Twice Mixing: A Rank Learning based Quality Assessment Approach for\n  Underwater Image Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To improve the quality of underwater images, various kinds of underwater\nimage enhancement (UIE) operators have been proposed during the past few years.\nHowever, the lack of effective objective evaluation methods limits the further\ndevelopment of UIE techniques. In this paper, we propose a novel rank learning\nguided no-reference quality assessment method for UIE. Our approach, termed\nTwice Mixing, is motivated by the observation that a mid-quality image can be\ngenerated by mixing a high-quality image with its low-quality version. Typical\nmixup algorithms linearly interpolate a given pair of input data. However, the\nhuman visual system is non-uniformity and non-linear in processing images.\nTherefore, instead of directly training a deep neural network based on the\nmixed images and their absolute scores calculated by linear combinations, we\npropose to train a Siamese Network to learn their quality rankings. Twice\nMixing is trained based on an elaborately formulated self-supervision\nmechanism. Specifically, before each iteration, we randomly generate two mixing\nratios which will be employed for both generating virtual images and guiding\nthe network training. In the test phase, a single branch of the network is\nextracted to predict the quality rankings of different UIE outputs. We conduct\nextensive experiments on both synthetic and real-world datasets. Experimental\nresults demonstrate that our approach outperforms the previous methods\nsignificantly.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 07:13:39 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Fu", "Zhenqi", ""], ["Fu", "Xueyang", ""], ["Huang", "Yue", ""], ["Ding", "Xinghao", ""]]}, {"id": "2102.00676", "submitter": "Zhenqi Fu", "authors": "Zhenqi Fu, Xiaopeng Lin, Wu Wang, Yue Huang, and Xinghao Ding", "title": "Underwater Image Enhancement via Learning Water Type Desensitized\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For underwater applications, the effects of light absorption and scattering\nresult in image degradation. Moreover, the complex and changeable imaging\nenvironment makes it difficult to provide a universal enhancement solution to\ncope with the diversity of water types. In this letter, we present a novel\nunderwater image enhancement (UIE) framework termed SCNet to address the above\nissues. SCNet is based on normalization schemes across both spatial and channel\ndimensions with the key idea of learning water type desensitized features.\nConsidering the diversity of degradation is mainly rooted in the strong\ncorrelation among pixels, we apply whitening to de-correlates activations\nacross spatial dimensions for each instance in a mini-batch. We also eliminate\nchannel-wise correlation by standardizing and re-injecting the first two\nmoments of the activations across channels. The normalization schemes of\nspatial and channel dimensions are performed at each scale of the U-Net to\nobtain multi-scale representations. With such latent encodings, the decoder can\neasily reconstruct the clean signal, and unaffected by the distortion types\ncaused by the water. Experimental results on two real-world UIE datasets show\nthat the proposed approach can successfully enhance images with diverse water\ntypes, and achieves competitive performance in visual quality improvement.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 07:34:54 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Fu", "Zhenqi", ""], ["Lin", "Xiaopeng", ""], ["Wang", "Wu", ""], ["Huang", "Yue", ""], ["Ding", "Xinghao", ""]]}, {"id": "2102.00682", "submitter": "Emanuele Dalsasso", "authors": "Emanuele Dalsasso, In\\`es Meraoumia, Lo\\\"ic Denis, Florence Tupin", "title": "Exploiting multi-temporal information for improved speckle reduction of\n  Sentinel-1 SAR images by deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning approaches show unprecedented results for speckle reduction in\nSAR amplitude images. The wide availability of multi-temporal stacks of SAR\nimages can improve even further the quality of denoising. In this paper, we\npropose a flexible yet efficient way to integrate temporal information into a\ndeep neural network for speckle suppression. Archives provide access to long\ntime-series of SAR images, from which multi-temporal averages can be computed\nwith virtually no remaining speckle fluctuations. The proposed method combines\nthis multi-temporal average and the image at a given date in the form of a\nratio image and uses a state-of-the-art neural network to remove the speckle in\nthis ratio image. This simple strategy is shown to offer a noticeable\nimprovement compared to filtering the original image without knowledge of the\nmulti-temporal average.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 07:48:55 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Dalsasso", "Emanuele", ""], ["Meraoumia", "In\u00e8s", ""], ["Denis", "Lo\u00efc", ""], ["Tupin", "Florence", ""]]}, {"id": "2102.00689", "submitter": "Rushuang Xu", "authors": "Rushuang Xu, MyeongAh Cho, and Sangyoun Lee", "title": "A NIR-to-VIS face recognition via part adaptive and relation attention\n  module", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the face recognition application scenario, we need to process facial\nimages captured in various conditions, such as at night by near-infrared (NIR)\nsurveillance cameras. The illumination difference between NIR and visible-light\n(VIS) causes a domain gap between facial images, and the variations in pose and\nemotion also make facial matching more difficult. Heterogeneous face\nrecognition (HFR) has difficulties in domain discrepancy, and many studies have\nfocused on extracting domain-invariant features, such as facial part relational\ninformation. However, when pose variation occurs, the facial component position\nchanges, and a different part relation is extracted. In this paper, we propose\na part relation attention module that crops facial parts obtained through a\nsemantic mask and performs relational modeling using each of these\nrepresentative features. Furthermore, we suggest component adaptive triplet\nloss function using adaptive weights for each part to reduce the intra-class\nidentity regardless of the domain as well as pose. Finally, our method exhibits\na performance improvement in the CASIA NIR-VIS 2.0 and achieves superior result\nin the BUAA-VisNir with large pose and emotion variations.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 08:13:39 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Xu", "Rushuang", ""], ["Cho", "MyeongAh", ""], ["Lee", "Sangyoun", ""]]}, {"id": "2102.00690", "submitter": "Yuxuan Liu", "authors": "Yuxuan Liu, Yuan Yixuan, Ming Liu", "title": "Ground-aware Monocular 3D Object Detection for Autonomous Driving", "comments": "8 pages, 6 figures, accepted by IEEE Robotics and Automation Letters\n  (RA-L)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the 3D position and orientation of objects in the environment with\na single RGB camera is a critical and challenging task for low-cost urban\nautonomous driving and mobile robots. Most of the existing algorithms are based\non the geometric constraints in 2D-3D correspondence, which stems from generic\n6D object pose estimation. We first identify how the ground plane provides\nadditional clues in depth reasoning in 3D detection in driving scenes. Based on\nthis observation, we then improve the processing of 3D anchors and introduce a\nnovel neural network module to fully utilize such application-specific priors\nin the framework of deep learning. Finally, we introduce an efficient neural\nnetwork embedded with the proposed module for 3D object detection. We further\nverify the power of the proposed module with a neural network designed for\nmonocular depth prediction. The two proposed networks achieve state-of-the-art\nperformances on the KITTI 3D object detection and depth prediction benchmarks,\nrespectively. The code will be published in\nhttps://www.github.com/Owen-Liuyuxuan/visualDet3D\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 08:18:24 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Liu", "Yuxuan", ""], ["Yixuan", "Yuan", ""], ["Liu", "Ming", ""]]}, {"id": "2102.00692", "submitter": "Emanuele Dalsasso", "authors": "Nicolas Gasnier, Emanuele Dalsasso, Lo\\\"ic Denis, Florence Tupin", "title": "Despeckling Sentinel-1 GRD images by deep learning and application to\n  narrow river segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a despeckling method for Sentinel-1 GRD images based on\nthe recently proposed framework \"SAR2SAR\": a self-supervised training strategy.\nTraining the deep neural network on collections of Sentinel 1 GRD images leads\nto a despeckling algorithm that is robust to space-variant spatial correlations\nof speckle. Despeckled images improve the detection of structures like narrow\nrivers. We apply a detector based on exogenous information and a linear\nfeatures detector and show that rivers are better segmented when the processing\nchain is applied to images pre-processed by our despeckling neural network.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 08:24:25 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Gasnier", "Nicolas", ""], ["Dalsasso", "Emanuele", ""], ["Denis", "Lo\u00efc", ""], ["Tupin", "Florence", ""]]}, {"id": "2102.00696", "submitter": "Selim Furkan Tekin", "authors": "Selim Furkan Tekin, Oguzhan Karaahmetoglu, Fatih Ilhan, Ismail Balaban\n  and Suleyman Serdar Kozat", "title": "Spatio-temporal Weather Forecasting and Attention Mechanism on\n  Convolutional LSTMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical weather forecasting on high-resolution physical models consume\nhours of computations on supercomputers. Application of deep learning and\nmachine learning methods in forecasting revealed new solutions in this area. In\nthis paper, we forecast high-resolution numeric weather data using both input\nweather data and observations by providing a novel deep learning architecture.\nWe formulate the problem as spatio-temporal prediction. Our model is composed\nof Convolutional Long-short Term Memory, and Convolutional Neural Network units\nwith encoder-decoder structure. We enhance the short-long term performance and\ninterpretability with an attention and a context matcher mechanism. We perform\nexperiments on high-scale, real-life, benchmark numerical weather dataset, ERA5\nhourly data on pressure levels, and forecast the temperature. The results show\nsignificant improvements in capturing both spatial and temporal correlations\nwith attention matrices focusing on different parts of the input series. Our\nmodel obtains the best validation and the best test score among the baseline\nmodels, including ConvLSTM forecasting network and U-Net. We provide\nqualitative and quantitative results and show that our model forecasts 10 time\nsteps with 3 hour frequency with an average of 2 degrees error. Our code and\nthe data are publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 08:30:42 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Tekin", "Selim Furkan", ""], ["Karaahmetoglu", "Oguzhan", ""], ["Ilhan", "Fatih", ""], ["Balaban", "Ismail", ""], ["Kozat", "Suleyman Serdar", ""]]}, {"id": "2102.00713", "submitter": "Jian Zhang", "authors": "Jian Zhang, Ying Tai, Taiping Yao, Jia Meng, Shouhong Ding, Chengjie\n  Wang, Jilin Li, Feiyue Huang, Rongrong Ji", "title": "Aurora Guard: Reliable Face Anti-Spoofing via Mobile Lighting System", "comments": "arXiv admin note: substantial text overlap with arXiv:1902.10311", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face authentication on mobile end has been widely applied in various\nscenarios. Despite the increasing reliability of cutting-edge face\nauthentication/verification systems to variations like blinking eye and subtle\nfacial expression, anti-spoofing against high-resolution rendering replay of\npaper photos or digital videos retains as an open problem. In this paper, we\npropose a simple yet effective face anti-spoofing system, termed Aurora Guard\n(AG). Our system firstly extracts the normal cues via light reflection\nanalysis, and then adopts an end-to-end trainable multi-task Convolutional\nNeural Network (CNN) to accurately recover subjects' intrinsic depth and\nmaterial map to assist liveness classification, along with the light CAPTCHA\nchecking mechanism in the regression branch to further improve the system\nreliability. Experiments on public Replay-Attack and CASIA datasets demonstrate\nthe merits of our proposed method over the state-of-the-arts. We also conduct\nextensive experiments on a large-scale dataset containing 12,000 live and\ndiverse spoofing samples, which further validates the generalization ability of\nour method in the wild.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 09:17:18 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Zhang", "Jian", ""], ["Tai", "Ying", ""], ["Yao", "Taiping", ""], ["Meng", "Jia", ""], ["Ding", "Shouhong", ""], ["Wang", "Chengjie", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""], ["Ji", "Rongrong", ""]]}, {"id": "2102.00719", "submitter": "Omri Bar", "authors": "Daniel Neimark, Omri Bar, Maya Zohar, Dotan Asselmann", "title": "Video Transformer Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents VTN, a transformer-based framework for video recognition.\nInspired by recent developments in vision transformers, we ditch the standard\napproach in video action recognition that relies on 3D ConvNets and introduce a\nmethod that classifies actions by attending to the entire video sequence\ninformation. Our approach is generic and builds on top of any given 2D spatial\nnetwork. In terms of wall runtime, it trains $16.1\\times$ faster and runs\n$5.1\\times$ faster during inference while maintaining competitive accuracy\ncompared to other state-of-the-art methods. It enables whole video analysis,\nvia a single end-to-end pass, while requiring $1.5\\times$ fewer GFLOPs. We\nreport competitive results on Kinetics-400 and present an ablation study of VTN\nproperties and the trade-off between accuracy and inference speed. We hope our\napproach will serve as a new baseline and start a fresh line of research in the\nvideo recognition domain. Code and models are available at:\nhttps://github.com/bomri/SlowFast/blob/master/projects/vtn/README.md\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 09:29:10 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 08:20:17 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Neimark", "Daniel", ""], ["Bar", "Omri", ""], ["Zohar", "Maya", ""], ["Asselmann", "Dotan", ""]]}, {"id": "2102.00721", "submitter": "Quentin Paletta", "authors": "Quentin Paletta, Guillaume Arbod and Joan Lasenby", "title": "Benchmarking of Deep Learning Irradiance Forecasting Models from Sky\n  Images -- an in-depth Analysis", "comments": "Manuscript accepted for publication in Solar Energy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of industrial applications, such as smart grids, power plant\noperation, hybrid system management or energy trading, could benefit from\nimproved short-term solar forecasting, addressing the intermittent energy\nproduction from solar panels. However, current approaches to modelling the\ncloud cover dynamics from sky images still lack precision regarding the spatial\nconfiguration of clouds, their temporal dynamics and physical interactions with\nsolar radiation. Benefiting from a growing number of large datasets, data\ndriven methods are being developed to address these limitations with promising\nresults. In this study, we compare four commonly used Deep Learning\narchitectures trained to forecast solar irradiance from sequences of\nhemispherical sky images and exogenous variables. To assess the relative\nperformance of each model, we used the Forecast Skill metric based on the smart\npersistence model, as well as ramp and time distortion metrics. The results\nshow that encoding spatiotemporal aspects of the sequence of sky images greatly\nimproved the predictions with 10 min ahead Forecast Skill reaching 20.4% on the\ntest year. However, based on the experimental data, we conclude that, with a\ncommon setup, Deep Learning models tend to behave just as a 'very smart\npersistence model', temporally aligned with the persistence model while\nmitigating its most penalising errors. Thus, despite being captured by the sky\ncameras, models often miss fundamental events causing large irradiance changes\nsuch as clouds obscuring the sun. We hope that our work will contribute to a\nshift of this approach to irradiance forecasting, from reactive to\nanticipatory.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 09:31:14 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 09:08:29 GMT"}, {"version": "v3", "created": "Mon, 24 May 2021 14:04:51 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Paletta", "Quentin", ""], ["Arbod", "Guillaume", ""], ["Lasenby", "Joan", ""]]}, {"id": "2102.00751", "submitter": "Jason Lin", "authors": "Jason Z. Lin and Jelena Bradic", "title": "Learning to Combat Noisy Labels via Classification Margins", "comments": "21 pages, 8 sets of figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A deep neural network trained on noisy labels is known to quickly lose its\npower to discriminate clean instances from noisy ones. After the early learning\nphase has ended, the network memorizes the noisy instances, which leads to a\ndegradation in generalization performance. To resolve this issue, we propose\nMARVEL (MARgins Via Early Learning), where we track the goodness of \"fit\" for\nevery instance by maintaining an epoch-history of its classification margins.\nBased on consecutive negative margins, we discard suspected noisy instances by\nzeroing out their weights. In addition, MARVEL+ upweights arduous instances\nenabling the network to learn a more nuanced representation of the\nclassification boundary. Experimental results on benchmark datasets with\nsynthetic label noise show that MARVEL outperforms other baselines consistently\nacross different noise levels, with a significantly larger margin under\nasymmetric noise.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 10:35:25 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Lin", "Jason Z.", ""], ["Bradic", "Jelena", ""]]}, {"id": "2102.00769", "submitter": "Yukai Shi", "authors": "Yukai Shi, Sen Zhang, Chenxing Zhou, Xiaodan Liang, Xiaojun Yang,\n  Liang Lin", "title": "GTAE: Graph-Transformer based Auto-Encoders for Linguistic-Constrained\n  Text Style Transfer", "comments": "The first two authors share equal-authorship;\n  Code:https://github.com/SenZHANG-GitHub/graph-text-style-transfer ;\n  benchmark: https://github.com/ykshi/text-style-transfer-benchmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-parallel text style transfer has attracted increasing research interests\nin recent years. Despite successes in transferring the style based on the\nencoder-decoder framework, current approaches still lack the ability to\npreserve the content and even logic of original sentences, mainly due to the\nlarge unconstrained model space or too simplified assumptions on latent\nembedding space. Since language itself is an intelligent product of humans with\ncertain grammars and has a limited rule-based model space by its nature,\nrelieving this problem requires reconciling the model capacity of deep neural\nnetworks with the intrinsic model constraints from human linguistic rules. To\nthis end, we propose a method called Graph Transformer based Auto Encoder\n(GTAE), which models a sentence as a linguistic graph and performs feature\nextraction and style transfer at the graph level, to maximally retain the\ncontent and the linguistic structure of original sentences. Quantitative\nexperiment results on three non-parallel text style transfer tasks show that\nour model outperforms state-of-the-art methods in content preservation, while\nachieving comparable performance on transfer accuracy and sentence naturalness.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 11:08:45 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Shi", "Yukai", ""], ["Zhang", "Sen", ""], ["Zhou", "Chenxing", ""], ["Liang", "Xiaodan", ""], ["Yang", "Xiaojun", ""], ["Lin", "Liang", ""]]}, {"id": "2102.00783", "submitter": "Andreas Kofler", "authors": "Andreas Kofler, Markus Haltmeier, Tobias Schaeffter and Christoph\n  Kolbitsch", "title": "An End-To-End-Trainable Iterative Network Architecture for Accelerated\n  Radial Multi-Coil 2D Cine MR Image Reconstruction", "comments": null, "journal-ref": null, "doi": "10.1002/mp.14809", "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Purpose: Iterative Convolutional Neural Networks (CNNs) which resemble\nunrolled learned iterative schemes have shown to consistently deliver\nstate-of-the-art results for image reconstruction problems across different\nimaging modalities. However, because these methodes include the forward model\nin the architecture, their applicability is often restricted to either\nrelatively small reconstruction problems or to problems with operators which\nare computationally cheap to compute. As a consequence, they have so far not\nbeen applied to dynamic non-Cartesian multi-coil reconstruction problems.\nMethods: In this work, we propose a CNN-architecture for image reconstruction\nof accelerated 2D radial cine MRI with multiple receiver coils. The network is\nbased on a computationally light CNN-component and a subsequent conjugate\ngradient (CG) method which can be jointly trained end-to-end using an efficient\ntraining strategy. We investigate the proposed training-strategy and compare\nour method to other well-known reconstruction techniques with learned and\nnon-learned regularization methods. Results: Our proposed method outperforms\nall other methods based on non-learned regularization. Further, it performs\nsimilar or better than a CNN-based method employing a 3D U-Net and a method\nusing adaptive dictionary learning. In addition, we empirically demonstrate\nthat even by training the network with only iteration, it is possible to\nincrease the length of the network at test time and further improve the\nresults. Conclusions: End-to-end training allows to highly reduce the number of\ntrainable parameters of and stabilize the reconstruction network. Further,\nbecause it is possible to change the length of the network at test time, the\nneed to find a compromise between the complexity of the CNN-block and the\nnumber of iterations in each CG-block becomes irrelevant.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 11:42:04 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Kofler", "Andreas", ""], ["Haltmeier", "Markus", ""], ["Schaeffter", "Tobias", ""], ["Kolbitsch", "Christoph", ""]]}, {"id": "2102.00798", "submitter": "Pu Sun", "authors": "Pu Sun, Yuezun Li, Honggang Qi and Siwei Lyu", "title": "Landmark Breaker: Obstructing DeepFake By Disturbing Landmark Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent development of Deep Neural Networks (DNN) has significantly\nincreased the realism of AI-synthesized faces, with the most notable examples\nbeing the DeepFakes. The DeepFake technology can synthesize a face of target\nsubject from a face of another subject, while retains the same face attributes.\nWith the rapidly increased social media portals (Facebook, Instagram, etc),\nthese realistic fake faces rapidly spread though the Internet, causing a broad\nnegative impact to the society. In this paper, we describe Landmark Breaker,\nthe first dedicated method to disrupt facial landmark extraction, and apply it\nto the obstruction of the generation of DeepFake videos.Our motivation is that\ndisrupting the facial landmark extraction can affect the alignment of input\nface so as to degrade the DeepFake quality. Our method is achieved using\nadversarial perturbations. Compared to the detection methods that only work\nafter DeepFake generation, Landmark Breaker goes one step ahead to prevent\nDeepFake generation. The experiments are conducted on three state-of-the-art\nfacial landmark extractors using the recent Celeb-DF dataset.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 12:27:08 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Sun", "Pu", ""], ["Li", "Yuezun", ""], ["Qi", "Honggang", ""], ["Lyu", "Siwei", ""]]}, {"id": "2102.00801", "submitter": "Kun Yan", "authors": "Kun Yan, Zied Bouraoui, Ping Wang, Shoaib Jameel, Steven Schockaert", "title": "Few-shot Image Classification with Multi-Facet Prototypes", "comments": "Accepted by ICASSP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of few-shot learning (FSL) is to learn how to recognize image\ncategories from a small number of training examples. A central challenge is\nthat the available training examples are normally insufficient to determine\nwhich visual features are most characteristic of the considered categories. To\naddress this challenge, we organize these visual features into facets, which\nintuitively group features of the same kind (e.g. features that are relevant to\nshape, color, or texture). This is motivated from the assumption that (i) the\nimportance of each facet differs from category to category and (ii) it is\npossible to predict facet importance from a pre-trained embedding of the\ncategory names. In particular, we propose an adaptive similarity measure,\nrelying on predicted facet importance weights for a given set of categories.\nThis measure can be used in combination with a wide array of existing\nmetric-based methods. Experiments on miniImageNet and CUB show that our\napproach improves the state-of-the-art in metric-based FSL.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 12:43:03 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Yan", "Kun", ""], ["Bouraoui", "Zied", ""], ["Wang", "Ping", ""], ["Jameel", "Shoaib", ""], ["Schockaert", "Steven", ""]]}, {"id": "2102.00811", "submitter": "Shai Bagon", "authors": "Roee Zamir and Shai Bagon and David Samocha and Yael Yagil and Ronen\n  Basri and Miri Sklair-Levy Meirav Galun", "title": "Segmenting Microcalcifications in Mammograms and its Applications", "comments": "To appear in SPIE medical imaging 2021", "journal-ref": null, "doi": "10.1117/12.2580398", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Microcalcifications are small deposits of calcium that appear in mammograms\nas bright white specks on the soft tissue background of the breast.\nMicrocalcifications may be a unique indication for Ductal Carcinoma in Situ\nbreast cancer, and therefore their accurate detection is crucial for diagnosis\nand screening. Manual detection of these tiny calcium residues in mammograms is\nboth time-consuming and error-prone, even for expert radiologists, since these\nmicrocalcifications are small and can be easily missed. Existing computerized\nalgorithms for detecting and segmenting microcalcifications tend to suffer from\na high false-positive rate, hindering their widespread use. In this paper, we\npropose an accurate calcification segmentation method using deep learning. We\nspecifically address the challenge of keeping the false positive rate low by\nsuggesting a strategy for focusing the hard pixels in the training phase.\nFurthermore, our accurate segmentation enables extracting meaningful statistics\non clusters of microcalcifications.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 12:58:54 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Zamir", "Roee", ""], ["Bagon", "Shai", ""], ["Samocha", "David", ""], ["Yagil", "Yael", ""], ["Basri", "Ronen", ""], ["Galun", "Miri Sklair-Levy Meirav", ""]]}, {"id": "2102.00813", "submitter": "Inioluwa Deborah Raji", "authors": "Inioluwa Deborah Raji, Genevieve Fried", "title": "About Face: A Survey of Facial Recognition Evaluation", "comments": "Presented at AAAI 2020 Workshop on AI Evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We survey over 100 face datasets constructed between 1976 to 2019 of 145\nmillion images of over 17 million subjects from a range of sources,\ndemographics and conditions. Our historical survey reveals that these datasets\nare contextually informed, shaped by changes in political motivations,\ntechnological capability and current norms. We discuss how such influences mask\nspecific practices (some of which may actually be harmful or otherwise\nproblematic) and make a case for the explicit communication of such details in\norder to establish a more grounded understanding of the technology's function\nin the real world.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 13:05:34 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Raji", "Inioluwa Deborah", ""], ["Fried", "Genevieve", ""]]}, {"id": "2102.00818", "submitter": "Frank Hannig", "authors": "Frank Hannig, Paolo Meloni, Matteo Spallanzani, Matthias Ziegler", "title": "Proceedings of the DATE Friday Workshop on System-level Design Methods\n  for Deep Learning on Heterogeneous Architectures (SLOHA 2021)", "comments": "Website of the workshop: https://www12.cs.fau.de/ws/sloha2021/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This volume contains the papers accepted at the first DATE Friday Workshop on\nSystem-level Design Methods for Deep Learning on Heterogeneous Architectures\n(SLOHA 2021), held virtually on February 5, 2021. SLOHA 2021 was co-located\nwith the Conference on Design, Automation and Test in Europe (DATE).\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 18:14:02 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Hannig", "Frank", ""], ["Meloni", "Paolo", ""], ["Spallanzani", "Matteo", ""], ["Ziegler", "Matthias", ""]]}, {"id": "2102.00831", "submitter": "Hobin Ryu", "authors": "Hobin Ryu, Sunghun Kang, Haeyong Kang, and Chang D. Yoo", "title": "Semantic Grouping Network for Video Captioning", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper considers a video caption generating network referred to as\nSemantic Grouping Network (SGN) that attempts (1) to group video frames with\ndiscriminating word phrases of partially decoded caption and then (2) to decode\nthose semantically aligned groups in predicting the next word. As consecutive\nframes are not likely to provide unique information, prior methods have focused\non discarding or merging repetitive information based only on the input video.\nThe SGN learns an algorithm to capture the most discriminating word phrases of\nthe partially decoded caption and a mapping that associates each phrase to the\nrelevant video frames - establishing this mapping allows semantically related\nframes to be clustered, which reduces redundancy. In contrast to the prior\nmethods, the continuous feedback from decoded words enables the SGN to\ndynamically update the video representation that adapts to the partially\ndecoded caption. Furthermore, a contrastive attention loss is proposed to\nfacilitate accurate alignment between a word phrase and video frames without\nmanual annotations. The SGN achieves state-of-the-art performances by\noutperforming runner-up methods by a margin of 2.1%p and 2.4%p in a CIDEr-D\nscore on MSVD and MSR-VTT datasets, respectively. Extensive experiments\ndemonstrate the effectiveness and interpretability of the SGN.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 13:40:56 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 11:53:17 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Ryu", "Hobin", ""], ["Kang", "Sunghun", ""], ["Kang", "Haeyong", ""], ["Yoo", "Chang D.", ""]]}, {"id": "2102.00841", "submitter": "Alexander Sagel", "authors": "Alexander Sagel, Julian W\\\"ormann, Hao Shen", "title": "Dynamic Texture Recognition via Nuclear Distances on Kernelized\n  Scattering Histogram Spaces", "comments": "\\c{opyright} 2021 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": "ICASSP 2021 - 2021 IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP)", "doi": "10.1109/ICASSP39728.2021.9414783", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance-based dynamic texture recognition is an important research field in\nmultimedia processing with applications ranging from retrieval to segmentation\nof video data. Based on the conjecture that the most distinctive characteristic\nof a dynamic texture is the appearance of its individual frames, this work\nproposes to describe dynamic textures as kernelized spaces of frame-wise\nfeature vectors computed using the Scattering transform. By combining these\nspaces with a basis-invariant metric, we get a framework that produces\ncompetitive results for nearest neighbor classification and state-of-the-art\nresults for nearest class center classification.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 13:54:24 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Sagel", "Alexander", ""], ["W\u00f6rmann", "Julian", ""], ["Shen", "Hao", ""]]}, {"id": "2102.00848", "submitter": "Sanja \\v{S}\\'cepanovi\\'c", "authors": "Sanja \\v{S}\\'cepanovi\\'c, Sagar Joglekar, Stephen Law, Daniele Quercia", "title": "Jane Jacobs in the Sky: Predicting Urban Vitality with Open Satellite\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The presence of people in an urban area throughout the day -- often called\n'urban vitality' -- is one of the qualities world-class cities aspire to the\nmost, yet it is one of the hardest to achieve. Back in the 1970s, Jane Jacobs\ntheorized urban vitality and found that there are four conditions required for\nthe promotion of life in cities: diversity of land use, small block sizes, the\nmix of economic activities, and concentration of people. To build proxies for\nthose four conditions and ultimately test Jane Jacobs's theory at scale,\nresearchers have had to collect both private and public data from a variety of\nsources, and that took decades. Here we propose the use of one single source of\ndata, which happens to be publicly available: Sentinel-2 satellite imagery. In\nparticular, since the first two conditions (diversity of land use and small\nblock sizes) are visible to the naked eye from satellite imagery, we tested\nwhether we could automatically extract them with a state-of-the-art\ndeep-learning framework and whether, in the end, the extracted features could\npredict vitality. In six Italian cities for which we had call data records, we\nfound that our framework is able to explain on average 55% of the variance in\nurban vitality extracted from those records.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 21:46:29 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["\u0160\u0107epanovi\u0107", "Sanja", ""], ["Joglekar", "Sagar", ""], ["Law", "Stephen", ""], ["Quercia", "Daniele", ""]]}, {"id": "2102.00863", "submitter": "Cinjon Resnick", "authors": "Cinjon Resnick, Or Litany, Cosmas Hei{\\ss}, Hugo Larochelle, Joan\n  Bruna, Kyunghyun Cho", "title": "Self-Supervised Equivariant Scene Synthesis from Video", "comments": "arXiv admin note: text overlap with arXiv:2011.05787", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a self-supervised framework to learn scene representations from\nvideo that are automatically delineated into background, characters, and their\nanimations. Our method capitalizes on moving characters being equivariant with\nrespect to their transformation across frames and the background being constant\nwith respect to that same transformation. After training, we can manipulate\nimage encodings in real time to create unseen combinations of the delineated\ncomponents. As far as we know, we are the first method to perform unsupervised\nextraction and synthesis of interpretable background, character, and animation.\nWe demonstrate results on three datasets: Moving MNIST with backgrounds, 2D\nvideo game sprites, and Fashion Modeling.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 14:17:31 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Resnick", "Cinjon", ""], ["Litany", "Or", ""], ["Hei\u00df", "Cosmas", ""], ["Larochelle", "Hugo", ""], ["Bruna", "Joan", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "2102.00951", "submitter": "Oana-Iuliana Popescu", "authors": "Oana-Iuliana Popescu, Maha Shadaydeh, Joachim Denzler", "title": "Counterfactual Generation with Knockoffs", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human interpretability of deep neural networks' decisions is crucial,\nespecially in domains where these directly affect human lives. Counterfactual\nexplanations of already trained neural networks can be generated by perturbing\ninput features and attributing importance according to the change in the\nclassifier's outcome after perturbation. Perturbation can be done by replacing\nfeatures using heuristic or generative in-filling methods. The choice of\nin-filling function significantly impacts the number of artifacts, i.e.,\nfalse-positive attributions. Heuristic methods result in false-positive\nartifacts because the image after the perturbation is far from the original\ndata distribution. Generative in-filling methods reduce artifacts by producing\nin-filling values that respect the original data distribution. However, current\ngenerative in-filling methods may also increase false-negatives due to the high\ncorrelation of in-filling values with the original data. In this paper, we\npropose to alleviate this by generating in-fillings with the\nstatistically-grounded Knockoffs framework, which was developed by Barber and\nCand\\`es in 2015 as a tool for variable selection with controllable false\ndiscovery rate. Knockoffs are statistically null-variables as decorrelated as\npossible from the original data, which can be swapped with the originals\nwithout changing the underlying data distribution. A comparison of different\nin-filling methods indicates that in-filling with knockoffs can reveal\nexplanations in a more causal sense while still maintaining the compactness of\nthe explanations.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 16:34:38 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Popescu", "Oana-Iuliana", ""], ["Shadaydeh", "Maha", ""], ["Denzler", "Joachim", ""]]}, {"id": "2102.01021", "submitter": "Felix Gonda", "authors": "Felix Gonda, Donglai Wei, Hanspeter Pfister", "title": "Consistent Recurrent Neural Networks for 3D Neuron Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a recurrent network for the 3D reconstruction of neurons that\nsequentially generates binary masks for every object in an image with\nspatio-temporal consistency. Our network models consistency in two parts: (i)\nlocal, which allows exploring non-occluding and temporally-adjacent object\nrelationships with bi-directional recurrence. (ii) non-local, which allows\nexploring long-range object relationships in the temporal domain with skip\nconnections. Our proposed network is end-to-end trainable from an input image\nto a sequence of object masks, and, compared to methods relying on object\nboundaries, its output does not require post-processing. We evaluate our method\non three benchmarks for neuron segmentation and achieved state-of-the-art\nperformance on the SNEMI3D challenge.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 17:54:46 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Gonda", "Felix", ""], ["Wei", "Donglai", ""], ["Pfister", "Hanspeter", ""]]}, {"id": "2102.01026", "submitter": "Guillermo Carbajal", "authors": "Guillermo Carbajal, Patricia Vitoria, Mauricio Delbracio, Pablo\n  Mus\\'e, Jos\\'e Lezama", "title": "Non-uniform Blur Kernel Estimation via Adaptive Basis Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion blur estimation remains an important task for scene analysis and image\nrestoration. In recent years, the removal of motion blur in photographs has\nseen impressive progress in the hands of deep learning-based methods, trained\nto map directly from blurry to sharp images. Characterization of the motion\nblur, on the other hand, has received less attention, and progress in\nmodel-based methods for deblurring lags behind that of data-driven end-to-end\napproaches. In this work we revisit the problem of characterizing dense,\nnon-uniform motion blur in a single image and propose a general non-parametric\nmodel for this task. Given a blurry image, a neural network is trained to\nestimate a set of image-adaptive basis motion kernels as well as the mixing\ncoefficients at the pixel level, producing a per-pixel motion blur field. We\nshow that our approach overcomes the limitations of existing non-uniform motion\nblur estimation methods and leads to extremely accurate motion blur kernels.\nWhen applied to real motion-blurred images, a variational non-uniform blur\nremoval method fed with the estimated blur kernels produces high-quality\nrestored images. Qualitative and quantitative evaluation shows that these\nresults are competitive or superior to results obtained with existing\nend-to-end deep learning (DL) based methods, thus bridging the gap between\nmodel-based and data-driven approaches.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 18:02:31 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 19:29:05 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Carbajal", "Guillermo", ""], ["Vitoria", "Patricia", ""], ["Delbracio", "Mauricio", ""], ["Mus\u00e9", "Pablo", ""], ["Lezama", "Jos\u00e9", ""]]}, {"id": "2102.01033", "submitter": "Francois Drielsma", "authors": "Francois Drielsma, Kazuhiro Terao, Laura Domin\\'e, Dae Heun Koh", "title": "Scalable, End-to-End, Deep-Learning-Based Data Reconstruction Chain for\n  Particle Imaging Detectors", "comments": "Third Workshop on Machine Learning and the Physical Sciences (NeurIPS\n  2020), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ex cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent inroads in Computer Vision (CV) and Machine Learning (ML) have\nmotivated a new approach to the analysis of particle imaging detector data.\nUnlike previous efforts which tackled isolated CV tasks, this paper introduces\nan end-to-end, ML-based data reconstruction chain for Liquid Argon Time\nProjection Chambers (LArTPCs), the state-of-the-art in precision imaging at the\nintensity frontier of neutrino physics. The chain is a multi-task network\ncascade which combines voxel-level feature extraction using Sparse\nConvolutional Neural Networks and particle superstructure formation using Graph\nNeural Networks. Each algorithm incorporates physics-informed inductive biases,\nwhile their collective hierarchy is used to enforce a causal structure. The\noutput is a comprehensive description of an event that may be used for\nhigh-level physics inference. The chain is end-to-end optimizable, eliminating\nthe need for time-intensive manual software adjustments. It is also the first\nimplementation to handle the unprecedented pile-up of dozens of high energy\nneutrino interactions, expected in the 3D-imaging LArTPC of the Deep\nUnderground Neutrino Experiment. The chain is trained as a whole and its\nperformance is assessed at each step using an open simulated data set.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 18:10:00 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Drielsma", "Francois", ""], ["Terao", "Kazuhiro", ""], ["Domin\u00e9", "Laura", ""], ["Koh", "Dae Heun", ""]]}, {"id": "2102.01059", "submitter": "Hamideh Kerdegari Dr", "authors": "Hamideh Kerdegari, Phung Tran Huy Nhat, Angela McBride, VITAL\n  Consortium, Reza Razavi, Nguyen Van Hao, Louise Thwaites, Sophie Yacoub,\n  Alberto Gomez", "title": "Automatic Detection of B-lines in Lung Ultrasound Videos From Severe\n  Dengue Patients", "comments": "5 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lung ultrasound (LUS) imaging is used to assess lung abnormalities, including\nthe presence of B-line artefacts due to fluid leakage into the lungs caused by\na variety of diseases. However, manual detection of these artefacts is\nchallenging. In this paper, we propose a novel methodology to automatically\ndetect and localize B-lines in LUS videos using deep neural networks trained\nwith weak labels. To this end, we combine a convolutional neural network (CNN)\nwith a long short-term memory (LSTM) network and a temporal attention\nmechanism. Four different models are compared using data from 60 patients.\nResults show that our best model can determine whether one-second clips contain\nB-lines or not with an F1 score of 0.81, and extracts a representative frame\nwith B-lines with an accuracy of 87.5%.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 18:49:23 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Kerdegari", "Hamideh", ""], ["Nhat", "Phung Tran Huy", ""], ["McBride", "Angela", ""], ["Consortium", "VITAL", ""], ["Razavi", "Reza", ""], ["Van Hao", "Nguyen", ""], ["Thwaites", "Louise", ""], ["Yacoub", "Sophie", ""], ["Gomez", "Alberto", ""]]}, {"id": "2102.01063", "submitter": "Ming Lin", "authors": "Ming Lin, Pichao Wang, Zhenhong Sun, Hesen Chen, Xiuyu Sun, Qi Qian,\n  Hao Li, Rong Jin", "title": "Zen-NAS: A Zero-Shot NAS for High-Performance Deep Image Recognition", "comments": "fix some typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accuracy predictor is a key component in Neural Architecture Search (NAS) for\nranking architectures. Building a high-quality accuracy predictor usually costs\nenormous computation. To address this issue, instead of using an accuracy\npredictor, we propose a novel zero-shot index dubbed Zen-Score to rank the\narchitectures. The Zen-Score represents the network expressivity and positively\ncorrelates with the model accuracy. The calculation of Zen-Score only takes a\nfew forward inferences through a randomly initialized network, without training\nnetwork parameters. Built upon the Zen-Score, we further propose a new NAS\nalgorithm, termed as Zen-NAS, by maximizing the Zen-Score of the target network\nunder given inference budgets. Within less than half GPU day, Zen-NAS is able\nto directly search high performance architectures in a data-free style.\nComparing with previous NAS methods, the proposed Zen-NAS is magnitude times\nfaster on multiple server-side and mobile-side GPU platforms with\nstate-of-the-art accuracy on ImageNet. Our source code and pre-trained models\nare released on https://github.com/idstcv/ZenNAS.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 18:53:40 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 04:15:29 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 23:33:28 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Lin", "Ming", ""], ["Wang", "Pichao", ""], ["Sun", "Zhenhong", ""], ["Chen", "Hesen", ""], ["Sun", "Xiuyu", ""], ["Qian", "Qi", ""], ["Li", "Hao", ""], ["Jin", "Rong", ""]]}, {"id": "2102.01066", "submitter": "Achal Dave", "authors": "Achal Dave, Piotr Doll\\'ar, Deva Ramanan, Alexander Kirillov, Ross\n  Girshick", "title": "Evaluating Large-Vocabulary Object Detectors: The Devil is in the\n  Details", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By design, average precision (AP) for object detection aims to treat all\nclasses independently: AP is computed independently per category and averaged.\nOn the one hand, this is desirable as it treats all classes, rare to frequent,\nequally. On the other hand, it ignores cross-category confidence calibration, a\nkey property in real-world use cases. Unfortunately, we find that on\nimbalanced, large-vocabulary datasets, the default implementation of AP is\nneither category independent, nor does it directly reward properly calibrated\ndetectors. In fact, we show that the default implementation produces a gameable\nmetric, where a simple, nonsensical re-ranking policy can improve AP by a large\nmargin. To address these limitations, we introduce two complementary metrics.\nFirst, we present a simple fix to the default AP implementation, ensuring that\nit is truly independent across categories as originally intended. We benchmark\nrecent advances in large-vocabulary detection and find that many reported gains\ndo not translate to improvements under our new per-class independent\nevaluation, suggesting recent improvements may arise from difficult to\ninterpret changes to cross-category rankings. Given the importance of reliably\nbenchmarking cross-category rankings, we consider a pooled version of AP\n(AP-pool) that rewards properly calibrated detectors by directly comparing\ncross-category rankings. Finally, we revisit classical approaches for\ncalibration and find that explicitly calibrating detectors improves\nstate-of-the-art on AP-pool by 1.7 points.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 18:56:02 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Dave", "Achal", ""], ["Doll\u00e1r", "Piotr", ""], ["Ramanan", "Deva", ""], ["Kirillov", "Alexander", ""], ["Girshick", "Ross", ""]]}, {"id": "2102.01072", "submitter": "Scott Freitas", "authors": "Scott Freitas, Rahul Duggal, Duen Horng Chau", "title": "MalNet: A Large-Scale Cybersecurity Image Database of Malicious Software", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision is playing an increasingly important role in automated\nmalware detection with to the rise of the image-based binary representation.\nThese binary images are fast to generate, require no feature engineering, and\nare resilient to popular obfuscation methods. Significant research has been\nconducted in this area, however, it has been restricted to small-scale or\nprivate datasets that only a few industry labs and research teams have access\nto. This lack of availability hinders examination of existing work, development\nof new research, and dissemination of ideas. We introduce MalNet, the largest\npublicly available cybersecurity image database, offering 133x more images and\n27x more classes than the only other public binary-image database. MalNet\ncontains over 1.2 million images across a hierarchy of 47 types and 696\nfamilies. We provide extensive analysis of MalNet, discussing its properties\nand provenance. The scale and diversity of MalNet unlocks new and exciting\ncybersecurity opportunities to the computer vision community--enabling\ndiscoveries and research directions that were previously not possible. The\ndatabase is publicly available at www.mal-net.org.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 02:59:03 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Freitas", "Scott", ""], ["Duggal", "Rahul", ""], ["Chau", "Duen Horng", ""]]}, {"id": "2102.01120", "submitter": "Nibaran Das", "authors": "Hmrishav Bandyopadhyay, Tanmoy Dasgupta, Nibaran Das, Mita Nasipuri", "title": "RectiNet-v2: A stacked network architecture for document image dewarping", "comments": "6 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of mobile and hand-held cameras, document images have found\ntheir way into almost every domain. Dewarping of these images for the removal\nof perspective distortions and folds is essential so that they can be\nunderstood by document recognition algorithms. For this, we propose an\nend-to-end CNN architecture that can produce distortion free document images\nfrom warped documents it takes as input. We train this model on warped document\nimages simulated synthetically to compensate for lack of enough natural data.\nOur method is novel in the use of a bifurcated decoder with shared weights to\nprevent intermingling of grid coordinates, in the use of residual networks in\nthe U-Net skip connections to allow flow of data from different receptive\nfields in the model, and in the use of a gated network to help the model focus\non structure and line level detail of the document image. We evaluate our\nmethod on the DocUNet dataset, a benchmark in this domain, and obtain results\ncomparable to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 19:26:17 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Bandyopadhyay", "Hmrishav", ""], ["Dasgupta", "Tanmoy", ""], ["Das", "Nibaran", ""], ["Nasipuri", "Mita", ""]]}, {"id": "2102.01143", "submitter": "Mohammad Imrul Jubair", "authors": "K. M. Arefeen Sultan, Mohammad Imrul Jubair, MD. Nahidul Islam, Sayed\n  Hossain Khan", "title": "toon2real: Translating Cartoon Images to Realistic Images", "comments": "Accepted as a short paper at ICTAI 2020", "journal-ref": null, "doi": "10.1109/ICTAI50040.2020.00178", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In terms of Image-to-image translation, Generative Adversarial Networks\n(GANs) has achieved great success even when it is used in the unsupervised\ndataset. In this work, we aim to translate cartoon images to photo-realistic\nimages using GAN. We apply several state-of-the-art models to perform this\ntask; however, they fail to perform good quality translations. We observe that\nthe shallow difference between these two domains causes this issue. Based on\nthis idea, we propose a method based on CycleGAN model for image translation\nfrom cartoon domain to photo-realistic domain. To make our model efficient, we\nimplemented Spectral Normalization which added stability in our model. We\ndemonstrate our experimental results and show that our proposed model has\nachieved the lowest Frechet Inception Distance score and better results\ncompared to another state-of-the-art technique, UNIT.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 20:22:05 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Sultan", "K. M. Arefeen", ""], ["Jubair", "Mohammad Imrul", ""], ["Islam", "MD. Nahidul", ""], ["Khan", "Sayed Hossain", ""]]}, {"id": "2102.01161", "submitter": "Keyang Zhou", "authors": "Keyang Zhou, Bharat Lal Bhatnagar, Bernt Schiele, Gerard Pons-Moll", "title": "Adjoint Rigid Transform Network: Task-conditioned Alignment of 3D Shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most learning methods for 3D data (point clouds, meshes) suffer significant\nperformance drops when the data is not carefully aligned to a canonical\norientation. Aligning real world 3D data collected from different sources is\nnon-trivial and requires manual intervention. In this paper, we propose the\nAdjoint Rigid Transform (ART) Network, a neural module which can be integrated\nwith a variety of 3D networks to significantly boost their performance. ART\nlearns to rotate input shapes to a learned canonical orientation, which is\ncrucial for a lot of tasks such as shape reconstruction, interpolation,\nnon-rigid registration, and latent disentanglement. ART achieves this with\nself-supervision and a rotation equivariance constraint on predicted rotations.\nThe remarkable result is that with only self-supervision, ART facilitates\nlearning a unique canonical orientation for both rigid and nonrigid shapes,\nwhich leads to a notable boost in performance of aforementioned tasks. We will\nrelease our code and pre-trained models for further research.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 20:58:45 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 21:37:10 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhou", "Keyang", ""], ["Bhatnagar", "Bharat Lal", ""], ["Schiele", "Bernt", ""], ["Pons-Moll", "Gerard", ""]]}, {"id": "2102.01163", "submitter": "Kaiping Chen", "authors": "Kaiping Chen, Sang Jung Kim, Sebastian Raschka, Qiantong Gao", "title": "Visual Framing of Science Conspiracy Videos: Integrating Machine\n  Learning with Communication Theories to Study the Use of Color and Brightness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed an explosion of science conspiracy videos on the\nInternet, challenging science epistemology and public understanding of science.\nScholars have started to examine the persuasion techniques used in conspiracy\nmessages such as uncertainty and fear yet, little is understood about the\nvisual narratives, especially how visual narratives differ in videos that\ndebunk conspiracies versus those that propagate conspiracies. This paper\naddresses this gap in understanding visual framing in conspiracy videos through\nanalyzing millions of frames from conspiracy and counter-conspiracy YouTube\nvideos using computational methods. We found that conspiracy videos tended to\nuse lower color variance and brightness, especially in thumbnails and earlier\nparts of the videos. This paper also demonstrates how researchers can integrate\ntextual and visual features for identifying conspiracies on social media and\ndiscusses the implications of computational modeling for scholars interested in\nstudying visual manipulation in the digital era.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 21:03:50 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Chen", "Kaiping", ""], ["Kim", "Sang Jung", ""], ["Raschka", "Sebastian", ""], ["Gao", "Qiantong", ""]]}, {"id": "2102.01172", "submitter": "Aniket Pramanik", "authors": "Aniket Pramanik, Mathews Jacob", "title": "Reconstruction and Segmentation of Parallel MR Data using Image Domain\n  DEEP-SLR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The main focus of this work is a novel framework for the joint reconstruction\nand segmentation of parallel MRI (PMRI) brain data. We introduce an image\ndomain deep network for calibrationless recovery of undersampled PMRI data. The\nproposed approach is the deep-learning (DL) based generalization of local\nlow-rank based approaches for uncalibrated PMRI recovery including CLEAR [6].\nSince the image domain approach exploits additional annihilation relations\ncompared to k-space based approaches, we expect it to offer improved\nperformance. To minimize segmentation errors resulting from undersampling\nartifacts, we combined the proposed scheme with a segmentation network and\ntrained it in an end-to-end fashion. In addition to reducing segmentation\nerrors, this approach also offers improved reconstruction performance by\nreducing overfitting; the reconstructed images exhibit reduced blurring and\nsharper edges than independently trained reconstruction network.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 21:15:59 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Pramanik", "Aniket", ""], ["Jacob", "Mathews", ""]]}, {"id": "2102.01187", "submitter": "Peiye Zhuang", "authors": "Peiye Zhuang, Oluwasanmi Koyejo, Alexander G. Schwing", "title": "Enjoy Your Editing: Controllable GANs for Image Editing via Latent Space\n  Navigation", "comments": "Accepted to ICLR 2021. 14 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controllable semantic image editing enables a user to change entire image\nattributes with a few clicks, e.g., gradually making a summer scene look like\nit was taken in winter. Classic approaches for this task use a Generative\nAdversarial Net (GAN) to learn a latent space and suitable latent-space\ntransformations. However, current approaches often suffer from attribute edits\nthat are entangled, global image identity changes, and diminished\nphoto-realism. To address these concerns, we learn multiple attribute\ntransformations simultaneously, integrate attribute regression into the\ntraining of transformation functions, and apply a content loss and an\nadversarial loss that encourages the maintenance of image identity and\nphoto-realism. We propose quantitative evaluation strategies for measuring\ncontrollable editing performance, unlike prior work, which primarily focuses on\nqualitative evaluation. Our model permits better control for both single- and\nmultiple-attribute editing while preserving image identity and realism during\ntransformation. We provide empirical results for both natural and synthetic\nimages, highlighting that our model achieves state-of-the-art performance for\ntargeted image manipulation.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 21:38:36 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 07:21:18 GMT"}, {"version": "v3", "created": "Sun, 28 Mar 2021 20:04:36 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhuang", "Peiye", ""], ["Koyejo", "Oluwasanmi", ""], ["Schwing", "Alexander G.", ""]]}, {"id": "2102.01191", "submitter": "Mariia Gladkova", "authors": "Mariia Gladkova, Rui Wang, Niclas Zeller, and Daniel Cremers", "title": "Tight Integration of Feature-based Relocalization in Monocular Direct\n  Visual Odometry", "comments": "ICRA 2021 camera-ready submission; 7 pages, 5 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a framework for integrating map-based relocalization\ninto online direct visual odometry. To achieve map-based relocalization for\ndirect methods, we integrate image features into Direct Sparse Odometry (DSO)\nand rely on feature matching to associate online visual odometry (VO) with a\npreviously built map. The integration of the relocalization poses is threefold.\nFirstly, they are incorporated as pose priors in the direct image alignment of\nthe front-end tracking. Secondly, they are tightly integrated into the back-end\nbundle adjustment. Thirdly, an online fusion module is further proposed to\ncombine relative VO poses and global relocalization poses in a pose graph to\nestimate keyframe-wise smooth and globally accurate poses. We evaluate our\nmethod on two multi-weather datasets showing the benefits of integrating\ndifferent handcrafted and learned features and demonstrating promising\nimprovements on camera tracking accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 21:41:05 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 13:15:27 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 08:07:48 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Gladkova", "Mariia", ""], ["Wang", "Rui", ""], ["Zeller", "Niclas", ""], ["Cremers", "Daniel", ""]]}, {"id": "2102.01256", "submitter": "Yuan Liang", "authors": "Yuan Liang, Weinan Song, Jiawei Yang, Liang Qiu, Kun Wang, Lei He", "title": "Atlas-aware ConvNetfor Accurate yet Robust Anatomical Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional networks (ConvNets) have achieved promising accuracy for\nvarious anatomical segmentation tasks. Despite the success, these methods can\nbe sensitive to data appearance variations. Considering the large variability\nof scans caused by artifacts, pathologies, and scanning setups, robust ConvNets\nare vital for clinical applications, while have not been fully explored. In\nthis paper, we propose to mitigate the challenge by enabling ConvNets'\nawareness of the underlying anatomical invariances among imaging scans.\nSpecifically, we introduce a fully convolutional Constraint Adoption Module\n(CAM) that incorporates probabilistic atlas priors as explicit constraints for\npredictions over a locally connected Conditional Random Field (CFR), which\neffectively reinforces the anatomical consistency of the labeling outputs. We\ndesign the CAM to be flexible for boosting various ConvNet, and compact for\nco-optimizing with ConvNets for fusion parameters that leads to the optimal\nperformance. We show the advantage of such atlas priors fusion is two-fold with\ntwo brain parcellation tasks. First, our models achieve state-of-the-art\naccuracy among ConvNet-based methods on both datasets, by significantly\nreducing structural abnormalities of predictions. Second, we can largely boost\nthe robustness of existing ConvNets, proved by: (i) testing on scans with\nsynthetic pathologies, and (ii) training and evaluation on scans of different\nscanning setups across datasets. Our method is proposing to be easily adopted\nto existing ConvNets by fine-tuning with CAM plugged in for accuracy and\nrobustness boosts.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 02:06:06 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Liang", "Yuan", ""], ["Song", "Weinan", ""], ["Yang", "Jiawei", ""], ["Qiu", "Liang", ""], ["Wang", "Kun", ""], ["He", "Lei", ""]]}, {"id": "2102.01279", "submitter": "Zhenmei Shi", "authors": "Zhenmei Shi, Fuhao Shi, Wei-Sheng Lai, Chia-Kai Liang, Yingyu Liang", "title": "Deep Online Fused Video Stabilization", "comments": "9 pages. Project page: https://zhmeishi.github.io/dvs/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep neural network (DNN) that uses both sensor data (gyroscope)\nand image content (optical flow) to stabilize videos through unsupervised\nlearning. The network fuses optical flow with real/virtual camera pose\nhistories into a joint motion representation. Next, the LSTM block infers the\nnew virtual camera pose, and this virtual pose is used to generate a warping\ngrid that stabilizes the frame. Novel relative motion representation as well as\na multi-stage training process are presented to optimize our model without any\nsupervision. To the best of our knowledge, this is the first DNN solution that\nadopts both sensor data and image for stabilization. We validate the proposed\nframework through ablation studies and demonstrated the proposed method\noutperforms the state-of-art alternative solutions via quantitative evaluations\nand a user study.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 03:35:07 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 01:59:44 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Shi", "Zhenmei", ""], ["Shi", "Fuhao", ""], ["Lai", "Wei-Sheng", ""], ["Liang", "Chia-Kai", ""], ["Liang", "Yingyu", ""]]}, {"id": "2102.01282", "submitter": "Qi Zheng", "authors": "Qi Zheng, Jianfeng Dong, Xiaoye Qu, Xun Yang, Shouling Ji, Xun Wang", "title": "Progressive Localization Networks for Language-based Moment Localization", "comments": "12pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper targets the task of language-based moment localization. The\nlanguage-based setting of this task allows for an open set of target\nactivities, resulting in a large variation of the temporal lengths of video\nmoments. Most existing methods prefer to first sample sufficient candidate\nmoments with various temporal lengths, and then match them with the given query\nto determine the target moment. However, candidate moments generated with a\nfixed temporal granularity may be suboptimal to handle the large variation in\nmoment lengths. To this end, we propose a novel multi-stage Progressive\nLocalization Network (PLN) which progressively localizes the target moment in a\ncoarse-to-fine manner. Specifically, each stage of PLN has a localization\nbranch, and focuses on candidate moments that are generated with a specific\ntemporal granularity. The temporal granularities of candidate moments are\ndifferent across the stages. Moreover, we devise a conditional feature\nmanipulation module and an upsampling connection to bridge the multiple\nlocalization branches. In this fashion, the later stages are able to absorb the\npreviously learned information, thus facilitating the more fine-grained\nlocalization. Extensive experiments on three public datasets demonstrate the\neffectiveness of our proposed PLN for language-based moment localization and\nits potential for localizing short moments in long videos.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 03:45:59 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Zheng", "Qi", ""], ["Dong", "Jianfeng", ""], ["Qu", "Xiaoye", ""], ["Yang", "Xun", ""], ["Ji", "Shouling", ""], ["Wang", "Xun", ""]]}, {"id": "2102.01284", "submitter": "Peng Yao", "authors": "Peng Yao, Shuwei Shen, Mengjuan Xu, Peng Liu, Fan Zhang, Jinyu Xing,\n  Pengfei Shao, Benjamin Kaffenberger, and Ronald X. Xu", "title": "Single Model Deep Learning on Imbalanced Small Datasets for Skin Lesion\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural network (DCNN) models have been widely explored for\nskin disease diagnosis and some of them have achieved the diagnostic outcomes\ncomparable or even superior to those of dermatologists. However, broad\nimplementation of DCNN in skin disease detection is hindered by small size and\ndata imbalance of the publically accessible skin lesion datasets. This paper\nproposes a novel data augmentation strategy for single model classification of\nskin lesions based on a small and imbalanced dataset. First, various DCNNs are\ntrained on this dataset to show that the models with moderate complexity\noutperform the larger models. Second, regularization DropOut and DropBlock are\nadded to reduce overfitting and a Modified RandAugment augmentation strategy is\nproposed to address the defects of sample underrepresentation in the small\ndataset. Finally, a novel Multi-Weighted Focal Loss function is introduced to\novercome the challenge of uneven sample size and classification difficulty. By\ncombining Modified RandAugment and Multi-weighted Focal Loss in a single DCNN\nmodel, we have achieved the classification accuracy comparable to those of\nmultiple ensembling models on the ISIC 2018 challenge test dataset. Our study\nshows that this method is able to achieve a high classification performance at\na low cost of computational resources and inference time, potentially suitable\nto implement in mobile devices for automated screening of skin lesions and many\nother malignancies in low resource settings.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 03:48:55 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Yao", "Peng", ""], ["Shen", "Shuwei", ""], ["Xu", "Mengjuan", ""], ["Liu", "Peng", ""], ["Zhang", "Fan", ""], ["Xing", "Jinyu", ""], ["Shao", "Pengfei", ""], ["Kaffenberger", "Benjamin", ""], ["Xu", "Ronald X.", ""]]}, {"id": "2102.01285", "submitter": "Jenhao Hsiao", "authors": "Jenhao Hsiao and Jiawei Chen and Chiuman Ho", "title": "GCF-Net: Gated Clip Fusion Network for Video Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, most of the accuracy gains for video action recognition have\ncome from the newly designed CNN architectures (e.g., 3D-CNNs). These models\nare trained by applying a deep CNN on single clip of fixed temporal length.\nSince each video segment are processed by the 3D-CNN module separately, the\ncorresponding clip descriptor is local and the inter-clip relationships are\ninherently implicit. Common method that directly averages the clip-level\noutputs as a video-level prediction is prone to fail due to the lack of\nmechanism that can extract and integrate relevant information to represent the\nvideo.\n  In this paper, we introduce the Gated Clip Fusion Network (GCF-Net) that can\ngreatly boost the existing video action classifiers with the cost of a tiny\ncomputation overhead. The GCF-Net explicitly models the inter-dependencies\nbetween video clips to strengthen the receptive field of local clip\ndescriptors. Furthermore, the importance of each clip to an action event is\ncalculated and a relevant subset of clips is selected accordingly for a\nvideo-level analysis. On a large benchmark dataset (Kinetics-600), the proposed\nGCF-Net elevates the accuracy of existing action classifiers by 11.49% (based\non central clip) and 3.67% (based on densely sampled clips) respectively.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 03:51:55 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Hsiao", "Jenhao", ""], ["Chen", "Jiawei", ""], ["Ho", "Chiuman", ""]]}, {"id": "2102.01289", "submitter": "Ziyi Liu", "authors": "Jie Yang, Mengchen Lin, Ziyi Liu, Ulian Shahnovich, Orly Yadid-Pecht", "title": "Mobile-end Tone Mapping based on Integral Image and Integral Histogram", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Wide dynamic range (WDR) image tone mapping is in high demand in many\napplications like film production, security monitoring, and photography. It is\nespecially crucial for mobile devices because most of the images taken today\nare from mobile phones, hence such technology is highly demanded in the\nconsumer market of mobile devices and is essential for a good customer\nexperience. However, high-quality and high-performance WDR image tone mapping\nimplementations are rarely found in the mobile-end. In this paper, we introduce\na high performance, mobile-end WDR image tone mapping implementation. It\nleverages the tone mapping results of multiple receptive fields and calculates\na suitable value for each pixel. The utilization of integral image and integral\nhistogram significantly reduce the required computation. Moreover, GPU parallel\ncomputation is used to increase the processing speed. The experimental results\nindicate that our implementation can process a high-resolution WDR image within\na second on mobile devices and produce appealing image quality.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 04:01:46 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Yang", "Jie", ""], ["Lin", "Mengchen", ""], ["Liu", "Ziyi", ""], ["Shahnovich", "Ulian", ""], ["Yadid-Pecht", "Orly", ""]]}, {"id": "2102.01301", "submitter": "Yi-Jun Cao", "authors": "Yi-Jun Cao, Chuan Lin, and Yong-Jie Li", "title": "Learning Crisp Boundaries Using Deep Refinement Network and Adaptive\n  Weighting Loss", "comments": "11 pages, 7 figures", "journal-ref": "IEEE Transactions on Multimedia, vol. 23, pp. 761-771, 2021", "doi": "10.1109/TED.2020.3041567", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant progress has been made in boundary detection with the help of\nconvolutional neural networks. Recent boundary detection models not only focus\non real object boundary detection but also \"crisp\" boundaries (precisely\nlocalized along the object's contour). There are two methods to evaluate crisp\nboundary performance. One uses more strict tolerance to measure the distance\nbetween the ground truth and the detected contour. The other focuses on\nevaluating the contour map without any postprocessing. In this study, we\nanalyze both methods and conclude that both methods are two aspects of crisp\ncontour evaluation. Accordingly, we propose a novel network named deep\nrefinement network (DRNet) that stacks multiple refinement modules to achieve\nricher feature representation and a novel loss function, which combines\ncross-entropy and dice loss through effective adaptive fusion. Experimental\nresults demonstrated that we achieve state-of-the-art performance for several\navailable datasets.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 04:22:35 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 07:15:10 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Cao", "Yi-Jun", ""], ["Lin", "Chuan", ""], ["Li", "Yong-Jie", ""]]}, {"id": "2102.01313", "submitter": "Miki Tanaka", "authors": "Miki Tanaka, Hitoshi Kiya", "title": "Fake-image detection with Robust Hashing", "comments": "to be appear in Life Tech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate whether robust hashing has a possibility to\nrobustly detect fake-images even when multiple manipulation techniques such as\nJPEG compression are applied to images for the first time. In an experiment,\nthe proposed fake detection with robust hashing is demonstrated to outperform\nstate-of-the-art one under the use of various datasets including fake images\ngenerated with GANs.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 05:10:37 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Tanaka", "Miki", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "2102.01315", "submitter": "Minyoung Chung", "authors": "Jusang Lee, Minyoung Chung, Minkyung Lee, Yeong-Gil Shin", "title": "Tooth Instance Segmentation from Cone-Beam CT Images through Point-based\n  Detection and Gaussian Disentanglement", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Individual tooth segmentation and identification from cone-beam computed\ntomography images are preoperative prerequisites for orthodontic treatments.\nInstance segmentation methods using convolutional neural networks have\ndemonstrated ground-breaking results on individual tooth segmentation tasks,\nand are used in various medical imaging applications. While point-based\ndetection networks achieve superior results on dental images, it is still a\nchallenging task to distinguish adjacent teeth because of their similar\ntopologies and proximate nature. In this study, we propose a point-based tooth\nlocalization network that effectively disentangles each individual tooth based\non a Gaussian disentanglement objective function. The proposed network first\nperforms heatmap regression accompanied by box regression for all the\nanatomical teeth. A novel Gaussian disentanglement penalty is employed by\nminimizing the sum of the pixel-wise multiplication of the heatmaps for all\nadjacent teeth pairs. Subsequently, individual tooth segmentation is performed\nby converting a pixel-wise labeling task to a distance map regression task to\nminimize false positives in adjacent regions of the teeth. Experimental results\ndemonstrate that the proposed algorithm outperforms state-of-the-art approaches\nby increasing the average precision of detection by 9.1%, which results in a\nhigh performance in terms of individual tooth segmentation. The primary\nsignificance of the proposed method is two-fold: 1) the introduction of a\npoint-based tooth detection framework that does not require additional\nclassification and 2) the design of a novel loss function that effectively\nseparates Gaussian distributions based on heatmap responses in the point-based\ndetection framework.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 05:15:50 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Lee", "Jusang", ""], ["Chung", "Minyoung", ""], ["Lee", "Minkyung", ""], ["Shin", "Yeong-Gil", ""]]}, {"id": "2102.01340", "submitter": "Francesco Paissan", "authors": "Francesco Paissan, Massimo Gottardi, Elisabetta Farella", "title": "Enabling energy efficient machine learning on a Ultra-Low-Power vision\n  sensor for IoT", "comments": "Presented at DATE Friday Workshop on System-level Design Methods for\n  Deep Learning on Heterogeneous Architectures (SLOHA 2021) (arXiv:2102.00818)", "journal-ref": null, "doi": null, "report-no": "SLOHA/2021/01", "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things (IoT) and smart city paradigm includes ubiquitous\ntechnology to extract context information in order to return useful services to\nusers and citizens. An essential role in this scenario is often played by\ncomputer vision applications, requiring the acquisition of images from specific\ndevices. The need for high-end cameras often penalizes this process since they\nare power-hungry and ask for high computational resources to be processed.\nThus, the availability of novel low-power vision sensors, implementing advanced\nfeatures like in-hardware motion detection, is crucial for computer vision in\nthe IoT domain. Unfortunately, to be highly energy-efficient, these sensors\nmight worsen the perception performance (e.g., resolution, frame rate, color).\nTherefore, domain-specific pipelines are usually delivered in order to exploit\nthe full potential of these cameras. This paper presents the development,\nanalysis, and embedded implementation of a realtime detection, classification\nand tracking pipeline able to exploit the full potential of background\nfiltering Smart Vision Sensors (SVS). The power consumption obtained for the\ninference - which requires 8ms - is 7.5 mW.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 06:39:36 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Paissan", "Francesco", ""], ["Gottardi", "Massimo", ""], ["Farella", "Elisabetta", ""]]}, {"id": "2102.01345", "submitter": "Etienne Dupuis", "authors": "Etienne Dupuis, David Novo, Ian O'Connor, Alberto Bosio", "title": "Fast Exploration of Weight Sharing Opportunities for CNN Compression", "comments": "Presented at DATE Friday Workshop on System-level Design Methods for\n  Deep Learning on Heterogeneous Architectures (SLOHA 2021) (arXiv:2102.00818)", "journal-ref": null, "doi": null, "report-no": "SLOHA/2021/05", "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational workload involved in Convolutional Neural Networks (CNNs)\nis typically out of reach for low-power embedded devices. There are a large\nnumber of approximation techniques to address this problem. These methods have\nhyper-parameters that need to be optimized for each CNNs using design space\nexploration (DSE). The goal of this work is to demonstrate that the DSE phase\ntime can easily explode for state of the art CNN. We thus propose the use of an\noptimized exploration process to drastically reduce the exploration time\nwithout sacrificing the quality of the output.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 06:45:56 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Dupuis", "Etienne", ""], ["Novo", "David", ""], ["O'Connor", "Ian", ""], ["Bosio", "Alberto", ""]]}, {"id": "2102.01351", "submitter": "Olivia Weng", "authors": "Olivia Weng, Alireza Khodamoradi, Ryan Kastner", "title": "Hardware-efficient Residual Networks for FPGAs", "comments": "Presented at DATE Friday Workshop on System-level Design Methods for\n  Deep Learning on Heterogeneous Architectures (SLOHA 2021) (arXiv:2102.00818)", "journal-ref": null, "doi": null, "report-no": "SLOHA/2021/10", "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual networks (ResNets) employ skip connections in their networks --\nreusing activations from previous layers -- to improve training convergence,\nbut these skip connections create challenges for hardware implementations of\nResNets. The hardware must either wait for skip connections to be processed\nbefore processing more incoming data or buffer them elsewhere. Without skip\nconnections, ResNets would be more hardware-efficient. Thus, we present the\nteacher-student learning method to gradually prune away all of a ResNet's skip\nconnections, constructing a network we call NonResNet. We show that when\nimplemented for FPGAs, NonResNet decreases ResNet's BRAM utilization by 9% and\nLUT utilization by 3% and increases throughput by 5%.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 06:50:22 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Weng", "Olivia", ""], ["Khodamoradi", "Alireza", ""], ["Kastner", "Ryan", ""]]}, {"id": "2102.01355", "submitter": "Andrew Lensen", "authors": "Andrew Lensen", "title": "Mining Feature Relationships in Data", "comments": "16 pages, accepted in EuroGP '21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  When faced with a new dataset, most practitioners begin by performing\nexploratory data analysis to discover interesting patterns and characteristics\nwithin data. Techniques such as association rule mining are commonly applied to\nuncover relationships between features (attributes) of the data. However,\nassociation rules are primarily designed for use on binary or categorical data,\ndue to their use of rule-based machine learning. A large proportion of\nreal-world data is continuous in nature, and discretisation of such data leads\nto inaccurate and less informative association rules. In this paper, we propose\nan alternative approach called feature relationship mining (FRM), which uses a\ngenetic programming approach to automatically discover symbolic relationships\nbetween continuous or categorical features in data. To the best of our\nknowledge, our proposed approach is the first such symbolic approach with the\ngoal of explicitly discovering relationships between features. Empirical\ntesting on a variety of real-world datasets shows the proposed method is able\nto find high-quality, simple feature relationships which can be easily\ninterpreted and which provide clear and non-trivial insight into data.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 07:06:16 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Lensen", "Andrew", ""]]}, {"id": "2102.01360", "submitter": "Chajin Shin", "authors": "Chajin Shin, Taeoh Kim, Sangjin Lee and Sangyoun Lee", "title": "Test-Time Adaptation for Out-of-distributed Image Inpainting", "comments": "ICIP 2021 Accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based image inpainting algorithms have shown great performance\nvia powerful learned prior from the numerous external natural images. However,\nthey show unpleasant results on the test image whose distribution is far from\nthe that of training images because their models are biased toward the training\nimages. In this paper, we propose a simple image inpainting algorithm with\ntest-time adaptation named AdaFill. Given a single out-of-distributed test\nimage, our goal is to complete hole region more naturally than the pre-trained\ninpainting models. To achieve this goal, we treat remained valid regions of the\ntest image as another training cues because natural images have strong internal\nsimilarities. From this test-time adaptation, our network can exploit\nexternally learned image priors from the pre-trained features as well as the\ninternal prior of the test image explicitly. Experimental results show that\nAdaFill outperforms other models on the various out-of-distribution test\nimages. Furthermore, the model named ZeroFill, that are not pre-trained also\nsometimes outperforms the pre-trained models.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 07:26:03 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 05:14:11 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Shin", "Chajin", ""], ["Kim", "Taeoh", ""], ["Lee", "Sangjin", ""], ["Lee", "Sangyoun", ""]]}, {"id": "2102.01381", "submitter": "Dong-Keon Kim", "authors": "Dong-Keon Kim, DongHee Kim, and Kwangsu Kim", "title": "Facial Manipulation Detection Based on the Color Distribution Analysis\n  in Edge Region", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a generalized and robust facial manipulation\ndetection method based on color distribution analysis of the vertical region of\nedge in a manipulated image. Most of the contemporary facial manipulation\nmethod involves pixel correction procedures for reducing awkwardness of pixel\nvalue differences along the facial boundary in a synthesized image. For this\nprocedure, there are distinctive differences in the facial boundary between\nface manipulated image and unforged natural image. Also, in the forged image,\nthere should be distinctive and unnatural features in the gap distribution\nbetween facial boundary and background edge region because it tends to damage\nthe natural effect of lighting. We design the neural network for detecting\nface-manipulated image with these distinctive features in facial boundary and\nbackground edge. Our extensive experiments show that our method outperforms\nother existing face manipulation detection methods on detecting synthesized\nface image in various datasets regardless of whether it has participated in\ntraining.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 08:19:35 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Kim", "Dong-Keon", ""], ["Kim", "DongHee", ""], ["Kim", "Kwangsu", ""]]}, {"id": "2102.01389", "submitter": "Ethan Cohen", "authors": "Ethan Cohen and Virginie Uhlmann", "title": "aura-net : robust segmentation of phase-contrast microscopy images with\n  few annotations", "comments": "Accepted at ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present AURA-net, a convolutional neural network (CNN) for the\nsegmentation of phase-contrast microscopy images. AURA-net uses transfer\nlearning to accelerate training and Attention mechanisms to help the network\nfocus on relevant image features. In this way, it can be trained efficiently\nwith a very limited amount of annotations. Our network can thus be used to\nautomate the segmentation of datasets that are generally considered too small\nfor deep learning techniques. AURA-net also uses a loss inspired by active\ncontours that is well-adapted to the specificity of phase-contrast images,\nfurther improving performance. We show that AURA-net outperforms\nstate-of-the-art alternatives in several small (less than 100images) datasets.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 08:47:14 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Cohen", "Ethan", ""], ["Uhlmann", "Virginie", ""]]}, {"id": "2102.01393", "submitter": "Stylianos Venieris", "authors": "Ilias Leontiadis, Stefanos Laskaridis, Stylianos I. Venieris, Nicholas\n  D. Lane", "title": "It's always personal: Using Early Exits for Efficient On-Device CNN\n  Personalisation", "comments": "Accepted at the 22nd International Workshop on Mobile Computing\n  Systems and Applications (HotMobile), 2021", "journal-ref": null, "doi": "10.1145/3446382.3448359", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-device machine learning is becoming a reality thanks to the availability\nof powerful hardware and model compression techniques. Typically, these models\nare pretrained on large GPU clusters and have enough parameters to generalise\nacross a wide variety of inputs. In this work, we observe that a much smaller,\npersonalised model can be employed to fit a specific scenario, resulting in\nboth higher accuracy and faster execution. Nevertheless, on-device training is\nextremely challenging, imposing excessive computational and memory requirements\neven for flagship smartphones. At the same time, on-device data availability\nmight be limited and samples are most frequently unlabelled. To this end, we\nintroduce PersEPhonEE, a framework that attaches early exits on the model and\npersonalises them on-device. These allow the model to progressively bypass a\nlarger part of the computation as more personalised data become available.\nMoreover, we introduce an efficient on-device algorithm that trains the early\nexits in a semi-supervised manner at a fraction of the whole network's\npersonalisation time. Results show that PersEPhonEE boosts accuracy by up to\n15.9% while dropping the training cost by up to 2.2x and inference latency by\n2.2-3.2x on average for the same accuracy, depending on the availability of\nlabels on-device.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 09:10:17 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Leontiadis", "Ilias", ""], ["Laskaridis", "Stefanos", ""], ["Venieris", "Stylianos I.", ""], ["Lane", "Nicholas D.", ""]]}, {"id": "2102.01404", "submitter": "Nayaneesh Kumar Mishra", "authors": "Nayaneesh Kumar Mishra, Satish Kumar Singh", "title": "Face Recognition Using $Sf_{3}CNN$ With Higher Feature Discrimination", "comments": "6 pages, 3 figures, 1 table, 5th IAPR International Conference on\n  Computer Vision & Image Processing 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of 2-dimensional Convolution Neural Networks (2D CNNs), the\nface recognition accuracy has reached above 99%. However, face recognition is\nstill a challenge in real world conditions. A video, instead of an image, as an\ninput can be more useful to solve the challenges of face recognition in real\nworld conditions. This is because a video provides more features than an image.\nHowever, 2D CNNs cannot take advantage of the temporal features present in the\nvideo. We therefore, propose a framework called $Sf_{3}CNN$ for face\nrecognition in videos. The $Sf_{3}CNN$ framework uses 3-dimensional Residual\nNetwork (3D Resnet) and A-Softmax loss for face recognition in videos. The use\nof 3D ResNet helps to capture both spatial and temporal features into one\ncompact feature map. However, the 3D CNN features must be highly discriminative\nfor efficient face recognition. The use of A-Softmax loss helps to extract\nhighly discriminative features from the video for face recognition. $Sf_{3}CNN$\nframework gives an increased accuracy of 99.10% on CVBL video database in\ncomparison to the previous 97% on the same database using 3D ResNets.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 09:47:31 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Mishra", "Nayaneesh Kumar", ""], ["Singh", "Satish Kumar", ""]]}, {"id": "2102.01405", "submitter": "Ruben Tolosana", "authors": "Ruben Tolosana, Juan Carlos Ruiz-Garcia, Ruben Vera-Rodriguez, Jaime\n  Herreros-Rodriguez, Sergio Romero-Tapiador, Aythami Morales, Julian Fierrez", "title": "Child-Computer Interaction: Recent Works, New Dataset, and Age Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We overview recent research in Child-Computer Interaction and describe our\nframework ChildCI intended for: i) generating a better understanding of the\ncognitive and neuromotor development of children while interacting with mobile\ndevices, and ii) enabling new applications in e-learning and e-health, among\nothers. Our framework includes a new mobile application, specific data\nacquisition protocols, and a first release of the ChildCI dataset (ChildCIdb\nv1), which is planned to be extended yearly to enable longitudinal studies. In\nour framework children interact with a tablet device, using both a pen stylus\nand the finger, performing different tasks that require different levels of\nneuromotor and cognitive skills. ChildCIdb comprises more than 400 children\nfrom 18 months to 8 years old, considering therefore the first three\ndevelopment stages of the Piaget's theory. In addition, and as a demonstration\nof the potential of the ChildCI framework, we include experimental results for\none of the many applications enabled by ChildCIdb: children age detection based\non device interaction. Different machine learning approaches are evaluated,\nproposing a new set of 34 global features to automatically detect age groups,\nachieving accuracy results over 90% and interesting findings in terms of the\ntype of features more useful for this task.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 09:51:58 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Tolosana", "Ruben", ""], ["Ruiz-Garcia", "Juan Carlos", ""], ["Vera-Rodriguez", "Ruben", ""], ["Herreros-Rodriguez", "Jaime", ""], ["Romero-Tapiador", "Sergio", ""], ["Morales", "Aythami", ""], ["Fierrez", "Julian", ""]]}, {"id": "2102.01439", "submitter": "Benedetta Tondi", "authors": "Yakun Niu, Benedetta Tondi, Yao Zhao, Rongrong Ni and Mauro Barni", "title": "Image Splicing Detection, Localization and Attribution via JPEG Primary\n  Quantization Matrix Estimation and Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detection of inconsistencies of double JPEG artefacts across different image\nregions is often used to detect local image manipulations, like image splicing,\nand to localize them. In this paper, we move one step further, proposing an\nend-to-end system that, in addition to detecting and localizing spliced\nregions, can also distinguish regions coming from different donor images. We\nassume that both the spliced regions and the background image have undergone a\ndouble JPEG compression, and use a local estimate of the primary quantization\nmatrix to distinguish between spliced regions taken from different sources. To\ndo so, we cluster the image blocks according to the estimated primary\nquantization matrix and refine the result by means of morphological\nreconstruction. The proposed method can work in a wide variety of settings\nincluding aligned and non-aligned double JPEG compression, and regardless of\nwhether the second compression is stronger or weaker than the first one. We\nvalidated the proposed approach by means of extensive experiments showing its\nsuperior performance with respect to baseline methods working in similar\nconditions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 11:21:49 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Niu", "Yakun", ""], ["Tondi", "Benedetta", ""], ["Zhao", "Yao", ""], ["Ni", "Rongrong", ""], ["Barni", "Mauro", ""]]}, {"id": "2102.01441", "submitter": "Nayaneesh Kumar Mishra", "authors": "Nayaneesh Kumar Mishra, Satish Kumar Singh", "title": "Face Recognition using 3D CNNs", "comments": "15 pages, 4 figures, Data Science Book", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The area of face recognition is one of the most widely researched areas in\nthe domain of computer vision and biometric. This is because, the non-intrusive\nnature of face biometric makes it comparatively more suitable for application\nin area of surveillance at public places such as airports. The application of\nprimitive methods in face recognition could not give very satisfactory\nperformance. However, with the advent of machine and deep learning methods and\ntheir application in face recognition, several major breakthroughs were\nobtained. The use of 2D Convolution Neural networks(2D CNN) in face recognition\ncrossed the human face recognition accuracy and reached to 99%. Still, robust\nface recognition in the presence of real world conditions such as variation in\nresolution, illumination and pose is a major challenge for researchers in face\nrecognition. In this work, we used video as input to the 3D CNN architectures\nfor capturing both spatial and time domain information from the video for face\nrecognition in real world environment. For the purpose of experimentation, we\nhave developed our own video dataset called CVBL video dataset. The use of 3D\nCNN for face recognition in videos shows promising results with DenseNets\nperforming the best with an accuracy of 97% on CVBL dataset.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 11:31:40 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Mishra", "Nayaneesh Kumar", ""], ["Singh", "Satish Kumar", ""]]}, {"id": "2102.01445", "submitter": "Constantin Seibold", "authors": "Constantin Seibold, Matthias A. Fink, Charlotte Goos, Hans-Ulrich\n  Kauczor, Heinz-Peter Schlemmer, Rainer Stiefelhagen, Jens Kleesiek", "title": "Prediction of low-keV monochromatic images from polyenergetic CT scans\n  for improved automatic detection of pulmonary embolism", "comments": "4 pages, ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detector-based spectral computed tomography is a recent dual-energy CT (DECT)\ntechnology that offers the possibility of obtaining spectral information. From\nthis spectral data, different types of images can be derived, amongst others\nvirtual monoenergetic (monoE) images. MonoE images potentially exhibit\ndecreased artifacts, improve contrast, and overall contain lower noise values,\nmaking them ideal candidates for better delineation and thus improved\ndiagnostic accuracy of vascular abnormalities.\n  In this paper, we are training convolutional neural networks~(CNN) that can\nemulate the generation of monoE images from conventional single energy CT\nacquisitions. For this task, we investigate several commonly used\nimage-translation methods. We demonstrate that these methods while creating\nvisually similar outputs, lead to a poorer performance when used for automatic\nclassification of pulmonary embolism (PE). We expand on these methods through\nthe use of a multi-task optimization approach, under which the networks achieve\nimproved classification as well as generation results, as reflected by PSNR and\nSSIM scores. Further, evaluating our proposed framework on a subset of the\nRSNA-PE challenge data set shows that we are able to improve the Area under the\nReceiver Operating Characteristic curve (AuROC) in comparison to a na\\\"ive\nclassification approach from 0.8142 to 0.8420.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 11:42:31 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 22:44:26 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Seibold", "Constantin", ""], ["Fink", "Matthias A.", ""], ["Goos", "Charlotte", ""], ["Kauczor", "Hans-Ulrich", ""], ["Schlemmer", "Heinz-Peter", ""], ["Stiefelhagen", "Rainer", ""], ["Kleesiek", "Jens", ""]]}, {"id": "2102.01460", "submitter": "Alberto Pretto", "authors": "Alessandro Saviolo, Matteo Bonotto, Daniele Evangelista, Marco\n  Imperoli, Emanuele Menegatti and Alberto Pretto", "title": "Learning to Segment Human Body Parts with Synthetically Trained Deep\n  Convolutional Networks", "comments": "Submitted to the 16th International Conference on Intelligent\n  Autonomous System (IAS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new framework for human body part segmentation based on\nDeep Convolutional Neural Networks trained using only synthetic data. The\nproposed approach achieves cutting-edge results without the need of training\nthe models with real annotated data of human body parts. Our contributions\ninclude a data generation pipeline, that exploits a game engine for the\ncreation of the synthetic data used for training the network, and a novel\npre-processing module, that combines edge response map and adaptive histogram\nequalization to guide the network to learn the shape of the human body parts\nensuring robustness to changes in the illumination conditions. For selecting\nthe best candidate architecture, we performed exhaustive tests on\nmanually-annotated images of real human body limbs. We further present an\nablation study to validate our pre-processing module. The results show that our\nmethod outperforms several state-of-the-art semantic segmentation networks by a\nlarge margin. We release an implementation of the proposed approach along with\nthe acquired datasets with this paper.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 12:26:50 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Saviolo", "Alessandro", ""], ["Bonotto", "Matteo", ""], ["Evangelista", "Daniele", ""], ["Imperoli", "Marco", ""], ["Menegatti", "Emanuele", ""], ["Pretto", "Alberto", ""]]}, {"id": "2102.01486", "submitter": "Cheng Ma", "authors": "Cheng Ma, Jiwen Lu, Jie Zhou", "title": "Rank-Consistency Deep Hashing for Scalable Multi-Label Image Search", "comments": null, "journal-ref": "IEEE Transactions on Multimedia, 2020", "doi": "10.1109/TMM.2020.3034534", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As hashing becomes an increasingly appealing technique for large-scale image\nretrieval, multi-label hashing is also attracting more attention for the\nability to exploit multi-level semantic contents. In this paper, we propose a\nnovel deep hashing method for scalable multi-label image search. Unlike\nexisting approaches with conventional objectives such as contrast and triplet\nlosses, we employ a rank list, rather than pairs or triplets, to provide\nsufficient global supervision information for all the samples. Specifically, a\nnew rank-consistency objective is applied to align the similarity orders from\ntwo spaces, the original space and the hamming space. A powerful loss function\nis designed to penalize the samples whose semantic similarity and hamming\ndistance are mismatched in two spaces. Besides, a multi-label softmax\ncross-entropy loss is presented to enhance the discriminative power with a\nconcise formulation of the derivative function. In order to manipulate the\nneighborhood structure of the samples with different labels, we design a\nmulti-label clustering loss to cluster the hashing vectors of the samples with\nthe same labels by reducing the distances between the samples and their\nmultiple corresponding class centers. The state-of-the-art experimental results\nachieved on three public multi-label datasets, MIRFLICKR-25K, IAPRTC12 and\nNUS-WIDE, demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 13:46:58 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Ma", "Cheng", ""], ["Lu", "Jiwen", ""], ["Zhou", "Jie", ""]]}, {"id": "2102.01523", "submitter": "Guorui Feng", "authors": "Yalan Qin, Guorui Feng, Hanzhou Wu, Yanli Ren and Xinpeng Zhang", "title": "Orientation Convolutional Networks for Image Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Convolutional Neural Networks (DCNNs) are capable of obtaining powerful\nimage representations, which have attracted great attentions in image\nrecognition. However, they are limited in modeling orientation transformation\nby the internal mechanism. In this paper, we develop Orientation Convolution\nNetworks (OCNs) for image recognition based on the proposed Landmark Gabor\nFilters (LGFs) that the robustness of the learned representation against\nchanged of orientation can be enhanced. By modulating the convolutional filter\nwith LGFs, OCNs can be compatible with any existing deep learning networks.\nLGFs act as a Gabor filter bank achieved by selecting $ p $ $ \\left( \\ll\nn\\right) $ representative Gabor filters as andmarks and express the original\nGabor filters as sparse linear combinations of these landmarks. Specifically,\nbased on a matrix factorization framework, a flexible integration for the local\nand the global structure of original Gabor filters by sparsity and low-rank\nconstraints is utilized. With the propogation of the low-rank structure, the\ncorresponding sparsity for representation of original Gabor filter bank can be\nsignificantly promoted. Experimental results over several benchmarks\ndemonstrate that our method is less sensitive to the orientation and produce\nhigher performance both in accuracy and cost, compared with the existing\nstate-of-art methods. Besides, our OCNs have few parameters to learn and can\nsignificantly reduce the complexity of training network.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 14:49:40 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Qin", "Yalan", ""], ["Feng", "Guorui", ""], ["Wu", "Hanzhou", ""], ["Ren", "Yanli", ""], ["Zhang", "Xinpeng", ""]]}, {"id": "2102.01530", "submitter": "Juan Miguel Valverde", "authors": "Juan Miguel Valverde, Vandad Imani, Ali Abdollahzadeh, Riccardo De\n  Feo, Mithilesh Prakash, Robert Ciszek, Jussi Tohka", "title": "Transfer Learning in Magnetic Resonance Brain Imaging: a Systematic\n  Review", "comments": "Accepted in Journal of Imaging", "journal-ref": null, "doi": "10.3390/jimaging7040066", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Transfer learning refers to machine learning techniques that focus on\nacquiring knowledge from related tasks to improve generalization in the tasks\nof interest. In MRI, transfer learning is important for developing strategies\nthat address the variation in MR images. Additionally, transfer learning is\nbeneficial to re-utilize machine learning models that were trained to solve\nrelated tasks to the task of interest. Our goal is to identify research\ndirections, gaps of knowledge, applications, and widely used strategies among\nthe transfer learning approaches applied in MR brain imaging. We performed a\nsystematic literature search for articles that applied transfer learning to MR\nbrain imaging. We screened 433 studies and we categorized and extracted\nrelevant information, including task type, application, and machine learning\nmethods. Furthermore, we closely examined brain MRI-specific transfer learning\napproaches and other methods that tackled privacy, unseen target domains, and\nunlabeled data. We found 129 articles that applied transfer learning to brain\nMRI tasks. The most frequent applications were dementia related classification\ntasks and brain tumor segmentation. A majority of articles utilized transfer\nlearning on convolutional neural networks (CNNs). Only few approaches were\nclearly brain MRI specific, considered privacy issues, unseen target domains or\nunlabeled data. We proposed a new categorization to group specific, widely-used\napproaches. There is an increasing interest in transfer learning within brain\nMRI. Public datasets have contributed to the popularity of Alzheimer's\ndiagnostics/prognostics and tumor segmentation. Likewise, the availability of\npretrained CNNs has promoted their utilization. Finally, the majority of the\nsurveyed studies did not examine in detail the interpretation of their\nstrategies after applying transfer learning, and did not compare to other\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 14:59:05 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 20:03:55 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Valverde", "Juan Miguel", ""], ["Imani", "Vandad", ""], ["Abdollahzadeh", "Ali", ""], ["De Feo", "Riccardo", ""], ["Prakash", "Mithilesh", ""], ["Ciszek", "Robert", ""], ["Tohka", "Jussi", ""]]}, {"id": "2102.01539", "submitter": "Sudipan Saha", "authors": "Sudipan Saha and Nasrullah Sheikh", "title": "Ultrasound Image Classification using ACGAN with Small Training Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  B-mode ultrasound imaging is a popular medical imaging technique. Like other\nimage processing tasks, deep learning has been used for analysis of B-mode\nultrasound images in the last few years. However, training deep learning models\nrequires large labeled datasets, which is often unavailable for ultrasound\nimages. The lack of large labeled data is a bottleneck for the use of deep\nlearning in ultrasound image analysis. To overcome this challenge, in this work\nwe exploit Auxiliary Classifier Generative Adversarial Network (ACGAN) that\ncombines the benefits of data augmentation and transfer learning in the same\nframework. We conduct experiment on a dataset of breast ultrasound images that\nshows the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 11:11:24 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Saha", "Sudipan", ""], ["Sheikh", "Nasrullah", ""]]}, {"id": "2102.01549", "submitter": "Yang Wen", "authors": "Yang Wen", "title": "Medical Datasets Collections for Artificial Intelligence-based Medical\n  Image Analysis", "comments": "6 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We collected 32 public datasets, of which 28 for medical imaging and 4 for\nnatural images, to conduct study. The images of these datasets are captured by\ndifferent cameras, thus vary from each other in modality, frame size and\ncapacity. For data accessibility, we also provide the websites of most datasets\nand hope this will help the readers reach the datasets.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 15:23:42 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 04:54:47 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2021 03:53:26 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Wen", "Yang", ""]]}, {"id": "2102.01558", "submitter": "Jiyang Qi", "authors": "Jiyang Qi, Yan Gao, Yao Hu, Xinggang Wang, Xiaoyu Liu, Xiang Bai,\n  Serge Belongie, Alan Yuille, Philip H.S. Torr, Song Bai", "title": "Occluded Video Instance Segmentation", "comments": "project page at https://songbai.site/ovis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can our video understanding systems perceive objects when a heavy occlusion\nexists in a scene?\n  To answer this question, we collect a large-scale dataset called OVIS for\noccluded video instance segmentation, that is, to simultaneously detect,\nsegment, and track instances in occluded scenes. OVIS consists of 296k\nhigh-quality instance masks from 25 semantic categories, where object\nocclusions usually occur. While our human vision systems can understand those\noccluded instances by contextual reasoning and association, our experiments\nsuggest that current video understanding systems are not satisfying. On the\nOVIS dataset, the highest AP achieved by state-of-the-art algorithms is only\n14.4, which reveals that we are still at a nascent stage for understanding\nobjects, instances, and videos in a real-world scenario. In experiments, a\nsimple plug-and-play module that performs temporal feature calibration is\nproposed to complement missing object cues caused by occlusion. Built upon\nMaskTrack R-CNN and SipMask, we obtain an AP of 15.1 and 14.5 on the OVIS\ndataset and achieve 32.1 and 35.1 on the YouTube-VIS dataset respectively, a\nremarkable improvement over the state-of-the-art methods. The OVIS dataset is\nreleased at http://songbai.site/ovis , and the project code will be available\nsoon.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 15:35:43 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 08:10:55 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2021 12:20:37 GMT"}, {"version": "v4", "created": "Tue, 30 Mar 2021 04:07:27 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Qi", "Jiyang", ""], ["Gao", "Yan", ""], ["Hu", "Yao", ""], ["Wang", "Xinggang", ""], ["Liu", "Xiaoyu", ""], ["Bai", "Xiang", ""], ["Belongie", "Serge", ""], ["Yuille", "Alan", ""], ["Torr", "Philip H. S.", ""], ["Bai", "Song", ""]]}, {"id": "2102.01579", "submitter": "Xiangyu Xu", "authors": "Xiangyu Xu, Yongrui Ma, Wenxiu Sun, Ming-Hsuan Yang", "title": "Exploiting Raw Images for Real-Scene Super-Resolution", "comments": "A larger version with higher-resolution figures is available at:\n  https://sites.google.com/view/xiangyuxu. arXiv admin note: text overlap with\n  arXiv:1905.12156", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2020", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Super-resolution is a fundamental problem in computer vision which aims to\novercome the spatial limitation of camera sensors. While significant progress\nhas been made in single image super-resolution, most algorithms only perform\nwell on synthetic data, which limits their applications in real scenarios. In\nthis paper, we study the problem of real-scene single image super-resolution to\nbridge the gap between synthetic data and real captured images. We focus on two\nissues of existing super-resolution algorithms: lack of realistic training data\nand insufficient utilization of visual information obtained from cameras. To\naddress the first issue, we propose a method to generate more realistic\ntraining data by mimicking the imaging process of digital cameras. For the\nsecond issue, we develop a two-branch convolutional neural network to exploit\nthe radiance information originally-recorded in raw images. In addition, we\npropose a dense channel-attention block for better image restoration as well as\na learning-based guided filter network for effective color correction. Our\nmodel is able to generalize to different cameras without deliberately training\non images from specific camera types. Extensive experiments demonstrate that\nthe proposed algorithm can recover fine details and clear structures, and\nachieve high-quality results for single image super-resolution in real scenes.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 16:10:15 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Xu", "Xiangyu", ""], ["Ma", "Yongrui", ""], ["Sun", "Wenxiu", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2102.01582", "submitter": "Mats Richter", "authors": "Mats L. Richter, Wolf Byttner, Ulf Krumnack, Ludwdig Schallner, Justin\n  Shenk", "title": "Size Matters", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fully convolutional neural networks can process input of arbitrary size by\napplying a combination of downsampling and pooling. However, we find that fully\nconvolutional image classifiers are not agnostic to the input size but rather\nshow significant differences in performance: presenting the same image at\ndifferent scales can result in different outcomes. A closer look reveals that\nthere is no simple relationship between input size and model performance (no\n`bigger is better'), but that each each network has a preferred input size, for\nwhich it shows best results. We investigate this phenomenon by applying\ndifferent methods, including spectral analysis of layer activations and probe\nclassifiers, showing that there are characteristic features depending on the\nnetwork architecture. From this we find that the size of discriminatory\nfeatures is critically influencing how the inference process is distributed\namong the layers.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 16:17:52 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 09:00:14 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Richter", "Mats L.", ""], ["Byttner", "Wolf", ""], ["Krumnack", "Ulf", ""], ["Schallner", "Ludwdig", ""], ["Shenk", "Justin", ""]]}, {"id": "2102.01586", "submitter": "Mohammad Hossein Jafari", "authors": "Mohammad H. Jafari, Christina Luong, Michael Tsang, Ang Nan Gu, Nathan\n  Van Woudenberg, Robert Rohling, Teresa Tsang, Purang Abolmaesumi", "title": "U-LanD: Uncertainty-Driven Video Landmark Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents U-LanD, a framework for joint detection of key frames and\nlandmarks in videos. We tackle a specifically challenging problem, where\ntraining labels are noisy and highly sparse. U-LanD builds upon a pivotal\nobservation: a deep Bayesian landmark detector solely trained on key video\nframes, has significantly lower predictive uncertainty on those frames vs.\nother frames in videos. We use this observation as an unsupervised signal to\nautomatically recognize key frames on which we detect landmarks. As a test-bed\nfor our framework, we use ultrasound imaging videos of the heart, where sparse\nand noisy clinical labels are only available for a single frame in each video.\nUsing data from 4,493 patients, we demonstrate that U-LanD can exceedingly\noutperform the state-of-the-art non-Bayesian counterpart by a noticeable\nabsolute margin of 42% in R2 score, with almost no overhead imposed on the\nmodel size. Our approach is generic and can be potentially applied to other\nchallenging data with noisy and sparse training labels.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 16:23:54 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Jafari", "Mohammad H.", ""], ["Luong", "Christina", ""], ["Tsang", "Michael", ""], ["Gu", "Ang Nan", ""], ["Van Woudenberg", "Nathan", ""], ["Rohling", "Robert", ""], ["Tsang", "Teresa", ""], ["Abolmaesumi", "Purang", ""]]}, {"id": "2102.01604", "submitter": "Ya\\\"el Balbastre", "authors": "Yael Balbastre, Mikael Brudfors, Michela Azzarito, Christian Lambert,\n  Martina F. Callaghan, John Ashburner", "title": "Model-based multi-parameter mapping", "comments": "20 pages, 6 figures, accepted at Medical Image Analysis", "journal-ref": null, "doi": "10.1016/j.media.2021.102149", "report-no": null, "categories": "cs.CV eess.IV stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantitative MR imaging is increasingly favoured for its richer information\ncontent and standardised measures. However, computing quantitative parameter\nmaps, such as those encoding longitudinal relaxation rate (R1), apparent\ntransverse relaxation rate (R2*) or magnetisation-transfer saturation (MTsat),\ninvolves inverting a highly non-linear function. Many methods for deriving\nparameter maps assume perfect measurements and do not consider how noise is\npropagated through the estimation procedure, resulting in needlessly noisy\nmaps. Instead, we propose a probabilistic generative (forward) model of the\nentire dataset, which is formulated and inverted to jointly recover (log)\nparameter maps with a well-defined probabilistic interpretation (e.g., maximum\nlikelihood or maximum a posteriori). The second order optimisation we propose\nfor model fitting achieves rapid and stable convergence thanks to a novel\napproximate Hessian. We demonstrate the utility of our flexible framework in\nthe context of recovering more accurate maps from data acquired using the\npopular multi-parameter mapping protocol. We also show how to incorporate a\njoint total variation prior to further decrease the noise in the maps, noting\nthat the probabilistic formulation allows the uncertainty on the recovered\nparameter maps to be estimated. Our implementation uses a PyTorch backend and\nbenefits from GPU acceleration. It is available at\nhttps://github.com/balbasty/nitorch.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 17:00:11 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 19:02:54 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Balbastre", "Yael", ""], ["Brudfors", "Mikael", ""], ["Azzarito", "Michela", ""], ["Lambert", "Christian", ""], ["Callaghan", "Martina F.", ""], ["Ashburner", "John", ""]]}, {"id": "2102.01670", "submitter": "Kale-Ab Tessera", "authors": "Kale-ab Tessera, Sara Hooker, Benjamin Rosman", "title": "Keep the Gradients Flowing: Using Gradient Flow to Study Sparse Network\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training sparse networks to converge to the same performance as dense neural\narchitectures has proven to be elusive. Recent work suggests that\ninitialization is the key. However, while this direction of research has had\nsome success, focusing on initialization alone appears to be inadequate. In\nthis paper, we take a broader view of training sparse networks and consider the\nrole of regularization, optimization, and architecture choices on sparse\nmodels. We propose a simple experimental framework, Same Capacity Sparse vs\nDense Comparison (SC-SDC), that allows for a fair comparison of sparse and\ndense networks. Furthermore, we propose a new measure of gradient flow,\nEffective Gradient Flow (EGF), that better correlates to performance in sparse\nnetworks. Using top-line metrics, SC-SDC and EGF, we show that default choices\nof optimizers, activation functions and regularizers used for dense networks\ncan disadvantage sparse networks. Based upon these findings, we show that\ngradient flow in sparse networks can be improved by reconsidering aspects of\nthe architecture design and the training regime. Our work suggests that\ninitialization is only one piece of the puzzle and taking a wider view of\ntailoring optimization to sparse networks yields promising results.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 18:40:26 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 02:49:34 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Tessera", "Kale-ab", ""], ["Hooker", "Sara", ""], ["Rosman", "Benjamin", ""]]}, {"id": "2102.01678", "submitter": "Rikiya Yamashita", "authors": "Rikiya Yamashita, Jin Long, Snikitha Banda, Jeanne Shen, Daniel L.\n  Rubin", "title": "Learning domain-agnostic visual representation for computational\n  pathology using medically-irrelevant style transfer augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suboptimal generalization of machine learning models on unseen data is a key\nchallenge which hampers the clinical applicability of such models to medical\nimaging. Although various methods such as domain adaptation and domain\ngeneralization have evolved to combat this challenge, learning robust and\ngeneralizable representations is core to medical image understanding, and\ncontinues to be a problem. Here, we propose STRAP (Style TRansfer Augmentation\nfor histoPathology), a form of data augmentation based on random style transfer\nfrom non-medical style source such as artistic paintings, for learning\ndomain-agnostic visual representations in computational pathology. Style\ntransfer replaces the low-level texture content of an image with the\nuninformative style of randomly selected style source image, while preserving\nthe original high-level semantic content. This improves robustness to domain\nshift and can be used as a simple yet powerful tool for learning\ndomain-agnostic representations. We demonstrate that STRAP leads to\nstate-of-the-art performance, particularly in the presence of domain shifts, on\ntwo particular classification tasks in computational pathology.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 18:50:16 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 23:07:49 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Yamashita", "Rikiya", ""], ["Long", "Jin", ""], ["Banda", "Snikitha", ""], ["Shen", "Jeanne", ""], ["Rubin", "Daniel L.", ""]]}, {"id": "2102.01690", "submitter": "Wei-Lin Hsiao", "authors": "Wei-Lin Hsiao, Kristen Grauman", "title": "From Culture to Clothing: Discovering the World Events Behind A Century\n  of Fashion Images", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion is intertwined with external cultural factors, but identifying these\nlinks remains a manual process limited to only the most salient phenomena. We\npropose a data-driven approach to identify specific cultural factors affecting\nthe clothes people wear. Using large-scale datasets of news articles and\nvintage photos spanning a century, we introduce a multi-modal statistical model\nto detect influence relationships between happenings in the world and people's\nchoice of clothing. Furthermore, we apply our model to improve the concrete\nvision tasks of visual style forecasting and photo timestamping on two\ndatasets. Our work is a first step towards a computational, scalable, and\neasily refreshable approach to link culture to clothing.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 18:58:21 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Hsiao", "Wei-Lin", ""], ["Grauman", "Kristen", ""]]}, {"id": "2102.01749", "submitter": "Ali Rahimpour Jounghani", "authors": "Zahra Salahshoori Nejad, Hamed Heravi, Ali Rahimpour Jounghani,\n  Abdollah Shahrezaie, Afshin Ebrahimi", "title": "Vehicle trajectory prediction in top-view image sequences based on deep\n  learning method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Annually, a large number of injuries and deaths around the world are related\nto motor vehicle accidents. This value has recently been reduced to some\nextent, via the use of driver-assistance systems. Developing driver-assistance\nsystems (i.e., automated driving systems) can play a crucial role in reducing\nthis number. Estimating and predicting surrounding vehicles' movement is\nessential for an automated vehicle and advanced safety systems. Moreover,\npredicting the trajectory is influenced by numerous factors, such as drivers'\nbehavior during accidents, history of the vehicle's movement and the\nsurrounding vehicles, and their position on the traffic scene. The vehicle must\nmove over a safe path in traffic and react to other drivers' unpredictable\nbehaviors in the shortest time. Herein, to predict automated vehicles' path, a\nmodel with low computational complexity is proposed, which is trained by images\ntaken from the road's aerial image. Our method is based on an encoder-decoder\nmodel that utilizes a social tensor to model the effect of the surrounding\nvehicles' movement on the target vehicle. The proposed model can predict the\nvehicle's future path in any freeway only by viewing the images related to the\nhistory of the target vehicle's movement and its neighbors. Deep learning was\nused as a tool for extracting the features of these images. Using the HighD\ndatabase, an image dataset of the road's aerial image was created, and the\nmodel's performance was evaluated on this new database. We achieved the RMSE of\n1.91 for the next 5 seconds and found that the proposed method had less error\nthan the best path-prediction methods in previous studies.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 20:48:19 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 03:06:01 GMT"}, {"version": "v3", "created": "Sun, 16 May 2021 18:39:08 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Nejad", "Zahra Salahshoori", ""], ["Heravi", "Hamed", ""], ["Jounghani", "Ali Rahimpour", ""], ["Shahrezaie", "Abdollah", ""], ["Ebrahimi", "Afshin", ""]]}, {"id": "2102.01767", "submitter": "Jorge Miguel Ferreira Da Silva", "authors": "Jorge Miguel Silva, Diogo Pratas, Rui Antunes, S\\'ergio Matos, and\n  Armando J. Pinho", "title": "Automatic analysis of artistic paintings using information-based\n  measures", "comments": "Website: http://panther.web.ua.pt 24 Pages; 19 pages article; 5 pages\n  supplementary material", "journal-ref": "Pattern Recognition (2021) 107864", "doi": "10.1016/j.patcog.2021.107864", "report-no": null, "categories": "cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The artistic community is increasingly relying on automatic computational\nanalysis for authentication and classification of artistic paintings. In this\npaper, we identify hidden patterns and relationships present in artistic\npaintings by analysing their complexity, a measure that quantifies the sum of\ncharacteristics of an object. Specifically, we apply Normalized Compression\n(NC) and the Block Decomposition Method (BDM) to a dataset of 4,266 paintings\nfrom 91 authors and examine the potential of these information-based measures\nas descriptors of artistic paintings. Both measures consistently described the\nequivalent types of paintings, authors, and artistic movements. Moreover,\ncombining the NC with a measure of the roughness of the paintings creates an\nefficient stylistic descriptor. Furthermore, by quantifying the local\ninformation of each painting, we define a fingerprint that describes critical\ninformation regarding the artists' style, their artistic influences, and shared\ntechniques. More fundamentally, this information describes how each author\ntypically composes and distributes the elements across the canvas and,\ntherefore, how their work is perceived. Finally, we demonstrate that regional\ncomplexity and two-point height difference correlation function are useful\nauxiliary features that improve current methodologies in style and author\nclassification of artistic paintings. The whole study is supported by an\nextensive website (http://panther.web.ua.pt) for fast author characterization\nand authentication.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 21:40:30 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Silva", "Jorge Miguel", ""], ["Pratas", "Diogo", ""], ["Antunes", "Rui", ""], ["Matos", "S\u00e9rgio", ""], ["Pinho", "Armando J.", ""]]}, {"id": "2102.01788", "submitter": "Ray Chang", "authors": "Yi-Shiou Duh, Ray Chang", "title": "Recurrent Neural Network for MoonBoard Climbing Route Classification and\n  Generation", "comments": "9 pages, 4 figures, 4 appendix figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Classifying the difficulties of climbing routes and generating new routes are\nboth challenging. Existing machine learning models not only fail to accurately\npredict a problem's difficulty, but they are also unable to generate reasonable\nproblems. In this work, we introduced \"BetaMove\", a new move preprocessing\npipeline we developed, in order to mimic a human climber's hand sequence. The\npreprocessed move sequences were then used to train both a route generator and\na grade predictor. By preprocessing a MoonBoard problem into a proper move\nsequence, the accuracy of our grade predictor reaches near human-level\nperformance, and our route generator produces new routes of much better quality\ncompared to previous work. We demonstrated that with BetaMove, we are able to\ninject human insights into the machine learning problems, and this can be the\nfoundations for future transfer learning on climbing style classification\nproblems.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 22:38:23 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Duh", "Yi-Shiou", ""], ["Chang", "Ray", ""]]}, {"id": "2102.01822", "submitter": "Md. Kamrul Hasan", "authors": "Tarun Kanti Ghosh, Md. Kamrul Hasan, Shidhartho Roy, Md. Ashraful\n  Alam, Eklas Hossain, Mohiuddin Ahmad", "title": "Multi-class probabilistic atlas-based whole heart segmentation method in\n  cardiac CT and MRI", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate and robust whole heart substructure segmentation is crucial in\ndeveloping clinical applications, such as computer-aided diagnosis and\ncomputer-aided surgery. However, segmentation of different heart substructures\nis challenging because of inadequate edge or boundary information, the\ncomplexity of the background and texture, and the diversity in different\nsubstructures' sizes and shapes. This article proposes a framework for\nmulti-class whole heart segmentation employing non-rigid registration-based\nprobabilistic atlas incorporating the Bayesian framework. We also propose a\nnon-rigid registration pipeline utilizing a multi-resolution strategy for\nobtaining the highest attainable mutual information between the moving and\nfixed images. We further incorporate non-rigid registration into the\nexpectation-maximization algorithm and implement different deep convolutional\nneural network-based encoder-decoder networks for ablation studies. All the\nextensive experiments are conducted utilizing the publicly available dataset\nfor the whole heart segmentation containing 20 MRI and 20 CT cardiac images.\nThe proposed approach exhibits an encouraging achievement, yielding a mean\nvolume overlapping error of 14.5 % for CT scans exceeding the state-of-the-art\nresults by a margin of 1.3 % in terms of the same metric. As the proposed\napproach provides better-results to delineate the different substructures of\nthe heart, it can be a medical diagnostic aiding tool for helping experts with\nquicker and more accurate results.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 01:02:09 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Ghosh", "Tarun Kanti", ""], ["Hasan", "Md. Kamrul", ""], ["Roy", "Shidhartho", ""], ["Alam", "Md. Ashraful", ""], ["Hossain", "Eklas", ""], ["Ahmad", "Mohiuddin", ""]]}, {"id": "2102.01824", "submitter": "Md. Kamrul Hasan", "authors": "Md. Kamrul Hasan, Shidhartho Roy, Chayan Mondal, Md. Ashraful Alam,\n  Md.Toufick E Elahi, Aishwariya Dutta, S. M. Taslim Uddin Raju, Md. Tasnim\n  Jawad, Mohiuddin Ahmad", "title": "Dermo-DOCTOR: A framework for concurrent skin lesion detection and\n  recognition using a deep convolutional neural network with end-to-end dual\n  encoders", "comments": "39 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated skin lesion analysis for simultaneous detection and recognition is\nstill challenging for inter-class homogeneity and intra-class heterogeneity,\nleading to low generic capability of a Single Convolutional Neural Network\n(CNN) with limited datasets. This article proposes an end-to-end deep CNN-based\nframework for simultaneous detection and recognition of the skin lesions, named\nDermo-DOCTOR, consisting of two encoders. The feature maps from two encoders\nare fused channel-wise, called Fused Feature Map (FFM). The FFM is utilized for\ndecoding in the detection sub-network, concatenating each stage of two\nencoders' outputs with corresponding decoder layers to retrieve the lost\nspatial information due to pooling in the encoders. For the recognition\nsub-network, the outputs of three fully connected layers, utilizing feature\nmaps of two encoders and FFM, are aggregated to obtain a final lesion class. We\ntrain and evaluate the proposed Dermo-Doctor utilizing two publicly available\nbenchmark datasets, such as ISIC-2016 and ISIC-2017. The achieved segmentation\nresults exhibit mean intersection over unions of 85.0 % and 80.0 % respectively\nfor ISIC-2016 and ISIC-2017 test datasets. The proposed Dermo-DOCTOR also\ndemonstrates praiseworthy success in lesion recognition, providing the areas\nunder the receiver operating characteristic curves of 0.98 and 0.91\nrespectively for those two datasets. The experimental results show that the\nproposed Dermo-DOCTOR outperforms the alternative methods mentioned in the\nliterature, designed for skin lesion detection and recognition. As the\nDermo-DOCTOR provides better-results on two different test datasets, even with\nlimited training data, it can be an auspicious computer-aided assistive tool\nfor dermatologists.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 01:14:52 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 13:46:30 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Hasan", "Md. Kamrul", ""], ["Roy", "Shidhartho", ""], ["Mondal", "Chayan", ""], ["Alam", "Md. Ashraful", ""], ["Elahi", "Md. Toufick E", ""], ["Dutta", "Aishwariya", ""], ["Raju", "S. M. Taslim Uddin", ""], ["Jawad", "Md. Tasnim", ""], ["Ahmad", "Mohiuddin", ""]]}, {"id": "2102.01850", "submitter": "Ru Li", "authors": "Ru Li, Chuan Wang, Shuaicheng Liu, Jue Wang, Guanghui Liu, Bing Zeng", "title": "UPHDR-GAN: Generative Adversarial Network for High Dynamic Range Imaging\n  with Unpaired Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes a method to effectively fuse multi-exposure inputs and\ngenerates high-quality high dynamic range (HDR) images with unpaired datasets.\nDeep learning-based HDR image generation methods rely heavily on paired\ndatasets. The ground truth provides information for the network getting HDR\nimages without ghosting. Datasets without ground truth are hard to apply to\ntrain deep neural networks. Recently, Generative Adversarial Networks (GAN)\nhave demonstrated their potentials of translating images from source domain X\nto target domain Y in the absence of paired examples. In this paper, we propose\na GAN-based network for solving such problems while generating enjoyable HDR\nresults, named UPHDR-GAN. The proposed method relaxes the constraint of paired\ndataset and learns the mapping from LDR domain to HDR domain. Although the pair\ndata are missing, UPHDR-GAN can properly handle the ghosting artifacts caused\nby moving objects or misalignments with the help of modified GAN loss, improved\ndiscriminator network and useful initialization phase. The proposed method\npreserves the details of important regions and improves the total image\nperceptual quality. Qualitative and quantitative comparisons against other\nmethods demonstrated the superiority of our method.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 03:09:14 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Li", "Ru", ""], ["Wang", "Chuan", ""], ["Liu", "Shuaicheng", ""], ["Wang", "Jue", ""], ["Liu", "Guanghui", ""], ["Zeng", "Bing", ""]]}, {"id": "2102.01860", "submitter": "An Yan", "authors": "An Yan, Xin Eric Wang, Tsu-Jui Fu, William Yang Wang", "title": "L2C: Describing Visual Differences Needs Semantic Understanding of\n  Individuals", "comments": "EACL-2021 short", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in language and vision push forward the research of\ncaptioning a single image to describing visual differences between image pairs.\nSuppose there are two images, I_1 and I_2, and the task is to generate a\ndescription W_{1,2} comparing them, existing methods directly model { I_1, I_2\n} -> W_{1,2} mapping without the semantic understanding of individuals. In this\npaper, we introduce a Learning-to-Compare (L2C) model, which learns to\nunderstand the semantic structures of these two images and compare them while\nlearning to describe each one. We demonstrate that L2C benefits from a\ncomparison between explicit semantic representations and single-image captions,\nand generalizes better on the new testing image pairs. It outperforms the\nbaseline on both automatic evaluation and human evaluation for the\nBirds-to-Words dataset.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 03:44:42 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Yan", "An", ""], ["Wang", "Xin Eric", ""], ["Fu", "Tsu-Jui", ""], ["Wang", "William Yang", ""]]}, {"id": "2102.01863", "submitter": "Raj Prateek Kosaraju", "authors": "Raj Prateek Kosaraju", "title": "Deep CNNs for large scale species classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large Scale image classification is a challenging problem within the field of\ncomputer vision. As the real world contains billions of different objects,\nunderstanding the performance of popular techniques and models is vital in\norder to apply them to real world tasks. In this paper, we evaluate techniques\nand popular CNN based deep learning architectures to perform large scale\nspecies classification on the dataset from iNaturalist 2019 Challenge. Methods\nutilizing dataset pruning and transfer learning are shown to outperform models\ntrained without either of the two techniques. The ResNext based classifier\noutperforms other model architectures over 10 epochs and achieves a top-one\nvalidation error of 0.68 when classifying amongst the 1,010 species.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 03:53:03 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Kosaraju", "Raj Prateek", ""]]}, {"id": "2102.01874", "submitter": "Ghazal Mazaheri", "authors": "Ghazal Mazaheri, Kevin Urrutia Avila, Amit K. Roy-Chowdhury", "title": "Learning to identify image manipulations in scientific publications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adherence to scientific community standards ensures objectivity, clarity,\nreproducibility, and helps prevent bias, fabrication, falsification, and\nplagiarism. To help scientific integrity officers and journal/publisher\nreviewers monitor if researchers stick with these standards, it is important to\nhave a solid procedure to detect duplication as one of the most frequent types\nof manipulation in scientific papers. Images in scientific papers are used to\nsupport the experimental description and the discussion of the findings.\nTherefore, in this work we focus on detecting the duplications in images as one\nof the most important parts of a scientific paper. We propose a framework that\ncombines image processing and deep learning methods to classify images in the\narticles as duplicated or unduplicated ones. We show that our method leads to a\n90% accuracy rate of detecting duplicated images, a ~ 13% improvement in\ndetection accuracy in comparison to other manipulation detection methods. We\nalso show how effective the pre-processing steps are by comparing our method to\nother state-of-art manipulation detectors which lack these steps.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 04:47:34 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Mazaheri", "Ghazal", ""], ["Avila", "Kevin Urrutia", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "2102.01882", "submitter": "Ammar Kamoona", "authors": "Ammar Mansoor Kamoona, Amirali Khodadadian Gostar, Alireza\n  Bab-Hadiashar, Reza Hoseinnezhad", "title": "Evaluation of Point Pattern Features for Anomaly Detection of Defect\n  within Random Finite Set Framework", "comments": "under review. 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Defect detection in the manufacturing industry is of utmost importance for\nproduct quality inspection. Recently, optical defect detection has been\ninvestigated as an anomaly detection using different deep learning methods.\nHowever, the recent works do not explore the use of point pattern features,\nsuch as SIFT for anomaly detection using the recently developed set-based\nmethods. In this paper, we present an evaluation of different point pattern\nfeature detectors and descriptors for defect detection application. The\nevaluation is performed within the random finite set framework. Handcrafted\npoint pattern features, such as SIFT as well as deep features are used in this\nevaluation. Random finite set-based defect detection is compared with\nstate-of-the-arts anomaly detection methods. The results show that using point\npattern features, such as SIFT as data points for random finite set-based\nanomaly detection achieves the most consistent defect detection accuracy on the\nMVTec-AD dataset.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 05:30:32 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Kamoona", "Ammar Mansoor", ""], ["Gostar", "Amirali Khodadadian", ""], ["Bab-Hadiashar", "Alireza", ""], ["Hoseinnezhad", "Reza", ""]]}, {"id": "2102.01886", "submitter": "Xuefeng Du", "authors": "Xuefeng Du, Jingfeng Zhang, Bo Han, Tongliang Liu, Yu Rong, Gang Niu,\n  Junzhou Huang, Masashi Sugiyama", "title": "Learning Diverse-Structured Networks for Adversarial Robustness", "comments": "ICML2021, code: https://github.com/d12306/dsnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In adversarial training (AT), the main focus has been the objective and\noptimizer while the model has been less studied, so that the models being used\nare still those classic ones in standard training (ST). Classic network\narchitectures (NAs) are generally worse than searched NAs in ST, which should\nbe the same in AT. In this paper, we argue that NA and AT cannot be handled\nindependently, since given a dataset, the optimal NA in ST would be no longer\noptimal in AT. That being said, AT is time-consuming itself; if we directly\nsearch NAs in AT over large search spaces, the computation will be practically\ninfeasible. Thus, we propose a diverse-structured network (DS-Net), to\nsignificantly reduce the size of the search space: instead of low-level\noperations, we only consider predefined atomic blocks, where an atomic block is\na time-tested building block like the residual block. There are only a few\natomic blocks and thus we can weight all atomic blocks rather than find the\nbest one in a searched block of DS-Net, which is an essential trade-off between\nexploring diverse structures and exploiting the best structures. Empirical\nresults demonstrate the advantages of DS-Net, i.e., weighting the atomic\nblocks.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 05:52:11 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 14:48:27 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2021 03:09:39 GMT"}, {"version": "v4", "created": "Fri, 18 Jun 2021 06:57:10 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Du", "Xuefeng", ""], ["Zhang", "Jingfeng", ""], ["Han", "Bo", ""], ["Liu", "Tongliang", ""], ["Rong", "Yu", ""], ["Niu", "Gang", ""], ["Huang", "Junzhou", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "2102.01889", "submitter": "Yangling Ma", "authors": "Yangling Ma, Zhouwang Yang", "title": "Multi-Instance Learning by Utilizing Structural Relationship among\n  Instances", "comments": "22 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Multi-Instance Learning(MIL) aims to learn the mapping between a bag of\ninstances and the bag-level label. Therefore, the relationships among instances\nare very important for learning the mapping. In this paper, we propose an MIL\nalgorithm based on a graph built by structural relationship among instances\nwithin a bag. Then, Graph Convolutional Network(GCN) and the graph-attention\nmechanism are used to learn bag-embedding. In the task of medical image\nclassification, our GCN-based MIL algorithm makes full use of the structural\nrelationships among patches(instances) in an original image space domain, and\nexperimental results verify that our method is more suitable for handling\nmedical high-resolution images. We also verify experimentally that the proposed\nmethod achieves better results than previous methods on five bechmark MIL\ndatasets and four medical image datasets.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 06:06:03 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Ma", "Yangling", ""], ["Yang", "Zhouwang", ""]]}, {"id": "2102.01894", "submitter": "Limin Wang", "authors": "Jing Tan, Jiaqi Tang, Limin Wang, Gangshan Wu", "title": "Relaxed Transformer Decoders for Direct Action Proposal Generation", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Temporal action proposal generation is an important and challenging task in\nvideo understanding, which aims at detecting all temporal segments containing\naction instances of interest. The existing proposal generation approaches are\ngenerally based on pre-defined anchor windows or heuristic bottom-up boundary\nmatching strategies. This paper presents a simple and end-to-end learnable\nframework (RTD-Net) for direct action proposal generation, by re-purposing a\nTransformer-alike architecture. To tackle the essential visual difference\nbetween time and space, we make three important improvements over the original\ntransformer detection framework (DETR). First, to deal with slowness prior in\nvideos, we replace the original Transformer encoder with a boundary attentive\nmodule to better capture long-range temporal information. Second, due to the\nambiguous temporal boundary and relatively sparse annotations, we present a\nrelaxed matching scheme to relieve the strict criteria of single assignment to\neach groundtruth. Finally, we devise a three-branch head to further improve the\nproposal confidence estimation by explicitly predicting its completeness.\nExtensive experiments on THUMOS14 and ActivityNet-1.3 benchmarks demonstrate\nthe effectiveness of RTD-Net, on both tasks of temporal action proposal\ngeneration and temporal action detection. Moreover, due to its simplicity in\ndesign, our framework is more efficient than previous proposal generation\nmethods, without non-maximum suppression post-processing. The code and models\nwill be made available at https://github.com/MCG-NJU/RTD-Action.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 06:29:28 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 08:43:34 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Tan", "Jing", ""], ["Tang", "Jiaqi", ""], ["Wang", "Limin", ""], ["Wu", "Gangshan", ""]]}, {"id": "2102.01897", "submitter": "WenHui Lei", "authors": "Wenhui Lei, Haochen Mei, Zhengwentai Sun, Shan Ye, Ran Gu, Huan Wang,\n  Rui Huang, Shichuan Zhang, Shaoting Zhang, Guotai Wang", "title": "Automatic Segmentation of Organs-at-Risk from Head-and-Neck CT using\n  Separable Convolutional Neural Network with Hard-Region-Weighted Loss", "comments": "Accepted by Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Nasopharyngeal Carcinoma (NPC) is a leading form of Head-and-Neck (HAN)\ncancer in the Arctic, China, Southeast Asia, and the Middle East/North Africa.\nAccurate segmentation of Organs-at-Risk (OAR) from Computed Tomography (CT)\nimages with uncertainty information is critical for effective planning of\nradiation therapy for NPC treatment. Despite the stateof-the-art performance\nachieved by Convolutional Neural Networks (CNNs) for automatic segmentation of\nOARs, existing methods do not provide uncertainty estimation of the\nsegmentation results for treatment planning, and their accuracy is still\nlimited by several factors, including the low contrast of soft tissues in CT,\nhighly imbalanced sizes of OARs and large inter-slice spacing. To address these\nproblems, we propose a novel framework for accurate OAR segmentation with\nreliable uncertainty estimation. First, we propose a Segmental Linear Function\n(SLF) to transform the intensity of CT images to make multiple organs more\ndistinguishable than existing methods based on a simple window width/level that\noften gives a better visibility of one organ while hiding the others. Second,\nto deal with the large inter-slice spacing, we introduce a novel 2.5D network\n(named as 3D-SepNet) specially designed for dealing with clinic HAN CT scans\nwith anisotropic spacing. Thirdly, existing hardness-aware loss function often\ndeal with class-level hardness, but our proposed attention to hard voxels (ATH)\nuses a voxel-level hardness strategy, which is more suitable to dealing with\nsome hard regions despite that its corresponding class may be easy. Our code is\nnow available at https://github.com/HiLab-git/SepNet.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 06:31:38 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Lei", "Wenhui", ""], ["Mei", "Haochen", ""], ["Sun", "Zhengwentai", ""], ["Ye", "Shan", ""], ["Gu", "Ran", ""], ["Wang", "Huan", ""], ["Huang", "Rui", ""], ["Zhang", "Shichuan", ""], ["Zhang", "Shaoting", ""], ["Wang", "Guotai", ""]]}, {"id": "2102.01906", "submitter": "Vinod Kumar Kurmi", "authors": "Vinod K Kurmi, Badri N. Patro, Venkatesh K. Subramanian, Vinay P.\n  Namboodiri", "title": "Do Not Forget to Attend to Uncertainty while Mitigating Catastrophic\n  Forgetting", "comments": "Accepted WACV 2021", "journal-ref": "WACV 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One of the major limitations of deep learning models is that they face\ncatastrophic forgetting in an incremental learning scenario. There have been\nseveral approaches proposed to tackle the problem of incremental learning. Most\nof these methods are based on knowledge distillation and do not adequately\nutilize the information provided by older task models, such as uncertainty\nestimation in predictions. The predictive uncertainty provides the\ndistributional information can be applied to mitigate catastrophic forgetting\nin a deep learning framework. In the proposed work, we consider a Bayesian\nformulation to obtain the data and model uncertainties. We also incorporate\nself-attention framework to address the incremental learning problem. We define\ndistillation losses in terms of aleatoric uncertainty and self-attention. In\nthe proposed work, we investigate different ablation analyses on these losses.\nFurthermore, we are able to obtain better results in terms of accuracy on\nstandard benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 06:54:52 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Kurmi", "Vinod K", ""], ["Patro", "Badri N.", ""], ["Subramanian", "Venkatesh K.", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "2102.01916", "submitter": "Yibing Liu", "authors": "Yibing Liu, Yangyang Guo, Jianhua Yin, Xuemeng Song, Weifeng Liu,\n  Liqiang Nie", "title": "Answer Questions with Right Image Regions: A Visual Attention\n  Regularization Approach", "comments": "Submitted to ToMM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Visual attention in Visual Question Answering (VQA) targets at locating the\nright image regions regarding the answer prediction. However, recent studies\nhave pointed out that the highlighted image regions from the visual attention\nare often irrelevant to the given question and answer, leading to model\nconfusion for correct visual reasoning. To tackle this problem, existing\nmethods mostly resort to aligning the visual attention weights with human\nattentions. Nevertheless, gathering such human data is laborious and expensive,\nmaking it burdensome to adapt well-developed models across datasets. To address\nthis issue, in this paper, we devise a novel visual attention regularization\napproach, namely AttReg, for better visual grounding in VQA. Specifically,\nAttReg firstly identifies the image regions which are essential for question\nanswering yet unexpectedly ignored (i.e., assigned with low attention weights)\nby the backbone model. And then a mask-guided learning scheme is leveraged to\nregularize the visual attention to focus more on these ignored key regions. The\nproposed method is very flexible and model-agnostic, which can be integrated\ninto most visual attention-based VQA models and require no human attention\nsupervision. Extensive experiments over three benchmark datasets, i.e., VQA-CP\nv2, VQA-CP v1, and VQA v2, have been conducted to evaluate the effectiveness of\nAttReg. As a by-product, when incorporating AttReg into the strong baseline\nLMH, our approach can achieve a new state-of-the-art accuracy of 59.92% with an\nabsolute performance gain of 6.93% on the VQA-CP v2 benchmark dataset. In\naddition to the effectiveness validation, we recognize that the faithfulness of\nthe visual attention in VQA has not been well explored in literature. In the\nlight of this, we propose to empirically validate such property of visual\nattention and compare it with the prevalent gradient-based approaches.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 07:33:30 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Liu", "Yibing", ""], ["Guo", "Yangyang", ""], ["Yin", "Jianhua", ""], ["Song", "Xuemeng", ""], ["Liu", "Weifeng", ""], ["Nie", "Liqiang", ""]]}, {"id": "2102.01921", "submitter": "Wolfgang Fuhl", "authors": "Wolfgang Fuhl", "title": "1000 Pupil Segmentations in a Second using Haar Like Features and\n  Statistical Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new approach for pupil segmentation. It can be\ncomputed and trained very efficiently, making it ideal for online use for high\nspeed eye trackers as well as for energy saving pupil detection in mobile eye\ntracking. The approach is inspired by the BORE and CBF algorithms and\ngeneralizes the binary comparison by Haar features. Since these features are\nintrinsically very susceptible to noise and fluctuating light conditions, we\ncombine them with conditional pupil shape probabilities. In addition, we also\nrank each feature according to its importance in determining the pupil shape.\nAnother advantage of our method is the use of statistical learning, which is\nvery efficient and can even be used online.\nhttps://atreus.informatik.uni-tuebingen.de/seafile/d/8e2ab8c3fdd444e1a135/?p=%2FStatsPupil&mode=list\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 07:45:04 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Fuhl", "Wolfgang", ""]]}, {"id": "2102.01929", "submitter": "Dogyoon Lee", "authors": "Dogyoon Lee, Jaeha Lee, Junhyeop Lee, Hyeongmin Lee, Minhyeok Lee,\n  Sungmin Woo, and Sangyoun Lee", "title": "Regularization Strategy for Point Cloud via Rigidly Mixed Sample", "comments": "CVPR2021 Accepted, 10 pages, 5 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is an effective regularization strategy to alleviate the\noverfitting, which is an inherent drawback of the deep neural networks.\nHowever, data augmentation is rarely considered for point cloud processing\ndespite many studies proposing various augmentation methods for image data.\nActually, regularization is essential for point clouds since lack of generality\nis more likely to occur in point cloud due to small datasets. This paper\nproposes a Rigid Subset Mix (RSMix), a novel data augmentation method for point\nclouds that generates a virtual mixed sample by replacing part of the sample\nwith shape-preserved subsets from another sample. RSMix preserves structural\ninformation of the point cloud sample by extracting subsets from each sample\nwithout deformation using a neighboring function. The neighboring function was\ncarefully designed considering unique properties of point cloud, unordered\nstructure and non-grid. Experiments verified that RSMix successfully\nregularized the deep neural networks with remarkable improvement for shape\nclassification. We also analyzed various combinations of data augmentations\nincluding RSMix with single and multi-view evaluations, based on abundant\nablation studies.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 08:03:59 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 07:22:35 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 08:47:21 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Lee", "Dogyoon", ""], ["Lee", "Jaeha", ""], ["Lee", "Junhyeop", ""], ["Lee", "Hyeongmin", ""], ["Lee", "Minhyeok", ""], ["Woo", "Sungmin", ""], ["Lee", "Sangyoun", ""]]}, {"id": "2102.01940", "submitter": "Xiaogang Jia", "authors": "Xiaogang Jia, Wei Chen, Zhengfa Liang, Mingfei Wu, Yusong Tan, Libo\n  Huang", "title": "Multi-Scale Cost Volumes Cascade Network for Stereo Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stereo matching is essential for robot navigation. However, the accuracy of\ncurrent widely used traditional methods is low, while methods based on CNN need\nexpensive computational cost and running time. This is because different cost\nvolumes play a crucial role in balancing speed and accuracy. Thus we propose\nMSCVNet, which combines traditional methods and neural networks to improve the\nquality of cost volume. Concretely, our network first generates multiple 3D\ncost volumes with different resolutions and then uses 2D convolutions to\nconstruct a novel cascade hourglass network for cost aggregation. Meanwhile, we\ndesign an algorithm to distinguish and calculate the loss for discontinuous\nareas of disparity result. According to the KITTI official website, our network\nis much faster than most top-performing methods (24 times than CSPN, 44 times\nthan GANet, etc.). Meanwhile, compared to traditional methods (SPS-St, SGM) and\nother real-time stereo matching networks (Fast DS-CS, DispNetC, and RTSNet,\netc.), our network achieves a big improvement in accuracy, demonstrating the\nfeasibility and capability of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 08:40:17 GMT"}, {"version": "v2", "created": "Sat, 13 Mar 2021 05:18:44 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Jia", "Xiaogang", ""], ["Chen", "Wei", ""], ["Liang", "Zhengfa", ""], ["Wu", "Mingfei", ""], ["Tan", "Yusong", ""], ["Huang", "Libo", ""]]}, {"id": "2102.01955", "submitter": "Zhaoyang Pang", "authors": "Zhaoyang Pang, Callum Biggs O'May, Bhavin Choksi, Rufin VanRullen", "title": "Predictive coding feedback results in perceived illusory contours in a\n  recurrent neural network", "comments": "Manuscript under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern feedforward convolutional neural networks (CNNs) can now solve some\ncomputer vision tasks at super-human levels. However, these networks only\nroughly mimic human visual perception. One difference from human vision is that\nthey do not appear to perceive illusory contours (e.g. Kanizsa squares) in the\nsame way humans do. Physiological evidence from visual cortex suggests that the\nperception of illusory contours could involve feedback connections. Would\nrecurrent feedback neural networks perceive illusory contours like humans? In\nthis work we equip a deep feedforward convolutional network with brain-inspired\nrecurrent dynamics. The network was first pretrained with an unsupervised\nreconstruction objective on a natural image dataset, to expose it to natural\nobject contour statistics. Then, a classification decision layer was added and\nthe model was finetuned on a form discrimination task: squares vs. randomly\noriented inducer shapes (no illusory contour). Finally, the model was tested\nwith the unfamiliar ''illusory contour'' configuration: inducer shapes oriented\nto form an illusory square. Compared with feedforward baselines, the iterative\n''predictive coding'' feedback resulted in more illusory contours being\nclassified as physical squares. The perception of the illusory contour was\nmeasurable in the luminance profile of the image reconstructions produced by\nthe model, demonstrating that the model really ''sees'' the illusion. Ablation\nstudies revealed that natural image pretraining and feedback error correction\nare both critical to the perception of the illusion. Finally we validated our\nconclusions in a deeper network (VGG): adding the same predictive coding\nfeedback dynamics again leads to the perception of illusory contours.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 09:07:09 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 14:43:15 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Pang", "Zhaoyang", ""], ["O'May", "Callum Biggs", ""], ["Choksi", "Bhavin", ""], ["VanRullen", "Rufin", ""]]}, {"id": "2102.01987", "submitter": "Muhammad Ferjad Naeem", "authors": "Muhammad Ferjad Naeem, Yongqin Xian, Federico Tombari, Zeynep Akata", "title": "Learning Graph Embeddings for Compositional Zero-shot Learning", "comments": "Accepted in IEEE CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In compositional zero-shot learning, the goal is to recognize unseen\ncompositions (e.g. old dog) of observed visual primitives states (e.g. old,\ncute) and objects (e.g. car, dog) in the training set. This is challenging\nbecause the same state can for example alter the visual appearance of a dog\ndrastically differently from a car. As a solution, we propose a novel graph\nformulation called Compositional Graph Embedding (CGE) that learns image\nfeatures, compositional classifiers, and latent representations of visual\nprimitives in an end-to-end manner. The key to our approach is exploiting the\ndependency between states, objects, and their compositions within a graph\nstructure to enforce the relevant knowledge transfer from seen to unseen\ncompositions. By learning a joint compatibility that encodes semantics between\nconcepts, our model allows for generalization to unseen compositions without\nrelying on an external knowledge base like WordNet. We show that in the\nchallenging generalized compositional zero-shot setting our CGE significantly\noutperforms the state of the art on MIT-States and UT-Zappos. We also propose a\nnew benchmark for this task based on the recent GQA dataset. Code is available\nat: https://github.com/ExplainableML/czsl\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 10:11:03 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 09:15:22 GMT"}, {"version": "v3", "created": "Mon, 3 May 2021 19:12:00 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Naeem", "Muhammad Ferjad", ""], ["Xian", "Yongqin", ""], ["Tombari", "Federico", ""], ["Akata", "Zeynep", ""]]}, {"id": "2102.01990", "submitter": "Yu Deng", "authors": "Yu Deng, Ling Wang, Chen Zhao, Shaojie Tang, Xiaoguang Cheng, Hong-Wen\n  Deng, Weihua Zhou", "title": "A Deep Learning-Based Approach to Extracting Periosteal and Endosteal\n  Contours of Proximal Femur in Quantitative CT Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic CT segmentation of proximal femur is crucial for the diagnosis and\nrisk stratification of orthopedic diseases; however, current methods for the\nfemur CT segmentation mainly rely on manual interactive segmentation, which is\ntime-consuming and has limitations in both accuracy and reproducibility. In\nthis study, we proposed an approach based on deep learning for the automatic\nextraction of the periosteal and endosteal contours of proximal femur in order\nto differentiate cortical and trabecular bone compartments. A three-dimensional\n(3D) end-to-end fully convolutional neural network, which can better combine\nthe information between neighbor slices and get more accurate segmentation\nresults, was developed for our segmentation task. 100 subjects aged from 50 to\n87 years with 24,399 slices of proximal femur CT images were enrolled in this\nstudy. The separation of cortical and trabecular bone derived from the QCT\nsoftware MIAF-Femur was used as the segmentation reference. We randomly divided\nthe whole dataset into a training set with 85 subjects for 10-fold\ncross-validation and a test set with 15 subjects for evaluating the performance\nof models. Two models with the same network structures were trained and they\nachieved a dice similarity coefficient (DSC) of 97.87% and 96.49% for the\nperiosteal and endosteal contours, respectively. To verify the excellent\nperformance of our model for femoral segmentation, we measured the volume of\ndifferent parts of the femur and compared it with the ground truth and the\nrelative errors between predicted result and ground truth are all less than 5%.\nIt demonstrated a strong potential for clinical use, including the hip fracture\nrisk prediction and finite element analysis.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 10:23:41 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2021 12:56:03 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Deng", "Yu", ""], ["Wang", "Ling", ""], ["Zhao", "Chen", ""], ["Tang", "Shaojie", ""], ["Cheng", "Xiaoguang", ""], ["Deng", "Hong-Wen", ""], ["Zhou", "Weihua", ""]]}, {"id": "2102.01998", "submitter": "Guang Yang A", "authors": "Guang Yang, Qinghao Ye, Jun Xia", "title": "Unbox the Black-box for the Medical Explainable AI via Multi-modal and\n  Multi-centre Data Fusion: A Mini-Review, Two Showcases and Beyond", "comments": "68 pages, 19 figures, submitted to the Information Fusion journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainable Artificial Intelligence (XAI) is an emerging research topic of\nmachine learning aimed at unboxing how AI systems' black-box choices are made.\nThis research field inspects the measures and models involved in\ndecision-making and seeks solutions to explain them explicitly. Many of the\nmachine learning algorithms can not manifest how and why a decision has been\ncast. This is particularly true of the most popular deep neural network\napproaches currently in use. Consequently, our confidence in AI systems can be\nhindered by the lack of explainability in these black-box models. The XAI\nbecomes more and more crucial for deep learning powered applications,\nespecially for medical and healthcare studies, although in general these deep\nneural networks can return an arresting dividend in performance. The\ninsufficient explainability and transparency in most existing AI systems can be\none of the major reasons that successful implementation and integration of AI\ntools into routine clinical practice are uncommon. In this study, we first\nsurveyed the current progress of XAI and in particular its advances in\nhealthcare applications. We then introduced our solutions for XAI leveraging\nmulti-modal and multi-centre data fusion, and subsequently validated in two\nshowcases following real clinical scenarios. Comprehensive quantitative and\nqualitative analyses can prove the efficacy of our proposed XAI solutions, from\nwhich we can envisage successful applications in a broader range of clinical\nquestions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 10:56:58 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Yang", "Guang", ""], ["Ye", "Qinghao", ""], ["Xia", "Jun", ""]]}, {"id": "2102.02000", "submitter": "David Sinclair D.Phil Oxon", "authors": "Dr David Sinclair and Dr Christopher Town", "title": "A generalised feature for low level vision", "comments": "12 pages 8 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This papers presents a novel quantised transform (the Sinclair-Town or ST\ntransform for short) that subsumes the rolls of both edge-detector, MSER style\nregion detector and corner detector. The transform is similar to the $unsharp$\ntransform but the difference from the local mean is quantised to 3 values\n(dark-neutral-light). The transform naturally leads to the definition of an\nappropriate local scale. A range of methods for extracting shape features form\nthe transformed image are presented. The generalized feature provides a robust\nbasis for establishing correspondence between images. The transform readily\nadmits more complicated kernel behaviour including multi-scale and asymmetric\nelements to prefer shorter scale or oriented local features.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 11:02:03 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Sinclair", "Dr David", ""], ["Town", "Dr Christopher", ""]]}, {"id": "2102.02005", "submitter": "Lorenzo Berlincioni", "authors": "My Kieu, Lorenzo Berlincioni, Leonardo Galteri, Marco Bertini, Andrew\n  D. Bagdanov, Alberto Del Bimbo", "title": "Robust pedestrian detection in thermal imagery using synthesized images", "comments": "Accepted at ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper we propose a method for improving pedestrian detection in the\nthermal domain using two stages: first, a generative data augmentation approach\nis used, then a domain adaptation method using generated data adapts an RGB\npedestrian detector. Our model, based on the Least-Squares Generative\nAdversarial Network, is trained to synthesize realistic thermal versions of\ninput RGB images which are then used to augment the limited amount of labeled\nthermal pedestrian images available for training. We apply our generative data\naugmentation strategy in order to adapt a pretrained YOLOv3 pedestrian detector\nto detection in the thermal-only domain. Experimental results demonstrate the\neffectiveness of our approach: using less than 50\\% of available real thermal\ntraining data, and relying on synthesized data generated by our model in the\ndomain adaptation phase, our detector achieves state-of-the-art results on the\nKAIST Multispectral Pedestrian Detection Benchmark; even if more real thermal\ndata is available adding GAN generated images to the training data results in\nimproved performance, thus showing that these images act as an effective form\nof data augmentation. To the best of our knowledge, our detector achieves the\nbest single-modality detection results on KAIST with respect to the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 11:08:31 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Kieu", "My", ""], ["Berlincioni", "Lorenzo", ""], ["Galteri", "Leonardo", ""], ["Bertini", "Marco", ""], ["Bagdanov", "Andrew D.", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "2102.02028", "submitter": "Francesc Llu\\'is", "authors": "Francesc Llu\\'is, Vasileios Chatziioannou, Alex Hofmann", "title": "Music source separation conditioned on 3D point clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, significant progress has been made in audio source separation by\nthe application of deep learning techniques. Current methods that combine both\naudio and visual information use 2D representations such as images to guide the\nseparation process. However, in order to (re)-create acoustically correct\nscenes for 3D virtual/augmented reality applications from recordings of real\nmusic ensembles, detailed information about each sound source in the 3D\nenvironment is required. This demand, together with the proliferation of 3D\nvisual acquisition systems like LiDAR or rgb-depth cameras, stimulates the\ncreation of models that can guide the audio separation using 3D visual\ninformation. This paper proposes a multi-modal deep learning model to perform\nmusic source separation conditioned on 3D point clouds of music performance\nrecordings. This model extracts visual features using 3D sparse convolutions,\nwhile audio features are extracted using dense convolutions. A fusion module\ncombines the extracted features to finally perform the audio source separation.\nIt is shown, that the presented model can distinguish the musical instruments\nfrom a single 3D point cloud frame, and perform source separation qualitatively\nsimilar to a reference case, where manually assigned instrument labels are\nprovided.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 12:18:35 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Llu\u00eds", "Francesc", ""], ["Chatziioannou", "Vasileios", ""], ["Hofmann", "Alex", ""]]}, {"id": "2102.02033", "submitter": "Yuhang Ding", "authors": "Yuhang Ding, Xin Yu, Yi Yang", "title": "Modeling the Probabilistic Distribution of Unlabeled Data forOne-shot\n  Medical Image Segmentation", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing image segmentation networks mainly leverage large-scale labeled\ndatasets to attain high accuracy. However, labeling medical images is very\nexpensive since it requires sophisticated expert knowledge. Thus, it is more\ndesirable to employ only a few labeled data in pursuing high segmentation\nperformance. In this paper, we develop a data augmentation method for one-shot\nbrain magnetic resonance imaging (MRI) image segmentation which exploits only\none labeled MRI image (named atlas) and a few unlabeled images. In particular,\nwe propose to learn the probability distributions of deformations (including\nshapes and intensities) of different unlabeled MRI images with respect to the\natlas via 3D variational autoencoders (VAEs). In this manner, our method is\nable to exploit the learned distributions of image deformations to generate new\nauthentic brain MRI images, and the number of generated samples will be\nsufficient to train a deep segmentation network. Furthermore, we introduce a\nnew standard segmentation benchmark to evaluate the generalization performance\nof a segmentation network through a cross-dataset setting (collected from\ndifferent sources). Extensive experiments demonstrate that our method\noutperforms the state-of-the-art one-shot medical segmentation methods. Our\ncode has been released at\nhttps://github.com/dyh127/Modeling-the-Probabilistic-Distribution-of-Unlabeled-Data.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 12:28:04 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Ding", "Yuhang", ""], ["Yu", "Xin", ""], ["Yang", "Yi", ""]]}, {"id": "2102.02038", "submitter": "Lu Liu", "authors": "Lu Liu, Tianyi Zhou, Guodong Long, Jing Jiang, Xuanyi Dong, Chengqi\n  Zhang", "title": "Isometric Propagation Network for Generalized Zero-shot Learning", "comments": "Accepted by ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) aims to classify images of an unseen class only\nbased on a few attributes describing that class but no access to any training\nsample. A popular strategy is to learn a mapping between the semantic space of\nclass attributes and the visual space of images based on the seen classes and\ntheir data. Thus, an unseen class image can be ideally mapped to its\ncorresponding class attributes. The key challenge is how to align the\nrepresentations in the two spaces. For most ZSL settings, the attributes for\neach seen/unseen class are only represented by a vector while the seen-class\ndata provide much more information. Thus, the imbalanced supervision from the\nsemantic and the visual space can make the learned mapping easily overfitting\nto the seen classes. To resolve this problem, we propose Isometric Propagation\nNetwork (IPN), which learns to strengthen the relation between classes within\neach space and align the class dependency in the two spaces. Specifically, IPN\nlearns to propagate the class representations on an auto-generated graph within\neach space. In contrast to only aligning the resulted static representation, we\nregularize the two dynamic propagation procedures to be isometric in terms of\nthe two graphs' edge weights per step by minimizing a consistency loss between\nthem. IPN achieves state-of-the-art performance on three popular ZSL\nbenchmarks. To evaluate the generalization capability of IPN, we further build\ntwo larger benchmarks with more diverse unseen classes and demonstrate the\nadvantages of IPN on them.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 12:45:38 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Liu", "Lu", ""], ["Zhou", "Tianyi", ""], ["Long", "Guodong", ""], ["Jiang", "Jing", ""], ["Dong", "Xuanyi", ""], ["Zhang", "Chengqi", ""]]}, {"id": "2102.02051", "submitter": "Zongbo Han", "authors": "Zongbo Han, Changqing Zhang, Huazhu Fu, Joey Tianyi Zhou", "title": "Trusted Multi-View Classification", "comments": "Accepted by ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-view classification (MVC) generally focuses on improving classification\naccuracy by using information from different views, typically integrating them\ninto a unified comprehensive representation for downstream tasks. However, it\nis also crucial to dynamically assess the quality of a view for different\nsamples in order to provide reliable uncertainty estimations, which indicate\nwhether predictions can be trusted. To this end, we propose a novel multi-view\nclassification method, termed trusted multi-view classification, which provides\na new paradigm for multi-view learning by dynamically integrating different\nviews at an evidence level. The algorithm jointly utilizes multiple views to\npromote both classification reliability and robustness by integrating evidence\nfrom each view. To achieve this, the Dirichlet distribution is used to model\nthe distribution of the class probabilities, parameterized with evidence from\ndifferent views and integrated with the Dempster-Shafer theory. The unified\nlearning framework induces accurate uncertainty and accordingly endows the\nmodel with both reliability and robustness for out-of-distribution samples.\nExtensive experimental results validate the effectiveness of the proposed model\nin accuracy, reliability and robustness.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 13:30:26 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Han", "Zongbo", ""], ["Zhang", "Changqing", ""], ["Fu", "Huazhu", ""], ["Zhou", "Joey Tianyi", ""]]}, {"id": "2102.02115", "submitter": "Wolfgang Fuhl", "authors": "Wolfgang Fuhl and Gjergji Kasneci and Enkelejda Kasneci", "title": "TEyeD: Over 20 million real-world eye images with Pupil, Eyelid, and\n  Iris 2D and 3D Segmentations, 2D and 3D Landmarks, 3D Eyeball, Gaze Vector,\n  and Eye Movement Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present TEyeD, the world's largest unified public data set of eye images\ntaken with head-mounted devices. TEyeD was acquired with seven different\nhead-mounted eye trackers. Among them, two eye trackers were integrated into\nvirtual reality (VR) or augmented reality (AR) devices. The images in TEyeD\nwere obtained from various tasks, including car rides, simulator rides, outdoor\nsports activities, and daily indoor activities. The data set includes 2D\\&3D\nlandmarks, semantic segmentation, 3D eyeball annotation and the gaze vector and\neye movement types for all images. Landmarks and semantic segmentation are\nprovided for the pupil, iris and eyelids. Video lengths vary from a few minutes\nto several hours. With more than 20 million carefully annotated images, TEyeD\nprovides a unique, coherent resource and a valuable foundation for advancing\nresearch in the field of computer vision, eye tracking and gaze estimation in\nmodern VR and AR applications. Data and code at\nhttps://unitc-my.sharepoint.com/:f:/g/personal/iitfu01_cloud_uni-tuebingen_de/EvrNPdtigFVHtCMeFKSyLlUBepOcbX0nEkamweeZa0s9SQ?e=fWEvPp\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 15:48:22 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Fuhl", "Wolfgang", ""], ["Kasneci", "Gjergji", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "2102.02147", "submitter": "Rishabh Goyal", "authors": "Rishabh Goyal, Joaquin Vanschoren, Victor van Acht, Stephan Nijssen", "title": "Fixed-point Quantization of Convolutional Neural Networks for Quantized\n  Inference on Embedded Platforms", "comments": "39 Pages, 40 Figures, Appendix with Supplementary Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) have proven to be a powerful\nstate-of-the-art method for image classification tasks. One drawback however is\nthe high computational complexity and high memory consumption of CNNs which\nmakes them unfeasible for execution on embedded platforms which are constrained\non physical resources needed to support CNNs. Quantization has often been used\nto efficiently optimize CNNs for memory and computational complexity at the\ncost of a loss of prediction accuracy. We therefore propose a method to\noptimally quantize the weights, biases and activations of each layer of a\npre-trained CNN while controlling the loss in inference accuracy to enable\nquantized inference. We quantize the 32-bit floating-point precision parameters\nto low bitwidth fixed-point representations thereby finding optimal bitwidths\nand fractional offsets for parameters of each layer of a given CNN. We quantize\nparameters of a CNN post-training without re-training it. Our method is\ndesigned to quantize parameters of a CNN taking into account how other\nparameters are quantized because ignoring quantization errors due to other\nquantized parameters leads to a low precision CNN with accuracy losses of up to\n50% which is far beyond what is acceptable. Our final method therefore gives a\nlow precision CNN with accuracy losses of less than 1%. As compared to a method\nused by commercial tools that quantize all parameters to 8-bits, our approach\nprovides quantized CNN with averages of 53% lower memory consumption and 77.5%\nlower cost of executing multiplications for the two CNNs trained on the four\ndatasets that we tested our work on. We find that layer-wise quantization of\nparameters significantly helps in this process.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 17:05:55 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Goyal", "Rishabh", ""], ["Vanschoren", "Joaquin", ""], ["van Acht", "Victor", ""], ["Nijssen", "Stephan", ""]]}, {"id": "2102.02153", "submitter": "Viviane Clay", "authors": "Viviane Clay, Peter K\\\"onig, Gordon Pipa, Kai-Uwe K\\\"uhnberger", "title": "Fast Concept Mapping: The Emergence of Human Abilities in Artificial\n  Neural Networks when Learning Embodied and Self-Supervised", "comments": "10 pages. Find associated code and data here:\n  https://github.com/vkakerbeck/FastConceptMapping", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most artificial neural networks used for object detection and recognition are\ntrained in a fully supervised setup. This is not only very resource consuming\nas it requires large data sets of labeled examples but also very different from\nhow humans learn. We introduce a setup in which an artificial agent first\nlearns in a simulated world through self-supervised exploration. Following\nthis, the representations learned through interaction with the world can be\nused to associate semantic concepts such as different types of doors. To do\nthis, we use a method we call fast concept mapping which uses correlated firing\npatterns of neurons to define and detect semantic concepts. This association\nworks instantaneous with very few labeled examples, similar to what we observe\nin humans in a phenomenon called fast mapping. Strikingly, this method already\nidentifies objects with as little as one labeled example which highlights the\nquality of the encoding learned self-supervised through embodiment using\ncuriosity-driven exploration. It therefor presents a feasible strategy for\nlearning concepts without much supervision and shows that through pure\ninteraction with the world meaningful representations of an environment can be\nlearned.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 17:19:49 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Clay", "Viviane", ""], ["K\u00f6nig", "Peter", ""], ["Pipa", "Gordon", ""], ["K\u00fchnberger", "Kai-Uwe", ""]]}, {"id": "2102.02267", "submitter": "Mohamed Chaabane", "authors": "Mohamed Chaabane, Peter Zhang, J. Ross Beveridge and Stephen O'Hara", "title": "DEFT: Detection Embeddings for Tracking", "comments": "Accepted at CVPR 2021, ADP3 Workshop on Autonomous Driving:\n  Perception, Prediction and Planning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most modern multiple object tracking (MOT) systems follow the\ntracking-by-detection paradigm, consisting of a detector followed by a method\nfor associating detections into tracks. There is a long history in tracking of\ncombining motion and appearance features to provide robustness to occlusions\nand other challenges, but typically this comes with the trade-off of a more\ncomplex and slower implementation. Recent successes on popular 2D tracking\nbenchmarks indicate that top-scores can be achieved using a state-of-the-art\ndetector and relatively simple associations relying on single-frame spatial\noffsets -- notably outperforming contemporary methods that leverage learned\nappearance features to help re-identify lost tracks. In this paper, we propose\nan efficient joint detection and tracking model named DEFT, or \"Detection\nEmbeddings for Tracking.\" Our approach relies on an appearance-based object\nmatching network jointly-learned with an underlying object detection network.\nAn LSTM is also added to capture motion constraints. DEFT has comparable\naccuracy and speed to the top methods on 2D online tracking leaderboards while\nhaving significant advantages in robustness when applied to more challenging\ntracking data. DEFT raises the bar on the nuScenes monocular 3D tracking\nchallenge, more than doubling the performance of the previous top method. Code\nis publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 20:00:44 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 22:25:52 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Chaabane", "Mohamed", ""], ["Zhang", "Peter", ""], ["Beveridge", "J. Ross", ""], ["O'Hara", "Stephen", ""]]}, {"id": "2102.02287", "submitter": "Fatemeh Taheri Dezaki", "authors": "Fatemeh Taheri Dezaki, Christina Luong, Tom Ginsberg, Robert Rohling,\n  Ken Gin, Purang Abolmaesumi, Teresa Tsang", "title": "Echo-SyncNet: Self-supervised Cardiac View Synchronization in\n  Echocardiography", "comments": "13 pages, 15 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In echocardiography (echo), an electrocardiogram (ECG) is conventionally used\nto temporally align different cardiac views for assessing critical\nmeasurements. However, in emergencies or point-of-care situations, acquiring an\nECG is often not an option, hence motivating the need for alternative temporal\nsynchronization methods. Here, we propose Echo-SyncNet, a self-supervised\nlearning framework to synchronize various cross-sectional 2D echo series\nwithout any external input. The proposed framework takes advantage of both\nintra-view and inter-view self supervisions. The former relies on\nspatiotemporal patterns found between the frames of a single echo cine and the\nlatter on the interdependencies between multiple cines. The combined\nsupervisions are used to learn a feature-rich embedding space where multiple\necho cines can be temporally synchronized. We evaluate the framework with\nmultiple experiments: 1) Using data from 998 patients, Echo-SyncNet shows\npromising results for synchronizing Apical 2 chamber and Apical 4 chamber\ncardiac views; 2) Using data from 3070 patients, our experiments reveal that\nthe learned representations of Echo-SyncNet outperform a supervised deep\nlearning method that is optimized for automatic detection of fine-grained\ncardiac phase; 3) We show the usefulness of the learned representations in a\none-shot learning scenario of cardiac keyframe detection. Without any\nfine-tuning, keyframes in 1188 validation patient studies are identified by\nsynchronizing them with only one labeled reference study. We do not make any\nprior assumption about what specific cardiac views are used for training and\nshow that Echo-SyncNet can accurately generalize to views not present in its\ntraining set. Project repository: github.com/fatemehtd/Echo-SyncNet.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 20:48:16 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Dezaki", "Fatemeh Taheri", ""], ["Luong", "Christina", ""], ["Ginsberg", "Tom", ""], ["Rohling", "Robert", ""], ["Gin", "Ken", ""], ["Abolmaesumi", "Purang", ""], ["Tsang", "Teresa", ""]]}, {"id": "2102.02301", "submitter": "J\\'er\\'emy Anger", "authors": "J\\'er\\'emy Anger, Thibaud Ehret, Gabriele Facciolo", "title": "Parallax estimation for push-frame satellite imagery: application to\n  super-resolution and 3D surface modeling from Skysat products", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent constellations of satellites, including the Skysat constellation, are\nable to acquire bursts of images. This new acquisition mode allows for modern\nimage restoration techniques, including multi-frame super-resolution. As the\nsatellite moves during the acquisition of the burst, elevation changes in the\nscene translate into noticeable parallax. This parallax hinders the results of\nthe restoration. To cope with this issue, we propose a novel parallax\nestimation method. The method is composed of a linear Plane+Parallax\ndecomposition of the apparent motion and a multi-frame optical flow algorithm\nthat exploits all frames simultaneously. Using SkySat L1A images, we show that\nthe estimated per-pixel displacements are important for applying multi-frame\nsuper-resolution on scenes containing elevation changes and that can also be\nused to estimate a coarse 3D surface model.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 21:24:22 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Anger", "J\u00e9r\u00e9my", ""], ["Ehret", "Thibaud", ""], ["Facciolo", "Gabriele", ""]]}, {"id": "2102.02320", "submitter": "Zaid Khan", "authors": "Zaid Khan and Yun Fu", "title": "One Label, One Billion Faces: Usage and Consistency of Racial Categories\n  in Computer Vision", "comments": "Published as a conference paper at the 4th ACM Conference on\n  Fairness, Accountability, and Transparency (FAccT 2021)", "journal-ref": null, "doi": "10.1145/3442188.3445920", "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision is widely deployed, has highly visible, society altering\napplications, and documented problems with bias and representation. Datasets\nare critical for benchmarking progress in fair computer vision, and often\nemploy broad racial categories as population groups for measuring group\nfairness. Similarly, diversity is often measured in computer vision datasets by\nascribing and counting categorical race labels. However, racial categories are\nill-defined, unstable temporally and geographically, and have a problematic\nhistory of scientific use. Although the racial categories used across datasets\nare superficially similar, the complexity of human race perception suggests the\nracial system encoded by one dataset may be substantially inconsistent with\nanother. Using the insight that a classifier can learn the racial system\nencoded by a dataset, we conduct an empirical study of computer vision datasets\nsupplying categorical race labels for face images to determine the\ncross-dataset consistency and generalization of racial categories. We find that\neach dataset encodes a substantially unique racial system, despite nominally\nequivalent racial categories, and some racial categories are systemically less\nconsistent than others across datasets. We find evidence that racial categories\nencode stereotypes, and exclude ethnic groups from categories on the basis of\nnonconformity to stereotypes. Representing a billion humans under one racial\ncategory may obscure disparities and create new ones by encoding stereotypes of\nracial systems. The difficulty of adequately converting the abstract concept of\nrace into a tool for measuring fairness underscores the need for a method more\nflexible and culturally aware than racial categories.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 22:50:04 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Khan", "Zaid", ""], ["Fu", "Yun", ""]]}, {"id": "2102.02337", "submitter": "Santhosh Kumar Ramakrishnan", "authors": "Santhosh K. Ramakrishnan, Tushar Nagarajan, Ziad Al-Halah, Kristen\n  Grauman", "title": "Environment Predictive Coding for Embodied Agents", "comments": "9 pages, 6 figures, appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce environment predictive coding, a self-supervised approach to\nlearn environment-level representations for embodied agents. In contrast to\nprior work on self-supervised learning for images, we aim to jointly encode a\nseries of images gathered by an agent as it moves about in 3D environments. We\nlearn these representations via a zone prediction task, where we intelligently\nmask out portions of an agent's trajectory and predict them from the unmasked\nportions, conditioned on the agent's camera poses. By learning such\nrepresentations on a collection of videos, we demonstrate successful transfer\nto multiple downstream navigation-oriented tasks. Our experiments on the\nphotorealistic 3D environments of Gibson and Matterport3D show that our method\noutperforms the state-of-the-art on challenging tasks with only a limited\nbudget of experience.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 23:43:16 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Ramakrishnan", "Santhosh K.", ""], ["Nagarajan", "Tushar", ""], ["Al-Halah", "Ziad", ""], ["Grauman", "Kristen", ""]]}, {"id": "2102.02371", "submitter": "Jiangke Lin", "authors": "Jiangke Lin, Yi Yuan, Zhengxia Zou", "title": "MeInGame: Create a Game Character Face from a Single Portrait", "comments": "Accepted to AAAI 2021. Code is now available at\n  https://github.com/FuxiCV/MeInGame", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many deep learning based 3D face reconstruction methods have been proposed\nrecently, however, few of them have applications in games. Current game\ncharacter customization systems either require players to manually adjust\nconsiderable face attributes to obtain the desired face, or have limited\nfreedom of facial shape and texture. In this paper, we propose an automatic\ncharacter face creation method that predicts both facial shape and texture from\na single portrait, and it can be integrated into most existing 3D games.\nAlthough 3D Morphable Face Model (3DMM) based methods can restore accurate 3D\nfaces from single images, the topology of 3DMM mesh is different from the\nmeshes used in most games. To acquire fidelity texture, existing methods\nrequire a large amount of face texture data for training, while building such\ndatasets is time-consuming and laborious. Besides, such a dataset collected\nunder laboratory conditions may not generalized well to in-the-wild situations.\nTo tackle these problems, we propose 1) a low-cost facial texture acquisition\nmethod, 2) a shape transfer algorithm that can transform the shape of a 3DMM\nmesh to games, and 3) a new pipeline for training 3D game face reconstruction\nnetworks. The proposed method not only can produce detailed and vivid game\ncharacters similar to the input portrait, but can also eliminate the influence\nof lighting and occlusions. Experiments show that our method outperforms\nstate-of-the-art methods used in games.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 02:12:19 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2021 03:27:07 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Lin", "Jiangke", ""], ["Yuan", "Yi", ""], ["Zou", "Zhengxia", ""]]}, {"id": "2102.02458", "submitter": "Christian Rathgeb", "authors": "Christian Rathgeb, Johannes Merkle, Johanna Scholz, Benjamin Tams,\n  Vanessa Nesterowicz", "title": "Deep Face Fuzzy Vault: Implementation and Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have achieved remarkable improvements in\nfacial recognition performance. Similar kinds of developments, e.g.\ndeconvolutional neural networks, have shown impressive results for\nreconstructing face images from their corresponding embeddings in the latent\nspace. This poses a severe security risk which necessitates the protection of\nstored deep face embeddings in order to prevent from misuse, e.g. identity\nfraud.\n  In this work, an unlinkable improved deep face fuzzy vault-based template\nprotection scheme is presented. To this end, a feature transformation method is\nintroduced which maps fixed-length real-valued deep face embeddings to\ninteger-valued feature sets. As part of said feature transformation, a detailed\nanalysis of different feature quantisation and binarisation techniques is\nconducted using features extracted with a state-of-the-art deep convolutional\nneural network trained with the additive angular margin loss (ArcFace). At key\nbinding, obtained feature sets are locked in an unlinkable improved fuzzy\nvault. For key retrieval, the efficiency of different polynomial reconstruction\ntechniques is investigated. The proposed feature transformation method and\ntemplate protection scheme are agnostic of the biometric characteristic and,\nthus, can be applied to virtually any biometric features computed by a deep\nneural network.\n  For the best configuration, a false non-match rate below 1% at a false match\nrate of 0.01%, is achieved in cross-database experiments on the FERET and\nFRGCv2 face databases. On average, a security level of up to approximately 28\nbits is obtained. This work presents the first effective face-based fuzzy vault\nscheme providing privacy protection of facial reference data as well as digital\nkey derivation from face.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 07:37:23 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Rathgeb", "Christian", ""], ["Merkle", "Johannes", ""], ["Scholz", "Johanna", ""], ["Tams", "Benjamin", ""], ["Nesterowicz", "Vanessa", ""]]}, {"id": "2102.02485", "submitter": "Shady Abu Hussein", "authors": "Shady Abu-Hussein, Tom Tirer, Se Young Chun, Yonina C. Eldar, and Raja\n  Giryes", "title": "Image Restoration by Deep Projected GSURE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ill-posed inverse problems appear in many image processing applications, such\nas deblurring and super-resolution. In recent years, solutions that are based\non deep Convolutional Neural Networks (CNNs) have shown great promise. Yet,\nmost of these techniques, which train CNNs using external data, are restricted\nto the observation models that have been used in the training phase. A recent\nalternative that does not have this drawback relies on learning the target\nimage using internal learning. One such prominent example is the Deep Image\nPrior (DIP) technique that trains a network directly on the input image with a\nleast-squares loss. In this paper, we propose a new image restoration framework\nthat is based on minimizing a loss function that includes a \"projected-version\"\nof the Generalized SteinUnbiased Risk Estimator (GSURE) and parameterization of\nthe latent image by a CNN. We demonstrate two ways to use our framework. In the\nfirst one, where no explicit prior is used, we show that the proposed approach\noutperforms other internal learning methods, such as DIP. In the second one, we\nshow that our GSURE-based loss leads to improved performance when used within a\nplug-and-play priors scheme.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 08:52:46 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Abu-Hussein", "Shady", ""], ["Tirer", "Tom", ""], ["Chun", "Se Young", ""], ["Eldar", "Yonina C.", ""], ["Giryes", "Raja", ""]]}, {"id": "2102.02502", "submitter": "Sebastian Bullinger", "authors": "Sebastian Bullinger, Christoph Bodensteiner, Michael Arens", "title": "3D Surface Reconstruction From Multi-Date Satellite Images", "comments": "Accepted at ISPRS Congress 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reconstruction of accurate three-dimensional environment models is one of\nthe most fundamental goals in the field of photogrammetry. Since satellite\nimages provide suitable properties for obtaining large-scale environment\nreconstructions, there exist a variety of Stereo Matching based methods to\nreconstruct point clouds for satellite image pairs. Recently, the first\nStructure from Motion (SfM) based approach has been proposed, which allows to\nreconstruct point clouds from multiple satellite images. In this work, we\npropose an extension of this SfM based pipeline that allows us to reconstruct\nnot only point clouds but watertight meshes including texture information. We\nprovide a detailed description of several steps that are mandatory to exploit\nstate-of-the-art mesh reconstruction algorithms in the context of satellite\nimagery. This includes a decomposition of finite projective camera calibration\nmatrices, a skew correction of corresponding depth maps and input images as\nwell as the recovery of real-world depth maps from reparameterized depth\nvalues. The paper presents an extensive quantitative evaluation on multi-date\nsatellite images demonstrating that the proposed pipeline combined with current\nmeshing algorithms outperforms state-of-the-art point cloud reconstruction\nalgorithms in terms of completeness and median error. We make the source code\nof our pipeline publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 09:23:21 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 12:50:05 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Bullinger", "Sebastian", ""], ["Bodensteiner", "Christoph", ""], ["Arens", "Michael", ""]]}, {"id": "2102.02531", "submitter": "Li Rui", "authors": "Rui Li, Chenxi Duan", "title": "ABCNet: Attentive Bilateral Contextual Network for Efficient Semantic\n  Segmentation of Fine-Resolution Remote Sensing Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation of remotely sensed images plays a crucial role in\nprecision agriculture, environmental protection, and economic assessment. In\nrecent years, substantial fine-resolution remote sensing images are available\nfor semantic segmentation. However, due to the complicated information caused\nby the increased spatial resolution, state-of-the-art deep learning algorithms\nnormally utilize complex network architectures for segmentation, which usually\nincurs high computational complexity. Specifically, the high-caliber\nperformance of the convolutional neural network (CNN) heavily relies on\nfine-grained spatial details (fine resolution) and sufficient contextual\ninformation (large receptive fields), both of which trigger high computational\ncosts. This crucially impedes their practicability and availability in\nreal-world scenarios that require real-time processing. In this paper, we\npropose an Attentive Bilateral Contextual Network (ABCNet), a convolutional\nneural network (CNN) with double branches, with prominently lower computational\nconsumptions compared to the cutting-edge algorithms, while maintaining a\ncompetitive accuracy. Code is available at https://github.com/lironui/ABCNet.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 10:43:08 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Li", "Rui", ""], ["Duan", "Chenxi", ""]]}, {"id": "2102.02547", "submitter": "Hai Pham", "authors": "Hai X. Pham and Ricardo Guerrero and Jiatong Li and Vladimir Pavlovic", "title": "CHEF: Cross-modal Hierarchical Embeddings for Food Domain Retrieval", "comments": "22 pages, accepted in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Despite the abundance of multi-modal data, such as image-text pairs, there\nhas been little effort in understanding the individual entities and their\ndifferent roles in the construction of these data instances. In this work, we\nendeavour to discover the entities and their corresponding importance in\ncooking recipes automaticall} as a visual-linguistic association problem. More\nspecifically, we introduce a novel cross-modal learning framework to jointly\nmodel the latent representations of images and text in the food image-recipe\nassociation and retrieval tasks. This model allows one to discover complex\nfunctional and hierarchical relationships between images and text, and among\ntextual parts of a recipe including title, ingredients and cooking\ninstructions. Our experiments show that by making use of efficient\ntree-structured Long Short-Term Memory as the text encoder in our computational\ncross-modal retrieval framework, we are not only able to identify the main\ningredients and cooking actions in the recipe descriptions without explicit\nsupervision, but we can also learn more meaningful feature representations of\nfood recipes, appropriate for challenging cross-modal retrieval and recipe\nadaption tasks.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 11:24:34 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Pham", "Hai X.", ""], ["Guerrero", "Ricardo", ""], ["Li", "Jiatong", ""], ["Pavlovic", "Vladimir", ""]]}, {"id": "2102.02629", "submitter": "Seokju Lee", "authors": "Seokju Lee, Sunghoon Im, Stephen Lin, In So Kweon", "title": "Learning Monocular Depth in Dynamic Scenes via Instance-Aware Projection\n  Consistency", "comments": "Accepted to AAAI 2021. Code/dataset/models are available at\n  https://github.com/SeokjuLee/Insta-DM. arXiv admin note: substantial text\n  overlap with arXiv:1912.09351", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end joint training framework that explicitly models\n6-DoF motion of multiple dynamic objects, ego-motion and depth in a monocular\ncamera setup without supervision. Our technical contributions are three-fold.\nFirst, we highlight the fundamental difference between inverse and forward\nprojection while modeling the individual motion of each rigid object, and\npropose a geometrically correct projection pipeline using a neural forward\nprojection module. Second, we design a unified instance-aware photometric and\ngeometric consistency loss that holistically imposes self-supervisory signals\nfor every background and object region. Lastly, we introduce a general-purpose\nauto-annotation scheme using any off-the-shelf instance segmentation and\noptical flow models to produce video instance segmentation maps that will be\nutilized as input to our training pipeline. These proposed elements are\nvalidated in a detailed ablation study. Through extensive experiments conducted\non the KITTI and Cityscapes dataset, our framework is shown to outperform the\nstate-of-the-art depth and motion estimation methods. Our code, dataset, and\nmodels are available at https://github.com/SeokjuLee/Insta-DM .\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 14:26:42 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Lee", "Seokju", ""], ["Im", "Sunghoon", ""], ["Lin", "Stephen", ""], ["Kweon", "In So", ""]]}, {"id": "2102.02662", "submitter": "Alexey Chernyavskiy", "authors": "Elvira Zainulina, Alexey Chernyavskiy, Dmitry V. Dylov", "title": "No-reference denoising of low-dose CT projections", "comments": "Accepted to ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Low-dose computed tomography (LDCT) became a clear trend in radiology with an\naspiration to refrain from delivering excessive X-ray radiation to the\npatients. The reduction of the radiation dose decreases the risks to the\npatients but raises the noise level, affecting the quality of the images and\ntheir ultimate diagnostic value. One mitigation option is to consider pairs of\nlow-dose and high-dose CT projections to train a denoising model using deep\nlearning algorithms; however, such pairs are rarely available in practice. In\nthis paper, we present a new self-supervised method for CT denoising. Unlike\nexisting self-supervised approaches, the proposed method requires only noisy CT\nprojections and exploits the connections between adjacent images. The\nexperiments carried out on an LDCT dataset demonstrate that our method is\nalmost as accurate as the supervised approach, while also outperforming the\nconsidered self-supervised denoising methods.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 13:51:33 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Zainulina", "Elvira", ""], ["Chernyavskiy", "Alexey", ""], ["Dylov", "Dmitry V.", ""]]}, {"id": "2102.02690", "submitter": "Kiret Dhindsa", "authors": "Umaseh Sivanesan, Luis H. Braga, Ranil R. Sonnadara, Kiret Dhindsa", "title": "TricycleGAN: Unsupervised Image Synthesis and Segmentation Based on\n  Shape Priors", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical image segmentation is routinely performed to isolate regions of\ninterest, such as organs and lesions. Currently, deep learning is the state of\nthe art for automatic segmentation, but is usually limited by the need for\nsupervised training with large datasets that have been manually segmented by\ntrained clinicians. The goal of semi-superised and unsupervised image\nsegmentation is to greatly reduce, or even eliminate, the need for training\ndata and therefore to minimze the burden on clinicians when training\nsegmentation models. To this end we introduce a novel network architecture for\ncapable of unsupervised and semi-supervised image segmentation called\nTricycleGAN. This approach uses three generative models to learn translations\nbetween medical images and segmentation maps using edge maps as an intermediate\nstep. Distinct from other approaches based on generative networks, TricycleGAN\nrelies on shape priors rather than colour and texture priors. As such, it is\nparticularly well-suited for several domains of medical imaging, such as\nultrasound imaging, where commonly used visual cues may be absent. We present\nexperiments with TricycleGAN on a clinical dataset of kidney ultrasound images\nand the benchmark ISIC 2018 skin lesion dataset.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 15:36:18 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Sivanesan", "Umaseh", ""], ["Braga", "Luis H.", ""], ["Sonnadara", "Ranil R.", ""], ["Dhindsa", "Kiret", ""]]}, {"id": "2102.02696", "submitter": "Chi Wang", "authors": "Chi Wang, Yunke Zhang, Miaomiao Cui, Jinlin Liu, Peiran Ren, Yin Yang,\n  Xuansong Xie, XianSheng Hua, Hujun Bao, Weiwei Xu", "title": "Active Boundary Loss for Semantic Segmentation", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel active boundary loss for semantic segmentation.\nIt can progressively encourage the alignment between predicted boundaries and\nground-truth boundaries during end-to-end training, which is not explicitly\nenforced in commonly used cross-entropy loss. Based on the predicted boundaries\ndetected from the segmentation results using current network parameters, we\nformulate the boundary alignment problem as a differentiable direction vector\nprediction problem to guide the movement of predicted boundaries in each\niteration. Our loss is model-agnostic and can be plugged into the training of\nsegmentation networks to improve the boundary details. Experimental results\nshow that training with the active boundary loss can effectively improve the\nboundary F-score and mean Intersection-over-Union on challenging image and\nvideo object segmentation datasets.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 15:47:54 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Wang", "Chi", ""], ["Zhang", "Yunke", ""], ["Cui", "Miaomiao", ""], ["Liu", "Jinlin", ""], ["Ren", "Peiran", ""], ["Yang", "Yin", ""], ["Xie", "Xuansong", ""], ["Hua", "XianSheng", ""], ["Bao", "Hujun", ""], ["Xu", "Weiwei", ""]]}, {"id": "2102.02706", "submitter": "Grigorios G. Anagnostopoulos Dr.", "authors": "Grigorios G. Anagnostopoulos and Alexandros Kalousis", "title": "ProxyFAUG: Proximity-based Fingerprint Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The proliferation of data-demanding machine learning methods has brought to\nlight the necessity for methodologies which can enlarge the size of training\ndatasets, with simple, rule-based methods. In-line with this concept, the\nfingerprint augmentation scheme proposed in this work aims to augment\nfingerprint datasets which are used to train positioning models. The proposed\nmethod utilizes fingerprints which are recorded in spacial proximity, in order\nto perform fingerprint augmentation, creating new fingerprints which combine\nthe features of the original ones. The proposed method of composing the new,\naugmented fingerprints is inspired by the crossover and mutation operators of\ngenetic algorithms. The ProxyFAUG method aims to improve the achievable\npositioning accuracy of fingerprint datasets, by introducing a rule-based,\nstochastic, proximity-based method of fingerprint augmentation. The performance\nof ProxyFAUG is evaluated in an outdoor Sigfox setting using a public dataset.\nThe best performing published positioning method on this dataset is improved by\n40% in terms of median error and 6% in terms of mean error, with the use of the\naugmented dataset. The analysis of the results indicate a systematic and\nsignificant performance improvement at the lower error quartiles, as indicated\nby the impressive improvement of the median error.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 15:59:30 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Anagnostopoulos", "Grigorios G.", ""], ["Kalousis", "Alexandros", ""]]}, {"id": "2102.02711", "submitter": "Soumick Chatterjee", "authors": "Chompunuch Sarasaen, Soumick Chatterjee, Mario Breitkopf, Georg Rose,\n  Andreas N\\\"urnberger and Oliver Speck", "title": "Fine-tuning deep learning model parameters for improved super-resolution\n  of dynamic MRI with prior-knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic imaging is a beneficial tool for interventions to assess\nphysiological changes. Nonetheless during dynamic MRI, while achieving a high\ntemporal resolution, the spatial resolution is compromised. To overcome this\nspatio-temporal trade-off, this research presents a super-resolution (SR) MRI\nreconstruction with prior knowledge based fine-tuning to maximise spatial\ninformation while preserving high temporal resolution of dynamic MRI. An U-Net\nbased network with perceptual loss is trained on a benchmark dataset and\nfine-tuned using one subject-specific static high resolution MRI as prior\nknowledge to obtain high resolution dynamic images during the inference stage.\n3D dynamic data for three subjects were acquired with different parameters to\ntest the generalisation capabilities of the network. The method was tested for\ndifferent levels of in-plane undersampling for dynamic MRI. The reconstructed\ndynamic SR results after fine-tuning showed higher similarity with the high\nresolution ground-truth, while quantitatively achieving statistically\nsignificant improvement. The average SSIM of the lowest resolution experimented\nduring this research (6.25~\\% of the k-space) before and after fine-tuning were\n0.939 $\\pm$ 0.008 and 0.957 $\\pm$ 0.006 respectively. This could theoretically\nresult in an acceleration factor of 16, which can potentially be acquired in\nless than half a second. The proposed approach shows that the super-resolution\nMRI reconstruction with prior-information can alleviate the spatio-temporal\ntrade-off in dynamic MRI, even for high acceleration factors.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 16:11:53 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 12:24:51 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Sarasaen", "Chompunuch", ""], ["Chatterjee", "Soumick", ""], ["Breitkopf", "Mario", ""], ["Rose", "Georg", ""], ["N\u00fcrnberger", "Andreas", ""], ["Speck", "Oliver", ""]]}, {"id": "2102.02717", "submitter": "Yiming Lin", "authors": "Yiming Lin, Jie Shen, Yujiang Wang, Maja Pantic", "title": "RoI Tanh-polar Transformer Network for Face Parsing in the Wild", "comments": "Accepted at Image and Vision Computing. Code is available on\n  https://github.com/hhj1897/face_parsing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Face parsing aims to predict pixel-wise labels for facial components of a\ntarget face in an image. Existing approaches usually crop the target face from\nthe input image with respect to a bounding box calculated during\npre-processing, and thus can only parse inner facial Regions of\nInterest~(RoIs). Peripheral regions like hair are ignored and nearby faces that\nare partially included in the bounding box can cause distractions. Moreover,\nthese methods are only trained and evaluated on near-frontal portrait images\nand thus their performance for in-the-wild cases has been unexplored. To\naddress these issues, this paper makes three contributions. First, we introduce\niBugMask dataset for face parsing in the wild, which consists of 21,866\ntraining images and 1,000 testing images. The training images are obtained by\naugmenting an existing dataset with large face poses. The testing images are\nmanually annotated with $11$ facial regions and there are large variations in\nsizes, poses, expressions and background. Second, we propose RoI Tanh-polar\ntransform that warps the whole image to a Tanh-polar representation with a\nfixed ratio between the face area and the context, guided by the target\nbounding box. The new representation contains all information in the original\nimage, and allows for rotation equivariance in the convolutional neural\nnetworks~(CNNs). Third, we propose a hybrid residual representation learning\nblock, coined HybridBlock, that contains convolutional layers in both the\nTanh-polar space and the Tanh-Cartesian space, allowing for receptive fields of\ndifferent shapes in CNNs. Through extensive experiments, we show that the\nproposed method improves the state-of-the-art for face parsing in the wild and\ndoes not require facial landmarks for alignment.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 16:25:26 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 18:46:31 GMT"}, {"version": "v3", "created": "Wed, 5 May 2021 16:27:24 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Lin", "Yiming", ""], ["Shen", "Jie", ""], ["Wang", "Yujiang", ""], ["Pantic", "Maja", ""]]}, {"id": "2102.02732", "submitter": "Anthony Bourached", "authors": "David G.Stork, Anthony Bourached, George H.Cann, and Ryan-Rhys\n  Griffiths", "title": "Computational identification of significant actors in paintings through\n  symbols and attributes", "comments": "Accepted as conference paper at Computer Vision and Art Analysis 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic analysis of fine art paintings presents a number of novel\ntechnical challenges to artificial intelligence, computer vision, machine\nlearning, and knowledge representation quite distinct from those arising in the\nanalysis of traditional photographs. The most important difference is that many\nrealist paintings depict stories or episodes in order to convey a lesson,\nmoral, or meaning. One early step in automatic interpretation and extraction of\nmeaning in artworks is the identifications of figures (actors). In Christian\nart, specifically, one must identify the actors in order to identify the\nBiblical episode or story depicted, an important step in understanding the\nartwork. We designed an automatic system based on deep convolutional neural\nnetworks and simple knowledge database to identify saints throughout six\ncenturies of Christian art based in large part upon saints symbols or\nattributes. Our work represents initial steps in the broad task of automatic\nsemantic interpretation of messages and meaning in fine art.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 16:42:41 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Stork", "David G.", ""], ["Bourached", "Anthony", ""], ["Cann", "George H.", ""], ["Griffiths", "Ryan-Rhys", ""]]}, {"id": "2102.02751", "submitter": "Abir Das", "authors": "Ankit Singh, Omprakash Chakraborty, Ashutosh Varshney, Rameswar Panda,\n  Rogerio Feris, Kate Saenko, Abir Das", "title": "Semi-Supervised Action Recognition with Temporal Contrastive Learning", "comments": "Accepted in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to recognize actions from only a handful of labeled videos is a\nchallenging problem due to the scarcity of tediously collected activity labels.\nWe approach this problem by learning a two-pathway temporal contrastive model\nusing unlabeled videos at two different speeds leveraging the fact that\nchanging video speed does not change an action. Specifically, we propose to\nmaximize the similarity between encoded representations of the same video at\ntwo different speeds as well as minimize the similarity between different\nvideos played at different speeds. This way we use the rich supervisory\ninformation in terms of `time' that is present in otherwise unsupervised pool\nof videos. With this simple yet effective strategy of manipulating video\nplayback rates, we considerably outperform video extensions of sophisticated\nstate-of-the-art semi-supervised image recognition methods across multiple\ndiverse benchmark datasets and network architectures. Interestingly, our\nproposed approach benefits from out-of-domain unlabeled videos showing\ngeneralization and robustness. We also perform rigorous ablations and analysis\nto validate our approach. Project page: https://cvir.github.io/TCL/.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 17:28:35 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 17:34:29 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Singh", "Ankit", ""], ["Chakraborty", "Omprakash", ""], ["Varshney", "Ashutosh", ""], ["Panda", "Rameswar", ""], ["Feris", "Rogerio", ""], ["Saenko", "Kate", ""], ["Das", "Abir", ""]]}, {"id": "2102.02754", "submitter": "Yuval Alaluf", "authors": "Yuval Alaluf, Or Patashnik, Daniel Cohen-Or", "title": "Only a Matter of Style: Age Transformation Using a Style-Based\n  Regression Model", "comments": "Accepted to SIGGRAPH 2021, project page available at\n  https://yuval-alaluf.github.io/SAM/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of age transformation illustrates the change of an individual's\nappearance over time. Accurately modeling this complex transformation over an\ninput facial image is extremely challenging as it requires making convincing,\npossibly large changes to facial features and head shape, while still\npreserving the input identity. In this work, we present an image-to-image\ntranslation method that learns to directly encode real facial images into the\nlatent space of a pre-trained unconditional GAN (e.g., StyleGAN) subject to a\ngiven aging shift. We employ a pre-trained age regression network to explicitly\nguide the encoder in generating the latent codes corresponding to the desired\nage. In this formulation, our method approaches the continuous aging process as\na regression task between the input age and desired target age, providing\nfine-grained control over the generated image. Moreover, unlike approaches that\noperate solely in the latent space using a prior on the path controlling age,\nour method learns a more disentangled, non-linear path. Finally, we demonstrate\nthat the end-to-end nature of our approach, coupled with the rich semantic\nlatent space of StyleGAN, allows for further editing of the generated images.\nQualitative and quantitative evaluations show the advantages of our method\ncompared to state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 17:33:28 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 17:31:01 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Alaluf", "Yuval", ""], ["Patashnik", "Or", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2102.02766", "submitter": "Omer Tov", "authors": "Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, Daniel Cohen-Or", "title": "Designing an Encoder for StyleGAN Image Manipulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, there has been a surge of diverse methods for performing image\nediting by employing pre-trained unconditional generators. Applying these\nmethods on real images, however, remains a challenge, as it necessarily\nrequires the inversion of the images into their latent space. To successfully\ninvert a real image, one needs to find a latent code that reconstructs the\ninput image accurately, and more importantly, allows for its meaningful\nmanipulation. In this paper, we carefully study the latent space of StyleGAN,\nthe state-of-the-art unconditional generator. We identify and analyze the\nexistence of a distortion-editability tradeoff and a distortion-perception\ntradeoff within the StyleGAN latent space. We then suggest two principles for\ndesigning encoders in a manner that allows one to control the proximity of the\ninversions to regions that StyleGAN was originally trained on. We present an\nencoder based on our two principles that is specifically designed for\nfacilitating editing on real images by balancing these tradeoffs. By evaluating\nits performance qualitatively and quantitatively on numerous challenging\ndomains, including cars and horses, we show that our inversion method, followed\nby common editing techniques, achieves superior real-image editing quality,\nwith only a small reconstruction accuracy drop.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 17:52:38 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Tov", "Omer", ""], ["Alaluf", "Yuval", ""], ["Nitzan", "Yotam", ""], ["Patashnik", "Or", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2102.02771", "submitter": "Jun Wang", "authors": "Jun Wang, Xiaohan Yu, Yongsheng Gao", "title": "Mask guided attention for fine-grained patchy image classification", "comments": "5 pages, two figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel mask guided attention (MGA) method for\nfine-grained patchy image classification. The key challenge of fine-grained\npatchy image classification lies in two folds, ultra-fine-grained\ninter-category variances among objects and very few data available for\ntraining. This motivates us to consider employing more useful supervision\nsignal to train a discriminative model within limited training samples.\nSpecifically, the proposed MGA integrates a pre-trained semantic segmentation\nmodel that produces auxiliary supervision signal, i.e., patchy attention mask,\nenabling a discriminative representation learning. The patchy attention mask\ndrives the classifier to filter out the insignificant parts of images (e.g.,\ncommon features between different categories), which enhances the robustness of\nMGA for the fine-grained patchy image classification. We verify the\neffectiveness of our method on three publicly available patchy image datasets.\nExperimental results demonstrate that our MGA method achieves superior\nperformance on three datasets compared with the state-of-the-art methods. In\naddition, our ablation study shows that MGA improves the accuracy by 2.25% and\n2% on the SoyCultivarVein and BtfPIS datasets, indicating its practicality\ntowards solving the fine-grained patchy image classification.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 17:54:50 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Wang", "Jun", ""], ["Yu", "Xiaohan", ""], ["Gao", "Yongsheng", ""]]}, {"id": "2102.02772", "submitter": "Abrar Faiyaz", "authors": "Abrar Faiyaz, Marvin Doyley, Giovanni Schifitto, Jianhui Zhong, Md\n  Nasir Uddin", "title": "Single-Shell NODDI Using Dictionary Learner Estimated Isotropic Volume\n  Fraction", "comments": "56 pages, 9 Figures, 2 Tables, Supplementary Document (attached)", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neurite orientation dispersion and density imaging (NODDI) enables the\nassessment of intracellular, extracellular and free water signals from\nmulti-shell diffusion MRI data. It is an insightful approach to characterize\nbrain tissue microstructure. Single-shell reconstruction for NODDI parameters\nhas been discouraged in previous studies caused by failure when fitting,\nespecially for the neurite density index (NDI). Here, we investigated the\npossibility of creating robust NODDI parameter maps with single-shell data,\nusing the isotropic volume fraction (fISO) as prior. Prior estimation was made\nindependent of the NODDI model constraint using a dictionary learning approach.\nFirst, we used a stochastic sparse dictionary-based network (DictNet) in\npredicting fISO which is trained with data obtained from in vivo and simulated\ndiffusion MRI data. In single-shell cases, the mean diffusivity (MD) and raw T2\nsignal with no diffusion weighting (S0) was incorporated in the dictionary for\nthe fISO estimation. Then, the NODDI framework was used with the known fISO to\nestimate the NDI and orientation dispersion index (ODI). The fISO estimated by\nour model was compared with other fISO estimators in the simulation. Further,\nusing both synthetic data simulation and human data collected on a 3T scanner,\nwe compared the performance of our dictionary-based learning prior NODDI (DLpN)\nwith the original NODDI for both single-shell and multi-shell data. Our results\nsuggest that DLpN derived NDI and ODI parameters for single-shell protocols are\ncomparable with original multi-shell NODDI, and protocol with b=2000 s/mm2\nperforms the best (error ~5% in white and grey matter). This may allow NODDI\nevaluation of studies on single-shell data by multi-shell scanning of two\nsubjects for DictNet fISO training.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 17:43:09 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 23:37:17 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Faiyaz", "Abrar", ""], ["Doyley", "Marvin", ""], ["Schifitto", "Giovanni", ""], ["Zhong", "Jianhui", ""], ["Uddin", "Md Nasir", ""]]}, {"id": "2102.02779", "submitter": "Jaemin Cho", "authors": "Jaemin Cho, Jie Lei, Hao Tan, Mohit Bansal", "title": "Unifying Vision-and-Language Tasks via Text Generation", "comments": "ICML 2021 (15 pages, 4 figures, 14 tables)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for vision-and-language learning typically require designing\ntask-specific architectures and objectives for each task. For example, a\nmulti-label answer classifier for visual question answering, a region scorer\nfor referring expression comprehension, and a language decoder for image\ncaptioning, etc. To alleviate these hassles, in this work, we propose a unified\nframework that learns different tasks in a single architecture with the same\nlanguage modeling objective, i.e., multimodal conditional text generation,\nwhere our models learn to generate labels in text based on the visual and\ntextual inputs. On 7 popular vision-and-language benchmarks, including visual\nquestion answering, referring expression comprehension, visual commonsense\nreasoning, most of which have been previously modeled as discriminative tasks,\nour generative approach (with a single unified architecture) reaches comparable\nperformance to recent task-specific state-of-the-art vision-and-language\nmodels. Moreover, our generative approach shows better generalization ability\non questions that have rare answers. Also, we show that our framework allows\nmulti-task learning in a single architecture with a single set of parameters,\nachieving similar performance to separately optimized single-task models. Our\ncode is publicly available at: https://github.com/j-min/VL-T5\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 17:59:30 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 23:12:46 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Cho", "Jaemin", ""], ["Lei", "Jie", ""], ["Tan", "Hao", ""], ["Bansal", "Mohit", ""]]}, {"id": "2102.02798", "submitter": "Pradyumna Reddy", "authors": "Pradyumna Reddy, Michael Gharbi, Michal Lukac, Niloy J. Mitra", "title": "Im2Vec: Synthesizing Vector Graphics without Vector Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vector graphics are widely used to represent fonts, logos, digital artworks,\nand graphic designs. But, while a vast body of work has focused on generative\nalgorithms for raster images, only a handful of options exists for vector\ngraphics. One can always rasterize the input graphic and resort to image-based\ngenerative approaches, but this negates the advantages of the vector\nrepresentation. The current alternative is to use specialized models that\nrequire explicit supervision on the vector graphics representation at training\ntime. This is not ideal because large-scale high quality vector-graphics\ndatasets are difficult to obtain. Furthermore, the vector representation for a\ngiven design is not unique, so models that supervise on the vector\nrepresentation are unnecessarily constrained. Instead, we propose a new neural\nnetwork that can generate complex vector graphics with varying topologies, and\nonly requires indirect supervision from readily-available raster training\nimages (i.e., with no vector counterparts). To enable this, we use a\ndifferentiable rasterization pipeline that renders the generated vector shapes\nand composites them together onto a raster canvas. We demonstrate our method on\na range of datasets, and provide comparison with state-of-the-art SVG-VAE and\nDeepSVG, both of which require explicit vector graphics supervision. Finally,\nwe also demonstrate our approach on the MNIST dataset, for which no groundtruth\nvector representation is available. Source code, datasets, and more results are\navailable at geometry.cs.ucl.ac.uk/projects/2021/Im2Vec/\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 18:39:45 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 10:34:00 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 10:48:23 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Reddy", "Pradyumna", ""], ["Gharbi", "Michael", ""], ["Lukac", "Michal", ""], ["Mitra", "Niloy J.", ""]]}, {"id": "2102.02804", "submitter": "Ilke Cugu", "authors": "Ilke Cugu, Emre Akbas", "title": "A Deeper Look into Convolutions via Pruning", "comments": "The codes are available at https://github.com/cuguilke/psykedelic", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are able to attain better visual\nrecognition performance than fully connected neural networks despite having\nmuch less parameters due to their parameter sharing principle. Hence, modern\narchitectures are designed to contain a very small number of fully-connected\nlayers, often at the end, after multiple layers of convolutions. It is\ninteresting to observe that we can replace large fully-connected layers with\nrelatively small groups of tiny matrices applied on the entire image. Moreover,\nalthough this strategy already reduces the number of parameters, most of the\nconvolutions can be eliminated as well, without suffering any loss in\nrecognition performance. However, there is no solid recipe to detect this\nhidden subset of convolutional neurons that is responsible for the majority of\nthe recognition work. Hence, in this work, we use the matrix characteristics\nbased on eigenvalues in addition to the classical weight-based importance\nassignment approach for pruning to shed light on the internal mechanisms of a\nwidely used family of CNNs, namely residual neural networks (ResNets), for the\nimage classification problem using CIFAR-10, CIFAR-100 and Tiny ImageNet\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 18:55:03 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Cugu", "Ilke", ""], ["Akbas", "Emre", ""]]}, {"id": "2102.02808", "submitter": "Syed Waqas Zamir", "authors": "Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad\n  Shahbaz Khan, Ming-Hsuan Yang, Ling Shao", "title": "Multi-Stage Progressive Image Restoration", "comments": "Accepted at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Image restoration tasks demand a complex balance between spatial details and\nhigh-level contextualized information while recovering images. In this paper,\nwe propose a novel synergistic design that can optimally balance these\ncompeting goals. Our main proposal is a multi-stage architecture, that\nprogressively learns restoration functions for the degraded inputs, thereby\nbreaking down the overall recovery process into more manageable steps.\nSpecifically, our model first learns the contextualized features using\nencoder-decoder architectures and later combines them with a high-resolution\nbranch that retains local information. At each stage, we introduce a novel\nper-pixel adaptive design that leverages in-situ supervised attention to\nreweight the local features. A key ingredient in such a multi-stage\narchitecture is the information exchange between different stages. To this end,\nwe propose a two-faceted approach where the information is not only exchanged\nsequentially from early to late stages, but lateral connections between feature\nprocessing blocks also exist to avoid any loss of information. The resulting\ntightly interlinked multi-stage architecture, named as MPRNet, delivers strong\nperformance gains on ten datasets across a range of tasks including image\nderaining, deblurring, and denoising. The source code and pre-trained models\nare available at https://github.com/swz30/MPRNet.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 18:57:07 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 11:02:52 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Zamir", "Syed Waqas", ""], ["Arora", "Aditya", ""], ["Khan", "Salman", ""], ["Hayat", "Munawar", ""], ["Khan", "Fahad Shahbaz", ""], ["Yang", "Ming-Hsuan", ""], ["Shao", "Ling", ""]]}, {"id": "2102.02811", "submitter": "Zhiqiang Tang", "authors": "Zhiqiang Tang, Yunhe Gao, Yi Zhu, Zhi Zhang, Mu Li, Dimitris Metaxas", "title": "SelfNorm and CrossNorm for Out-of-Distribution Robustness", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalization techniques are crucial in stabilizing and accelerating the\ntraining of deep neural networks. However, they are mainly designed for the\nindependent and identically distributed (IID) data, not satisfying many\nreal-world out-of-distribution (OOD) situations. Unlike most previous works,\nthis paper presents two normalization methods, SelfNorm and CrossNorm, to\npromote OOD generalization. SelfNorm uses attention to recalibrate statistics\n(channel-wise mean and variance), while CrossNorm exchanges the statistics\nbetween feature maps. SelfNorm and CrossNorm can complement each other in OOD\ngeneralization, though exploring different directions in statistics usage.\nExtensive experiments on different domains (vision and language), tasks\n(classification and segmentation), and settings (supervised and\nsemi-supervised) show their effectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 18:59:20 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Tang", "Zhiqiang", ""], ["Gao", "Yunhe", ""], ["Zhu", "Yi", ""], ["Zhang", "Zhi", ""], ["Li", "Mu", ""], ["Metaxas", "Dimitris", ""]]}, {"id": "2102.02828", "submitter": "Jason McEwen", "authors": "Jason D. McEwen, Christopher G. R. Wallis, Augustine N. Mavor-Parker", "title": "Scattering Networks on the Sphere for Scalable and Rotationally\n  Equivariant Spherical CNNs", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV astro-ph.IM cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Convolutional neural networks (CNNs) constructed natively on the sphere have\nbeen developed recently and shown to be highly effective for the analysis of\nspherical data. While an efficient framework has been formulated, spherical\nCNNs are nevertheless highly computationally demanding; typically they cannot\nscale beyond spherical signals of thousands of pixels. We develop scattering\nnetworks constructed natively on the sphere that provide a powerful\nrepresentational space for spherical data. Spherical scattering networks are\ncomputationally scalable and exhibit rotational equivariance, while their\nrepresentational space is invariant to isometries and provides efficient and\nstable signal representations. By integrating scattering networks as an\nadditional type of layer in the generalized spherical CNN framework, we show\nhow they can be leveraged to scale spherical CNNs to the high-resolution data\ntypical of many practical applications, with spherical signals of many tens of\nmegapixels and beyond.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 19:00:01 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 15:06:53 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["McEwen", "Jason D.", ""], ["Wallis", "Christopher G. R.", ""], ["Mavor-Parker", "Augustine N.", ""]]}, {"id": "2102.02861", "submitter": "Srikrishna Jaganathan", "authors": "Srikrishna Jaganathan, Jian Wang, Anja Borsdorf, Andreas Maier", "title": "Learning the Update Operator for 2D/3D Image Registration", "comments": "7 pages, 2 figures, Accepted for Bildverarbeitung f\\\"ur die Medizin,\n  07.-09.03.2021", "journal-ref": null, "doi": "10.1007/978-3-658-33198-6_27", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image guidance in minimally invasive interventions is usually provided using\nlive 2D X-ray imaging. To enhance the information available during the\nintervention, the preoperative volume can be overlaid over the 2D images using\n2D/3D image registration. Recently, deep learning-based 2D/3D registration\nmethods have shown promising results by improving computational efficiency and\nrobustness. However, there is still a gap in terms of registration accuracy\ncompared to traditional optimization-based methods. We aim to address this gap\nby incorporating traditional methods in deep neural networks using known\noperator learning. As an initial step in this direction, we propose to learn\nthe update step of an iterative 2D/3D registration framework based on the\nPoint-to-Plane Correspondence model. We embed the Point-to-Plane Correspondence\nmodel as a known operator in our deep neural network and learn the update step\nfor the iterative registration. We show an improvement of 1.8 times in terms of\nregistration accuracy for the update step prediction compared to learning\nwithout the known operator.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 19:52:59 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Jaganathan", "Srikrishna", ""], ["Wang", "Jian", ""], ["Borsdorf", "Anja", ""], ["Maier", "Andreas", ""]]}, {"id": "2102.02885", "submitter": "Jiasong Chen", "authors": "Jiasong Chen, Linchen Qian, Timur Urakov, Weiyong Gu, Liang Liang", "title": "Adversarial Robustness Study of Convolutional Neural Network for Lumbar\n  Disk Shape Reconstruction from MR images", "comments": "Published at SPIE Medical Imaging: Image Processing 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning technologies using deep neural networks (DNNs), especially\nconvolutional neural networks (CNNs), have made automated, accurate, and fast\nmedical image analysis a reality for many applications, and some DNN-based\nmedical image analysis systems have even been FDA-cleared. Despite the\nprogress, challenges remain to build DNNs as reliable as human expert doctors.\nIt is known that DNN classifiers may not be robust to noises: by adding a small\namount of noise to an input image, a DNN classifier may make a wrong\nclassification of the noisy image (i.e., in-distribution adversarial sample),\nwhereas it makes the right classification of the clean image. Another issue is\ncaused by out-of-distribution samples that are not similar to any sample in the\ntraining set. Given such a sample as input, the output of a DNN will become\nmeaningless. In this study, we investigated the in-distribution (IND) and\nout-of-distribution (OOD) adversarial robustness of a representative CNN for\nlumbar disk shape reconstruction from spine MR images. To study the\nrelationship between dataset size and robustness to IND adversarial attacks, we\nused a data augmentation method to create training sets with different levels\nof shape variations. We utilized the PGD-based algorithm for IND adversarial\nattacks and extended it for OOD adversarial attacks to generate OOD adversarial\nsamples for model testing. The results show that IND adversarial training can\nimprove the CNN robustness to IND adversarial attacks, and larger training\ndatasets may lead to higher IND robustness. However, it is still a challenge to\ndefend against OOD adversarial attacks.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 20:57:49 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Chen", "Jiasong", ""], ["Qian", "Linchen", ""], ["Urakov", "Timur", ""], ["Gu", "Weiyong", ""], ["Liang", "Liang", ""]]}, {"id": "2102.02886", "submitter": "Daniel Lenton", "authors": "Daniel Lenton, Fabio Pardo, Fabian Falck, Stephen James, Ronald Clark", "title": "Ivy: Templated Deep Learning for Inter-Framework Portability", "comments": "Code at https://github.com/ivy-dl/ivy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce Ivy, a templated Deep Learning (DL) framework which abstracts\nexisting DL frameworks. Ivy unifies the core functions of these frameworks to\nexhibit consistent call signatures, syntax and input-output behaviour. New\nhigh-level framework-agnostic functions and classes, which are usable alongside\nframework-specific code, can then be implemented as compositions of the unified\nlow-level Ivy functions. Ivy currently supports TensorFlow, PyTorch, MXNet, Jax\nand NumPy. We also release four pure-Ivy libraries for mechanics, 3D vision,\nrobotics, and differentiable environments. Through our evaluations, we show\nthat Ivy can significantly reduce lines of code with a runtime overhead of less\nthan 1% in most cases. We welcome developers to join the Ivy community by\nwriting their own functions, layers and libraries in Ivy, maximizing their\naudience and helping to accelerate DL research through inter-framework\ncodebases. More information can be found at https://ivy-dl.org.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 20:58:37 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 18:26:14 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 17:59:16 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Lenton", "Daniel", ""], ["Pardo", "Fabio", ""], ["Falck", "Fabian", ""], ["James", "Stephen", ""], ["Clark", "Ronald", ""]]}, {"id": "2102.02887", "submitter": "Shiwei Liu", "authors": "Shiwei Liu, Lu Yin, Decebal Constantin Mocanu, Mykola Pechenizkiy", "title": "Do We Actually Need Dense Over-Parameterization? In-Time\n  Over-Parameterization in Sparse Training", "comments": "16 pages; 10 figures; Published in Proceedings of the 38th\n  International Conference on Machine Learning. Code can be found\n  https://github.com/Shiweiliuiiiiiii/In-Time-Over-Parameterization", "journal-ref": "Proceedings of the 38th International Conference on Machine\n  Learning (2021)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new perspective on training deep neural\nnetworks capable of state-of-the-art performance without the need for the\nexpensive over-parameterization by proposing the concept of In-Time\nOver-Parameterization (ITOP) in sparse training. By starting from a random\nsparse network and continuously exploring sparse connectivities during\ntraining, we can perform an Over-Parameterization in the space-time manifold,\nclosing the gap in the expressibility between sparse training and dense\ntraining. We further use ITOP to understand the underlying mechanism of Dynamic\nSparse Training (DST) and indicate that the benefits of DST come from its\nability to consider across time all possible parameters when searching for the\noptimal sparse connectivity. As long as there are sufficient parameters that\nhave been reliably explored during training, DST can outperform the dense\nneural network by a large margin. We present a series of experiments to support\nour conjecture and achieve the state-of-the-art sparse training performance\nwith ResNet-50 on ImageNet. More impressively, our method achieves dominant\nperformance over the overparameterization-based sparse methods at extreme\nsparsity levels. When trained on CIFAR-100, our method can match the\nperformance of the dense model even at an extreme sparsity (98%). Code can be\nfound https://github.com/Shiweiliuiiiiiii/In-Time-Over-Parameterization.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 20:59:31 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 23:36:57 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 05:01:46 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Liu", "Shiwei", ""], ["Yin", "Lu", ""], ["Mocanu", "Decebal Constantin", ""], ["Pechenizkiy", "Mykola", ""]]}, {"id": "2102.02895", "submitter": "Hrithwik Shalu", "authors": "Joseph Stember and Hrithwik Shalu", "title": "Deep reinforcement learning-based image classification achieves perfect\n  testing set accuracy for MRI brain tumors with a training set of only 30\n  images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Image classification may be the fundamental task in imaging\nartificial intelligence. We have recently shown that reinforcement learning can\nachieve high accuracy for lesion localization and segmentation even with\nminuscule training sets. Here, we introduce reinforcement learning for image\nclassification. In particular, we apply the approach to normal vs.\ntumor-containing 2D MRI brain images.\n  Materials and Methods: We applied multi-step image classification to allow\nfor combined Deep Q learning and TD(0) Q learning. We trained on a set of 30\nimages (15 normal and 15 tumor-containing). We tested on a separate set of 30\nimages (15 normal and 15 tumor-containing). For comparison, we also trained and\ntested a supervised deep-learning classification network on the same set of\ntraining and testing images.\n  Results: Whereas the supervised approach quickly overfit the training data\nand as expected performed poorly on the testing set (57% accuracy, just over\nrandom guessing), the reinforcement learning approach achieved an accuracy of\n100%.\n  Conclusion: We have shown a proof-of-principle application of reinforcement\nlearning to the classification of brain tumors. We achieved perfect testing set\naccuracy with a training set of merely 30 images.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 21:31:22 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 05:37:21 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Stember", "Joseph", ""], ["Shalu", "Hrithwik", ""]]}, {"id": "2102.02896", "submitter": "Gedeon Muhawenayo Mr.", "authors": "Gedeon Muhawenayo and Georgia Gkioxari", "title": "Compressed Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning approaches have achieved unprecedented performance in visual\nrecognition tasks such as object detection and pose estimation. However,\nstate-of-the-art models have millions of parameters represented as floats which\nmake them computationally expensive and constrain their deployment on hardware\nsuch as mobile phones and IoT nodes. Most commonly, activations of deep neural\nnetworks tend to be sparse thus proving that models are over parametrized with\nredundant neurons. Model compression techniques, such as pruning and\nquantization, have recently shown promising results by improving model\ncomplexity with little loss in performance. In this work, we extended pruning,\na compression technique that discards unnecessary model connections, and weight\nsharing techniques for the task of object detection. With our approach, we are\nable to compress a state-of-the-art object detection model by 30.0% without a\nloss in performance. We also show that our compressed model can be easily\ninitialized with existing pre-trained weights, and thus is able to fully\nutilize published state-of-the-art model zoos.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 21:32:56 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Muhawenayo", "Gedeon", ""], ["Gkioxari", "Georgia", ""]]}, {"id": "2102.02902", "submitter": "Akila De Silva", "authors": "Akila de Silva, Issei Mori, Gregory Dusek, James Davis and Alex Pang", "title": "Automated Rip Current Detection with Region based Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper presents a machine learning approach for the automatic\nidentification of rip currents with breaking waves. Rip currents are dangerous\nfast moving currents of water that result in many deaths by sweeping people out\nto sea. Most people do not know how to recognize rip currents in order to avoid\nthem. Furthermore, efforts to forecast rip currents are hindered by lack of\nobservations to help train and validate hazard models. The presence of web cams\nand smart phones have made video and still imagery of the coast ubiquitous and\nprovide a potential source of rip current observations. These same devices\ncould aid public awareness of the presence of rip currents. What is lacking is\na method to detect the presence or absence of rip currents from coastal\nimagery. This paper provides expert labeled training and test data sets for rip\ncurrents. We use Faster-RCNN and a custom temporal aggregation stage to make\ndetections from still images or videos with higher measured accuracy than both\nhumans and other methods of rip current detection previously reported in the\nliterature.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 21:39:49 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["de Silva", "Akila", ""], ["Mori", "Issei", ""], ["Dusek", "Gregory", ""], ["Davis", "James", ""], ["Pang", "Alex", ""]]}, {"id": "2102.02912", "submitter": "Karthik Shetty", "authors": "Karthik Shetty, Annette Birkhold, Norbert Strobel, Bernhard Egger,\n  Srikrishna Jaganathan, Markus Kowarschik, Andreas Maier", "title": "Deep Learning compatible Differentiable X-ray Projections for Inverse\n  Rendering", "comments": "7 pages, 3 figures, Accepted for Bildverarbeitung f\\\"ur die Medizin\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many minimally invasive interventional procedures still rely on 2D\nfluoroscopic imaging. Generating a patient-specific 3D model from these X-ray\nprojection data would allow to improve the procedural workflow, e.g. by\nproviding assistance functions such as automatic positioning. To accomplish\nthis, two things are required. First, a statistical human shape model of the\nhuman anatomy and second, a differentiable X-ray renderer. In this work, we\npropose a differentiable renderer by deriving the distance travelled by a ray\ninside mesh structures to generate a distance map. To demonstrate its\nfunctioning, we use it for simulating X-ray images from human shape models.\nThen we show its application by solving the inverse problem, namely\nreconstructing 3D models from real 2D fluoroscopy images of the pelvis, which\nis an ideal anatomical structure for patient registration. This is accomplished\nby an iterative optimization strategy using gradient descent. With the majority\nof the pelvis being in the fluoroscopic field of view, we achieve a mean\nHausdorff distance of 30 mm between the reconstructed model and the ground\ntruth segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 22:06:05 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Shetty", "Karthik", ""], ["Birkhold", "Annette", ""], ["Strobel", "Norbert", ""], ["Egger", "Bernhard", ""], ["Jaganathan", "Srikrishna", ""], ["Kowarschik", "Markus", ""], ["Maier", "Andreas", ""]]}, {"id": "2102.02913", "submitter": "Yang Yang", "authors": "Yadong Lu, Yinhao Zhu, Yang Yang, Amir Said, Taco S Cohen", "title": "Progressive Neural Image Compression with Nested Quantization and Latent\n  Ordering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present PLONQ, a progressive neural image compression scheme which pushes\nthe boundary of variable bitrate compression by allowing quality scalable\ncoding with a single bitstream. In contrast to existing learned variable\nbitrate solutions which produce separate bitstreams for each quality, it\nenables easier rate-control and requires less storage. Leveraging the latent\nscaling based variable bitrate solution, we introduce nested quantization, a\nmethod that defines multiple quantization levels with nested quantization\ngrids, and progressively refines all latents from the coarsest to the finest\nquantization level. To achieve finer progressiveness in between any two\nquantization levels, latent elements are incrementally refined with an\nimportance ordering defined in the rate-distortion sense. To the best of our\nknowledge, PLONQ is the first learning-based progressive image coding scheme\nand it outperforms SPIHT, a well-known wavelet-based progressive image codec.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 22:06:13 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Lu", "Yadong", ""], ["Zhu", "Yinhao", ""], ["Yang", "Yang", ""], ["Said", "Amir", ""], ["Cohen", "Taco S", ""]]}, {"id": "2102.02956", "submitter": "Chong Xiang", "authors": "Chong Xiang, Prateek Mittal", "title": "DetectorGuard: Provably Securing Object Detectors against Localized\n  Patch Hiding Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art object detectors are vulnerable to localized patch hiding\nattacks where an adversary introduces a small adversarial patch to make\ndetectors miss the detection of salient objects. The patch attacker can carry\nout a physical-world attack by printing and attaching an adversarial patch to\nthe victim object. In this paper, we propose DetectorGuard, the first general\nframework for building provably robust detectors against localized patch hiding\nattacks. To start with, we aim to take advantage of recent advancements of\nrobust image classification research by asking: can we adapt robust image\nclassifiers for robust object detection? Unfortunately, due to their task\ndifference, an object detector naively adapted from a robust image classifier\n1) may not necessarily be robust in the adversarial setting or 2) even maintain\ndecent performance in the clean setting. To build a high-performance robust\nobject detector, we propose an objectness explaining strategy: we adapt a\nrobust image classifier to predict objectness for every image location and then\nexplain each objectness using the bounding boxes predicted by a conventional\nobject detector. If all objectness is well explained, we output the predictions\nmade by the conventional object detector; otherwise, we issue an attack alert.\nNotably, 1) in the adversarial setting, we formally prove the end-to-end\nrobustness of DetectorGuard on certified objects, i.e., it either detects the\nobject or triggers an alert, against any patch hiding attacker within our\nthreat model; 2) in the clean setting, we have almost the same performance as\nstate-of-the-art object detectors. Our evaluation on the PASCAL VOC, MS COCO,\nand KITTI datasets further demonstrates that DetectorGuard achieves the first\nprovable robustness against localized patch hiding attacks at a negligible cost\n(<1%) of clean performance.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 02:02:21 GMT"}, {"version": "v2", "created": "Sun, 9 May 2021 13:14:02 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Xiang", "Chong", ""], ["Mittal", "Prateek", ""]]}, {"id": "2102.02963", "submitter": "Hong Chen", "authors": "Hong Chen, Yifei Huang, Hiroya Takamura, Hideki Nakayama", "title": "Commonsense Knowledge Aware Concept Selection For Diverse and\n  Informative Visual Storytelling", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual storytelling is a task of generating relevant and interesting stories\nfor given image sequences. In this work we aim at increasing the diversity of\nthe generated stories while preserving the informative content from the images.\nWe propose to foster the diversity and informativeness of a generated story by\nusing a concept selection module that suggests a set of concept candidates.\nThen, we utilize a large scale pre-trained model to convert concepts and images\ninto full stories. To enrich the candidate concepts, a commonsense knowledge\ngraph is created for each image sequence from which the concept candidates are\nproposed. To obtain appropriate concepts from the graph, we propose two novel\nmodules that consider the correlation among candidate concepts and the\nimage-concept correlation. Extensive automatic and human evaluation results\ndemonstrate that our model can produce reasonable concepts. This enables our\nmodel to outperform the previous models by a large margin on the diversity and\ninformativeness of the story, while retaining the relevance of the story to the\nimage sequence.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 02:15:28 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Chen", "Hong", ""], ["Huang", "Yifei", ""], ["Takamura", "Hiroya", ""], ["Nakayama", "Hideki", ""]]}, {"id": "2102.02971", "submitter": "Shukan Liu", "authors": "Shukan Liu, Ruilin Xu, Boying Geng, Qiao Sun, Li Duan, and Yiming Liu", "title": "Metaknowledge Extraction Based on Multi-Modal Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The triple-based knowledge in large-scale knowledge bases is most likely\nlacking in structural logic and problematic of conducting knowledge hierarchy.\nIn this paper, we introduce the concept of metaknowledge to knowledge\nengineering research for the purpose of structural knowledge construction.\nTherefore, the Metaknowledge Extraction Framework and Document Structure Tree\nmodel are presented to extract and organize metaknowledge elements (titles,\nauthors, abstracts, sections, paragraphs, etc.), so that it is feasible to\nextract the structural knowledge from multi-modal documents. Experiment results\nhave proved the effectiveness of metaknowledge elements extraction by our\nframework. Meanwhile, detailed examples are given to demonstrate what exactly\nmetaknowledge is and how to generate it. At the end of this paper, we propose\nand analyze the task flow of metaknowledge applications and the associations\nbetween knowledge and metaknowledge.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 03:04:36 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Liu", "Shukan", ""], ["Xu", "Ruilin", ""], ["Geng", "Boying", ""], ["Sun", "Qiao", ""], ["Duan", "Li", ""], ["Liu", "Yiming", ""]]}, {"id": "2102.02972", "submitter": "Jilin Tang", "authors": "Jilin Tang, Yi Yuan, Tianjia Shao, Yong Liu, Mengmeng Wang, Kun Zhou", "title": "Structure-aware Person Image Generation with Pose Decomposition and\n  Semantic Correlation", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we tackle the problem of pose guided person image generation,\nwhich aims to transfer a person image from the source pose to a novel target\npose while maintaining the source appearance. Given the inefficiency of\nstandard CNNs in handling large spatial transformation, we propose a\nstructure-aware flow based method for high-quality person image generation.\nSpecifically, instead of learning the complex overall pose changes of human\nbody, we decompose the human body into different semantic parts (e.g., head,\ntorso, and legs) and apply different networks to predict the flow fields for\nthese parts separately. Moreover, we carefully design the network modules to\neffectively capture the local and global semantic correlations of features\nwithin and among the human parts respectively. Extensive experimental results\nshow that our method can generate high-quality results under large pose\ndiscrepancy and outperforms state-of-the-art methods in both qualitative and\nquantitative comparisons.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 03:07:57 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Tang", "Jilin", ""], ["Yuan", "Yi", ""], ["Shao", "Tianjia", ""], ["Liu", "Yong", ""], ["Wang", "Mengmeng", ""], ["Zhou", "Kun", ""]]}, {"id": "2102.02996", "submitter": "Jingjing Ren", "authors": "Jingjing Ren and Xiaowei Hu and Lei Zhu and Xuemiao Xu and Yangyang Xu\n  and Weiming Wang and Zijun Deng and Pheng-Ann Heng", "title": "Deep Texture-Aware Features for Camouflaged Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Camouflaged object detection is a challenging task that aims to identify\nobjects having similar texture to the surroundings. This paper presents to\namplify the subtle texture difference between camouflaged objects and the\nbackground for camouflaged object detection by formulating multiple\ntexture-aware refinement modules to learn the texture-aware features in a deep\nconvolutional neural network. The texture-aware refinement module computes the\ncovariance matrices of feature responses to extract the texture information,\ndesigns an affinity loss to learn a set of parameter maps that help to separate\nthe texture between camouflaged objects and the background, and adopts a\nboundary-consistency loss to explore the object detail structures.We evaluate\nour network on the benchmark dataset for camouflaged object detection both\nqualitatively and quantitatively. Experimental results show that our approach\noutperforms various state-of-the-art methods by a large margin.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 04:38:32 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Ren", "Jingjing", ""], ["Hu", "Xiaowei", ""], ["Zhu", "Lei", ""], ["Xu", "Xuemiao", ""], ["Xu", "Yangyang", ""], ["Wang", "Weiming", ""], ["Deng", "Zijun", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "2102.03011", "submitter": "Oliver Wang", "authors": "Felix Klose and Oliver Wang and Jean-Charles Bazin and Marcus Magnor\n  and Alexander Sorkine-Hornung", "title": "Sampling Based Scene-Space Video Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many compelling video processing effects can be achieved if per-pixel depth\ninformation and 3D camera calibrations are known. However, the success of such\nmethods is highly dependent on the accuracy of this \"scene-space\" information.\nWe present a novel, sampling-based framework for processing video that enables\nhigh-quality scene-space video effects in the presence of inevitable errors in\ndepth and camera pose estimation. Instead of trying to improve the explicit 3D\nscene representation, the key idea of our method is to exploit the high\nredundancy of approximate scene information that arises due to most scene\npoints being visible multiple times across many frames of video. Based on this\nobservation, we propose a novel pixel gathering and filtering approach. The\ngathering step is general and collects pixel samples in scene-space, while the\nfiltering step is application-specific and computes a desired output video from\nthe gathered sample sets. Our approach is easily parallelizable and has been\nimplemented on GPU, allowing us to take full advantage of large volumes of\nvideo data and facilitating practical runtimes on HD video using a standard\ndesktop computer. Our generic scene-space formulation is able to\ncomprehensively describe a multitude of video processing applications such as\ndenoising, deblurring, super resolution, object removal, computational shutter\nfunctions, and other scene-space camera effects. We present results for various\ncasually captured, hand-held, moving, compressed, monocular videos depicting\nchallenging scenes recorded in uncontrolled environments.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 05:55:04 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Klose", "Felix", ""], ["Wang", "Oliver", ""], ["Bazin", "Jean-Charles", ""], ["Magnor", "Marcus", ""], ["Sorkine-Hornung", "Alexander", ""]]}, {"id": "2102.03026", "submitter": "Chunhua Shen", "authors": "Zhi Tian, Bowen Zhang, Hao Chen, Chunhua Shen", "title": "Instance and Panoptic Segmentation Using Conditional Convolutions", "comments": "Fixing typos. Extended version of arXiv:2003.05664", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a simple yet effective framework for instance and panoptic\nsegmentation, termed CondInst (conditional convolutions for instance and\npanoptic segmentation). In the literature, top-performing instance segmentation\nmethods typically follow the paradigm of Mask R-CNN and rely on ROI operations\n(typically ROIAlign) to attend to each instance. In contrast, we propose to\nattend to the instances with dynamic conditional convolutions. Instead of using\ninstance-wise ROIs as inputs to the instance mask head of fixed weights, we\ndesign dynamic instance-aware mask heads, conditioned on the instances to be\npredicted. CondInst enjoys three advantages: 1.) Instance and panoptic\nsegmentation are unified into a fully convolutional network, eliminating the\nneed for ROI cropping and feature alignment. 2.) The elimination of the ROI\ncropping also significantly improves the output instance mask resolution. 3.)\nDue to the much improved capacity of dynamically-generated conditional\nconvolutions, the mask head can be very compact (e.g., 3 conv. layers, each\nhaving only 8 channels), leading to significantly faster inference time per\ninstance and making the overall inference time almost constant, irrelevant to\nthe number of instances. We demonstrate a simpler method that can achieve\nimproved accuracy and inference speed on both instance and panoptic\nsegmentation tasks. On the COCO dataset, we outperform a few state-of-the-art\nmethods. We hope that CondInst can be a strong baseline for instance and\npanoptic segmentation. Code is available at: https://git.io/AdelaiDet\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 06:57:02 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 01:09:20 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Tian", "Zhi", ""], ["Zhang", "Bowen", ""], ["Chen", "Hao", ""], ["Shen", "Chunhua", ""]]}, {"id": "2102.03065", "submitter": "Jang-Hyun Kim", "authors": "Jang-Hyun Kim, Wonho Choo, Hosan Jeong, Hyun Oh Song", "title": "Co-Mixup: Saliency Guided Joint Mixup with Supermodular Diversity", "comments": "Published at ICLR 2021 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep neural networks show great performance on fitting to the training\ndistribution, improving the networks' generalization performance to the test\ndistribution and robustness to the sensitivity to input perturbations still\nremain as a challenge. Although a number of mixup based augmentation strategies\nhave been proposed to partially address them, it remains unclear as to how to\nbest utilize the supervisory signal within each input data for mixup from the\noptimization perspective. We propose a new perspective on batch mixup and\nformulate the optimal construction of a batch of mixup data maximizing the data\nsaliency measure of each individual mixup data and encouraging the supermodular\ndiversity among the constructed mixup data. This leads to a novel discrete\noptimization problem minimizing the difference between submodular functions. We\nalso propose an efficient modular approximation based iterative submodular\nminimization algorithm for efficient mixup computation per each minibatch\nsuitable for minibatch based neural network training. Our experiments show the\nproposed method achieves the state of the art generalization, calibration, and\nweakly supervised localization results compared to other mixup methods. The\nsource code is available at https://github.com/snu-mllab/Co-Mixup.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 09:12:02 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Kim", "Jang-Hyun", ""], ["Choo", "Wonho", ""], ["Jeong", "Hosan", ""], ["Song", "Hyun Oh", ""]]}, {"id": "2102.03082", "submitter": "Harshana Habaragamuwa", "authors": "Harshana Habaragamuwa, Yu Oishi, Kenichi Tanaka", "title": "Achieving Explainability for Plant Disease Classification with\n  Disentangled Variational Autoencoders", "comments": "45 pages, 21 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agricultural image recognition tasks are becoming increasingly dependent on\ndeep learning (DL). Despite its excellent performance, it is difficult to\ncomprehend what type of logic or features DL uses in its decision making. This\nhas become a roadblock for the implementation and development of DL-based image\nrecognition methods because knowing the logic or features used in decision\nmaking, such as in a classification task, is very important for verification,\nalgorithm improvement, training data improvement, knowledge extraction, etc. To\nmitigate such problems, we developed a classification method based on a\nvariational autoencoder architecture that can show not only the location of the\nmost important features but also what variations of that particular feature are\nused. Using the PlantVillage dataset, we achieved an acceptable level of\nexplainability without sacrificing the accuracy of the classification. Although\nthe proposed method was tested for disease diagnosis in some crops, the method\ncan be extended to other crops as well as other image classification tasks. In\nthe future, we hope to use this explainable artificial intelligence algorithm\nin disease identification tasks, such as the identification of potato blackleg\ndisease and potato virus Y (PVY), and other image classification tasks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 10:04:00 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 01:26:55 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Habaragamuwa", "Harshana", ""], ["Oishi", "Yu", ""], ["Tanaka", "Kenichi", ""]]}, {"id": "2102.03099", "submitter": "Ye Lyu", "authors": "Ye Lyu, George Vosselman, Gui-Song Xia, Michael Ying Yang", "title": "Bidirectional Multi-scale Attention Networks for Semantic Segmentation\n  of Oblique UAV Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation for aerial platforms has been one of the fundamental\nscene understanding task for the earth observation. Most of the semantic\nsegmentation research focused on scenes captured in nadir view, in which\nobjects have relatively smaller scale variation compared with scenes captured\nin oblique view. The huge scale variation of objects in oblique images limits\nthe performance of deep neural networks (DNN) that process images in a single\nscale fashion. In order to tackle the scale variation issue, in this paper, we\npropose the novel bidirectional multi-scale attention networks, which fuse\nfeatures from multiple scales bidirectionally for more adaptive and effective\nfeature extraction. The experiments are conducted on the UAVid2020 dataset and\nhave shown the effectiveness of our method. Our model achieved the\nstate-of-the-art (SOTA) result with a mean intersection over union (mIoU) score\nof 70.80%.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 11:02:15 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Lyu", "Ye", ""], ["Vosselman", "George", ""], ["Xia", "Gui-Song", ""], ["Yang", "Michael Ying", ""]]}, {"id": "2102.03111", "submitter": "Tongxue Zhou", "authors": "Tongxue Zhou, St\\'ephane Canu, Pierre Vera and Su Ruan", "title": "3D Medical Multi-modal Segmentation Network Guided by Multi-source\n  Correlation Constraint", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of multimodal segmentation, the correlation between different\nmodalities can be considered for improving the segmentation results. In this\npaper, we propose a multi-modality segmentation network with a correlation\nconstraint. Our network includes N model-independent encoding paths with N\nimage sources, a correlation constraint block, a feature fusion block, and a\ndecoding path. The model independent encoding path can capture\nmodality-specific features from the N modalities. Since there exists a strong\ncorrelation between different modalities, we first propose a linear correlation\nblock to learn the correlation between modalities, then a loss function is used\nto guide the network to learn the correlated features based on the linear\ncorrelation block. This block forces the network to learn the latent correlated\nfeatures which are more relevant for segmentation. Considering that not all the\nfeatures extracted from the encoders are useful for segmentation, we propose to\nuse dual attention based fusion block to recalibrate the features along the\nmodality and spatial paths, which can suppress less informative features and\nemphasize the useful ones. The fused feature representation is finally\nprojected by the decoder to obtain the segmentation result. Our experiment\nresults tested on BraTS-2018 dataset for brain tumor segmentation demonstrate\nthe effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 11:23:12 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Zhou", "Tongxue", ""], ["Canu", "St\u00e9phane", ""], ["Vera", "Pierre", ""], ["Ruan", "Su", ""]]}, {"id": "2102.03113", "submitter": "Andreas Aakerberg", "authors": "Andreas Aakerberg, Kamal Nasrollahi, Thomas B. Moeslund", "title": "Real-World Super-Resolution of Face-Images from Surveillance Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing face image Super-Resolution (SR) methods assume that the\nLow-Resolution (LR) images were artificially downsampled from High-Resolution\n(HR) images with bicubic interpolation. This operation changes the natural\nimage characteristics and reduces noise. Hence, SR methods trained on such data\nmost often fail to produce good results when applied to real LR images. To\nsolve this problem, we propose a novel framework for generation of realistic\nLR/HR training pairs. Our framework estimates realistic blur kernels, noise\ndistributions, and JPEG compression artifacts to generate LR images with\nsimilar image characteristics as the ones in the source domain. This allows us\nto train a SR model using high quality face images as Ground-Truth (GT). For\nbetter perceptual quality we use a Generative Adversarial Network (GAN) based\nSR model where we have exchanged the commonly used VGG-loss [24] with\nLPIPS-loss [52]. Experimental results on both real and artificially corrupted\nface images show that our method results in more detailed reconstructions with\nless noise compared to existing State-of-the-Art (SoTA) methods. In addition,\nwe show that the traditional non-reference Image Quality Assessment (IQA)\nmethods fail to capture this improvement and demonstrate that the more recent\nNIMA metric [16] correlates better with human perception via Mean Opinion Rank\n(MOR).\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 11:38:30 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Aakerberg", "Andreas", ""], ["Nasrollahi", "Kamal", ""], ["Moeslund", "Thomas B.", ""]]}, {"id": "2102.03115", "submitter": "Nibaran Das", "authors": "Md Osman Gani, Somenath Kuiry, Alaka Das, Mita Nasipuri, Nibaran Das", "title": "Multispectral Object Detection with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Object detection in natural scenes can be a challenging task. In many\nreal-life situations, the visible spectrum is not suitable for traditional\ncomputer vision tasks. Moving outside the visible spectrum range, such as the\nthermal spectrum or the near-infrared (NIR) images, is much more beneficial in\nlow visibility conditions, NIR images are very helpful for understanding the\nobject's material quality. In this work, we have taken images with both the\nThermal and NIR spectrum for the object detection task. As multi-spectral data\nwith both Thermal and NIR is not available for the detection task, we needed to\ncollect data ourselves. Data collection is a time-consuming process, and we\nfaced many obstacles that we had to overcome. We train the YOLO v3 network from\nscratch to detect an object from multi-spectral images. Also, to avoid\noverfitting, we have done data augmentation and tune hyperparameters.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 11:39:14 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Gani", "Md Osman", ""], ["Kuiry", "Somenath", ""], ["Das", "Alaka", ""], ["Nasipuri", "Mita", ""], ["Das", "Nibaran", ""]]}, {"id": "2102.03137", "submitter": "Herv\\'e Le Borgne", "authors": "Yannick Le Cacheux and Herv\\'e Le Borgne and Michel Crucianu", "title": "Zero-shot Learning with Deep Neural Networks for Object Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Zero-shot learning deals with the ability to recognize objects without any\nvisual training sample. To counterbalance this lack of visual data, each class\nto recognize is associated with a semantic prototype that reflects the\nessential features of the object. The general approach is to learn a mapping\nfrom visual data to semantic prototypes, then use it at inference to classify\nvisual samples from the class prototypes only. Different settings of this\ngeneral configuration can be considered depending on the use case of interest,\nin particular whether one only wants to classify objects that have not been\nemployed to learn the mapping or whether one can use unlabelled visual examples\nto learn the mapping. This chapter presents a review of the approaches based on\ndeep neural networks to tackle the ZSL problem. We highlight findings that had\na large impact on the evolution of this domain and list its current challenges.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 12:27:42 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Cacheux", "Yannick Le", ""], ["Borgne", "Herv\u00e9 Le", ""], ["Crucianu", "Michel", ""]]}, {"id": "2102.03141", "submitter": "Tobias Hinz", "authors": "Tobias Hinz and Matthew Fisher and Oliver Wang and Eli Shechtman and\n  Stefan Wermter", "title": "CharacterGAN: Few-Shot Keypoint Character Animation and Reposing", "comments": "Code and supplementary material can be found at\n  https://github.com/tohinz/CharacterGAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We introduce CharacterGAN, a generative model that can be trained on only a\nfew samples (8 - 15) of a given character. Our model generates novel poses\nbased on keypoint locations, which can be modified in real time while providing\ninteractive feedback, allowing for intuitive reposing and animation. Since we\nonly have very limited training samples, one of the key challenges lies in how\nto address (dis)occlusions, e.g. when a hand moves behind or in front of a\nbody. To address this, we introduce a novel layering approach which explicitly\nsplits the input keypoints into different layers which are processed\nindependently. These layers represent different parts of the character and\nprovide a strong implicit bias that helps to obtain realistic results even with\nstrong (dis)occlusions. To combine the features of individual layers we use an\nadaptive scaling approach conditioned on all keypoints. Finally, we introduce a\nmask connectivity constraint to reduce distortion artifacts that occur with\nextreme out-of-distribution poses at test time. We show that our approach\noutperforms recent baselines and creates realistic animations for diverse\ncharacters. We also show that our model can handle discrete state changes, for\nexample a profile facing left or right, that the different layers do indeed\nlearn features specific for the respective keypoints in those layers, and that\nour model scales to larger datasets when more data is available.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 12:38:15 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 11:12:28 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Hinz", "Tobias", ""], ["Fisher", "Matthew", ""], ["Wang", "Oliver", ""], ["Shechtman", "Eli", ""], ["Wermter", "Stefan", ""]]}, {"id": "2102.03156", "submitter": "Quentin Bouniot", "authors": "Quentin Bouniot, Romaric Audigier, Ang\\'elique Loesch", "title": "Optimal Transport as a Defense Against Adversarial Attacks", "comments": "Accepted at ICPR2020. Code is available at\n  https://github.com/CEA-LIST/adv-sat", "journal-ref": null, "doi": "10.1109/ICPR48806.2021.9413327", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning classifiers are now known to have flaws in the representations\nof their class. Adversarial attacks can find a human-imperceptible perturbation\nfor a given image that will mislead a trained model. The most effective methods\nto defend against such attacks trains on generated adversarial examples to\nlearn their distribution. Previous work aimed to align original and adversarial\nimage representations in the same way as domain adaptation to improve\nrobustness. Yet, they partially align the representations using approaches that\ndo not reflect the geometry of space and distribution. In addition, it is\ndifficult to accurately compare robustness between defended models. Until now,\nthey have been evaluated using a fixed perturbation size. However, defended\nmodels may react differently to variations of this perturbation size. In this\npaper, the analogy of domain adaptation is taken a step further by exploiting\noptimal transport theory. We propose to use a loss between distributions that\nfaithfully reflect the ground distance. This leads to SAT (Sinkhorn Adversarial\nTraining), a more robust defense against adversarial attacks. Then, we propose\nto quantify more precisely the robustness of a model to adversarial attacks\nover a wide range of perturbation sizes using a different metric, the Area\nUnder the Accuracy Curve (AUAC). We perform extensive experiments on both\nCIFAR-10 and CIFAR-100 datasets and show that our defense is globally more\nrobust than the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 13:24:36 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 08:06:59 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Bouniot", "Quentin", ""], ["Audigier", "Romaric", ""], ["Loesch", "Ang\u00e9lique", ""]]}, {"id": "2102.03176", "submitter": "Ryan Furlong", "authors": "Ryan Furlong, Vincent O'Brien, James Garland, Francisco\n  Dominguez-Mateos", "title": "Metric Embedding Sub-discrimination Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep metric learning is a technique used in a variety of discriminative tasks\nto achieve zero-shot, one-shot or few-shot learning. When applied, the system\nlearns an embedding space where a non-parametric approach, such as \\gls{knn},\ncan be used to discriminate features during test time. This work focuses on\ninvestigating to what extent feature information contained within this\nembedding space can be used to carry out sub-discrimination in the feature\nspace. The study shows that within a discrimination embedding, the information\non the salient attributes needed to solve the problem of sub-discrimination is\nsaved within the embedding and that this inherent information can be used to\ncarry out sub-discriminative tasks. To demonstrate this, an embedding designed\ninitially to discriminate faces is used to differentiate several attributes\nsuch as gender, age and skin tone, without any additional training. The study\nis split into two study cases: intra class discrimination where all the\nembeddings took into consideration are from the same identity; and extra class\ndiscrimination where the embeddings represent different identities. After the\nstudy, it is shown that it is possible to infer common attributes to different\nidentities. The system can also perform extra class sub-discrimination with a\nhigh accuracy rate, notably 99.3\\%, 99.3\\% and 94.1\\% for gender, skin tone,\nand age, respectively. Intra class tests show more mixed results with more\nnuanced attributes like emotions not being reliably classified, while more\ndistinct attributes such as thick-framed glasses and beards, achieving 97.2\\%\nand 95.8\\% accuracy, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 13:53:10 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Furlong", "Ryan", ""], ["O'Brien", "Vincent", ""], ["Garland", "James", ""], ["Dominguez-Mateos", "Francisco", ""]]}, {"id": "2102.03213", "submitter": "Lucas Prado Osco", "authors": "Diogo Nunes Gon\\c{c}alves, Mauro dos Santos de Arruda, Hemerson\n  Pistori, Vanessa Jord\\~ao Marcato Fernandes, Ana Paula Marques Ramos,\n  Danielle Elis Garcia Furuya, Lucas Prado Osco, Hongjie He, Jonathan Li,\n  Jos\\'e Marcato Junior, Wesley Nunes Gon\\c{c}alves", "title": "A Deep Learning Approach Based on Graphs to Detect Plantation Lines", "comments": "19 pages, 11 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep learning-based networks are among the most prominent methods to learn\nlinear patterns and extract this type of information from diverse imagery\nconditions. Here, we propose a deep learning approach based on graphs to detect\nplantation lines in UAV-based RGB imagery presenting a challenging scenario\ncontaining spaced plants. The first module of our method extracts a feature map\nthroughout the backbone, which consists of the initial layers of the VGG16.\nThis feature map is used as an input to the Knowledge Estimation Module (KEM),\norganized in three concatenated branches for detecting 1) the plant positions,\n2) the plantation lines, and 3) for the displacement vectors between the\nplants. A graph modeling is applied considering each plant position on the\nimage as vertices, and edges are formed between two vertices (i.e. plants).\nFinally, the edge is classified as pertaining to a certain plantation line\nbased on three probabilities (higher than 0.5): i) in visual features obtained\nfrom the backbone; ii) a chance that the edge pixels belong to a line, from the\nKEM step; and iii) an alignment of the displacement vectors with the edge, also\nfrom KEM. Experiments were conducted in corn plantations with different growth\nstages and patterns with aerial RGB imagery. A total of 564 patches with 256 x\n256 pixels were used and randomly divided into training, validation, and\ntesting sets in a proportion of 60\\%, 20\\%, and 20\\%, respectively. The\nproposed method was compared against state-of-the-art deep learning methods,\nand achieved superior performance with a significant margin, returning\nprecision, recall, and F1-score of 98.7\\%, 91.9\\%, and 95.1\\%, respectively.\nThis approach is useful in extracting lines with spaced plantation patterns and\ncould be implemented in scenarios where plantation gaps occur, generating lines\nwith few-to-none interruptions.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 14:56:42 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Gon\u00e7alves", "Diogo Nunes", ""], ["de Arruda", "Mauro dos Santos", ""], ["Pistori", "Hemerson", ""], ["Fernandes", "Vanessa Jord\u00e3o Marcato", ""], ["Ramos", "Ana Paula Marques", ""], ["Furuya", "Danielle Elis Garcia", ""], ["Osco", "Lucas Prado", ""], ["He", "Hongjie", ""], ["Li", "Jonathan", ""], ["Junior", "Jos\u00e9 Marcato", ""], ["Gon\u00e7alves", "Wesley Nunes", ""]]}, {"id": "2102.03214", "submitter": "Sixing Yu", "authors": "Sixing Yu, Arya Mazaheri, Ali Jannesari", "title": "GNN-RL Compression: Topology-Aware Network Pruning using Multi-stage\n  Graph Embedding and Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model compression is an essential technique for deploying deep neural\nnetworks (DNNs) on power and memory-constrained resources. However, existing\nmodel-compression methods often rely on human expertise and focus on\nparameters' local importance, ignoring the rich topology information within\nDNNs. In this paper, we propose a novel multi-stage graph embedding technique\nbased on graph neural networks (GNNs) to identify the DNNs' topology and use\nreinforcement learning (RL) to find a suitable compression policy. We performed\nresource-constrained (i.e., FLOPs) channel pruning and compared our approach\nwith state-of-the-art compression methods using over-parameterized DNNs (e.g.,\nResNet and VGG-16) and mobile-friendly DNNs (e.g., MobileNet and ShuffleNet).\nWe evaluated our method on various models from typical to mobile-friendly\nnetworks, such as ResNet family, VGG-16, MobileNet-v1/v2, and ShuffleNet. The\nresults demonstrate that our method can prune dense networks (e.g., VGG-16) by\nup to 80% of their original FLOPs. More importantly, our method outperformed\nstate-of-the-art methods and achieved a higher accuracy by up to 1.84% for\nShuffleNet-v1. Furthermore, following our approach, the pruned VGG-16 achieved\na noticeable 1.38$\\times$ speed up and 141 MB GPU memory reduction.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 14:59:32 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Yu", "Sixing", ""], ["Mazaheri", "Arya", ""], ["Jannesari", "Ali", ""]]}, {"id": "2102.03228", "submitter": "Xuesong Shi", "authors": "Ming Ouyang, Xuesong Shi, Yujie Wang, Yuxin Tian, Yingzhe Shen, Dawei\n  Wang, Peng Wang, Zhiqiang Cao", "title": "A Collaborative Visual SLAM Framework for Service Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid deployment of service robots, a method should be established\nto allow multiple robots to work in the same place to collaborate and share the\nspatial information. To this end, we present a collaborative visual\nsimultaneous localization and mapping (SLAM) framework particularly designed\nfor service robot scenarios. With an edge server maintaining a map database and\nperforming global optimization, each robot can register to an existing map,\nupdate the map, or build new maps, all with a unified interface and low\ncomputation and memory cost. To enable real-time information sharing, we design\na simple but effective communication pipeline and a novel landmark retrieval\nmethod to augment each client's local map with nearby landmarks from the\nserver. The framework is general enough to support both RGB-D and monocular\ncameras, as well as robots with multiple cameras, taking the rigid constraints\nbetween cameras into consideration. The proposed framework has been fully\nimplemented and verified with public datasets and live experiments.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 15:19:07 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 08:55:51 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Ouyang", "Ming", ""], ["Shi", "Xuesong", ""], ["Wang", "Yujie", ""], ["Tian", "Yuxin", ""], ["Shen", "Yingzhe", ""], ["Wang", "Dawei", ""], ["Wang", "Peng", ""], ["Cao", "Zhiqiang", ""]]}, {"id": "2102.03233", "submitter": "Abhishek Sharma", "authors": "Abhishek Sharma, Maks Ovsjanikov", "title": "Matrix Decomposition on Graphs: A Functional View", "comments": "Under Review. arXiv admin note: substantial text overlap with\n  arXiv:2009.14343", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a functional view of matrix decomposition problems on graphs such\nas geometric matrix completion and graph regularized dimensionality reduction.\nOur unifying framework is based on the key idea that using a reduced basis to\nrepresent functions on the product space is sufficient to recover a low rank\nmatrix approximation even from a sparse signal. We validate our framework on\nseveral real and synthetic benchmarks (for both problems) where it either\noutperforms state of the art or achieves competitive results at a fraction of\nthe computational effort of prior work.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 15:28:11 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Sharma", "Abhishek", ""], ["Ovsjanikov", "Maks", ""]]}, {"id": "2102.03239", "submitter": "Christian M. Dahl", "authors": "Christian M. Dahl, Torben S. D. Johansen, Emil N. S{\\o}rensen,\n  Christian E. Westermann and Simon F. Wittrock", "title": "Applications of Machine Learning in Document Digitisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data acquisition forms the primary step in all empirical research. The\navailability of data directly impacts the quality and extent of conclusions and\ninsights. In particular, larger and more detailed datasets provide convincing\nanswers even to complex research questions. The main problem is that 'large and\ndetailed' usually implies 'costly and difficult', especially when the data\nmedium is paper and books. Human operators and manual transcription have been\nthe traditional approach for collecting historical data. We instead advocate\nthe use of modern machine learning techniques to automate the digitisation\nprocess. We give an overview of the potential for applying machine digitisation\nfor data collection through two illustrative applications. The first\ndemonstrates that unsupervised layout classification applied to raw scans of\nnurse journals can be used to construct a treatment indicator. Moreover, it\nallows an assessment of assignment compliance. The second application uses\nattention-based neural networks for handwritten text recognition in order to\ntranscribe age and birth and death dates from a large collection of Danish\ndeath certificates. We describe each step in the digitisation pipeline and\nprovide implementation insights.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 15:35:28 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Dahl", "Christian M.", ""], ["Johansen", "Torben S. D.", ""], ["S\u00f8rensen", "Emil N.", ""], ["Westermann", "Christian E.", ""], ["Wittrock", "Simon F.", ""]]}, {"id": "2102.03243", "submitter": "Rafael Pereira Msc", "authors": "Rafael S. Pereira, Alexis Joly, Patrick Valduriez, Fabio Porto", "title": "Hyperspherical embedding for novel class classification", "comments": "9 pages with 10 figures and 6 tables. Not currently published", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning models have become increasingly useful in many different\nindustries. On the domain of image classification, convolutional neural\nnetworks proved the ability to learn robust features for the closed set\nproblem, as shown in many different datasets, such as MNIST FASHIONMNIST,\nCIFAR10, CIFAR100, and IMAGENET. These approaches use deep neural networks with\ndense layers with softmax activation functions in order to learn features that\ncan separate classes in a latent space. However, this traditional approach is\nnot useful for identifying classes unseen on the training set, known as the\nopen set problem. A similar problem occurs in scenarios involving learning on\nsmall data. To tackle both problems, few-shot learning has been proposed. In\nparticular, metric learning learns features that obey constraints of a metric\ndistance in the latent space in order to perform classification. However, while\nthis approach proves to be useful for the open set problem, current\nimplementation requires pair-wise training, where both positive and negative\nexamples of similar images are presented during the training phase, which\nlimits the applicability of these approaches in large data or large class\nscenarios given the combinatorial nature of the possible inputs.In this paper,\nwe present a constraint-based approach applied to the representations in the\nlatent space under the normalized softmax loss, proposed by[18]. We\nexperimentally validate the proposed approach for the classification of unseen\nclasses on different datasets using both metric learning and the normalized\nsoftmax loss, on disjoint and joint scenarios. Our results show that not only\nour proposed strategy can be efficiently trained on larger set of classes, as\nit does not require pairwise learning, but also present better classification\nresults than the metric learning strategies surpassing its accuracy by a\nsignificant margin.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 15:42:13 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Pereira", "Rafael S.", ""], ["Joly", "Alexis", ""], ["Valduriez", "Patrick", ""], ["Porto", "Fabio", ""]]}, {"id": "2102.03247", "submitter": "Hanqing Chao", "authors": "Hanqing Chao, Kun Wang, Yiwei He, Junping Zhang, Jianfeng Feng", "title": "GaitSet: Cross-view Gait Recognition through Utilizing Gait as a Deep\n  Set", "comments": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI). Journal version of arXiv:1811.06186 (AAAI 2019). Code\n  is available at https://github.com/AbnerHqC/GaitSet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gait is a unique biometric feature that can be recognized at a distance;\nthus, it has broad applications in crime prevention, forensic identification,\nand social security. To portray a gait, existing gait recognition methods\nutilize either a gait template which makes it difficult to preserve temporal\ninformation, or a gait sequence that maintains unnecessary sequential\nconstraints and thus loses the flexibility of gait recognition. In this paper,\nwe present a novel perspective that utilizes gait as a deep set, which means\nthat a set of gait frames are integrated by a global-local fused deep network\ninspired by the way our left- and right-hemisphere processes information to\nlearn information that can be used in identification. Based on this deep set\nperspective, our method is immune to frame permutations, and can naturally\nintegrate frames from different videos that have been acquired under different\nscenarios, such as diverse viewing angles, different clothes, or different\nitem-carrying conditions. Experiments show that under normal walking\nconditions, our single-model method achieves an average rank-1 accuracy of\n96.1% on the CASIA-B gait dataset and an accuracy of 87.9% on the OU-MVLP gait\ndataset. Under various complex scenarios, our model also exhibits a high level\nof robustness. It achieves accuracies of 90.8% and 70.3% on CASIA-B under\nbag-carrying and coat-wearing walking conditions respectively, significantly\noutperforming the best existing methods. Moreover, the proposed method\nmaintains a satisfactory accuracy even when only small numbers of frames are\navailable in the test samples; for example, it achieves 85.0% on CASIA-B even\nwhen using only 7 frames. The source code has been released at\nhttps://github.com/AbnerHqC/GaitSet.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 15:49:54 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Chao", "Hanqing", ""], ["Wang", "Kun", ""], ["He", "Yiwei", ""], ["Zhang", "Junping", ""], ["Feng", "Jianfeng", ""]]}, {"id": "2102.03266", "submitter": "Federico Marmoreo", "authors": "Federico Marmoreo, Jacopo Cavazza, Vittorio Murino", "title": "Transductive Zero-Shot Learning by Decoupled Feature Generation", "comments": "Published at the IEEE/CVF Winter Conference on Computer Vision (WACV)\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address zero-shot learning (ZSL), the problem of\nrecognizing categories for which no labeled visual data are available during\ntraining. We focus on the transductive setting, in which unlabelled visual data\nfrom unseen classes is available. State-of-the-art paradigms in ZSL typically\nexploit generative adversarial networks to synthesize visual features from\nsemantic attributes. We posit that the main limitation of these approaches is\nto adopt a single model to face two problems: 1) generating realistic visual\nfeatures, and 2) translating semantic attributes into visual cues. Differently,\nwe propose to decouple such tasks, solving them separately. In particular, we\ntrain an unconditional generator to solely capture the complexity of the\ndistribution of visual data and we subsequently pair it with a conditional\ngenerator devoted to enrich the prior knowledge of the data distribution with\nthe semantic content of the class embeddings. We present a detailed ablation\nstudy to dissect the effect of our proposed decoupling approach, while\ndemonstrating its superiority over the related state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 16:17:52 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 17:20:55 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Marmoreo", "Federico", ""], ["Cavazza", "Jacopo", ""], ["Murino", "Vittorio", ""]]}, {"id": "2102.03285", "submitter": "Pierluigi Zama Ramirez", "authors": "Pierluigi Zama Ramirez, Alessio Tonioni, Federico Tombari", "title": "Unsupervised Novel View Synthesis from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel view synthesis from a single image aims at generating novel views from\na single input image of an object. Several works recently achieved remarkable\nresults, though require some form of multi-view supervision at training time,\ntherefore limiting their deployment in real scenarios. This work aims at\nrelaxing this assumption enabling training of conditional generative model for\nnovel view synthesis in a completely unsupervised manner. We first pre-train a\npurely generative decoder model using a GAN formulation while at the same time\ntraining an encoder network to invert the mapping from latent code to images.\nThen we swap encoder and decoder and train the network as a conditioned GAN\nwith a mixture of auto-encoder-like objective and self-distillation. At test\ntime, given a view of an object, our model first embeds the image content in a\nlatent code and regresses its pose w.r.t. a canonical reference system, then\ngenerates novel views of it by keeping the code and varying the pose. We show\nthat our framework achieves results comparable to the state of the art on\nShapeNet and that it can be employed on unconstrained collections of natural\nimages, where no competing method can be trained.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 16:56:04 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Ramirez", "Pierluigi Zama", ""], ["Tonioni", "Alessio", ""], ["Tombari", "Federico", ""]]}, {"id": "2102.03310", "submitter": "Michal Ciszewski", "authors": "Micha{\\l} Ciszewski, Jakob S\\\"ohl, Geurt Jongbloed", "title": "Improving state estimation through projection post-processing for\n  activity recognition in football", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The past decade has seen an increased interest in human activity recognition.\nMost commonly, the raw data coming from sensors attached to body parts are\nunannotated, which creates a need for fast labelling method. Part of the\nprocedure is choosing or designing an appropriate performance measure. We\npropose a new performance measure, the Locally Time-Shifted Measure, which\naddresses the issue of timing uncertainty of state transitions in the\nclassification result. Our main contribution is a novel post-processing method\nfor binary activity recognition. It improves the accuracy of the classification\nmethods, by correcting for unrealistically short activities in the estimate.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 17:32:39 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 09:43:01 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Ciszewski", "Micha\u0142", ""], ["S\u00f6hl", "Jakob", ""], ["Jongbloed", "Geurt", ""]]}, {"id": "2102.03326", "submitter": "Franck Davoine", "authors": "Edouard Capellier, Franck Davoine, Veronique Cherfaoui, You Li", "title": "Fusion of neural networks, for LIDAR-based evidential road mapping", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LIDAR sensors are usually used to provide autonomous vehicles with 3D\nrepresentations of their environment. In ideal conditions, geometrical models\ncould detect the road in LIDAR scans, at the cost of a manual tuning of\nnumerical constraints, and a lack of flexibility. We instead propose an\nevidential pipeline, to accumulate road detection results obtained from neural\nnetworks. First, we introduce RoadSeg, a new convolutional architecture that is\noptimized for road detection in LIDAR scans. RoadSeg is used to classify\nindividual LIDAR points as either belonging to the road, or not. Yet, such\npoint-level classification results need to be converted into a dense\nrepresentation, that can be used by an autonomous vehicle. We thus secondly\npresent an evidential road mapping algorithm, that fuses consecutive road\ndetection results. We benefitted from a reinterpretation of logistic\nclassifiers, which can be seen as generating a collection of simple evidential\nmass functions. An evidential grid map that depicts the road can then be\nobtained, by projecting the classification results from RoadSeg into grid\ncells, and by handling moving objects via conflict analysis. The system was\ntrained and evaluated on real-life data. A python implementation maintains a 10\nHz framerate. Since road labels were needed for training, a soft labelling\nprocedure, relying lane-level HD maps, was used to generate coarse training and\nvalidation sets. An additional test set was manually labelled for evaluation\npurposes. So as to reach satisfactory results, the system fuses road detection\nresults obtained from three variants of RoadSeg, processing different LIDAR\nfeatures.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 18:14:36 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Capellier", "Edouard", ""], ["Davoine", "Franck", ""], ["Cherfaoui", "Veronique", ""], ["Li", "You", ""]]}, {"id": "2102.03391", "submitter": "Justin Sanchez", "authors": "Anbumalar Saravanan, Justin Sanchez, Hassan Ghasemzadeh, Aurelia\n  Macabasco-O'Connell and Hamed Tabkhi", "title": "Single Run Action Detector over Video Stream -- A Privacy Preserving\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper takes initial strides at designing and evaluating a vision-based\nsystem for privacy ensured activity monitoring. The proposed technology\nutilizing Artificial Intelligence (AI)-empowered proactive systems offering\ncontinuous monitoring, behavioral analysis, and modeling of human activities.\nTo this end, this paper presents Single Run Action Detector (S-RAD) which is a\nreal-time privacy-preserving action detector that performs end-to-end action\nlocalization and classification. It is based on Faster-RCNN combined with\ntemporal shift modeling and segment based sampling to capture the human\nactions. Results on UCF-Sports and UR Fall dataset present comparable accuracy\nto State-of-the-Art approaches with significantly lower model size and\ncomputation demand and the ability for real-time execution on edge embedded\ndevice (e.g. Nvidia Jetson Xavier).\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 19:27:38 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Saravanan", "Anbumalar", ""], ["Sanchez", "Justin", ""], ["Ghasemzadeh", "Hassan", ""], ["Macabasco-O'Connell", "Aurelia", ""], ["Tabkhi", "Hamed", ""]]}, {"id": "2102.03393", "submitter": "Abhishek Bihani", "authors": "Abhishek Bihani, Hugh Daigle, Javier E. Santos, Christopher Landry,\n  Masa Prodanovic, Kitty Milliken", "title": "MudrockNet: Semantic Segmentation of Mudrock SEM Images through Deep\n  Learning", "comments": "24 pages, 8 figures, submitted to Computers and Geosciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation and analysis of individual pores and grains of mudrocks from\nscanning electron microscope images is non-trivial because of noise, imaging\nartifacts, variation in pixel grayscale values across images, and overlaps in\ngrayscale values among different physical features such as silt grains, clay\ngrains, and pores in an image, which make their identification difficult.\nMoreover, because grains and pores often have overlapping grayscale values,\ndirect application of threshold-based segmentation techniques is not\nsufficient. Recent advances in the field of computer vision have made it easier\nand faster to segment images and identify multiple occurrences of such features\nin an image, provided that ground-truth data for training the algorithm is\navailable. Here, we propose a deep learning SEM image segmentation model,\nMudrockNet based on Google's DeepLab-v3+ architecture implemented with the\nTensorFlow library. The ground-truth data was obtained from an image-processing\nworkflow applied to scanning electron microscope images of uncemented muds from\nthe Kumano Basin offshore Japan at depths < 1.1 km. The trained deep learning\nmodel obtained a pixel-accuracy about 90%, and predictions for the test data\nobtained a mean intersection over union (IoU) of 0.6591 for silt grains and\n0.6642 for pores. We also compared our model with the random forest classifier\nusing trainable Weka segmentation in ImageJ, and it was observed that\nMudrockNet gave better predictions for both silt grains and pores. The size,\nconcentration, and spatial arrangement of the silt and clay grains can affect\nthe petrophysical properties of a mudrock, and an automated method to\naccurately identify the different grains and pores in mudrocks can help improve\nreservoir and seal characterization for petroleum exploration and anthropogenic\nwaste sequestration.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 19:38:44 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Bihani", "Abhishek", ""], ["Daigle", "Hugh", ""], ["Santos", "Javier E.", ""], ["Landry", "Christopher", ""], ["Prodanovic", "Masa", ""], ["Milliken", "Kitty", ""]]}, {"id": "2102.03424", "submitter": "Ye Zhu", "authors": "Ye Zhu, Yu Wu, Hugo Latapie, Yi Yang, Yan Yan", "title": "Learning Audio-Visual Correlations from Variational Cross-Modal\n  Generation", "comments": "Accepted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  People can easily imagine the potential sound while seeing an event. This\nnatural synchronization between audio and visual signals reveals their\nintrinsic correlations. To this end, we propose to learn the audio-visual\ncorrelations from the perspective of cross-modal generation in a\nself-supervised manner, the learned correlations can be then readily applied in\nmultiple downstream tasks such as the audio-visual cross-modal localization and\nretrieval. We introduce a novel Variational AutoEncoder (VAE) framework that\nconsists of Multiple encoders and a Shared decoder (MS-VAE) with an additional\nWasserstein distance constraint to tackle the problem. Extensive experiments\ndemonstrate that the optimized latent representation of the proposed MS-VAE can\neffectively learn the audio-visual correlations and can be readily applied in\nmultiple audio-visual downstream tasks to achieve competitive performance even\nwithout any given label information during training.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 21:27:00 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2021 14:44:47 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Zhu", "Ye", ""], ["Wu", "Yu", ""], ["Latapie", "Hugo", ""], ["Yang", "Yi", ""], ["Yan", "Yan", ""]]}, {"id": "2102.03442", "submitter": "Yan Lu", "authors": "Yan Lu and Yuanchao Shu", "title": "Custom Object Detection via Multi-Camera Self-Supervised Learning", "comments": "7 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes MCSSL, a self-supervised learning approach for building\ncustom object detection models in multi-camera networks. MCSSL associates\nbounding boxes between cameras with overlapping fields of view by leveraging\nepipolar geometry and state-of-the-art tracking and reID algorithms, and\nprudently generates two sets of pseudo-labels to fine-tune backbone and\ndetection networks respectively in an object detection model. To train\neffectively on pseudo-labels,a powerful reID-like pretext task with consistency\nloss is constructed for model customization. Our evaluation shows that compared\nwith legacy selftraining methods, MCSSL improves average mAP by 5.44% and 6.76%\non WildTrack and CityFlow dataset, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 23:11:14 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Lu", "Yan", ""], ["Shu", "Yuanchao", ""]]}, {"id": "2102.03444", "submitter": "Dominik Drees", "authors": "Dominik Drees, Aaron Scherzinger, Ren\\'e H\\\"agerling, Friedemann\n  Kiefer, Xiaoyi Jiang", "title": "Scalable Robust Graph and Feature Extraction for Arbitrary Vessel\n  Networks in Large Volumetric Datasets", "comments": null, "journal-ref": "BMC Bioinformatics 22 (2021) 346", "doi": "10.1186/s12859-021-04262-w", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in 3D imaging technologies provide novel insights to\nresearchers and reveal finer and more detail of examined specimen, especially\nin the biomedical domain, but also impose huge challenges regarding scalability\nfor automated analysis algorithms due to rapidly increasing dataset sizes. In\nparticular, existing research towards automated vessel network analysis does\nnot consider memory requirements of proposed algorithms and often generates a\nlarge number of spurious branches for structures consisting of many voxels.\nAdditionally, very often these algorithms have further restrictions such as the\nlimitation to tree topologies or relying on the properties of specific image\nmodalities. We present a scalable pipeline (in terms of computational cost,\nrequired main memory and robustness) that extracts an annotated abstract graph\nrepresentation from the foreground segmentation of vessel networks of arbitrary\ntopology and vessel shape. Only a single, dimensionless, a-priori determinable\nparameter is required. By careful engineering of individual pipeline stages and\na novel iterative refinement scheme we are, for the first time, able to analyze\nthe topology of volumes of roughly 1TB on commodity hardware. An implementation\nof the presented pipeline is publicly available in version 5.1 of the volume\nrendering and processing engine Voreen (https://www.uni-muenster.de/Voreen/).\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 23:13:09 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Drees", "Dominik", ""], ["Scherzinger", "Aaron", ""], ["H\u00e4gerling", "Ren\u00e9", ""], ["Kiefer", "Friedemann", ""], ["Jiang", "Xiaoyi", ""]]}, {"id": "2102.03456", "submitter": "Nael Fasfous", "authors": "Nael Fasfous, Manoj-Rohit Vemparala, Alexander Frickenstein, Lukas\n  Frickenstein, Walter Stechele", "title": "BinaryCoP: Binary Neural Network-based COVID-19 Face-Mask Wear and\n  Positioning Predictor on Edge Devices", "comments": "Accepted at IEEE IPDPS-RAW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Face masks have long been used in many areas of everyday life to protect\nagainst the inhalation of hazardous fumes and particles. They also offer an\neffective solution in healthcare for bi-directional protection against\nair-borne diseases. Wearing and positioning the mask correctly is essential for\nits function. Convolutional neural networks (CNNs) offer an excellent solution\nfor face recognition and classification of correct mask wearing and\npositioning. In the context of the ongoing COVID-19 pandemic, such algorithms\ncan be used at entrances to corporate buildings, airports, shopping areas, and\nother indoor locations, to mitigate the spread of the virus. These application\nscenarios impose major challenges to the underlying compute platform. The\ninference hardware must be cheap, small and energy efficient, while providing\nsufficient memory and compute power to execute accurate CNNs at a reasonably\nlow latency. To maintain data privacy of the public, all processing must remain\non the edge-device, without any communication with cloud servers. To address\nthese challenges, we present a low-power binary neural network classifier for\ncorrect facial-mask wear and positioning. The classification task is\nimplemented on an embedded FPGA, performing high-throughput binary operations.\nClassification can take place at up to ~6400 frames-per-second, easily enabling\nmulti-camera, speed-gate settings or statistics collection in crowd settings.\nWhen deployed on a single entrance or gate, the idle power consumption is\nreduced to 1.6W, improving the battery-life of the device. We achieve an\naccuracy of up to 98% for four wearing positions of the MaskedFace-Net dataset.\nTo maintain equivalent classification accuracy for all face structures,\nskin-tones, hair types, and mask types, the algorithms are tested for their\nability to generalize the relevant features over all subjects using the\nGrad-CAM approach.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 00:14:06 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 08:48:39 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Fasfous", "Nael", ""], ["Vemparala", "Manoj-Rohit", ""], ["Frickenstein", "Alexander", ""], ["Frickenstein", "Lukas", ""], ["Stechele", "Walter", ""]]}, {"id": "2102.03495", "submitter": "Furao Shen", "authors": "Junyi An and Fengshan Liu and Jian Zhao and Furao Shen", "title": "IC Networks: Remodeling the Basic Unit for Convolutional Neural Networks", "comments": "7 pages, 3 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) is a class of artificial neural networks\nwidely used in computer vision tasks. Most CNNs achieve excellent performance\nby stacking certain types of basic units. In addition to increasing the depth\nand width of the network, designing more effective basic units has become an\nimportant research topic. Inspired by the elastic collision model in physics,\nwe present a general structure which can be integrated into the existing CNNs\nto improve their performance. We term it the \"Inter-layer Collision\" (IC)\nstructure. Compared to the traditional convolution structure, the IC structure\nintroduces nonlinearity and feature recalibration in the linear convolution\noperation, which can capture more fine-grained features. In addition, a new\ntraining method, namely weak logit distillation (WLD), is proposed to speed up\nthe training of IC networks by extracting knowledge from pre-trained basic\nmodels. In the ImageNet experiment, we integrate the IC structure into\nResNet-50 and reduce the top-1 error from 22.38% to 21.75%, which also catches\nup the top-1 error of ResNet-100 (21.75%) with nearly half of FLOPs.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 03:15:43 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["An", "Junyi", ""], ["Liu", "Fengshan", ""], ["Zhao", "Jian", ""], ["Shen", "Furao", ""]]}, {"id": "2102.03501", "submitter": "Xin Yi", "authors": "Xin Yi, Bo Ma, Yulin Zhang, Longyao Liu, JiaHao Wu", "title": "Two-Step Image Dehazing with Intra-domain and Inter-domain Adaptation", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caused by the difference of data distributions, intra-domain gap and\ninter-domain gap are widely present in image processing tasks. In the field of\nimage dehazing, certain previous works have paid attention to the inter-domain\ngap between the synthetic domain and the real domain. However, those methods\nonly establish the connection from the source domain to the target domain\nwithout taking into account the large distribution shift within the target\ndomain (intra-domain gap). In this work, we propose a Two-Step Dehazing Network\n(TSDN) with an intra-domain adaptation and a constrained inter-domain\nadaptation. First, we subdivide the distributions within the synthetic domain\ninto subsets and mine the optimal subset (easy samples) by loss-based\nsupervision. To alleviate the intra-domain gap of the synthetic domain, we\npropose an intra-domain adaptation to align distributions of other subsets to\nthe optimal subset by adversarial learning. Finally, we conduct the constrained\ninter-domain adaptation from the real domain to the optimal subset of the\nsynthetic domain, alleviating the domain shift between domains as well as the\ndistribution shift within the real domain. Extensive experimental results\ndemonstrate that our framework performs favorably against the state-of-the-art\nalgorithms both on the synthetic datasets and the real datasets.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 04:02:14 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 11:46:37 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Yi", "Xin", ""], ["Ma", "Bo", ""], ["Zhang", "Yulin", ""], ["Liu", "Longyao", ""], ["Wu", "JiaHao", ""]]}, {"id": "2102.03503", "submitter": "Chieh-Yun Chen", "authors": "Chien-Lung Chou, Chieh-Yun Chen, Chia-Wei Hsieh, Hong-Han Shuai,\n  Jiaying Liu, and Wen-Huang Cheng", "title": "Template-Free Try-on Image Synthesis via Semantic-guided Optimization", "comments": "Accepted by IEEE TNNLS (2021). 14 pages including 2 pages of\n  reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The virtual try-on task is so attractive that it has drawn considerable\nattention in the field of computer vision. However, presenting the\nthree-dimensional (3D) physical characteristic (e.g., pleat and shadow) based\non a 2D image is very challenging. Although there have been several previous\nstudies on 2D-based virtual try-on work, most 1) required user-specified target\nposes that are not user-friendly and may not be the best for the target\nclothing, and 2) failed to address some problematic cases, including facial\ndetails, clothing wrinkles and body occlusions. To address these two\nchallenges, in this paper, we propose an innovative template-free try-on image\nsynthesis (TF-TIS) network. The TF-TIS first synthesizes the target pose\naccording to the user-specified in-shop clothing. Afterward, given an in-shop\nclothing image, a user image, and a synthesized pose, we propose a novel model\nfor synthesizing a human try-on image with the target clothing in the best\nfitting pose. The qualitative and quantitative experiments both indicate that\nthe proposed TF-TIS outperforms the state-of-the-art methods, especially for\ndifficult cases.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 04:12:14 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Chou", "Chien-Lung", ""], ["Chen", "Chieh-Yun", ""], ["Hsieh", "Chia-Wei", ""], ["Shuai", "Hong-Han", ""], ["Liu", "Jiaying", ""], ["Cheng", "Wen-Huang", ""]]}, {"id": "2102.03512", "submitter": "Amit Unde", "authors": "Amit Satish Unde and Renu M. Rameshan", "title": "MOTS R-CNN: Cosine-margin-triplet loss for multi-object tracking", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the central tasks of multi-object tracking involves learning a\ndistance metric that is consistent with the semantic similarities of objects.\nThe design of an appropriate loss function that encourages discriminative\nfeature learning is among the most crucial challenges in deep neural\nnetwork-based metric learning. Despite significant progress, slow convergence\nand a poor local optimum of the existing contrastive and triplet loss based\ndeep metric learning methods necessitates a better solution. In this paper, we\npropose cosine-margin-contrastive (CMC) and cosine-margin-triplet (CMT) loss by\nreformulating both contrastive and triplet loss functions from the perspective\nof cosine distance. The proposed reformulation as a cosine loss is achieved by\nfeature normalization which distributes the learned features on a hypersphere.\nWe then propose the MOTS R-CNN framework for joint multi-object tracking and\nsegmentation, particularly targeted at improving the tracking performance.\nSpecifically, the tracking problem is addressed through deep metric learning\nbased on the proposed loss functions. We propose a scale-invariant tracking by\nusing a multi-layer feature aggregation scheme to make the model robust against\nobject scale variations and occlusions. The MOTS R-CNN achieves the\nstate-of-the-art tracking performance on the KITTI MOTS dataset. We show that\nthe MOTS R-CNN reduces the identity switching by $62\\%$ and $61\\%$ on cars and\npedestrians, respectively in comparison to Track R-CNN.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 05:03:29 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Unde", "Amit Satish", ""], ["Rameshan", "Renu M.", ""]]}, {"id": "2102.03513", "submitter": "Rafael Dowsley", "authors": "Sikha Pentyala and Rafael Dowsley and Martine De Cock", "title": "Privacy-Preserving Video Classification with Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many video classification applications require access to personal data,\nthereby posing an invasive security risk to the users' privacy. We propose a\nprivacy-preserving implementation of single-frame method based video\nclassification with convolutional neural networks that allows a party to infer\na label from a video without necessitating the video owner to disclose their\nvideo to other entities in an unencrypted manner. Similarly, our approach\nremoves the requirement of the classifier owner from revealing their model\nparameters to outside entities in plaintext. To this end, we combine existing\nSecure Multi-Party Computation (MPC) protocols for private image classification\nwith our novel MPC protocols for oblivious single-frame selection and secure\nlabel aggregation across frames. The result is an end-to-end privacy-preserving\nvideo classification pipeline. We evaluate our proposed solution in an\napplication for private human emotion recognition. Our results across a variety\nof security settings, spanning honest and dishonest majority configurations of\nthe computing parties, and for both passive and active adversaries, demonstrate\nthat videos can be classified with state-of-the-art accuracy, and without\nleaking sensitive user information.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 05:05:31 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Pentyala", "Sikha", ""], ["Dowsley", "Rafael", ""], ["De Cock", "Martine", ""]]}, {"id": "2102.03520", "submitter": "Jie Mei", "authors": "Jie Mei, Jenq-Neng Hwang, Suzanne Romain, Craig Rose, Braden Moore,\n  and Kelsey Magrane", "title": "Video-based Hierarchical Species Classification for Longline Fishing\n  Monitoring", "comments": "To be published in CVAUI2020 in conjunction with ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of electronic monitoring (EM) of longline fishing is to monitor the\nfish catching activities on fishing vessels, either for the regulatory\ncompliance or catch counting. Hierarchical classification based on videos\nallows for inexpensive and efficient fish species identification of catches\nfrom longline fishing, where fishes are under severe deformation and\nself-occlusion during the catching process. More importantly, the flexibility\nof hierarchical classification mitigates the laborious efforts of human reviews\nby providing confidence scores in different hierarchical levels. Some related\nworks either use cascaded models for hierarchical classification or make\npredictions per image or predict one overlapping hierarchical data structure of\nthe dataset in advance. However, with a known non-overlapping hierarchical data\nstructure provided by fisheries scientists, our method enforces the\nhierarchical data structure and introduces an efficient training and inference\nstrategy for video-based fisheries data. Our experiments show that the proposed\nmethod outperforms the classic flat classification system significantly and our\nablation study justifies our contributions in CNN model design, training\nstrategy, and the video-based inference schemes for the hierarchical fish\nspecies classification task.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 06:10:52 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Mei", "Jie", ""], ["Hwang", "Jenq-Neng", ""], ["Romain", "Suzanne", ""], ["Rose", "Craig", ""], ["Moore", "Braden", ""], ["Magrane", "Kelsey", ""]]}, {"id": "2102.03526", "submitter": "Maria Brbic", "authors": "Kaidi Cao, Maria Brbic, Jure Leskovec", "title": "Open-World Semi-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised and semi-supervised learning methods have been traditionally\ndesigned for the closed-world setting based on the assumption that unlabeled\ntest data contains only classes previously encountered in the labeled training\ndata. However, the real world is inherently open and dynamic, and thus novel,\npreviously unseen classes may appear in the test data or during the model\ndeployment. Here, we introduce a new open-world semi-supervised learning\nsetting in which the model is required to recognize previously seen classes, as\nwell as to discover novel classes never seen in the labeled dataset. To tackle\nthe problem, we propose ORCA, an approach that learns to simultaneously\nclassify and cluster the data. ORCA classifies examples from the unlabeled\ndataset to previously seen classes, or forms a novel class by grouping similar\nexamples together. The key idea in ORCA is in introducing uncertainty based\nadaptive margin that effectively circumvents the bias caused by the imbalance\nof variance between seen and novel classes/clusters. We demonstrate that ORCA\naccurately discovers novel classes and assigns samples to previously seen\nclasses on benchmark image classification datasets, including CIFAR and\nImageNet. Remarkably, despite solving the harder task ORCA outperforms\nsemi-supervised methods on seen classes, as well as novel class discovery\nmethods on novel classes, achieving 7% and 151% improvements on seen and novel\nclasses in the ImageNet dataset.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 07:11:07 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 07:25:25 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Cao", "Kaidi", ""], ["Brbic", "Maria", ""], ["Leskovec", "Jure", ""]]}, {"id": "2102.03532", "submitter": "Maheshi Dissanayake", "authors": "Shanaka Ramesh Gunasekara and H.N.T.K.Kaldera and Maheshi B.\n  Dissanayake", "title": "A Systematic Approach for MRI Brain Tumor Localization, and Segmentation\n  using Deep Learning and Active Contouring", "comments": "accepted for publication in Journal of Healthcare Engineering,\n  Hindawi in 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  One of the main requirements of tumor extraction is the annotation and\nsegmentation of tumor boundaries correctly. For this purpose, we present a\nthreefold deep learning architecture. First classifiers are implemented with a\ndeep convolutional neural network(CNN) andsecond a region-based convolutional\nneural network (R-CNN) is performed on the classified images to localize the\ntumor regions of interest. As the third and final stage, the concentratedtumor\nboundary is contoured for the segmentation process by using the\nChan-Vesesegmentation algorithm. As the typical edge detection algorithms based\non gradients of pixel intensity tend to fail in the medical image segmentation\nprocess, an active contour algorithm defined with the level set function is\nproposed. Specifically, Chan- Vese algorithm was applied to detect the tumor\nboundaries for the segmentation process. To evaluate the performance of the\noverall system, Dice Score,Rand Index (RI), Variation of Information (VOI),\nGlobal Consistency Error (GCE), Boundary Displacement Error (BDE), Mean\nabsolute error (MAE), and Peak Signal to Noise Ratio (PSNR) werecalculated by\ncomparing the segmented boundary area which is the final output of the\nproposed, against the demarcations of the subject specialists which is the gold\nstandard. Overall performance of the proposed architecture for both glioma and\nmeningioma segmentation is with average dice score of 0.92, (also, with RI of\n0.9936, VOI of 0.0301, GCE of 0.004, BDE of 2.099, PSNR of 77.076 and MAE of\n52.946), pointing to high reliability of the proposed architecture.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 07:53:02 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Gunasekara", "Shanaka Ramesh", ""], ["Kaldera", "H. N. T. K.", ""], ["Dissanayake", "Maheshi B.", ""]]}, {"id": "2102.03539", "submitter": "Haipeng Zhang", "authors": "Haipeng Zhang, Zhong Cao, Ziang Yan, Changshui Zhang", "title": "Sill-Net: Feature Augmentation with Separated Illumination\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For visual object recognition tasks, the illumination variations can cause\ndistinct changes in object appearance and thus confuse the deep neural network\nbased recognition models. Especially for some rare illumination conditions,\ncollecting sufficient training samples could be time-consuming and expensive.\nTo solve this problem, in this paper we propose a novel neural network\narchitecture called Separating-Illumination Network (Sill-Net). Sill-Net learns\nto separate illumination features from images, and then during training we\naugment training samples with these separated illumination features in the\nfeature space. Experimental results demonstrate that our approach outperforms\ncurrent state-of-the-art methods in several object classification benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 09:00:10 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 05:57:34 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Zhang", "Haipeng", ""], ["Cao", "Zhong", ""], ["Yan", "Ziang", ""], ["Zhang", "Changshui", ""]]}, {"id": "2102.03550", "submitter": "Hualie Jiang", "authors": "Hualie Jiang, Zhe Sheng, Siyu Zhu, Zilong Dong, Rui Huang", "title": "UniFuse: Unidirectional Fusion for 360$^{\\circ}$ Panorama Depth\n  Estimation", "comments": "IEEE Robotics and Automation Letters and ICRA 2021; Demo:\n  https://youtu.be/9vm9OMksvrc; Code:\n  https://github.com/alibaba/UniFuse-Unidirectional-Fusion", "journal-ref": null, "doi": "10.1109/LRA.2021.3058957", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning depth from spherical panoramas is becoming a popular research topic\nbecause a panorama has a full field-of-view of the environment and provides a\nrelatively complete description of a scene. However, applying well-studied CNNs\nfor perspective images to the standard representation of spherical panoramas,\ni.e., the equirectangular projection, is suboptimal, as it becomes distorted\ntowards the poles. Another representation is the cubemap projection, which is\ndistortion-free but discontinued on edges and limited in the field-of-view.\nThis paper introduces a new framework to fuse features from the two\nprojections, unidirectionally feeding the cubemap features to the\nequirectangular features only at the decoding stage. Unlike the recent\nbidirectional fusion approach operating at both the encoding and decoding\nstages, our fusion scheme is much more efficient. Besides, we also designed a\nmore effective fusion module for our fusion scheme. Experiments verify the\neffectiveness of our proposed fusion strategy and module, and our model\nachieves state-of-the-art performance on four popular datasets. Additional\nexperiments show that our model also has the advantages of model complexity and\ngeneralization capability.The code is available at\nhttps://github.com/alibaba/UniFuse-Unidirectional-Fusion.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 10:01:09 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 13:50:50 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Jiang", "Hualie", ""], ["Sheng", "Zhe", ""], ["Zhu", "Siyu", ""], ["Dong", "Zilong", ""], ["Huang", "Rui", ""]]}, {"id": "2102.03558", "submitter": "Xiaoke Peng", "authors": "Nan Jiang, Xuehui Yu, Xiaoke Peng, Yuqi Gong, Zhenjun Han", "title": "SM+: Refined Scale Match for Tiny Person Detection", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting tiny objects ( e.g., less than 20 x 20 pixels) in large-scale\nimages is an important yet open problem. Modern CNN-based detectors are\nchallenged by the scale mismatch between the dataset for network pre-training\nand the target dataset for detector training. In this paper, we investigate the\nscale alignment between pre-training and target datasets, and propose a new\nrefined Scale Match method (termed SM+) for tiny person detection. SM+ improves\nthe scale match from image level to instance level, and effectively promotes\nthe similarity between pre-training and target dataset. Moreover, considering\nSM+ possibly destroys the image structure, a new probabilistic structure\ninpainting (PSI) method is proposed for the background processing. Experiments\nconducted across various detectors show that SM+ noticeably improves the\nperformance on TinyPerson, and outperforms the state-of-the-art detectors with\na significant margin.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 10:34:16 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Jiang", "Nan", ""], ["Yu", "Xuehui", ""], ["Peng", "Xiaoke", ""], ["Gong", "Yuqi", ""], ["Han", "Zhenjun", ""]]}, {"id": "2102.03586", "submitter": "Zenghao Chai", "authors": "Zenghao Chai, Chun Yuan, Zhihui Lin, Yunpeng Bai", "title": "CMS-LSTM: Context-Embedding and Multi-Scale Spatiotemporal-Expression\n  LSTM for Video Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting variation and spatiotemporal features via limited frames remains\nas an unsolved and challenging problem in video prediction. Inherent\nuncertainty among consecutive frames exacerbates the difficulty in long-term\nprediction. To tackle the problem, we focus on capturing context correlations\nand multi-scale spatiotemporal flows, then propose CMS-LSTM by integrating two\neffective and lightweight blocks, namely Context-Embedding (CE) and\nSpatiotemporal-Expression (SE) block, into ConvLSTM backbone. CE block is\ndesigned for abundant context interactions, while SE block focuses on\nmulti-scale spatiotemporal expression in hidden states. The newly introduced\nblocks also facilitate other spatiotemporal models (e.g., PredRNN, SA-ConvLSTM)\nto produce representative implicit features for video prediction. Qualitative\nand quantitative experiments demonstrate the effectiveness and flexibility of\nour proposed method. We use fewer parameters to reach markedly state-of-the-art\nresults on Moving MNIST and TaxiBJ datasets in numbers of metrics. All source\ncode is available at https://github.com/czh-98/CMS-LSTM.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 14:24:40 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Chai", "Zenghao", ""], ["Yuan", "Chun", ""], ["Lin", "Zhihui", ""], ["Bai", "Yunpeng", ""]]}, {"id": "2102.03602", "submitter": "Frank Dennis Julca Aguilar", "authors": "Frank Julca-Aguilar, Jason Taylor, Mario Bijelic, Fahim Mannan, Ethan\n  Tseng, Felix Heide", "title": "Gated3D: Monocular 3D Object Detection From Temporal Illumination Cues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Today's state-of-the-art methods for 3D object detection are based on lidar,\nstereo, or monocular cameras. Lidar-based methods achieve the best accuracy,\nbut have a large footprint, high cost, and mechanically-limited angular\nsampling rates, resulting in low spatial resolution at long ranges. Recent\napproaches based on low-cost monocular or stereo cameras promise to overcome\nthese limitations but struggle in low-light or low-contrast regions as they\nrely on passive CMOS sensors. In this work, we propose a novel 3D object\ndetection modality that exploits temporal illumination cues from a low-cost\nmonocular gated imager. We propose a novel deep detector architecture, Gated3D,\nthat is tailored to temporal illumination cues from three gated images. Gated\nimages allow us to exploit mature 2D object feature extractors that guide the\n3D predictions through a frustum segment estimation. We assess the proposed\nmethod on a novel 3D detection dataset that includes gated imagery captured in\nover 10,000 km of driving data. We validate that our method outperforms\nstate-of-the-art monocular and stereo approaches at long distances. We will\nrelease our code and dataset, opening up a new sensor modality as an avenue to\nreplace lidar in autonomous driving.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 16:06:51 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Julca-Aguilar", "Frank", ""], ["Taylor", "Jason", ""], ["Bijelic", "Mario", ""], ["Mannan", "Fahim", ""], ["Tseng", "Ethan", ""], ["Heide", "Felix", ""]]}, {"id": "2102.03673", "submitter": "Leena Mathur", "authors": "Leena Mathur and Maja J Matari\\'c", "title": "Unsupervised Audio-Visual Subspace Alignment for High-Stakes Deception\n  Detection", "comments": "Accepted at ICASSP 2021 \\c{opyright} 2021 IEEE. Personal use of this\n  material is permitted. Permission from IEEE must be obtained for all other\n  uses, in current or future media, including reprinting/republishing this\n  material for advertising or promotional purposes, creating new collective\n  works, for resale or redistribution to servers or lists, or reuse of\n  copyrighted components of this work", "journal-ref": null, "doi": "10.1109/ICASSP39728.2021.9413550", "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated systems that detect deception in high-stakes situations can enhance\nsocietal well-being across medical, social work, and legal domains. Existing\nmodels for detecting high-stakes deception in videos have been supervised, but\nlabeled datasets to train models can rarely be collected for most real-world\napplications. To address this problem, we propose the first multimodal\nunsupervised transfer learning approach that detects real-world, high-stakes\ndeception in videos without using high-stakes labels. Our subspace-alignment\n(SA) approach adapts audio-visual representations of deception in\nlab-controlled low-stakes scenarios to detect deception in real-world,\nhigh-stakes situations. Our best unsupervised SA models outperform models\nwithout SA, outperform human ability, and perform comparably to a number of\nexisting supervised models. Our research demonstrates the potential for\nintroducing subspace-based transfer learning to model high-stakes deception and\nother social behaviors in real-world contexts with a scarcity of labeled\nbehavioral data.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 21:53:12 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Mathur", "Leena", ""], ["Matari\u0107", "Maja J", ""]]}, {"id": "2102.03675", "submitter": "Zhengzhong Tu", "authors": "Zhengzhong Tu", "title": "Predicting Eye Fixations Under Distortion Using Bayesian Observers", "comments": "18 pages, single-column. Project report", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Visual attention is very an essential factor that affects how human perceives\nvisual signals. This report investigates how distortions in an image could\ndistract human's visual attention using Bayesian visual search models,\nspecifically, Maximum-a-posteriori (MAP)\n\\cite{findlay1982global}\\cite{eckstein2001quantifying} and Entropy Limit\nMinimization (ELM) \\cite{najemnik2009simple}, which predict eye fixation\nmovements based on a Bayesian probabilistic framework. Experiments on modified\nMAP and ELM models on JPEG-compressed images containing blocking or ringing\nartifacts were conducted and we observed that compression artifacts can affect\nvisual attention. We hope this work sheds light on the interactions between\nvisual attention and perceptual quality.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 21:57:08 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Tu", "Zhengzhong", ""]]}, {"id": "2102.03700", "submitter": "Fredrik Westling", "authors": "Fredrik Westling and James Underwood and Mitch Bryson", "title": "A procedure for automated tree pruning suggestion using LiDAR scans of\n  fruit trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In fruit tree growth, pruning is an important management practice for\npreventing overcrowding, improving canopy access to light and promoting\nregrowth. Due to the slow nature of agriculture, decisions in pruning are\ntypically made using tradition or rules of thumb rather than data-driven\nanalysis. Many existing algorithmic, simulation-based approaches rely on\nhigh-fidelity digital captures or purely computer-generated fruit trees, and\nare unable to provide specific results on an orchard scale. We present a\nframework for suggesting pruning strategies on LiDAR-scanned commercial fruit\ntrees using a scoring function with a focus on improving light distribution\nthroughout the canopy. A scoring function to assess the quality of the tree\nshape based on its light availability and size was developed for comparative\nanalysis between trees, and was validated against yield characteristics,\ndemonstrating a reasonable correlation against fruit count with an $R^2$ score\nof 0.615 for avocado and 0.506 for mango. A tool was implemented for simulating\npruning by algorithmically estimating which parts of a tree point cloud would\nbe removed given specific cut points using structural analysis of the tree,\nvalidated experimentally with an average F1 score of 0.78 across 144\nexperiments. Finally, new pruning locations were suggested and we used the\nprevious two stages to estimate the improvement of the tree given these\nsuggestions. The light distribution was improved by up to 25.15\\%,\ndemonstrating a 16\\% improvement over commercial pruning on a real tree, and\ncertain cut points were discovered which improved light distribution with a\nsmaller negative impact on tree volume. The final results suggest value in the\nframework as a decision making tool for commercial growers, or as a starting\npoint for automated pruning since the entire process can be performed with\nlittle human intervention.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 02:18:56 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Westling", "Fredrik", ""], ["Underwood", "James", ""], ["Bryson", "Mitch", ""]]}, {"id": "2102.03710", "submitter": "Seyed Mehdi Iranmanesh", "authors": "Seyed Mehdi Iranmanesh and Nasser M. Nasrabadi", "title": "HGAN: Hybrid Generative Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a simple approach to train Generative Adversarial\nNetworks (GANs) in order to avoid a \\textit {mode collapse} issue. Implicit\nmodels such as GANs tend to generate better samples compared to explicit models\nthat are trained on tractable data likelihood. However, GANs overlook the\nexplicit data density characteristics which leads to undesirable quantitative\nevaluations and mode collapse. To bridge this gap, we propose a hybrid\ngenerative adversarial network (HGAN) for which we can enforce data density\nestimation via an autoregressive model and support both adversarial and\nlikelihood framework in a joint training manner which diversify the estimated\ndensity in order to cover different modes. We propose to use an adversarial\nnetwork to \\textit {transfer knowledge} from an autoregressive model (teacher)\nto the generator (student) of a GAN model. A novel deep architecture within the\nGAN formulation is developed to adversarially distill the autoregressive model\ninformation in addition to simple GAN training approach. We conduct extensive\nexperiments on real-world datasets (i.e., MNIST, CIFAR-10, STL-10) to\ndemonstrate the effectiveness of the proposed HGAN under qualitative and\nquantitative evaluations. The experimental results show the superiority and\ncompetitiveness of our method compared to the baselines.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 03:54:12 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Iranmanesh", "Seyed Mehdi", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "2102.03716", "submitter": "Zhuo Feng", "authors": "Wuxinlin Cheng, Chenhui Deng, Zhiqiang Zhao, Yaohui Cai, Zhiru Zhang,\n  Zhuo Feng", "title": "SPADE: A Spectral Method for Black-Box Adversarial Robustness Evaluation", "comments": "The 2021 International Conference on Machine Learning (ICML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A black-box spectral method is introduced for evaluating the adversarial\nrobustness of a given machine learning (ML) model. Our approach, named SPADE,\nexploits bijective distance mapping between the input/output graphs constructed\nfor approximating the manifolds corresponding to the input/output data. By\nleveraging the generalized Courant-Fischer theorem, we propose a SPADE score\nfor evaluating the adversarial robustness of a given model, which is proved to\nbe an upper bound of the best Lipschitz constant under the manifold setting. To\nreveal the most non-robust data samples highly vulnerable to adversarial\nattacks, we develop a spectral graph embedding procedure leveraging dominant\ngeneralized eigenvectors. This embedding step allows assigning each data sample\na robustness score that can be further harnessed for more effective adversarial\ntraining. Our experiments show the proposed SPADE method leads to promising\nempirical results for neural network models that are adversarially trained with\nthe MNIST and CIFAR-10 data sets.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 04:41:26 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 16:09:39 GMT"}, {"version": "v3", "created": "Sun, 13 Jun 2021 23:02:14 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Cheng", "Wuxinlin", ""], ["Deng", "Chenhui", ""], ["Zhao", "Zhiqiang", ""], ["Cai", "Yaohui", ""], ["Zhang", "Zhiru", ""], ["Feng", "Zhuo", ""]]}, {"id": "2102.03722", "submitter": "Kenneth Co", "authors": "Zhongyuan Hau, Kenneth T. Co, Soteris Demetriou, Emil C. Lupu", "title": "Object Removal Attacks on LiDAR-based 3D Object Detectors", "comments": "Accepted to AutoSec at NDSS 2021", "journal-ref": null, "doi": "10.14722/autosec.2021.23", "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LiDARs play a critical role in Autonomous Vehicles' (AVs) perception and\ntheir safe operations. Recent works have demonstrated that it is possible to\nspoof LiDAR return signals to elicit fake objects. In this work we demonstrate\nhow the same physical capabilities can be used to mount a new, even more\ndangerous class of attacks, namely Object Removal Attacks (ORAs). ORAs aim to\nforce 3D object detectors to fail. We leverage the default setting of LiDARs\nthat record a single return signal per direction to perturb point clouds in the\nregion of interest (RoI) of 3D objects. By injecting illegitimate points behind\nthe target object, we effectively shift points away from the target objects'\nRoIs. Our initial results using a simple random point selection strategy show\nthat the attack is effective in degrading the performance of commonly used 3D\nobject detection models.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 05:34:14 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Hau", "Zhongyuan", ""], ["Co", "Kenneth T.", ""], ["Demetriou", "Soteris", ""], ["Lupu", "Emil C.", ""]]}, {"id": "2102.03725", "submitter": "Ping Chen", "authors": "Ping Chen, Yujin Chen, Dong Yang, Fangyin Wu, Qin Li, Qingpei Xia,\n  Yong Tan", "title": "I2UV-HandNet: Image-to-UV Prediction Network for Accurate and\n  High-fidelity 3D Hand Mesh Modeling", "comments": "Accepted by ICCV2021; 14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing a high-precision and high-fidelity 3D human hand from a color\nimage plays a central role in replicating a realistic virtual hand in\nhuman-computer interaction and virtual reality applications. The results of\ncurrent methods are lacking in accuracy and fidelity due to various hand poses\nand severe occlusions. In this study, we propose an I2UV-HandNet model for\naccurate hand pose and shape estimation as well as 3D hand super-resolution\nreconstruction. Specifically, we present the first UV-based 3D hand shape\nrepresentation. To recover a 3D hand mesh from an RGB image, we design an\nAffineNet to predict a UV position map from the input in an image-to-image\ntranslation fashion. To obtain a higher fidelity shape, we exploit an\nadditional SRNet to transform the low-resolution UV map outputted by AffineNet\ninto a high-resolution one. For the first time, we demonstrate the\ncharacterization capability of the UV-based hand shape representation. Our\nexperiments show that the proposed method achieves state-of-the-art performance\non several challenging benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 05:49:50 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 11:20:25 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Chen", "Ping", ""], ["Chen", "Yujin", ""], ["Yang", "Dong", ""], ["Wu", "Fangyin", ""], ["Li", "Qin", ""], ["Xia", "Qingpei", ""], ["Tan", "Yong", ""]]}, {"id": "2102.03726", "submitter": "Bo Yang", "authors": "Bo Yang, Hengwei Zhang, Yuchen Zhang, Kaiyong Xu, Jindong Wang", "title": "Adversarial example generation with AdaBelief Optimizer and Crop\n  Invariance", "comments": "9pages, 3figures, 7tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are vulnerable to adversarial examples, which are\ncrafted by applying small, human-imperceptible perturbations on the original\nimages, so as to mislead deep neural networks to output inaccurate predictions.\nAdversarial attacks can thus be an important method to evaluate and select\nrobust models in safety-critical applications. However, under the challenging\nblack-box setting, most existing adversarial attacks often achieve relatively\nlow success rates on adversarially trained networks and advanced defense\nmodels. In this paper, we propose AdaBelief Iterative Fast Gradient Method\n(ABI-FGM) and Crop-Invariant attack Method (CIM) to improves the\ntransferability of adversarial examples. ABI-FGM and CIM can be readily\nintegrated to build a strong gradient-based attack to further boost the success\nrates of adversarial examples for black-box attacks. Moreover, our method can\nalso be naturally combined with other gradient-based attack methods to build a\nmore robust attack to generate more transferable adversarial examples against\nthe defense models. Extensive experiments on the ImageNet dataset demonstrate\nthe method's effectiveness. Whether on adversarially trained networks or\nadvanced defense models, our method has higher success rates than\nstate-of-the-art gradient-based attack methods.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 06:00:36 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Yang", "Bo", ""], ["Zhang", "Hengwei", ""], ["Zhang", "Yuchen", ""], ["Xu", "Kaiyong", ""], ["Wang", "Jindong", ""]]}, {"id": "2102.03728", "submitter": "Buu Phan", "authors": "Buu Phan, Fahim Mannan, Felix Heide", "title": "Adversarial Imaging Pipelines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adversarial attacks play an essential role in understanding deep neural\nnetwork predictions and improving their robustness. Existing attack methods aim\nto deceive convolutional neural network (CNN)-based classifiers by manipulating\nRGB images that are fed directly to the classifiers. However, these approaches\ntypically neglect the influence of the camera optics and image processing\npipeline (ISP) that produce the network inputs. ISPs transform RAW measurements\nto RGB images and traditionally are assumed to preserve adversarial patterns.\nHowever, these low-level pipelines can, in fact, destroy, introduce or amplify\nadversarial patterns that can deceive a downstream detector. As a result,\noptimized patterns can become adversarial for the classifier after being\ntransformed by a certain camera ISP and optic but not for others. In this work,\nwe examine and develop such an attack that deceives a specific camera ISP while\nleaving others intact, using the same down-stream classifier. We frame\ncamera-specific attacks as a multi-task optimization problem, relying on a\ndifferentiable approximation for the ISP itself. We validate the proposed\nmethod using recent state-of-the-art automotive hardware ISPs, achieving 92%\nfooling rate when attacking a specific ISP. We demonstrate physical optics\nattacks with 90% fooling rate for a specific camera lenses.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 06:10:54 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 05:48:42 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Phan", "Buu", ""], ["Mannan", "Fahim", ""], ["Heide", "Felix", ""]]}, {"id": "2102.03747", "submitter": "Jie Li", "authors": "Jie Li, Yu Hu", "title": "DPointNet: A Density-Oriented PointNet for 3D Object Detection in Point\n  Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For current object detectors, the scale of the receptive field of feature\nextraction operators usually increases layer by layer. Those operators are\ncalled scale-oriented operators in this paper, such as the convolution layer in\nCNN, and the set abstraction layer in PointNet++. The scale-oriented operators\nare appropriate for 2D images with multi-scale objects, but not natural for 3D\npoint clouds with multi-density but scale-invariant objects. In this paper, we\nput forward a novel density-oriented PointNet (DPointNet) for 3D object\ndetection in point clouds, in which the density of points increases layer by\nlayer. In experiments for object detection, the DPointNet is applied to\nPointRCNN, and the results show that the model with the new operator can\nachieve better performance and higher speed than the baseline PointRCNN, which\nverify the effectiveness of the proposed DPointNet.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 09:01:03 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Li", "Jie", ""], ["Hu", "Yu", ""]]}, {"id": "2102.03771", "submitter": "Yue Pan", "authors": "Yue Pan, Pengchuan Xiao, Yujie He, Zhenlei Shao, Zesong Li", "title": "MULLS: Versatile LiDAR SLAM via Multi-metric Linear Least Square", "comments": "Codes: https://github.com/YuePanEdward/MULLS, Accepted by ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of autonomous driving and mobile mapping calls for\noff-the-shelf LiDAR SLAM solutions that are adaptive to LiDARs of different\nspecifications on various complex scenarios. To this end, we propose MULLS, an\nefficient, low-drift, and versatile 3D LiDAR SLAM system. For the front-end,\nroughly classified feature points (ground, facade, pillar, beam, etc.) are\nextracted from each frame using dual-threshold ground filtering and principal\ncomponents analysis. Then the registration between the current frame and the\nlocal submap is accomplished efficiently by the proposed multi-metric linear\nleast square iterative closest point algorithm. Point-to-point (plane, line)\nerror metrics within each point class are jointly optimized with a linear\napproximation to estimate the ego-motion. Static feature points of the\nregistered frame are appended into the local map to keep it updated. For the\nback-end, hierarchical pose graph optimization is conducted among regularly\nstored history submaps to reduce the drift resulting from dead reckoning.\nExtensive experiments are carried out on three datasets with more than 100,000\nframes collected by seven types of LiDAR on various outdoor and indoor\nscenarios. On the KITTI benchmark, MULLS ranks among the top LiDAR-only SLAM\nsystems with real-time performance.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 10:42:42 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 21:26:08 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 09:04:02 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Pan", "Yue", ""], ["Xiao", "Pengchuan", ""], ["He", "Yujie", ""], ["Shao", "Zhenlei", ""], ["Li", "Zesong", ""]]}, {"id": "2102.03778", "submitter": "Subhankar Ghosh", "authors": "Subhankar Ghosh", "title": "Adversarial Training of Variational Auto-encoders for Continual\n  Zero-shot Learning(A-CZSL)", "comments": "The paper has been accepted in IJCNN21. It contains 8pages and 10\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing artificial neural networks(ANNs) fail to learn\ncontinually due to catastrophic forgetting, while humans can do the same by\nmaintaining previous tasks' performances. Although storing all the previous\ndata can alleviate the problem, it takes a large memory, infeasible in\nreal-world utilization. We propose a continual zero-shot learning model(A-CZSL)\nthat is more suitable in real-case scenarios to address the issue that can\nlearn sequentially and distinguish classes the model has not seen during\ntraining. Further, to enhance the reliability, we develop A-CZSL for a single\nhead continual learning setting where task identity is revealed during the\ntraining process but not during the testing. We present a hybrid network that\nconsists of a shared VAE module to hold information of all tasks and\ntask-specific private VAE modules for each task. The model's size grows with\neach task to prevent catastrophic forgetting of task-specific skills, and it\nincludes a replay approach to preserve shared skills. We demonstrate our hybrid\nmodel outperforms the baselines and is effective on several datasets, i.e.,\nCUB, AWA1, AWA2, and aPY. We show our method is superior in class sequentially\nlearning with ZSL(Zero-Shot Learning) and GZSL(Generalized Zero-Shot Learning).\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 11:21:24 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 20:07:24 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Ghosh", "Subhankar", ""]]}, {"id": "2102.03794", "submitter": "Shizhan Lu", "authors": "Yu Han, Shizhan Lu, Haiyan Xu", "title": "A self-adaptive and robust fission clustering algorithm via heat\n  diffusion and maximal turning angle", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cluster analysis, which focuses on the grouping and categorization of similar\nelements, is widely used in various fields of research. A novel and fast\nclustering algorithm, fission clustering algorithm, is proposed in recent year.\nIn this article, we propose a robust fission clustering (RFC) algorithm and a\nself-adaptive noise identification method. The RFC and the self-adaptive noise\nidentification method are combine to propose a self-adaptive robust fission\nclustering (SARFC) algorithm. Several frequently-used datasets were applied to\ntest the performance of the proposed clustering approach and to compare the\nresults with those of other algorithms. The comprehensive comparisons indicate\nthat the proposed method has advantages over other common methods.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 13:16:47 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Han", "Yu", ""], ["Lu", "Shizhan", ""], ["Xu", "Haiyan", ""]]}, {"id": "2102.03814", "submitter": "Theerawit Wilaiprasitporn", "authors": "Phairot Autthasan, Rattanaphon Chaisaen, Thapanun Sudhawiyangkul,\n  Phurin Rangpong, Suktipol Kiatthaveephong, Nat Dilokthanakul, Gun\n  Bhakdisongkhram, Huy Phan, Cuntai Guan and Theerawit Wilaiprasitporn", "title": "MIN2Net: End-to-End Multi-Task Learning for Subject-Independent Motor\n  Imagery EEG Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Advances in the motor imagery (MI)-based brain-computer interfaces (BCIs)\nallow control of several applications by decoding neurophysiological phenomena,\nwhich are usually recorded by electroencephalography (EEG) using a non-invasive\ntechnique. Despite great advances in MI-based BCI, EEG rhythms are specific to\na subject and various changes over time. These issues point to significant\nchallenges to enhance the classification performance, especially in a\nsubject-independent manner. To overcome these challenges, we propose MIN2Net, a\nnovel end-to-end multi-task learning to tackle this task. We integrate deep\nmetric learning into a multi-task autoencoder to learn a compact and\ndiscriminative latent representation from EEG and perform classification\nsimultaneously. This approach reduces the complexity in pre-processing, results\nin significant performance improvement on EEG classification. Experimental\nresults in a subject-independent manner show that MIN2Net outperforms the\nstate-of-the-art techniques, achieving an F1-score improvement of 6.72%, and\n2.23% on the SMR-BCI, and OpenBMI datasets, respectively. We demonstrate that\nMIN2Net improves discriminative information in the latent representation. This\nstudy indicates the possibility and practicality of using this model to develop\nMI-based BCI applications for new users without the need for calibration.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 15:20:23 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 08:03:59 GMT"}, {"version": "v3", "created": "Thu, 20 May 2021 09:48:47 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Autthasan", "Phairot", ""], ["Chaisaen", "Rattanaphon", ""], ["Sudhawiyangkul", "Thapanun", ""], ["Rangpong", "Phurin", ""], ["Kiatthaveephong", "Suktipol", ""], ["Dilokthanakul", "Nat", ""], ["Bhakdisongkhram", "Gun", ""], ["Phan", "Huy", ""], ["Guan", "Cuntai", ""], ["Wilaiprasitporn", "Theerawit", ""]]}, {"id": "2102.03837", "submitter": "Lei Qi", "authors": "Zekun Li, Wei Zhao, Feng Shi, Lei Qi, Xingzhi Xie, Ying Wei,\n  Zhongxiang Ding, Yang Gao, Shangjie Wu, Jun Liu, Yinghuan Shi, Dinggang Shen", "title": "A novel multiple instance learning framework for COVID-19 severity\n  assessment via data augmentation and self-supervised learning", "comments": "To appear in Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to fast and accurately assess the severity level of COVID-19 is an\nessential problem, when millions of people are suffering from the pandemic\naround the world. Currently, the chest CT is regarded as a popular and\ninformative imaging tool for COVID-19 diagnosis. However, we observe that there\nare two issues -- weak annotation and insufficient data that may obstruct\nautomatic COVID-19 severity assessment with CT images. To address these\nchallenges, we propose a novel three-component method, i.e., 1) a deep multiple\ninstance learning component with instance-level attention to jointly classify\nthe bag and also weigh the instances, 2) a bag-level data augmentation\ncomponent to generate virtual bags by reorganizing high confidential instances,\nand 3) a self-supervised pretext component to aid the learning process. We have\nsystematically evaluated our method on the CT images of 229 COVID-19 cases,\nincluding 50 severe and 179 non-severe cases. Our method could obtain an\naverage accuracy of 95.8%, with 93.6% sensitivity and 96.4% specificity, which\noutperformed previous works.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 16:30:18 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Li", "Zekun", ""], ["Zhao", "Wei", ""], ["Shi", "Feng", ""], ["Qi", "Lei", ""], ["Xie", "Xingzhi", ""], ["Wei", "Ying", ""], ["Ding", "Zhongxiang", ""], ["Gao", "Yang", ""], ["Wu", "Shangjie", ""], ["Liu", "Jun", ""], ["Shi", "Yinghuan", ""], ["Shen", "Dinggang", ""]]}, {"id": "2102.03858", "submitter": "Zaharah A. Bukhsh", "authors": "Zaharah A. Bukhsh, Nils Jansen, Aaqib Saeed", "title": "Damage detection using in-domain and cross-domain transfer learning", "comments": "15 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the capabilities of transfer learning in the area of\nstructural health monitoring. In particular, we are interested in damage\ndetection for concrete structures. Typical image datasets for such problems are\nrelatively small, calling for the transfer of learned representation from a\nrelated large-scale dataset. Past efforts of damage detection using images have\nmainly considered cross-domain transfer learning approaches using pre-trained\nImageNet models that are subsequently fine-tuned for the target task. However,\nthere are rising concerns about the generalizability of ImageNet\nrepresentations for specific target domains, such as for visual inspection and\nmedical imaging. We, therefore, propose a combination of in-domain and\ncross-domain transfer learning strategies for damage detection in bridges. We\nperform comprehensive comparisons to study the impact of cross-domain and\nin-domain transfer, with various initialization strategies, using six publicly\navailable visual inspection datasets. The pre-trained models are also evaluated\nfor their ability to cope with the extremely low-data regime. We show that the\ncombination of cross-domain and in-domain transfer persistently shows superior\nperformance even with tiny datasets. Likewise, we also provide visual\nexplanations of predictive models to enable algorithmic transparency and\nprovide insights to experts about the intrinsic decision-logic of typically\nblack-box deep models.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 17:36:27 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Bukhsh", "Zaharah A.", ""], ["Jansen", "Nils", ""], ["Saeed", "Aaqib", ""]]}, {"id": "2102.03881", "submitter": "Moshe Eliasof", "authors": "Moshe Eliasof, Tue Boesen, Eldad Haber, Chen Keasar, Eran Treister", "title": "Mimetic Neural Networks: A unified framework for Protein Design and\n  Folding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in machine learning techniques for protein folding\nmotivate better results in its inverse problem -- protein design. In this work\nwe introduce a new graph mimetic neural network, MimNet, and show that it is\npossible to build a reversible architecture that solves the structure and\ndesign problems in tandem, allowing to improve protein design when the\nstructure is better estimated. We use the ProteinNet data set and show that the\nstate of the art results in protein design can be improved, given recent\narchitectures for protein folding.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 18:53:52 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Eliasof", "Moshe", ""], ["Boesen", "Tue", ""], ["Haber", "Eldad", ""], ["Keasar", "Chen", ""], ["Treister", "Eran", ""]]}, {"id": "2102.03889", "submitter": "Alessandro Lameiras Koerich", "authors": "Jonathan de Matos and Steve Tsham Mpinda Ataky and Alceu de Souza\n  Britto Jr. and Luiz Eduardo Soares de Oliveira and Alessandro Lameiras\n  Koerich", "title": "Machine Learning Methods for Histopathological Image Analysis: A Review", "comments": "45 pages. arXiv admin note: text overlap with arXiv:1904.07900", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Histopathological images (HIs) are the gold standard for evaluating some\ntypes of tumors for cancer diagnosis. The analysis of such images is not only\ntime and resource consuming, but also very challenging even for experienced\npathologists, resulting in inter- and intra-observer disagreements. One of the\nways of accelerating such an analysis is to use computer-aided diagnosis (CAD)\nsystems. In this paper, we present a review on machine learning methods for\nhistopathological image analysis, including shallow and deep learning methods.\nWe also cover the most common tasks in HI analysis, such as segmentation and\nfeature extraction. In addition, we present a list of publicly available and\nprivate datasets that have been used in HI research.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 19:12:32 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["de Matos", "Jonathan", ""], ["Ataky", "Steve Tsham Mpinda", ""], ["Britto", "Alceu de Souza", "Jr."], ["de Oliveira", "Luiz Eduardo Soares", ""], ["Koerich", "Alessandro Lameiras", ""]]}, {"id": "2102.03897", "submitter": "Chetan Srinidhi L", "authors": "Chetan L. Srinidhi, Seung Wook Kim, Fu-Der Chen, Anne L. Martel", "title": "Self-supervised driven consistency training for annotation efficient\n  histopathology image analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a neural network with a large labeled dataset is still a dominant\nparadigm in computational histopathology. However, obtaining such exhaustive\nmanual annotations is often expensive, laborious, and prone to inter and\nIntra-observer variability. While recent self-supervised and semi-supervised\nmethods can alleviate this need by learn-ing unsupervised feature\nrepresentations, they still struggle to generalize well to downstream tasks\nwhen the number of labeled instances is small. In this work, we overcome this\nchallenge by leveraging both task-agnostic and task-specific unlabeled data\nbased on two novel strategies: i) a self-supervised pretext task that harnesses\nthe underlying multi-resolution contextual cues in histology whole-slide images\nto learn a powerful supervisory signal for unsupervised representation\nlearning; ii) a new teacher-student semi-supervised consistency paradigm that\nlearns to effectively transfer the pretrained representations to downstream\ntasks based on prediction consistency with the task-specific un-labeled data.\nWe carry out extensive validation experiments on three histopathology benchmark\ndatasets across two classification and one regression-based tasks, i.e., tumor\nmetastasis detection, tissue type classification, and tumor cellularity\nquantification. Under limited-label data, the proposed method yields tangible\nimprovements, which is close or even outperforming other state-of-the-art\nself-supervised and supervised baselines. Furthermore, we empirically show that\nthe idea of bootstrapping the self-supervised pretrained features is an\neffective way to improve the task-specific semi-supervised learning on standard\nbenchmarks. Code and pretrained models will be made available at:\nhttps://github.com/srinidhiPY/SSL_CR_Histo\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 19:46:21 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 23:26:44 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Srinidhi", "Chetan L.", ""], ["Kim", "Seung Wook", ""], ["Chen", "Fu-Der", ""], ["Martel", "Anne L.", ""]]}, {"id": "2102.03898", "submitter": "Rodolfo Quispe", "authors": "Rodolfo Quispe and Cuiling Lan and Wenjun Zeng and Helio Pedrini", "title": "AttributeNet: Attribute Enhanced Vehicle Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vehicle Re-Identification (V-ReID) is a critical task that associates the\nsame vehicle across images from different camera viewpoints. Many works explore\nattribute clues to enhance V-ReID; however, there is usually a lack of\neffective interaction between the attribute-related modules and final V-ReID\nobjective. In this work, we propose a new method to efficiently explore\ndiscriminative information from vehicle attributes (e.g., color and type). We\nintroduce AttributeNet (ANet) that jointly extracts identity-relevant features\nand attribute features. We enable the interaction by distilling the\nReID-helpful attribute feature and adding it into the general ReID feature to\nincrease the discrimination power. Moreover, we propose a constraint, named\nAmelioration Constraint (AC), which encourages the feature after adding\nattribute features onto the general ReID feature to be more discriminative than\nthe original general ReID feature. We validate the effectiveness of our\nframework on three challenging datasets. Experimental results show that our\nmethod achieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 19:51:02 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Quispe", "Rodolfo", ""], ["Lan", "Cuiling", ""], ["Zeng", "Wenjun", ""], ["Pedrini", "Helio", ""]]}, {"id": "2102.03921", "submitter": "Roman Malashin", "authors": "Roman Malashin ((1) Pavlov institute of Physiology RAS, (2) State\n  University of Aerospace Instrumentation, Saint-Petersburg, Russia)", "title": "Sparsely ensembled convolutional neural network classifiers via\n  reinforcement learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider convolutional neural network (CNN) ensemble learning with the\nobjective function inspired by least action principle; it includes resource\nconsumption component. We teach an agent to perceive images through the set of\npre-trained classifiers and want the resulting dynamically configured system to\nunfold the computational graph with the trajectory that refers to the minimal\nnumber of operations and maximal expected accuracy. The proposed agent's\narchitecture implicitly approximates the required classifier selection function\nwith the help of reinforcement learning. Our experimental results prove, that\nif the agent exploits the dynamic (and context-dependent) structure of\ncomputations, it outperforms conventional ensemble learning.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 21:26:57 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Malashin", "Roman", ""]]}, {"id": "2102.03924", "submitter": "Seong Jae Hwang", "authors": "Anthony Sicilia, Xingchen Zhao, Seong Jae Hwang", "title": "Domain Adversarial Neural Networks for Domain Generalization: When It\n  Works and How to Improve", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Theoretically, domain adaptation is a well-researched problem. Further, this\ntheory has been well-used in practice. In particular, we note the bound on\ntarget error given by Ben-David et al. (2010) and the well-known\ndomain-aligning algorithm based on this work using Domain Adversarial Neural\nNetworks (DANN) presented by Ganin and Lempitsky (2015). Recently, multiple\nvariants of DANN have been proposed for the related problem of domain\ngeneralization, but without much discussion of the original motivating bound.\nIn this paper, we investigate the validity of DANN in domain generalization\nfrom this perspective. We investigate conditions under which application of\nDANN makes sense and further consider DANN as a dynamic process during\ntraining. Our investigation suggests that the application of DANN to domain\ngeneralization may not be as straightforward as it seems. To address this, we\ndesign an algorithmic extension to DANN in the domain generalization case. Our\nexperimentation validates both theory and algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 21:33:41 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Sicilia", "Anthony", ""], ["Zhao", "Xingchen", ""], ["Hwang", "Seong Jae", ""]]}, {"id": "2102.03932", "submitter": "Fazael Ayatollahi", "authors": "Fazael Ayatollahi (1 and 2), Shahriar B. Shokouhi (1), Ritse M. Mann\n  (2), Jonas Teuwen (2 and 3) ((1) Electrical Engineering Department, Iran\n  University of Science and Technology (IUST), Tehran, Iran, (2) Department of\n  Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen,\n  the Netherlands, (3) Department of Radiation Oncology, Netherlands Cancer\n  Institute, Amsterdam, the Netherlands)", "title": "Automatic Breast Lesion Detection in Ultrafast DCE-MRI Using Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Purpose: We propose a deep learning-based computer-aided detection (CADe)\nmethod to detect breast lesions in ultrafast DCE-MRI sequences. This method\nuses both the three-dimensional spatial information and temporal information\nobtained from the early-phase of the dynamic acquisition.Methods: The proposed\nCADe method, based on a modified 3D RetinaNet model, operates on ultrafast T1\nweighted sequences, which are preprocessed for motion compensation, temporal\nnormalization, and are cropped before passing into the model. The model is\noptimized to enable the detection of relatively small breast lesions in a\nscreening setting, focusing on detection of lesions that are harder to\ndifferentiate from confounding structures inside the breast.Results: The method\nwas developed based on a dataset consisting of 489 ultrafast MRI studies\nobtained from 462 patients containing a total of 572 lesions (365 malignant,\n207 benign) and achieved a detection rate, sensitivity, and detection rate of\nbenign lesions of 0.90, 0.95, and 0.86 at 4 false positives per normal breast\nwith a 10-fold cross-validation, respectively.Conclusions: The deep learning\narchitecture used for the proposed CADe application can efficiently detect\nbenign and malignant lesions on ultrafast DCE-MRI. Furthermore, utilizing the\nless visible hard-to detect-lesions in training improves the learning process\nand, subsequently, detection of malignant breast lesions.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 22:03:39 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Ayatollahi", "Fazael", "", "1 and 2"], ["Shokouhi", "Shahriar B.", "", "2 and 3"], ["Mann", "Ritse M.", "", "2 and 3"], ["Teuwen", "Jonas", "", "2 and 3"]]}, {"id": "2102.03939", "submitter": "Nikolaos Zioulis Mr.", "authors": "Nikolaos Zioulis, Federico Alvarez, Dimitrios Zarpalas, Petros Daras", "title": "Single-Shot Cuboids: Geodesics-based End-to-end Manhattan Aligned Layout\n  Estimation from Spherical Panoramas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It has been shown that global scene understanding tasks like layout\nestimation can benefit from wider field of views, and specifically spherical\npanoramas. While much progress has been made recently, all previous approaches\nrely on intermediate representations and postprocessing to produce\nManhattan-aligned estimates. In this work we show how to estimate full room\nlayouts in a single-shot, eliminating the need for postprocessing. Our work is\nthe first to directly infer Manhattan-aligned outputs. To achieve this, our\ndata-driven model exploits direct coordinate regression and is supervised\nend-to-end. As a result, we can explicitly add quasi-Manhattan constraints,\nwhich set the necessary conditions for a homography-based Manhattan alignment\nmodule. Finally, we introduce the geodesic heatmaps and loss and a\nboundary-aware center of mass calculation that facilitate higher quality\nkeypoint estimation in the spherical domain. Our models and code are publicly\navailable at https://vcl3d.github.io/SingleShotCuboids/.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 22:52:59 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 08:46:37 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Zioulis", "Nikolaos", ""], ["Alvarez", "Federico", ""], ["Zarpalas", "Dimitrios", ""], ["Daras", "Petros", ""]]}, {"id": "2102.03942", "submitter": "Eva Cetinic", "authors": "Eva Cetinic", "title": "Iconographic Image Captioning for Artworks", "comments": "Accepted at Workshop on Fine Art Pattern Extraction and Recognition\n  (FAPER), ICPR, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning implies automatically generating textual descriptions of\nimages based only on the visual input. Although this has been an extensively\naddressed research topic in recent years, not many contributions have been made\nin the domain of art historical data. In this particular context, the task of\nimage captioning is confronted with various challenges such as the lack of\nlarge-scale datasets of image-text pairs, the complexity of meaning associated\nwith describing artworks and the need for expert-level annotations. This work\naims to address some of those challenges by utilizing a novel large-scale\ndataset of artwork images annotated with concepts from the Iconclass\nclassification system designed for art and iconography. The annotations are\nprocessed into clean textual description to create a dataset suitable for\ntraining a deep neural network model on the image captioning task. Motivated by\nthe state-of-the-art results achieved in generating captions for natural\nimages, a transformer-based vision-language pre-trained model is fine-tuned\nusing the artwork image dataset. Quantitative evaluation of the results is\nperformed using standard image captioning metrics. The quality of the generated\ncaptions and the model's capacity to generalize to new data is explored by\nemploying the model on a new collection of paintings and performing an analysis\nof the relation between commonly generated captions and the artistic genre. The\noverall results suggest that the model can generate meaningful captions that\nexhibit a stronger relevance to the art historical context, particularly in\ncomparison to captions obtained from models trained only on natural image\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 23:11:33 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Cetinic", "Eva", ""]]}, {"id": "2102.03973", "submitter": "Xin Zhao", "authors": "Xin Zhao, Jifeng Guo, Lin Wang, Fanqi Li, Junteng Zheng and Bo Yang", "title": "Solid Texture Synthesis using Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solid texture synthesis, as an effective way to extend 2D exemplar to a\nvolumetric texture, exhibits advantages in numerous application domains.\nHowever, existing methods generally suffer from synthesis distortion due to the\nunder-utilization of information. In this paper, we propose a novel approach\nfor the solid texture synthesis based on generative adversarial networks(GANs),\nnamed STS-GAN, learning the distribution of 2D exemplars with volumetric\noperation in a feature-free manner. The multi-scale discriminators evaluate the\nsimilarities between patch exemplars and slices from generated volume,\npromoting the generator to synthesize realistic solid texture. Experimental\nresults demonstrate that the proposed method can synthesize high-quality solid\ntexture with similar visual characteristics to the exemplar.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 02:51:34 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 11:29:46 GMT"}, {"version": "v3", "created": "Sat, 29 May 2021 16:12:31 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhao", "Xin", ""], ["Guo", "Jifeng", ""], ["Wang", "Lin", ""], ["Li", "Fanqi", ""], ["Zheng", "Junteng", ""], ["Yang", "Bo", ""]]}, {"id": "2102.03982", "submitter": "Jinjiang Guo Ph.D.", "authors": "Jinjiang Guo, Vincent Vidal, Irene Cheng, Anup Basu, Atilla Baskurt,\n  Guillaume Lavoue", "title": "Subjective and Objective Visual Quality Assessment of Textured 3D Meshes", "comments": null, "journal-ref": null, "doi": "10.1145/2996296", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Objective visual quality assessment of 3D models is a fundamental issue in\ncomputer graphics. Quality assessment metrics may allow a wide range of\nprocesses to be guided and evaluated, such as level of detail creation,\ncompression, filtering, and so on. Most computer graphics assets are composed\nof geometric surfaces on which several texture images can be mapped to 11 make\nthe rendering more realistic. While some quality assessment metrics exist for\ngeometric surfaces, almost no research has been conducted on the evaluation of\ntexture-mapped 3D models. In this context, we present a new subjective study to\nevaluate the perceptual quality of textured meshes, based on a paired\ncomparison protocol. We introduce both texture and geometry distortions on a\nset of 5 reference models to produce a database of 136 distorted models,\nevaluated using two rendering protocols. Based on analysis of the results, we\npropose two new metrics for visual quality assessment of textured mesh, as\noptimized linear combinations of accurate geometry and texture quality\nmeasurements. These proposed perceptual metrics outperform their counterparts\nin terms of correlation with human opinion. The database, along with the\nassociated subjective scores, will be made publicly available online.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 03:26:41 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Guo", "Jinjiang", ""], ["Vidal", "Vincent", ""], ["Cheng", "Irene", ""], ["Basu", "Anup", ""], ["Baskurt", "Atilla", ""], ["Lavoue", "Guillaume", ""]]}, {"id": "2102.03983", "submitter": "Zhiqiang Shen", "authors": "Zhiqiang Shen and Zechun Liu and Jie Qin and Marios Savvides and\n  Kwang-Ting Cheng", "title": "Partial Is Better Than All: Revisiting Fine-tuning Strategy for Few-shot\n  Learning", "comments": "AAAI 2021. A search based fine-tuning strategy for few-shot learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of few-shot learning is to learn a classifier that can recognize\nunseen classes from limited support data with labels. A common practice for\nthis task is to train a model on the base set first and then transfer to novel\nclasses through fine-tuning (Here fine-tuning procedure is defined as\ntransferring knowledge from base to novel data, i.e. learning to transfer in\nfew-shot scenario.) or meta-learning. However, as the base classes have no\noverlap to the novel set, simply transferring whole knowledge from base data is\nnot an optimal solution since some knowledge in the base model may be biased or\neven harmful to the novel class. In this paper, we propose to transfer partial\nknowledge by freezing or fine-tuning particular layer(s) in the base model.\nSpecifically, layers will be imposed different learning rates if they are\nchosen to be fine-tuned, to control the extent of preserved transferability. To\ndetermine which layers to be recast and what values of learning rates for them,\nwe introduce an evolutionary search based method that is efficient to\nsimultaneously locate the target layers and determine their individual learning\nrates. We conduct extensive experiments on CUB and mini-ImageNet to demonstrate\nthe effectiveness of our proposed method. It achieves the state-of-the-art\nperformance on both meta-learning and non-meta based frameworks. Furthermore,\nwe extend our method to the conventional pre-training + fine-tuning paradigm\nand obtain consistent improvement.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 03:27:05 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Shen", "Zhiqiang", ""], ["Liu", "Zechun", ""], ["Qin", "Jie", ""], ["Savvides", "Marios", ""], ["Cheng", "Kwang-Ting", ""]]}, {"id": "2102.03984", "submitter": "Guangming Yao", "authors": "Guangming Yao, Yi Yuan, Tianjia Shao, Shuang Li, Shanqi Liu, Yong Liu,\n  Mengmeng Wang, Kun Zhou", "title": "One-shot Face Reenactment Using Appearance Adaptive Normalization", "comments": "9 pages, 8 figures,3 tables ,Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper proposes a novel generative adversarial network for one-shot face\nreenactment, which can animate a single face image to a different\npose-and-expression (provided by a driving image) while keeping its original\nappearance. The core of our network is a novel mechanism called appearance\nadaptive normalization, which can effectively integrate the appearance\ninformation from the input image into our face generator by modulating the\nfeature maps of the generator using the learned adaptive parameters.\nFurthermore, we specially design a local net to reenact the local facial\ncomponents (i.e., eyes, nose and mouth) first, which is a much easier task for\nthe network to learn and can in turn provide explicit anchors to guide our face\ngenerator to learn the global appearance and pose-and-expression. Extensive\nquantitative and qualitative experiments demonstrate the significant efficacy\nof our model compared with prior one-shot methods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 03:36:30 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 10:38:37 GMT"}, {"version": "v3", "created": "Mon, 26 Apr 2021 08:05:06 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Yao", "Guangming", ""], ["Yuan", "Yi", ""], ["Shao", "Tianjia", ""], ["Li", "Shuang", ""], ["Liu", "Shanqi", ""], ["Liu", "Yong", ""], ["Wang", "Mengmeng", ""], ["Zhou", "Kun", ""]]}, {"id": "2102.03992", "submitter": "Babak Maser", "authors": "Babak Maser, Andreas Uhl", "title": "Identifying the Origin of Finger Vein Samples Using Texture Descriptors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Identifying the origin of a sample image in biometric systems can be\nbeneficial for data authentication in case of attacks against the system and\nfor initiating sensor-specific processing pipelines in sensor-heterogeneous\nenvironments. Motivated by shortcomings of the photo response non-uniformity\n(PRNU) based method in the biometric context, we use a texture classification\napproach to detect the origin of finger vein sample images. Based on eight\npublicly available finger vein datasets and applying eight classical yet simple\ntexture descriptors and SVM classification, we demonstrate excellent sensor\nmodel identification results for raw finger vein samples as well as for the\nmore challenging region of interest data. The observed results establish\ntexture descriptors as effective competitors to PRNU in finger vein sensor\nmodel identification.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 03:59:14 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Maser", "Babak", ""], ["Uhl", "Andreas", ""]]}, {"id": "2102.04010", "submitter": "Aojun Zhou", "authors": "Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan,\n  Wenxiu Sun, Hongsheng Li", "title": "Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch", "comments": "ICLR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity in Deep Neural Networks (DNNs) has been widely studied to compress\nand accelerate the models on resource-constrained environments. It can be\ngenerally categorized into unstructured fine-grained sparsity that zeroes out\nmultiple individual weights distributed across the neural network, and\nstructured coarse-grained sparsity which prunes blocks of sub-networks of a\nneural network. Fine-grained sparsity can achieve a high compression ratio but\nis not hardware friendly and hence receives limited speed gains. On the other\nhand, coarse-grained sparsity cannot concurrently achieve both apparent\nacceleration on modern GPUs and decent performance. In this paper, we are the\nfirst to study training from scratch an N:M fine-grained structured sparse\nnetwork, which can maintain the advantages of both unstructured fine-grained\nsparsity and structured coarse-grained sparsity simultaneously on specifically\ndesigned GPUs. Specifically, a 2:4 sparse network could achieve 2x speed-up\nwithout performance drop on Nvidia A100 GPUs. Furthermore, we propose a novel\nand effective ingredient, sparse-refined straight-through estimator (SR-STE),\nto alleviate the negative influence of the approximated gradients computed by\nvanilla STE during optimization. We also define a metric, Sparse Architecture\nDivergence (SAD), to measure the sparse network's topology change during the\ntraining process. Finally, We justify SR-STE's advantages with SAD and\ndemonstrate the effectiveness of SR-STE by performing comprehensive experiments\non various tasks. Source codes and models are available at\nhttps://github.com/NM-sparsity/NM-sparsity.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 05:55:47 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 10:18:00 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zhou", "Aojun", ""], ["Ma", "Yukun", ""], ["Zhu", "Junnan", ""], ["Liu", "Jianbo", ""], ["Zhang", "Zhijie", ""], ["Yuan", "Kun", ""], ["Sun", "Wenxiu", ""], ["Li", "Hongsheng", ""]]}, {"id": "2102.04014", "submitter": "Binh-Son Hua", "authors": "Trung Nguyen, Quang-Hieu Pham, Tam Le, Tung Pham, Nhat Ho, Binh-Son\n  Hua", "title": "Point-set Distances for Learning Representations of 3D Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning an effective representation of 3D point clouds requires a good\nmetric to measure the discrepancy between two 3D point sets, which is\nnon-trivial due to their irregularity. Most of the previous works resort to\nusing the Chamfer discrepancy or Earth Mover's distance, but those metrics are\neither ineffective in measuring the differences between point clouds or\ncomputationally expensive. In this paper, we conduct a systematic study with\nextensive experiments on distance metrics for 3D point clouds. From this study,\nwe propose to use a variant of the Wasserstein distance, named the sliced\nWasserstein distance, for learning representations of 3D point clouds.\nExperiments show that the sliced Wasserstein distance allows the neural network\nto learn a more efficient representation compared to the Chamfer discrepancy.\nWe demonstrate the efficiency of the sliced Wasserstein metric on several tasks\nin 3D computer vision including training a point cloud autoencoder, generative\nmodeling, transfer learning, and point cloud registration.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 06:09:13 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Nguyen", "Trung", ""], ["Pham", "Quang-Hieu", ""], ["Le", "Tam", ""], ["Pham", "Tung", ""], ["Ho", "Nhat", ""], ["Hua", "Binh-Son", ""]]}, {"id": "2102.04016", "submitter": "Osman Tursun", "authors": "Osman Tursun, Simon Denman, Sridha Sridharan, Ethan Goan and Clinton\n  Fookes", "title": "An Efficient Framework for Zero-Shot Sketch-Based Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Zero-shot Sketch-based Image Retrieval (ZS-SBIR) has attracted the\nattention of the computer vision community due to it's real-world applications,\nand the more realistic and challenging setting than found in SBIR. ZS-SBIR\ninherits the main challenges of multiple computer vision problems including\ncontent-based Image Retrieval (CBIR), zero-shot learning and domain adaptation.\nThe majority of previous studies using deep neural networks have achieved\nimproved results through either projecting sketch and images into a common\nlow-dimensional space or transferring knowledge from seen to unseen classes.\nHowever, those approaches are trained with complex frameworks composed of\nmultiple deep convolutional neural networks (CNNs) and are dependent on\ncategory-level word labels. This increases the requirements on training\nresources and datasets. In comparison, we propose a simple and efficient\nframework that does not require high computational training resources, and can\nbe trained on datasets without semantic categorical labels. Furthermore, at\ntraining and inference stages our method only uses a single CNN. In this work,\na pre-trained ImageNet CNN (e.g., ResNet50) is fine-tuned with three proposed\nlearning objects: domain-aware quadruplet loss, semantic classification loss,\nand semantic knowledge preservation loss. The domain-aware quadruplet and\nsemantic classification losses are introduced to learn discriminative, semantic\nand domain invariant features through considering ZS-SBIR as object detection\nand verification problem. ...\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 06:10:37 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Tursun", "Osman", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Goan", "Ethan", ""], ["Fookes", "Clinton", ""]]}, {"id": "2102.04018", "submitter": "Ivan Bajic", "authors": "Mateen Ulhaq, Ivan V. Baji\\'c", "title": "Analysis of Latent-Space Motion for Collaborative Intelligence", "comments": "6 pages, 6 figures, extended version of an IEEE ICASSP 2021 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When the input to a deep neural network (DNN) is a video signal, a sequence\nof feature tensors is produced at the intermediate layers of the model. If\nneighboring frames of the input video are related through motion, a natural\nquestion is, \"what is the relationship between the corresponding feature\ntensors?\" By analyzing the effect of common DNN operations on optical flow, we\nshow that the motion present in each channel of a feature tensor is\napproximately equal to the scaled version of the input motion. The analysis is\nvalidated through experiments utilizing common motion models. %These results\nwill be useful in collaborative intelligence applications where sequences of\nfeature tensors need to be compressed or further analyzed.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 06:22:07 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Ulhaq", "Mateen", ""], ["Baji\u0107", "Ivan V.", ""]]}, {"id": "2102.04024", "submitter": "Scott Sun", "authors": "Scott Sun, Dennis Melamed, Kris Kitani", "title": "IDOL: Inertial Deep Orientation-Estimation and Localization", "comments": "To be published in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many smartphone applications use inertial measurement units (IMUs) to sense\nmovement, but the use of these sensors for pedestrian localization can be\nchallenging due to their noise characteristics. Recent data-driven inertial\nodometry approaches have demonstrated the increasing feasibility of inertial\nnavigation. However, they still rely upon conventional smartphone orientation\nestimates that they assume to be accurate, while in fact these orientation\nestimates can be a significant source of error. To address the problem of\ninaccurate orientation estimates, we present a two-stage, data-driven pipeline\nusing a commodity smartphone that first estimates device orientations and then\nestimates device position. The orientation module relies on a recurrent neural\nnetwork and Extended Kalman Filter to obtain orientation estimates that are\nused to then rotate raw IMU measurements into the appropriate reference frame.\nThe position module then passes those measurements through another recurrent\nnetwork architecture to perform localization. Our proposed method outperforms\nstate-of-the-art methods in both orientation and position error on a large\ndataset we constructed that contains 20 hours of pedestrian motion across 3\nbuildings and 15 subjects.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 06:41:47 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Sun", "Scott", ""], ["Melamed", "Dennis", ""], ["Kitani", "Kris", ""]]}, {"id": "2102.04033", "submitter": "Shiyao Wang", "authors": "Shiyao Wang, Qi Liu, Tiezheng Ge, Defu Lian and Zhiqiang Zhang", "title": "A Hybrid Bandit Model with Visual Priors for Creative Ranking in Display\n  Advertising", "comments": "To be published in the International World Wide Web Conference (WWW)\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creative plays a great important role in e-commerce for exhibiting products.\nSellers usually create multiple creatives for comprehensive demonstrations,\nthus it is crucial to display the most appealing design to maximize the\nClick-Through Rate~(CTR). For this purpose, modern recommender systems\ndynamically rank creatives when a product is proposed for a user. However, this\ntask suffers more cold-start problem than conventional products recommendation\nIn this paper, we propose a hybrid bandit model with visual priors which first\nmakes predictions with a visual evaluation, and then naturally evolves to focus\non the specialities through the hybrid bandit model. Our contributions are\nthree-fold: 1) We present a visual-aware ranking model (called VAM) that\nincorporates a list-wise ranking loss for ordering the creatives according to\nthe visual appearance. 2) Regarding visual evaluations as a prior, the hybrid\nbandit model (called HBM) is proposed to evolve consistently to make better\nposteriori estimations by taking more observations into consideration for\nonline scenarios. 3) A first large-scale creative dataset, CreativeRanking, is\nconstructed, which contains over 1.7M creatives of 500k products as well as\ntheir real impression and click data. Extensive experiments have also been\nconducted on both our dataset and public Mushroom dataset, demonstrating the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 07:11:20 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 08:24:00 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Wang", "Shiyao", ""], ["Liu", "Qi", ""], ["Ge", "Tiezheng", ""], ["Lian", "Defu", ""], ["Zhang", "Zhiqiang", ""]]}, {"id": "2102.04034", "submitter": "Andrew Palmer", "authors": "Andrew W. Palmer, Albi Sema, Wolfram Martens, Peter Rudolph and\n  Wolfgang Waizenegger", "title": "The Autonomous Siemens Tram", "comments": "6 pages, presented at the 2020 International Conference on\n  Intelligent Transportation Systems (ITSC)", "journal-ref": "A. W. Palmer, A. Sema, W. Martens, P. Rudolph and W. Waizenegger,\n  \"The Autonomous Siemens Tram,\" 2020 IEEE 23rd ITSC, 2020, pp. 1-6", "doi": "10.1109/ITSC45102.2020.9294699", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the Autonomous Siemens Tram that was publicly\ndemonstrated in Potsdam, Germany during the InnoTrans 2018 exhibition. The\nsystem was built on a Siemens Combino tram and used a multi-modal sensor suite\nto localize the vehicle, and to detect and respond to traffic signals and\nobstacles. An overview of the hardware and the developed localization, signal\nhandling, and obstacle handling components is presented, along with a summary\nof their performance.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 07:13:58 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Palmer", "Andrew W.", ""], ["Sema", "Albi", ""], ["Martens", "Wolfram", ""], ["Rudolph", "Peter", ""], ["Waizenegger", "Wolfgang", ""]]}, {"id": "2102.04035", "submitter": "Lijuan Liu", "authors": "Lijuan Liu, Yin Yang, Yi Yuan, Tianjia Shao, He Wang and Kun Zhou", "title": "In-game Residential Home Planning via Visual Context-aware Global\n  Relation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose an effective global relation learning algorithm to\nrecommend an appropriate location of a building unit for in-game customization\nof residential home complex. Given a construction layout, we propose a visual\ncontext-aware graph generation network that learns the implicit global\nrelations among the scene components and infers the location of a new building\nunit. The proposed network takes as input the scene graph and the corresponding\ntop-view depth image. It provides the location recommendations for a\nnewly-added building units by learning an auto-regressive edge distribution\nconditioned on existing scenes. We also introduce a global graph-image matching\nloss to enhance the awareness of essential geometry semantics of the site.\nQualitative and quantitative experiments demonstrate that the recommended\nlocation well reflects the implicit spatial rules of components in the\nresidential estates, and it is instructive and practical to locate the building\nunits in the 3D scene of the complex construction.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 07:15:47 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 13:10:23 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Liu", "Lijuan", ""], ["Yang", "Yin", ""], ["Yuan", "Yi", ""], ["Shao", "Tianjia", ""], ["Wang", "He", ""], ["Zhou", "Kun", ""]]}, {"id": "2102.04046", "submitter": "Geng Chen", "authors": "Hong-Bo Bi, Zi-Qi Liu, Kang Wang, Bo Dong, Geng Chen, Ji-Quan Ma", "title": "Towards Accurate RGB-D Saliency Detection with Complementary Attention\n  and Adaptive Integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency detection based on the complementary information from RGB images and\ndepth maps has recently gained great popularity. In this paper, we propose\nComplementary Attention and Adaptive Integration Network (CAAI-Net), a novel\nRGB-D saliency detection model that integrates complementary attention based\nfeature concentration and adaptive cross-modal feature fusion into a unified\nframework for accurate saliency detection. Specifically, we propose a\ncontext-aware complementary attention (CCA) module, which consists of a feature\ninteraction component, a complementary attention component, and a\nglobal-context component. The CCA module first utilizes the feature interaction\ncomponent to extract rich local context features. The resulting features are\nthen fed into the complementary attention component, which employs the\ncomplementary attention generated from adjacent levels to guide the attention\nat the current layer so that the mutual background disturbances are suppressed\nand the network focuses more on the areas with salient objects. Finally, we\nutilize a specially-designed adaptive feature integration (AFI) module, which\nsufficiently considers the low-quality issue of depth maps, to aggregate the\nRGB and depth features in an adaptive manner. Extensive experiments on six\nchallenging benchmark datasets demonstrate that CAAI-Net is an effective\nsaliency detection model and outperforms nine state-of-the-art models in terms\nof four widely-used metrics. In addition, extensive ablation studies confirm\nthe effectiveness of the proposed CCA and AFI modules.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 08:08:30 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Bi", "Hong-Bo", ""], ["Liu", "Zi-Qi", ""], ["Wang", "Kang", ""], ["Dong", "Bo", ""], ["Chen", "Geng", ""], ["Ma", "Ji-Quan", ""]]}, {"id": "2102.04057", "submitter": "Apostolos Modas", "authors": "Apostolos Modas and Alessio Xompero and Ricardo Sanchez-Matilla and\n  Pascal Frossard and Andrea Cavallaro", "title": "Improving filling level classification with adversarial training", "comments": "Accepted to the 28th IEEE International Conference on Image\n  Processing (ICIP) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of classifying - from a single image - the level\nof content in a cup or a drinking glass. This problem is made challenging by\nseveral ambiguities caused by transparencies, shape variations and partial\nocclusions, and by the availability of only small training datasets. In this\npaper, we tackle this problem with an appropriate strategy for transfer\nlearning. Specifically, we use adversarial training in a generic source dataset\nand then refine the training with a task-specific dataset. We also discuss and\nexperimentally evaluate several training strategies and their combination on a\nrange of container types of the CORSMAL Containers Manipulation dataset. We\nshow that transfer learning with adversarial training in the source domain\nconsistently improves the classification accuracy on the test set and limits\nthe overfitting of the classifier to specific features of the training data.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 08:32:56 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 09:36:06 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Modas", "Apostolos", ""], ["Xompero", "Alessio", ""], ["Sanchez-Matilla", "Ricardo", ""], ["Frossard", "Pascal", ""], ["Cavallaro", "Andrea", ""]]}, {"id": "2102.04060", "submitter": "Maxime Ferrera", "authors": "Maxime Ferrera, Alexandre Eudes, Julien Moras, Martial Sanfourche, Guy\n  Le Besnerais", "title": "OV$^{2}$SLAM : A Fully Online and Versatile Visual SLAM for Real-Time\n  Applications", "comments": "Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L). Code is available at : \\url{https://github.com/ov2slam/ov2slam}", "journal-ref": "IEEE Robotics and Automation Letters, IEEE 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications of Visual SLAM, such as augmented reality, virtual reality,\nrobotics or autonomous driving, require versatile, robust and precise\nsolutions, most often with real-time capability. In this work, we describe\nOV$^{2}$SLAM, a fully online algorithm, handling both monocular and stereo\ncamera setups, various map scales and frame-rates ranging from a few Hertz up\nto several hundreds. It combines numerous recent contributions in visual\nlocalization within an efficient multi-threaded architecture. Extensive\ncomparisons with competing algorithms shows the state-of-the-art accuracy and\nreal-time performance of the resulting algorithm. For the benefit of the\ncommunity, we release the source code:\n\\url{https://github.com/ov2slam/ov2slam}.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 08:39:23 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Ferrera", "Maxime", ""], ["Eudes", "Alexandre", ""], ["Moras", "Julien", ""], ["Sanfourche", "Martial", ""], ["Besnerais", "Guy Le", ""]]}, {"id": "2102.04075", "submitter": "Kai Chen", "authors": "Kai Chen, Qi Lv, Taihe Yi", "title": "Fast and Reliable Probabilistic Face Embeddings in the Wild", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic Face Embeddings (PFE) can improve face recognition performance\nin unconstrained scenarios by integrating data uncertainty into the feature\nrepresentation. However, existing PFE methods tend to be over-confident in\nestimating uncertainty and is too slow to apply to large-scale face matching.\nThis paper proposes a regularized probabilistic face embedding method to\nimprove the robustness and speed of PFE. Specifically, the mutual likelihood\nscore (MLS) metric used in PFE is simplified to speedup the matching of face\nfeature pairs. Then, an output-constraint loss is proposed to penalize the\nvariance of the uncertainty output, which can regularize the output of the\nneural network. In addition, an identification preserving loss is proposed to\nimprove the discriminative of the MLS metric, and a multi-layer feature fusion\nmodule is proposed to improve the neural network's uncertainty estimation\nability. Comprehensive experiments show that the proposed method can achieve\ncomparable or better results in 9 benchmarks than the state-of-the-art methods,\nand can improve the performance of risk-controlled face recognition. The code\nof our work is publicly available in GitHub\n(https://github.com/KaenChan/ProbFace).\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 09:27:57 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 08:35:32 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 04:37:05 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Chen", "Kai", ""], ["Lv", "Qi", ""], ["Yi", "Taihe", ""]]}, {"id": "2102.04091", "submitter": "Elena Luna", "authors": "Elena Luna, Juan C. SanMiguel, Jose M. Mart\\'inez, and Marcos\n  Escudero-Vi\\~nolo", "title": "Online Clustering-based Multi-Camera Vehicle Tracking in Scenarios with\n  overlapping FOVs", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Target Multi-Camera (MTMC) vehicle tracking is an essential task of\nvisual traffic monitoring, one of the main research fields of Intelligent\nTransportation Systems. Several offline approaches have been proposed to\naddress this task; however, they are not compatible with real-world\napplications due to their high latency and post-processing requirements. In\nthis paper, we present a new low-latency online approach for MTMC tracking in\nscenarios with partially overlapping fields of view (FOVs), such as road\nintersections. Firstly, the proposed approach detects vehicles at each camera.\nThen, the detections are merged between cameras by applying cross-camera\nclustering based on appearance and location. Lastly, the clusters containing\ndifferent detections of the same vehicle are temporally associated to compute\nthe tracks on a frame-by-frame basis. The experiments show promising\nlow-latency results while addressing real-world challenges such as the a priori\nunknown and time-varying number of targets and the continuous state estimation\nof them without performing any post-processing of the trajectories.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 09:55:55 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Luna", "Elena", ""], ["SanMiguel", "Juan C.", ""], ["Mart\u00ednez", "Jose M.", ""], ["Escudero-Vi\u00f1olo", "Marcos", ""]]}, {"id": "2102.04136", "submitter": "Kenneth Blomqvist", "authors": "Jo\\\"el Bachmann, Kenneth Blomqvist, Julian F\\\"orster, Roland Siegwart", "title": "Points2Vec: Unsupervised Object-level Feature Learning from Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised representation learning techniques, such as learning word\nembeddings, have had a significant impact on the field of natural language\nprocessing. Similar representation learning techniques have not yet become\ncommonplace in the context of 3D vision. This, despite the fact that the\nphysical 3D spaces have a similar semantic structure to bodies of text: words\nare surrounded by words that are semantically related, just like objects are\nsurrounded by other objects that are similar in concept and usage.\n  In this work, we exploit this structure in learning semantically meaningful\nlow dimensional vector representations of objects. We learn these vector\nrepresentations by mining a dataset of scanned 3D spaces using an unsupervised\nalgorithm. We represent objects as point clouds, a flexible and general\nrepresentation for 3D data, which we encode into a vector representation. We\nshow that using our method to include context increases the ability of a\nclustering algorithm to distinguish different semantic classes from each other.\nFurthermore, we show that our algorithm produces continuous and meaningful\nobject embeddings through interpolation experiments.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 11:29:57 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Bachmann", "Jo\u00ebl", ""], ["Blomqvist", "Kenneth", ""], ["F\u00f6rster", "Julian", ""], ["Siegwart", "Roland", ""]]}, {"id": "2102.04139", "submitter": "Ali Ghofrani", "authors": "Ali Ghofrani, Rahil Mahdian Toroghi, Seyed Mojtaba Tabatabaie", "title": "APS: A Large-Scale Multi-Modal Indoor Camera Positioning System", "comments": "15 pages, 11 figures, MedPRAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Navigation inside a closed area with no GPS-signal accessibility is a highly\nchallenging task. In order to tackle this problem, recently the imaging-based\nmethods have grabbed the attention of many researchers. These methods either\nextract the features (e.g. using SIFT, or SOSNet) and map the descriptive ones\nto the camera position and rotation information, or deploy an end-to-end system\nthat directly estimates this information out of RGB images, similar to PoseNet.\nWhile the former methods suffer from heavy computational burden during the test\nprocess, the latter suffers from lack of accuracy and robustness against\nenvironmental changes and object movements. However, end-to-end systems are\nquite fast during the test and inference and are pretty qualified for\nreal-world applications, even though their training phase could be longer than\nthe former ones. In this paper, a novel multi-modal end-to-end system for\nlarge-scale indoor positioning has been proposed, namely APS (Alpha Positioning\nSystem), which integrates a Pix2Pix GAN network to reconstruct the point cloud\npair of the input query image, with a deep CNN network in order to robustly\nestimate the position and rotation information of the camera. For this\nintegration, the existing datasets have the shortcoming of paired RGB/point\ncloud images for indoor environments. Therefore, we created a new dataset to\nhandle this situation. By implementing the proposed APS system, we could\nachieve a highly accurate camera positioning with a precision level of less\nthan a centimeter.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 11:37:09 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Ghofrani", "Ali", ""], ["Toroghi", "Rahil Mahdian", ""], ["Tabatabaie", "Seyed Mojtaba", ""]]}, {"id": "2102.04144", "submitter": "Mostafa Sadeghi", "authors": "Mostafa Sadeghi, Xavier Alameda-Pineda", "title": "Switching Variational Auto-Encoders for Noise-Agnostic Audio-visual\n  Speech Enhancement", "comments": "2021 IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, audio-visual speech enhancement has been tackled in the\nunsupervised settings based on variational auto-encoders (VAEs), where during\ntraining only clean data is used to train a generative model for speech, which\nat test time is combined with a noise model, e.g. nonnegative matrix\nfactorization (NMF), whose parameters are learned without supervision.\nConsequently, the proposed model is agnostic to the noise type. When visual\ndata are clean, audio-visual VAE-based architectures usually outperform the\naudio-only counterpart. The opposite happens when the visual data are corrupted\nby clutter, e.g. the speaker not facing the camera. In this paper, we propose\nto find the optimal combination of these two architectures through time. More\nprecisely, we introduce the use of a latent sequential variable with Markovian\ndependencies to switch between different VAE architectures through time in an\nunsupervised manner: leading to switching variational auto-encoder (SwVAE). We\npropose a variational factorization to approximate the computationally\nintractable posterior distribution. We also derive the corresponding\nvariational expectation-maximization algorithm to estimate the parameters of\nthe model and enhance the speech signal. Our experiments demonstrate the\npromising performance of SwVAE.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 11:45:02 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Sadeghi", "Mostafa", ""], ["Alameda-Pineda", "Xavier", ""]]}, {"id": "2102.04154", "submitter": "Jan Metzen", "authors": "Jan Hendrik Metzen, Maksym Yatsura", "title": "Efficient Certified Defenses Against Patch Attacks on Image Classifiers", "comments": "accepted at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial patches pose a realistic threat model for physical world attacks\non autonomous systems via their perception component. Autonomous systems in\nsafety-critical domains such as automated driving should thus contain a\nfail-safe fallback component that combines certifiable robustness against\npatches with efficient inference while maintaining high performance on clean\ninputs. We propose BagCert, a novel combination of model architecture and\ncertification procedure that allows efficient certification. We derive a loss\nthat enables end-to-end optimization of certified robustness against patches of\ndifferent sizes and locations. On CIFAR10, BagCert certifies 10.000 examples in\n43 seconds on a single GPU and obtains 86% clean and 60% certified accuracy\nagainst 5x5 patches.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 12:11:41 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Metzen", "Jan Hendrik", ""], ["Yatsura", "Maksym", ""]]}, {"id": "2102.04179", "submitter": "Nuno M. Rodrigues", "authors": "Nuno M. Rodrigues, Jo\\~ao E. Batista, Leonardo Trujillo, Bernardo\n  Duarte, Mario Giacobini, Leonardo Vanneschi, Sara Silva", "title": "Plotting time: On the usage of CNNs for time series classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel approach for time series classification where we represent\ntime series data as plot images and feed them to a simple CNN, outperforming\nseveral state-of-the-art methods. We propose a simple and highly replicable way\nof plotting the time series, and feed these images as input to a non-optimized\nshallow CNN, without any normalization or residual connections. These\nrepresentations are no more than default line plots using the time series data,\nwhere the only pre-processing applied is to reduce the number of white pixels\nin the image. We compare our method with different state-of-the-art methods\nspecialized in time series classification on two real-world non public\ndatasets, as well as 98 datasets of the UCR dataset collection. The results\nshow that our approach is very promising, achieving the best results on both\nreal-world datasets and matching / beating the best state-of-the-art methods in\nsix UCR datasets. We argue that, if a simple naive design like ours can obtain\nsuch good results, it is worth further exploring the capabilities of using\nimage representation of time series data, along with more powerful CNNs, for\nclassification and other related tasks.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 13:23:01 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Rodrigues", "Nuno M.", ""], ["Batista", "Jo\u00e3o E.", ""], ["Trujillo", "Leonardo", ""], ["Duarte", "Bernardo", ""], ["Giacobini", "Mario", ""], ["Vanneschi", "Leonardo", ""], ["Silva", "Sara", ""]]}, {"id": "2102.04202", "submitter": "Shoffan Saifullah", "authors": "Shoffan Saifullah", "title": "Segmentasi Citra Menggunakan Metode Watershed Transform Berdasarkan\n  Image Enhancement Dalam Mendeteksi Embrio Telur", "comments": "8 pages, in Indonesian language, 6 figures", "journal-ref": "Systemic: Information System and Informatics Journal, 5(2),\n  (2019), 53-60", "doi": "10.29080/systemic.v5i2.798", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image processing can be applied in the detection of egg embryos. The egg\nembryos detection is processed using a segmentation process. The segmentation\ndivides the image according to the area that is divided. This process requires\nimprovement of the image that is processed to obtain optimal results. This\nstudy will analyze the detection of egg embryos based on image processing with\nimage enhancement and the concept of segmentation using the watershed method.\nImage enhancement in preprocessing in image improvement uses a combination of\nContrast Limited Adaptive Histogram Equalization (CLAHE) and Histogram\nEqualization (HE) methods. The grayscale egg image is corrected using the CLAHE\nmethod, and the results are reprocessed using HE. The image improvement results\nshow that the CLAHE-HE combination method gives a clear picture of the object\narea of the egg image that has an embryo. The segmentation process using image\nconversion to black and white image and watershed segmentation can clearly show\nthe object of a chicken egg that has an embryo. The results of segmentation can\ndivide the area of the egg having embryos in a real and accurate way with a\npercentage \\approx 98\\%.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 14:03:51 GMT"}], "update_date": "2021-02-14", "authors_parsed": [["Saifullah", "Shoffan", ""]]}, {"id": "2102.04223", "submitter": "Yonghyun Kim", "authors": "Yonghyun Kim and Wonpyo Park", "title": "Multi-level Distance Regularization for Deep Metric Learning", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel distance-based regularization method for deep metric\nlearning called Multi-level Distance Regularization (MDR). MDR explicitly\ndisturbs a learning procedure by regularizing pairwise distances between\nembedding vectors into multiple levels that represents a degree of similarity\nbetween a pair. In the training stage, the model is trained with both MDR and\nan existing loss function of deep metric learning, simultaneously; the two\nlosses interfere with the objective of each other, and it makes the learning\nprocess difficult. Moreover, MDR prevents some examples from being ignored or\noverly influenced in the learning process. These allow the parameters of the\nembedding network to be settle on a local optima with better generalization.\nWithout bells and whistles, MDR with simple Triplet loss achieves\nthe-state-of-the-art performance in various benchmark datasets: CUB-200-2011,\nCars-196, Stanford Online Products, and In-Shop Clothes Retrieval. We\nextensively perform ablation studies on its behaviors to show the effectiveness\nof MDR. By easily adopting our MDR, the previous approaches can be improved in\nperformance and generalization ability.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 14:16:07 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Kim", "Yonghyun", ""], ["Park", "Wonpyo", ""]]}, {"id": "2102.04266", "submitter": "David Noever", "authors": "David Noever, Samantha E. Miller Noever", "title": "Overhead MNIST: A Benchmark Satellite Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The research presents an overhead view of 10 important objects and follows\nthe general formatting requirements of the most popular machine learning task:\ndigit recognition with MNIST. This dataset offers a public benchmark extracted\nfrom over a million human-labelled and curated examples. The work outlines the\nkey multi-class object identification task while matching with prior work in\nhandwriting, cancer detection, and retail datasets. A prototype deep learning\napproach with transfer learning and convolutional neural networks (MobileNetV2)\ncorrectly identifies the ten overhead classes with an average accuracy of\n96.7%. This model exceeds the peak human performance of 93.9%. For upgrading\nsatellite imagery and object recognition, this new dataset benefits diverse\nendeavors such as disaster relief, land use management, and other traditional\nremote sensing tasks. The work extends satellite benchmarks with new\ncapabilities to identify efficient and compact algorithms that might work\non-board small satellites, a practical task for future multi-sensor\nconstellations. The dataset is available on Kaggle and Github.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 14:57:49 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Noever", "David", ""], ["Noever", "Samantha E. Miller", ""]]}, {"id": "2102.04300", "submitter": "Kaoutar Ben Ahmed", "authors": "Kaoutar Ben Ahmed, Lawrence O. Hall, Dmitry B. Goldgof, Gregory M.\n  Goldgof, Rahul Paul", "title": "Deep Learning Models May Spuriously Classify Covid-19 from X-ray Images\n  Based on Confounders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying who is infected with the Covid-19 virus is critical for\ncontrolling its spread. X-ray machines are widely available worldwide and can\nquickly provide images that can be used for diagnosis. A number of recent\nstudies claim it may be possible to build highly accurate models, using deep\nlearning, to detect Covid-19 from chest X-ray images. This paper explores the\nrobustness and generalization ability of convolutional neural network models in\ndiagnosing Covid-19 disease from frontal-view (AP/PA), raw chest X-ray images\nthat were lung field cropped. Some concerning observations are made about high\nperforming models that have learned to rely on confounding features related to\nthe data source, rather than the patient's lung pathology, when differentiating\nbetween Covid-19 positive and negative labels. Specifically, these models\nlikely made diagnoses based on confounding factors such as patient age or image\nprocessing artifacts, rather than medically relevant information.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 21:33:06 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Ahmed", "Kaoutar Ben", ""], ["Hall", "Lawrence O.", ""], ["Goldgof", "Dmitry B.", ""], ["Goldgof", "Gregory M.", ""], ["Paul", "Rahul", ""]]}, {"id": "2102.04306", "submitter": "Jieneng Chen", "authors": "Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan\n  Wang, Le Lu, Alan L. Yuille, Yuyin Zhou", "title": "TransUNet: Transformers Make Strong Encoders for Medical Image\n  Segmentation", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical image segmentation is an essential prerequisite for developing\nhealthcare systems, especially for disease diagnosis and treatment planning. On\nvarious medical image segmentation tasks, the u-shaped architecture, also known\nas U-Net, has become the de-facto standard and achieved tremendous success.\nHowever, due to the intrinsic locality of convolution operations, U-Net\ngenerally demonstrates limitations in explicitly modeling long-range\ndependency. Transformers, designed for sequence-to-sequence prediction, have\nemerged as alternative architectures with innate global self-attention\nmechanisms, but can result in limited localization abilities due to\ninsufficient low-level details. In this paper, we propose TransUNet, which\nmerits both Transformers and U-Net, as a strong alternative for medical image\nsegmentation. On one hand, the Transformer encodes tokenized image patches from\na convolution neural network (CNN) feature map as the input sequence for\nextracting global contexts. On the other hand, the decoder upsamples the\nencoded features which are then combined with the high-resolution CNN feature\nmaps to enable precise localization.\n  We argue that Transformers can serve as strong encoders for medical image\nsegmentation tasks, with the combination of U-Net to enhance finer details by\nrecovering localized spatial information. TransUNet achieves superior\nperformances to various competing methods on different medical applications\nincluding multi-organ segmentation and cardiac segmentation. Code and models\nare available at https://github.com/Beckschen/TransUNet.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 16:10:50 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Chen", "Jieneng", ""], ["Lu", "Yongyi", ""], ["Yu", "Qihang", ""], ["Luo", "Xiangde", ""], ["Adeli", "Ehsan", ""], ["Wang", "Yan", ""], ["Lu", "Le", ""], ["Yuille", "Alan L.", ""], ["Zhou", "Yuyin", ""]]}, {"id": "2102.04331", "submitter": "Ali Karimi", "authors": "Ali Karimi, Ramin Toosi, Mohammad Ali Akhaee", "title": "Soccer Event Detection Using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Event detection is an important step in extracting knowledge from the video.\nIn this paper, we propose a deep learning approach to detect events in a soccer\nmatch emphasizing the distinction between images of red and yellow cards and\nthe correct detection of the images of selected events from other images. This\nmethod includes the following three modules: i) the variational autoencoder\n(VAE) module to differentiate between soccer images and others image, ii) the\nimage classification module to classify the images of events, and iii) the\nfine-grain image classification module to classify the images of red and yellow\ncards. Additionally, a new dataset was introduced for soccer images\nclassification that is employed to train the networks mentioned in the paper.\nIn the final section, 10 UEFA Champions League matches are used to evaluate the\nnetworks' performance and precision in detecting the events. The experiments\ndemonstrate that the proposed method achieves better performance than\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 16:37:47 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Karimi", "Ali", ""], ["Toosi", "Ramin", ""], ["Akhaee", "Mohammad Ali", ""]]}, {"id": "2102.04341", "submitter": "Brandon Wagstaff", "authors": "Justin Tomasi, Brandon Wagstaff, Steven L. Waslander, Jonathan Kelly", "title": "Learned Camera Gain and Exposure Control for Improved Visual Feature\n  Detection and Matching", "comments": "Accepted to IEEE Robotics and Automation Letters and to the IEEE\n  International Conference on Robotics and Automation (ICRA) 2021", "journal-ref": null, "doi": "10.1109/LRA.2021.3058909", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successful visual navigation depends upon capturing images that contain\nsufficient useful information. In this paper, we explore a data-driven approach\nto account for environmental lighting changes, improving the quality of images\nfor use in visual odometry (VO) or visual simultaneous localization and mapping\n(SLAM). We train a deep convolutional neural network model to predictively\nadjust camera gain and exposure time parameters such that consecutive images\ncontain a maximal number of matchable features. The training process is fully\nself-supervised: our training signal is derived from an underlying VO or SLAM\npipeline and, as a result, the model is optimized to perform well with that\nspecific pipeline. We demonstrate through extensive real-world experiments that\nour network can anticipate and compensate for dramatic lighting changes (e.g.,\ntransitions into and out of road tunnels), maintaining a substantially higher\nnumber of inlier feature matches than competing camera parameter control\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 16:46:09 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 17:52:10 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Tomasi", "Justin", ""], ["Wagstaff", "Brandon", ""], ["Waslander", "Steven L.", ""], ["Kelly", "Jonathan", ""]]}, {"id": "2102.04353", "submitter": "Anirban Santara", "authors": "Krzysztof Choromanski, Deepali Jain, Jack Parker-Holder, Xingyou Song,\n  Valerii Likhosherstov, Anirban Santara, Aldo Pacchiano, Yunhao Tang, Adrian\n  Weller", "title": "Unlocking Pixels for Reinforcement Learning via Implicit Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There has recently been significant interest in training reinforcement\nlearning (RL) agents in vision-based environments. This poses many challenges,\nsuch as high dimensionality and potential for observational overfitting through\nspurious correlations. A promising approach to solve both of these problems is\na self-attention bottleneck, which provides a simple and effective framework\nfor learning high performing policies, even in the presence of distractions.\nHowever, due to poor scalability of attention architectures, these methods do\nnot scale beyond low resolution visual inputs, using large patches (thus small\nattention matrices). In this paper we make use of new efficient attention\nalgorithms, recently shown to be highly effective for Transformers, and\ndemonstrate that these new techniques can be applied in the RL setting. This\nallows our attention-based controllers to scale to larger visual inputs, and\nfacilitate the use of smaller patches, even individual pixels, improving\ngeneralization. In addition, we propose a new efficient algorithm approximating\nsoftmax attention with what we call hybrid random features, leveraging the\ntheory of angular kernels. We show theoretically and empirically that hybrid\nrandom features is a promising approach when using attention for vision-based\nRL.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 17:00:26 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 16:07:52 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 15:53:45 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Choromanski", "Krzysztof", ""], ["Jain", "Deepali", ""], ["Parker-Holder", "Jack", ""], ["Song", "Xingyou", ""], ["Likhosherstov", "Valerii", ""], ["Santara", "Anirban", ""], ["Pacchiano", "Aldo", ""], ["Tang", "Yunhao", ""], ["Weller", "Adrian", ""]]}, {"id": "2102.04362", "submitter": "Chee Seng Chan", "authors": "Ding Sheng Ong, Chee Seng Chan, Kam Woh Ng, Lixin Fan, Qiang Yang", "title": "Protecting Intellectual Property of Generative Adversarial Networks from\n  Ambiguity Attack", "comments": "Accepted at CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ever since Machine Learning as a Service (MLaaS) emerges as a viable business\nthat utilizes deep learning models to generate lucrative revenue, Intellectual\nProperty Right (IPR) has become a major concern because these deep learning\nmodels can easily be replicated, shared, and re-distributed by any unauthorized\nthird parties. To the best of our knowledge, one of the prominent deep learning\nmodels - Generative Adversarial Networks (GANs) which has been widely used to\ncreate photorealistic image are totally unprotected despite the existence of\npioneering IPR protection methodology for Convolutional Neural Networks (CNNs).\nThis paper therefore presents a complete protection framework in both black-box\nand white-box settings to enforce IPR protection on GANs. Empirically, we show\nthat the proposed method does not compromise the original GANs performance\n(i.e. image generation, image super-resolution, style transfer), and at the\nsame time, it is able to withstand both removal and ambiguity attacks against\nembedded watermarks.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 17:12:20 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 03:31:03 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Ong", "Ding Sheng", ""], ["Chan", "Chee Seng", ""], ["Ng", "Kam Woh", ""], ["Fan", "Lixin", ""], ["Yang", "Qiang", ""]]}, {"id": "2102.04366", "submitter": "Lucas Prado Osco", "authors": "Mauro dos Santos de Arruda, Lucas Prado Osco, Plabiany Rodrigo Acosta,\n  Diogo Nunes Gon\\c{c}alves, Jos\\'e Marcato Junior, Ana Paula Marques Ramos,\n  Edson Takashi Matsubara, Zhipeng Luo, Jonathan Li, Jonathan de Andrade Silva,\n  Wesley Nunes Gon\\c{c}alves", "title": "Counting and Locating High-Density Objects Using Convolutional Neural\n  Network", "comments": "15 pages, 10 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper presents a Convolutional Neural Network (CNN) approach for\ncounting and locating objects in high-density imagery. To the best of our\nknowledge, this is the first object counting and locating method based on a\nfeature map enhancement and a Multi-Stage Refinement of the confidence map. The\nproposed method was evaluated in two counting datasets: tree and car. For the\ntree dataset, our method returned a mean absolute error (MAE) of 2.05, a\nroot-mean-squared error (RMSE) of 2.87 and a coefficient of determination\n(R$^2$) of 0.986. For the car dataset (CARPK and PUCPR+), our method was\nsuperior to state-of-the-art methods. In the these datasets, our approach\nachieved an MAE of 4.45 and 3.16, an RMSE of 6.18 and 4.39, and an R$^2$ of\n0.975 and 0.999, respectively. The proposed method is suitable for dealing with\nhigh object-density, returning a state-of-the-art performance for counting and\nlocating objects.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 17:17:10 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["de Arruda", "Mauro dos Santos", ""], ["Osco", "Lucas Prado", ""], ["Acosta", "Plabiany Rodrigo", ""], ["Gon\u00e7alves", "Diogo Nunes", ""], ["Junior", "Jos\u00e9 Marcato", ""], ["Ramos", "Ana Paula Marques", ""], ["Matsubara", "Edson Takashi", ""], ["Luo", "Zhipeng", ""], ["Li", "Jonathan", ""], ["Silva", "Jonathan de Andrade", ""], ["Gon\u00e7alves", "Wesley Nunes", ""]]}, {"id": "2102.04378", "submitter": "Shuting He", "authors": "Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li, Wei Jiang", "title": "TransReID: Transformer-based Object Re-Identification", "comments": "Code is available at https://github.com/heshuting555/TransReID", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting robust feature representation is one of the key challenges in\nobject re-identification (ReID). Although convolution neural network\n(CNN)-based methods have achieved great success, they only process one local\nneighborhood at a time and suffer from information loss on details caused by\nconvolution and downsampling operators (e.g. pooling and strided convolution).\nTo overcome these limitations, we propose a pure transformer-based object ReID\nframework named TransReID. Specifically, we first encode an image as a sequence\nof patches and build a transformer-based strong baseline with a few critical\nimprovements, which achieves competitive results on several ReID benchmarks\nwith CNN-based methods. To further enhance the robust feature learning in the\ncontext of transformers, two novel modules are carefully designed. (i) The\njigsaw patch module (JPM) is proposed to rearrange the patch embeddings via\nshift and patch shuffle operations which generates robust features with\nimproved discrimination ability and more diversified coverage. (ii) The side\ninformation embeddings (SIE) is introduced to mitigate feature bias towards\ncamera/view variations by plugging in learnable embeddings to incorporate these\nnon-visual clues. To the best of our knowledge, this is the first work to adopt\na pure transformer for ReID research. Experimental results of TransReID are\nsuperior promising, which achieve state-of-the-art performance on both person\nand vehicle ReID benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 17:33:59 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 15:40:42 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["He", "Shuting", ""], ["Luo", "Hao", ""], ["Wang", "Pichao", ""], ["Wang", "Fan", ""], ["Li", "Hao", ""], ["Jiang", "Wei", ""]]}, {"id": "2102.04379", "submitter": "Georgios Chochlakis", "authors": "Georgios Chochlakis, Efthymios Georgiou, Alexandros Potamianos", "title": "End-to-end Generative Zero-shot Learning via Few-shot Learning", "comments": "12 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contemporary state-of-the-art approaches to Zero-Shot Learning (ZSL) train\ngenerative nets to synthesize examples conditioned on the provided metadata.\nThereafter, classifiers are trained on these synthetic data in a supervised\nmanner. In this work, we introduce Z2FSL, an end-to-end generative ZSL\nframework that uses such an approach as a backbone and feeds its synthesized\noutput to a Few-Shot Learning (FSL) algorithm. The two modules are trained\njointly. Z2FSL solves the ZSL problem with a FSL algorithm, reducing, in\neffect, ZSL to FSL. A wide class of algorithms can be integrated within our\nframework. Our experimental results show consistent improvement over several\nbaselines. The proposed method, evaluated across standard benchmarks, shows\nstate-of-the-art or competitive performance in ZSL and Generalized ZSL tasks.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 17:35:37 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Chochlakis", "Georgios", ""], ["Georgiou", "Efthymios", ""], ["Potamianos", "Alexandros", ""]]}, {"id": "2102.04400", "submitter": "Vasudevan Lakshminarayanan", "authors": "Hardit Singh, Simarjeet Saini, Vasudevan Lakshminarayanan", "title": "Rapid Classification of Glaucomatous Fundus Images", "comments": "Submitted for publication in JOSA A: Optics and Image Science,\n  currently under revision", "journal-ref": null, "doi": "10.1364/JOSAA.415395", "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new method for training convolutional neural networks which\nintegrates reinforcement learning along with supervised learning and use ti for\ntransfer learning for classification of glaucoma from colored fundus images.\nThe training method uses hill climbing techniques via two different climber\ntypes, viz \"random movment\" and \"random detection\" integrated with supervised\nlearning model though stochastic gradient descent with momentum (SGDM) model.\nThe model was trained and tested using the Drishti GS and RIM-ONE-r2 datasets\nhaving glaucomatous and normal fundus images. The performance metrics for\nprediction was tested by transfer learning on five CNN architectures, namely\nGoogLenet, DesnseNet-201, NASNet, VGG-19 and Inception-resnet-v2. A fivefold\nclassification was used for evaluating the perfroamnace and high sensitivities\nwhile high maintaining high accuracies were achieved. Of the models tested, the\ndenseNet-201 architecture performed the best in terms of sensitivity and area\nunder the curve (AUC). This method of training allows transfer learning on\nsmall datasets and can be applied for tele-ophthalmology applications including\ntraining with local datasets.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 18:06:25 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Singh", "Hardit", ""], ["Saini", "Simarjeet", ""], ["Lakshminarayanan", "Vasudevan", ""]]}, {"id": "2102.04420", "submitter": "Adam Van Etten", "authors": "Adam Van Etten, Daniel Hogan, Jesus Martinez-Manso, Jacob Shermeyer,\n  Nicholas Weir, Ryan Lewis", "title": "The Multi-Temporal Urban Development SpaceNet Dataset", "comments": "8 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Satellite imagery analytics have numerous human development and disaster\nresponse applications, particularly when time series methods are involved. For\nexample, quantifying population statistics is fundamental to 67 of the 231\nUnited Nations Sustainable Development Goals Indicators, but the World Bank\nestimates that over 100 countries currently lack effective Civil Registration\nsystems. To help address this deficit and develop novel computer vision methods\nfor time series data, we present the Multi-Temporal Urban Development SpaceNet\n(MUDS, also known as SpaceNet 7) dataset. This open source dataset consists of\nmedium resolution (4.0m) satellite imagery mosaics, which includes 24 images\n(one per month) covering >100 unique geographies, and comprises >40,000 km2 of\nimagery and exhaustive polygon labels of building footprints therein, totaling\nover 11M individual annotations. Each building is assigned a unique identifier\n(i.e. address), which permits tracking of individual objects over time. Label\nfidelity exceeds image resolution; this \"omniscient labeling\" is a unique\nfeature of the dataset, and enables surprisingly precise algorithmic models to\nbe crafted. We demonstrate methods to track building footprint construction (or\ndemolition) over time, thereby directly assessing urbanization. Performance is\nmeasured with the newly developed SpaceNet Change and Object Tracking (SCOT)\nmetric, which quantifies both object tracking as well as change detection. We\ndemonstrate that despite the moderate resolution of the data, we are able to\ntrack individual building identifiers over time. This task has broad\nimplications for disaster preparedness, the environment, infrastructure\ndevelopment, and epidemic prevention.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 18:28:52 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Van Etten", "Adam", ""], ["Hogan", "Daniel", ""], ["Martinez-Manso", "Jesus", ""], ["Shermeyer", "Jacob", ""], ["Weir", "Nicholas", ""], ["Lewis", "Ryan", ""]]}, {"id": "2102.04432", "submitter": "Manoj Kumar", "authors": "Manoj Kumar, Dirk Weissenborn, Nal Kalchbrenner", "title": "Colorization Transformer", "comments": "ICLR 2021 Camera Ready. See\n  https://openreview.net/forum?id=5NA1PinlGFu for more details", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Colorization Transformer, a novel approach for diverse high\nfidelity image colorization based on self-attention. Given a grayscale image,\nthe colorization proceeds in three steps. We first use a conditional\nautoregressive transformer to produce a low resolution coarse coloring of the\ngrayscale image. Our architecture adopts conditional transformer layers to\neffectively condition grayscale input. Two subsequent fully parallel networks\nupsample the coarse colored low resolution image into a finely colored high\nresolution image. Sampling from the Colorization Transformer produces diverse\ncolorings whose fidelity outperforms the previous state-of-the-art on\ncolorising ImageNet based on FID results and based on a human evaluation in a\nMechanical Turk test. Remarkably, in more than 60% of cases human evaluators\nprefer the highest rated among three generated colorings over the ground truth.\nThe code and pre-trained checkpoints for Colorization Transformer are publicly\navailable at\nhttps://github.com/google-research/google-research/tree/master/coltran\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 18:45:06 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2021 08:38:49 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Kumar", "Manoj", ""], ["Weissenborn", "Dirk", ""], ["Kalchbrenner", "Nal", ""]]}, {"id": "2102.04442", "submitter": "Adrian Bulat", "authors": "Adrian Bulat and Enrique S\\'anchez-Lozano and Georgios Tzimiropoulos", "title": "Improving memory banks for unsupervised learning with large mini-batch,\n  consistency and hard negative mining", "comments": "Accepted at ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important component of unsupervised learning by instance-based\ndiscrimination is a memory bank for storing a feature representation for each\ntraining sample in the dataset. In this paper, we introduce 3 improvements to\nthe vanilla memory bank-based formulation which brings massive accuracy gains:\n(a) Large mini-batch: we pull multiple augmentations for each sample within the\nsame batch and show that this leads to better models and enhanced memory bank\nupdates. (b) Consistency: we enforce the logits obtained by different\naugmentations of the same sample to be close without trying to enforce\ndiscrimination with respect to negative samples as proposed by previous\napproaches. (c) Hard negative mining: since instance discrimination is not\nmeaningful for samples that are too visually similar, we devise a novel nearest\nneighbour approach for improving the memory bank that gradually merges\nextremely similar data samples that were previously forced to be apart by the\ninstance level classification loss. Overall, our approach greatly improves the\nvanilla memory-bank based instance discrimination and outperforms all existing\nmethods for both seen and unseen testing categories with cosine similarity.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 18:56:33 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Bulat", "Adrian", ""], ["S\u00e1nchez-Lozano", "Enrique", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "2102.04515", "submitter": "Gulshan Saleem", "authors": "Nisar Ahmed, Hafiz Muhammad Shahzad Asif, Gulshan Saleem", "title": "Leaf Image-based Plant Disease Identification using Color and Texture\n  Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identification of plant disease is usually done through visual inspection or\nduring laboratory examination which causes delays resulting in yield loss by\nthe time identification is complete. On the other hand, complex deep learning\nmodels perform the task with reasonable performance but due to their large size\nand high computational requirements, they are not suited to mobile and handheld\ndevices. Our proposed approach contributes automated identification of plant\ndiseases which follows a sequence of steps involving pre-processing,\nsegmentation of diseased leaf area, calculation of features based on the\nGray-Level Co-occurrence Matrix (GLCM), feature selection and classification.\nIn this study, six color features and twenty-two texture features have been\ncalculated. Support vector machines is used to perform one-vs-one\nclassification of plant disease. The proposed model of disease identification\nprovides an accuracy of 98.79% with a standard deviation of 0.57 on 10-fold\ncross-validation. The accuracy on a self-collected dataset is 82.47% for\ndisease identification and 91.40% for healthy and diseased classification. The\nreported performance measures are better or comparable to the existing\napproaches and highest among the feature-based methods, presenting it as the\nmost suitable method to automated leaf-based plant disease identification. This\nprototype system can be extended by adding more disease categories or targeting\nspecific crop or disease categories.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 20:32:56 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Ahmed", "Nisar", ""], ["Asif", "Hafiz Muhammad Shahzad", ""], ["Saleem", "Gulshan", ""]]}, {"id": "2102.04525", "submitter": "Michael Yeung", "authors": "Michael Yeung, Evis Sala, Carola-Bibiane Sch\\\"onlieb, Leonardo Rundo", "title": "Unified Focal loss: Generalising Dice and cross entropy-based losses to\n  handle class imbalanced medical image segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation methods are an important advancement in medical image\nanalysis. Machine learning techniques, and deep neural networks in particular,\nare the state-of-the-art for most medical image segmentation tasks. Issues with\nclass imbalance pose a significant challenge in medical datasets, with lesions\noften occupying a considerably smaller volume relative to the background. Loss\nfunctions used in the training of deep learning algorithms differ in their\nrobustness to class imbalance, with direct consequences for model convergence.\nThe most commonly used loss functions for segmentation are based on either the\ncross entropy loss, Dice loss or a combination of the two. We propose a Unified\nFocal loss, a new framework that generalises Dice and cross entropy-based\nlosses for handling class imbalance. We evaluate our proposed loss function on\nthree highly class imbalanced, publicly available medical imaging datasets:\nBreast Ultrasound 2017 (BUS2017), Brain Tumour Segmentation 2020 (BraTS20) and\nKidney Tumour Segmentation 2019 (KiTS19). We compare our loss function\nperformance against six Dice or cross entropy-based loss functions, and\ndemonstrate that our proposed loss function is robust to class imbalance,\noutperforming the other loss functions across datasets. Finally, we use the\nUnified Focal loss together with deep supervision to achieve state-of-the-art\nresults without modification of the original U-Net architecture, with a mean\nDice similarity coefficient (DSC)=0.948 on BUS2017, enhancing tumour region\nDSC=0.800 on BraTS20 and kidney tumour DSC=0.758 on KiTS19. This highlights the\nimportance of carefully selecting a suitable loss function prior to the use of\nmore complex architectures.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 20:47:38 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 08:49:30 GMT"}, {"version": "v3", "created": "Mon, 24 May 2021 14:06:06 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Yeung", "Michael", ""], ["Sala", "Evis", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""], ["Rundo", "Leonardo", ""]]}, {"id": "2102.04530", "submitter": "Ran Cheng", "authors": "Ran Cheng, Ryan Razani, Ehsan Taghavi, Enxu Li, Bingbing Liu", "title": "(AF)2-S3Net: Attentive Feature Fusion with Adaptive Feature Selection\n  for Sparse Semantic Segmentation Network", "comments": "10 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous robotic systems and self driving cars rely on accurate perception\nof their surroundings as the safety of the passengers and pedestrians is the\ntop priority. Semantic segmentation is one the essential components of\nenvironmental perception that provides semantic information of the scene.\nRecently, several methods have been introduced for 3D LiDAR semantic\nsegmentation. While, they can lead to improved performance, they are either\nafflicted by high computational complexity, therefore are inefficient, or lack\nfine details of smaller instances. To alleviate this problem, we propose\nAF2-S3Net, an end-to-end encoder-decoder CNN network for 3D LiDAR semantic\nsegmentation. We present a novel multi-branch attentive feature fusion module\nin the encoder and a unique adaptive feature selection module with feature map\nre-weighting in the decoder. Our AF2-S3Net fuses the voxel based learning and\npoint-based learning into a single framework to effectively process the large\n3D scene. Our experimental results show that the proposed method outperforms\nthe state-of-the-art approaches on the large-scale SemanticKITTI benchmark,\nranking 1st on the competitive public leaderboard competition upon publication.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 21:04:21 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Cheng", "Ran", ""], ["Razani", "Ryan", ""], ["Taghavi", "Ehsan", ""], ["Li", "Enxu", ""], ["Liu", "Bingbing", ""]]}, {"id": "2102.04566", "submitter": "Lucas Prado Osco", "authors": "Patrik Ol\\~a Bressan, Jos\\'e Marcato Junior, Jos\\'e Augusto Correa\n  Martins, Diogo Nunes Gon\\c{c}alves, Daniel Matte Freitas, Lucas Prado Osco,\n  Jonathan de Andrade Silva, Zhipeng Luo, Jonathan Li, Raymundo Cordero Garcia,\n  Wesley Nunes Gon\\c{c}alves", "title": "Semantic Segmentation with Labeling Uncertainty and Class Imbalance", "comments": "15 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recently, methods based on Convolutional Neural Networks (CNN) achieved\nimpressive success in semantic segmentation tasks. However, challenges such as\nthe class imbalance and the uncertainty in the pixel-labeling process are not\ncompletely addressed. As such, we present a new approach that calculates a\nweight for each pixel considering its class and uncertainty during the labeling\nprocess. The pixel-wise weights are used during training to increase or\ndecrease the importance of the pixels. Experimental results show that the\nproposed approach leads to significant improvements in three challenging\nsegmentation tasks in comparison to baseline methods. It was also proved to be\nmore invariant to noise. The approach presented here may be used within a wide\nrange of semantic segmentation methods to improve their robustness.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 22:53:33 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Bressan", "Patrik Ol\u00e3", ""], ["Junior", "Jos\u00e9 Marcato", ""], ["Martins", "Jos\u00e9 Augusto Correa", ""], ["Gon\u00e7alves", "Diogo Nunes", ""], ["Freitas", "Daniel Matte", ""], ["Osco", "Lucas Prado", ""], ["Silva", "Jonathan de Andrade", ""], ["Luo", "Zhipeng", ""], ["Li", "Jonathan", ""], ["Garcia", "Raymundo Cordero", ""], ["Gon\u00e7alves", "Wesley Nunes", ""]]}, {"id": "2102.04568", "submitter": "Chris Kennedy", "authors": "Chris J. Kennedy, Julia Vassey, Ho-Chun Herbert Chang, Jennifer B.\n  Unger, Emilio Ferrara", "title": "Tracking e-cigarette warning label compliance on Instagram with deep\n  learning", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The U.S. Food & Drug Administration (FDA) requires that e-cigarette\nadvertisements include a prominent warning label that reminds consumers that\nnicotine is addictive. However, the high volume of vaping-related posts on\nsocial media makes compliance auditing expensive and time-consuming, suggesting\nthat an automated, scalable method is needed. We sought to develop and evaluate\na deep learning system designed to automatically determine if an Instagram post\npromotes vaping, and if so, if an FDA-compliant warning label was included or\nif a non-compliant warning label was visible in the image. We compiled and\nlabeled a dataset of 4,363 Instagram images, of which 44% were vaping-related,\n3% contained FDA-compliant warning labels, and 4% contained non-compliant\nlabels. Using a 20% test set for evaluation, we tested multiple neural network\nvariations: image processing backbone model (Inceptionv3, ResNet50,\nEfficientNet), data augmentation, progressive layer unfreezing, output bias\ninitialization designed for class imbalance, and multitask learning. Our final\nmodel achieved an area under the curve (AUC) and [accuracy] of 0.97 [92%] on\nvaping classification, 0.99 [99%] on FDA-compliant warning labels, and 0.94\n[97%] on non-compliant warning labels. We conclude that deep learning models\ncan effectively identify vaping posts on Instagram and track compliance with\nFDA warning label requirements.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 22:56:21 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Kennedy", "Chris J.", ""], ["Vassey", "Julia", ""], ["Chang", "Ho-Chun Herbert", ""], ["Unger", "Jennifer B.", ""], ["Ferrara", "Emilio", ""]]}, {"id": "2102.04582", "submitter": "Abdallah Chehade", "authors": "Mayuresh Savargaonkar, Abdallah Chehade and Samir Rawashdeh", "title": "RMOPP: Robust Multi-Objective Post-Processing for Effective Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few decades, many architectures have been developed that\nharness the power of neural networks to detect objects in near real-time.\nTraining such systems requires substantial time across multiple GPUs and\nmassive labeled training datasets. Although the goal of these systems is\ngeneralizability, they are often impractical in real-life applications due to\nflexibility, robustness, or speed issues. This paper proposes RMOPP: A robust\nmulti-objective post-processing algorithm to boost the performance of fast\npre-trained object detectors with a negligible impact on their speed.\nSpecifically, RMOPP is a statistically driven, post-processing algorithm that\nallows for simultaneous optimization of precision and recall. A unique feature\nof RMOPP is the Pareto frontier that identifies dominant possible\npost-processed detectors to optimize for both precision and recall. RMOPP\nexplores the full potential of a pre-trained object detector and is deployable\nfor near real-time predictions. We also provide a compelling test case on\nYOLOv2 using the MS-COCO dataset.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 00:02:38 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Savargaonkar", "Mayuresh", ""], ["Chehade", "Abdallah", ""], ["Rawashdeh", "Samir", ""]]}, {"id": "2102.04590", "submitter": "Mona Zehni", "authors": "Mona Zehni, Zhizhen Zhao", "title": "UVTomo-GAN: An adversarial learning based approach for unknown view\n  X-ray tomographic reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tomographic reconstruction recovers an unknown image given its projections\nfrom different angles. State-of-the-art methods addressing this problem assume\nthe angles associated with the projections are known a-priori. Given this\nknowledge, the reconstruction process is straightforward as it can be\nformulated as a convex problem. Here, we tackle a more challenging setting: 1)\nthe projection angles are unknown, 2) they are drawn from an unknown\nprobability distribution. In this set-up our goal is to recover the image and\nthe projection angle distribution using an unsupervised adversarial learning\napproach. For this purpose, we formulate the problem as a distribution matching\nbetween the real projection lines and the generated ones from the estimated\nimage and projection distribution. This is then solved by reaching the\nequilibrium in a min-max game between a generator and a discriminator. Our\nnovel contribution is to recover the unknown projection distribution and the\nimage simultaneously using adversarial learning. To accommodate this, we use\nGumbel-softmax approximation of samples from categorical distribution to\napproximate the generator's loss as a function of the unknown image and the\nprojection distribution. Our approach can be generalized to different inverse\nproblems. Our simulation results reveal the ability of our method in\nsuccessfully recovering the image and the projection distribution in various\nsettings.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 00:51:25 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Zehni", "Mona", ""], ["Zhao", "Zhizhen", ""]]}, {"id": "2102.04593", "submitter": "Gabriele Di Cerbo", "authors": "Gabriele Di Cerbo, Ali Hirsa, Ahmad Shayaan", "title": "Regularized Generative Adversarial Network", "comments": "18 pages. Comments are welcome!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for generating samples from a probability distribution\nthat differs from the probability distribution of the training set. We use an\nadversarial process that simultaneously trains three networks, a generator and\ntwo discriminators. We refer to this new model as regularized generative\nadversarial network (RegGAN). We evaluate RegGAN on a synthetic dataset\ncomposed of gray scale images and we further show that it can be used to learn\nsome pre-specified notions in topology (basic topology properties). The work is\nmotivated by practical problems encountered while using generative methods in\nthe art world.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 01:13:36 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Di Cerbo", "Gabriele", ""], ["Hirsa", "Ali", ""], ["Shayaan", "Ahmad", ""]]}, {"id": "2102.04604", "submitter": "Haochen Wang", "authors": "Haochen Wang, Xiaolong Jiang, Haibing Ren, Yao Hu, Song Bai", "title": "SwiftNet: Real-time Video Object Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we present SwiftNet for real-time semisupervised video object\nsegmentation (one-shot VOS), which reports 77.8% J &F and 70 FPS on DAVIS 2017\nvalidation dataset, leading all present solutions in overall accuracy and speed\nperformance. We achieve this by elaborately compressing spatiotemporal\nredundancy in matching-based VOS via Pixel-Adaptive Memory (PAM). Temporally,\nPAM adaptively triggers memory updates on frames where objects display\nnoteworthy inter-frame variations. Spatially, PAM selectively performs memory\nupdate and match on dynamic pixels while ignoring the static ones,\nsignificantly reducing redundant computations wasted on segmentation-irrelevant\npixels. To promote efficient reference encoding, light-aggregation encoder is\nalso introduced in SwiftNet deploying reversed sub-pixel. We hope SwiftNet\ncould set a strong and efficient baseline for real-time VOS and facilitate its\napplication in mobile vision. The source code of SwiftNet can be found at\nhttps://github.com/haochenheheda/SwiftNet.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 02:22:48 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 01:52:30 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Wang", "Haochen", ""], ["Jiang", "Xiaolong", ""], ["Ren", "Haibing", ""], ["Hu", "Yao", ""], ["Bai", "Song", ""]]}, {"id": "2102.04615", "submitter": "Jo\\~ao Gabriel Zago", "authors": "Jo\\~ao G. Zago, Fabio L. Baldissera, Eric A. Antonelo and Rodrigo T.\n  Saad", "title": "Benford's law: what does it say on adversarial images?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are fragile to small perturbations in\nthe input images. These networks are thus prone to malicious attacks that\nperturb the inputs to force a misclassification. Such slightly manipulated\nimages aimed at deceiving the classifier are known as adversarial images. In\nthis work, we investigate statistical differences between natural images and\nadversarial ones. More precisely, we show that employing a proper image\ntransformation and for a class of adversarial attacks, the distribution of the\nleading digit of the pixels in adversarial images deviates from Benford's law.\nThe stronger the attack, the more distant the resulting distribution is from\nBenford's law. Our analysis provides a detailed investigation of this new\napproach that can serve as a basis for alternative adversarial example\ndetection methods that do not need to modify the original CNN classifier\nneither work on the raw high-dimensional pixels as features to defend against\nattacks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 02:50:29 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Zago", "Jo\u00e3o G.", ""], ["Baldissera", "Fabio L.", ""], ["Antonelo", "Eric A.", ""], ["Saad", "Rodrigo T.", ""]]}, {"id": "2102.04621", "submitter": "Xinchen Liu", "authors": "Jinkai Zheng, Xinchen Liu, Chenggang Yan, Jiyong Zhang, Wu Liu,\n  Xiaoping Zhang, Tao Mei", "title": "TraND: Transferable Neighborhood Discovery for Unsupervised Cross-domain\n  Gait Recognition", "comments": "Accepted by ISCAS 2021. 5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gait, i.e., the movement pattern of human limbs during locomotion, is a\npromising biometric for the identification of persons. Despite significant\nimprovement in gait recognition with deep learning, existing studies still\nneglect a more practical but challenging scenario -- unsupervised cross-domain\ngait recognition which aims to learn a model on a labeled dataset then adapts\nit to an unlabeled dataset. Due to the domain shift and class gap, directly\napplying a model trained on one source dataset to other target datasets usually\nobtains very poor results. Therefore, this paper proposes a Transferable\nNeighborhood Discovery (TraND) framework to bridge the domain gap for\nunsupervised cross-domain gait recognition. To learn effective prior knowledge\nfor gait representation, we first adopt a backbone network pre-trained on the\nlabeled source data in a supervised manner. Then we design an end-to-end\ntrainable approach to automatically discover the confident neighborhoods of\nunlabeled samples in the latent space. During training, the class consistency\nindicator is adopted to select confident neighborhoods of samples based on\ntheir entropy measurements. Moreover, we explore a high-entropy-first neighbor\nselection strategy, which can effectively transfer prior knowledge to the\ntarget domain. Our method achieves state-of-the-art results on two public\ndatasets, i.e., CASIA-B and OU-LP.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 03:07:07 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Zheng", "Jinkai", ""], ["Liu", "Xinchen", ""], ["Yan", "Chenggang", ""], ["Zhang", "Jiyong", ""], ["Liu", "Wu", ""], ["Zhang", "Xiaoping", ""], ["Mei", "Tao", ""]]}, {"id": "2102.04639", "submitter": "Jie Mei", "authors": "Jie Mei, Jenq-Neng Hwang, Suzanne Romain, Craig Rose, Braden Moore,\n  Kelsey Magrane", "title": "Absolute 3D Pose Estimation and Length Measurement of Severely Deformed\n  Fish from Monocular Videos in Longline Fishing", "comments": "Accepted to ICASSP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular absolute 3D fish pose estimation allows for efficient fish length\nmeasurement in the longline fisheries, where fishes are under severe\ndeformation during the catching process. This task is challenging since it\nrequires locating absolute 3D fish keypoints based on a short monocular video\nclip. Unlike related works, which either require expensive 3D ground-truth data\nand/or multiple-view images to provide depth information, or are limited to\nrigid objects, we propose a novel frame-based method to estimate the absolute\n3D fish pose and fish length from a single-view 2D segmentation mask. We first\nintroduce a relative 3D fish template. By minimizing an objective function, our\nmethod systematically estimates the relative 3D pose of the target fish and\nfish 2D keypoints in the image. Finally, with a closed-form solution, the\nrelative 3D fish pose can help locate absolute 3D keypoints, resulting in the\nframe-based absolute fish length measurement, which is further refined based on\nthe statistical temporal inference for the optimal fish length measurement from\nthe video clip. Our experiments show that this method can accurately estimate\nthe absolute 3D fish pose and further measure the absolute length, even\noutperforming the state-of-the-art multi-view method.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 04:15:51 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Mei", "Jie", ""], ["Hwang", "Jenq-Neng", ""], ["Romain", "Suzanne", ""], ["Rose", "Craig", ""], ["Moore", "Braden", ""], ["Magrane", "Kelsey", ""]]}, {"id": "2102.04652", "submitter": "Xiangzeng Zhou", "authors": "Xiangzeng Zhou and Pan Pan and Yun Zheng and Yinghui Xu and Rong Jin", "title": "Large Scale Long-tailed Product Recognition System at Alibaba", "comments": "Acccepted by CIKM 2020", "journal-ref": "In Proceedings of the 29th ACM International Conference on\n  Information and Knowledge Management (CIKM20), 3353-3356 (2020)", "doi": "10.1145/3340531.3417445", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A practical large scale product recognition system suffers from the\nphenomenon of long-tailed imbalanced training data under the E-commercial\ncircumstance at Alibaba. Besides product images at Alibaba, plenty of image\nrelated side information (e.g. title, tags) reveal rich semantic information\nabout images. Prior works mainly focus on addressing the long tail problem in\nvisual perspective only, but lack of consideration of leveraging the side\ninformation. In this paper, we present a novel side information based large\nscale visual recognition co-training~(SICoT) system to deal with the long tail\nproblem by leveraging the image related side information. In the proposed\nco-training system, we firstly introduce a bilinear word attention module\naiming to construct a semantic embedding over the noisy side information. A\nvisual feature and semantic embedding co-training scheme is then designed to\ntransfer knowledge from classes with abundant training data (head classes) to\nclasses with few training data (tail classes) in an end-to-end fashion.\nExtensive experiments on four challenging large scale datasets, whose numbers\nof classes range from one thousand to one million, demonstrate the scalable\neffectiveness of the proposed SICoT system in alleviating the long tail\nproblem. In the visual search platform\nPailitao\\footnote{http://www.pailitao.com} at Alibaba, we settle a practical\nlarge scale product recognition application driven by the proposed SICoT\nsystem, and achieve a significant gain of unique visitor~(UV) conversion rate.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 05:34:30 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Zhou", "Xiangzeng", ""], ["Pan", "Pan", ""], ["Zheng", "Yun", ""], ["Xu", "Yinghui", ""], ["Jin", "Rong", ""]]}, {"id": "2102.04656", "submitter": "Kang Zhao", "authors": "Kang Zhao, Pan Pan, Yun Zheng, Yanhao Zhang, Changxu Wang, Yingya\n  Zhang, Yinghui Xu, Rong Jin", "title": "Large-Scale Visual Search with Binary Distributed Graph at Alibaba", "comments": "This paper has been accepted by CIKM2019. Proceedings of the 28th ACM\n  International Conference on Information and Knowledge Management. 2019", "journal-ref": null, "doi": "10.1145/3357384.3357834", "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based approximate nearest neighbor search has attracted more and more\nattentions due to its online search advantages. Numbers of methods studying the\nenhancement of speed and recall have been put forward. However, few of them\nfocus on the efficiency and scale of offline graph-construction. For a deployed\nvisual search system with several billions of online images in total, building\na billion-scale offline graph in hours is essential, which is almost\nunachievable by most existing methods. In this paper, we propose a novel\nalgorithm called Binary Distributed Graph to solve this problem. Specifically,\nwe combine binary codes with graph structure to speedup online and offline\nprocedures, and achieve comparable performance with the ones in real-value\nbased scenarios by recalling more binary candidates. Furthermore, the\ngraph-construction is optimized to completely distributed implementation, which\nsignificantly accelerates the offline process and gets rid of the limitation of\nmemory and disk within a single machine. Experimental comparisons on Alibaba\nCommodity Data Set (more than three billion images) show that the proposed\nmethod outperforms the state-of-the-art with respect to the online/offline\ntrade-off.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 05:51:34 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Zhao", "Kang", ""], ["Pan", "Pan", ""], ["Zheng", "Yun", ""], ["Zhang", "Yanhao", ""], ["Wang", "Changxu", ""], ["Zhang", "Yingya", ""], ["Xu", "Yinghui", ""], ["Jin", "Rong", ""]]}, {"id": "2102.04667", "submitter": "Yanhao Zhang", "authors": "Yanhao Zhang, Pan Pan, Yun Zheng, Kang Zhao, Jianmin Wu, Yinghui Xu,\n  Rong Jin", "title": "Virtual ID Discovery from E-commerce Media at Alibaba: Exploiting\n  Richness of User Click Behavior for Visual Search Relevance", "comments": "accepted by CIKM 2019", "journal-ref": "CIKM 2019: 2489-2497", "doi": "10.1145/3357384.3357800", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual search plays an essential role for E-commerce. To meet the search\ndemands of users and promote shopping experience at Alibaba, visual search\nrelevance of real-shot images is becoming the bottleneck. Traditional visual\nsearch paradigm is usually based upon supervised learning with labeled data.\nHowever, large-scale categorical labels are required with expensive human\nannotations, which limits its applicability and also usually fails in\ndistinguishing the real-shot images. In this paper, we propose to discover\nVirtual ID from user click behavior to improve visual search relevance at\nAlibaba. As a totally click-data driven approach, we collect various types of\nclick data for training deep networks without any human annotations at all. In\nparticular, Virtual ID are learned as classification supervision with co-click\nembedding, which explores image relationship from user co-click behaviors to\nguide category prediction and feature learning. Concretely, we deploy Virtual\nID Category Network by integrating first-clicks and switch-clicks as\nregularizer. Incorporating triplets and list constraints, Virtual ID Feature\nNetwork is trained in a joint classification and ranking manner. Benefiting\nfrom exploration of user click data, our networks are more effective to encode\nricher supervision and better distinguish real-shot images in terms of category\nand feature. To validate our method for visual search relevance, we conduct an\nextensive set of offline and online experiments on the collected real-shot\nimages. We consistently achieve better experimental results across all\ncomponents, compared with alternative and state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 06:31:20 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Zhang", "Yanhao", ""], ["Pan", "Pan", ""], ["Zheng", "Yun", ""], ["Zhao", "Kang", ""], ["Wu", "Jianmin", ""], ["Xu", "Yinghui", ""], ["Jin", "Rong", ""]]}, {"id": "2102.04674", "submitter": "Yanhao Zhang", "authors": "Yanhao Zhang, Pan Pan, Yun Zheng, Kang Zhao, Yingya Zhang, Xiaofeng\n  Ren, Rong Jin", "title": "Visual Search at Alibaba", "comments": "accepted by KDD 2018", "journal-ref": "KDD 2018: 993-1001", "doi": "10.1145/3219819.3219820", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the large scale visual search algorithm and system\ninfrastructure at Alibaba. The following challenges are discussed under the\nE-commercial circumstance at Alibaba (a) how to handle heterogeneous image data\nand bridge the gap between real-shot images from user query and the online\nimages. (b) how to deal with large scale indexing for massive updating data.\n(c) how to train deep models for effective feature representation without huge\nhuman annotations. (d) how to improve the user engagement by considering the\nquality of the content. We take advantage of large image collection of Alibaba\nand state-of-the-art deep learning techniques to perform visual search at\nscale. We present solutions and implementation details to overcome those\nproblems and also share our learnings from building such a large scale\ncommercial visual search engine. Specifically, model and search-based fusion\napproach is introduced to effectively predict categories. Also, we propose a\ndeep CNN model for joint detection and feature learning by mining user click\nbehavior. The binary index engine is designed to scale up indexing without\ncompromising recall and precision. Finally, we apply all the stages into an\nend-to-end system architecture, which can simultaneously achieve highly\nefficient and scalable performance adapting to real-shot images. Extensive\nexperiments demonstrate the advancement of each module in our system. We hope\nvisual search at Alibaba becomes more widely incorporated into today's\ncommercial applications.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 06:46:50 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Zhang", "Yanhao", ""], ["Pan", "Pan", ""], ["Zheng", "Yun", ""], ["Zhao", "Kang", ""], ["Zhang", "Yingya", ""], ["Ren", "Xiaofeng", ""], ["Jin", "Rong", ""]]}, {"id": "2102.04686", "submitter": "Abdur Rahim Mohammad Forkan", "authors": "Abdur Rahim Mohammad Forkan, Yong-Bin Kang, Prem Prakash Jayaraman,\n  Kewen Liao, Rohit Kaul, Graham Morgan, Rajiv Ranjan, Samir Sinha", "title": "CorrDetector: A Framework for Structural Corrosion Detection from Drone\n  Images using Ensemble Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new technique that applies automated image\nanalysis in the area of structural corrosion monitoring and demonstrate\nimproved efficacy compared to existing approaches. Structural corrosion\nmonitoring is the initial step of the risk-based maintenance philosophy and\ndepends on an engineer's assessment regarding the risk of building failure\nbalanced against the fiscal cost of maintenance. This introduces the\nopportunity for human error which is further complicated when restricted to\nassessment using drone captured images for those areas not reachable by humans\ndue to many background noises. The importance of this problem has promoted an\nactive research community aiming to support the engineer through the use of\nartificial intelligence (AI) image analysis for corrosion detection. In this\npaper, we advance this area of research with the development of a framework,\nCorrDetector. CorrDetector uses a novel ensemble deep learning approach\nunderpinned by convolutional neural networks (CNNs) for structural\nidentification and corrosion feature extraction. We provide an empirical\nevaluation using real-world images of a complicated structure (e.g.\ntelecommunication tower) captured by drones, a typical scenario for engineers.\nOur study demonstrates that the ensemble approach of \\model significantly\noutperforms the state-of-the-art in terms of classification accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 07:27:16 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Forkan", "Abdur Rahim Mohammad", ""], ["Kang", "Yong-Bin", ""], ["Jayaraman", "Prem Prakash", ""], ["Liao", "Kewen", ""], ["Kaul", "Rohit", ""], ["Morgan", "Graham", ""], ["Ranjan", "Rajiv", ""], ["Sinha", "Samir", ""]]}, {"id": "2102.04699", "submitter": "Rajiv Kumar V", "authors": "Rajiv Kumar, Rishabh Dabral, G. Sivakumar", "title": "Learning Unsupervised Cross-domain Image-to-Image Translation Using a\n  Shared Discriminator", "comments": null, "journal-ref": null, "doi": "10.5220/0010184102560264", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image-to-image translation is used to transform images from a\nsource domain to generate images in a target domain without using source-target\nimage pairs. Promising results have been obtained for this problem in an\nadversarial setting using two independent GANs and attention mechanisms. We\npropose a new method that uses a single shared discriminator between the two\nGANs, which improves the overall efficacy. We assess the qualitative and\nquantitative results on image transfiguration, a cross-domain translation task,\nin a setting where the target domain shares similar semantics to the source\ndomain. Our results indicate that even without adding attention mechanisms, our\nmethod performs at par with attention-based methods and generates images of\ncomparable quality.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 08:26:23 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Kumar", "Rajiv", ""], ["Dabral", "Rishabh", ""], ["Sivakumar", "G.", ""]]}, {"id": "2102.04700", "submitter": "Peidong Liu", "authors": "Peidong Liu, Gengwei Zhang, Bochao Wang, Hang Xu, Xiaodan Liang, Yong\n  Jiang, Zhenguo Li", "title": "Loss Function Discovery for Object Detection via Convergence-Simulation\n  Driven Search", "comments": "Accepted by ICLR2021 Poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing proper loss functions for vision tasks has been a long-standing\nresearch direction to advance the capability of existing models. For object\ndetection, the well-established classification and regression loss functions\nhave been carefully designed by considering diverse learning challenges.\nInspired by the recent progress in network architecture search, it is\ninteresting to explore the possibility of discovering new loss function\nformulations via directly searching the primitive operation combinations. So\nthat the learned losses not only fit for diverse object detection challenges to\nalleviate huge human efforts, but also have better alignment with evaluation\nmetric and good mathematical convergence property. Beyond the previous\nauto-loss works on face recognition and image classification, our work makes\nthe first attempt to discover new loss functions for the challenging object\ndetection from primitive operation levels. We propose an effective\nconvergence-simulation driven evolutionary search algorithm, called\nCSE-Autoloss, for speeding up the search progress by regularizing the\nmathematical rationality of loss candidates via convergence property\nverification and model optimization simulation. CSE-Autoloss involves the\nsearch space that cover a wide range of the possible variants of existing\nlosses and discovers best-searched loss function combination within a short\ntime (around 1.5 wall-clock days). We conduct extensive evaluations of loss\nfunction search on popular detectors and validate the good generalization\ncapability of searched losses across diverse architectures and datasets. Our\nexperiments show that the best-discovered loss function combinations outperform\ndefault combinations by 1.1% and 0.8% in terms of mAP for two-stage and\none-stage detectors on COCO respectively. Our searched losses are available at\nhttps://github.com/PerdonLiu/CSE-Autoloss.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 08:34:52 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Liu", "Peidong", ""], ["Zhang", "Gengwei", ""], ["Wang", "Bochao", ""], ["Xu", "Hang", ""], ["Liang", "Xiaodan", ""], ["Jiang", "Yong", ""], ["Li", "Zhenguo", ""]]}, {"id": "2102.04727", "submitter": "Yanhao Zhang", "authors": "Yanhao Zhang, Qiang Wang, Pan Pan, Yun Zheng, Cheng Da, Siyang Sun and\n  Yinghui Xu", "title": "Fashion Focus: Multi-modal Retrieval System for Video Commodity\n  Localization in E-commerce", "comments": "accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, live-stream and short video shopping in E-commerce have grown\nexponentially. However, the sellers are required to manually match images of\nthe selling products to the timestamp of exhibition in the untrimmed video,\nresulting in a complicated process. To solve the problem, we present an\ninnovative demonstration of multi-modal retrieval system called \"Fashion\nFocus\", which enables to exactly localize the product images in the online\nvideo as the focuses. Different modality contributes to the community\nlocalization, including visual content, linguistic features and interaction\ncontext are jointly investigated via presented multi-modal learning. Our system\nemploys two procedures for analysis, including video content structuring and\nmulti-modal retrieval, to automatically achieve accurate video-to-shop\nmatching. Fashion Focus presents a unified framework that can orientate the\nconsumers towards relevant product exhibitions during watching videos and help\nthe sellers to effectively deliver the products over search and recommendation.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 09:45:04 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Zhang", "Yanhao", ""], ["Wang", "Qiang", ""], ["Pan", "Pan", ""], ["Zheng", "Yun", ""], ["Da", "Cheng", ""], ["Sun", "Siyang", ""], ["Xu", "Yinghui", ""]]}, {"id": "2102.04738", "submitter": "Jinn-Liang Liu", "authors": "Der-Hau Lee and Jinn-Liang Liu", "title": "End-to-End Deep Learning of Lane Detection and Path Prediction for\n  Real-Time Autonomous Driving", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end three-task convolutional neural network (3TCNN)\nhaving two regression branches of bounding boxes and Hu moments and one\nclassification branch of object masks for lane detection and road recognition.\nThe Hu-moment regressor performs lane localization and road guidance using\nlocal and global Hu moments of segmented lane objects, respectively. Based on\n3TCNN, we then propose lateral offset and path prediction (PP) algorithms to\nform an integrated model (3TCNN-PP) that can predict driving path with dynamic\nestimation of lane centerline and path curvature for real-time autonomous\ndriving. We also develop a CNN-PP simulator that can be used to train a CNN by\nreal or artificial traffic images, test it by artificial images, quantify its\ndynamic errors, and visualize its qualitative performance. Simulation results\nshow that 3TCNN-PP is comparable to related CNNs and better than a previous\nCNN-PP, respectively. The code, annotated data, and simulation videos of this\nwork can be found on our website for further research on NN-PP algorithms of\nautonomous driving.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 10:04:39 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Lee", "Der-Hau", ""], ["Liu", "Jinn-Liang", ""]]}, {"id": "2102.04750", "submitter": "Pedro Vicente", "authors": "Alexandre Almeida, Pedro Vicente, Alexandre Bernardino", "title": "Where is my hand? Deep hand segmentation for visual self-recognition in\n  humanoid robots", "comments": "13 pages, 12 figures, Submitted to Journal of Robotics and Autonomous\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to distinguish between the self and the background is of\nparamount importance for robotic tasks. The particular case of hands, as the\nend effectors of a robotic system that more often enter into contact with other\nelements of the environment, must be perceived and tracked with precision to\nexecute the intended tasks with dexterity and without colliding with obstacles.\nThey are fundamental for several applications, from Human-Robot Interaction\ntasks to object manipulation. Modern humanoid robots are characterized by high\nnumber of degrees of freedom which makes their forward kinematics models very\nsensitive to uncertainty. Thus, resorting to vision sensing can be the only\nsolution to endow these robots with a good perception of the self, being able\nto localize their body parts with precision. In this paper, we propose the use\nof a Convolution Neural Network (CNN) to segment the robot hand from an image\nin an egocentric view. It is known that CNNs require a huge amount of data to\nbe trained. To overcome the challenge of labeling real-world images, we propose\nthe use of simulated datasets exploiting domain randomization techniques. We\nfine-tuned the Mask-RCNN network for the specific task of segmenting the hand\nof the humanoid robot Vizzy. We focus our attention on developing a methodology\nthat requires low amounts of data to achieve reasonable performance while\ngiving detailed insight on how to properly generate variability in the training\ndataset. Moreover, we analyze the fine-tuning process within the complex model\nof Mask-RCNN, understanding which weights should be transferred to the new task\nof segmenting robot hands. Our final model was trained solely on synthetic\nimages and achieves an average IoU of 82% on synthetic validation data and\n56.3% on real test data. These results were achieved with only 1000 training\nimages and 3 hours of training time using a single GPU.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 10:34:32 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Almeida", "Alexandre", ""], ["Vicente", "Pedro", ""], ["Bernardino", "Alexandre", ""]]}, {"id": "2102.04760", "submitter": "Sahand Sharifzadeh", "authors": "Sahand Sharifzadeh, Sina Moayed Baharlou, Martin Schmitt, Hinrich\n  Sch\\\"utze, Volker Tresp", "title": "Improving Visual Reasoning by Exploiting The Knowledge in Texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new framework for training image-based classifiers from\na combination of texts and images with very few labels. We consider a\nclassification framework with three modules: a backbone, a relational reasoning\ncomponent, and a classification component. While the backbone can be trained\nfrom unlabeled images by self-supervised learning, we can fine-tune the\nrelational reasoning and the classification components from external sources of\nknowledge instead of annotated images. By proposing a transformer-based model\nthat creates structured knowledge from textual input, we enable the utilization\nof the knowledge in texts. We show that, compared to the supervised baselines\nwith 1% of the annotated images, we can achieve ~8x more accurate results in\nscene graph classification, ~3x in object classification, and ~1.5x in\npredicate classification.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 11:21:44 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Sharifzadeh", "Sahand", ""], ["Baharlou", "Sina Moayed", ""], ["Schmitt", "Martin", ""], ["Sch\u00fctze", "Hinrich", ""], ["Tresp", "Volker", ""]]}, {"id": "2102.04762", "submitter": "Linwei Ye", "authors": "Linwei Ye, Mrigank Rochan, Zhi Liu, Xiaoqin Zhang and Yang Wang", "title": "Referring Segmentation in Images and Videos with Cross-Modal\n  Self-Attention Network", "comments": "14 pages, 8 figures. arXiv admin note: substantial text overlap with\n  arXiv:1904.04745", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3054384", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of referring segmentation in images and videos with\nnatural language. Given an input image (or video) and a referring expression,\nthe goal is to segment the entity referred by the expression in the image or\nvideo. In this paper, we propose a cross-modal self-attention (CMSA) module to\nutilize fine details of individual words and the input image or video, which\neffectively captures the long-range dependencies between linguistic and visual\nfeatures. Our model can adaptively focus on informative words in the referring\nexpression and important regions in the visual input. We further propose a\ngated multi-level fusion (GMLF) module to selectively integrate self-attentive\ncross-modal features corresponding to different levels of visual features. This\nmodule controls the feature fusion of information flow of features at different\nlevels with high-level and low-level semantic information related to different\nattentive words. Besides, we introduce cross-frame self-attention (CFSA) module\nto effectively integrate temporal information in consecutive frames which\nextends our method in the case of referring segmentation in videos. Experiments\non benchmark datasets of four referring image datasets and two actor and action\nvideo segmentation datasets consistently demonstrate that our proposed approach\noutperforms existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 11:27:59 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Ye", "Linwei", ""], ["Rochan", "Mrigank", ""], ["Liu", "Zhi", ""], ["Zhang", "Xiaoqin", ""], ["Wang", "Yang", ""]]}, {"id": "2102.04776", "submitter": "Emilien Dupont", "authors": "Emilien Dupont, Yee Whye Teh, Arnaud Doucet", "title": "Generative Models as Distributions of Functions", "comments": "Added point clouds experiments, quantitative evaluations and link to\n  github repo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models are typically trained on grid-like data such as images. As\na result, the size of these models usually scales directly with the underlying\ngrid resolution. In this paper, we abandon discretized grids and instead\nparameterize individual data points by continuous functions. We then build\ngenerative models by learning distributions over such functions. By treating\ndata points as functions, we can abstract away from the specific type of data\nwe train on and construct models that scale independently of signal resolution.\nTo train our model, we use an adversarial approach with a discriminator that\nacts on continuous signals. Through experiments on both images and 3D shapes,\nwe demonstrate that our model can learn rich distributions of functions\nindependently of data type and resolution.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 11:47:55 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 18:04:08 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Dupont", "Emilien", ""], ["Teh", "Yee Whye", ""], ["Doucet", "Arnaud", ""]]}, {"id": "2102.04780", "submitter": "Sutharsan Mahendren Mr", "authors": "Sutharsan Mahendren, Chamira Edussooriya, Ranga Rodrigo", "title": "Diverse Single Image Generation with Controllable Global Structure\n  through Self-Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image generation from a single image using generative adversarial networks is\nquite interesting due to the realism of generated images. However, recent\napproaches need improvement for such realistic and diverse image generation,\nwhen the global context of the image is important such as in face, animal, and\narchitectural image generation. This is mainly due to the use of fewer\nconvolutional layers for mainly capturing the patch statistics and, thereby,\nnot being able to capture global statistics very well. We solve this problem by\nusing attention blocks at selected scales and feeding a random Gaussian blurred\nimage to the discriminator for training. Our results are visually better than\nthe state-of-the-art particularly in generating images that require global\ncontext. The diversity of our image generation, measured using the average\nstandard deviation of pixels, is also better.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 11:52:48 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 05:22:34 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Mahendren", "Sutharsan", ""], ["Edussooriya", "Chamira", ""], ["Rodrigo", "Ranga", ""]]}, {"id": "2102.04782", "submitter": "Kang Zhao", "authors": "Kang Zhao, Sida Huang, Pan Pan, Yinghan Li, Yingya Zhang, Zhenyu Gu,\n  Yinghui Xu", "title": "Distribution Adaptive INT8 Quantization for Training CNNs", "comments": "This paper has been accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researches have demonstrated that low bit-width (e.g., INT8) quantization can\nbe employed to accelerate the inference process. It makes the gradient\nquantization very promising since the backward propagation requires\napproximately twice more computation than forward one. Due to the variability\nand uncertainty of gradient distribution, a lot of methods have been proposed\nto attain training stability. However, most of them ignore the channel-wise\ngradient distributions and the impact of gradients with different magnitudes,\nresulting in the degradation of final accuracy. In this paper, we propose a\nnovel INT8 quantization training framework for convolutional neural network to\naddress the above issues. Specifically, we adopt Gradient Vectorized\nQuantization to quantize the gradient, based on the observation that layer-wise\ngradients contain multiple distributions along the channel dimension. Then,\nMagnitude-aware Clipping Strategy is introduced by taking the magnitudes of\ngradients into consideration when minimizing the quantization error, and we\npresent a theoretical derivation to solve the quantization parameters of\ndifferent distributions. Experimental results on broad range of computer vision\ntasks, such as image classification, object detection and video classification,\ndemonstrate that the proposed Distribution Adaptive INT8 Quantization training\nmethod has achieved almost lossless training accuracy for different backbones,\nincluding ResNet, MobileNetV2, InceptionV3, VGG and AlexNet, which is superior\nto the state-of-the-art techniques. Moreover, we further implement the INT8\nkernel that can accelerate the training iteration more than 200% under the\nlatest Turing architecture, i.e., our method excels on both training accuracy\nand speed.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 11:58:10 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Zhao", "Kang", ""], ["Huang", "Sida", ""], ["Pan", "Pan", ""], ["Li", "Yinghan", ""], ["Zhang", "Yingya", ""], ["Gu", "Zhenyu", ""], ["Xu", "Yinghui", ""]]}, {"id": "2102.04798", "submitter": "Kateryna Chumachenko", "authors": "Kateryna Chumachenko, Jenni Raitoharju, Alexandros Iosifidis, Moncef\n  Gabbouj", "title": "Ensembling object detectors for image and video data analysis", "comments": "Accepted to ICASSP 2021.(C)2021 IEEE.Personal use of this material is\n  permitted.Permission from IEEE must be obtained for all other uses,in any\n  current or future media,including reprinting/republishing this material for\n  advertising or promotional purposes,creating new collective works,for resale\n  or redistribution to servers or lists,or reuse of any copyrighted component\n  of this work in other works", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method for ensembling the outputs of multiple\nobject detectors for improving detection performance and precision of bounding\nboxes on image data. We further extend it to video data by proposing a\ntwo-stage tracking-based scheme for detection refinement. The proposed method\ncan be used as a standalone approach for improving object detection\nperformance, or as a part of a framework for faster bounding box annotation in\nunseen datasets, assuming that the objects of interest are those present in\nsome common public datasets.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 12:38:16 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Chumachenko", "Kateryna", ""], ["Raitoharju", "Jenni", ""], ["Iosifidis", "Alexandros", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "2102.04803", "submitter": "Enze Xie", "authors": "Enze Xie, Jian Ding, Wenhai Wang, Xiaohang Zhan, Hang Xu, Peize Sun,\n  Zhenguo Li, Ping Luo", "title": "DetCo: Unsupervised Contrastive Learning for Object Detection", "comments": "ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised contrastive learning achieves great success in learning image\nrepresentations with CNN. Unlike most recent methods that focused on improving\naccuracy of image classification, we present a novel contrastive learning\napproach, named DetCo, which fully explores the contrasts between global image\nand local image patches to learn discriminative representations for object\ndetection. DetCo has several appealing benefits. (1) It is carefully designed\nby investigating the weaknesses of current self-supervised methods, which\ndiscard important representations for object detection. (2) DetCo builds\nhierarchical intermediate contrastive losses between global image and local\npatches to improve object detection, while maintaining global representations\nfor image recognition. Theoretical analysis shows that the local patches\nactually remove the contextual information of an image, improving the lower\nbound of mutual information for better contrastive learning. (3) Extensive\nexperiments on PASCAL VOC, COCO and Cityscapes demonstrate that DetCo not only\noutperforms state-of-the-art methods on object detection, but also on\nsegmentation, pose estimation, and 3D shape prediction, while it is still\ncompetitive on image classification. For example, on PASCAL VOC, DetCo-100ep\nachieves 57.4 mAP, which is on par with the result of MoCov2-800ep. Moreover,\nDetCo consistently outperforms supervised method by 1.6/1.2/1.0 AP on Mask\nRCNN-C4/FPN/RetinaNet with 1x schedule. Code will be released at\n\\href{https://github.com/xieenze/DetCo}{\\color{blue}{\\tt\ngithub.com/xieenze/DetCo}}.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 12:47:20 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 07:54:34 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Xie", "Enze", ""], ["Ding", "Jian", ""], ["Wang", "Wenhai", ""], ["Zhan", "Xiaohang", ""], ["Xu", "Hang", ""], ["Sun", "Peize", ""], ["Li", "Zhenguo", ""], ["Luo", "Ping", ""]]}, {"id": "2102.04816", "submitter": "Abdelrahman Abdallah", "authors": "Daniyar Nurseitov, Kairat Bostanbekov, Maksat Kanatov, Anel Alimova,\n  Abdelrahman Abdallah, Galymzhan Abdimanap", "title": "Classification of Handwritten Names of Cities and Handwritten Text\n  Recognition using Various Deep Learning Models", "comments": null, "journal-ref": "Advances in Science, Technology and Engineering Systems. 5,\n  934-943 (2020)", "doi": "10.25046/aj0505114", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article discusses the problem of handwriting recognition in Kazakh and\nRussian languages. This area is poorly studied since in the literature there\nare almost no works in this direction. We have tried to describe various\napproaches and achievements of recent years in the development of handwritten\nrecognition models in relation to Cyrillic graphics. The first model uses deep\nconvolutional neural networks (CNNs) for feature extraction and a fully\nconnected multilayer perceptron neural network (MLP) for word classification.\nThe second model, called SimpleHTR, uses CNN and recurrent neural network (RNN)\nlayers to extract information from images. We also proposed the Bluechet and\nPuchserver models to compare the results. Due to the lack of available open\ndatasets in Russian and Kazakh languages, we carried out work to collect data\nthat included handwritten names of countries and cities from 42 different\nCyrillic words, written more than 500 times in different handwriting. We also\nused a handwritten database of Kazakh and Russian languages (HKR). This is a\nnew database of Cyrillic words (not only countries and cities) for the Russian\nand Kazakh languages, created by the authors of this work.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 13:34:16 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Nurseitov", "Daniyar", ""], ["Bostanbekov", "Kairat", ""], ["Kanatov", "Maksat", ""], ["Alimova", "Anel", ""], ["Abdallah", "Abdelrahman", ""], ["Abdimanap", "Galymzhan", ""]]}, {"id": "2102.04823", "submitter": "Alessandra Di Pierro", "authors": "Riccardo Mengoni, Massimiliano Incudini, Alessandra Di Pierro", "title": "Facial Expression Recognition on a Quantum Computer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of facial expression recognition and show a possible\nsolution using a quantum machine learning approach. In order to define an\nefficient classifier for a given dataset, our approach substantially exploits\nquantum interference. By representing face expressions via graphs, we define a\nclassifier as a quantum circuit that manipulates the graphs adjacency matrices\nencoded into the amplitudes of some appropriately defined quantum states. We\ndiscuss the accuracy of the quantum classifier evaluated on the quantum\nsimulator available on the IBM Quantum Experience cloud platform, and compare\nit with the accuracy of one of the best classical classifier.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 13:48:00 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Mengoni", "Riccardo", ""], ["Incudini", "Massimiliano", ""], ["Di Pierro", "Alessandra", ""]]}, {"id": "2102.04838", "submitter": "Weipan Xu", "authors": "Ying Li, Weipan Xu, Haohui Chen, Junhao Jiang, Xun Li", "title": "A Histogram Thresholding Improvement to Mask R-CNN for Scalable\n  Segmentation of New and Old Rural Buildings", "comments": "6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping new and old buildings are of great significance for understanding\nsocio-economic development in rural areas. In recent years, deep neural\nnetworks have achieved remarkable building segmentation results in\nhigh-resolution remote sensing images. However, the scarce training data and\nthe varying geographical environments have posed challenges for scalable\nbuilding segmentation. This study proposes a novel framework based on Mask\nR-CNN, named HTMask R-CNN, to extract new and old rural buildings even when the\nlabel is scarce. The framework adopts the result of single-object instance\nsegmentation from the orthodox Mask R-CNN. Further, it classifies the rural\nbuildings into new and old ones based on a dynamic grayscale threshold inferred\nfrom the result of a two-object instance segmentation task where training data\nis scarce. We found that the framework can extract more buildings and achieve a\nmuch higher mean Average Precision (mAP) than the orthodox Mask R-CNN model. We\ntested the novel framework's performance with increasing training data and\nfound that it converged even when the training samples were limited. This\nframework's main contribution is to allow scalable segmentation by using\nsignificantly fewer training samples than traditional machine learning\npractices. That makes mapping China's new and old rural buildings viable.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 02:09:11 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Li", "Ying", ""], ["Xu", "Weipan", ""], ["Chen", "Haohui", ""], ["Jiang", "Junhao", ""], ["Li", "Xun", ""]]}, {"id": "2102.04848", "submitter": "Yu Liu", "authors": "Yu Liu, Lianghua Huang, Pan Pan, Bin Wang, Yinghui Xu, Rong Jin", "title": "Train a One-Million-Way Instance Classifier for Unsupervised Visual\n  Representation Learning", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simple unsupervised visual representation learning\nmethod with a pretext task of discriminating all images in a dataset using a\nparametric, instance-level classifier. The overall framework is a replica of a\nsupervised classification model, where semantic classes (e.g., dog, bird, and\nship) are replaced by instance IDs. However, scaling up the classification task\nfrom thousands of semantic labels to millions of instance labels brings\nspecific challenges including 1) the large-scale softmax computation; 2) the\nslow convergence due to the infrequent visiting of instance samples; and 3) the\nmassive number of negative classes that can be noisy. This work presents\nseveral novel techniques to handle these difficulties. First, we introduce a\nhybrid parallel training framework to make large-scale training feasible.\nSecond, we present a raw-feature initialization mechanism for classification\nweights, which we assume offers a contrastive prior for instance discrimination\nand can clearly speed up converge in our experiments. Finally, we propose to\nsmooth the labels of a few hardest classes to avoid optimizing over very\nsimilar negative pairs. While being conceptually simple, our framework achieves\ncompetitive or superior performance compared to state-of-the-art unsupervised\napproaches, i.e., SimCLR, MoCoV2, and PIC under ImageNet linear evaluation\nprotocol and on several downstream visual tasks, verifying that full instance\nclassification is a strong pretraining technique for many semantic visual\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 14:44:18 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Liu", "Yu", ""], ["Huang", "Lianghua", ""], ["Pan", "Pan", ""], ["Wang", "Bin", ""], ["Xu", "Yinghui", ""], ["Jin", "Rong", ""]]}, {"id": "2102.04860", "submitter": "Yimin Peng", "authors": "Yimin Peng, Yunlong Li, Zijing Fang", "title": "An underwater binocular stereo matching algorithm based on the best\n  search domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binocular stereo vision is an important branch of machine vision, which\nimitates the human eye and matches the left and right images captured by the\ncamera based on epipolar constraints. The matched disparity map can be\ncalculated according to the camera imaging model to obtain a depth map, and\nthen the depth map is converted to a point cloud image to obtain spatial point\ncoordinates, thereby achieving the purpose of ranging. However, due to the\ninfluence of illumination under water, the captured images no longer meet the\nepipolar constraints, and the changes in imaging models make traditional\ncalibration methods no longer applicable. Therefore, this paper proposes a new\nunderwater real-time calibration method and a matching method based on the best\nsearch domain to improve the accuracy of underwater distance measurement using\nbinoculars.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 14:53:07 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Peng", "Yimin", ""], ["Li", "Yunlong", ""], ["Fang", "Zijing", ""]]}, {"id": "2102.04866", "submitter": "Jennifer Hobbs", "authors": "Jennifer Hobbs, Ivan Dozier, Naira Hovakimyan", "title": "Residue Density Segmentation for Monitoring and Optimizing Tillage\n  Practices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"No-till\" and cover cropping are often identified as the leading simple, best\nmanagement practices for carbon sequestration in agriculture. However, the root\nof the problem is more complex, with the potential benefits of these approaches\ndepending on numerous factors including a field's soil type(s), topography, and\nmanagement history. Instead of using computer vision approaches to simply\nclassify a field a still vs. no-till, we instead seek to identify the degree of\nresidue coverage across afield through a probabilistic deep learning\nsegmentation approach to enable more accurate analysis of carbon holding\npotential and realization. This approach will not only provide more precise\ninsights into currently implemented practices, but also enable a more accurate\nidentification process of fields with the greatest potential for adopting new\npractices to significantly impact carbon sequestration in agriculture.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 15:00:45 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Hobbs", "Jennifer", ""], ["Dozier", "Ivan", ""], ["Hovakimyan", "Naira", ""]]}, {"id": "2102.04869", "submitter": "Hojjat Salehinejad", "authors": "Hojjat Salehinejad, Jumpei Kitamura, Noah Ditkofsky, Amy Lin, Aditya\n  Bharatha, Suradech Suthiphosuwan, Hui-Ming Lin, Jefferson R. Wilson, Muhammad\n  Mamdani, and Errol Colak", "title": "A Real-World Demonstration of Machine Learning Generalizability:\n  Intracranial Hemorrhage Detection on Head CT", "comments": "This paper is under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) holds great promise in transforming healthcare. While\npublished studies have shown the utility of ML models in interpreting medical\nimaging examinations, these are often evaluated under laboratory settings. The\nimportance of real world evaluation is best illustrated by case studies that\nhave documented successes and failures in the translation of these models into\nclinical environments. A key prerequisite for the clinical adoption of these\ntechnologies is demonstrating generalizable ML model performance under real\nworld circumstances. The purpose of this study was to demonstrate that ML model\ngeneralizability is achievable in medical imaging with the detection of\nintracranial hemorrhage (ICH) on non-contrast computed tomography (CT) scans\nserving as the use case. An ML model was trained using 21,784 scans from the\nRSNA Intracranial Hemorrhage CT dataset while generalizability was evaluated\nusing an external validation dataset obtained from our busy trauma and\nneurosurgical center. This real world external validation dataset consisted of\nevery unenhanced head CT scan (n = 5,965) performed in our emergency department\nin 2019 without exclusion. The model demonstrated an AUC of 98.4%, sensitivity\nof 98.8%, and specificity of 98.0%, on the test dataset. On external\nvalidation, the model demonstrated an AUC of 95.4%, sensitivity of 91.3%, and\nspecificity of 94.1%. Evaluating the ML model using a real world external\nvalidation dataset that is temporally and geographically distinct from the\ntraining dataset indicates that ML generalizability is achievable in medical\nimaging applications.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 15:05:48 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Salehinejad", "Hojjat", ""], ["Kitamura", "Jumpei", ""], ["Ditkofsky", "Noah", ""], ["Lin", "Amy", ""], ["Bharatha", "Aditya", ""], ["Suthiphosuwan", "Suradech", ""], ["Lin", "Hui-Ming", ""], ["Wilson", "Jefferson R.", ""], ["Mamdani", "Muhammad", ""], ["Colak", "Errol", ""]]}, {"id": "2102.04906", "submitter": "Yizeng Han", "authors": "Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, Yulin Wang", "title": "Dynamic Neural Networks: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic neural network is an emerging research topic in deep learning.\nCompared to static models which have fixed computational graphs and parameters\nat the inference stage, dynamic networks can adapt their structures or\nparameters to different inputs, leading to notable advantages in terms of\naccuracy, computational efficiency, adaptiveness, etc. In this survey, we\ncomprehensively review this rapidly developing area by dividing dynamic\nnetworks into three main categories: 1) instance-wise dynamic models that\nprocess each instance with data-dependent architectures or parameters; 2)\nspatial-wise dynamic networks that conduct adaptive computation with respect to\ndifferent spatial locations of image data and 3) temporal-wise dynamic models\nthat perform adaptive inference along the temporal dimension for sequential\ndata such as videos and texts. The important research problems of dynamic\nnetworks, e.g., architecture design, decision making scheme, optimization\ntechnique and applications, are reviewed systematically. Finally, we discuss\nthe open problems in this field together with interesting future research\ndirections.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 16:02:00 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 13:45:05 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 14:16:16 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Han", "Yizeng", ""], ["Huang", "Gao", ""], ["Song", "Shiji", ""], ["Yang", "Le", ""], ["Wang", "Honghui", ""], ["Wang", "Yulin", ""]]}, {"id": "2102.04924", "submitter": "Roee Cates", "authors": "Roee Cates, Daphna Weinshall", "title": "More Is More -- Narrowing the Generalization Gap by Adding\n  Classification Heads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overfit is a fundamental problem in machine learning in general, and in deep\nlearning in particular. In order to reduce overfit and improve generalization\nin the classification of images, some employ invariance to a group of\ntransformations, such as rotations and reflections. However, since not all\nobjects exhibit necessarily the same invariance, it seems desirable to allow\nthe network to learn the useful level of invariance from the data. To this end,\nmotivated by self-supervision, we introduce an architecture enhancement for\nexisting neural network models based on input transformations, termed\n'TransNet', together with a training algorithm suitable for it. Our model can\nbe employed during training time only and then pruned for prediction, resulting\nin an equivalent architecture to the base model. Thus pruned, we show that our\nmodel improves performance on various data-sets while exhibiting improved\ngeneralization, which is achieved in turn by enforcing soft invariance on the\nconvolutional kernels of the last layer in the base model. Theoretical analysis\nis provided to support the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 16:30:33 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 12:16:26 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Cates", "Roee", ""], ["Weinshall", "Daphna", ""]]}, {"id": "2102.04938", "submitter": "Oleksii Bashkanov", "authors": "Oleksii Bashkanov, Anneke Meyer, Daniel Schindele, Martin Schostak,\n  Klaus T\\\"onnies, Christian Hansen, Marko Rak", "title": "Learning Multi-Modal Volumetric Prostate Registration with Weak\n  Inter-Subject Spatial Correspondence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies demonstrated the eligibility of convolutional neural networks\n(CNNs) for solving the image registration problem. CNNs enable faster\ntransformation estimation and greater generalization capability needed for\nbetter support during medical interventions. Conventional fully-supervised\ntraining requires a lot of high-quality ground truth data such as\nvoxel-to-voxel transformations, which typically are attained in a too tedious\nand error-prone manner. In our work, we use weakly-supervised learning, which\noptimizes the model indirectly only via segmentation masks that are a more\naccessible ground truth than the deformation fields. Concerning the weak\nsupervision, we investigate two segmentation similarity measures: multiscale\nDice similarity coefficient (mDSC) and the similarity between\nsegmentation-derived signed distance maps (SDMs). We show that the combination\nof mDSC and SDM similarity measures results in a more accurate and natural\ntransformation pattern together with a stronger gradient coverage. Furthermore,\nwe introduce an auxiliary input to the neural network for the prior information\nabout the prostate location in the MR sequence, which mostly is available\npreoperatively. This approach significantly outperforms the standard two-input\nmodels. With weakly labelled MR-TRUS prostate data, we showed registration\nquality comparable to the state-of-the-art deep learning-based method.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 16:48:59 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Bashkanov", "Oleksii", ""], ["Meyer", "Anneke", ""], ["Schindele", "Daniel", ""], ["Schostak", "Martin", ""], ["T\u00f6nnies", "Klaus", ""], ["Hansen", "Christian", ""], ["Rak", "Marko", ""]]}, {"id": "2102.04942", "submitter": "F\\'elix G. Harvey", "authors": "F\\'elix G. Harvey, Mike Yurick, Derek Nowrouzezahrai, Christopher Pal", "title": "Robust Motion In-betweening", "comments": "Published at SIGGRAPH 2020", "journal-ref": null, "doi": "10.1145/3386569.3392480", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this work we present a novel, robust transition generation technique that\ncan serve as a new tool for 3D animators, based on adversarial recurrent neural\nnetworks. The system synthesizes high-quality motions that use\ntemporally-sparse keyframes as animation constraints. This is reminiscent of\nthe job of in-betweening in traditional animation pipelines, in which an\nanimator draws motion frames between provided keyframes. We first show that a\nstate-of-the-art motion prediction model cannot be easily converted into a\nrobust transition generator when only adding conditioning information about\nfuture keyframes. To solve this problem, we then propose two novel additive\nembedding modifiers that are applied at each timestep to latent representations\nencoded inside the network's architecture. One modifier is a time-to-arrival\nembedding that allows variations of the transition length with a single model.\nThe other is a scheduled target noise vector that allows the system to be\nrobust to target distortions and to sample different transitions given fixed\nkeyframes. To qualitatively evaluate our method, we present a custom\nMotionBuilder plugin that uses our trained model to perform in-betweening in\nproduction scenarios. To quantitatively evaluate performance on transitions and\ngeneralizations to longer time horizons, we present well-defined in-betweening\nbenchmarks on a subset of the widely used Human3.6M dataset and on LaFAN1, a\nnovel high quality motion capture dataset that is more appropriate for\ntransition generation. We are releasing this new dataset along with this work,\nwith accompanying code for reproducing our baseline results.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 16:52:45 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Harvey", "F\u00e9lix G.", ""], ["Yurick", "Mike", ""], ["Nowrouzezahrai", "Derek", ""], ["Pal", "Christopher", ""]]}, {"id": "2102.04960", "submitter": "Huan Yin", "authors": "Huan Yin, Xuecheng Xu, Yue Wang and Rong Xiong", "title": "Radar-to-Lidar: Heterogeneous Place Recognition via Joint Learning", "comments": "Published by Frontiers in Robotics and AI. The published version is\n  available at\n  https://www.frontiersin.org/articles/10.3389/frobt.2021.661199/full . The\n  source code is available at\n  https://github.com/ZJUYH/radar-to-lidar-place-recognition", "journal-ref": null, "doi": "10.3389/frobt.2021.661199", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Place recognition is critical for both offline mapping and online\nlocalization. However, current single-sensor based place recognition still\nremains challenging in adverse conditions. In this paper, a heterogeneous\nmeasurements based framework is proposed for long-term place recognition, which\nretrieves the query radar scans from the existing lidar maps. To achieve this,\na deep neural network is built with joint training in the learning stage, and\nthen in the testing stage, shared embeddings of radar and lidar are extracted\nfor heterogeneous place recognition. To validate the effectiveness of the\nproposed method, we conduct tests and generalization experiments on the\nmulti-session public datasets compared to other competitive methods. The\nexperimental results indicate that our model is able to perform multiple place\nrecognitions: lidar-to-lidar, radar-to-radar and radar-to-lidar, while the\nlearned model is trained only once. We also release the source code publicly:\nhttps://github.com/ZJUYH/radar-to-lidar-place-recognition.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 15:34:58 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 09:31:08 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Yin", "Huan", ""], ["Xu", "Xuecheng", ""], ["Wang", "Yue", ""], ["Xiong", "Rong", ""]]}, {"id": "2102.04965", "submitter": "Michal Balazia", "authors": "Michal Balazia, S L Happy, Francois Bremond, Antitza Dantcheva", "title": "How Unique Is a Face: An Investigative Study", "comments": "Preprint. Full paper accepted at the IEEE/IAPR International\n  Conference on Pattern Recognition (ICPR), Milan, Italy, Jan 2021. 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Face recognition has been widely accepted as a means of identification in\napplications ranging from border control to security in the banking sector.\nSurprisingly, while widely accepted, we still lack the understanding of\nuniqueness or distinctiveness of faces as biometric modality. In this work, we\nstudy the impact of factors such as image resolution, feature representation,\ndatabase size, age and gender on uniqueness denoted by the Kullback-Leibler\ndivergence between genuine and impostor distributions. Towards understanding\nthe impact, we present experimental results on the datasets AT&T, LFW,\nIMDb-Face, as well as ND-TWINS, with the feature extraction algorithms VGGFace,\nVGG16, ResNet50, InceptionV3, MobileNet and DenseNet121, that reveal the\nquantitative impact of the named factors. While these are early results, our\nfindings indicate the need for a better understanding of the concept of\nbiometric uniqueness and its implication on face recognition.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 17:35:39 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Balazia", "Michal", ""], ["Happy", "S L", ""], ["Bremond", "Francois", ""], ["Dantcheva", "Antitza", ""]]}, {"id": "2102.04969", "submitter": "Xiao-Wei Chen", "authors": "Xiao-wei Chen (Sun Yat-sen University)", "title": "Semantic Borrowing for Generalized Zero-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized zero-shot learning (GZSL) is one of the most realistic problems,\nbut also one of the most challenging problems due to the partiality of the\nclassifier to supervised classes. Instance-borrowing methods and synthesizing\nmethods solve this problem to some extent with the help of testing semantics,\nbut therefore neither can be used under the class-inductive instance-inductive\n(CIII) training setting where testing data are not available, and the latter\nrequire the training process of a classifier after generating examples. In\ncontrast, a novel method called Semantic Borrowing for improving GZSL methods\nwith compatibility metric learning under CIII is proposed in this paper. It\nborrows similar semantics in the training set, so that the classifier can model\nthe relationship between the semantics of zero-shot and supervised classes more\naccurately during training. In practice, the information of semantics of unseen\nor unknown classes would not be available for training while this approach does\nNOT need any information of semantics of unseen or unknown classes. The\nexperimental results on representative GZSL benchmark datasets show that it can\nreduce the partiality of the classifier to supervised classes and improve the\nperformance of generalized zero-shot classification.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 12:14:28 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Chen", "Xiao-wei", "", "Sun Yat-sen University"]]}, {"id": "2102.04980", "submitter": "Soravit Changpinyo", "authors": "Soravit Changpinyo, Jordi Pont-Tuset, Vittorio Ferrari, Radu Soricut", "title": "Telling the What while Pointing to the Where: Multimodal Queries for\n  Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most existing image retrieval systems use text queries as a way for the user\nto express what they are looking for. However, fine-grained image retrieval\noften requires the ability to also express the where in the image the content\nthey are looking for is. The text modality can only cumbersomely express such\nlocalization preferences, whereas pointing is a more natural fit. In this\npaper, we propose an image retrieval setup with a new form of multimodal\nqueries, where the user simultaneously uses both spoken natural language (the\nwhat) and mouse traces over an empty canvas (the where) to express the\ncharacteristics of the desired target image. We then describe simple\nmodifications to an existing image retrieval model, enabling it to operate in\nthis setup. Qualitative and quantitative experiments show that our model\neffectively takes this spatial guidance into account, and provides\nsignificantly more accurate retrieval results compared to text-only equivalent\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 17:54:34 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 07:55:58 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Changpinyo", "Soravit", ""], ["Pont-Tuset", "Jordi", ""], ["Ferrari", "Vittorio", ""], ["Soricut", "Radu", ""]]}, {"id": "2102.04990", "submitter": "Subarna Tripathi", "authors": "Subarna Tripathi and Kien Nguyen and Tanaya Guha and Bang Du and\n  Truong Q. Nguyen", "title": "SG2Caps: Revisiting Scene Graphs for Image Captioning", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mainstream image captioning models rely on Convolutional Neural Network\n(CNN) image features with an additional attention to salient regions and\nobjects to generate captions via recurrent models. Recently, scene graph\nrepresentations of images have been used to augment captioning models so as to\nleverage their structural semantics, such as object entities, relationships and\nattributes. Several studies have noted that naive use of scene graphs from a\nblack-box scene graph generator harms image caption-ing performance, and scene\ngraph-based captioning mod-els have to incur the overhead of explicit use of\nimage features to generate decent captions. Addressing these challenges, we\npropose a framework, SG2Caps, that utilizes only the scene graph labels for\ncompetitive image caption-ing performance. The basic idea is to close the\nsemantic gap between two scene graphs - one derived from the input image and\nthe other one from its caption. In order to achieve this, we leverage the\nspatial location of objects and the Human-Object-Interaction (HOI) labels as an\nadditional HOI graph. Our framework outperforms existing scene graph-only\ncaptioning models by a large margin (CIDEr score of 110 vs 71) indicating scene\ngraphs as a promising representation for image captioning. Direct utilization\nof the scene graph labels avoids expensive graph convolutions over\nhigh-dimensional CNN features resulting in 49%fewer trainable parameters.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 18:00:53 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Tripathi", "Subarna", ""], ["Nguyen", "Kien", ""], ["Guha", "Tanaya", ""], ["Du", "Bang", ""], ["Nguyen", "Truong Q.", ""]]}, {"id": "2102.04993", "submitter": "Marc G\\'orriz Blanch", "authors": "Marc G\\'orriz, Saverio Blasi, Alan F. Smeaton, Noel E. O'Connor, Marta\n  Mrak", "title": "Attention-Based Neural Networks for Chroma Intra Prediction in Video\n  Coding", "comments": null, "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, 2020", "doi": "10.1109/JSTSP.2020.3044482", "report-no": null, "categories": "eess.IV cs.CC cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural networks can be successfully used to improve several modules of\nadvanced video coding schemes. In particular, compression of colour components\nwas shown to greatly benefit from usage of machine learning models, thanks to\nthe design of appropriate attention-based architectures that allow the\nprediction to exploit specific samples in the reference region. However, such\narchitectures tend to be complex and computationally intense, and may be\ndifficult to deploy in a practical video coding pipeline. This work focuses on\nreducing the complexity of such methodologies, to design a set of simplified\nand cost-effective attention-based architectures for chroma intra-prediction. A\nnovel size-agnostic multi-model approach is proposed to reduce the complexity\nof the inference process. The resulting simplified architecture is still\ncapable of outperforming state-of-the-art methods. Moreover, a collection of\nsimplifications is presented in this paper, to further reduce the complexity\noverhead of the proposed prediction architecture. Thanks to these\nsimplifications, a reduction in the number of parameters of around 90% is\nachieved with respect to the original attention-based methodologies.\nSimplifications include a framework for reducing the overhead of the\nconvolutional operations, a simplified cross-component processing model\nintegrated into the original architecture, and a methodology to perform\ninteger-precision approximations with the aim to obtain fast and hardware-aware\nimplementations. The proposed schemes are integrated into the Versatile Video\nCoding (VVC) prediction pipeline, retaining compression efficiency of\nstate-of-the-art chroma intra-prediction methods based on neural networks,\nwhile offering different directions for significantly reducing coding\ncomplexity.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 18:01:22 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["G\u00f3rriz", "Marc", ""], ["Blasi", "Saverio", ""], ["Smeaton", "Alan F.", ""], ["O'Connor", "Noel E.", ""], ["Mrak", "Marta", ""]]}, {"id": "2102.05001", "submitter": "Joao Florindo", "authors": "Joao B. Florindo, Eduardo Abreu", "title": "An application of a pseudo-parabolic modeling to texture image\n  recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we present a novel methodology for texture image recognition\nusing a partial differential equation modeling. More specifically, we employ\nthe pseudo-parabolic Buckley-Leverett equation to provide a dynamics to the\ndigital image representation and collect local descriptors from those images\nevolving in time. For the local descriptors we employ the magnitude and signal\nbinary patterns and a simple histogram of these features was capable of\nachieving promising results in a classification task. We compare the accuracy\nover well established benchmark texture databases and the results demonstrate\ncompetitiveness, even with the most modern deep learning approaches. The\nachieved results open space for future investigation on this type of modeling\nfor image analysis, especially when there is no large amount of data for\ntraining deep learning models and therefore model-based approaches arise as\nsuitable alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 18:08:42 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Florindo", "Joao B.", ""], ["Abreu", "Eduardo", ""]]}, {"id": "2102.05011", "submitter": "Kiri Wagstaff", "authors": "Kiri Wagstaff (1), Steven Lu (1), Emily Dunkel (1), Kevin Grimes (1),\n  Brandon Zhao (2), Jesse Cai (3), Shoshanna B. Cole (4), Gary Doran (1),\n  Raymond Francis (1), Jake Lee (1), and Lukas Mandrake (1) ((1) Jet Propulsion\n  Laboratory, California Institute of Technology, (2) Duke University, (3)\n  California Institute of Technology, (4) Space Science Institute)", "title": "Mars Image Content Classification: Three Years of NASA Deployment and\n  Recent Advances", "comments": "Published at the Thirty-Third Annual Conference on Innovative\n  Applications of Artificial Intelligence (IAAI-21). IAAI Innovative\n  Application Award. 10 pages, 11 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The NASA Planetary Data System hosts millions of images acquired from the\nplanet Mars. To help users quickly find images of interest, we have developed\nand deployed content-based classification and search capabilities for Mars\norbital and surface images. The deployed systems are publicly accessible using\nthe PDS Image Atlas. We describe the process of training, evaluating,\ncalibrating, and deploying updates to two CNN classifiers for images collected\nby Mars missions. We also report on three years of deployment including usage\nstatistics, lessons learned, and plans for the future.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 18:26:25 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Wagstaff", "Kiri", ""], ["Lu", "Steven", ""], ["Dunkel", "Emily", ""], ["Grimes", "Kevin", ""], ["Zhao", "Brandon", ""], ["Cai", "Jesse", ""], ["Cole", "Shoshanna B.", ""], ["Doran", "Gary", ""], ["Francis", "Raymond", ""], ["Lee", "Jake", ""], ["Mandrake", "Lukas", ""]]}, {"id": "2102.05067", "submitter": "Silvia Cascianelli PhD", "authors": "Silvia Cascianelli, Gabriele Costante, Alessandro Devo, Thomas A.\n  Ciarfuglia, Paolo Valigi, Mario L. Fravolini", "title": "The Role of the Input in Natural Language Video Description", "comments": "In IEEE Transactions on Multimedia", "journal-ref": "IEEE Transactions on Multimedia, 22(1), 271-283 (2019)", "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural Language Video Description (NLVD) has recently received strong\ninterest in the Computer Vision, Natural Language Processing (NLP), Multimedia,\nand Autonomous Robotics communities. The State-of-the-Art (SotA) approaches\nobtained remarkable results when tested on the benchmark datasets. However,\nthose approaches poorly generalize to new datasets. In addition, none of the\nexisting works focus on the processing of the input to the NLVD systems, which\nis both visual and textual. In this work, it is presented an extensive study\ndealing with the role of the visual input, evaluated with respect to the\noverall NLP performance. This is achieved performing data augmentation of the\nvisual component, applying common transformations to model camera distortions,\nnoise, lighting, and camera positioning, that are typical in real-world\noperative scenarios. A t-SNE based analysis is proposed to evaluate the effects\nof the considered transformations on the overall visual data distribution. For\nthis study, it is considered the English subset of Microsoft Research Video\nDescription (MSVD) dataset, which is used commonly for NLVD. It was observed\nthat this dataset contains a relevant amount of syntactic and semantic errors.\nThese errors have been amended manually, and the new version of the dataset\n(called MSVD-v2) is used in the experimentation. The MSVD-v2 dataset is\nreleased to help to gain insight into the NLVD problem.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 19:00:35 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Cascianelli", "Silvia", ""], ["Costante", "Gabriele", ""], ["Devo", "Alessandro", ""], ["Ciarfuglia", "Thomas A.", ""], ["Valigi", "Paolo", ""], ["Fravolini", "Mario L.", ""]]}, {"id": "2102.05090", "submitter": "Akanda Wahid -Ul- Ashraf", "authors": "Marcin Budka, Akanda Wahid Ul Ashraf, Scott Neville, Alun Mackrill,\n  Matthew Bennett", "title": "Deep Multilabel CNN for Forensic Footwear Impression Descriptor\n  Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years deep neural networks have become the workhorse of computer\nvision. In this paper, we employ a deep learning approach to classify footwear\nimpression's features known as \\emph{descriptors} for forensic use cases.\nWithin this process, we develop and evaluate an effective technique for feeding\ndownsampled greyscale impressions to a neural network pre-trained on data from\na different domain. Our approach relies on learnable preprocessing layer paired\nwith multiple interpolation methods used in parallel. We empirically show that\nthis technique outperforms using a single type of interpolated image without\nlearnable preprocessing, and can help to avoid the computational penalty\nrelated to using high resolution inputs, by making more efficient use of the\nlow resolution inputs. We also investigate the effect of preserving the aspect\nratio of the inputs, which leads to considerable boost in accuracy without\nincreasing the computational budget with respect to squished rectangular\nimages. Finally, we formulate a set of best practices for transfer learning\nwith greyscale inputs, potentially widely applicable in computer vision tasks\nranging from footwear impression classification to medical imaging.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 19:39:28 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Budka", "Marcin", ""], ["Ashraf", "Akanda Wahid Ul", ""], ["Neville", "Scott", ""], ["Mackrill", "Alun", ""], ["Bennett", "Matthew", ""]]}, {"id": "2102.05095", "submitter": "Gedas Bertasius", "authors": "Gedas Bertasius, Heng Wang, Lorenzo Torresani", "title": "Is Space-Time Attention All You Need for Video Understanding?", "comments": "Accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a convolution-free approach to video classification built\nexclusively on self-attention over space and time. Our method, named\n\"TimeSformer,\" adapts the standard Transformer architecture to video by\nenabling spatiotemporal feature learning directly from a sequence of\nframe-level patches. Our experimental study compares different self-attention\nschemes and suggests that \"divided attention,\" where temporal attention and\nspatial attention are separately applied within each block, leads to the best\nvideo classification accuracy among the design choices considered. Despite the\nradically new design, TimeSformer achieves state-of-the-art results on several\naction recognition benchmarks, including the best reported accuracy on\nKinetics-400 and Kinetics-600. Finally, compared to 3D convolutional networks,\nour model is faster to train, it can achieve dramatically higher test\nefficiency (at a small drop in accuracy), and it can also be applied to much\nlonger video clips (over one minute long). Code and models are available at:\nhttps://github.com/facebookresearch/TimeSformer.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 19:49:33 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 14:52:06 GMT"}, {"version": "v3", "created": "Fri, 16 Apr 2021 14:41:50 GMT"}, {"version": "v4", "created": "Wed, 9 Jun 2021 14:48:13 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Bertasius", "Gedas", ""], ["Wang", "Heng", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "2102.05104", "submitter": "Sahar Abdelnabi", "authors": "Sahar Abdelnabi and Mario Fritz", "title": "\"What's in the box?!\": Deflecting Adversarial Attacks by Randomly\n  Deploying Adversarially-Disjoint Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are now widely deployed in real-world applications.\nHowever, the existence of adversarial examples has been long considered a real\nthreat to such models. While numerous defenses aiming to improve the robustness\nhave been proposed, many have been shown ineffective. As these vulnerabilities\nare still nowhere near being eliminated, we propose an alternative\ndeployment-based defense paradigm that goes beyond the traditional white-box\nand black-box threat models. Instead of training a single partially-robust\nmodel, one could train a set of same-functionality, yet, adversarially-disjoint\nmodels with minimal in-between attack transferability. These models could then\nbe randomly and individually deployed, such that accessing one of them\nminimally affects the others. Our experiments on CIFAR-10 and a wide range of\nattacks show that we achieve a significantly lower attack transferability\nacross our disjoint models compared to a baseline of ensemble diversity. In\naddition, compared to an adversarially trained set, we achieve a higher average\nrobust accuracy while maintaining the accuracy of clean examples.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 20:07:13 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 13:53:52 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Abdelnabi", "Sahar", ""], ["Fritz", "Mario", ""]]}, {"id": "2102.05105", "submitter": "Angel Villar-Corrales", "authors": "Angel Villar-Corrales, Franziska Schirrmacher and Christian Riess", "title": "Deep learning architectural designs for super-resolution of noisy images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in deep learning have led to significant improvements in\nsingle image super-resolution (SR) research. However, due to the amplification\nof noise during the upsampling steps, state-of-the-art methods often fail at\nreconstructing high-resolution images from noisy versions of their\nlow-resolution counterparts. However, this is especially important for images\nfrom unknown cameras with unseen types of image degradation. In this work, we\npropose to jointly perform denoising and super-resolution. To this end, we\ninvestigate two architectural designs: \"in-network\" combines both tasks at\nfeature level, while \"pre-network\" first performs denoising and then\nsuper-resolution. Our experiments show that both variants have specific\nadvantages: The in-network design obtains the strongest results when the type\nof image corruption is aligned in the training and testing dataset, for any\nchoice of denoiser. The pre-network design exhibits superior performance on\nunseen types of image corruption, which is a pathological failure case of\nexisting super-resolution models. We hope that these findings help to enable\nsuper-resolution also in less constrained scenarios where source camera or\nimaging conditions are not well controlled. Source code and pretrained models\nare available at https://github.com/\nangelvillar96/super-resolution-noisy-images.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 20:09:42 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Villar-Corrales", "Angel", ""], ["Schirrmacher", "Franziska", ""], ["Riess", "Christian", ""]]}, {"id": "2102.05113", "submitter": "Abhishek Sinha", "authors": "Abhishek Sinha, Kumar Ayush, Jiaming Song, Burak Uzkent, Hongxia Jin,\n  Stefano Ermon", "title": "Negative Data Augmentation", "comments": "Accepted at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data augmentation is often used to enlarge datasets with synthetic samples\ngenerated in accordance with the underlying data distribution. To enable a\nwider range of augmentations, we explore negative data augmentation strategies\n(NDA)that intentionally create out-of-distribution samples. We show that such\nnegative out-of-distribution samples provide information on the support of the\ndata distribution, and can be leveraged for generative modeling and\nrepresentation learning. We introduce a new GAN training objective where we use\nNDA as an additional source of synthetic data for the discriminator. We prove\nthat under suitable conditions, optimizing the resulting objective still\nrecovers the true data distribution but can directly bias the generator towards\navoiding samples that lack the desired structure. Empirically, models trained\nwith our method achieve improved conditional/unconditional image generation\nalong with improved anomaly detection capabilities. Further, we incorporate the\nsame negative data augmentation strategy in a contrastive learning framework\nfor self-supervised representation learning on images and videos, achieving\nimproved performance on downstream image classification, object detection, and\naction recognition tasks. These results suggest that prior knowledge on what\ndoes not constitute valid data is an effective form of weak supervision across\na range of unsupervised learning tasks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 20:28:35 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Sinha", "Abhishek", ""], ["Ayush", "Kumar", ""], ["Song", "Jiaming", ""], ["Uzkent", "Burak", ""], ["Jin", "Hongxia", ""], ["Ermon", "Stefano", ""]]}, {"id": "2102.05117", "submitter": "Kamak Ebadi", "authors": "Kamak Ebadi, Matteo Palieri, Sally Wood, Curtis Padgett, Ali-akbar\n  Agha-mohammadi", "title": "DARE-SLAM: Degeneracy-Aware and Resilient Loop Closing in\n  Perceptually-Degraded Environments", "comments": "Accepted for publication in Journal of Intelligent and Robotic\n  Systems, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Enabling fully autonomous robots capable of navigating and exploring\nlarge-scale, unknown and complex environments has been at the core of robotics\nresearch for several decades. A key requirement in autonomous exploration is\nbuilding accurate and consistent maps of the unknown environment that can be\nused for reliable navigation. Loop closure detection, the ability to assert\nthat a robot has returned to a previously visited location, is crucial for\nconsistent mapping as it reduces the drift caused by error accumulation in the\nestimated robot trajectory. Moreover, in multi-robot systems, loop closures\nenable merging local maps obtained by a team of robots into a consistent global\nmap of the environment. In this paper, we present a degeneracy-aware and\ndrift-resilient loop closing method to improve place recognition and resolve 3D\nlocation ambiguities for simultaneous localization and mapping (SLAM) in\nGPS-denied, large-scale and perceptually-degraded environments. More\nspecifically, we focus on SLAM in subterranean environments (e.g., lava tubes,\ncaves, and mines) that represent examples of complex and ambiguous environments\nwhere current methods have inadequate performance.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 20:37:17 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Ebadi", "Kamak", ""], ["Palieri", "Matteo", ""], ["Wood", "Sally", ""], ["Padgett", "Curtis", ""], ["Agha-mohammadi", "Ali-akbar", ""]]}, {"id": "2102.05150", "submitter": "Yizhou Wang", "authors": "Yizhou Wang, Zhongyu Jiang, Yudong Li, Jenq-Neng Hwang, Guanbin Xing,\n  Hui Liu", "title": "RODNet: A Real-Time Radar Object Detection Network Cross-Supervised by\n  Camera-Radar Fused Object 3D Localization", "comments": "IEEE Journal of Selected Topics in Signal Processing Special Issue on\n  Recent Advances in Automotive Radar Signal Processing. arXiv admin note: text\n  overlap with arXiv:2003.01816", "journal-ref": null, "doi": "10.1109/JSTSP.2021.3058895", "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various autonomous or assisted driving strategies have been facilitated\nthrough the accurate and reliable perception of the environment around a\nvehicle. Among the commonly used sensors, radar has usually been considered as\na robust and cost-effective solution even in adverse driving scenarios, e.g.,\nweak/strong lighting or bad weather. Instead of considering to fuse the\nunreliable information from all available sensors, perception from pure radar\ndata becomes a valuable alternative that is worth exploring. In this paper, we\npropose a deep radar object detection network, named RODNet, which is\ncross-supervised by a camera-radar fused algorithm without laborious annotation\nefforts, to effectively detect objects from the radio frequency (RF) images in\nreal-time. First, the raw signals captured by millimeter-wave radars are\ntransformed to RF images in range-azimuth coordinates. Second, our proposed\nRODNet takes a sequence of RF images as the input to predict the likelihood of\nobjects in the radar field of view (FoV). Two customized modules are also added\nto handle multi-chirp information and object relative motion. Instead of using\nhuman-labeled ground truth for training, the proposed RODNet is\ncross-supervised by a novel 3D localization of detected objects using a\ncamera-radar fusion (CRF) strategy in the training stage. Finally, we propose a\nmethod to evaluate the object detection performance of the RODNet. Due to no\nexisting public dataset available for our task, we create a new dataset, named\nCRUW, which contains synchronized RGB and RF image sequences in various driving\nscenarios. With intensive experiments, our proposed cross-supervised RODNet\nachieves 86% average precision and 88% average recall of object detection\nperformance, which shows the robustness to noisy scenarios in various driving\nconditions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 22:01:55 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Wang", "Yizhou", ""], ["Jiang", "Zhongyu", ""], ["Li", "Yudong", ""], ["Hwang", "Jenq-Neng", ""], ["Xing", "Guanbin", ""], ["Liu", "Hui", ""]]}, {"id": "2102.05176", "submitter": "Tom\\'a\\v{s} Chobola", "authors": "Tom\\'a\\v{s} Chobola, Daniel Va\\v{s}ata, Pavel Kord\\'ik", "title": "Transfer learning based few-shot classification using optimal transport\n  mapping from preprocessed latent space of backbone neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  MetaDL Challenge 2020 focused on image classification tasks in few-shot\nsettings. This paper describes second best submission in the competition. Our\nmeta learning approach modifies the distribution of classes in a latent space\nproduced by a backbone network for each class in order to better follow the\nGaussian distribution. After this operation which we call Latent Space\nTransform algorithm, centers of classes are further aligned in an iterative\nfashion of the Expectation Maximisation algorithm to utilize information in\nunlabeled data that are often provided on top of few labelled instances. For\nthis task, we utilize optimal transport mapping using the Sinkhorn algorithm.\nOur experiments show that this approach outperforms previous works as well as\nother variants of the algorithm, using K-Nearest Neighbour algorithm, Gaussian\nMixture Models, etc.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 23:10:58 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 16:04:28 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Chobola", "Tom\u00e1\u0161", ""], ["Va\u0161ata", "Daniel", ""], ["Kord\u00edk", "Pavel", ""]]}, {"id": "2102.05210", "submitter": "Guanglei Zhang", "authors": "Xiangyu Zhao, Peng Zhang, Fan Song, Guangda Fan, Yangyang Sun, Yujia\n  Wang, Zheyuan Tian, Luqi Zhang, Guanglei Zhang", "title": "D2A U-Net: Automatic Segmentation of COVID-19 Lesions from CT Slices\n  with Dilated Convolution and Dual Attention Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coronavirus Disease 2019 (COVID-19) has caused great casualties and becomes\nalmost the most urgent public health events worldwide. Computed tomography (CT)\nis a significant screening tool for COVID-19 infection, and automated\nsegmentation of lung infection in COVID-19 CT images will greatly assist\ndiagnosis and health care of patients. However, accurate and automatic\nsegmentation of COVID-19 lung infections remains to be challenging. In this\npaper we propose a dilated dual attention U-Net (D2A U-Net) for COVID-19 lesion\nsegmentation in CT slices based on dilated convolution and a novel dual\nattention mechanism to address the issues above. We introduce a dilated\nconvolution module in model decoder to achieve large receptive field, which\nrefines decoding process and contributes to segmentation accuracy. Also, we\npresent a dual attention mechanism composed of two attention modules which are\ninserted to skip connection and model decoder respectively. The dual attention\nmechanism is utilized to refine feature maps and reduce semantic gap between\ndifferent levels of the model. The proposed method has been evaluated on\nopen-source dataset and outperforms cutting edges methods in semantic\nsegmentation. Our proposed D2A U-Net with pretrained encoder achieves a Dice\nscore of 0.7298 and recall score of 0.7071. Besides, we also build a simplified\nD2A U-Net without pretrained encoder to provide a fair comparison with other\nmodels trained from scratch, which still outperforms popular U-Net family\nmodels with a Dice score of 0.7047 and recall score of 0.6626. Our experiment\nresults have shown that by introducing dilated convolution and dual attention\nmechanism, the number of false positives is significantly reduced, which\nimproves sensitivity to COVID-19 lesions and subsequently brings significant\nincrease to Dice score.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 01:21:59 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Zhao", "Xiangyu", ""], ["Zhang", "Peng", ""], ["Song", "Fan", ""], ["Fan", "Guangda", ""], ["Sun", "Yangyang", ""], ["Wang", "Yujia", ""], ["Tian", "Zheyuan", ""], ["Zhang", "Luqi", ""], ["Zhang", "Guanglei", ""]]}, {"id": "2102.05212", "submitter": "Moein Shakeri", "authors": "Moein Shakeri, Shing Yan Loo, Hong Zhang", "title": "Polarimetric Monocular Dense Mapping Using Relative Deep Depth Prior", "comments": "9 pages, 9 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with polarimetric dense map reconstruction based on a\npolarization camera with the help of relative depth information as a prior. In\ngeneral, polarization imaging is able to reveal information about surface\nnormal such as azimuth and zenith angles, which can support the development of\nsolutions to the problem of dense reconstruction, especially in texture-poor\nregions. However, polarimetric shape cues are ambiguous due to two types of\npolarized reflection (specular/diffuse). Although methods have been proposed to\naddress this issue, they either are offline and therefore not practical in\nrobotics applications, or use incomplete polarimetric cues, leading to\nsub-optimal performance. In this paper, we propose an online reconstruction\nmethod that uses full polarimetric cues available from the polarization camera.\nWith our online method, we can propagate sparse depth values both along and\nperpendicular to iso-depth contours. Through comprehensive experiments on\nchallenging image sequences, we demonstrate that our method is able to\nsignificantly improve the accuracy of the depthmap as well as increase its\ndensity, specially in regions of poor texture.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 01:34:37 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Shakeri", "Moein", ""], ["Loo", "Shing Yan", ""], ["Zhang", "Hong", ""]]}, {"id": "2102.05216", "submitter": "Sara Bunian", "authors": "Sara Bunian, Kai Li, Chaima Jemmali, Casper Harteveld, Yun Fu, Magy\n  Seif El-Nasr", "title": "VINS: Visual Search for Mobile User Interface Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching for relative mobile user interface (UI) design examples can aid\ninterface designers in gaining inspiration and comparing design alternatives.\nHowever, finding such design examples is challenging, especially as current\nsearch systems rely on only text-based queries and do not consider the UI\nstructure and content into account. This paper introduces VINS, a visual search\nframework, that takes as input a UI image (wireframe, high-fidelity) and\nretrieves visually similar design examples. We first survey interface designers\nto better understand their example finding process. We then develop a\nlarge-scale UI dataset that provides an accurate specification of the\ninterface's view hierarchy (i.e., all the UI components and their specific\nlocation). By utilizing this dataset, we propose an object-detection based\nimage retrieval framework that models the UI context and hierarchical\nstructure. The framework achieves a mean Average Precision of 76.39\\% for the\nUI detection and high performance in querying similar UI designs.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 01:46:33 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Bunian", "Sara", ""], ["Li", "Kai", ""], ["Jemmali", "Chaima", ""], ["Harteveld", "Casper", ""], ["Fu", "Yun", ""], ["El-Nasr", "Magy Seif", ""]]}, {"id": "2102.05218", "submitter": "Qian Yang", "authors": "Qian Yang, Jianyi Zhang, Weituo Hao, Gregory Spell, Lawrence Carin", "title": "FLOP: Federated Learning on Medical Datasets using Partial Networks", "comments": "To appear in KDD 2021", "journal-ref": null, "doi": "10.1145/3447548.3467185", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outbreak of COVID-19 Disease due to the novel coronavirus has caused a\nshortage of medical resources. To aid and accelerate the diagnosis process,\nautomatic diagnosis of COVID-19 via deep learning models has recently been\nexplored by researchers across the world. While different data-driven deep\nlearning models have been developed to mitigate the diagnosis of COVID-19, the\ndata itself is still scarce due to patient privacy concerns. Federated Learning\n(FL) is a natural solution because it allows different organizations to\ncooperatively learn an effective deep learning model without sharing raw data.\nHowever, recent studies show that FL still lacks privacy protection and may\ncause data leakage. We investigate this challenging problem by proposing a\nsimple yet effective algorithm, named \\textbf{F}ederated \\textbf{L}earning\n\\textbf{o}n Medical Datasets using \\textbf{P}artial Networks (FLOP), that\nshares only a partial model between the server and clients. Extensive\nexperiments on benchmark data and real-world healthcare tasks show that our\napproach achieves comparable or better performance while reducing the privacy\nand security risks. Of particular interest, we conduct experiments on the\nCOVID-19 dataset and find that our FLOP algorithm can allow different hospitals\nto collaboratively and effectively train a partially shared model without\nsharing local patients' data.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 01:56:58 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 01:41:15 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Yang", "Qian", ""], ["Zhang", "Jianyi", ""], ["Hao", "Weituo", ""], ["Spell", "Gregory", ""], ["Carin", "Lawrence", ""]]}, {"id": "2102.05228", "submitter": "Hyungsik Jung", "authors": "Hyungsik Jung and Youngrock Oh", "title": "Towards Better Explanations of Class Activation Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing demands for understanding the internal behavior of convolutional\nneural networks (CNNs) have led to remarkable improvements in explanation\nmethods. Particularly, several class activation mapping (CAM) based methods,\nwhich generate visual explanation maps by a linear combination of activation\nmaps from CNNs, have been proposed. However, the majority of the methods lack a\nclear theoretical basis on how they assign the coefficients of the linear\ncombination. In this paper, we revisit the intrinsic linearity of CAM with\nrespect to the activation maps; we construct an explanation model of CNN as a\nlinear function of binary variables that denote the existence of the\ncorresponding activation maps. With this approach, the explanation model can be\ndetermined by additive feature attribution methods in an analytic manner. We\nthen demonstrate the adequacy of SHAP values, which is a unique solution for\nthe explanation model with a set of desirable properties, as the coefficients\nof CAM. Since the exact SHAP values are unattainable, we introduce an efficient\napproximation method, LIFT-CAM, based on DeepLIFT. Our proposed LIFT-CAM can\nestimate the SHAP values of the activation maps with high speed and accuracy.\nFurthermore, it greatly outperforms other previous CAM-based methods in both\nqualitative and quantitative aspects.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 02:43:50 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 03:14:33 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Jung", "Hyungsik", ""], ["Oh", "Youngrock", ""]]}, {"id": "2102.05229", "submitter": "Binjie Qin", "authors": "Dongdong Hao, Song Ding, Linwei Qiu, Yisong Lv, Baowei Fei, Yueqi Zhu,\n  Binjie Qin", "title": "Sequential vessel segmentation via deep channel attention network", "comments": "14", "journal-ref": "Neural Networks, 2020", "doi": "10.1016/j.neunet.2020.05.005", "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper develops a novel encoder-decoder deep network architecture which\nexploits the several contextual frames of 2D+t sequential images in a sliding\nwindow centered at current frame to segment 2D vessel masks from the current\nframe. The architecture is equipped with temporal-spatial feature extraction in\nencoder stage, feature fusion in skip connection layers and channel attention\nmechanism in decoder stage. In the encoder stage, a series of 3D convolutional\nlayers are employed to hierarchically extract temporal-spatial features. Skip\nconnection layers subsequently fuse the temporal-spatial feature maps and\ndeliver them to the corresponding decoder stages. To efficiently discriminate\nvessel features from the complex and noisy backgrounds in the XCA images, the\ndecoder stage effectively utilizes channel attention blocks to refine the\nintermediate feature maps from skip connection layers for subsequently decoding\nthe refined features in 2D ways to produce the segmented vessel masks.\nFurthermore, Dice loss function is implemented to train the proposed deep\nnetwork in order to tackle the class imbalance problem in the XCA data due to\nthe wide distribution of complex background artifacts. Extensive experiments by\ncomparing our method with other state-of-the-art algorithms demonstrate the\nproposed method's superior performance over other methods in terms of the\nquantitative metrics and visual validation. The source codes are at\nhttps://github.com/Binjie-Qin/SVS-net\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 02:45:08 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Hao", "Dongdong", ""], ["Ding", "Song", ""], ["Qiu", "Linwei", ""], ["Lv", "Yisong", ""], ["Fei", "Baowei", ""], ["Zhu", "Yueqi", ""], ["Qin", "Binjie", ""]]}, {"id": "2102.05231", "submitter": "Jinggang Zhuo", "authors": "Yufan Li, Jinggang Zhuo, Ling Fan, Harry Jiannan Wang", "title": "Culture-inspired Multi-modal Color Palette Generation and Colorization:\n  A Chinese Youth Subculture Case", "comments": "accepted by the 3rd IEEE Workshop on Artificial Intelligence for Art\n  Creation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Color is an essential component of graphic design, acting not only as a\nvisual factor but also carrying cultural implications. However, existing\nresearch on algorithmic color palette generation and colorization largely\nignores the cultural aspect. In this paper, we contribute to this line of\nresearch by first constructing a unique color dataset inspired by a specific\nculture, i.e., Chinese Youth Subculture (CYS), which is an vibrant and trending\ncultural group especially for the Gen Z population. We show that the colors\nused in CYS have special aesthetic and semantic characteristics that are\ndifferent from generic color theory. We then develop an interactive multi-modal\ngenerative framework to create CYS-styled color palettes, which can be used to\nput a CYS twist on images using our automatic colorization model. Our framework\nis illustrated via a demo system designed with the human-in-the-loop principle\nthat constantly provides feedback to our algorithms. User studies are also\nconducted to evaluate our generation results.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 02:56:37 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Li", "Yufan", ""], ["Zhuo", "Jinggang", ""], ["Fan", "Ling", ""], ["Wang", "Harry Jiannan", ""]]}, {"id": "2102.05234", "submitter": "Jingbo Yang", "authors": "Jingbo Yang, Ruge Zhao, Meixian Zhu, David Hallac, Jaka Sodnik, Jure\n  Leskovec", "title": "Driver2vec: Driver Identification from Automotive Data", "comments": "7 pages, 3 figures, 6 tables in the main text. First publisehd to 6th\n  Workshop on Mining and Learning from Time Series (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing focus on privacy protection, alternative methods to identify\nvehicle operator without the use of biometric identifiers have gained traction\nfor automotive data analysis. The wide variety of sensors installed on modern\nvehicles enable autonomous driving, reduce accidents and improve vehicle\nhandling. On the other hand, the data these sensors collect reflect drivers'\nhabit. Drivers' use of turn indicators, following distance, rate of\nacceleration, etc. can be transformed to an embedding that is representative of\ntheir behavior and identity. In this paper, we develop a deep learning\narchitecture (Driver2vec) to map a short interval of driving data into an\nembedding space that represents the driver's behavior to assist in driver\nidentification. We develop a custom model that leverages performance gains of\ntemporal convolutional networks, embedding separation power of triplet loss and\nclassification accuracy of gradient boosting decision trees. Trained on a\ndataset of 51 drivers provided by Nervtech, Driver2vec is able to accurately\nidentify the driver from a short 10-second interval of sensor data, achieving\nan average pairwise driver identification accuracy of 83.1% from this 10-second\ninterval, which is remarkably higher than performance obtained in previous\nstudies. We then analyzed performance of Driver2vec to show that its\nperformance is consistent across scenarios and that modeling choices are sound.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 03:09:13 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Yang", "Jingbo", ""], ["Zhao", "Ruge", ""], ["Zhu", "Meixian", ""], ["Hallac", "David", ""], ["Sodnik", "Jaka", ""], ["Leskovec", "Jure", ""]]}, {"id": "2102.05241", "submitter": "Xuankai Liu", "authors": "Fengting Li, Xuankai Liu, Xiaoli Zhang, Qi Li, Kun Sun, Kang Li", "title": "Detecting Localized Adversarial Examples: A Generic Approach using\n  Critical Region Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been applied in a wide range of\napplications,e.g.,face recognition and image classification; however,they are\nvulnerable to adversarial examples. By adding a small amount of imperceptible\nperturbations,an attacker can easily manipulate the outputs of a DNN.\nParticularly,the localized adversarial examples only perturb a small and\ncontiguous region of the target object,so that they are robust and effective in\nboth digital and physical worlds. Although the localized adversarial examples\nhave more severe real-world impacts than traditional pixel attacks,they have\nnot been well addressed in the literature. In this paper,we propose a generic\ndefense system called TaintRadar to accurately detect localized adversarial\nexamples via analyzing critical regions that have been manipulated by\nattackers. The main idea is that when removing critical regions from input\nimages,the ranking changes of adversarial labels will be larger than those of\nbenign labels. Compared with existing defense solutions,TaintRadar can\neffectively capture sophisticated localized partial attacks, e.g.,the\neye-glasses attack,while not requiring additional training or fine-tuning of\nthe original model's structure. Comprehensive experiments have been conducted\nin both digital and physical worlds to verify the effectiveness and robustness\nof our defense.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 03:31:16 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2021 02:29:21 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Li", "Fengting", ""], ["Liu", "Xuankai", ""], ["Zhang", "Xiaoli", ""], ["Li", "Qi", ""], ["Sun", "Kun", ""], ["Li", "Kang", ""]]}, {"id": "2102.05249", "submitter": "Arash Mahyari", "authors": "Arash Mahyari", "title": "Policy Augmentation: An Exploration Strategy for Faster Convergence of\n  Deep Reinforcement Learning Algorithms", "comments": "proceedings of 46th IEEE International Conference on Acoustics,\n  Speech, and Signal Processing (ICASSP), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite advancements in deep reinforcement learning algorithms, developing an\neffective exploration strategy is still an open problem. Most existing\nexploration strategies either are based on simple heuristics, or require the\nmodel of the environment, or train additional deep neural networks to generate\nimagination-augmented paths. In this paper, a revolutionary algorithm, called\nPolicy Augmentation, is introduced. Policy Augmentation is based on a newly\ndeveloped inductive matrix completion method. The proposed algorithm augments\nthe values of unexplored state-action pairs, helping the agent take actions\nthat will result in high-value returns while the agent is in the early\nepisodes. Training deep reinforcement learning algorithms with high-value\nrollouts leads to the faster convergence of deep reinforcement learning\nalgorithms. Our experiments show the superior performance of Policy\nAugmentation. The code can be found at:\nhttps://github.com/arashmahyari/PolicyAugmentation.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 03:51:45 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Mahyari", "Arash", ""]]}, {"id": "2102.05258", "submitter": "Shan You", "authors": "Xiu Su, Shan You, Tao Huang, Fei Wang, Chen Qian, Changshui Zhang,\n  Chang Xu", "title": "Locally Free Weight Sharing for Network Width Search", "comments": "Accepted by ICLR 2021 Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching for network width is an effective way to slim deep neural networks\nwith hardware budgets. With this aim, a one-shot supernet is usually leveraged\nas a performance evaluator to rank the performance \\wrt~different width.\nNevertheless, current methods mainly follow a manually fixed weight sharing\npattern, which is limited to distinguish the performance gap of different\nwidth. In this paper, to better evaluate each width, we propose a locally free\nweight sharing strategy (CafeNet) accordingly. In CafeNet, weights are more\nfreely shared, and each width is jointly indicated by its base channels and\nfree channels, where free channels are supposed to loCAte FrEely in a local\nzone to better represent each width. Besides, we propose to further reduce the\nsearch space by leveraging our introduced FLOPs-sensitive bins. As a result,\nour CafeNet can be trained stochastically and get optimized within a min-min\nstrategy. Extensive experiments on ImageNet, CIFAR-10, CelebA and MS COCO\ndataset have verified our superiority comparing to other state-of-the-art\nbaselines. For example, our method can further boost the benchmark NAS network\nEfficientNet-B0 by 0.41\\% via searching its width more delicately.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 04:36:09 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 13:31:12 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Su", "Xiu", ""], ["You", "Shan", ""], ["Huang", "Tao", ""], ["Wang", "Fei", ""], ["Qian", "Chen", ""], ["Zhang", "Changshui", ""], ["Xu", "Chang", ""]]}, {"id": "2102.05262", "submitter": "Guillaume Charpiat", "authors": "Guillaume Charpiat, Nicolas Girard, Loris Felardos, Yuliya Tarabalka", "title": "Input Similarity from the Neural Network Perspective", "comments": "Published at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We first exhibit a multimodal image registration task, for which a neural\nnetwork trained on a dataset with noisy labels reaches almost perfect accuracy,\nfar beyond noise variance. This surprising auto-denoising phenomenon can be\nexplained as a noise averaging effect over the labels of similar input\nexamples. This effect theoretically grows with the number of similar examples;\nthe question is then to define and estimate the similarity of examples.\n  We express a proper definition of similarity, from the neural network\nperspective, i.e. we quantify how undissociable two inputs $A$ and $B$ are,\ntaking a machine learning viewpoint: how much a parameter variation designed to\nchange the output for $A$ would impact the output for $B$ as well?\n  We study the mathematical properties of this similarity measure, and show how\nto use it on a trained network to estimate sample density, in low complexity,\nenabling new types of statistical analysis for neural networks. We analyze data\nby retrieving samples perceived as similar by the network, and are able to\nquantify the denoising effect without requiring true labels. We also propose,\nduring training, to enforce that examples known to be similar should also be\nseen as similar by the network, and notice speed-up training effects for\ncertain datasets.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 04:57:30 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Charpiat", "Guillaume", ""], ["Girard", "Nicolas", ""], ["Felardos", "Loris", ""], ["Tarabalka", "Yuliya", ""]]}, {"id": "2102.05275", "submitter": "Tairu Qiu", "authors": "Tairu Qiu, Guanxian Chen, Zhongang Qi, Bin Li, Ying Shan, Xiangyang\n  Xue", "title": "A Generic Object Re-identification System for Short Videos", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Short video applications like TikTok and Kwai have been a great hit recently.\nIn order to meet the increasing demands and take full advantage of visual\ninformation in short videos, objects in each short video need to be located and\nanalyzed as an upstream task. A question is thus raised -- how to improve the\naccuracy and robustness of object detection, tracking, and re-identification\nacross tons of short videos with hundreds of categories and complicated visual\neffects (VFX). To this end, a system composed of a detection module, a tracking\nmodule and a generic object re-identification module, is proposed in this\npaper, which captures features of major objects from short videos. In\nparticular, towards the high efficiency demands in practical short video\napplication, a Temporal Information Fusion Network (TIFN) is proposed in the\nobject detection module, which shows comparable accuracy and improved time\nefficiency to the state-of-the-art video object detector. Furthermore, in order\nto mitigate the fragmented issue of tracklets in short videos, a Cross-Layer\nPointwise Siamese Network (CPSN) is proposed in the tracking module to enhance\nthe robustness of the appearance model. Moreover, in order to evaluate the\nproposed system, two challenge datasets containing real-world short videos are\nbuilt for video object trajectory extraction and generic object\nre-identification respectively. Overall, extensive experiments for each module\nand the whole system demonstrate the effectiveness and efficiency of our\nsystem.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 05:45:09 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Qiu", "Tairu", ""], ["Chen", "Guanxian", ""], ["Qi", "Zhongang", ""], ["Li", "Bin", ""], ["Shan", "Ying", ""], ["Xue", "Xiangyang", ""]]}, {"id": "2102.05334", "submitter": "Yael Mathov", "authors": "Yael Mathov, Lior Rokach, Yuval Elovici", "title": "Enhancing Real-World Adversarial Patches with 3D Modeling Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Although many studies have examined adversarial examples in the real world,\nmost of them relied on 2D photos of the attack scene; thus, the attacks\nproposed cannot address realistic environments with 3D objects or varied\nconditions. Studies that use 3D objects are limited, and in many cases, the\nreal-world evaluation process is not replicable by other researchers,\npreventing others from reproducing the results. In this study, we present a\nframework that crafts an adversarial patch for an existing real-world scene.\nOur approach uses a 3D digital approximation of the scene as a simulation of\nthe real world. With the ability to add and manipulate any element in the\ndigital scene, our framework enables the attacker to improve the patch's\nrobustness in real-world settings. We use the framework to create a patch for\nan everyday scene and evaluate its performance using a novel evaluation process\nthat ensures that our results are reproducible in both the digital space and\nthe real world. Our evaluation results show that the framework can generate\nadversarial patches that are robust to different settings in the real world.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 09:16:09 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Mathov", "Yael", ""], ["Rokach", "Lior", ""], ["Elovici", "Yuval", ""]]}, {"id": "2102.05346", "submitter": "Michael K\\\"olle", "authors": "Michael K\\\"olle, Dominik Laupheimer, Stefan Schmohl, Norbert Haala,\n  Franz Rottensteiner, Jan Dirk Wegner, Hugo Ledoux", "title": "The Hessigheim 3D (H3D) Benchmark on Semantic Segmentation of\n  High-Resolution 3D Point Clouds and Textured Meshes from UAV LiDAR and\n  Multi-View-Stereo", "comments": "H3D can be retrieved from\n  https://ifpwww.ifp.uni-stuttgart.de/benchmark/hessigheim/default.aspx", "journal-ref": null, "doi": "10.1016/j.ophoto.2021.100001", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automated semantic segmentation and object detection are of great importance\nin geospatial data analysis. However, supervised machine learning systems such\nas convolutional neural networks require large corpora of annotated training\ndata. Especially in the geospatial domain, such datasets are quite scarce.\nWithin this paper, we aim to alleviate this issue by introducing a new\nannotated 3D dataset that is unique in three ways: i) The dataset consists of\nboth an Unmanned Aerial Vehicle (UAV) laser scanning point cloud and a 3D\ntextured mesh. ii) The point cloud features a mean point density of about 800\npts/sqm and the oblique imagery used for 3D mesh texturing realizes a ground\nsampling distance of about 2-3 cm. This enables the identification of\nfine-grained structures and represents the state of the art in UAV-based\nmapping. iii) Both data modalities will be published for a total of three\nepochs allowing applications such as change detection. The dataset depicts the\nvillage of Hessigheim (Germany), henceforth referred to as H3D. It is designed\nto promote research in the field of 3D data analysis on one hand and to\nevaluate and rank existing and emerging approaches for semantic segmentation of\nboth data modalities on the other hand. Ultimately, we hope that H3D will\nbecome a widely used benchmark dataset in company with the well-established\nISPRS Vaihingen 3D Semantic Labeling Challenge benchmark (V3D). The dataset can\nbe downloaded from\nhttps://ifpwww.ifp.uni-stuttgart.de/benchmark/hessigheim/default.aspx.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 09:33:48 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 19:25:51 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["K\u00f6lle", "Michael", ""], ["Laupheimer", "Dominik", ""], ["Schmohl", "Stefan", ""], ["Haala", "Norbert", ""], ["Rottensteiner", "Franz", ""], ["Wegner", "Jan Dirk", ""], ["Ledoux", "Hugo", ""]]}, {"id": "2102.05348", "submitter": "Benjia Zhou", "authors": "Benjia Zhou, Yunan Li and Jun Wan", "title": "Regional Attention with Architecture-Rebuilt 3D Network for RGB-D\n  Gesture Recognition", "comments": "Accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human gesture recognition has drawn much attention in the area of computer\nvision. However, the performance of gesture recognition is always influenced by\nsome gesture-irrelevant factors like the background and the clothes of\nperformers. Therefore, focusing on the regions of hand/arm is important to the\ngesture recognition. Meanwhile, a more adaptive architecture-searched network\nstructure can also perform better than the block-fixed ones like Resnet since\nit increases the diversity of features in different stages of the network\nbetter. In this paper, we propose a regional attention with\narchitecture-rebuilt 3D network (RAAR3DNet) for gesture recognition. We replace\nthe fixed Inception modules with the automatically rebuilt structure through\nthe network via Neural Architecture Search (NAS), owing to the different shape\nand representation ability of features in the early, middle, and late stage of\nthe network. It enables the network to capture different levels of feature\nrepresentations at different layers more adaptively. Meanwhile, we also design\na stackable regional attention module called dynamic-static Attention (DSA),\nwhich derives a Gaussian guidance heatmap and dynamic motion map to highlight\nthe hand/arm regions and the motion information in the spatial and temporal\ndomains, respectively. Extensive experiments on two recent large-scale RGB-D\ngesture datasets validate the effectiveness of the proposed method and show it\noutperforms state-of-the-art methods. The codes of our method are available at:\nhttps://github.com/zhoubenjia/RAAR3DNet.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 09:36:00 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 16:01:03 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Zhou", "Benjia", ""], ["Li", "Yunan", ""], ["Wan", "Jun", ""]]}, {"id": "2102.05368", "submitter": "Thibault Maho", "authors": "Thibault Maho, Beno\\^it Bonnet, Teddy Furon, Erwan Le Merrer", "title": "RoBIC: A benchmark suite for assessing classifiers robustness", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many defenses have emerged with the development of adversarial attacks.\nModels must be objectively evaluated accordingly. This paper systematically\ntackles this concern by proposing a new parameter-free benchmark we coin RoBIC.\nRoBIC fairly evaluates the robustness of image classifiers using a new\nhalf-distortion measure. It gauges the robustness of the network against white\nand black box attacks, independently of its accuracy. RoBIC is faster than the\nother available benchmarks. We present the significant differences in the\nrobustness of 16 recent models as assessed by RoBIC.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 10:13:39 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Maho", "Thibault", ""], ["Bonnet", "Beno\u00eet", ""], ["Furon", "Teddy", ""], ["Merrer", "Erwan Le", ""]]}, {"id": "2102.05399", "submitter": "Murari Mandal", "authors": "Prateek Garg, Murari Mandal, Pratik Narang", "title": "Improving Aerial Instance Segmentation in the Dark with Self-Supervised\n  Low Light Enhancement", "comments": "Accepted at AAAI Conference on Artificial Intelligence (AAAI),\n  Student Abstract, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low light conditions in aerial images adversely affect the performance of\nseveral vision based applications. There is a need for methods that can\nefficiently remove the low light attributes and assist in the performance of\nkey vision tasks. In this work, we propose a new method that is capable of\nenhancing the low light image in a self-supervised fashion, and sequentially\napply detection and segmentation tasks in an end-to-end manner. The proposed\nmethod occupies a very small overhead in terms of memory and computational\npower over the original algorithm and delivers superior results. Additionally,\nwe propose the generation of a new low light aerial dataset using GANs, which\ncan be used to evaluate vision based networks for similar adverse conditions.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 12:24:40 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Garg", "Prateek", ""], ["Mandal", "Murari", ""], ["Narang", "Pratik", ""]]}, {"id": "2102.05401", "submitter": "Fatemeh Sharifizadeh", "authors": "Fatemeh Sharifizadeh, Mohammad Ganjtabesh, Abbas Nowzari-Dalini", "title": "Enhancing efficiency of object recognition in different categorization\n  levels by reinforcement learning in modular spiking neural networks", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human visual system contains a hierarchical sequence of modules that take\npart in visual perception at superordinate, basic, and subordinate\ncategorization levels. During the last decades, various computational models\nhave been proposed to mimic the hierarchical feed-forward processing of visual\ncortex, but many critical characteristics of the visual system, such actual\nneural processing and learning mechanisms, are ignored. Pursuing the line of\nbiological inspiration, we propose a computational model for object recognition\nin different categorization levels, in which a spiking neural network equipped\nwith the reinforcement learning rule is used as a module at each categorization\nlevel. Each module solves the object recognition problem at each categorization\nlevel, solely based on the earliest spike of class-specific neurons at its last\nlayer, without using any external classifier. According to the required\ninformation at each categorization level, the relevant band-pass filtered\nimages are utilized. The performance of our proposed model is evaluated by\nvarious appraisal criteria with three benchmark datasets and significant\nimprovement in recognition accuracy of our proposed model is achieved in all\nexperiments.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 12:33:20 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Sharifizadeh", "Fatemeh", ""], ["Ganjtabesh", "Mohammad", ""], ["Nowzari-Dalini", "Abbas", ""]]}, {"id": "2102.05402", "submitter": "Ziang Ren", "authors": "Ren Liu, Ziang Ren", "title": "Application of Yolo on Mask Detection Task", "comments": "7 pages, 4 figures, submitted to The 13th International Conference on\n  Computer Research and Development (ICCRD 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  2020 has been a year marked by the COVID-19 pandemic. This event has caused\ndisruptions to many aspects of normal life. An important aspect in reducing the\nimpact of the pandemic is to control its spread. Studies have shown that one\neffective method in reducing the transmission of COVID-19 is to wear masks.\nStrict mask-wearing policies have been met with not only public sensation but\nalso practical difficulty. We cannot hope to manually check if everyone on a\nstreet is wearing a mask properly. Existing technology to help automate mask\nchecking uses deep learning models on real-time surveillance camera footages.\nThe current dominant method to perform real-time mask detection uses Mask-RCNN\nwith ResNet as the backbone. While giving good detection results, this method\nis computationally intensive and its efficiency in real-time face mask\ndetection is not ideal. Our research proposes a new approach to mask detection\nby replacing Mask-R-CNN with a more efficient model \"YOLO\" to increase the\nprocessing speed of real-time mask detection and not compromise on accuracy.\nBesides, given the small volume as well as extreme imbalance of the mask\ndetection datasets, we adopt a latest progress made in few-shot visual\nclassification, simple CNAPs, to improve the classification performance.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 12:34:47 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Liu", "Ren", ""], ["Ren", "Ziang", ""]]}, {"id": "2102.05418", "submitter": "Murari Mandal", "authors": "Harsh Sinha, Aditya Mehta, Murari Mandal, Pratik Narang", "title": "Learning to Enhance Visual Quality via Hyperspectral Domain Mapping", "comments": "Accepted at AAAI Conference on Artificial Intelligence (AAAI),\n  Student Abstract, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based methods have achieved remarkable success in image\nrestoration and enhancement, but most such methods rely on RGB input images.\nThese methods fail to take into account the rich spectral distribution of\nnatural images. We propose a deep architecture, SpecNet, which computes\nspectral profile to estimate pixel-wise dynamic range adjustment of a given\nimage. First, we employ an unpaired cycle-consistent framework to generate\nhyperspectral images (HSI) from low-light input images. HSI is further used to\ngenerate a normal light image of the same scene. We incorporate a\nself-supervision and a spectral profile regularization network to infer a\nplausible HSI from an RGB image. We evaluate the benefits of optimizing the\nspectral profile for real and fake images in low-light conditions on the LOL\nDataset.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 13:27:34 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Sinha", "Harsh", ""], ["Mehta", "Aditya", ""], ["Mandal", "Murari", ""], ["Narang", "Pratik", ""]]}, {"id": "2102.05424", "submitter": "Jintai Chen", "authors": "Jintai Chen, Bohan Yu, Biwen Lei, Ruiwei Feng, Danny Z. Chen, Jian Wu", "title": "Doctor Imitator: A Graph-based Bone Age Assessment Framework Using Hand\n  Radiographs", "comments": null, "journal-ref": "International Conference on Medical Image Computing and\n  Computer-Assisted Intervention (2020)", "doi": "10.1007/978-3-030-59725-2_74", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bone age assessment is challenging in clinical practice due to the\ncomplicated bone age assessment process. Current automatic bone age assessment\nmethods were designed with rare consideration of the diagnostic logistics and\nthus may yield certain uninterpretable hidden states and outputs. Consequently,\ndoctors can find it hard to cooperate with such models harmoniously because it\nis difficult to check the correctness of the model predictions. In this work,\nwe propose a new graph-based deep learning framework for bone age assessment\nwith hand radiographs, called Doctor Imitator (DI). The architecture of DI is\ndesigned to learn the diagnostic logistics of doctors using the scoring methods\n(e.g., the Tanner-Whitehouse method) for bone age assessment. Specifically, the\nconvolutions of DI capture the local features of the anatomical regions of\ninterest (ROIs) on hand radiographs and predict the ROI scores by our proposed\nAnatomy-based Group Convolution, summing up for bone age prediction. Besides,\nwe develop a novel Dual Graph-based Attention module to compute\npatient-specific attention for ROI features and context attention for ROI\nscores. As far as we know, DI is the first automatic bone age assessment\nframework following the scoring methods without fully supervised hand\nradiographs. Experiments on hand radiographs with only bone age supervision\nverify that DI can achieve excellent performance with sparse parameters and\nprovide more interpretability.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 13:45:39 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Chen", "Jintai", ""], ["Yu", "Bohan", ""], ["Lei", "Biwen", ""], ["Feng", "Ruiwei", ""], ["Chen", "Danny Z.", ""], ["Wu", "Jian", ""]]}, {"id": "2102.05426", "submitter": "Yuhang Li", "authors": "Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei\n  Yu, Wei Wang, Shi Gu", "title": "BRECQ: Pushing the Limit of Post-Training Quantization by Block\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We study the challenging task of neural network quantization without\nend-to-end retraining, called Post-training Quantization (PTQ). PTQ usually\nrequires a small subset of training data but produces less powerful quantized\nmodels than Quantization-Aware Training (QAT). In this work, we propose a novel\nPTQ framework, dubbed BRECQ, which pushes the limits of bitwidth in PTQ down to\nINT2 for the first time. BRECQ leverages the basic building blocks in neural\nnetworks and reconstructs them one-by-one. In a comprehensive theoretical study\nof the second-order error, we show that BRECQ achieves a good balance between\ncross-layer dependency and generalization error. To further employ the power of\nquantization, the mixed precision technique is incorporated in our framework by\napproximating the inter-layer and intra-layer sensitivity. Extensive\nexperiments on various handcrafted and searched neural architectures are\nconducted for both image classification and object detection tasks. And for the\nfirst time we prove that, without bells and whistles, PTQ can attain 4-bit\nResNet and MobileNetV2 comparable with QAT and enjoy 240 times faster\nproduction of quantized models. Codes are available at\nhttps://github.com/yhhhli/BRECQ.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 13:46:16 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 09:34:39 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Li", "Yuhang", ""], ["Gong", "Ruihao", ""], ["Tan", "Xu", ""], ["Yang", "Yang", ""], ["Hu", "Peng", ""], ["Zhang", "Qi", ""], ["Yu", "Fengwei", ""], ["Wang", "Wei", ""], ["Gu", "Shi", ""]]}, {"id": "2102.05447", "submitter": "Jianzhu Guo", "authors": "Xiaqing Xu, Qiang Meng, Yunxiao Qin, Jianzhu Guo, Chenxu Zhao, Feng\n  Zhou, and Zhen Lei", "title": "Searching for Alignment in Face Recognition", "comments": "Accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard pipeline of current face recognition frameworks consists of four\nindividual steps: locating a face with a rough bounding box and several\nfiducial landmarks, aligning the face image using a pre-defined template,\nextracting representations and comparing. Among them, face detection, landmark\ndetection and representation learning have long been studied and a lot of works\nhave been proposed. As an essential step with a significant impact on\nrecognition performance, the alignment step has attracted little attention. In\nthis paper, we first explore and highlight the effects of different alignment\ntemplates on face recognition. Then, for the first time, we try to search for\nthe optimal template automatically. We construct a well-defined searching space\nby decomposing the template searching into the crop size and vertical shift,\nand propose an efficient method Face Alignment Policy Search (FAPS). Besides, a\nwell-designed benchmark is proposed to evaluate the searched policy.\nExperiments on our proposed benchmark validate the effectiveness of our method\nto improve face recognition performance.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 14:09:16 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 15:03:10 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Xu", "Xiaqing", ""], ["Meng", "Qiang", ""], ["Qin", "Yunxiao", ""], ["Guo", "Jianzhu", ""], ["Zhao", "Chenxu", ""], ["Zhou", "Feng", ""], ["Lei", "Zhen", ""]]}, {"id": "2102.05450", "submitter": "Sriprabha Ramanarayanan", "authors": "Madhu Mithra K K, Sriprabha Ramanarayanan, Keerthi Ram, Mohanasankar\n  Sivaprakasam", "title": "Reference-based Texture transfer for Single Image Super-resolution of\n  Magnetic Resonance images", "comments": "Accepted at ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Magnetic Resonance Imaging (MRI) is a valuable clinical diagnostic modality\nfor spine pathologies with excellent characterization for infection, tumor,\ndegenerations, fractures and herniations. However in surgery, image-guided\nspinal procedures continue to rely on CT and fluoroscopy, as MRI slice\nresolutions are typically insufficient. Building upon state-of-the-art single\nimage super-resolution, we propose a reference-based, unpaired multi-contrast\ntexture-transfer strategy for deep learning based in-plane and across-plane MRI\nsuper-resolution. We use the scattering transform to relate the texture\nfeatures of image patches to unpaired reference image patches, and additionally\na loss term for multi-contrast texture. We apply our scheme in different\nsuper-resolution architectures, observing improvement in PSNR and SSIM for 4x\nsuper-resolution in most of the cases.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 14:12:48 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["K", "Madhu Mithra K", ""], ["Ramanarayanan", "Sriprabha", ""], ["Ram", "Keerthi", ""], ["Sivaprakasam", "Mohanasankar", ""]]}, {"id": "2102.05451", "submitter": "Yaron Strauch", "authors": "Yaron Strauch (University of Southampton), Jo Grundy (University of\n  Southampton)", "title": "Two Novel Performance Improvements for Evolving CNN Topologies", "comments": "Accepted to AAAI-21 Workshop W17: Learning Network Architecture\n  during Training. 5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) are the state-of-the-art algorithms for\nthe processing of images. However the configuration and training of these\nnetworks is a complex task requiring deep domain knowledge, experience and much\ntrial and error. Using genetic algorithms, competitive CNN topologies for image\nrecognition can be produced for any specific purpose, however in previous work\nthis has come at high computational cost. In this work two novel approaches are\npresented to the utilisation of these algorithms, effective in reducing\ncomplexity and training time by nearly 20%. This is accomplished via\nregularisation directly on training time, and the use of partial training to\nenable early ranking of individual architectures. Both approaches are validated\non the benchmark CIFAR10 data set, and maintain accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 14:17:51 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Strauch", "Yaron", "", "University of Southampton"], ["Grundy", "Jo", "", "University of\n  Southampton"]]}, {"id": "2102.05454", "submitter": "Xinyi Li", "authors": "Xinyi Li, Haibin Ling", "title": "On the Robustness of Multi-View Rotation Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rotation averaging is a synchronization process on single or multiple\nrotation groups, and is a fundamental problem in many computer vision tasks\nsuch as multi-view structure from motion (SfM). Specifically, rotation\naveraging involves the recovery of an underlying pose-graph consistency from\npairwise relative camera poses. Specifically, given pairwise motion in rotation\ngroups, especially 3-dimensional rotation groups (\\eg, $\\mathbb{SO}(3)$), one\nis interested in recovering the original signal of multiple rotations with\nrespect to a fixed frame. In this paper, we propose a robust framework to solve\nmultiple rotation averaging problem, especially in the cases that a significant\namount of noisy measurements are present. By introducing the $\\epsilon$-cycle\nconsistency term into the solver, we enable the robust initialization scheme to\nbe implemented into the IRLS solver. Instead of conducting the costly edge\nremoval, we implicitly constrain the negative effect of erroneous measurements\nby weight reducing, such that IRLS failures caused by poor initialization can\nbe effectively avoided. Experiment results demonstrate that our proposed\napproach outperforms state of the arts on various benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 05:47:37 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Li", "Xinyi", ""], ["Ling", "Haibin", ""]]}, {"id": "2102.05498", "submitter": "Enzo Tartaglione", "authors": "Daniele Perlo, Enzo Tartaglione, Luca Bertero, Paola Cassoni, Marco\n  Grangetto", "title": "Dysplasia grading of colorectal polyps through CNN analysis of WSI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Colorectal cancer is a leading cause of cancer death for both men and women.\nFor this reason, histopathological characterization of colorectal polyps is the\nmajor instrument for the pathologist in order to infer the actual risk for\ncancer and to guide further follow-up. Colorectal polyps diagnosis includes the\nevaluation of the polyp type, and more importantly, the grade of dysplasia.\nThis latter evaluation represents a critical step for the clinical follow-up.\nThe proposed deep learning-based classification pipeline is based on\nstate-of-the-art convolutional neural network, trained using proper\ncountermeasures to tackle WSI high resolution and very imbalanced dataset. The\nexperimental results show that one can successfully classify adenomas dysplasia\ngrade with 70% accuracy, which is in line with the pathologists' concordance.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 15:40:27 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Perlo", "Daniele", ""], ["Tartaglione", "Enzo", ""], ["Bertero", "Luca", ""], ["Cassoni", "Paola", ""], ["Grangetto", "Marco", ""]]}, {"id": "2102.05509", "submitter": "Sebastian Cygert", "authors": "Sebastian Cygert, Andrzej Czy\\.zewski", "title": "Robustness in Compressed Neural Networks for Object Detection", "comments": "IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model compression techniques allow to significantly reduce the computational\ncost associated with data processing by deep neural networks with only a minor\ndecrease in average accuracy. Simultaneously, reducing the model size may have\na large effect on noisy cases or objects belonging to less frequent classes. It\nis a crucial problem from the perspective of the models' safety, especially for\nobject detection in the autonomous driving setting, which is considered in this\nwork. It was shown in the paper that the sensitivity of compressed models to\ndifferent distortion types is nuanced, and some of the corruptions are heavily\nimpacted by the compression methods (i.e., additive noise), while others (blur\neffect) are only slightly affected. A common way to improve the robustness of\nmodels is to use data augmentation, which was confirmed to positively affect\nmodels' robustness, also for highly compressed models. It was further shown\nthat while data imbalance methods brought only a slight increase in accuracy\nfor the baseline model (without compression), the impact was more striking at\nhigher compression rates for the structured pruning. Finally, methods for\nhandling data imbalance brought a significant improvement of the pruned models'\nworst-detected class accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 15:52:11 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 21:14:27 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Cygert", "Sebastian", ""], ["Czy\u017cewski", "Andrzej", ""]]}, {"id": "2102.05543", "submitter": "Fahdi Kanavati", "authors": "Fahdi Kanavati, Masayuki Tsuneki", "title": "Partial transfusion: on the expressive influence of trainable batch norm\n  parameters for transfer learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning from ImageNet is the go-to approach when applying deep\nlearning to medical images. The approach is either to fine-tune a pre-trained\nmodel or use it as a feature extractor. Most modern architecture contain batch\nnormalisation layers, and fine-tuning a model with such layers requires taking\na few precautions as they consist of trainable and non-trainable weights and\nhave two operating modes: training and inference. Attention is primarily given\nto the non-trainable weights used during inference, as they are the primary\nsource of unexpected behaviour or degradation in performance during transfer\nlearning. It is typically recommended to fine-tune the model with the batch\nnormalisation layers kept in inference mode during both training and inference.\nIn this paper, we pay closer attention instead to the trainable weights of the\nbatch normalisation layers, and we explore their expressive influence in the\ncontext of transfer learning. We find that only fine-tuning the trainable\nweights (scale and centre) of the batch normalisation layers leads to similar\nperformance as to fine-tuning all of the weights, with the added benefit of\nfaster convergence. We demonstrate this on a variety of seven publicly\navailable medical imaging datasets, using four different model architectures.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 16:29:03 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Kanavati", "Fahdi", ""], ["Tsuneki", "Masayuki", ""]]}, {"id": "2102.05567", "submitter": "Werner Creixell", "authors": "Diego Lazcano, Nicol\\'as Fredes and Werner Creixell", "title": "Hyperbolic Generative Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, Hyperbolic Spaces in the context of Non-Euclidean Deep Learning\nhave gained popularity because of their ability to represent hierarchical data.\nWe propose that it is possible to take advantage of the hierarchical\ncharacteristic present in the images by using hyperbolic neural networks in a\nGAN architecture. In this study, different configurations using fully connected\nhyperbolic layers in the GAN, CGAN, and WGAN are tested, in what we call the\nHGAN, HCGAN, and HWGAN, respectively. The results are measured using the\nInception Score (IS) and the Fr\\'echet Inception Distance (FID) on the MNIST\ndataset. Depending on the configuration and space curvature, better results are\nachieved for each proposed hyperbolic versions than their euclidean\ncounterpart.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 16:55:27 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Lazcano", "Diego", ""], ["Fredes", "Nicol\u00e1s", ""], ["Creixell", "Werner", ""]]}, {"id": "2102.05582", "submitter": "Brian McClannahan", "authors": "Brian McClannahan, Cucong Zhong, Guanghui Wang", "title": "Classification of Long Noncoding RNA Elements Using Deep Convolutional\n  Neural Networks and Siamese Networks", "comments": "arXiv admin note: text overlap with arXiv:2008.10580", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In the last decade, the discovery of noncoding RNA(ncRNA) has exploded.\nClassifying these ncRNA is critical todetermining their function. This thesis\nproposes a new methodemploying deep convolutional neural networks (CNNs) to\nclassifyncRNA sequences. To this end, this paper first proposes anefficient\napproach to convert the RNA sequences into imagescharacterizing their\nbase-pairing probability. As a result, clas-sifying RNA sequences is converted\nto an image classificationproblem that can be efficiently solved by available\nCNN-basedclassification models. This research also considers the\nfoldingpotential of the ncRNAs in addition to their primary sequence.Based on\nthe proposed approach, a benchmark image classifi-cation dataset is generated\nfrom the RFAM database of ncRNAsequences. In addition, three classical CNN\nmodels and threeSiamese network models have been implemented and comparedto\ndemonstrate the superior performance and efficiency of theproposed approach.\nExtensive experimental results show thegreat potential of using deep learning\napproaches for RNAclassification.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 17:26:38 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["McClannahan", "Brian", ""], ["Zhong", "Cucong", ""], ["Wang", "Guanghui", ""]]}, {"id": "2102.05607", "submitter": "Volker Steinhage", "authors": "Timm Haucke and Volker Steinhage", "title": "Exploiting Depth Information for Wildlife Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera traps are a proven tool in biology and specifically biodiversity\nresearch. However, camera traps including depth estimation are not widely\ndeployed, despite providing valuable context about the scene and facilitating\nthe automation of previously laborious manual ecological methods. In this\nstudy, we propose an automated camera trap-based approach to detect and\nidentify animals using depth estimation. To detect and identify individual\nanimals, we propose a novel method D-Mask R-CNN for the so-called instance\nsegmentation which is a deep learning-based technique to detect and delineate\neach distinct object of interest appearing in an image or a video clip. An\nexperimental evaluation shows the benefit of the additional depth estimation in\nterms of improved average precision scores of the animal detection compared to\nthe standard approach that relies just on the image information. This novel\napproach was also evaluated in terms of a proof-of-concept in a zoo scenario\nusing an RGB-D camera trap.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 18:10:34 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Haucke", "Timm", ""], ["Steinhage", "Volker", ""]]}, {"id": "2102.05610", "submitter": "Sheng Li Dr.", "authors": "Sheng Li, Mingxing Tan, Ruoming Pang, Andrew Li, Liqun Cheng, Quoc Le,\n  Norman P. Jouppi", "title": "Searching for Fast Model Families on Datacenter Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural Architecture Search (NAS), together with model scaling, has shown\nremarkable progress in designing high accuracy and fast convolutional\narchitecture families. However, as neither NAS nor model scaling considers\nsufficient hardware architecture details, they do not take full advantage of\nthe emerging datacenter (DC) accelerators. In this paper, we search for fast\nand accurate CNN model families for efficient inference on DC accelerators. We\nfirst analyze DC accelerators and find that existing CNNs suffer from\ninsufficient operational intensity, parallelism, and execution efficiency.\nThese insights let us create a DC-accelerator-optimized search space, with\nspace-to-depth, space-to-batch, hybrid fused convolution structures with\nvanilla and depthwise convolutions, and block-wise activation functions. On top\nof our DC accelerator optimized neural architecture search space, we further\npropose a latency-aware compound scaling (LACS), the first multi-objective\ncompound scaling method optimizing both accuracy and latency. Our LACS\ndiscovers that network depth should grow much faster than image size and\nnetwork width, which is quite different from previous compound scaling results.\nWith the new search space and LACS, our search and scaling on datacenter\naccelerators results in a new model series named EfficientNet-X. EfficientNet-X\nis up to more than 2X faster than EfficientNet (a model series with\nstate-of-the-art trade-off on FLOPs and accuracy) on TPUv3 and GPUv100, with\ncomparable accuracy. EfficientNet-X is also up to 7X faster than recent RegNet\nand ResNeSt on TPUv3 and GPUv100.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 18:15:40 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Li", "Sheng", ""], ["Tan", "Mingxing", ""], ["Pang", "Ruoming", ""], ["Li", "Andrew", ""], ["Cheng", "Liqun", ""], ["Le", "Quoc", ""], ["Jouppi", "Norman P.", ""]]}, {"id": "2102.05644", "submitter": "Alaaeldin El-Nouby", "authors": "Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, Herv\\'e J\\'egou", "title": "Training Vision Transformers for Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers have shown outstanding results for natural language\nunderstanding and, more recently, for image classification. We here extend this\nwork and propose a transformer-based approach for image retrieval: we adopt\nvision transformers for generating image descriptors and train the resulting\nmodel with a metric learning objective, which combines a contrastive loss with\na differential entropy regularizer. Our results show consistent and significant\nimprovements of transformers over convolution-based approaches. In particular,\nour method outperforms the state of the art on several public benchmarks for\ncategory-level retrieval, namely Stanford Online Product, In-Shop and CUB-200.\nFurthermore, our experiments on ROxford and RParis also show that, in\ncomparable settings, transformers are competitive for particular object\nretrieval, especially in the regime of short vector representations and\nlow-resolution images.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 18:56:41 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["El-Nouby", "Alaaeldin", ""], ["Neverova", "Natalia", ""], ["Laptev", "Ivan", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "2102.05645", "submitter": "Andrew Brown", "authors": "Andrew Brown, Ernesto Coto, Andrew Zisserman", "title": "Automated Video Labelling: Identifying Faces by Corroborative Evidence", "comments": null, "journal-ref": "IEEE 4th International Conference on Multimedia Information\n  Processing and Retrieval (IEEE MIPR 2021)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a method for automatically labelling all faces in video archives,\nsuch as TV broadcasts, by combining multiple evidence sources and multiple\nmodalities (visual and audio). We target the problem of ever-growing online\nvideo archives, where an effective, scalable indexing solution cannot require a\nuser to provide manual annotation or supervision. To this end, we make three\nkey contributions: (1) We provide a novel, simple, method for determining if a\nperson is famous or not using image-search engines. In turn this enables a\nface-identity model to be built reliably and robustly, and used for high\nprecision automatic labelling; (2) We show that even for less-famous people,\nimage-search engines can then be used for corroborative evidence to accurately\nlabel faces that are named in the scene or the speech; (3) Finally, we\nquantitatively demonstrate the benefits of our approach on different video\ndomains and test settings, such as TV shows and news broadcasts. Our method\nworks across three disparate datasets without any explicit domain adaptation,\nand sets new state-of-the-art results on all the public benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 18:57:52 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Brown", "Andrew", ""], ["Coto", "Ernesto", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2102.05646", "submitter": "Bharat Singh", "authors": "Bharat Singh, Mahyar Najibi, Abhishek Sharma and Larry S. Davis", "title": "Scale Normalized Image Pyramids with AutoFocus for Object Detection", "comments": "Accepted in T-PAMI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient foveal framework to perform object detection. A scale\nnormalized image pyramid (SNIP) is generated that, like human vision, only\nattends to objects within a fixed size range at different scales. Such a\nrestriction of objects' size during training affords better learning of\nobject-sensitive filters, and therefore, results in better accuracy. However,\nthe use of an image pyramid increases the computational cost. Hence, we propose\nan efficient spatial sub-sampling scheme which only operates on fixed-size\nsub-regions likely to contain objects (as object locations are known during\ntraining). The resulting approach, referred to as Scale Normalized Image\nPyramid with Efficient Resampling or SNIPER, yields up to 3 times speed-up\nduring training. Unfortunately, as object locations are unknown during\ninference, the entire image pyramid still needs processing. To this end, we\nadopt a coarse-to-fine approach, and predict the locations and extent of\nobject-like regions which will be processed in successive scales of the image\npyramid. Intuitively, it's akin to our active human-vision that first skims\nover the field-of-view to spot interesting regions for further processing and\nonly recognizes objects at the right resolution. The resulting algorithm is\nreferred to as AutoFocus and results in a 2.5-5 times speed-up during inference\nwhen used with SNIP.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 18:57:53 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Singh", "Bharat", ""], ["Najibi", "Mahyar", ""], ["Sharma", "Abhishek", ""], ["Davis", "Larry S.", ""]]}, {"id": "2102.05692", "submitter": "Mollie Bianchi", "authors": "Mollie Bianchi and Timothy D. Barfoot", "title": "UAV Localization Using Autoencoded Satellite Images", "comments": "Accepted for publication in RA-L 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and demonstrate a fast, robust method for using satellite images\nto localize an Unmanned Aerial Vehicle (UAV). Previous work using satellite\nimages has large storage and computation costs and is unable to run in real\ntime. In this work, we collect Google Earth (GE) images for a desired flight\npath offline and an autoencoder is trained to compress these images to a\nlow-dimensional vector representation while retaining the key features. This\ntrained autoencoder is used to compress a real UAV image, which is then\ncompared to the precollected, nearby, autoencoded GE images using an\ninner-product kernel. This results in a distribution of weights over the\ncorresponding GE image poses and is used to generate a single localization and\nassociated covariance to represent uncertainty. Our localization is computed in\n1% of the time of the current standard and is able to achieve a comparable RMSE\nof less than 3m in our experiments, where we robustly matched UAV images from\nsix runs spanning the lighting conditions of a single day to the same map of\nsatellite images.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 19:08:10 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Bianchi", "Mollie", ""], ["Barfoot", "Timothy D.", ""]]}, {"id": "2102.05705", "submitter": "Sarah Tymochko", "authors": "Tegan Emerson, Sarah Tymochko, George Stantchev, Jason A. Edelberg,\n  Michael Wilson, and Colin C. Olson", "title": "A Topological Approach for Motion Track Discrimination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting small targets at range is difficult because there is not enough\nspatial information present in an image sub-region containing the target to use\ncorrelation-based methods to differentiate it from dynamic confusers present in\nthe scene. Moreover, this lack of spatial information also disqualifies the use\nof most state-of-the-art deep learning image-based classifiers. Here, we use\ncharacteristics of target tracks extracted from video sequences as data from\nwhich to derive distinguishing topological features that help robustly\ndifferentiate targets of interest from confusers. In particular, we calculate\npersistent homology from time-delayed embeddings of dynamic statistics\ncalculated from motion tracks extracted from a wide field-of-view video stream.\nIn short, we use topological methods to extract features related to target\nmotion dynamics that are useful for classification and disambiguation and show\nthat small targets can be detected at range with high probability.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 19:25:38 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Emerson", "Tegan", ""], ["Tymochko", "Sarah", ""], ["Stantchev", "George", ""], ["Edelberg", "Jason A.", ""], ["Wilson", "Michael", ""], ["Olson", "Colin C.", ""]]}, {"id": "2102.05715", "submitter": "Sai Aparna Aketi", "authors": "Sai Aparna Aketi, Amandeep Singh, Jan Rabaey", "title": "Sparse-Push: Communication- & Energy-Efficient Decentralized Distributed\n  Learning over Directed & Time-Varying Graphs with non-IID Datasets", "comments": "12 pages, 7 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current deep learning (DL) systems rely on a centralized computing paradigm\nwhich limits the amount of available training data, increases system latency,\nand adds privacy and security constraints. On-device learning, enabled by\ndecentralized and distributed training of DL models over peer-to-peer\nwirelessly connected edge devices, not only alleviate the above limitations but\nalso enable next-gen applications that need DL models to continuously interact\nand learn from their environment. However, this necessitates the development of\nnovel training algorithms that train DL models over time-varying and directed\npeer-to-peer graph structures while minimizing the amount of communication\nbetween the devices and also being resilient to non-IID data distributions. In\nthis work we propose, Sparse-Push, a communication efficient decentralized\ndistributed training algorithm that supports training over peer-to-peer,\ndirected, and time-varying graph topologies. The proposed algorithm enables\n466x reduction in communication with only 1% degradation in performance when\ntraining various DL models such as ResNet-20 and VGG11 over the CIFAR-10\ndataset. Further, we demonstrate how communication compression can lead to\nsignificant performance degradation in-case of non-IID datasets, and propose\nSkew-Compensated Sparse Push algorithm that recovers this performance drop\nwhile maintaining similar levels of communication compression.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 19:41:11 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 02:05:24 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Aketi", "Sai Aparna", ""], ["Singh", "Amandeep", ""], ["Rabaey", "Jan", ""]]}, {"id": "2102.05775", "submitter": "Yue Meng", "authors": "Yue Meng, Rameswar Panda, Chung-Ching Lin, Prasanna Sattigeri, Leonid\n  Karlinsky, Kate Saenko, Aude Oliva, Rogerio Feris", "title": "AdaFuse: Adaptive Temporal Fusion Network for Efficient Action\n  Recognition", "comments": "Accepted to ICLR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Temporal modelling is the key for efficient video action recognition. While\nunderstanding temporal information can improve recognition accuracy for dynamic\nactions, removing temporal redundancy and reusing past features can\nsignificantly save computation leading to efficient action recognition. In this\npaper, we introduce an adaptive temporal fusion network, called AdaFuse, that\ndynamically fuses channels from current and past feature maps for strong\ntemporal modelling. Specifically, the necessary information from the historical\nconvolution feature maps is fused with current pruned feature maps with the\ngoal of improving both recognition accuracy and efficiency. In addition, we use\na skipping operation to further reduce the computation cost of action\nrecognition. Extensive experiments on Something V1 & V2, Jester and\nMini-Kinetics show that our approach can achieve about 40% computation savings\nwith comparable accuracy to state-of-the-art methods. The project page can be\nfound at https://mengyuest.github.io/AdaFuse/\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 23:31:02 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Meng", "Yue", ""], ["Panda", "Rameswar", ""], ["Lin", "Chung-Ching", ""], ["Sattigeri", "Prasanna", ""], ["Karlinsky", "Leonid", ""], ["Saenko", "Kate", ""], ["Oliva", "Aude", ""], ["Feris", "Rogerio", ""]]}, {"id": "2102.05811", "submitter": "Karel Mundnich", "authors": "Karel Mundnich and Alexandra Fenster and Aparna Khare and Shiva\n  Sundaram", "title": "Audiovisual Highlight Detection in Videos", "comments": "5 pages, 2 figures, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we test the hypothesis that interesting events in unstructured\nvideos are inherently audiovisual. We combine deep image representations for\nobject recognition and scene understanding with representations from an\naudiovisual affect recognition model. To this set, we include content agnostic\naudio-visual synchrony representations and mel-frequency cepstral coefficients\nto capture other intrinsic properties of audio. These features are used in a\nmodular supervised model. We present results from two experiments: efficacy\nstudy of single features on the task, and an ablation study where we leave one\nfeature out at a time. For the video summarization task, our results indicate\nthat the visual features carry most information, and including audiovisual\nfeatures improves over visual-only information. To better study the task of\nhighlight detection, we run a pilot experiment with highlights annotations for\na small subset of video clips and fine-tune our best model on it. Results\nindicate that we can transfer knowledge from the video summarization task to a\nmodel trained specifically for the task of highlight detection.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 02:24:00 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Mundnich", "Karel", ""], ["Fenster", "Alexandra", ""], ["Khare", "Aparna", ""], ["Sundaram", "Shiva", ""]]}, {"id": "2102.05822", "submitter": "Jianjin Xu", "authors": "Jianjin Xu, Zheyang Xiong, Xiaolin Hu", "title": "Frame Difference-Based Temporal Loss for Video Stylization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural style transfer models have been used to stylize an ordinary video to\nspecific styles. To ensure temporal inconsistency between the frames of the\nstylized video, a common approach is to estimate the optic flow of the pixels\nin the original video and make the generated pixels match the estimated optical\nflow. This is achieved by minimizing an optical flow-based (OFB) loss during\nmodel training. However, optical flow estimation is itself a challenging task,\nparticularly in complex scenes. In addition, it incurs a high computational\ncost. We propose a much simpler temporal loss called the frame difference-based\n(FDB) loss to solve the temporal inconsistency problem. It is defined as the\ndistance between the difference between the stylized frames and the difference\nbetween the original frames. The differences between the two frames are\nmeasured in both the pixel space and the feature space specified by the\nconvolutional neural networks. A set of human behavior experiments involving 62\nsubjects with 25,600 votes showed that the performance of the proposed FDB loss\nmatched that of the OFB loss. The performance was measured by subjective\nevaluation of stability and stylization quality of the generated videos on two\ntypical video stylization models. The results suggest that the proposed FDB\nloss is a strong alternative to the commonly used OFB loss for video\nstylization.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 02:59:55 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Xu", "Jianjin", ""], ["Xiong", "Zheyang", ""], ["Hu", "Xiaolin", ""]]}, {"id": "2102.05843", "submitter": "Sobhan Moosavi", "authors": "Sobhan Moosavi, Pravar D. Mahajan, Srinivasan Parthasarathy, Colleen\n  Saunders-Chukwu, and Rajiv Ramnath", "title": "Driving Style Representation in Convolutional Recurrent Neural Network\n  Model of Driver Identification", "comments": "12 pages, research on driving style representation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Identifying driving styles is the task of analyzing the behavior of drivers\nin order to capture variations that will serve to discriminate different\ndrivers from each other. This task has become a prerequisite for a variety of\napplications, including usage-based insurance, driver coaching, driver action\nprediction, and even in designing autonomous vehicles; because driving style\nencodes essential information needed by these applications. In this paper, we\npresent a deep-neural-network architecture, we term D-CRNN, for building\nhigh-fidelity representations for driving style, that combine the power of\nconvolutional neural networks (CNN) and recurrent neural networks (RNN). Using\nCNN, we capture semantic patterns of driver behavior from trajectories (such as\na turn or a braking event). We then find temporal dependencies between these\nsemantic patterns using RNN to encode driving style. We demonstrate the\neffectiveness of these techniques for driver identification by learning driving\nstyle through extensive experiments conducted on several large, real-world\ndatasets, and comparing the results with the state-of-the-art deep-learning and\nnon-deep-learning solutions. These experiments also demonstrate a useful\nexample of bias removal, by presenting how we preprocess the input data by\nsampling dissimilar trajectories for each driver to prevent spatial\nmemorization. Finally, this paper presents an analysis of the contribution of\ndifferent attributes for driver identification; we find that engine RPM, Speed,\nand Acceleration are the best combination of features.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 04:33:43 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Moosavi", "Sobhan", ""], ["Mahajan", "Pravar D.", ""], ["Parthasarathy", "Srinivasan", ""], ["Saunders-Chukwu", "Colleen", ""], ["Ramnath", "Rajiv", ""]]}, {"id": "2102.05847", "submitter": "Zheng Shi", "authors": "Zheng Shi, Ethan Tseng, Mario Bijelic, Werner Ritter, Felix Heide", "title": "ZeroScatter: Domain Transfer for Long Distance Imaging and Vision\n  through Scattering Media", "comments": "2021 IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), project page available at\n  https://light.princeton.edu/publication/zeroscatter/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adverse weather conditions, including snow, rain, and fog, pose a major\nchallenge for both human and computer vision. Handling these environmental\nconditions is essential for safe decision making, especially in autonomous\nvehicles, robotics, and drones. Most of today's supervised imaging and vision\napproaches, however, rely on training data collected in the real world that is\nbiased towards good weather conditions, with dense fog, snow, and heavy rain as\noutliers in these datasets. Without training data, let alone paired data,\nexisting autonomous vehicles often limit themselves to good conditions and stop\nwhen dense fog or snow is detected. In this work, we tackle the lack of\nsupervised training data by combining synthetic and indirect supervision. We\npresent ZeroScatter, a domain transfer method for converting RGB-only captures\ntaken in adverse weather into clear daytime scenes. ZeroScatter exploits\nmodel-based, temporal, multi-view, multi-modal, and adversarial cues in a joint\nfashion, allowing us to train on unpaired, biased data. We assess the proposed\nmethod on in-the-wild captures, and the proposed method outperforms existing\nmonocular descattering approaches by 2.8 dB PSNR on controlled fog chamber\nmeasurements.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 04:41:17 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 21:27:30 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Shi", "Zheng", ""], ["Tseng", "Ethan", ""], ["Bijelic", "Mario", ""], ["Ritter", "Werner", ""], ["Heide", "Felix", ""]]}, {"id": "2102.05869", "submitter": "Johan Lilius", "authors": "Bogdan Iancu, Valentin Soloviev, Luca Zelioli, Johan Lilius", "title": "ABOShips -- An Inshore and Offshore Maritime Vessel Detection Dataset\n  with Precise Annotations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Availability of domain-specific datasets is an essential problem in object\ndetection. Maritime vessel detection of inshore and offshore datasets is no\nexception, there is a limited number of studies addressing this need. For that\nreason, we collected a dataset of images of maritime vessels taking into\naccount different factors: background variation, atmospheric conditions,\nillumination, visible proportion, occlusion and scale variation. Vessel\ninstances (including 9 types of vessels), seamarks and miscellaneous floaters\nwere precisely annotated: we employed a first round of labelling and\nsubsequently, we used the CSRT [1] tracker to trace inconsistencies and relabel\ninadequate label instances. Moreover, we evaluated the the out-of-the-box\nperformance of four prevalent object detection algorithms (Faster R-CNN [2],\nR-FCN [3], SSD [4] and EfficientDet [5]). The algorithms were previously\ntrained on the Microsoft COCO dataset. We compare their accuracy based on\nfeature extractor and object size. Our experiments show that Faster R-CNN with\nInception-Resnet v2 outperforms the other algorithms, except in the large\nobject category where EfficientDet surpasses the latter.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 07:05:33 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Iancu", "Bogdan", ""], ["Soloviev", "Valentin", ""], ["Zelioli", "Luca", ""], ["Lilius", "Johan", ""]]}, {"id": "2102.05874", "submitter": "David Malmgren-Hansen Mr.", "authors": "David Malmgren-Hansen, Allan Aasbjerg Nielsen and Leif Toudal Pedersen", "title": "Explainability in CNN Models By Means of Z-Scores", "comments": "Intended and accepted for the \"Deep Learning Meets Earth Sciences:\n  From Hybrid Modeling to Explainability\" workshop at IGARSS 2020, but was\n  redrawn due to authors being unable to participate when lockdown restrictions\n  moved the conference days. The work was conducted 2019 under the Automated\n  Sea Ice Products (ASIP) project funded by the Innovation Fund Denmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper explores the similarities of output layers in Neural Networks\n(NNs) with logistic regression to explain importance of inputs by Z-scores. The\nnetwork analyzed, a network for fusion of Synthetic Aperture Radar (SAR) and\nMicrowave Radiometry (MWR) data, is applied to prediction of arctic sea ice.\nWith the analysis the importance of MWR relative to SAR is found to favor MWR\ncomponents. Further, as the model represents image features at different\nscales, the relative importance of these are as well analyzed. The suggested\nmethodology offers a simple and easy framework for analyzing output layer\ncomponents and can reduce the number of components for further analysis with\ne.g. common NN visualization methods.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 07:22:38 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Malmgren-Hansen", "David", ""], ["Nielsen", "Allan Aasbjerg", ""], ["Pedersen", "Leif Toudal", ""]]}, {"id": "2102.05897", "submitter": "Jasmin Breitenstein", "authors": "Jasmin Breitenstein and Jan-Aike Term\\\"ohlen and Daniel Lipinski and\n  Tim Fingscheidt", "title": "Corner Cases for Visual Perception in Automated Driving: Some Guidance\n  on Detection Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated driving has become a major topic of interest not only in the active\nresearch community but also in mainstream media reports. Visual perception of\nsuch intelligent vehicles has experienced large progress in the last decade\nthanks to advances in deep learning techniques but some challenges still\nremain. One such challenge is the detection of corner cases. They are\nunexpected and unknown situations that occur while driving. Conventional visual\nperception methods are often not able to detect them because corner cases have\nnot been witnessed during training. Hence, their detection is highly\nsafety-critical, and detection methods can be applied to vast amounts of\ncollected data to select suitable training data. A reliable detection of corner\ncases will not only further automate the data selection procedure and increase\nsafety in autonomous driving but can thereby also affect the public acceptance\nof the new technology in a positive manner. In this work, we continue a\nprevious systematization of corner cases on different levels by an extended set\nof examples for each level. Moreover, we group detection approaches into\ndifferent categories and link them with the corner case levels. Hence, we give\ndirections to showcase specific corner cases and basic guidelines on how to\ntechnically detect them.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 09:06:13 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Breitenstein", "Jasmin", ""], ["Term\u00f6hlen", "Jan-Aike", ""], ["Lipinski", "Daniel", ""], ["Fingscheidt", "Tim", ""]]}, {"id": "2102.05918", "submitter": "Chao Jia", "authors": "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham,\n  Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy\n  Text Supervision", "comments": "ICML 2021", "journal-ref": "International Conference on Machine Learning 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained representations are becoming crucial for many NLP and perception\ntasks. While representation learning in NLP has transitioned to training on raw\ntext without human annotations, visual and vision-language representations\nstill rely heavily on curated training datasets that are expensive or require\nexpert knowledge. For vision applications, representations are mostly learned\nusing datasets with explicit class labels such as ImageNet or OpenImages. For\nvision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all\ninvolve a non-trivial data collection (and cleaning) process. This costly\ncuration process limits the size of datasets and hence hinders the scaling of\ntrained models. In this paper, we leverage a noisy dataset of over one billion\nimage alt-text pairs, obtained without expensive filtering or post-processing\nsteps in the Conceptual Captions dataset. A simple dual-encoder architecture\nlearns to align visual and language representations of the image and text pairs\nusing a contrastive loss. We show that the scale of our corpus can make up for\nits noise and leads to state-of-the-art representations even with such a simple\nlearning scheme. Our visual representation achieves strong performance when\ntransferred to classification tasks such as ImageNet and VTAB. The aligned\nvisual and language representations enables zero-shot image classification and\nalso set new state-of-the-art results on Flickr30K and MSCOCO image-text\nretrieval benchmarks, even when compared with more sophisticated\ncross-attention models. The representations also enable cross-modality search\nwith complex text and text + image queries.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 10:08:12 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 07:51:39 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Jia", "Chao", ""], ["Yang", "Yinfei", ""], ["Xia", "Ye", ""], ["Chen", "Yi-Ting", ""], ["Parekh", "Zarana", ""], ["Pham", "Hieu", ""], ["Le", "Quoc V.", ""], ["Sung", "Yunhsuan", ""], ["Li", "Zhen", ""], ["Duerig", "Tom", ""]]}, {"id": "2102.05950", "submitter": "Sohail Ahmed Khan", "authors": "Sohail Ahmed Khan, Alessandro Artusi, Hang Dai", "title": "Adversarially robust deepfake media detection using fused convolutional\n  neural network predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deepfakes are synthetically generated images, videos or audios, which\nfraudsters use to manipulate legitimate information. Current deepfake detection\nsystems struggle against unseen data. To address this, we employ three\ndifferent deep Convolutional Neural Network (CNN) models, (1) VGG16, (2)\nInceptionV3, and (3) XceptionNet to classify fake and real images extracted\nfrom videos. We also constructed a fusion of the deep CNN models to improve the\nrobustness and generalisation capability. The proposed technique outperforms\nstate-of-the-art models with 96.5% accuracy, when tested on publicly available\nDeepFake Detection Challenge (DFDC) test data, comprising of 400 videos. The\nfusion model achieves 99% accuracy on lower quality DeepFake-TIMIT dataset\nvideos and 91.88% on higher quality DeepFake-TIMIT videos. In addition to this,\nwe prove that prediction fusion is more robust against adversarial attacks. If\none model is compromised by an adversarial attack, the prediction fusion does\nnot let it affect the overall classification.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 11:28:00 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Khan", "Sohail Ahmed", ""], ["Artusi", "Alessandro", ""], ["Dai", "Hang", ""]]}, {"id": "2102.05963", "submitter": "Alejandro Sztrajman", "authors": "Alejandro Sztrajman, Gilles Rainer, Tobias Ritschel, Tim Weyrich", "title": "Neural BRDF Representation and Importance Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controlled capture of real-world material appearance yields tabulated sets of\nhighly realistic reflectance data. In practice, however, its high memory\nfootprint requires compressing into a representation that can be used\nefficiently in rendering while remaining faithful to the original. Previous\nworks in appearance encoding often prioritised one of these requirements at the\nexpense of the other, by either applying high-fidelity array compression\nstrategies not suited for efficient queries during rendering, or by fitting a\ncompact analytic model that lacks expressiveness. We present a compact neural\nnetwork-based representation of BRDF data that combines high-accuracy\nreconstruction with efficient practical rendering via built-in interpolation of\nreflectance. We encode BRDFs as lightweight networks, and propose a training\nscheme with adaptive angular sampling, critical for the accurate reconstruction\nof specular highlights. Additionally, we propose a novel approach to make our\nrepresentation amenable to importance sampling: rather than inverting the\ntrained networks, we learn to encode them in a more compact embedding that can\nbe mapped to parameters of an analytic BRDF for which importance sampling is\nknown. We evaluate encoding results on isotropic and anisotropic BRDFs from\nmultiple real-world datasets, and importance sampling performance for isotropic\nBRDFs mapped to two different analytic models.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 12:00:24 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 12:38:18 GMT"}, {"version": "v3", "created": "Fri, 14 May 2021 21:57:47 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Sztrajman", "Alejandro", ""], ["Rainer", "Gilles", ""], ["Ritschel", "Tobias", ""], ["Weyrich", "Tim", ""]]}, {"id": "2102.05971", "submitter": "Sheng Zhang", "authors": "Jiahao Xie, Sheng Zhang, Jianwei Lu, Ye Luo", "title": "L-SNet: from Region Localization to Scale Invariant Medical Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coarse-to-fine models and cascade segmentation architectures are widely\nadopted to solve the problem of large scale variations in medical image\nsegmentation. However, those methods have two primary limitations: the\nfirst-stage segmentation becomes a performance bottleneck; the lack of overall\ndifferentiability makes the training process of two stages asynchronous and\ninconsistent. In this paper, we propose a differentiable two-stage network\narchitecture to tackle these problems. In the first stage, a localization\nnetwork (L-Net) locates Regions of Interest (RoIs) in a detection fashion; in\nthe second stage, a segmentation network (S-Net) performs fine segmentation on\nthe recalibrated RoIs; a RoI recalibration module between L-Net and S-Net\neliminating the inconsistencies. Experimental results on the public dataset\nshow that our method outperforms state-of-the-art coarse-to-fine models with\nnegligible computation overheads.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 12:29:39 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Xie", "Jiahao", ""], ["Zhang", "Sheng", ""], ["Lu", "Jianwei", ""], ["Luo", "Ye", ""]]}, {"id": "2102.05973", "submitter": "Przemys{\\l}aw Spurek", "authors": "Przemys{\\l}aw Spurek, Artur Kasymov, Marcin Mazur, Diana Janik,\n  S{\\l}awomir Tadeja, {\\L}ukasz Struski, Jacek Tabor, Tomasz Trzci\\'nski", "title": "HyperPocket: Generative Point Cloud Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scanning real-life scenes with modern registration devices typically give\nincomplete point cloud representations, mostly due to the limitations of the\nscanning process and 3D occlusions. Therefore, completing such partial\nrepresentations remains a fundamental challenge of many computer vision\napplications. Most of the existing approaches aim to solve this problem by\nlearning to reconstruct individual 3D objects in a synthetic setup of an\nuncluttered environment, which is far from a real-life scenario. In this work,\nwe reformulate the problem of point cloud completion into an object\nhallucination task. Thus, we introduce a novel autoencoder-based architecture\ncalled HyperPocket that disentangles latent representations and, as a result,\nenables the generation of multiple variants of the completed 3D point clouds.\nWe split point cloud processing into two disjoint data streams and leverage a\nhypernetwork paradigm to fill the spaces, dubbed pockets, that are left by the\nmissing object parts. As a result, the generated point clouds are not only\nsmooth but also plausible and geometrically consistent with the scene. Our\nmethod offers competitive performances to the other state-of-the-art models,\nand it enables a~plethora of novel applications.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 12:30:03 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Spurek", "Przemys\u0142aw", ""], ["Kasymov", "Artur", ""], ["Mazur", "Marcin", ""], ["Janik", "Diana", ""], ["Tadeja", "S\u0142awomir", ""], ["Struski", "\u0141ukasz", ""], ["Tabor", "Jacek", ""], ["Trzci\u0144ski", "Tomasz", ""]]}, {"id": "2102.05984", "submitter": "Przemys{\\l}aw Spurek", "authors": "Przemys{\\l}aw Spurek, Sebastian Winczowski, Maciej Zi\\k{e}ba, Tomasz\n  Trzci\\'nski, Kacper Kania", "title": "Modeling 3D Surface Manifolds with a Locally Conditioned Atlas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently proposed 3D object reconstruction methods represent a mesh with an\natlas - a set of planar patches approximating the surface. However, their\napplication in a real-world scenario is limited since the surfaces of\nreconstructed objects contain discontinuities, which degrades the quality of\nthe final mesh. This is mainly caused by independent processing of individual\npatches, and in this work, we postulate to mitigate this limitation by\npreserving local consistency around patch vertices. To that end, we introduce a\nLocally Conditioned Atlas (LoCondA), a framework for representing a 3D object\nhierarchically in a generative model. Firstly, the model maps a point cloud of\nan object into a sphere. Secondly, by leveraging a spherical prior, we enforce\nthe mapping to be locally consistent on the sphere and on the target object.\nThis way, we can sample a mesh quad on that sphere and project it back onto the\nobject's manifold. With LoCondA, we can produce topologically diverse objects\nwhile maintaining quads to be stitched together. We show that the proposed\napproach provides structurally coherent reconstructions while producing meshes\nof quality comparable to the competitors.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 13:04:49 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Spurek", "Przemys\u0142aw", ""], ["Winczowski", "Sebastian", ""], ["Zi\u0119ba", "Maciej", ""], ["Trzci\u0144ski", "Tomasz", ""], ["Kania", "Kacper", ""]]}, {"id": "2102.05998", "submitter": "Alexander Sch\\\"afer", "authors": "Alexander Sch\\\"afer, Gerd Reis, Didier Stricker", "title": "A Survey on Synchronous Augmented, Virtual and Mixed Reality Remote\n  Collaboration Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote collaboration systems have become increasingly important in today's\nsociety, especially during times where physical distancing is advised.\nIndustry, research and individuals face the challenging task of collaborating\nand networking over long distances. While video and teleconferencing are\nalready widespread, collaboration systems in augmented, virtual, and mixed\nreality are still a niche technology. We provide an overview of recent\ndevelopments of synchronous remote collaboration systems and create a taxonomy\nby dividing them into three main components that form such systems:\nEnvironment, Avatars, and Interaction. A thorough overview of existing systems\nis given, categorising their main contributions in order to help researchers\nworking in different fields by providing concise information about specific\ntopics such as avatars, virtual environment, visualisation styles and\ninteraction. The focus of this work is clearly on synchronised collaboration\nfrom a distance. A total of 82 unique systems for remote collaboration are\ndiscussed, including more than 100 publications and 25 commercial systems.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 13:33:51 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Sch\u00e4fer", "Alexander", ""], ["Reis", "Gerd", ""], ["Stricker", "Didier", ""]]}, {"id": "2102.06022", "submitter": "Marco Roccetti", "authors": "Luca Casini, Marco Roccetti, Giovanni Delnevo, Nicolo' Marchetti,\n  Valentina Orru'", "title": "The Barrier of meaning in archaeological data science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Archaeologists, like other scientists, are experiencing a data-flood in their\ndiscipline, fueled by a surge in computing power and devices that enable the\ncreation, collection, storage and transfer of an increasingly complex (and\nlarge) amount of data, such as remotely sensed imagery from a multitude of\nsources. In this paper, we pose the preliminary question if this increasing\navailability of information actually needs new computerized techniques, and\nArtificial Intelligence methods, to make new and deeper understanding into\narchaeological problems. Simply said, while it is a fact that Deep Learning\n(DL) has become prevalent as a type of machine learning design inspired by the\nway humans learn, and utilized to perform automatic actions people might\ndescribe as intelligent, we want to anticipate, here, a discussion around the\nsubject whether machines, trained following this procedure, can extrapolate,\nfrom archaeological data, concepts and meaning in the same way that humans\nwould do. Even prior to getting to technical results, we will start our\nreflection with a very basic concept: Is a collection of satellite images with\nnotable archaeological sites informative enough to instruct a DL machine to\ndiscover new archaeological sites, as well as other potential locations of\ninterest? Further, what if similar results could be reached with less\nintelligent machines that learn by having people manually program them with\nrules? Finally: If with barrier of meaning we refer to the extent to which\nhuman-like understanding can be achieved by a machine, where should be posed\nthat barrier in the archaeological data science?\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 17:24:45 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Casini", "Luca", ""], ["Roccetti", "Marco", ""], ["Delnevo", "Giovanni", ""], ["Marchetti", "Nicolo'", ""], ["Orru'", "Valentina", ""]]}, {"id": "2102.06060", "submitter": "Tae Jun Jang", "authors": "Tae Jun Jang, Kang Cheol Kim, Hyun Cheol Cho, Jin Keun Seo", "title": "A fully automated method for 3D individual tooth identification and\n  segmentation in dental CBCT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and automatic segmentation of three-dimensional (3D) individual\nteeth from cone-beam computerized tomography (CBCT) images is a challenging\nproblem because of the difficulty in separating an individual tooth from\nadjacent teeth and its surrounding alveolar bone. Thus, this paper proposes a\nfully automated method of identifying and segmenting 3D individual teeth from\ndental CBCT images. The proposed method addresses the aforementioned difficulty\nby developing a deep learning-based hierarchical multi-step model. First, it\nautomatically generates upper and lower jaws panoramic images to overcome the\ncomputational complexity caused by high-dimensional data and the curse of\ndimensionality associated with limited training dataset. The obtained 2D\npanoramic images are then used to identify 2D individual teeth and capture\nloose- and tight- regions of interest (ROIs) of 3D individual teeth. Finally,\naccurate 3D individual tooth segmentation is achieved using both loose and\ntight ROIs. Experimental results showed that the proposed method achieved an\nF1-score of 93.35% for tooth identification and a Dice similarity coefficient\nof 94.79% for individual 3D tooth segmentation. The results demonstrate that\nthe proposed method provides an effective clinical and practical framework for\ndigital dentistry.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 15:07:23 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Jang", "Tae Jun", ""], ["Kim", "Kang Cheol", ""], ["Cho", "Hyun Cheol", ""], ["Seo", "Jin Keun", ""]]}, {"id": "2102.06096", "submitter": "Hamid Tizhoosh", "authors": "Antonio Sze-To, Abtin Riasatian, Hamid R. Tizhoosh", "title": "Searching for Pneumothorax in X-Ray Images Using Autoencoded Deep\n  Features", "comments": "Under review for publication since May 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Fast diagnosis and treatment of pneumothorax, a collapsed or dropped lung, is\ncrucial to avoid fatalities. Pneumothorax is typically detected on a chest\nX-ray image through visual inspection by experienced radiologists. However, the\ndetection rate is quite low. Therefore, there is a strong need for automated\ndetection systems to assist radiologists. Despite the high accuracy levels\ngenerally reported for deep learning classifiers in many applications, they may\nnot be useful in clinical practice due to the lack of large number of\nhigh-quality labelled images as well as a lack of interpretation possibility.\nAlternatively, searching in the archive of past cases to find matching images\nmay serve as a 'virtual second opinion' through accessing the metadata of\nmatched evidently diagnosed cases. To use image search as a triaging/diagnosis\ntool, all chest X-ray images must first be tagged with identifiers, i.e., deep\nfeatures. Then, given a query chest X-ray image, the majority vote among the\ntop k retrieved images can provide a more explainable output. While image\nsearch can be clinically more viable, its detection performance needs to be\ninvestigated at a scale closer to real-world practice. We combined 3 public\ndatasets to assemble a repository with more than 550,000 chest X-ray images. We\ndeveloped the Autoencoding Thorax Net (short AutoThorax-Net) for image search\nin chest radiographs compressing three inputs: the left chest side, the flipped\nright side, and the entire chest image. Experimental results show that image\nsearch based on AutoThorax-Net features can achieve high identification rates\nproviding a path towards real-world deployment. We achieved 92% AUC accuracy\nfor a semi-automated search in 194,608 images (pneumothorax and normal) and 82%\nAUC accuracy for fully automated search in 551,383 images (normal, pneumothorax\nand many other chest diseases).\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 16:21:06 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Sze-To", "Antonio", ""], ["Riasatian", "Abtin", ""], ["Tizhoosh", "Hamid R.", ""]]}, {"id": "2102.06102", "submitter": "Moran Xu", "authors": "Moran Xu, Dianlin Hu, Weifei Wu, and Weiwen Wu", "title": "Deep Iteration Assisted by Multi-level Obey-pixel Network Discriminator\n  (DIAMOND) for Medical Image Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image restoration is a typical ill-posed problem, and it contains various\ntasks. In the medical imaging field, an ill-posed image interrupts diagnosis\nand even following image processing. Both traditional iterative and up-to-date\ndeep networks have attracted much attention and obtained a significant\nimprovement in reconstructing satisfying images. This study combines their\nadvantages into one unified mathematical model and proposes a general image\nrestoration strategy to deal with such problems. This strategy consists of two\nmodules. First, a novel generative adversarial net(GAN) with WGAN-GP training\nis built to recover image structures and subtle details. Then, a deep iteration\nmodule promotes image quality with a combination of pre-trained deep networks\nand compressed sensing algorithms by ADMM optimization. (D)eep (I)teration\nmodule suppresses image artifacts and further recovers subtle image details,\n(A)ssisted by (M)ulti-level (O)bey-pixel feature extraction networks\n(D)iscriminator to recover general structures. Therefore, the proposed strategy\nis named DIAMOND.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 16:57:33 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Xu", "Moran", ""], ["Hu", "Dianlin", ""], ["Wu", "Weifei", ""], ["Wu", "Weiwen", ""]]}, {"id": "2102.06108", "submitter": "Rinon Gal", "authors": "Rinon Gal, Dana Cohen, Amit Bermano, Daniel Cohen-Or", "title": "SWAGAN: A Style-based Wavelet-driven Generative Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, considerable progress has been made in the visual quality of\nGenerative Adversarial Networks (GANs). Even so, these networks still suffer\nfrom degradation in quality for high-frequency content, stemming from a\nspectrally biased architecture, and similarly unfavorable loss functions. To\naddress this issue, we present a novel general-purpose Style and WAvelet based\nGAN (SWAGAN) that implements progressive generation in the frequency domain.\nSWAGAN incorporates wavelets throughout its generator and discriminator\narchitectures, enforcing a frequency-aware latent representation at every step\nof the way. This approach yields enhancements in the visual quality of the\ngenerated images, and considerably increases computational performance. We\ndemonstrate the advantage of our method by integrating it into the SyleGAN2\nframework, and verifying that content generation in the wavelet domain leads to\nhigher quality images with more realistic high-frequency content. Furthermore,\nwe verify that our model's latent space retains the qualities that allow\nStyleGAN to serve as a basis for a multitude of editing tasks, and show that\nour frequency-aware approach also induces improved downstream visual quality.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 16:43:10 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Gal", "Rinon", ""], ["Cohen", "Dana", ""], ["Bermano", "Amit", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2102.06109", "submitter": "Sean McGregor", "authors": "Claire Leibowicz, Sean McGregor, Aviv Ovadya", "title": "The Deepfake Detection Dilemma: A Multistakeholder Exploration of\n  Adversarial Dynamics in Synthetic Media", "comments": "11 pages, 8 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Synthetic media detection technologies label media as either synthetic or\nnon-synthetic and are increasingly used by journalists, web platforms, and the\ngeneral public to identify misinformation and other forms of problematic\ncontent. As both well-resourced organizations and the non-technical general\npublic generate more sophisticated synthetic media, the capacity for purveyors\nof problematic content to adapt induces a \\newterm{detection dilemma}: as\ndetection practices become more accessible, they become more easily\ncircumvented. This paper describes how a multistakeholder cohort from academia,\ntechnology platforms, media entities, and civil society organizations active in\nsynthetic media detection and its socio-technical implications evaluates the\ndetection dilemma. Specifically, we offer an assessment of detection contexts\nand adversary capacities sourced from the broader, global AI and media\nintegrity community concerned with mitigating the spread of harmful synthetic\nmedia. A collection of personas illustrates the intersection between\nunsophisticated and highly-resourced sponsors of misinformation in the context\nof their technical capacities. This work concludes that there is no \"best\"\napproach to navigating the detector dilemma, but derives a set of implications\nfrom multistakeholder input to better inform detection process decisions and\npolicies, in practice.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 16:44:09 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Leibowicz", "Claire", ""], ["McGregor", "Sean", ""], ["Ovadya", "Aviv", ""]]}, {"id": "2102.06120", "submitter": "Man M. Ho", "authors": "Man M. Ho, Jinjia Zhou", "title": "Deep Photo Scan: Semi-supervised learning for dealing with the\n  real-world degradation in smartphone photo scanning", "comments": "Our work is available at https://minhmanho.github.io/dpscan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical photographs now can be conveniently scanned by smartphones and\nstored forever as a digital version, but the scanned photos are not restored\nwell. One solution is to train a supervised deep neural network on many digital\nphotos and the corresponding scanned photos. However, human annotation costs a\nhuge resource leading to limited training data. Previous works create training\npairs by simulating degradation using image processing techniques. Their\nsynthetic images are formed with perfectly scanned photos in latent space. Even\nso, the real-world degradation in smartphone photo scanning remains unsolved\nsince it is more complicated due to real lens defocus, lighting conditions,\nlosing details via printing, various photo materials, and more. To solve these\nproblems, we propose a Deep Photo Scan (DPScan) based on semi-supervised\nlearning. First, we present the way to produce real-world degradation and\nprovide the DIV2K-SCAN dataset for smartphone-scanned photo restoration.\nSecond, by using DIV2K-SCAN, we adopt the concept of Generative Adversarial\nNetworks to learn how to degrade a high-quality image as if it were scanned by\na real smartphone, then generate pseudo-scanned photos for unscanned photos.\nFinally, we propose to train on the scanned and pseudo-scanned photos\nrepresenting a semi-supervised approach with a cycle process as: high-quality\nimages --> real-/pseudo-scanned photos --> reconstructed images. The proposed\nsemi-supervised scheme can balance between supervised and unsupervised errors\nwhile optimizing to limit imperfect pseudo inputs but still enhance\nrestoration. As a result, the proposed DPScan quantitatively and qualitatively\noutperforms its baseline architecture, state-of-the-art academic research, and\nindustrial products in smartphone photo scanning.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 17:00:57 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Ho", "Man M.", ""], ["Zhou", "Jinjia", ""]]}, {"id": "2102.06164", "submitter": "Roberto Vega", "authors": "Roberto Vega, Pouneh Gorji, Zichen Zhang, Xuebin Qin, Abhilash\n  Rakkunedeth Hareendranathan, Jeevesh Kapur, Jacob L. Jaremko, Russell Greiner", "title": "Sample Efficient Learning of Image-Based Diagnostic Classifiers Using\n  Probabilistic Labels", "comments": "To appear in the Proceedings of the 24 th International Conference on\n  Artificial Intelligence and Statistics (AISTATS) 2021, San Diego,California,\n  USA. PMLR: Volume 130", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning approaches often require huge datasets to achieve good\ngeneralization. This complicates its use in tasks like image-based medical\ndiagnosis, where the small training datasets are usually insufficient to learn\nappropriate data representations. For such sensitive tasks it is also important\nto provide the confidence in the predictions. Here, we propose a way to learn\nand use probabilistic labels to train accurate and calibrated deep networks\nfrom relatively small datasets. We observe gains of up to 22% in the accuracy\nof models trained with these labels, as compared with traditional approaches,\nin three classification tasks: diagnosis of hip dysplasia, fatty liver, and\nglaucoma. The outputs of models trained with probabilistic labels are\ncalibrated, allowing the interpretation of its predictions as proper\nprobabilities. We anticipate this approach will apply to other tasks where few\ntraining instances are available and expert knowledge can be encoded as\nprobabilities.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 18:13:56 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Vega", "Roberto", ""], ["Gorji", "Pouneh", ""], ["Zhang", "Zichen", ""], ["Qin", "Xuebin", ""], ["Hareendranathan", "Abhilash Rakkunedeth", ""], ["Kapur", "Jeevesh", ""], ["Jaremko", "Jacob L.", ""], ["Greiner", "Russell", ""]]}, {"id": "2102.06169", "submitter": "Md. Kamrul Hasan", "authors": "Md. Kamrul Hasan, Md. Tasnim Jawad, Kazi Nasim Imtiaz Hasan, Sajal\n  Basak Partha, Md. Masum Al Masba, Shumit Saha", "title": "COVID-19 identification from volumetric chest CT scans using a\n  progressively resized 3D-CNN incorporating segmentation, augmentation, and\n  class-rebalancing", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The novel COVID-19 is a global pandemic disease overgrowing worldwide.\nComputer-aided screening tools with greater sensitivity is imperative for\ndisease diagnosis and prognosis as early as possible. It also can be a helpful\ntool in triage for testing and clinical supervision of COVID-19 patients.\nHowever, designing such an automated tool from non-invasive radiographic images\nis challenging as many manually annotated datasets are not publicly available\nyet, which is the essential core requirement of supervised learning schemes.\nThis article proposes a 3D Convolutional Neural Network (CNN)-based\nclassification approach considering both the inter- and intra-slice spatial\nvoxel information. The proposed system is trained in an end-to-end manner on\nthe 3D patches from the whole volumetric CT images to enlarge the number of\ntraining samples, performing the ablation studies on patch size determination.\nWe integrate progressive resizing, segmentation, augmentations, and\nclass-rebalancing to our 3D network. The segmentation is a critical\nprerequisite step for COVID-19 diagnosis enabling the classifier to learn\nprominent lung features while excluding the outer lung regions of the CT scans.\nWe evaluate all the extensive experiments on a publicly available dataset,\nnamed MosMed, having binary- and multi-class chest CT image partitions. Our\nexperimental results are very encouraging, yielding areas under the ROC curve\nof 0.914 and 0.893 for the binary- and multi-class tasks, respectively,\napplying 5-fold cross-validations. Our method's promising results delegate it\nas a favorable aiding tool for clinical practitioners and radiologists to\nassess COVID-19.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 18:16:18 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 17:47:21 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Hasan", "Md. Kamrul", ""], ["Jawad", "Md. Tasnim", ""], ["Hasan", "Kazi Nasim Imtiaz", ""], ["Partha", "Sajal Basak", ""], ["Masba", "Md. Masum Al", ""], ["Saha", "Shumit", ""]]}, {"id": "2102.06171", "submitter": "Andrew Brock", "authors": "Andrew Brock, Soham De, Samuel L. Smith, Karen Simonyan", "title": "High-Performance Large-Scale Image Recognition Without Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch normalization is a key component of most image classification models,\nbut it has many undesirable properties stemming from its dependence on the\nbatch size and interactions between examples. Although recent work has\nsucceeded in training deep ResNets without normalization layers, these models\ndo not match the test accuracies of the best batch-normalized networks, and are\noften unstable for large learning rates or strong data augmentations. In this\nwork, we develop an adaptive gradient clipping technique which overcomes these\ninstabilities, and design a significantly improved class of Normalizer-Free\nResNets. Our smaller models match the test accuracy of an EfficientNet-B7 on\nImageNet while being up to 8.7x faster to train, and our largest models attain\na new state-of-the-art top-1 accuracy of 86.5%. In addition, Normalizer-Free\nmodels attain significantly better performance than their batch-normalized\ncounterparts when finetuning on ImageNet after large-scale pre-training on a\ndataset of 300 million labeled images, with our best models obtaining an\naccuracy of 89.2%. Our code is available at https://github.com/deepmind/\ndeepmind-research/tree/master/nfnets\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 18:23:20 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Brock", "Andrew", ""], ["De", "Soham", ""], ["Smith", "Samuel L.", ""], ["Simonyan", "Karen", ""]]}, {"id": "2102.06183", "submitter": "Jie Lei", "authors": "Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit\n  Bansal, Jingjing Liu", "title": "Less is More: ClipBERT for Video-and-Language Learning via Sparse\n  Sampling", "comments": "12 pages, 5 figures, 11 tables. - Happy Chinese New Year!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The canonical approach to video-and-language learning (e.g., video question\nanswering) dictates a neural model to learn from offline-extracted dense video\nfeatures from vision models and text features from language models. These\nfeature extractors are trained independently and usually on tasks different\nfrom the target domains, rendering these fixed features sub-optimal for\ndownstream tasks. Moreover, due to the high computational overload of dense\nvideo features, it is often difficult (or infeasible) to plug feature\nextractors directly into existing approaches for easy finetuning. To provide a\nremedy to this dilemma, we propose a generic framework ClipBERT that enables\naffordable end-to-end learning for video-and-language tasks, by employing\nsparse sampling, where only a single or a few sparsely sampled short clips from\na video are used at each training step. Experiments on text-to-video retrieval\nand video question answering on six datasets demonstrate that ClipBERT\noutperforms (or is on par with) existing methods that exploit full-length\nvideos, suggesting that end-to-end learning with just a few sparsely sampled\nclips is often more accurate than using densely extracted offline features from\nfull-length videos, proving the proverbial less-is-more principle. Videos in\nthe datasets are from considerably different domains and lengths, ranging from\n3-second generic domain GIF videos to 180-second YouTube human activity videos,\nshowing the generalization ability of our approach. Comprehensive ablation\nstudies and thorough analyses are provided to dissect what factors lead to this\nsuccess. Our code is publicly available at https://github.com/jayleicn/ClipBERT\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 18:50:16 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Lei", "Jie", ""], ["Li", "Linjie", ""], ["Zhou", "Luowei", ""], ["Gan", "Zhe", ""], ["Berg", "Tamara L.", ""], ["Bansal", "Mohit", ""], ["Liu", "Jingjing", ""]]}, {"id": "2102.06191", "submitter": "Wouter Van Gansbeke", "authors": "Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Luc Van\n  Gool", "title": "Unsupervised Semantic Segmentation by Contrasting Object Mask Proposals", "comments": "Paper and supplementary - Code:\n  https://github.com/wvangansbeke/Unsupervised-Semantic-Segmentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to learn dense semantic representations of images without\nsupervision is an important problem in computer vision. However, despite its\nsignificance, this problem remains rather unexplored, with a few exceptions\nthat considered unsupervised semantic segmentation on small-scale datasets with\na narrow visual domain. In this paper, we make a first attempt to tackle the\nproblem on datasets that have been traditionally utilized for the supervised\ncase. To achieve this, we introduce a two-step framework that adopts a\npredetermined mid-level prior in a contrastive optimization objective to learn\npixel embeddings. This marks a large deviation from existing works that relied\non proxy tasks or end-to-end clustering. Additionally, we argue about the\nimportance of having a prior that contains information about objects, or their\nparts, and discuss several possibilities to obtain such a prior in an\nunsupervised manner.\n  Experimental evaluation shows that our method comes with key advantages over\nexisting works. First, the learned pixel embeddings can be directly clustered\nin semantic groups using K-Means on PASCAL. Under the fully unsupervised\nsetting, there is no precedent in solving the semantic segmentation task on\nsuch a challenging benchmark. Second, our representations can improve over\nstrong baselines when transferred to new datasets, e.g. COCO and DAVIS. The\ncode is available.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 18:54:47 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 20:09:56 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Van Gansbeke", "Wouter", ""], ["Vandenhende", "Simon", ""], ["Georgoulis", "Stamatios", ""], ["Van Gool", "Luc", ""]]}, {"id": "2102.06192", "submitter": "Samet Hicsonmez", "authors": "Samet Hicsonmez, Nermin Samet, Emre Akbas, Pinar Duygulu", "title": "Adversarial Segmentation Loss for Sketch Colorization", "comments": "ICIP 2021 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new method for generating color images from sketches or edge\nmaps. Current methods either require some form of additional user-guidance or\nare limited to the \"paired\" translation approach. We argue that segmentation\ninformation could provide valuable guidance for sketch colorization. To this\nend, we propose to leverage semantic image segmentation, as provided by a\ngeneral purpose panoptic segmentation network, to create an additional\nadversarial loss function. Our loss function can be integrated to any baseline\nGAN model. Our method is not limited to datasets that contain segmentation\nlabels, and it can be trained for \"unpaired\" translation tasks. We show the\neffectiveness of our method on four different datasets spanning scene level\nindoor, outdoor, and children book illustration images using qualitative,\nquantitative and user study analysis. Our model improves its baseline up to 35\npoints on the FID metric. Our code and pretrained models can be found at\nhttps://github.com/giddyyupp/AdvSegLoss.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 18:54:56 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 16:13:26 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Hicsonmez", "Samet", ""], ["Samet", "Nermin", ""], ["Akbas", "Emre", ""], ["Duygulu", "Pinar", ""]]}, {"id": "2102.06195", "submitter": "Yufei Ye", "authors": "Yufei Ye, Shubham Tulsiani, Abhinav Gupta", "title": "Shelf-Supervised Mesh Prediction in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We aim to infer 3D shape and pose of object from a single image and propose a\nlearning-based approach that can train from unstructured image collections,\nsupervised by only segmentation outputs from off-the-shelf recognition systems\n(i.e. 'shelf-supervised'). We first infer a volumetric representation in a\ncanonical frame, along with the camera pose. We enforce the representation\ngeometrically consistent with both appearance and masks, and also that the\nsynthesized novel views are indistinguishable from image collections. The\ncoarse volumetric prediction is then converted to a mesh-based representation,\nwhich is further refined in the predicted camera frame. These two steps allow\nboth shape-pose factorization from image collections and per-instance\nreconstruction in finer details. We examine the method on both synthetic and\nreal-world datasets and demonstrate its scalability on 50 categories in the\nwild, an order of magnitude more classes than existing works.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 18:57:10 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Ye", "Yufei", ""], ["Tulsiani", "Shubham", ""], ["Gupta", "Abhinav", ""]]}, {"id": "2102.06199", "submitter": "Shih-Yang Su", "authors": "Shih-Yang Su, Frank Yu, Michael Zollhoefer and Helge Rhodin", "title": "A-NeRF: Surface-free Human 3D Pose Refinement via Neural Rendering", "comments": "Project website:\n  https://lemonatsu.github.io/ANeRF-Surface-free-Pose-Refinement/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning has reshaped the classical motion capture pipeline,\ngenerative, analysis-by-synthesis elements are still in use to recover fine\ndetails if a high-quality 3D model of the user is available. Unfortunately,\nobtaining such a model for every user a priori is challenging, time-consuming,\nand limits the application scenarios. We propose a novel test-time optimization\napproach for monocular motion capture that learns a volumetric body model of\nthe user in a self-supervised manner. To this end, our approach combines the\nadvantages of neural radiance fields with an articulated skeleton\nrepresentation. Our proposed skeleton embedding serves as a common reference\nthat links constraints across time, thereby reducing the number of required\ncamera views from traditionally dozens of calibrated cameras, down to a single\nuncalibrated one. As a starting point, we employ the output of an off-the-shelf\nmodel that predicts the 3D skeleton pose. The volumetric body shape and\nappearance is then learned from scratch, while jointly refining the initial\npose estimate. Our approach is self-supervised and does not require any\nadditional ground truth labels for appearance, pose, or 3D shape. We\ndemonstrate that our novel combination of a discriminative pose estimation\ntechnique with surface-free analysis-by-synthesis outperforms purely\ndiscriminative monocular pose estimation approaches and generalizes well to\nmultiple views.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 18:58:31 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Su", "Shih-Yang", ""], ["Yu", "Frank", ""], ["Zollhoefer", "Michael", ""], ["Rhodin", "Helge", ""]]}, {"id": "2102.06205", "submitter": "Yu-Lun Liu", "authors": "Yu-Lun Liu, Wei-Sheng Lai, Ming-Hsuan Yang, Yung-Yu Chuang, Jia-Bin\n  Huang", "title": "Hybrid Neural Fusion for Full-frame Video Stabilization", "comments": "Project page: https://alex04072000.github.io/FuSta/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing video stabilization methods often generate visible distortion or\nrequire aggressive cropping of frame boundaries, resulting in smaller field of\nviews. In this work, we present a frame synthesis algorithm to achieve\nfull-frame video stabilization. We first estimate dense warp fields from\nneighboring frames and then synthesize the stabilized frame by fusing the\nwarped contents. Our core technical novelty lies in the learning-based\nhybrid-space fusion that alleviates artifacts caused by optical flow inaccuracy\nand fast-moving objects. We validate the effectiveness of our method on the\nNUS, selfie, and DeepStab video datasets. Extensive experiment results\ndemonstrate the merits of our approach over prior video stabilization methods.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 18:59:45 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 20:32:36 GMT"}, {"version": "v3", "created": "Sat, 27 Mar 2021 13:58:21 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Liu", "Yu-Lun", ""], ["Lai", "Wei-Sheng", ""], ["Yang", "Ming-Hsuan", ""], ["Chuang", "Yung-Yu", ""], ["Huang", "Jia-Bin", ""]]}, {"id": "2102.06260", "submitter": "Lucas Kruitwagen", "authors": "Lucas Kruitwagen", "title": "Towards DeepSentinel: An extensible corpus of labelled Sentinel-1 and -2\n  imagery and a general-purpose sensor-fusion semantic embedding model", "comments": "Proposal presented at NeurIPS 2020 Climate Change AI Workshop and ESA\n  Phi-Week. Copernicus Masters finalist. 14 pages; 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Earth observation offers new insight into anthropogenic changes to nature,\nand how these changes are effecting (and are effected by) the built environment\nand the real economy. With the global availability of medium-resolution\n(10-30m) synthetic aperture radar (SAR) Sentinel-1 and multispectral Sentinel-2\nimagery, machine learning can be employed to offer these insights at scale,\nunbiased to the reporting of companies and countries. In this paper, I\nintroduce DeepSentinel, a data pipeline and experimentation framework for\nproducing general-purpose semantic embeddings of paired Sentinel-1 and\nSentinel-2 imagery. I document the development of an extensible corpus of\nlabelled and unlabelled imagery for the purposes of sensor fusion research.\nWith this new dataset I develop a set of experiments applying popular\nself-supervision methods and encoder architectures to a land cover\nclassification problem. Tile2vec spatial encoding with a self-attention enabled\nResNet model outperforms deeper ResNet variants as well as pretraining with\nvariational autoencoding and contrastive loss. All supporting and derived data\nand code are made publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 20:33:47 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Kruitwagen", "Lucas", ""]]}, {"id": "2102.06285", "submitter": "Shruti Jadon", "authors": "Shruti Jadon", "title": "COVID-19 detection from scarce chest x-ray image data using few-shot\n  deep learning approach", "comments": "10 pages, 5 figures. Proceedings Volume 11601, Medical Imaging 2021:\n  Imaging Informatics for Healthcare, Research, and Applications", "journal-ref": null, "doi": "10.1117/12.2581496", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the current COVID-19 pandemic situation, there is an urgent need to screen\ninfected patients quickly and accurately. Using deep learning models trained on\nchest X-ray images can become an efficient method for screening COVID-19\npatients in these situations. Deep learning approaches are already widely used\nin the medical community. However, they require a large amount of data to be\naccurate. The open-source community collectively has made efforts to collect\nand annotate the data, but it is not enough to train an accurate deep learning\nmodel. Few-shot learning is a sub-field of machine learning that aims to learn\nthe objective with less amount of data. In this work, we have experimented with\nwell-known solutions for data scarcity in deep learning to detect COVID-19.\nThese include data augmentation, transfer learning, and few-shot learning, and\nunsupervised learning. We have also proposed a custom few-shot learning\napproach to detect COVID-19 using siamese networks. Our experimental results\nshowcased that we can implement an efficient and accurate deep learning model\nfor COVID-19 detection by adopting the few-shot learning approaches even with\nless amount of data. Using our proposed approach we were able to achieve 96.4%\naccuracy an improvement from 83% using baseline models.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 22:06:03 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 17:53:40 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Jadon", "Shruti", ""]]}, {"id": "2102.06288", "submitter": "Taewoo Kim", "authors": "Taewoo Kim, Chaeyeon Chung, Sunghyun Park, Gyojung Gu, Keonmin Nam,\n  Wonzo Choe, Jaesung Lee, Jaegul Choo", "title": "K-Hairstyle: A Large-scale Korean hairstyle dataset for virtual hair\n  editing and hairstyle classification", "comments": "hair dataset, classification, segmentation, hair dyeing, hairstyle\n  translation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hair and beauty industry is one of the fastest growing industries. This\nled to the development of various applications, such as virtual hair dyeing or\nhairstyle translations, to satisfy the need of the customers. Although there\nare several public hair datasets available for these applications, they consist\nof limited number of images with low resolution, which restrict their\nperformance on high-quality hair editing. Therefore, we introduce a novel\nlarge-scale Korean hairstyle dataset, K-hairstyle, 256,679 with high-resolution\nimages. In addition, K-hairstyle contains various hair attributes annotated by\nKorean expert hair stylists and hair segmentation masks. We validate the\neffectiveness of our dataset by leveraging several applications, such as\nhairstyle translation, and hair classification and hair retrieval. Furthermore,\nwe will release K-hairstyle soon.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 22:20:05 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Kim", "Taewoo", ""], ["Chung", "Chaeyeon", ""], ["Park", "Sunghyun", ""], ["Gu", "Gyojung", ""], ["Nam", "Keonmin", ""], ["Choe", "Wonzo", ""], ["Lee", "Jaesung", ""], ["Choo", "Jaegul", ""]]}, {"id": "2102.06307", "submitter": "Damien Garreau", "authors": "Damien Garreau, Dina Mardaoui", "title": "What does LIME really see in images?", "comments": "30 pages, 13 figures, accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of modern algorithms on certain computer vision tasks such as\nobject recognition is now close to that of humans. This success was achieved at\nthe price of complicated architectures depending on millions of parameters and\nit has become quite challenging to understand how particular predictions are\nmade. Interpretability methods propose to give us this understanding. In this\npaper, we study LIME, perhaps one of the most popular. On the theoretical side,\nwe show that when the number of generated examples is large, LIME explanations\nare concentrated around a limit explanation for which we give an explicit\nexpression. We further this study for elementary shape detectors and linear\nmodels. As a consequence of this analysis, we uncover a connection between LIME\nand integrated gradients, another explanation method. More precisely, the LIME\nexplanations are similar to the sum of integrated gradients over the\nsuperpixels used in the preprocessing step of LIME.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 23:18:53 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 09:08:34 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Garreau", "Damien", ""], ["Mardaoui", "Dina", ""]]}, {"id": "2102.06315", "submitter": "Mengwei Ren", "authors": "Mengwei Ren, Neel Dey, James Fishbaugh, Guido Gerig", "title": "Segmentation-Renormalized Deep Feature Modulation for Unpaired Image\n  Harmonization", "comments": "Accepted by IEEE Transactions on Medical Imaging. Code available at\n  https://github.com/mengweiren/segmentation-renormalized-harmonization", "journal-ref": null, "doi": "10.1109/TMI.2021.3059726", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks are now ubiquitous in large-scale multi-center imaging studies.\nHowever, the direct aggregation of images across sites is contraindicated for\ndownstream statistical and deep learning-based image analysis due to\ninconsistent contrast, resolution, and noise. To this end, in the absence of\npaired data, variations of Cycle-consistent Generative Adversarial Networks\nhave been used to harmonize image sets between a source and target domain.\nImportantly, these methods are prone to instability, contrast inversion,\nintractable manipulation of pathology, and steganographic mappings which limit\ntheir reliable adoption in real-world medical imaging. In this work, based on\nan underlying assumption that morphological shape is consistent across imaging\nsites, we propose a segmentation-renormalized image translation framework to\nreduce inter-scanner heterogeneity while preserving anatomical layout. We\nreplace the affine transformations used in the normalization layers within\ngenerative networks with trainable scale and shift parameters conditioned on\njointly learned anatomical segmentation embeddings to modulate features at\nevery level of translation. We evaluate our methodologies against recent\nbaselines across several imaging modalities (T1w MRI, FLAIR MRI, and OCT) on\ndatasets with and without lesions. Segmentation-renormalization for translation\nGANs yields superior image harmonization as quantified by Inception distances,\ndemonstrates improved downstream utility via post-hoc segmentation accuracy,\nand improved robustness to translation perturbation and self-adversarial\nattacks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 23:53:51 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 00:36:50 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Ren", "Mengwei", ""], ["Dey", "Neel", ""], ["Fishbaugh", "James", ""], ["Gerig", "Guido", ""]]}, {"id": "2102.06328", "submitter": "Trung Quang Tran", "authors": "Trung Quang Tran, Mingu Kang, Daeyoung Kim", "title": "ReRankMatch: Semi-Supervised Learning with Semantics-Oriented Similarity\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes integrating semantics-oriented similarity representation\ninto RankingMatch, a recently proposed semi-supervised learning method. Our\nmethod, dubbed ReRankMatch, aims to deal with the case in which labeled and\nunlabeled data share non-overlapping categories. ReRankMatch encourages the\nmodel to produce the similar image representations for the samples likely\nbelonging to the same class. We evaluate our method on various datasets such as\nCIFAR-10, CIFAR-100, SVHN, STL-10, and Tiny ImageNet. We obtain promising\nresults (4.21% error rate on CIFAR-10 with 4000 labels, 22.32% error rate on\nCIFAR-100 with 10000 labels, and 2.19% error rate on SVHN with 1000 labels)\nwhen the amount of labeled data is sufficient to learn semantics-oriented\nsimilarity representation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 02:20:47 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Tran", "Trung Quang", ""], ["Kang", "Mingu", ""], ["Kim", "Daeyoung", ""]]}, {"id": "2102.06358", "submitter": "Ye Luo", "authors": "Ye Luo and Shiqing Fan", "title": "Min-Max-Plus Neural Networks", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.LG math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a new model of neural networks called Min-Max-Plus Neural Networks\n(MMP-NNs) based on operations in tropical arithmetic. In general, an MMP-NN is\ncomposed of three types of alternately stacked layers, namely linear layers,\nmin-plus layers and max-plus layers. Specifically, the latter two types of\nlayers constitute the nonlinear part of the network which is trainable and more\nsophisticated compared to the nonlinear part of conventional neural networks.\nIn addition, we show that with higher capability of nonlinearity expression,\nMMP-NNs are universal approximators of continuous functions, even when the\nnumber of multiplication operations is tremendously reduced (possibly to none\nin certain extreme cases). Furthermore, we formulate the backpropagation\nalgorithm in the training process of MMP-NNs and introduce an algorithm of\nnormalization to improve the rate of convergence in training.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 06:09:20 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Luo", "Ye", ""], ["Fan", "Shiqing", ""]]}, {"id": "2102.06366", "submitter": "Sahaj Garg", "authors": "Sahaj Garg, Anirudh Jain, Joe Lou, Mitchell Nahmias", "title": "Confounding Tradeoffs for Neural Network Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many neural network quantization techniques have been developed to decrease\nthe computational and memory footprint of deep learning. However, these methods\nare evaluated subject to confounding tradeoffs that may affect inference\nacceleration or resource complexity in exchange for higher accuracy. In this\nwork, we articulate a variety of tradeoffs whose impact is often overlooked and\nempirically analyze their impact on uniform and mixed-precision post-training\nquantization, finding that these confounding tradeoffs may have a larger impact\non quantized network accuracy than the actual quantization methods themselves.\nBecause these tradeoffs constrain the attainable hardware acceleration for\ndifferent use-cases, we encourage researchers to explicitly report these design\nchoices through the structure of \"quantization cards.\" We expect quantization\ncards to help researchers compare methods more effectively and engineers\ndetermine the applicability of quantization techniques for their hardware.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 06:58:08 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Garg", "Sahaj", ""], ["Jain", "Anirudh", ""], ["Lou", "Joe", ""], ["Nahmias", "Mitchell", ""]]}, {"id": "2102.06386", "submitter": "Shigemichi Matsuzaki", "authors": "Shigemichi Matsuzaki, Jun Miura and Hiroaki Masuzawa", "title": "Multi-source Pseudo-label Learning of Semantic Segmentation for the\n  Scene Recognition of Agricultural Mobile Robots", "comments": "10 pages, 7 figures, submitted to Machine Vision And Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes a novel method of training a semantic segmentation model\nfor environment recognition of agricultural mobile robots by unsupervised\ndomain adaptation exploiting publicly available datasets of outdoor scenes that\nare different from our target environments i.e., greenhouses. In conventional\nsemantic segmentation methods, the labels are given by manual annotation, which\nis a tedious and time-consuming task. A method to work around the necessity of\nthe manual annotation is unsupervised domain adaptation (UDA) that transfer\nknowledge from labeled source datasets to unlabeled target datasets. Most of\nthe UDA methods of semantic segmentation are validated by tasks of adaptation\nfrom non-photorealistic synthetic images of urban scenes to real ones. However,\nthe effectiveness of the methods is not well studied in the case of adaptation\nto other types of environments, such as greenhouses. In addition, it is not\nalways possible to prepare appropriate source datasets for such environments.\nIn this paper, we adopt an existing training method of UDA to a task of\ntraining a model for greenhouse images. We propose to use multiple publicly\navailable datasets of outdoor images as source datasets, and also propose a\nsimple yet effective method of generating pseudo-labels by transferring\nknowledge from the source datasets that have different appearance and a label\nset from the target datasets. We demonstrate in experiments that by combining\nour proposed method of pseudo-label generation with the existing training\nmethod, the performance was improved by up to 14.3% of mIoU compared to the\nbest score of the single-source training.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 08:17:10 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Matsuzaki", "Shigemichi", ""], ["Miura", "Jun", ""], ["Masuzawa", "Hiroaki", ""]]}, {"id": "2102.06388", "submitter": "Roohallah Alizadehsani", "authors": "Roohallah Alizadehsani, Danial Sharifrazi, Navid Hoseini Izadi, Javad\n  Hassannataj Joloudari, Afshin Shoeibi, Juan M. Gorriz, Sadiq Hussain, Juan E.\n  Arco, Zahra Alizadeh Sani, Fahime Khozeimeh, Abbas Khosravi, Saeid Nahavandi,\n  Sheikh Mohammed Shariful Islam, U Rajendra Acharya", "title": "Uncertainty-Aware Semi-supervised Method using Large Unlabelled and\n  Limited Labeled COVID-19 Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new coronavirus has caused more than 1 million deaths and continues to\nspread rapidly. This virus targets the lungs, causing respiratory distress\nwhich can be mild or severe. The X-ray or computed tomography (CT) images of\nlungs can reveal whether the patient is infected with COVID-19 or not. Many\nresearchers are trying to improve COVID-19 detection using artificial\nintelligence. In this paper, relying on Generative Adversarial Networks (GAN),\nwe propose a Semi-supervised Classification using Limited Labelled Data (SCLLD)\nfor automated COVID-19 detection. Our motivation is to develop learning method\nwhich can cope with scenarios that preparing labelled data is time consuming or\nexpensive. We further improved the detection accuracy of the proposed method by\napplying Sobel edge detection. The GAN discriminator output is a probability\nvalue which is used for classification in this work. The proposed system is\ntrained using 10,000 CT scans collected from Omid hospital. Also, we validate\nour system using the public dataset. The proposed method is compared with other\nstate of the art supervised methods such as Gaussian processes. To the best of\nour knowledge, this is the first time a COVID-19 semi-supervised detection\nmethod is presented. Our method is capable of learning from a mixture of\nlimited labelled and unlabelled data where supervised learners fail due to lack\nof sufficient amount of labelled data. Our semi-supervised training method\nsignificantly outperforms the supervised training of Convolutional Neural\nNetwork (CNN) in case labelled training data is scarce. Our method has achieved\nan accuracy of 99.60%, sensitivity of 99.39%, and specificity of 99.80% where\nCNN (trained supervised) has achieved an accuracy of 69.87%, sensitivity of\n94%, and specificity of 46.40%.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 08:20:20 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Alizadehsani", "Roohallah", ""], ["Sharifrazi", "Danial", ""], ["Izadi", "Navid Hoseini", ""], ["Joloudari", "Javad Hassannataj", ""], ["Shoeibi", "Afshin", ""], ["Gorriz", "Juan M.", ""], ["Hussain", "Sadiq", ""], ["Arco", "Juan E.", ""], ["Sani", "Zahra Alizadeh", ""], ["Khozeimeh", "Fahime", ""], ["Khosravi", "Abbas", ""], ["Nahavandi", "Saeid", ""], ["Islam", "Sheikh Mohammed Shariful", ""], ["Acharya", "U Rajendra", ""]]}, {"id": "2102.06406", "submitter": "Nikolay Dagaev", "authors": "Nikolay Dagaev, Brett D. Roads, Xiaoliang Luo, Daniel N. Barry,\n  Kaustubh R. Patil, Bradley C. Love", "title": "A Too-Good-to-be-True Prior to Reduce Shortcut Reliance", "comments": "14 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their impressive performance in object recognition and other tasks\nunder standard testing conditions, deep networks often fail to generalize to\nout-of-distribution (o.o.d.) samples. One cause for this shortcoming is that\nmodern architectures tend to rely on \"shortcuts\" - superficial features that\ncorrelate with categories without capturing deeper invariants that hold across\ncontexts. Real-world concepts often possess a complex structure that can vary\nsuperficially across contexts, which can make the most intuitive and promising\nsolutions in one context not generalize to others. One potential way to improve\no.o.d. generalization is to assume simple solutions are unlikely to be valid\nacross contexts and avoid them, which we refer to as the too-good-to-be-true\nprior. A low-capacity network (LCN) with a shallow architecture should only be\nable to learn surface relationships, including shortcuts. We find that LCNs can\nserve as shortcut detectors. Furthermore, an LCN's predictions can be used in a\ntwo-stage approach to encourage a high-capacity network (HCN) to rely on deeper\ninvariant features that should generalize broadly. In particular, items that\nthe LCN can master are downweighted when training the HCN. Using a modified\nversion of the CIFAR-10 dataset in which we introduced shortcuts, we found that\nthe two-stage LCN-HCN approach reduced reliance on shortcuts and facilitated\no.o.d. generalization.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 09:17:24 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 10:49:25 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Dagaev", "Nikolay", ""], ["Roads", "Brett D.", ""], ["Luo", "Xiaoliang", ""], ["Barry", "Daniel N.", ""], ["Patil", "Kaustubh R.", ""], ["Love", "Bradley C.", ""]]}, {"id": "2102.06407", "submitter": "Tanveer Hussain Mr.", "authors": "Tanveer Hussain, Saeed Anwar, Amin Ullah, Khan Muhammad, and Sung Wook\n  Baik", "title": "Densely Deformable Efficient Salient Object Detection Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Salient Object Detection (SOD) domain using RGB-D data has lately emerged\nwith some current models' adequately precise results. However, they have\nrestrained generalization abilities and intensive computational complexity. In\nthis paper, inspired by the best background/foreground separation abilities of\ndeformable convolutions, we employ them in our Densely Deformable Network\n(DDNet) to achieve efficient SOD. The salient regions from densely deformable\nconvolutions are further refined using transposed convolutions to optimally\ngenerate the saliency maps. Quantitative and qualitative evaluations using the\nrecent SOD dataset against 22 competing techniques show our method's efficiency\nand effectiveness. We also offer evaluation using our own created\ncross-dataset, surveillance-SOD (S-SOD), to check the trained models' validity\nin terms of their applicability in diverse scenarios. The results indicate that\nthe current models have limited generalization potentials, demanding further\nresearch in this direction. Our code and new dataset will be publicly available\nat https://github.com/tanveer-hussain/EfficientSOD\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 09:17:38 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Hussain", "Tanveer", ""], ["Anwar", "Saeed", ""], ["Ullah", "Amin", ""], ["Muhammad", "Khan", ""], ["Baik", "Sung Wook", ""]]}, {"id": "2102.06448", "submitter": "Haoran Chen", "authors": "Haoran Chen, Jianmin Li, Simone Frintrop, Xiaolin Hu", "title": "Annotation Cleaning for the MSR-Video to Text Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The video captioning task is to describe the video contents with natural\nlanguage by the machine. Many methods have been proposed for solving this task.\nA large dataset called MSR Video to Text (MSR-VTT) is often used as the\nbenckmark dataset for testing the performance of the methods. However, we found\nthat the human annotations, i.e., the descriptions of video contents in the\ndataset are quite noisy, e.g., there are many duplicate captions and many\ncaptions contain grammatical problems. These problems may pose difficulties to\nvideo captioning models for learning. We cleaned the MSR-VTT annotations by\nremoving these problems, then tested several typical video captioning models on\nthe cleaned dataset. Experimental results showed that data cleaning boosted the\nperformances of the models measured by popular quantitative metrics. We\nrecruited subjects to evaluate the results of a model trained on the original\nand cleaned datasets. The human behavior experiment demonstrated that trained\non the cleaned dataset, the model generated captions that were more coherent\nand more relevant to contents of the video clips. The cleaned dataset is\npublicly available.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 11:14:56 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 04:22:49 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Chen", "Haoran", ""], ["Li", "Jianmin", ""], ["Frintrop", "Simone", ""], ["Hu", "Xiaolin", ""]]}, {"id": "2102.06479", "submitter": "Philipp Benz", "authors": "Chaoning Zhang, Philipp Benz, Adil Karjauv, In So Kweon", "title": "Universal Adversarial Perturbations Through the Lens of Deep\n  Steganography: Towards A Fourier Perspective", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The booming interest in adversarial attacks stems from a misalignment between\nhuman vision and a deep neural network (DNN), i.e. a human imperceptible\nperturbation fools the DNN. Moreover, a single perturbation, often called\nuniversal adversarial perturbation (UAP), can be generated to fool the DNN for\nmost images. A similar misalignment phenomenon has recently also been observed\nin the deep steganography task, where a decoder network can retrieve a secret\nimage back from a slightly perturbed cover image. We attempt explaining the\nsuccess of both in a unified manner from the Fourier perspective. We perform\ntask-specific and joint analysis and reveal that (a) frequency is a key factor\nthat influences their performance based on the proposed entropy metric for\nquantifying the frequency distribution; (b) their success can be attributed to\na DNN being highly sensitive to high-frequency content. We also perform feature\nlayer analysis for providing deep insight on model generalization and\nrobustness. Additionally, we propose two new variants of universal\nperturbations: (1) Universal Secret Adversarial Perturbation (USAP) that\nsimultaneously achieves attack and hiding; (2) high-pass UAP (HP-UAP) that is\nless visible to the human eye.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 12:26:39 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Zhang", "Chaoning", ""], ["Benz", "Philipp", ""], ["Karjauv", "Adil", ""], ["Kweon", "In So", ""]]}, {"id": "2102.06496", "submitter": "Christina Runkel", "authors": "Christina Runkel, Christian Etmann, Michael M\\\"oller, Carola-Bibiane\n  Sch\\\"onlieb", "title": "Depthwise Separable Convolutions Allow for Fast and Memory-Efficient\n  Spectral Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing number of models require the control of the spectral norm of\nconvolutional layers of a neural network. While there is an abundance of\nmethods for estimating and enforcing upper bounds on those during training,\nthey are typically costly in either memory or time. In this work, we introduce\na very simple method for spectral normalization of depthwise separable\nconvolutions, which introduces negligible computational and memory overhead. We\ndemonstrate the effectiveness of our method on image classification tasks using\nstandard architectures like MobileNetV2.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 12:55:42 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Runkel", "Christina", ""], ["Etmann", "Christian", ""], ["M\u00f6ller", "Michael", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "2102.06507", "submitter": "Aly Magassouba", "authors": "Aly Magassouba, Komei Sugiura, Angelica Nakayama, Tsubasa Hirakawa,\n  Takayoshi Yamashita, Hironobu Fujiyoshi, Hisashi Kawai", "title": "Predicting and Attending to Damaging Collisions for Placing Everyday\n  Objects in Photo-Realistic Simulations", "comments": "18 pages, 7 figures, 5 tables. Submitted to Advanced Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Placing objects is a fundamental task for domestic service robots (DSRs).\nThus, inferring the collision-risk before a placing motion is crucial for\nachieving the requested task. This problem is particularly challenging because\nit is necessary to predict what happens if an object is placed in a cluttered\ndesignated area. We show that a rule-based approach that uses plane detection,\nto detect free areas, performs poorly. To address this, we develop PonNet,\nwhich has multimodal attention branches and a self-attention mechanism to\npredict damaging collisions, based on RGBD images. Our method can visualize the\nrisk of damaging collisions, which is convenient because it enables the user to\nunderstand the risk. For this purpose, we build and publish an original dataset\nthat contains 12,000 photo-realistic images of specific placing areas, with\ndaily life objects, in home environments. The experimental results show that\nour approach improves accuracy compared with the baseline methods.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 13:21:45 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Magassouba", "Aly", ""], ["Sugiura", "Komei", ""], ["Nakayama", "Angelica", ""], ["Hirakawa", "Tsubasa", ""], ["Yamashita", "Takayoshi", ""], ["Fujiyoshi", "Hironobu", ""], ["Kawai", "Hisashi", ""]]}, {"id": "2102.06515", "submitter": "David Bouget", "authors": "David Bouget, Andr\\'e Pedersen, Johanna Vanel, Haakon O. Leira, Thomas\n  Lang{\\o}", "title": "Mediastinal lymph nodes segmentation using 3D convolutional neural\n  network ensembles and anatomical priors guiding", "comments": "18 pages, 8 figures, submitted to Computer Methods in Biomechanics\n  and Biomedical Engineering: Imaging & Visualization", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As lung cancer evolves, the presence of enlarged and potentially malignant\nlymph nodes must be assessed to properly estimate disease progression and\nselect the best treatment strategy. Following the clinical guidelines,\nestimation of short-axis diameter and mediastinum station are paramount for\ncorrect diagnosis. A method for accurate and automatic segmentation is hence\ndecisive for quantitatively describing lymph nodes. In this study, the use of\n3D convolutional neural networks, either through slab-wise schemes or the\nleveraging of downsampled entire volumes, is investigated. Furthermore, the\npotential impact from simple ensemble strategies is considered. As lymph nodes\nhave similar attenuation values to nearby anatomical structures, we suggest\nusing the knowledge of other organs as prior information to guide the\nsegmentation task. To assess the segmentation and instance detection\nperformances, a 5-fold cross-validation strategy was followed over a dataset of\n120 contrast-enhanced CT volumes. For the 1178 lymph nodes with a short-axis\ndiameter $\\geq10$ mm, our best performing approach reached a patient-wise\nrecall of 92%, a false positive per patient ratio of 5, and a segmentation\noverlap of 80.5%. The method performs similarly well across all stations.\nFusing a slab-wise and a full volume approach within an ensemble scheme\ngenerated the best performances. The anatomical priors guiding strategy is\npromising, yet a larger set than four organs appears needed to generate an\noptimal benefit. A larger dataset is also mandatory, given the wide range of\nexpressions a lymph node can exhibit (i.e., shape, location, and attenuation),\nand contrast uptake variations.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 14:51:34 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Bouget", "David", ""], ["Pedersen", "Andr\u00e9", ""], ["Vanel", "Johanna", ""], ["Leira", "Haakon O.", ""], ["Lang\u00f8", "Thomas", ""]]}, {"id": "2102.06529", "submitter": "David Kadish", "authors": "David Kadish, Sebastian Risi, Anders Sundnes L{\\o}vlie", "title": "Improving Object Detection in Art Images Using Only Style Transfer", "comments": "8 pages, 7 figures, 3 tables, accepted at IJCNN2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite recent advances in object detection using deep learning neural\nnetworks, these neural networks still struggle to identify objects in art\nimages such as paintings and drawings. This challenge is known as the cross\ndepiction problem and it stems in part from the tendency of neural networks to\nprioritize identification of an object's texture over its shape. In this paper\nwe propose and evaluate a process for training neural networks to localize\nobjects - specifically people - in art images. We generate a large dataset for\ntraining and validation by modifying the images in the COCO dataset using AdaIn\nstyle transfer. This dataset is used to fine-tune a Faster R-CNN object\ndetection network, which is then tested on the existing People-Art testing\ndataset. The result is a significant improvement on the state of the art and a\nnew way forward for creating datasets to train neural networks to process art\nimages.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 13:48:46 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 21:58:09 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Kadish", "David", ""], ["Risi", "Sebastian", ""], ["L\u00f8vlie", "Anders Sundnes", ""]]}, {"id": "2102.06535", "submitter": "Zainab Abohashima", "authors": "Essam H. Houssein, Zainab Abohashima, Mohamed Elhoseny, Waleed M.\n  Mohamed", "title": "Hybrid quantum convolutional neural networks model for COVID-19\n  prediction using chest X-Ray images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the great efforts to find an effective way for COVID-19 prediction,\nthe virus nature and mutation represent a critical challenge to diagnose the\ncovered cases. However, developing a model to predict COVID-19 via Chest X-Ray\n(CXR) images with accurate performance is necessary to help in early diagnosis.\nIn this paper, a hybrid quantum-classical convolutional Neural Networks (HQCNN)\nmodel used the random quantum circuits (RQCs) as a base to detect COVID-19\npatients with CXR images. A collection of 6952 CXR images, including 1161\nCOVID-19, 1575 normal, and 5216 pneumonia images, were used as a dataset in\nthis work. The proposed HQCNN model achieved higher performance with an\naccuracy of 98.4\\% and a sensitivity of 99.3\\% on the first dataset cases.\nBesides, it obtained an accuracy of 99\\% and a sensitivity of 99.7\\% on the\nsecond dataset cases. Also, it achieved accuracy, and sensitivity of 88.6\\%,\nand 88.7\\%, respectively, on the third multi-class dataset cases. Furthermore,\nthe HQCNN model outperforms various models in balanced accuracy, precision,\nF1-measure, and AUC-ROC score. The experimental results are achieved by the\nproposed model prove its ability in predicting positive COVID-19 cases.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 18:22:53 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Houssein", "Essam H.", ""], ["Abohashima", "Zainab", ""], ["Elhoseny", "Mohamed", ""], ["Mohamed", "Waleed M.", ""]]}, {"id": "2102.06564", "submitter": "Mehmet Guzel", "authors": "Mustafa Zor, Erkan Bostanci, Mehmet Serdar Guzel, Erinc Karatas", "title": "Analysis of Interpolation based Image In-painting Approaches", "comments": "Image in-painting, Interpolation, Cubic interpolation, Kriging\n  interpolation, Radial based function, High dimensional model representation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpolation and internal painting are one of the basic approaches in image\ninternal painting, which is used to eliminate undesirable parts that occur in\ndigital images or to enhance faulty parts. This study was designed to compare\nthe interpolation algorithms used in image in-painting in the literature.\nErrors and noise generated on the colour and grayscale formats of some of the\ncommonly used standard images in the literature were corrected by using Cubic,\nKriging, Radial based function and High dimensional model representation\napproaches and the results were compared using standard image comparison\ncriteria, namely, PSNR (peak signal-to-noise ratio), SSIM (Structural\nSIMilarity), Mean Square Error (MSE). According to the results obtained from\nthe study, the absolute superiority of the methods against each other was not\nobserved. However, Kriging and RBF interpolation give better results both for\nnumerical data and visual evaluation for image in-painting problems with large\narea losses.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 15:03:41 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Zor", "Mustafa", ""], ["Bostanci", "Erkan", ""], ["Guzel", "Mehmet Serdar", ""], ["Karatas", "Erinc", ""]]}, {"id": "2102.06583", "submitter": "Konstantin Sofiiuk", "authors": "Konstantin Sofiiuk, Ilia A. Petrov and Anton Konushin", "title": "Reviving Iterative Training with Mask Guidance for Interactive\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works on click-based interactive segmentation have demonstrated\nstate-of-the-art results by using various inference-time optimization schemes.\nThese methods are considerably more computationally expensive compared to\nfeedforward approaches, as they require performing backward passes through a\nnetwork during inference and are hard to deploy on mobile frameworks that\nusually support only forward passes. In this paper, we extensively evaluate\nvarious design choices for interactive segmentation and discover that new\nstate-of-the-art results can be obtained without any additional optimization\nschemes. Thus, we propose a simple feedforward model for click-based\ninteractive segmentation that employs the segmentation masks from previous\nsteps. It allows not only to segment an entirely new object, but also to start\nwith an external mask and correct it. When analyzing the performance of models\ntrained on different datasets, we observe that the choice of a training dataset\ngreatly impacts the quality of interactive segmentation. We find that the\nmodels trained on a combination of COCO and LVIS with diverse and high-quality\nannotations show performance superior to all existing models. The code and\ntrained models are available at\nhttps://github.com/saic-vul/ritm_interactive_segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 15:44:31 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Sofiiuk", "Konstantin", ""], ["Petrov", "Ilia A.", ""], ["Konushin", "Anton", ""]]}, {"id": "2102.06591", "submitter": "Ye Yu", "authors": "Ye Yu and William A. P. Smith", "title": "Outdoor inverse rendering from a single image using multiview\n  self-supervision", "comments": "arXiv admin note: substantial text overlap with arXiv:1811.12328", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show how to perform scene-level inverse rendering to recover\nshape, reflectance and lighting from a single, uncontrolled image using a fully\nconvolutional neural network. The network takes an RGB image as input,\nregresses albedo, shadow and normal maps from which we infer least squares\noptimal spherical harmonic lighting coefficients. Our network is trained using\nlarge uncontrolled multiview and timelapse image collections without ground\ntruth. By incorporating a differentiable renderer, our network can learn from\nself-supervision. Since the problem is ill-posed we introduce additional\nsupervision. Our key insight is to perform offline multiview stereo (MVS) on\nimages containing rich illumination variation. From the MVS pose and depth\nmaps, we can cross project between overlapping views such that Siamese training\ncan be used to ensure consistent estimation of photometric invariants. MVS\ndepth also provides direct coarse supervision for normal map estimation. We\nbelieve this is the first attempt to use MVS supervision for learning inverse\nrendering. In addition, we learn a statistical natural illumination prior. We\nevaluate performance on inverse rendering, normal map estimation and intrinsic\nimage decomposition benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 16:01:18 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Yu", "Ye", ""], ["Smith", "William A. P.", ""]]}, {"id": "2102.06603", "submitter": "James O' Neill", "authors": "James O' Neill, Danushka Bollegala", "title": "Semantically-Conditioned Negative Samples for Efficient Contrastive\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Negative sampling is a limiting factor w.r.t. the generalization of\nmetric-learned neural networks. We show that uniform negative sampling provides\nlittle information about the class boundaries and thus propose three novel\ntechniques for efficient negative sampling: drawing negative samples from (1)\nthe top-$k$ most semantically similar classes, (2) the top-$k$ most\nsemantically similar samples and (3) interpolating between contrastive latent\nrepresentations to create pseudo negatives. Our experiments on CIFAR-10,\nCIFAR-100 and Tiny-ImageNet-200 show that our proposed \\textit{Semantically\nConditioned Negative Sampling} and Latent Mixup lead to consistent performance\nimprovements. In the standard supervised learning setting, on average we\nincrease test accuracy by 1.52\\% percentage points on CIFAR-10 across various\nnetwork architectures. In the knowledge distillation setting, (1) the\nperformance of student networks increase by 4.56\\% percentage points on\nTiny-ImageNet-200 and 3.29\\% on CIFAR-100 over student networks trained with no\nteacher and (2) 1.23\\% and 1.72\\% respectively over a \\textit{hard-to-beat}\nbaseline (Hinton et al., 2015).\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 16:26:52 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Neill", "James O'", ""], ["Bollegala", "Danushka", ""]]}, {"id": "2102.06605", "submitter": "Yifan Zhang", "authors": "Yifan Zhang, Bryan Hooi, Dapeng Hu, Jian Liang, Jiashi Feng", "title": "Unleashing the Power of Contrastive Self-Supervised Visual Models via\n  Contrast-Regularized Fine-Tuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive self-supervised learning (CSL) leverages unlabeled data to train\nmodels that provide instance-discriminative visual representations uniformly\nscattered in the feature space. In deployment, the common practice is to\ndirectly fine-tune models with the cross-entropy loss, which however may not be\nan optimal strategy. Although cross-entropy tends to separate inter-class\nfeatures, the resulted models still have limited capability of reducing\nintra-class feature scattering that inherits from pre-training, and thus may\nsuffer unsatisfactory performance on downstream tasks. In this paper, we\ninvestigate whether applying contrastive learning to fine-tuning would bring\nfurther benefits, and analytically find that optimizing the supervised\ncontrastive loss benefits both class-discriminative representation learning and\nmodel optimization during fine-tuning. Inspired by these findings, we propose\nContrast-regularized tuning (Core-tuning), a novel approach for fine-tuning\ncontrastive self-supervised visual models. Instead of simply adding the\ncontrastive loss to the objective of fine-tuning, Core-tuning also generates\nhard sample pairs for more effective contrastive learning through a novel\nfeature mixup strategy, as well as improves the generalizability of the model\nby smoothing the decision boundary via mixed samples. Extensive experiments on\nimage classification and semantic segmentation verify the effectiveness of\nCore-tuning.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 16:31:24 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Zhang", "Yifan", ""], ["Hooi", "Bryan", ""], ["Hu", "Dapeng", ""], ["Liang", "Jian", ""], ["Feng", "Jiashi", ""]]}, {"id": "2102.06624", "submitter": "Mohamed Abderrahmen Abid", "authors": "Mohamed Abderrahmen Abid, Ihsen Hedhli, Christian Gagn\\'e", "title": "A Generative Model for Hallucinating Diverse Versions of Super\n  Resolution Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditionally, the main focus of image super-resolution techniques is on\nrecovering the most likely high-quality images from low-quality images, using a\none-to-one low- to high-resolution mapping. Proceeding that way, we ignore the\nfact that there are generally many valid versions of high-resolution images\nthat map to a given low-resolution image. We are tackling in this work the\nproblem of obtaining different high-resolution versions from the same\nlow-resolution image using Generative Adversarial Models. Our learning approach\nmakes use of high frequencies available in the training high-resolution images\nfor preserving and exploring in an unsupervised manner the structural\ninformation available within these images. Experimental results on the CelebA\ndataset confirm the effectiveness of the proposed method, which allows the\ngeneration of both realistic and diverse high-resolution images from\nlow-resolution images.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 17:11:42 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Abid", "Mohamed Abderrahmen", ""], ["Hedhli", "Ihsen", ""], ["Gagn\u00e9", "Christian", ""]]}, {"id": "2102.06650", "submitter": "Seong Jae Hwang", "authors": "Xingchen Zhao, Anthony Sicilia, Davneet Minhas, Erin O'Connor, Howard\n  Aizenstein, William Klunk, Dana Tudorascu, Seong Jae Hwang", "title": "Robust White Matter Hyperintensity Segmentation on Unseen Domain", "comments": "IEEE International Symposium on Biomedical Imaging 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Typical machine learning frameworks heavily rely on an underlying assumption\nthat training and test data follow the same distribution. In medical imaging\nwhich increasingly begun acquiring datasets from multiple sites or scanners,\nthis identical distribution assumption often fails to hold due to systematic\nvariability induced by site or scanner dependent factors. Therefore, we cannot\nsimply expect a model trained on a given dataset to consistently work well, or\ngeneralize, on a dataset from another distribution. In this work, we address\nthis problem, investigating the application of machine learning models to\nunseen medical imaging data. Specifically, we consider the challenging case of\nDomain Generalization (DG) where we train a model without any knowledge about\nthe testing distribution. That is, we train on samples from a set of\ndistributions (sources) and test on samples from a new, unseen distribution\n(target). We focus on the task of white matter hyperintensity (WMH) prediction\nusing the multi-site WMH Segmentation Challenge dataset and our local in-house\ndataset. We identify how two mechanically distinct DG approaches, namely domain\nadversarial learning and mix-up, have theoretical synergy. Then, we show\ndrastic improvements of WMH prediction on an unseen target domain.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 17:44:11 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 03:16:20 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Zhao", "Xingchen", ""], ["Sicilia", "Anthony", ""], ["Minhas", "Davneet", ""], ["O'Connor", "Erin", ""], ["Aizenstein", "Howard", ""], ["Klunk", "William", ""], ["Tudorascu", "Dana", ""], ["Hwang", "Seong Jae", ""]]}, {"id": "2102.06657", "submitter": "Pingchuan Ma", "authors": "Pingchuan Ma, Stavros Petridis, Maja Pantic", "title": "End-to-end Audio-visual Speech Recognition with Conformers", "comments": "Accepted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a hybrid CTC/Attention model based on a ResNet-18\nand Convolution-augmented transformer (Conformer), that can be trained in an\nend-to-end manner. In particular, the audio and visual encoders learn to\nextract features directly from raw pixels and audio waveforms, respectively,\nwhich are then fed to conformers and then fusion takes place via a Multi-Layer\nPerceptron (MLP). The model learns to recognise characters using a combination\nof CTC and an attention mechanism. We show that end-to-end training, instead of\nusing pre-computed visual features which is common in the literature, the use\nof a conformer, instead of a recurrent network, and the use of a\ntransformer-based language model, significantly improve the performance of our\nmodel. We present results on the largest publicly available datasets for\nsentence-level speech recognition, Lip Reading Sentences 2 (LRS2) and Lip\nReading Sentences 3 (LRS3), respectively. The results show that our proposed\nmodels raise the state-of-the-art performance by a large margin in audio-only,\nvisual-only, and audio-visual experiments.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 18:00:08 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Ma", "Pingchuan", ""], ["Petridis", "Stavros", ""], ["Pantic", "Maja", ""]]}, {"id": "2102.06665", "submitter": "Thomas Pock", "authors": "Dominik Narnhofer and Alexander Effland and Erich Kobler and Kerstin\n  Hammernik and Florian Knoll and Thomas Pock", "title": "Bayesian Uncertainty Estimation of Learned Variational MRI\n  Reconstruction", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep learning approaches focus on improving quantitative scores of\ndedicated benchmarks, and therefore only reduce the observation-related\n(aleatoric) uncertainty. However, the model-immanent (epistemic) uncertainty is\nless frequently systematically analyzed. In this work, we introduce a Bayesian\nvariational framework to quantify the epistemic uncertainty. To this end, we\nsolve the linear inverse problem of undersampled MRI reconstruction in a\nvariational setting. The associated energy functional is composed of a data\nfidelity term and the total deep variation (TDV) as a learned parametric\nregularizer. To estimate the epistemic uncertainty we draw the parameters of\nthe TDV regularizer from a multivariate Gaussian distribution, whose mean and\ncovariance matrix are learned in a stochastic optimal control problem. In\nseveral numerical experiments, we demonstrate that our approach yields\ncompetitive results for undersampled MRI reconstruction. Moreover, we can\naccurately quantify the pixelwise epistemic uncertainty, which can serve\nradiologists as an additional resource to visualize reconstruction reliability.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 18:08:14 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Narnhofer", "Dominik", ""], ["Effland", "Alexander", ""], ["Kobler", "Erich", ""], ["Hammernik", "Kerstin", ""], ["Knoll", "Florian", ""], ["Pock", "Thomas", ""]]}, {"id": "2102.06679", "submitter": "Fabio Maria Carlucci", "authors": "Luca Robbiano and Muhammad Rameez Ur Rahman and Fabio Galasso and\n  Barbara Caputo and Fabio Maria Carlucci", "title": "Adversarial Branch Architecture Search for Unsupervised Domain\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Unsupervised Domain Adaptation (UDA) is a key field in visual recognition, as\nit enables robust performances across different visual domains. In the deep\nlearning era, the performance of UDA methods has been driven by better losses\nand by improved network architectures, specifically the addition of auxiliary\ndomain-alignment branches to pre-trained backbones. However, all the neural\narchitectures proposed so far are hand-crafted, which might hinder further\nprogress.\n  The current copious offspring of Neural Architecture Search (NAS) only\nalleviates hand-crafting so far, as it requires labels for model selection,\nwhich are not available in UDA, and is usually applied to the whole\narchitecture, while using pre-trained models is a strict requirement for high\nperformance. No prior work has addressed these aspects in the context of NAS\nfor UDA.\n  Here we propose an Adversarial Branch Architecture Search (ABAS) for UDA, to\nlearn the auxiliary branch network from data without handcrafting. Our main\ncontribution include i. a novel data-driven ensemble approach for model\nselection, to circumvent the lack of target labels, and ii. a pipeline to\nautomatically search for the best performing auxiliary branch.\n  To the best of our knowledge, ABAS is the first NAS method for UDA to comply\nwith a pre-trained backbone, a strict requirement for high performance. ABAS\noutputs both the optimal auxiliary branch and its trained parameters. When\napplied to two modern UDA techniques, DANN and ALDA, it improves performance on\nthree standard CV datasets (Office31, Office-Home and PACS). In all cases, ABAS\nrobustly finds the branch architectures which yield best performances. Code\nwill be released.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 18:35:35 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 19:02:01 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Robbiano", "Luca", ""], ["Rahman", "Muhammad Rameez Ur", ""], ["Galasso", "Fabio", ""], ["Caputo", "Barbara", ""], ["Carlucci", "Fabio Maria", ""]]}, {"id": "2102.06685", "submitter": "Rui Li", "authors": "Rui Li, Xiantuo He, Danna Xue, Shaolin Su, Qing Mao, Yu Zhu, Jinqiu\n  Sun, Yanning Zhang", "title": "Learning Depth via Leveraging Semantics: Self-supervised Monocular Depth\n  Estimation with Both Implicit and Explicit Semantic Guidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised depth estimation has made a great success in learning depth\nfrom unlabeled image sequences. While the mappings between image and pixel-wise\ndepth are well-studied in current methods, the correlation between image, depth\nand scene semantics, however, is less considered. This hinders the network to\nbetter understand the real geometry of the scene, since the contextual clues,\ncontribute not only the latent representations of scene depth, but also the\nstraight constraints for depth map. In this paper, we leverage the two benefits\nby proposing the implicit and explicit semantic guidance for accurate\nself-supervised depth estimation. We propose a Semantic-aware Spatial Feature\nAlignment (SSFA) scheme to effectively align implicit semantic features with\ndepth features for scene-aware depth estimation. We also propose a\nsemantic-guided ranking loss to explicitly constrain the estimated depth maps\nto be consistent with real scene contextual properties. Both semantic label\nnoise and prediction uncertainty is considered to yield reliable depth\nsupervisions. Extensive experimental results show that our method produces high\nquality depth maps which are consistently superior either on complex scenes or\ndiverse semantic categories, and outperforms the state-of-the-art methods by a\nsignificant margin.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 14:29:51 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Li", "Rui", ""], ["He", "Xiantuo", ""], ["Xue", "Danna", ""], ["Su", "Shaolin", ""], ["Mao", "Qing", ""], ["Zhu", "Yu", ""], ["Sun", "Jinqiu", ""], ["Zhang", "Yanning", ""]]}, {"id": "2102.06687", "submitter": "Hongliu Cao", "authors": "Hongliu Cao, Eoin Thomas", "title": "Destination similarity based on implicit user interest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the digitization of travel industry, it is more and more important to\nunderstand users from their online behaviors. However, online travel industry\ndata are more challenging to analyze due to extra sparseness, dispersed user\nhistory actions, fast change of user interest and lack of direct or indirect\nfeedbacks. In this work, a new similarity method is proposed to measure the\ndestination similarity in terms of implicit user interest. By comparing the\nproposed method to several other widely used similarity measures in recommender\nsystems, the proposed method achieves a significant improvement on travel data.\nKey words: Destination similarity, Travel industry, Recommender System,\nImplicit user interest\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 18:45:23 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 15:36:36 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Cao", "Hongliu", ""], ["Thomas", "Eoin", ""]]}, {"id": "2102.06690", "submitter": "Youngjun Cho", "authors": "Youngjun Cho", "title": "Rethinking Eye-blink: Assessing Task Difficulty through Physiological\n  Representation of Spontaneous Blinking", "comments": "[Accepted version] In Proceedings of CHI Conference on Human Factors\n  in Computing Systems (CHI '21), May 8-13, 2021, Yokohama, Japan. ACM, New\n  York, NY, USA. 19 Pages. https://doi.org/10.1145/3411764.3445577", "journal-ref": null, "doi": "10.1145/3411764.3445577", "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous assessment of task difficulty and mental workload is essential in\nimproving the usability and accessibility of interactive systems. Eye tracking\ndata has often been investigated to achieve this ability, with reports on the\nlimited role of standard blink metrics. Here, we propose a new approach to the\nanalysis of eye-blink responses for automated estimation of task difficulty.\nThe core module is a time-frequency representation of eye-blink, which aims to\ncapture the richness of information reflected on blinking. In our first study,\nwe show that this method significantly improves the sensitivity to task\ndifficulty. We then demonstrate how to form a framework where the represented\npatterns are analyzed with multi-dimensional Long Short-Term Memory recurrent\nneural networks for their non-linear mapping onto difficulty-related\nparameters. This framework outperformed other methods that used hand-engineered\nfeatures. This approach works with any built-in camera, without requiring\nspecialized devices. We conclude by discussing how Rethinking Eye-blink can\nbenefit real-world applications.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 18:47:13 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Cho", "Youngjun", ""]]}, {"id": "2102.06696", "submitter": "Mohamad Shahbazi", "authors": "Mohamad Shahbazi, Zhiwu Huang, Danda Pani Paudel, Ajad Chhatkuli, Luc\n  Van Gool", "title": "Efficient Conditional GAN Transfer with Knowledge Propagation across\n  Classes", "comments": "The is available at: https://github.com/mshahbazi72/cGANTransfer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative adversarial networks (GANs) have shown impressive results in both\nunconditional and conditional image generation. In recent literature, it is\nshown that pre-trained GANs, on a different dataset, can be transferred to\nimprove the image generation from a small target data. The same, however, has\nnot been well-studied in the case of conditional GANs (cGANs), which provides\nnew opportunities for knowledge transfer compared to unconditional setup. In\nparticular, the new classes may borrow knowledge from the related old classes,\nor share knowledge among themselves to improve the training. This motivates us\nto study the problem of efficient conditional GAN transfer with knowledge\npropagation across classes. To address this problem, we introduce a new GAN\ntransfer method to explicitly propagate the knowledge from the old classes to\nthe new classes. The key idea is to enforce the popularly used conditional\nbatch normalization (BN) to learn the class-specific information of the new\nclasses from that of the old classes, with implicit knowledge sharing among the\nnew ones. This allows for an efficient knowledge propagation from the old\nclasses to the new ones, with the BN parameters increasing linearly with the\nnumber of new classes. The extensive evaluation demonstrates the clear\nsuperiority of the proposed method over state-of-the-art competitors for\nefficient conditional GAN transfer tasks. The code is available at:\nhttps://github.com/mshahbazi72/cGANTransfer\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 18:55:34 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 09:57:27 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Shahbazi", "Mohamad", ""], ["Huang", "Zhiwu", ""], ["Paudel", "Danda Pani", ""], ["Chhatkuli", "Ajad", ""], ["Van Gool", "Luc", ""]]}, {"id": "2102.06697", "submitter": "Mohammadreza Noormandipour", "authors": "Mohammadreza Noormandipour, Hanchen Wang", "title": "Matching Point Sets with Quantum Circuit Learning", "comments": "16 pages, 11 figures, 1 table. Major numerical calculations added for\n  the second version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a parameterised quantum circuit learning approach to\npoint set matching problem. In contrast to previous annealing-based methods, we\npropose a quantum circuit-based framework whose parameters are optimised via\ndescending the gradients w.r.t a kernel-based loss function. We formulate the\nshape matching problem into a distribution learning task; that is, to learn the\ndistribution of the optimal transformation parameters. We show that this\nframework is able to find multiple optimal solutions for symmetric shapes and\nis more accurate, scalable and robust than the previous annealing-based method.\nCode, data and pre-trained weights are available at the project page:\n\\href{https://hansen7.github.io/qKC}{https://hansen7.github.io/qKC}\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 18:55:49 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 19:59:06 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Noormandipour", "Mohammadreza", ""], ["Wang", "Hanchen", ""]]}, {"id": "2102.06725", "submitter": "Andrew Shin", "authors": "Takuya Narihira, Javier Alonsogarcia, Fabien Cardinaux, Akio Hayakawa,\n  Masato Ishii, Kazunori Iwaki, Thomas Kemp, Yoshiyuki Kobayashi, Lukas Mauch,\n  Akira Nakamura, Yukio Obuchi, Andrew Shin, Kenji Suzuki, Stephen Tiedmann,\n  Stefan Uhlich, Takuya Yashima, Kazuki Yoshiyama", "title": "Neural Network Libraries: A Deep Learning Framework Designed from\n  Engineers' Perspectives", "comments": "https://nnabla.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While there exist a plethora of deep learning tools and frameworks, the\nfast-growing complexity of the field brings new demands and challenges, such as\nmore flexible network design, speedy computation on distributed setting, and\ncompatibility between different tools. In this paper, we introduce Neural\nNetwork Libraries (https://nnabla.org), a deep learning framework designed from\nengineer's perspective, with emphasis on usability and compatibility as its\ncore design principles. We elaborate on each of our design principles and its\nmerits, and validate our attempts via experiments.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 19:12:16 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 05:07:16 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Narihira", "Takuya", ""], ["Alonsogarcia", "Javier", ""], ["Cardinaux", "Fabien", ""], ["Hayakawa", "Akio", ""], ["Ishii", "Masato", ""], ["Iwaki", "Kazunori", ""], ["Kemp", "Thomas", ""], ["Kobayashi", "Yoshiyuki", ""], ["Mauch", "Lukas", ""], ["Nakamura", "Akira", ""], ["Obuchi", "Yukio", ""], ["Shin", "Andrew", ""], ["Suzuki", "Kenji", ""], ["Tiedmann", "Stephen", ""], ["Uhlich", "Stefan", ""], ["Yashima", "Takuya", ""], ["Yoshiyama", "Kazuki", ""]]}, {"id": "2102.06729", "submitter": "Joris Gu\\'erin", "authors": "Igor Garcia Ballhausen Sampaio and Luigy Machaca and Jos\\'e Viterbo\n  and Joris Gu\\'erin", "title": "A novel method for object detection using deep learning and CAD models", "comments": "8 pages, 4 figures, 2 tables, To appear in the proceedings of the\n  23rd International Conference on Enterprise Information Systems (ICEIS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object Detection (OD) is an important computer vision problem for industry,\nwhich can be used for quality control in the production lines, among other\napplications. Recently, Deep Learning (DL) methods have enabled practitioners\nto train OD models performing well on complex real world images. However, the\nadoption of these models in industry is still limited by the difficulty and the\nsignificant cost of collecting high quality training datasets. On the other\nhand, when applying OD to the context of production lines, CAD models of the\nobjects to be detected are often available. In this paper, we introduce a fully\nautomated method that uses a CAD model of an object and returns a fully trained\nOD model for detecting this object. To do this, we created a Blender script\nthat generates realistic labeled datasets of images containing the object,\nwhich are then used for training the OD model. The method is validated\nexperimentally on two practical examples, showing that this approach can\ngenerate OD models performing well on real images, while being trained only on\nsynthetic images. The proposed method has potential to facilitate the adoption\nof object detection models in industry as it is easy to adapt for new objects\nand highly flexible. Hence, it can result in significant costs reduction, gains\nin productivity and improved products quality.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 19:19:45 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Sampaio", "Igor Garcia Ballhausen", ""], ["Machaca", "Luigy", ""], ["Viterbo", "Jos\u00e9", ""], ["Gu\u00e9rin", "Joris", ""]]}, {"id": "2102.06732", "submitter": "Jiapeng Wang", "authors": "Jiapeng Wang, Chongyu Liu, Lianwen Jin, Guozhi Tang, Jiaxin Zhang,\n  Shuaitao Zhang, Qianying Wang, Yaqiang Wu, Mingxiang Cai", "title": "Towards Robust Visual Information Extraction in Real World: New Dataset\n  and Novel Solution", "comments": "8 pages, 5 figures, to be published in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual information extraction (VIE) has attracted considerable attention\nrecently owing to its various advanced applications such as document\nunderstanding, automatic marking and intelligent education. Most existing works\ndecoupled this problem into several independent sub-tasks of text spotting\n(text detection and recognition) and information extraction, which completely\nignored the high correlation among them during optimization. In this paper, we\npropose a robust visual information extraction system (VIES) towards real-world\nscenarios, which is a unified end-to-end trainable framework for simultaneous\ntext detection, recognition and information extraction by taking a single\ndocument image as input and outputting the structured information.\nSpecifically, the information extraction branch collects abundant visual and\nsemantic representations from text spotting for multimodal feature fusion and\nconversely, provides higher-level semantic clues to contribute to the\noptimization of text spotting. Moreover, regarding the shortage of public\nbenchmarks, we construct a fully-annotated dataset called EPHOIE\n(https://github.com/HCIILAB/EPHOIE), which is the first Chinese benchmark for\nboth text spotting and visual information extraction. EPHOIE consists of 1,494\nimages of examination paper head with complex layouts and background, including\na total of 15,771 Chinese handwritten or printed text instances. Compared with\nthe state-of-the-art methods, our VIES shows significant superior performance\non the EPHOIE dataset and achieves a 9.01% F-score gain on the widely used\nSROIE dataset under the end-to-end scenario.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 11:05:24 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Wang", "Jiapeng", ""], ["Liu", "Chongyu", ""], ["Jin", "Lianwen", ""], ["Tang", "Guozhi", ""], ["Zhang", "Jiaxin", ""], ["Zhang", "Shuaitao", ""], ["Wang", "Qianying", ""], ["Wu", "Yaqiang", ""], ["Cai", "Mingxiang", ""]]}, {"id": "2102.06733", "submitter": "Zan Huang", "authors": "Zan Huang", "title": "Revisiting the details when evaluating a visual tracker", "comments": "5 pages, 3 figures, 3 tables, just a tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Visual tracking algorithms are naturally adopted in various applications,\nthere have been several benchmarks and many tracking algorithms, more expected\nto appear in the future. In this report, I focus on single object tracking and\nrevisit the details of tracker evaluation based on widely used OTB\\cite{otb}\nbenchmark by introducing a simpler, accurate, and extensible method for tracker\nevaluation and comparison. Experimental results suggest that there may not be\nan absolute winner among tracking algorithms. We have to perform detailed\nanalysis to select suitable trackers for use cases.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 13:43:27 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Huang", "Zan", ""]]}, {"id": "2102.06777", "submitter": "Eslam Bakr Mohamed", "authors": "Eslam Mohamed, Abdelrahman Shaker, Ahmad El-Sallab, Mayada Hadhoud", "title": "INSTA-YOLO: Real-Time Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Instance segmentation has gained recently huge attention in various computer\nvision applications. It aims at providing different IDs to different objects of\nthe scene, even if they belong to the same class. Instance segmentation is\nusually performed as a two-stage pipeline. First, an object is detected, then\nsemantic segmentation within the detected box area is performed which involves\ncostly up-sampling. In this paper, we propose Insta-YOLO, a novel one-stage\nend-to-end deep learning model for real-time instance segmentation. Instead of\npixel-wise prediction, our model predicts instances as object contours\nrepresented by 2D points in Cartesian space. We evaluate our model on three\ndatasets, namely, Carvana,Cityscapes and Airbus. We compare our results to the\nstate-of-the-art models for instance segmentation. The results show our model\nachieves competitive accuracy in terms of mAP at twice the speed on GTX-1080\nGPU.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 21:17:29 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 19:37:06 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Mohamed", "Eslam", ""], ["Shaker", "Abdelrahman", ""], ["El-Sallab", "Ahmad", ""], ["Hadhoud", "Mayada", ""]]}, {"id": "2102.06792", "submitter": "Anselmo Ferreira", "authors": "Anselmo Ferreira, Ehsan Nowroozi and Mauro Barni", "title": "VIPPrint: A Large Scale Dataset of Printed and Scanned Images for\n  Synthetic Face Images Detection and Source Linking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The possibility of carrying out a meaningful forensics analysis on printed\nand scanned images plays a major role in many applications. First of all,\nprinted documents are often associated with criminal activities, such as\nterrorist plans, child pornography pictures, and even fake packages.\nAdditionally, printing and scanning can be used to hide the traces of image\nmanipulation or the synthetic nature of images, since the artifacts commonly\nfound in manipulated and synthetic images are gone after the images are printed\nand scanned. A problem hindering research in this area is the lack of large\nscale reference datasets to be used for algorithm development and benchmarking.\nMotivated by this issue, we present a new dataset composed of a large number of\nsynthetic and natural printed face images. To highlight the difficulties\nassociated with the analysis of the images of the dataset, we carried out an\nextensive set of experiments comparing several printer attribution methods. We\nalso verified that state-of-the-art methods to distinguish natural and\nsynthetic face images fail when applied to print and scanned images. We\nenvision that the availability of the new dataset and the preliminary\nexperiments we carried out will motivate and facilitate further research in\nthis area.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 13:00:29 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Ferreira", "Anselmo", ""], ["Nowroozi", "Ehsan", ""], ["Barni", "Mauro", ""]]}, {"id": "2102.06793", "submitter": "Ernest Davis", "authors": "Ernest Davis", "title": "Unanswerable Questions about Images and Texts", "comments": "15 pages, 4 figures", "journal-ref": "Frontiers in Artificial Intelligence: Language and Computation.\n  July 2020", "doi": "10.3389/frai.2020.00051", "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Questions about a text or an image that cannot be answered raise distinctive\nissues for an AI. This note discusses the problem of unanswerable questions in\nVQA (visual question answering), in QA (visual question answering), and in AI\ngenerally.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 17:56:15 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Davis", "Ernest", ""]]}, {"id": "2102.06802", "submitter": "Xingyu Li", "authors": "Xingyu Li", "title": "Blind stain separation using model-aware generative learning and its\n  applications on fluorescence microscopy images", "comments": "Accepted by IPML2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiple stains are usually used to highlight biological substances in\nbiomedical image analysis. To decompose multiple stains for co-localization\nquantification, blind source separation is usually performed. Prior model-based\nstain separation methods usually rely on stains' spatial distributions over an\nimage and may fail to solve the co-localization problem. With the advantage of\nmachine learning, deep generative models are used for this purpose. Since prior\nknowledge of imaging models is ignored in purely data-driven solutions, these\nmethods may be sub-optimal. In this study, a novel learning-based blind source\nseparation framework is proposed, where the physical model of biomedical\nimaging is incorporated to regularize the learning process. The introduced\nmodel-relevant adversarial loss couples all generators in the framework and\nlimits the capacities of the generative models. Further more, a training\nalgorithm is innovated for the proposed framework to avoid inter-generator\nconfusion during learning. This paper particularly takes fluorescence unmixing\nin fluorescence microscopy images as an application example of the proposed\nframework. Qualitative and quantitative experimentation on a public\nfluorescence microscopy image set demonstrates the superiority of the proposed\nmethod over both prior model-based approaches and learning-based methods.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 22:39:39 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Li", "Xingyu", ""]]}, {"id": "2102.06810", "submitter": "Yuandong Tian", "authors": "Yuandong Tian and Xinlei Chen and Surya Ganguli", "title": "Understanding self-supervised Learning Dynamics without Contrastive\n  Pairs", "comments": "ICML 2021 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While contrastive approaches of self-supervised learning (SSL) learn\nrepresentations by minimizing the distance between two augmented views of the\nsame data point (positive pairs) and maximizing views from different data\npoints (negative pairs), recent \\emph{non-contrastive} SSL (e.g., BYOL and\nSimSiam) show remarkable performance {\\it without} negative pairs, with an\nextra learnable predictor and a stop-gradient operation. A fundamental question\narises: why do these methods not collapse into trivial representations? We\nanswer this question via a simple theoretical study and propose a novel\napproach, DirectPred, that \\emph{directly} sets the linear predictor based on\nthe statistics of its inputs, without gradient training. On ImageNet, it\nperforms comparably with more complex two-layer non-linear predictors that\nemploy BatchNorm and outperforms a linear predictor by $2.5\\%$ in 300-epoch\ntraining (and $5\\%$ in 60-epoch). DirectPred is motivated by our theoretical\nstudy of the nonlinear learning dynamics of non-contrastive SSL in simple\nlinear networks. Our study yields conceptual insights into how non-contrastive\nSSL methods learn, how they avoid representational collapse, and how multiple\nfactors, like predictor networks, stop-gradients, exponential moving averages,\nand weight decay all come into play. Our simple theory recapitulates the\nresults of real-world ablation studies in both STL-10 and ImageNet. Code is\nreleased\\footnote{\\url{https://github.com/facebookresearch/luckmatters/tree/master/ssl}}.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 22:57:28 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 16:51:55 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Tian", "Yuandong", ""], ["Chen", "Xinlei", ""], ["Ganguli", "Surya", ""]]}, {"id": "2102.06837", "submitter": "Ikhsanul Habibie", "authors": "Ikhsanul Habibie, Weipeng Xu, Dushyant Mehta, Lingjie Liu, Hans-Peter\n  Seidel, Gerard Pons-Moll, Mohamed Elgharib, Christian Theobalt", "title": "Learning Speech-driven 3D Conversational Gestures from Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose the first approach to automatically and jointly synthesize both\nthe synchronous 3D conversational body and hand gestures, as well as 3D face\nand head animations, of a virtual character from speech input. Our algorithm\nuses a CNN architecture that leverages the inherent correlation between facial\nexpression and hand gestures. Synthesis of conversational body gestures is a\nmulti-modal problem since many similar gestures can plausibly accompany the\nsame input speech. To synthesize plausible body gestures in this setting, we\ntrain a Generative Adversarial Network (GAN) based model that measures the\nplausibility of the generated sequences of 3D body motion when paired with the\ninput audio features. We also contribute a new way to create a large corpus of\nmore than 33 hours of annotated body, hand, and face data from in-the-wild\nvideos of talking people. To this end, we apply state-of-the-art monocular\napproaches for 3D body and hand pose estimation as well as dense 3D face\nperformance capture to the video corpus. In this way, we can train on orders of\nmagnitude more data than previous algorithms that resort to complex in-studio\nmotion capture solutions, and thereby train more expressive synthesis\nalgorithms. Our experiments and user study show the state-of-the-art quality of\nour speech-synthesized full 3D character animations.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 01:05:39 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Habibie", "Ikhsanul", ""], ["Xu", "Weipeng", ""], ["Mehta", "Dushyant", ""], ["Liu", "Lingjie", ""], ["Seidel", "Hans-Peter", ""], ["Pons-Moll", "Gerard", ""], ["Elgharib", "Mohamed", ""], ["Theobalt", "Christian", ""]]}, {"id": "2102.06841", "submitter": "Ivan Bajic", "authors": "Ivan V. Baji\\'c, Weisi Lin, Yonghong Tian", "title": "Collaborative Intelligence: Challenges and Opportunities", "comments": "5 pages, 2 figures, accepted for presentation at IEEE ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents an overview of the emerging area of collaborative\nintelligence (CI). Our goal is to raise awareness in the signal processing\ncommunity of the challenges and opportunities in this area of growing\nimportance, where key developments are expected to come from signal processing\nand related disciplines. The paper surveys the current state of the art in CI,\nwith special emphasis on signal processing-related challenges in feature\ncompression, error resilience, privacy, and system-level design.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 01:24:05 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Baji\u0107", "Ivan V.", ""], ["Lin", "Weisi", ""], ["Tian", "Yonghong", ""]]}, {"id": "2102.06864", "submitter": "Yomna Safaa El-Din MSc.", "authors": "Yomna Safaa El-Din, Mohamed N. Moustafa and Hani Mahdi", "title": "Adversarial Unsupervised Domain Adaptation Guided with Deep Clustering\n  for Face Presentation Attack Detection", "comments": "10 pages, 2 figures, to be published in IMPROVE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Face Presentation Attack Detection (PAD) has drawn increasing attentions to\nsecure the face recognition systems that are widely used in many applications.\nConventional face anti-spoofing methods have been proposed, assuming that\ntesting is from the same domain used for training, and so cannot generalize\nwell on unseen attack scenarios. The trained models tend to overfit to the\nacquisition sensors and attack types available in the training data. In light\nof this, we propose an end-to-end learning framework based on Domain Adaptation\n(DA) to improve PAD generalization capability. Labeled source-domain samples\nare used to train the feature extractor and classifier via cross-entropy loss,\nwhile unsupervised data from the target domain are utilized in adversarial DA\napproach causing the model to learn domain-invariant features. Using DA alone\nin face PAD fails to adapt well to target domain that is acquired in different\nconditions with different devices and attack types than the source domain. And\nso, in order to keep the intrinsic properties of the target domain, deep\nclustering of target samples is performed. Training and deep clustering are\nperformed end-to-end, and experiments performed on several public benchmark\ndatasets validate that our proposed Deep Clustering guided Unsupervised Domain\nAdaptation (DCDA) can learn more generalized information compared with the\nstate-of-the-art classification error on the target domain.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 05:34:40 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["El-Din", "Yomna Safaa", ""], ["Moustafa", "Mohamed N.", ""], ["Mahdi", "Hani", ""]]}, {"id": "2102.06867", "submitter": "Changxing Ding", "authors": "Shengcong Chen, Changxing Ding, Minfeng Liu, and Dacheng Tao", "title": "CPP-Net: Context-aware Polygon Proposal Network for Nucleus Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nucleus segmentation is a challenging task due to the crowded distribution\nand blurry boundaries of nuclei. Recent approaches represent nuclei by means of\npolygons to differentiate between touching and overlapping nuclei and have\naccordingly achieved promising performance. Each polygon is represented by a\nset of centroid-to-boundary distances, which are in turn predicted by features\nof the centroid pixel for a single nucleus. However, using the centroid pixel\nalone does not provide sufficient contextual information for robust prediction.\nTo handle this problem, we propose a Context-aware Polygon Proposal Network\n(CPP-Net) for nucleus segmentation. First, we sample a point set rather than\none single pixel within each cell for distance prediction. This strategy\nsubstantially enhances contextual information and thereby improves the\nrobustness of the prediction. Second, we propose a Confidence-based Weighting\nModule, which adaptively fuses the predictions from the sampled point set.\nThird, we introduce a novel Shape-Aware Perceptual (SAP) loss that constrains\nthe shape of the predicted polygons. Here, the SAP loss is based on an\nadditional network that is pre-trained by means of mapping the centroid\nprobability map and the pixel-to-boundary distance maps to a different nucleus\nrepresentation. Extensive experiments justify the effectiveness of each\ncomponent in the proposed CPP-Net. Finally, CPP-Net is found to achieve\nstate-of-the-art performance on three publicly available databases, namely\nDSB2018, BBBC06, and PanNuke. Code of this paper will be released.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 05:59:52 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Chen", "Shengcong", ""], ["Ding", "Changxing", ""], ["Liu", "Minfeng", ""], ["Tao", "Dacheng", ""]]}, {"id": "2102.06868", "submitter": "Jerome Quenum", "authors": "Jerome Quenum, Kehan Wang, Avideh Zakhor", "title": "Fast, Accurate Barcode Detection in Ultra High-Resolution Images", "comments": "5 pages, 4 figures, 3 tables, GitHub Link added, Initial ArXiv\n  Submission is 13 Feb 2021, Accepted at IEEE International Conference on Image\n  Processing, September 2021, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Object detection in Ultra High-Resolution (UHR) images has long been a\nchallenging problem in computer vision due to the varying scales of the\ntargeted objects. When it comes to barcode detection, resizing UHR input images\nto smaller sizes often leads to the loss of pertinent information, while\nprocessing them directly is highly inefficient and computationally expensive.\nIn this paper, we propose using semantic segmentation to achieve a fast and\naccurate detection of barcodes of various scales in UHR images. Our pipeline\ninvolves a modified Region Proposal Network (RPN) on images of size greater\nthan 10k$\\times$10k and a newly proposed Y-Net segmentation network, followed\nby a post-processing workflow for fitting a bounding box around each segmented\nbarcode mask. The end-to-end system has a latency of 16 milliseconds, which is\n$2.5\\times$ faster than YOLOv4 and $5.9\\times$ faster than Mask R-CNN. In terms\nof accuracy, our method outperforms YOLOv4 and Mask R-CNN by a $mAP$ of 5.5%\nand 47.1% respectively, on a synthetic dataset. We have made available the\ngenerated synthetic barcode dataset and its code at\nhttp://www.github.com/viplabB/SBD/.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 05:59:59 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 20:59:28 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Quenum", "Jerome", ""], ["Wang", "Kehan", ""], ["Zakhor", "Avideh", ""]]}, {"id": "2102.06870", "submitter": "Wissam Baddar", "authors": "Wissam J. Baddar, Seungju Han, Seonmin Rhee, Jae-Joon Han", "title": "Self-Reorganizing and Rejuvenating CNNs for Increasing Model Capacity\n  Utilization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose self-reorganizing and rejuvenating convolutional\nneural networks; a biologically inspired method for improving the computational\nresource utilization of neural networks. The proposed method utilizes the\nchannel activations of a convolution layer in order to reorganize that layers\nparameters. The reorganized parameters are clustered to avoid parameter\nredundancies. As such, redundant neurons with similar activations are merged\nleaving room for the remaining parameters to rejuvenate. The rejuvenated\nparameters learn different features to supplement those learned by the\nreorganized surviving parameters. As a result, the network capacity utilization\nincreases improving the baseline network performance without any changes to the\nnetwork structure. The proposed method can be applied to various network\narchitectures during the training stage, or applied to a pre-trained model\nimproving its performance. Experimental results showed that the proposed method\nis model-agnostic and can be applied to any backbone architecture increasing\nits performance due to the elevated utilization of the network capacity.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 06:19:45 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Baddar", "Wissam J.", ""], ["Han", "Seungju", ""], ["Rhee", "Seonmin", ""], ["Han", "Jae-Joon", ""]]}, {"id": "2102.06882", "submitter": "Sri Kalyan Yarlagadda", "authors": "Sri Kalyan Yarlagadda, Daniel Mas Montserrat, David Guerra, Carol J.\n  Boushey, Deborah A. Kerr, Fengqing Zhu", "title": "Saliency-Aware Class-Agnostic Food Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advances in image-based dietary assessment methods have allowed nutrition\nprofessionals and researchers to improve the accuracy of dietary assessment,\nwhere images of food consumed are captured using smartphones or wearable\ndevices. These images are then analyzed using computer vision methods to\nestimate energy and nutrition content of the foods. Food image segmentation,\nwhich determines the regions in an image where foods are located, plays an\nimportant role in this process. Current methods are data dependent, thus cannot\ngeneralize well for different food types. To address this problem, we propose a\nclass-agnostic food image segmentation method. Our method uses a pair of eating\nscene images, one before start eating and one after eating is completed. Using\ninformation from both the before and after eating images, we can segment food\nimages by finding the salient missing objects without any prior information\nabout the food class. We model a paradigm of top down saliency which guides the\nattention of the human visual system (HVS) based on a task to find the salient\nmissing objects in a pair of images. Our method is validated on food images\ncollected from a dietary study which showed promising results.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 08:05:19 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Yarlagadda", "Sri Kalyan", ""], ["Montserrat", "Daniel Mas", ""], ["Guerra", "David", ""], ["Boushey", "Carol J.", ""], ["Kerr", "Deborah A.", ""], ["Zhu", "Fengqing", ""]]}, {"id": "2102.06883", "submitter": "Roohallah Alizadehsani", "authors": "Danial Sharifrazi, Roohallah Alizadehsani, Mohamad Roshanzamir, Javad\n  Hassannataj Joloudari, Afshin Shoeibi, Mahboobeh Jafari, Sadiq Hussain, Zahra\n  Alizadeh Sani, Fereshteh Hasanzadeh, Fahime Khozeimeh, Abbas Khosravi, Saeid\n  Nahavandi, Maryam Panahiazar, Assef Zare, Sheikh Mohammed Shariful Islam, U\n  Rajendra Acharya", "title": "Fusion of convolution neural network, support vector machine and Sobel\n  filter for accurate detection of COVID-19 patients using X-ray images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coronavirus (COVID-19) is currently the most common contagious disease\nwhich is prevalent all over the world. The main challenge of this disease is\nthe primary diagnosis to prevent secondary infections and its spread from one\nperson to another. Therefore, it is essential to use an automatic diagnosis\nsystem along with clinical procedures for the rapid diagnosis of COVID-19 to\nprevent its spread. Artificial intelligence techniques using computed\ntomography (CT) images of the lungs and chest radiography have the potential to\nobtain high diagnostic performance for Covid-19 diagnosis. In this study, a\nfusion of convolutional neural network (CNN), support vector machine (SVM), and\nSobel filter is proposed to detect COVID-19 using X-ray images. A new X-ray\nimage dataset was collected and subjected to high pass filter using a Sobel\nfilter to obtain the edges of the images. Then these images are fed to CNN deep\nlearning model followed by SVM classifier with ten-fold cross validation\nstrategy. This method is designed so that it can learn with not many data. Our\nresults show that the proposed CNN-SVM with Sobel filtering (CNN-SVM+Sobel)\nachieved the highest classification accuracy of 99.02% in accurate detection of\nCOVID-19. It showed that using Sobel filter can improve the performance of CNN.\nUnlike most of the other researches, this method does not use a pre-trained\nnetwork. We have also validated our developed model using six public databases\nand obtained the highest performance. Hence, our developed model is ready for\nclinical application\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 08:08:36 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Sharifrazi", "Danial", ""], ["Alizadehsani", "Roohallah", ""], ["Roshanzamir", "Mohamad", ""], ["Joloudari", "Javad Hassannataj", ""], ["Shoeibi", "Afshin", ""], ["Jafari", "Mahboobeh", ""], ["Hussain", "Sadiq", ""], ["Sani", "Zahra Alizadeh", ""], ["Hasanzadeh", "Fereshteh", ""], ["Khozeimeh", "Fahime", ""], ["Khosravi", "Abbas", ""], ["Nahavandi", "Saeid", ""], ["Panahiazar", "Maryam", ""], ["Zare", "Assef", ""], ["Islam", "Sheikh Mohammed Shariful", ""], ["Acharya", "U Rajendra", ""]]}, {"id": "2102.06900", "submitter": "Raghavendra Selvan", "authors": "Raghavendra Selvan, Erik B Dam, Jens Petersen", "title": "Segmenting two-dimensional structures with strided tensor networks", "comments": "Accepted to be presented at the 27th international conference on\n  Information Processing in Medical Imaging (IPMI-2021), Bornholm, Denmark.\n  Source code at https://github.com/raghavian/strided-tenet. Version 2: Minor\n  fixes to notation in Eq.1 and typos", "journal-ref": null, "doi": "10.1007/978-3-030-78191-0_31", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tensor networks provide an efficient approximation of operations involving\nhigh dimensional tensors and have been extensively used in modelling quantum\nmany-body systems. More recently, supervised learning has been attempted with\ntensor networks, primarily focused on tasks such as image classification. In\nthis work, we propose a novel formulation of tensor networks for supervised\nimage segmentation which allows them to operate on high resolution medical\nimages. We use the matrix product state (MPS) tensor network on non-overlapping\npatches of a given input image to predict the segmentation mask by learning a\npixel-wise linear classification rule in a high dimensional space. The proposed\nmodel is end-to-end trainable using backpropagation. It is implemented as a\nStrided Tensor Network to reduce the parameter complexity. The performance of\nthe proposed method is evaluated on two public medical imaging datasets and\ncompared to relevant baselines. The evaluation shows that the strided tensor\nnetwork yields competitive performance compared to CNN-based models while using\nfewer resources. Additionally, based on the experiments we discuss the\nfeasibility of using fully linear models for segmentation tasks.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 11:06:34 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 20:29:07 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Selvan", "Raghavendra", ""], ["Dam", "Erik B", ""], ["Petersen", "Jens", ""]]}, {"id": "2102.06942", "submitter": "Philip M\\\"uller", "authors": "Philip M\\\"uller, Vladimir Golkov, Valentina Tomassini, Daniel Cremers", "title": "Rotation-Equivariant Deep Learning for Diffusion MRI", "comments": "24 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks are successful, but they have recently been\noutperformed by new neural networks that are equivariant under rotations and\ntranslations. These new networks work better because they do not struggle with\nlearning each possible orientation of each image feature separately. So far,\nthey have been proposed for 2D and 3D data. Here we generalize them to 6D\ndiffusion MRI data, ensuring joint equivariance under 3D roto-translations in\nimage space and the matching 3D rotations in $q$-space, as dictated by the\nimage formation. Such equivariant deep learning is appropriate for diffusion\nMRI, because microstructural and macrostructural features such as neural fibers\ncan appear at many different orientations, and because even\nnon-rotation-equivariant deep learning has so far been the best method for many\ndiffusion MRI tasks. We validate our equivariant method on multiple-sclerosis\nlesion segmentation. Our proposed neural networks yield better results and\nrequire fewer scans for training compared to non-rotation-equivariant deep\nlearning. They also inherit all the advantages of deep learning over classical\ndiffusion MRI methods. Our implementation is available at\nhttps://github.com/philip-mueller/equivariant-deep-dmri and can be used off the\nshelf without understanding the mathematical background.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 15:18:34 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["M\u00fcller", "Philip", ""], ["Golkov", "Vladimir", ""], ["Tomassini", "Valentina", ""], ["Cremers", "Daniel", ""]]}, {"id": "2102.06944", "submitter": "Saman Motamed", "authors": "Saman Motamed and Farzad Khalvati", "title": "Multi-class Generative Adversarial Nets for Semi-supervised Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  From generating never-before-seen images to domain adaptation, applications\nof Generative Adversarial Networks (GANs) spread wide in the domain of vision\nand graphics problems. With the remarkable ability of GANs in learning the\ndistribution and generating images of a particular class, they can be used for\nsemi-supervised classification tasks. However, the problem is that if two\nclasses of images share similar characteristics, the GAN might learn to\ngeneralize and hinder the classification of the two classes. In this paper, we\nuse various images from MNIST and Fashion-MNIST datasets to illustrate how\nsimilar images cause the GAN to generalize, leading to the poor classification\nof images. We propose a modification to the traditional training of GANs that\nallows for improved multi-class classification in similar classes of images in\na semi-supervised learning framework.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 15:26:17 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 16:25:31 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Motamed", "Saman", ""], ["Khalvati", "Farzad", ""]]}, {"id": "2102.06955", "submitter": "Frederik Beuth", "authors": "Frederik Beuth, Tobias Schlosser, Michael Friedrich, Danny Kowerko", "title": "Improving Automated Visual Fault Detection by Combining a Biologically\n  Plausible Model of Visual Attention with Deep Learning", "comments": "This work is an extended arXiv version of the original conference\n  article published in \"IECON 2020\":\n  https://ieeexplore.ieee.org/abstract/document/9255234 . The work has been\n  extended regarding visual attention", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a long-term goal to transfer biological processing principles as well\nas the power of human recognition into machine vision and engineering systems.\nOne of such principles is visual attention, a smart human concept which focuses\nprocessing on a part of a scene. In this contribution, we utilize attention to\nimprove the automatic detection of defect patterns for wafers within the domain\nof semiconductor manufacturing. Previous works in the domain have often\nutilized classical machine learning approaches such as KNNs, SVMs, or MLPs,\nwhile a few have already used modern approaches like deep neural networks\n(DNNs). However, one problem in the domain is that the faults are often very\nsmall and have to be detected within a larger size of the chip or even the\nwafer. Therefore, small structures in the size of pixels have to be detected in\na vast amount of image data. One interesting principle of the human brain for\nsolving this problem is visual attention. Hence, we employ here a biologically\nplausible model of visual attention for automatic visual inspection. We propose\na hybrid system of visual attention and a deep neural network. As demonstrated,\nour system achieves among other decisive advantages an improvement in accuracy\nfrom 81% to 92%, and an increase in accuracy for detecting faults from 67% to\n88%. Hence, the error rates are reduced from 19% to 8%, and notably from 33% to\n12% for detecting a fault in a chip. These results show that attention can\ngreatly improve the performance of visual inspection systems. Furthermore, we\nconduct a broad evaluation, identifying specific advantages of the biological\nattention model in this application, and benchmarks standard deep learning\napproaches as an alternative with and without attention.\n  This work is an extended arXiv version of the original conference article\npublished in \"IECON 2020\", which has been extended regarding visual attention.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 16:50:41 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Beuth", "Frederik", ""], ["Schlosser", "Tobias", ""], ["Friedrich", "Michael", ""], ["Kowerko", "Danny", ""]]}, {"id": "2102.06968", "submitter": "Jacob John", "authors": "Jacob John", "title": "Discrete Cosine Transform in JPEG Compression", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image Compression has become an absolute necessity in today's day and age.\nWith the advent of the Internet era, compressing files to share among other\nusers is quintessential. Several efforts have been made to reduce file sizes\nwhile still maintain image quality in order to transmit files even on limited\nbandwidth connections. This paper discusses the need for Discrete Cosine\nTransform or DCT in the compression of images in Joint Photographic Experts\nGroup or JPEG file format. Via an intensive literature study, this paper first\nintroduces DCT and JPEG Compression. The section preceding it discusses how\nJPEG compression is implemented by DCT. The last section concludes with further\nreal world applications of DCT in image processing.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 17:50:21 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["John", "Jacob", ""]]}, {"id": "2102.06979", "submitter": "Abdelrahman Eldesokey", "authors": "Abdelrahman Eldesokey, Michael Felsberg", "title": "Normalized Convolution Upsampling for Refined Optical Flow Estimation", "comments": "Published at the 16th International Conference on Computer Vision\n  Theory and Applications (VISAPP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical flow is a regression task where convolutional neural networks (CNNs)\nhave led to major breakthroughs. However, this comes at major computational\ndemands due to the use of cost-volumes and pyramidal representations. This was\nmitigated by producing flow predictions at quarter the resolution, which are\nupsampled using bilinear interpolation during test time. Consequently, fine\ndetails are usually lost and post-processing is needed to restore them. We\npropose the Normalized Convolution UPsampler (NCUP), an efficient joint\nupsampling approach to produce the full-resolution flow during the training of\noptical flow CNNs. Our proposed approach formulates the upsampling task as a\nsparse problem and employs the normalized convolutional neural networks to\nsolve it. We evaluate our upsampler against existing joint upsampling\napproaches when trained end-to-end with a a coarse-to-fine optical flow CNN\n(PWCNet) and we show that it outperforms all other approaches on the\nFlyingChairs dataset while having at least one order fewer parameters.\nMoreover, we test our upsampler with a recurrent optical flow CNN (RAFT) and we\nachieve state-of-the-art results on Sintel benchmark with ~6% error reduction,\nand on-par on the KITTI dataset, while having 7.5% fewer parameters (see Figure\n1). Finally, our upsampler shows better generalization capabilities than RAFT\nwhen trained and evaluated on different datasets.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 18:34:03 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Eldesokey", "Abdelrahman", ""], ["Felsberg", "Michael", ""]]}, {"id": "2102.06982", "submitter": "Neelambuj Chaturvedi", "authors": "Neelambuj Chaturvedi", "title": "DeepRA: Predicting Joint Damage From Radiographs Using CNN with\n  Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Joint damage in Rheumatoid Arthritis (RA) is assessed by manually inspecting\nand grading radiographs of hands and feet. This is a tedious task which\nrequires trained experts whose subjective assessment leads to low inter-rater\nagreement. An algorithm which can automatically predict the joint level damage\nin hands and feet can help optimize this process, which will eventually aid the\ndoctors in better patient care and research. In this paper, we propose a\ntwo-staged approach which amalgamates object detection and convolution neural\nnetworks with attention which can efficiently and accurately predict the\noverall and joint level narrowing and erosion from patients radiographs. This\napproach has been evaluated on hands and feet radiographs of patients suffering\nfrom RA and has achieved a weighted root mean squared error (RMSE) of 1.358 and\n1.404 in predicting joint level narrowing and erosion Sharp van der Heijde\n(SvH) scores which is 31% and 19% improvement with respect to the baseline SvH\nscores, respectively. The proposed approach achieved a weighted absolute error\nof 1.456 in predicting the overall damage in hands and feet radiographs for the\npatients which is a 79% improvement as compared to the baseline. Our method\nalso provides an inherent capability to provide explanations for model\npredictions using attention weights, which is essential given the black box\nnature of deep learning models. The proposed approach was developed during the\nRA2 Dream Challenge hosted by Dream Challenges and secured 4th and 8th position\nin predicting overall and joint level narrowing and erosion SvH scores from\nradiographs.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 18:48:01 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Chaturvedi", "Neelambuj", ""]]}, {"id": "2102.06997", "submitter": "Alessandro Lameiras Koerich", "authors": "Steve Tsham Mpinda Ataky and Alessandro Lameiras Koerich", "title": "A Novel Bio-Inspired Texture Descriptor based on Biodiversity and\n  Taxonomic Measures", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture can be defined as the change of image intensity that forms repetitive\npatterns, resulting from physical properties of the object's roughness or\ndifferences in a reflection on the surface. Considering that texture forms a\ncomplex system of patterns in a non-deterministic way, biodiversity concepts\ncan help texture characterization in images. This paper proposes a novel\napproach capable of quantifying such a complex system of diverse patterns\nthrough species diversity and richness and taxonomic distinctiveness. The\nproposed approach considers each image channel as a species ecosystem and\ncomputes species diversity and richness measures as well as taxonomic measures\nto describe the texture. The proposed approach takes advantage of ecological\npatterns' invariance characteristics to build a permutation, rotation, and\ntranslation invariant descriptor. Experimental results on three datasets of\nnatural texture images and two datasets of histopathological images have shown\nthat the proposed texture descriptor has advantages over several texture\ndescriptors and deep methods.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 20:14:14 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2021 19:51:07 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Ataky", "Steve Tsham Mpinda", ""], ["Koerich", "Alessandro Lameiras", ""]]}, {"id": "2102.07037", "submitter": "Minhyeok Lee", "authors": "Minhyeok Lee, Junhyeop Lee, Dogyoon Lee, Woojin Kim, Sangwon Hwang,\n  Sangyoun Lee", "title": "Robust Lane Detection via Expanded Self Attention", "comments": "10 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The image-based lane detection algorithm is one of the key technologies in\nautonomous vehicles. Modern deep learning methods achieve high performance in\nlane detection, but it is still difficult to accurately detect lanes in\nchallenging situations such as congested roads and extreme lighting conditions.\nTo be robust on these challenging situations, it is important to extract global\ncontextual information even from limited visual cues. In this paper, we propose\na simple but powerful self-attention mechanism optimized for lane detection\ncalled the Expanded Self Attention (ESA) module. Inspired by the simple\ngeometric structure of lanes, the proposed method predicts the confidence of a\nlane along the vertical and horizontal directions in an image. The prediction\nof the confidence enables estimating occluded locations by extracting global\ncontextual information. ESA module can be easily implemented and applied to any\nencoder-decoder-based model without increasing the inference time. The\nperformance of our method is evaluated on three popular lane detection\nbenchmarks (TuSimple, CULane and BDD100K). We achieve state-of-the-art\nperformance in CULane and BDD100K and distinct improvement on TuSimple dataset.\nThe experimental results show that our approach is robust to occlusion and\nextreme lighting conditions.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 00:29:55 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Lee", "Minhyeok", ""], ["Lee", "Junhyeop", ""], ["Lee", "Dogyoon", ""], ["Kim", "Woojin", ""], ["Hwang", "Sangwon", ""], ["Lee", "Sangyoun", ""]]}, {"id": "2102.07064", "submitter": "Zirui Wang", "authors": "Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, Victor Adrian Prisacariu", "title": "NeRF--: Neural Radiance Fields Without Known Camera Parameters", "comments": "project page see nerfmm.active.vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the problem of novel view synthesis (NVS) from 2D images\nwithout known camera poses and intrinsics. Among various NVS techniques, Neural\nRadiance Field (NeRF) has recently gained popularity due to its remarkable\nsynthesis quality. Existing NeRF-based approaches assume that the camera\nparameters associated with each input image are either directly accessible at\ntraining, or can be accurately estimated with conventional techniques based on\ncorrespondences, such as Structure-from-Motion. In this work, we propose an\nend-to-end framework, termed NeRF--, for training NeRF models given only RGB\nimages, without pre-computed camera parameters. Specifically, we show that the\ncamera parameters, including both intrinsics and extrinsics, can be\nautomatically discovered via joint optimisation during the training of the NeRF\nmodel. On the standard LLFF benchmark, our model achieves comparable novel view\nsynthesis results compared to the baseline trained with COLMAP pre-computed\ncamera parameters. We also conduct extensive analyses to understand the model\nbehaviour under different camera trajectories, and show that in scenarios where\nCOLMAP fails, our model still produces robust results.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 03:52:34 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 10:45:13 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2021 08:15:40 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Wang", "Zirui", ""], ["Wu", "Shangzhe", ""], ["Xie", "Weidi", ""], ["Chen", "Min", ""], ["Prisacariu", "Victor Adrian", ""]]}, {"id": "2102.07067", "submitter": "Shan An", "authors": "Shan An, Xiajie Zhang, Dong Wei, Haogang Zhu, Jianyu Yang, and\n  Konstantinos A. Tsintotas", "title": "FastHand: Fast Hand Pose Estimation From A Monocular Camera", "comments": "submitted to ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hand gesture recognition constitutes the initial step in most methods related\nto human-robot interaction. There are two key challenges in this task. The\nfirst one corresponds to the difficulty of achieving stable and accurate hand\nlandmark predictions in real-world scenarios, while the second to the decreased\ntime of forward inference. In this paper, we propose a fast and accurate\nframework for hand pose estimation, dubbed as \"FastHand\". Using a lightweight\nencoder-decoder network architecture, we achieve to fulfil the requirements of\npractical applications running on embedded devices. The encoder consists of\ndeep layers with a small number of parameters, while the decoder makes use of\nspatial location information to obtain more accurate results. The evaluation\ntook place on two publicly available datasets demonstrating the improved\nperformance of the proposed pipeline compared to other state-of-the-art\napproaches. FastHand offers high accuracy scores while reaching a speed of 25\nframes per second on an NVIDIA Jetson TX2 graphics processing unit.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 04:12:41 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["An", "Shan", ""], ["Zhang", "Xiajie", ""], ["Wei", "Dong", ""], ["Zhu", "Haogang", ""], ["Yang", "Jianyu", ""], ["Tsintotas", "Konstantinos A.", ""]]}, {"id": "2102.07074", "submitter": "Yifan Jiang", "authors": "Yifan Jiang, Shiyu Chang, Zhangyang Wang", "title": "TransGAN: Two Pure Transformers Can Make One Strong GAN, and That Can\n  Scale Up", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent explosive interest on transformers has suggested their potential\nto become powerful ``universal\" models for computer vision tasks, such as\nclassification, detection, and segmentation. While those attempts mainly study\nthe discriminative models, we explore transformers on some more notoriously\ndifficult vision tasks, e.g., generative adversarial networks (GANs). Our goal\nis to conduct the first pilot study in building a GAN completely free of\nconvolutions, using only pure transformer-based architectures. Our vanilla GAN\narchitecture, dubbed TransGAN, consists of a memory-friendly transformer-based\ngenerator that progressively increases feature resolution, and correspondingly\na multi-scale discriminator to capture simultaneously semantic contexts and\nlow-level textures. On top of them, we introduce the new module of grid\nself-attention for alleviating the memory bottleneck further, in order to scale\nup TransGAN to high-resolution generation. We also develop a unique training\nrecipe including a series of techniques that can mitigate the training\ninstability issues of TransGAN, such as data augmentation, modified\nnormalization, and relative position encoding. Our best architecture achieves\nhighly competitive performance compared to current state-of-the-art GANs using\nconvolutional backbones. Specifically, TransGAN sets new state-of-the-art\ninception score of 10.43 and FID of 18.28 on STL-10, outperforming StyleGAN-V2.\nWhen it comes to higher-resolution (e.g. 256 x 256) generation tasks, such as\non CelebA-HQ and LSUN-Church, TransGAN continues to produce diverse visual\nexamples with high fidelity and impressive texture details. In addition, we\ndive deep into the transformer-based generation models to understand how their\nbehaviors differ from convolutional ones, by visualizing training dynamics. The\ncode is available at https://github.com/VITA-Group/TransGAN.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 05:24:48 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 05:51:12 GMT"}, {"version": "v3", "created": "Mon, 14 Jun 2021 03:19:23 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Jiang", "Yifan", ""], ["Chang", "Shiyu", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2102.07077", "submitter": "Ethan Shen", "authors": "Ethan Shen, Maria Brbic, Nicholas Monath, Jiaqi Zhai, Manzil Zaheer,\n  Jure Leskovec", "title": "Model-Agnostic Graph Regularization for Few-Shot Learning", "comments": "NeurIPS Workshop on Meta-Learning 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many domains, relationships between categories are encoded in the\nknowledge graph. Recently, promising results have been achieved by\nincorporating knowledge graph as side information in hard classification tasks\nwith severely limited data. However, prior models consist of highly complex\narchitectures with many sub-components that all seem to impact performance. In\nthis paper, we present a comprehensive empirical study on graph embedded\nfew-shot learning. We introduce a graph regularization approach that allows a\ndeeper understanding of the impact of incorporating graph information between\nlabels. Our proposed regularization is widely applicable and model-agnostic,\nand boosts the performance of any few-shot learning model, including\nfine-tuning, metric-based, and optimization-based meta-learning. Our approach\nimproves the performance of strong base learners by up to 2% on Mini-ImageNet\nand 6.7% on ImageNet-FS, outperforming state-of-the-art graph embedded methods.\nAdditional analyses reveal that graph regularizing models result in a lower\nloss for more difficult tasks, such as those with fewer shots and less\ninformative support examples.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 05:28:13 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Shen", "Ethan", ""], ["Brbic", "Maria", ""], ["Monath", "Nicholas", ""], ["Zhai", "Jiaqi", ""], ["Zaheer", "Manzil", ""], ["Leskovec", "Jure", ""]]}, {"id": "2102.07085", "submitter": "Jing Jin", "authors": "Jing Jin and Hui Liu and Junhui Hou and Hongkai Xiong", "title": "Light Field Reconstruction via Attention-Guided Deep Fusion of Hybrid\n  Lenses", "comments": "14 pages, 8 figures. arXiv admin note: text overlap with\n  arXiv:1907.09640", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper explores the problem of reconstructing high-resolution light field\n(LF) images from hybrid lenses, including a high-resolution camera surrounded\nby multiple low-resolution cameras. The performance of existing methods is\nstill limited, as they produce either blurry results on plain textured areas or\ndistortions around depth discontinuous boundaries. To tackle this challenge, we\npropose a novel end-to-end learning-based approach, which can comprehensively\nutilize the specific characteristics of the input from two complementary and\nparallel perspectives. Specifically, one module regresses a spatially\nconsistent intermediate estimation by learning a deep multidimensional and\ncross-domain feature representation, while the other module warps another\nintermediate estimation, which maintains the high-frequency textures, by\npropagating the information of the high-resolution view. We finally leverage\nthe advantages of the two intermediate estimations adaptively via the learned\nattention maps, leading to the final high-resolution LF image with satisfactory\nresults on both plain textured areas and depth discontinuous boundaries.\nBesides, to promote the effectiveness of our method trained with simulated\nhybrid data on real hybrid data captured by a hybrid LF imaging system, we\ncarefully design the network architecture and the training strategy. Extensive\nexperiments on both real and simulated hybrid data demonstrate the significant\nsuperiority of our approach over state-of-the-art ones. To the best of our\nknowledge, this is the first end-to-end deep learning method for LF\nreconstruction from a real hybrid input. We believe our framework could\npotentially decrease the cost of high-resolution LF data acquisition and\nbenefit LF data storage and transmission.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 06:44:47 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Jin", "Jing", ""], ["Liu", "Hui", ""], ["Hou", "Junhui", ""], ["Xiong", "Hongkai", ""]]}, {"id": "2102.07092", "submitter": "Heeseung Kwon", "authors": "Heeseung Kwon, Manjin Kim, Suha Kwak, Minsu Cho", "title": "Learning Self-Similarity in Space and Time as Generalized Motion for\n  Action Recognition", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal convolution often fails to learn motion dynamics in videos\nand thus an effective motion representation is required for video understanding\nin the wild. In this paper, we propose a rich and robust motion representation\nbased on spatio-temporal self-similarity (STSS). Given a sequence of frames,\nSTSS represents each local region as similarities to its neighbors in space and\ntime. By converting appearance features into relational values, it enables the\nlearner to better recognize structural patterns in space and time. We leverage\nthe whole volume of STSS and let our model learn to extract an effective motion\nrepresentation from it. The proposed neural block, dubbed SELFY, can be easily\ninserted into neural architectures and trained end-to-end without additional\nsupervision. With a sufficient volume of the neighborhood in space and time, it\neffectively captures long-term interaction and fast motion in the video,\nleading to robust action recognition. Our experimental analysis demonstrates\nits superiority over previous methods for motion modeling as well as its\ncomplementarity to spatio-temporal features from direct convolution. On the\nstandard action recognition benchmarks, Something-Something-V1 & V2, Diving-48,\nand FineGym, the proposed method achieves the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 07:32:55 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Kwon", "Heeseung", ""], ["Kim", "Manjin", ""], ["Kwak", "Suha", ""], ["Cho", "Minsu", ""]]}, {"id": "2102.07125", "submitter": "Sourav Mishra", "authors": "Sourav Mishra and Suresh Sundaram", "title": "Self Regulated Learning Mechanism for Data Efficient Knowledge\n  Distillation", "comments": "8 pages, 5 figures, 6 tables, 27 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing methods for distillation do not efficiently utilize the training\ndata. This work presents a novel approach to perform distillation using only a\nsubset of the training data, making it more data-efficient. For this purpose,\nthe training of the teacher model is modified to include self-regulation\nwherein a sample in the training set is used for updating model parameters in\nthe backward pass either if it is misclassified or the model is not confident\nenough in its prediction. This modification restricts the participation of\nsamples, unlike the conventional training method. The number of times a sample\nparticipates in the self-regulated training process is a measure of its\nsignificance towards the model's knowledge. The significance values are used to\nweigh the losses incurred on the corresponding samples in the distillation\nprocess. This method is named significance-based distillation. Two other\nmethods are proposed for comparison where the student model learns by\ndistillation and incorporating self-regulation as the teacher model, either\nutilizing the significance information computed during the teacher's training\nor not. These methods are named hybrid and regulated distillations,\nrespectively. Experiments on benchmark datasets show that the proposed methods\nachieve similar performance as other state-of-the-art methods for knowledge\ndistillation while utilizing a significantly less number of samples.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 10:43:13 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 08:14:17 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Mishra", "Sourav", ""], ["Sundaram", "Suresh", ""]]}, {"id": "2102.07156", "submitter": "Rishabh Tiwari", "authors": "Rishabh Tiwari, Udbhav Bamba, Arnav Chavan, Deepak K. Gupta", "title": "ChipNet: Budget-Aware Pruning with Heaviside Continuous Approximations", "comments": "Accepted at ICLR 2021 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Structured pruning methods are among the effective strategies for extracting\nsmall resource-efficient convolutional neural networks from their dense\ncounterparts with minimal loss in accuracy. However, most existing methods\nstill suffer from one or more limitations, that include 1) the need for\ntraining the dense model from scratch with pruning-related parameters embedded\nin the architecture, 2) requiring model-specific hyperparameter settings, 3)\ninability to include budget-related constraint in the training process, and 4)\ninstability under scenarios of extreme pruning. In this paper, we present\nChipNet, a deterministic pruning strategy that employs continuous Heaviside\nfunction and a novel crispness loss to identify a highly sparse network out of\nan existing dense network. Our choice of continuous Heaviside function is\ninspired by the field of design optimization, where the material distribution\ntask is posed as a continuous optimization problem, but only discrete values (0\nor 1) are practically feasible and expected as final outcomes. Our approach's\nflexible design facilitates its use with different choices of budget\nconstraints while maintaining stability for very low target budgets.\nExperimental results show that ChipNet outperforms state-of-the-art structured\npruning methods by remarkable margins of up to 16.1% in terms of accuracy.\nFurther, we show that the masks obtained with ChipNet are transferable across\ndatasets. For certain cases, it was observed that masks transferred from a\nmodel trained on feature-rich teacher dataset provide better performance on the\nstudent dataset than those obtained by directly pruning on the student data\nitself.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 13:54:50 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Tiwari", "Rishabh", ""], ["Bamba", "Udbhav", ""], ["Chavan", "Arnav", ""], ["Gupta", "Deepak K.", ""]]}, {"id": "2102.07192", "submitter": "Mohammad Faiyaz Khan", "authors": "Mohammad Faiyaz Khan, S.M. Sadiq-Ur-Rahman Shifath, and Md. Saiful\n  Islam", "title": "Improved Bengali Image Captioning via deep convolutional neural network\n  based encoder-decoder model", "comments": "Accepted in \"IJCACI 2020: International Joint Conference on Advances\n  in Computational Intelligence\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Image Captioning is an arduous task of producing syntactically and\nsemantically correct textual descriptions of an image in natural language with\ncontext related to the image. Existing notable pieces of research in Bengali\nImage Captioning (BIC) are based on encoder-decoder architecture. This paper\npresents an end-to-end image captioning system utilizing a multimodal\narchitecture by combining a one-dimensional convolutional neural network (CNN)\nto encode sequence information with a pre-trained ResNet-50 model image encoder\nfor extracting region-based visual features. We investigate our approach's\nperformance on the BanglaLekhaImageCaptions dataset using the existing\nevaluation metrics and perform a human evaluation for qualitative analysis.\nExperiments show that our approach's language encoder captures the fine-grained\ninformation in the caption, and combined with the image features, it generates\naccurate and diversified caption. Our work outperforms all the existing BIC\nworks and achieves a new state-of-the-art (SOTA) performance by scoring 0.651\non BLUE-1, 0.572 on CIDEr, 0.297 on METEOR, 0.434 on ROUGE, and 0.357 on SPICE.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 16:44:17 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Khan", "Mohammad Faiyaz", ""], ["Shifath", "S. M. Sadiq-Ur-Rahman", ""], ["Islam", "Md. Saiful", ""]]}, {"id": "2102.07225", "submitter": "Xiaobin Hu", "authors": "Xiaobin Hu", "title": "Multi-Texture GAN: Exploring the Multi-Scale Texture Translation for\n  Brain MR Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inter-scanner and inter-protocol discrepancy in MRI datasets are known to\nlead to significant quantification variability. Hence image-to-image or\nscanner-to-scanner translation is a crucial frontier in the area of medical\nimage analysis with a lot of potential applications. Nonetheless, a significant\npercentage of existing algorithms cannot explicitly exploit and preserve\ntexture details from target scanners and offers individual solutions towards\nspecialized task-specific architectures. In this paper, we design a multi-scale\ntexture transfer to enrich the reconstruction images with more details.\nSpecifically, after calculating textural similarity, the multi-scale texture\ncan adaptively transfer the texture information from target images or reference\nimages to restored images. Different from the pixel-wise matching space as done\nby previous algorithms, we match texture features in a multi-scale scheme\nimplemented in the neural space. The matching mechanism can exploit multi-scale\nneural transfer that encourages the model to grasp more semantic-related and\nlesion-related priors from the target or reference images. We evaluate our\nmulti-scale texture GAN on three different tasks without any task-specific\nmodifications: cross-protocol super-resolution of diffusion MRI, T1-Flair, and\nFlair-T2 modality translation. Our multi-texture GAN rehabilitates more\nhigh-resolution structures (i.e., edges and anatomy), texture (i.e., contrast\nand pixel intensities), and lesion information (i.e., tumor). The extensively\nquantitative and qualitative experiments demonstrate that our method achieves\nsuperior results in inter-protocol or inter-scanner translation over\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 19:14:06 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Hu", "Xiaobin", ""]]}, {"id": "2102.07228", "submitter": "Adrian Shajkofci", "authors": "Adrian Shajkofci, Michael Liebling", "title": "Estimating Nonplanar Flow from 2D Motion-blurred Widefield Microscopy\n  Images via Deep Learning", "comments": "Accepted to ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical flow is a method aimed at predicting the movement velocity of any\npixel in the image and is used in medicine and biology to estimate flow of\nparticles in organs or organelles. However, a precise optical flow measurement\nrequires images taken at high speed and low exposure time, which induces\nphototoxicity due to the increase in illumination power. We are looking here to\nestimate the three-dimensional movement vector field of moving out-of-plane\nparticles using normal light conditions and a standard microscope camera.\n  We present a method to predict, from a single textured wide-field microscopy\nimage, the movement of out-of-plane particles using the local characteristics\nof the motion blur. We estimated the velocity vector field from the local\nestimation of the blur model parameters using an deep neural network and\nachieved a prediction with a regression coefficient of 0.92 between the ground\ntruth simulated vector field and the output of the network. This method could\nenable microscopists to gain insights about the dynamic properties of samples\nwithout the need for high-speed cameras or high-intensity light exposure.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 19:44:28 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Shajkofci", "Adrian", ""], ["Liebling", "Michael", ""]]}, {"id": "2102.07243", "submitter": "Jason Kamran Jr Eshraghian", "authors": "Dennis Robey, Wesley Thio, Herbert Iu, Jason Eshraghian", "title": "Naturalizing Neuromorphic Vision Event Streams Using GANs", "comments": "5 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dynamic vision sensors are able to operate at high temporal resolutions\nwithin resource constrained environments, though at the expense of capturing\nstatic content. The sparse nature of event streams enables efficient downstream\nprocessing tasks as they are suited for power-efficient spiking neural\nnetworks. One of the challenges associated with neuromorphic vision is the lack\nof interpretability of event streams. While most application use-cases do not\nintend for the event stream to be visually interpreted by anything other than a\nclassification network, there is a lost opportunity to integrating these\nsensors in spaces that conventional high-speed CMOS sensors cannot go. For\nexample, biologically invasive sensors such as endoscopes must fit within\nstringent power budgets, which do not allow MHz-speeds of image integration.\nWhile dynamic vision sensing can fill this void, the interpretation challenge\nremains and will degrade confidence in clinical diagnostics. The use of\ngenerative adversarial networks presents a possible solution to overcoming and\ncompensating for a vision chip's poor spatial resolution and lack of\ninterpretability. In this paper, we methodically apply the Pix2Pix network to\nnaturalize the event stream from spike-converted CIFAR-10 and Linnaeus 5\ndatasets. The quality of the network is benchmarked by performing image\nclassification of naturalized event streams, which converges to within 2.81% of\nequivalent raw images, and an associated improvement over unprocessed event\nstreams by 13.19% for the CIFAR-10 and Linnaeus 5 datasets.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 20:48:30 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Robey", "Dennis", ""], ["Thio", "Wesley", ""], ["Iu", "Herbert", ""], ["Eshraghian", "Jason", ""]]}, {"id": "2102.07271", "submitter": "Yongwan Lim", "authors": "Yongwan Lim, Shrikanth S. Narayanan, Krishna S. Nayak", "title": "Attention-gated convolutional neural networks for off-resonance\n  correction of spiral real-time MRI", "comments": "8 pages, 4 figures, 1 table", "journal-ref": "28th Int. Soc. Magn. Reson. Med. (ISMRM) Scientific Sessions,\n  2020, p.1005", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiral acquisitions are preferred in real-time MRI because of their\nefficiency, which has made it possible to capture vocal tract dynamics during\nnatural speech. A fundamental limitation of spirals is blurring and signal loss\ndue to off-resonance, which degrades image quality at air-tissue boundaries.\nHere, we present a new CNN-based off-resonance correction method that\nincorporates an attention-gate mechanism. This leverages spatial and channel\nrelationships of filtered outputs and improves the expressiveness of the\nnetworks. We demonstrate improved performance with the attention-gate, on 1.5\nTesla spiral speech RT-MRI, compared to existing off-resonance correction\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 23:43:50 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Lim", "Yongwan", ""], ["Narayanan", "Shrikanth S.", ""], ["Nayak", "Krishna S.", ""]]}, {"id": "2102.07280", "submitter": "Sina Mohammadi", "authors": "Sina Mohammadi, Mariana Belgiu, Alfred Stein", "title": "3D Fully Convolutional Neural Networks with Intersection Over Union Loss\n  for Crop Mapping from Multi-Temporal Satellite Images", "comments": "Submitted to IGARSS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information on cultivated crops is relevant for a large number of food\nsecurity studies. Different scientific efforts are dedicated to generate this\ninformation from remote sensing images by means of machine learning methods.\nUnfortunately, these methods do not account for the spatial-temporal\nrelationships inherent in remote sensing images. In our paper, we explore the\ncapability of a 3D Fully Convolutional Neural Network (FCN) to map crop types\nfrom multi-temporal images. In addition, we propose the Intersection Over Union\n(IOU) loss function for increasing the overlap between the predicted classes\nand ground truth data. The proposed method was applied to identify soybean and\ncorn from a study area situated in the US corn belt using multi-temporal\nLandsat images. The study shows that our method outperforms related methods,\nobtaining a Kappa coefficient of 90.8%. We conclude that using the IOU Loss\nfunction provides a superior choice to learn individual crop types.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 00:15:53 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Mohammadi", "Sina", ""], ["Belgiu", "Mariana", ""], ["Stein", "Alfred", ""]]}, {"id": "2102.07304", "submitter": "Mingu Kang", "authors": "Mingu Kang, Trung Quang Tran, Seungju Cho, Daeyoung Kim", "title": "CAP-GAN: Towards Adversarial Robustness with Cycle-consistent\n  Attentional Purification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attack is aimed at fooling the target classifier with\nimperceptible perturbation. Adversarial examples, which are carefully crafted\nwith a malicious purpose, can lead to erroneous predictions, resulting in\ncatastrophic accidents. To mitigate the effects of adversarial attacks, we\npropose a novel purification model called CAP-GAN. CAP-GAN takes account of the\nidea of pixel-level and feature-level consistency to achieve reasonable\npurification under cycle-consistent learning. Specifically, we utilize the\nguided attention module and knowledge distillation to convey meaningful\ninformation to the purification model. Once a model is fully trained, inputs\nwould be projected into the purification model and transformed into clean-like\nimages. We vary the capacity of the adversary to argue the robustness against\nvarious types of attack strategies. On the CIFAR-10 dataset, CAP-GAN\noutperforms other pre-processing based defenses under both black-box and\nwhite-box settings.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 02:23:33 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 02:26:40 GMT"}, {"version": "v3", "created": "Tue, 25 May 2021 13:22:10 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Kang", "Mingu", ""], ["Tran", "Trung Quang", ""], ["Cho", "Seungju", ""], ["Kim", "Daeyoung", ""]]}, {"id": "2102.07318", "submitter": "Yiming Xu", "authors": "Yiming Xu, Jiaxin Li, Yiheng Peng, Yan Ding and Hua-Liang Wei", "title": "A Global to Local Double Embedding Method for Multi-person Pose\n  Estimation", "comments": "ACCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-person pose estimation is a fundamental and challenging problem to many\ncomputer vision tasks. Most existing methods can be broadly categorized into\ntwo classes: top-down and bottom-up methods. Both of the two types of methods\ninvolve two stages, namely, person detection and joints detection.\nConventionally, the two stages are implemented separately without considering\ntheir interactions between them, and this may inevitably cause some issue\nintrinsically. In this paper, we present a novel method to simplify the\npipeline by implementing person detection and joints detection simultaneously.\nWe propose a Double Embedding (DE) method to complete the multi-person pose\nestimation task in a global-to-local way. DE consists of Global Embedding (GE)\nand Local Embedding (LE). GE encodes different person instances and processes\ninformation covering the whole image and LE encodes the local limbs\ninformation. GE functions for the person detection in top-down strategy while\nLE connects the rest joints sequentially which functions for joint grouping and\ninformation processing in A bottom-up strategy. Based on LE, we design the\nMutual Refine Machine (MRM) to reduce the prediction difficulty in complex\nscenarios. MRM can effectively realize the information communicating between\nkeypoints and further improve the accuracy. We achieve the competitive results\non benchmarks MSCOCO, MPII and CrowdPose, demonstrating the effectiveness and\ngeneralization ability of our method.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 03:13:38 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 03:55:59 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Xu", "Yiming", ""], ["Li", "Jiaxin", ""], ["Peng", "Yiheng", ""], ["Ding", "Yan", ""], ["Wei", "Hua-Liang", ""]]}, {"id": "2102.07343", "submitter": "He Chen", "authors": "He Chen, Hyojoon Park, Kutay Macit, Ladislav Kavan", "title": "Capturing Detailed Deformations of Moving Human Bodies", "comments": "20 pages, 25 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a new method to capture detailed human motion, sampling more than\n1000 unique points on the body. Our method outputs highly accurate 4D\n(spatio-temporal) point coordinates and, crucially, automatically assigns a\nunique label to each of the points. The locations and unique labels of the\npoints are inferred from individual 2D input images only, without relying on\ntemporal tracking or any human body shape or skeletal kinematics models.\nTherefore, our captured point trajectories contain all of the details from the\ninput images, including motion due to breathing, muscle contractions and flesh\ndeformation, and are well suited to be used as training data to fit advanced\nmodels of the human body and its motion. The key idea behind our system is a\nnew type of motion capture suit which contains a special pattern with\ncheckerboard-like corners and two-letter codes. The images from our\nmulti-camera system are processed by a sequence of neural networks which are\ntrained to localize the corners and recognize the codes, while being robust to\nsuit stretching and self-occlusions of the body. Our system relies only on\nstandard RGB or monochrome sensors and fully passive lighting and the passive\nsuit, making our method easy to replicate, deploy and use. Our experiments\ndemonstrate highly accurate captures of a wide variety of human poses,\nincluding challenging motions such as yoga, gymnastics, or rolling on the\nground.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 04:58:13 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 23:21:08 GMT"}, {"version": "v3", "created": "Fri, 30 Apr 2021 23:04:56 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chen", "He", ""], ["Park", "Hyojoon", ""], ["Macit", "Kutay", ""], ["Kavan", "Ladislav", ""]]}, {"id": "2102.07346", "submitter": "Kenji Kawaguchi", "authors": "Kenji Kawaguchi", "title": "On the Theory of Implicit Deep Learning: Global Convergence with\n  Implicit Layers", "comments": "ICLR 2021. Selected for ICLR Spotlight (top 6% submissions)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep equilibrium model uses implicit layers, which are implicitly defined\nthrough an equilibrium point of an infinite sequence of computation. It avoids\nany explicit computation of the infinite sequence by finding an equilibrium\npoint directly via root-finding and by computing gradients via implicit\ndifferentiation. In this paper, we analyze the gradient dynamics of deep\nequilibrium models with nonlinearity only on weight matrices and non-convex\nobjective functions of weights for regression and classification. Despite\nnon-convexity, convergence to global optimum at a linear rate is guaranteed\nwithout any assumption on the width of the models, allowing the width to be\nsmaller than the output dimension and the number of data points. Moreover, we\nprove a relation between the gradient dynamics of the deep implicit layer and\nthe dynamics of trust region Newton method of a shallow explicit layer. This\nmathematically proven relation along with our numerical observation suggests\nthe importance of understanding implicit bias of implicit layers and an open\nproblem on the topic. Our proofs deal with implicit layers, weight tying and\nnonlinearity on weights, and differ from those in the related literature.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 05:08:11 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 18:39:14 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Kawaguchi", "Kenji", ""]]}, {"id": "2102.07354", "submitter": "Thao Do", "authors": "Thao Do and Daeyoung Kim", "title": "QuickBrowser: A Unified Model to Detect and Read Simple Object in\n  Real-time", "comments": "Accepted at 2021 International Joint Conference on Neural Networks\n  (IJCNN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  There are many real-life use cases such as barcode scanning or billboard\nreading where people need to detect objects and read the object contents.\nCommonly existing methods are first trying to localize object regions, then\ndetermine layout and lastly classify content units. However, for simple fixed\nstructured objects like license plates, this approach becomes overkill and\nlengthy to run. This work aims to solve this detect-and-read problem in a\nlightweight way by integrating multi-digit recognition into a one-stage object\ndetection model. Our unified method not only eliminates the duplication in\nfeature extraction (one for localizing, one again for classifying) but also\nprovides useful contextual information around object regions for\nclassification. Additionally, our choice of backbones and modifications in\narchitecture, loss function, data augmentation and training make the method\nrobust, efficient and speedy. Secondly, we made a public benchmark dataset of\ndiverse real-life 1D barcodes for a reliable evaluation, which we collected,\nannotated and checked carefully. Eventually, experimental results prove the\nmethod's efficiency on the barcode problem by outperforming industrial tools in\nboth detecting and decoding rates with a real-time fps at a VGA-similar\nresolution. It also did a great job expectedly on the license-plate recognition\ntask (on the AOLP dataset) by outperforming the current state-of-the-art method\nsignificantly in terms of recognition rate and inference time.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 05:47:40 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 08:52:06 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Do", "Thao", ""], ["Kim", "Daeyoung", ""]]}, {"id": "2102.07355", "submitter": "Paritosh Parmar", "authors": "Paritosh Parmar, Brendan Morris", "title": "Win-Fail Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current video/action understanding systems have demonstrated impressive\nperformance on large recognition tasks. However, they might be limiting\nthemselves to learning to recognize spatiotemporal patterns, rather than\nattempting to thoroughly understand the actions. To spur progress in the\ndirection of a truer, deeper understanding of videos, we introduce the task of\nwin-fail action recognition -- differentiating between successful and failed\nattempts at various activities. We introduce a first of its kind paired\nwin-fail action understanding dataset with samples from the following domains:\n\"General Stunts,\" \"Internet Wins-Fails,\" \"Trick Shots,\" and \"Party Games.\"\nUnlike existing action recognition datasets, intra-class variation is high\nmaking the task challenging, yet feasible. We systematically analyze the\ncharacteristics of the win-fail task/dataset with prototypical action\nrecognition networks and a novel video retrieval task. While current action\nrecognition methods work well on our task/dataset, they still leave a large gap\nto achieve high performance. We hope to motivate more work towards the true\nunderstanding of actions/videos. Dataset will be available from\nhttps://github.com/ParitoshParmar/Win-Fail-Action-Recognition.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 06:03:10 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Parmar", "Paritosh", ""], ["Morris", "Brendan", ""]]}, {"id": "2102.07358", "submitter": "Shichao Xu", "authors": "Shichao Xu, Lixu Wang, Yixuan Wang, Qi Zhu", "title": "Weak Adaptation Learning -- Addressing Cross-domain Data Insufficiency\n  with Weak Annotator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data quantity and quality are crucial factors for data-driven learning\nmethods. In some target problem domains, there are not many data samples\navailable, which could significantly hinder the learning process. While data\nfrom similar domains may be leveraged to help through domain adaptation,\nobtaining high-quality labeled data for those source domains themselves could\nbe difficult or costly. To address such challenges on data insufficiency for\nclassification problem in a target domain, we propose a weak adaptation\nlearning (WAL) approach that leverages unlabeled data from a similar source\ndomain, a low-cost weak annotator that produces labels based on task-specific\nheuristics, labeling rules, or other methods (albeit with inaccuracy), and a\nsmall amount of labeled data in the target domain. Our approach first conducts\na theoretical analysis on the error bound of the trained classifier with\nrespect to the data quantity and the performance of the weak annotator, and\nthen introduces a multi-stage weak adaptation learning method to learn an\naccurate classifier by lowering the error bound. Our experiments demonstrate\nthe effectiveness of our approach in learning an accurate classifier with\nlimited labeled data in the target domain and unlabeled data in the source\ndomain.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 06:19:25 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Xu", "Shichao", ""], ["Wang", "Lixu", ""], ["Wang", "Yixuan", ""], ["Zhu", "Qi", ""]]}, {"id": "2102.07360", "submitter": "Ehsan Kazemi Dr", "authors": "Ehsan Kazemi, Thomas Kerdreux and Liquang Wang", "title": "Generating Structured Adversarial Attacks Using Frank-Wolfe Method", "comments": "arXiv admin note: substantial text overlap with arXiv:2007.01855", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  White box adversarial perturbations are generated via iterative optimization\nalgorithms most often by minimizing an adversarial loss on a $\\ell_p$\nneighborhood of the original image, the so-called distortion set. Constraining\nthe adversarial search with different norms results in disparately structured\nadversarial examples. Here we explore several distortion sets with\nstructure-enhancing algorithms. These new structures for adversarial examples\nmight provide challenges for provable and empirical robust mechanisms. Because\nadversarial robustness is still an empirical field, defense mechanisms should\nalso reasonably be evaluated against differently structured attacks. Besides,\nthese structured adversarial perturbations may allow for larger distortions\nsize than their $\\ell_p$ counter-part while remaining imperceptible or\nperceptible as natural distortions of the image. We will demonstrate in this\nwork that the proposed structured adversarial examples can significantly bring\ndown the classification accuracy of adversarialy trained classifiers while\nshowing low $\\ell_2$ distortion rate. For instance, on ImagNet dataset the\nstructured attacks drop the accuracy of adversarial model to near zero with\nonly 50\\% of $\\ell_2$ distortion generated using white-box attacks like PGD. As\na byproduct, our finding on structured adversarial examples can be used for\nadversarial regularization of models to make models more robust or improve\ntheir generalization performance on datasets which are structurally different.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 06:36:50 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Kazemi", "Ehsan", ""], ["Kerdreux", "Thomas", ""], ["Wang", "Liquang", ""]]}, {"id": "2102.07365", "submitter": "Priyadarshini Kumari", "authors": "Priyadarshini Kumari, Siddhartha Chaudhuri, Vivek Borkar, Subhasis\n  Chaudhuri", "title": "A Unified Batch Selection Policy for Active Metric Learning", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT cs.MM math.IT", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Active metric learning is the problem of incrementally selecting high-utility\nbatches of training data (typically, ordered triplets) to annotate, in order to\nprogressively improve a learned model of a metric over some input domain as\nrapidly as possible. Standard approaches, which independently assess the\ninformativeness of each triplet in a batch, are susceptible to highly\ncorrelated batches with many redundant triplets and hence low overall utility.\nWhile a recent work \\cite{kumari2020batch} proposes batch-decorrelation\nstrategies for metric learning, they rely on ad hoc heuristics to estimate the\ncorrelation between two triplets at a time. We present a novel batch active\nmetric learning method that leverages the Maximum Entropy Principle to learn\nthe least biased estimate of triplet distribution for a given set of prior\nconstraints. To avoid redundancy between triplets, our method collectively\nselects batches with maximum joint entropy, which simultaneously captures both\ninformativeness and diversity. We take advantage of the submodularity of the\njoint entropy function to construct a tractable solution using an efficient\ngreedy algorithm based on Gram-Schmidt orthogonalization that is provably\n$\\left( 1 - \\frac{1}{e} \\right)$-optimal. Our approach is the first batch\nactive metric learning method to define a unified score that balances\ninformativeness and diversity for an entire batch of triplets. Experiments with\nseveral real-world datasets demonstrate that our algorithm is robust,\ngeneralizes well to different applications and input modalities, and\nconsistently outperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 06:55:17 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 04:11:42 GMT"}, {"version": "v3", "created": "Tue, 16 Mar 2021 06:04:22 GMT"}, {"version": "v4", "created": "Mon, 5 Apr 2021 12:16:21 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Kumari", "Priyadarshini", ""], ["Chaudhuri", "Siddhartha", ""], ["Borkar", "Vivek", ""], ["Chaudhuri", "Subhasis", ""]]}, {"id": "2102.07373", "submitter": "Junxuan Huang", "authors": "Junxuan Huang and Junsong Yuan and Chunming Qiao", "title": "Generation For Adaption: A GAN-Based Approach for 3D Domain Adaption\n  with Point Cloud Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Recent deep networks have achieved good performance on a variety of 3d points\nclassification tasks. However, these models often face challenges in \"wild\ntasks\".There are considerable differences between the labeled training/source\ndata collected by one Lidar and unseen test/target data collected by a\ndifferent Lidar. Unsupervised domain adaptation (UDA) seeks to overcome such a\nproblem without target domain labels.Instead of aligning features between\nsource data and target data,we propose a method that use a Generative\nadversarial network to generate synthetic data from the source domain so that\nthe output is close to the target domain.Experiments show that our approach\nperforms better than other state-of-the-art UDA methods in three popular 3D\nobject/scene datasets (i.e., ModelNet, ShapeNet and ScanNet) for cross-domain\n3D objects classification.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 07:24:10 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 13:48:22 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 02:02:02 GMT"}, {"version": "v4", "created": "Mon, 10 May 2021 06:36:45 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Huang", "Junxuan", ""], ["Yuan", "Junsong", ""], ["Qiao", "Chunming", ""]]}, {"id": "2102.07444", "submitter": "Chaofan Tao", "authors": "Chaofan Tao, Rui Lin, Quan Chen, Zhaoyang Zhang, Ping Luo, Ngai Wong", "title": "FAT: Learning Low-Bitwidth Parametric Representation via Frequency-Aware\n  Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning convolutional neural networks (CNNs) with low bitwidth is\nchallenging because performance may drop significantly after quantization.\nPrior arts often discretize the network weights by carefully tuning\nhyper-parameters of quantization (e.g. non-uniform stepsize and layer-wise\nbitwidths), which are complicated and sub-optimal because the full-precision\nand low-precision models have a large discrepancy. This work presents a novel\nquantization pipeline, Frequency-Aware Transformation (FAT), which has several\nappealing benefits. (1) Rather than designing complicated quantizers like\nexisting works, FAT learns to transform network weights in the frequency domain\nbefore quantization, making them more amenable to training in low bitwidth. (2)\nWith FAT, CNNs can be easily trained in low precision using simple standard\nquantizers without tedious hyper-parameter tuning. Theoretical analysis shows\nthat FAT improves both uniform and non-uniform quantizers. (3) FAT can be\neasily plugged into many CNN architectures. When training ResNet-18 and\nMobileNet-V2 in 4 bits, FAT plus a simple rounding operation already achieves\n70.5% and 69.2% top-1 accuracy on ImageNet without bells and whistles,\noutperforming recent state-of-the-art by reducing 54.9X and 45.7X computations\nagainst full-precision models. We hope FAT provides a novel perspective for\nmodel quantization. Code is available at\n\\url{https://github.com/ChaofanTao/FAT_Quantization}.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 10:35:20 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 05:28:07 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Tao", "Chaofan", ""], ["Lin", "Rui", ""], ["Chen", "Quan", ""], ["Zhang", "Zhaoyang", ""], ["Luo", "Ping", ""], ["Wong", "Ngai", ""]]}, {"id": "2102.07448", "submitter": "Senthil Yogamani", "authors": "Varun Ravi Kumar, Senthil Yogamani, Hazem Rashed, Ganesh Sitsu,\n  Christian Witt, Isabelle Leang, Stefan Milz and Patrick M\\\"ader", "title": "OmniDet: Surround View Cameras based Multi-task Visual Perception\n  Network for Autonomous Driving", "comments": "Camera ready version accepted for RA-L and ICRA 2021 publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surround View fisheye cameras are commonly deployed in automated driving for\n360\\deg{} near-field sensing around the vehicle. This work presents a\nmulti-task visual perception network on unrectified fisheye images to enable\nthe vehicle to sense its surrounding environment. It consists of six primary\ntasks necessary for an autonomous driving system: depth estimation, visual\nodometry, semantic segmentation, motion segmentation, object detection, and\nlens soiling detection. We demonstrate that the jointly trained model performs\nbetter than the respective single task versions. Our multi-task model has a\nshared encoder providing a significant computational advantage and has\nsynergized decoders where tasks support each other. We propose a novel camera\ngeometry based adaptation mechanism to encode the fisheye distortion model both\nat training and inference. This was crucial to enable training on the WoodScape\ndataset, comprised of data from different parts of the world collected by 12\ndifferent cameras mounted on three different cars with different intrinsics and\nviewpoints. Given that bounding boxes is not a good representation for\ndistorted fisheye images, we also extend object detection to use a polygon with\nnon-uniformly sampled vertices. We additionally evaluate our model on standard\nautomotive datasets, namely KITTI and Cityscapes. We obtain the\nstate-of-the-art results on KITTI for depth estimation and pose estimation\ntasks and competitive performance on the other tasks. We perform extensive\nablation studies on various architecture choices and task weighting\nmethodologies. A short video at https://youtu.be/xbSjZ5OfPes provides\nqualitative results.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 10:46:24 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Kumar", "Varun Ravi", ""], ["Yogamani", "Senthil", ""], ["Rashed", "Hazem", ""], ["Sitsu", "Ganesh", ""], ["Witt", "Christian", ""], ["Leang", "Isabelle", ""], ["Milz", "Stefan", ""], ["M\u00e4der", "Patrick", ""]]}, {"id": "2102.07455", "submitter": "Rakesh Dhakhsina Murthy", "authors": "Sree Premkumar, Vimal Premkumar, and Rakesh Dhakshinamurthy", "title": "Video Analytics on IoT devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep Learning (DL) combined with advanced model optimization methods such as\nRC-NN and Edge2Train has enabled offline execution of large networks on the IoT\ndevices. In this paper, we compare the modern Deep Learning (DL) based video\nanalytics approaches with the standard Computer Vision (CV) based approaches\nand finally, discuss the best-suited approach for video analytics on IoT\ndevices.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 11:06:46 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Premkumar", "Sree", ""], ["Premkumar", "Vimal", ""], ["Dhakshinamurthy", "Rakesh", ""]]}, {"id": "2102.07480", "submitter": "Huseyin Uvet", "authors": "Rahmetullah Varol, Sevde Omeroglu, Zeynep Karavelioglu, Gizem Aydemir,\n  Aslihan Karadag, Hanife Ecenur Meco, Gizem Calibasi Kocal, Muhammed Enes\n  Oruc, Gokhan Bora Esmer, Yasemin Basbinar, Huseyin Uvet", "title": "Holographic Cell Stiffness Mapping Using Acoustic Stimulation", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.bio-ph cs.CV q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Accurate assessment of stiffness distribution is essential due to the\ncritical role of single cell mechanobiology in the regulation of many vital\ncellular processes such as proliferation, adhesion, migration, and motility.\nCell stiffness is one of the fundamental mechanical properties of the cell and\nis greatly affected by the intracellular tensional forces, cytoskeletal\nprestress, and cytoskeleton structure. Herein, we propose a novel holographic\nsingle-cell stiffness measurement technique that can obtain the stiffness\ndistribution over a cell membrane at high resolution and in real-time. The\nproposed imaging method coupled with acoustic signals allows us to assess the\ncell stiffness distribution with a low error margin and label-free manner. We\ndemonstrate the proposed technique on HCT116 (Human Colorectal Carcinoma) cells\nand CTC-mimicked HCT116 cells by induction with transforming growth factor-beta\n(TGF-\\b{eta}). Validation studies of the proposed approach were carried out on\ncertified polystyrene microbeads with known stiffness levels. Its performance\nwas evaluated in comparison with the AFM results obtained for the relevant\ncells. When the experimental results were examined, the proposed methodology\nshows utmost performance over average cell stiffness values for HCT116, and\nCTC-mimicked HCT116 cells were found as 1.08 kPa, and 0.88 kPa, respectively.\nThe results confirm that CTC-mimicked HCT116 cells lose their adhesion ability\nto enter the vascular circulation and metastasize. They also exhibit a softer\nstiffness profile compared to adherent forms of the cancer cells. Hence, the\nproposed technique is a significant, reliable, and faster alternative for\nin-vitro cell stiffness characterization tools. It can be utilized for various\napplications where single-cell analysis is required, such as disease modeling,\ndrug testing, diagnostics, and many more.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 11:38:21 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Varol", "Rahmetullah", ""], ["Omeroglu", "Sevde", ""], ["Karavelioglu", "Zeynep", ""], ["Aydemir", "Gizem", ""], ["Karadag", "Aslihan", ""], ["Meco", "Hanife Ecenur", ""], ["Kocal", "Gizem Calibasi", ""], ["Oruc", "Muhammed Enes", ""], ["Esmer", "Gokhan Bora", ""], ["Basbinar", "Yasemin", ""], ["Uvet", "Huseyin", ""]]}, {"id": "2102.07482", "submitter": "Pedro Gomes", "authors": "Pedro Gomes, Silvia Rossi, Laura Toni", "title": "Spatio-temporal Graph-RNN for Point Cloud Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose an end-to-end learning network to predict future\nframes in a point cloud sequence. As main novelty, an initial layer learns\ntopological information of point clouds as geometric features, to form\nrepresentative spatio-temporal neighborhoods. This module is followed by\nmultiple Graph-RNN cells. Each cell learns points dynamics (i.e., RNN states)\nby processing each point jointly with the spatio-temporal neighbouring points.\nWe tested the network performance with a MINST dataset of moving digits, a\nsynthetic human bodies motions and JPEG dynamic bodies datasets. Simulation\nresults demonstrate that our method outperforms baseline ones that neglect\ngeometry features information.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 11:39:40 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 11:43:31 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2021 15:10:02 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Gomes", "Pedro", ""], ["Rossi", "Silvia", ""], ["Toni", "Laura", ""]]}, {"id": "2102.07510", "submitter": "Andrea Sebastiani", "authors": "Pasquale Cascarano, Elena Loli Piccolomini, Elena Morotti, Andrea\n  Sebastiani", "title": "Plug-and-Play gradient-based denoisers applied to CT image enhancement", "comments": "Submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blur and noise corrupting Computed Tomography (CT) images can hide or distort\nsmall but important details, negatively affecting the diagnosis. In this paper,\nwe present a novel gradient-based Plug-and-Play algorithm, constructed on the\nHalf-Quadratic Splitting scheme, and we apply it to restore CT images. In\nparticular, we consider different schemes encompassing external and internal\ndenoisers as priors, defined on the image gradient domain. The internal prior\nis based on the Total Variation functional. The external denoiser is\nimplemented by a deep Convolutional Neural Network (CNN) trained on the\ngradient domain (and not on the image one, as in state-of-the-art works). We\nalso prove a general fixed-point convergence theorem under weak assumptions on\nboth internal and external denoisers. The experiments confirm the effectiveness\nof the proposed framework in restoring blurred noisy CT images, both in\nsimulated and real medical settings. The achieved enhancements in the restored\nimages are really remarkable, if compared to the results of many\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 12:19:28 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 10:10:08 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Cascarano", "Pasquale", ""], ["Piccolomini", "Elena Loli", ""], ["Morotti", "Elena", ""], ["Sebastiani", "Andrea", ""]]}, {"id": "2102.07611", "submitter": "Sobhan Shafiei", "authors": "Sobhan Shafiei, Morteza Babaie, Shivam Kalra, H.R.Tizhoosh", "title": "Colored Kimia Path24 Dataset: Configurations and Benchmarks with Deep\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The Kimia Path24 dataset has been introduced as a classification and\nretrieval dataset for digital pathology. Although it provides multi-class data,\nthe color information has been neglected in the process of extracting patches.\nThe staining information plays a major role in the recognition of tissue\npatterns. To address this drawback, we introduce the color version of Kimia\nPath24 by recreating sample patches from all 24 scans to propose Kimia Path24C.\nWe run extensive experiments to determine the best configuration for selected\npatches. To provide preliminary results for setting a benchmark for the new\ndataset, we utilize VGG16, InceptionV3 and DenseNet-121 model as feature\nextractors. Then, we use these feature vectors to retrieve test patches. The\naccuracy of image retrieval using DenseNet was 95.92% while the highest\naccuracy using InceptionV3 and VGG16 reached 92.45% and 92%, respectively. We\nalso experimented with \"deep barcodes\" and established that with a small loss\nin accuracy (e.g., 93.43% for binarized features for DenseNet instead of 95.92%\nwhen the features themselves are used), the search operations can be\nsignificantly accelerated.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 15:53:41 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Shafiei", "Sobhan", ""], ["Babaie", "Morteza", ""], ["Kalra", "Shivam", ""], ["Tizhoosh", "H. R.", ""]]}, {"id": "2102.07615", "submitter": "Shaheer Ullah Saeed", "authors": "Shaheer U. Saeed, Yunguan Fu, Zachary M. C. Baum, Qianye Yang,\n  Mirabela Rusu, Richard E. Fan, Geoffrey A. Sonn, Dean C. Barratt, Yipeng Hu", "title": "Learning image quality assessment by reinforcing task amenable data\n  selection", "comments": "Accepted at IPMI 2021 (The 27th international conference on\n  Information Processing in Medical Imaging)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a type of image quality assessment as a\ntask-specific measurement, which can be used to select images that are more\namenable to a given target task, such as image classification or segmentation.\nWe propose to train simultaneously two neural networks for image selection and\na target task using reinforcement learning. A controller network learns an\nimage selection policy by maximising an accumulated reward based on the target\ntask performance on the controller-selected validation set, whilst the target\ntask predictor is optimised using the training set. The trained controller is\ntherefore able to reject those images that lead to poor accuracy in the target\ntask. In this work, we show that the controller-predicted image quality can be\nsignificantly different from the task-specific image quality labels that are\nmanually defined by humans. Furthermore, we demonstrate that it is possible to\nlearn effective image quality assessment without using a ``clean'' validation\nset, thereby avoiding the requirement for human labelling of images with\nrespect to their amenability for the task. Using $6712$, labelled and\nsegmented, clinical ultrasound images from $259$ patients, experimental results\non holdout data show that the proposed image quality assessment achieved a mean\nclassification accuracy of $0.94\\pm0.01$ and a mean segmentation Dice of\n$0.89\\pm0.02$, by discarding $5\\%$ and $15\\%$ of the acquired images,\nrespectively. The significantly improved performance was observed for both\ntested tasks, compared with the respective $0.90\\pm0.01$ and $0.82\\pm0.02$ from\nnetworks without considering task amenability. This enables image quality\nfeedback during real-time ultrasound acquisition among many other medical\nimaging applications.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 15:57:20 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Saeed", "Shaheer U.", ""], ["Fu", "Yunguan", ""], ["Baum", "Zachary M. C.", ""], ["Yang", "Qianye", ""], ["Rusu", "Mirabela", ""], ["Fan", "Richard E.", ""], ["Sonn", "Geoffrey A.", ""], ["Barratt", "Dean C.", ""], ["Hu", "Yipeng", ""]]}, {"id": "2102.07624", "submitter": "Matteo Tomei", "authors": "Matteo Tomei, Lorenzo Baraldi, Simone Calderara, Simone Bronzin, Rita\n  Cucchiara", "title": "RMS-Net: Regression and Masking for Soccer Event Spotting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed action spotting task consists in finding the exact\ntimestamp in which an event occurs. This task fits particularly well for soccer\nvideos, where events correspond to salient actions strictly defined by soccer\nrules (a goal occurs when the ball crosses the goal line). In this paper, we\ndevise a lightweight and modular network for action spotting, which can\nsimultaneously predict the event label and its temporal offset using the same\nunderlying features. We enrich our model with two training strategies: the\nfirst one for data balancing and uniform sampling, the second for masking\nambiguous frames and keeping the most discriminative visual cues. When tested\non the SoccerNet dataset and using standard features, our full proposal exceeds\nthe current state of the art by 3 Average-mAP points. Additionally, it reaches\na gain of more than 10 Average-mAP points on the test set when fine-tuned in\ncombination with a strong 2D backbone.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 16:04:18 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Tomei", "Matteo", ""], ["Baraldi", "Lorenzo", ""], ["Calderara", "Simone", ""], ["Bronzin", "Simone", ""], ["Cucchiara", "Rita", ""]]}, {"id": "2102.07680", "submitter": "Max Horn", "authors": "Max Horn, Kumar Shridhar, Elrich Groenewald, Philipp F. M. Baumann", "title": "Translational Equivariance in Kernelizable Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Transformer architectures have show remarkable success, they are bound\nto the computation of all pairwise interactions of input element and thus\nsuffer from limited scalability. Recent work has been successful by avoiding\nthe computation of the complete attention matrix, yet leads to problems down\nthe line. The absence of an explicit attention matrix makes the inclusion of\ninductive biases relying on relative interactions between elements more\nchallenging. An extremely powerful inductive bias is translational\nequivariance, which has been conjectured to be responsible for much of the\nsuccess of Convolutional Neural Networks on image recognition tasks. In this\nwork we show how translational equivariance can be implemented in efficient\nTransformers based on kernelizable attention - Performers. Our experiments\nhighlight that the devised approach significantly improves robustness of\nPerformers to shifts of input images compared to their naive application. This\nrepresents an important step on the path of replacing Convolutional Neural\nNetworks with more expressive Transformer architectures and will help to\nimprove sample efficiency and robustness in this realm.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 17:14:15 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Horn", "Max", ""], ["Shridhar", "Kumar", ""], ["Groenewald", "Elrich", ""], ["Baumann", "Philipp F. M.", ""]]}, {"id": "2102.07682", "submitter": "Aykut Erdem", "authors": "Aysun Kocak, Erkut Erdem and Aykut Erdem", "title": "A Gated Fusion Network for Dynamic Saliency Prediction", "comments": "Project page: https://hucvl.github.io/GFSalNet/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predicting saliency in videos is a challenging problem due to complex\nmodeling of interactions between spatial and temporal information, especially\nwhen ever-changing, dynamic nature of videos is considered. Recently,\nresearchers have proposed large-scale datasets and models that take advantage\nof deep learning as a way to understand what's important for video saliency.\nThese approaches, however, learn to combine spatial and temporal features in a\nstatic manner and do not adapt themselves much to the changes in the video\ncontent. In this paper, we introduce Gated Fusion Network for dynamic saliency\n(GFSalNet), the first deep saliency model capable of making predictions in a\ndynamic way via gated fusion mechanism. Moreover, our model also exploits\nspatial and channel-wise attention within a multi-scale architecture that\nfurther allows for highly accurate predictions. We evaluate the proposed\napproach on a number of datasets, and our experimental analysis demonstrates\nthat it outperforms or is highly competitive with the state of the art.\nImportantly, we show that it has a good generalization ability, and moreover,\nexploits temporal information more effectively via its adaptive fusion scheme.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 17:18:37 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Kocak", "Aysun", ""], ["Erdem", "Erkut", ""], ["Erdem", "Aykut", ""]]}, {"id": "2102.07708", "submitter": "Steve Kench", "authors": "Steve Kench, Samuel J. Cooper", "title": "Generating 3D structures from a 2D slice with GAN-based dimensionality\n  expansion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative adversarial networks (GANs) can be trained to generate 3D image\ndata, which is useful for design optimisation. However, this conventionally\nrequires 3D training data, which is challenging to obtain. 2D imaging\ntechniques tend to be faster, higher resolution, better at phase identification\nand more widely available. Here, we introduce a generative adversarial network\narchitecture, SliceGAN, which is able to synthesise high fidelity 3D datasets\nusing a single representative 2D image. This is especially relevant for the\ntask of material microstructure generation, as a cross-sectional micrograph can\ncontain sufficient information to statistically reconstruct 3D samples. Our\narchitecture implements the concept of uniform information density, which both\nensures that generated volumes are equally high quality at all points in space,\nand that arbitrarily large volumes can be generated. SliceGAN has been\nsuccessfully trained on a diverse set of materials, demonstrating the\nwidespread applicability of this tool. The quality of generated micrographs is\nshown through a statistical comparison of synthetic and real datasets of a\nbattery electrode in terms of key microstructural metrics. Finally, we find\nthat the generation time for a $10^8$ voxel volume is on the order of a few\nseconds, yielding a path for future studies into high-throughput\nmicrostructural optimisation.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 18:46:17 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Kench", "Steve", ""], ["Cooper", "Samuel J.", ""]]}, {"id": "2102.07726", "submitter": "Yazan Qiblawey", "authors": "Yazan Qiblawey, Anas Tahir, Muhammad E. H. Chowdhury, Amith Khandakar,\n  Serkan Kiranyaz, Tawsifur Rahman, Nabil Ibtehaz, Sakib Mahmud, Somaya\n  Al-Madeed, Farayi Musharavati", "title": "Detection and severity classification of COVID-19 in CT images using\n  deep learning", "comments": "9 Figures, 8 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the breakout of coronavirus disease (COVID-19), the computer-aided\ndiagnosis has become a necessity to prevent the spread of the virus. Detecting\nCOVID-19 at an early stage is essential to reduce the mortality risk of the\npatients. In this study, a cascaded system is proposed to segment the lung,\ndetect, localize, and quantify COVID-19 infections from computed tomography\n(CT) images Furthermore, the system classifies the severity of COVID-19 as\nmild, moderate, severe, or critical based on the percentage of infected lungs.\nAn extensive set of experiments were performed using state-of-the-art deep\nEncoder-Decoder Convolutional Neural Networks (ED-CNNs), UNet, and Feature\nPyramid Network (FPN), with different backbone (encoder) structures using the\nvariants of DenseNet and ResNet. The conducted experiments showed the best\nperformance for lung region segmentation with Dice Similarity Coefficient (DSC)\nof 97.19% and Intersection over Union (IoU) of 95.10% using U-Net model with\nthe DenseNet 161 encoder. Furthermore, the proposed system achieved an elegant\nperformance for COVID-19 infection segmentation with a DSC of 94.13% and IoU of\n91.85% using the FPN model with the DenseNet201 encoder. The achieved\nperformance is significantly superior to previous methods for COVID-19 lesion\nlocalization. Besides, the proposed system can reliably localize infection of\nvarious shapes and sizes, especially small infection regions, which are rarely\nconsidered in recent studies. Moreover, the proposed system achieved high\nCOVID-19 detection performance with 99.64% sensitivity and 98.72% specificity.\nFinally, the system was able to discriminate between different severity levels\nof COVID-19 infection over a dataset of 1,110 subjects with sensitivity values\nof 98.3%, 71.2%, 77.8%, and 100% for mild, moderate, severe, and critical\ninfections, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 18:23:34 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Qiblawey", "Yazan", ""], ["Tahir", "Anas", ""], ["Chowdhury", "Muhammad E. H.", ""], ["Khandakar", "Amith", ""], ["Kiranyaz", "Serkan", ""], ["Rahman", "Tawsifur", ""], ["Ibtehaz", "Nabil", ""], ["Mahmud", "Sakib", ""], ["Al-Madeed", "Somaya", ""], ["Musharavati", "Farayi", ""]]}, {"id": "2102.07737", "submitter": "Burhaneddin Yaman", "authors": "Burhaneddin Yaman, Seyed Amir Hossein Hosseini, Mehmet Ak\\c{c}akaya", "title": "Zero-Shot Self-Supervised Learning for MRI Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) has emerged as a powerful tool for accelerated MRI\nreconstruction, but these methods often necessitate a database of fully-sampled\nmeasurements for training. Recent self-supervised and unsupervised learning\napproaches enable training without fully-sampled data. However, a database of\nundersampled measurements may not be available in many scenarios, especially\nfor scans involving contrast or recently developed translational acquisitions.\nMoreover, database-trained models may not generalize well when the unseen\nmeasurements differ in terms of sampling pattern, acceleration rate, SNR, image\ncontrast, and anatomy. Such challenges necessitate a new methodology that can\nenable scan-specific DL MRI reconstruction without any external training\ndatasets. In this work, we propose a zero-shot self-supervised learning\napproach to perform scan-specific accelerated MRI reconstruction to tackle\nthese issues. The proposed approach splits available measurements for each scan\ninto three disjoint sets. Two of these sets are used to enforce data\nconsistency and define loss during training, while the last set is used to\nestablish an early stopping criterion. In the presence of models pre-trained on\na database with different image characteristics, we show that the proposed\napproach can be combined with transfer learning to further improve\nreconstruction quality.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 18:34:38 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 04:47:58 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 17:10:42 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Yaman", "Burhaneddin", ""], ["Hosseini", "Seyed Amir Hossein", ""], ["Ak\u00e7akaya", "Mehmet", ""]]}, {"id": "2102.07753", "submitter": "Jenny Seidenschwarz", "authors": "Jenny Seidenschwarz, Ismail Elezi, Laura Leal-Taix\\'e", "title": "Learning Intra-Batch Connections for Deep Metric Learning", "comments": "Accepted to International Conference on Machine Learning (ICML) 2021,\n  includes non-archival supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of metric learning is to learn a function that maps samples to a\nlower-dimensional space where similar samples lie closer than dissimilar ones.\nParticularly, deep metric learning utilizes neural networks to learn such a\nmapping. Most approaches rely on losses that only take the relations between\npairs or triplets of samples into account, which either belong to the same\nclass or two different classes. However, these methods do not explore the\nembedding space in its entirety. To this end, we propose an approach based on\nmessage passing networks that takes all the relations in a mini-batch into\naccount. We refine embedding vectors by exchanging messages among all samples\nin a given batch allowing the training process to be aware of its overall\nstructure. Since not all samples are equally important to predict a decision\nboundary, we use an attention mechanism during message passing to allow samples\nto weigh the importance of each neighbor accordingly. We achieve\nstate-of-the-art results on clustering and image retrieval on the CUB-200-2011,\nCars196, Stanford Online Products, and In-Shop Clothes datasets. To facilitate\nfurther research, we make available the code and the models at\nhttps://github.com/dvl-tum/intra_batch_connections.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 18:50:00 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 07:10:54 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 08:56:31 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Seidenschwarz", "Jenny", ""], ["Elezi", "Ismail", ""], ["Leal-Taix\u00e9", "Laura", ""]]}, {"id": "2102.07764", "submitter": "Daniel Lenton", "authors": "Daniel Lenton, Stephen James, Ronald Clark, Andrew J. Davison", "title": "End-to-End Egospheric Spatial Memory", "comments": "Conference paper at ICLR 2021. Implementation:\n  https://github.com/ivy-dl/memory Project page: https://djl11.github.io/ESM/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spatial memory, or the ability to remember and recall specific locations and\nobjects, is central to autonomous agents' ability to carry out tasks in real\nenvironments. However, most existing artificial memory modules are not very\nadept at storing spatial information. We propose a parameter-free module,\nEgospheric Spatial Memory (ESM), which encodes the memory in an ego-sphere\naround the agent, enabling expressive 3D representations. ESM can be trained\nend-to-end via either imitation or reinforcement learning, and improves both\ntraining efficiency and final performance against other memory baselines on\nboth drone and manipulator visuomotor control tasks. The explicit egocentric\ngeometry also enables us to seamlessly combine the learned controller with\nother non-learned modalities, such as local obstacle avoidance. We further show\napplications to semantic segmentation on the ScanNet dataset, where ESM\nnaturally combines image-level and map-level inference modalities. Through our\nbroad set of experiments, we show that ESM provides a general computation graph\nfor embodied spatial reasoning, and the module forms a bridge between real-time\nmapping systems and differentiable memory architectures. Implementation at:\nhttps://github.com/ivy-dl/memory.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 18:59:07 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 18:56:39 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Lenton", "Daniel", ""], ["James", "Stephen", ""], ["Clark", "Ronald", ""], ["Davison", "Andrew J.", ""]]}, {"id": "2102.07799", "submitter": "Sam Sattarzadeh", "authors": "Mahesh Sudhakar, Sam Sattarzadeh, Konstantinos N. Plataniotis,\n  Jongseong Jang, Yeonjeong Jeong, Hyunwoo Kim", "title": "Ada-SISE: Adaptive Semantic Input Sampling for Efficient Explanation of\n  Convolutional Neural Networks", "comments": "5 pages, 4 figures. Accepted in 2021 IEEE International Conference on\n  Acoustics, Speech, and Signal Processing (ICASSP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainable AI (XAI) is an active research area to interpret a neural\nnetwork's decision by ensuring transparency and trust in the task-specified\nlearned models. Recently, perturbation-based model analysis has shown better\ninterpretation, but backpropagation techniques are still prevailing because of\ntheir computational efficiency. In this work, we combine both approaches as a\nhybrid visual explanation algorithm and propose an efficient interpretation\nmethod for convolutional neural networks. Our method adaptively selects the\nmost critical features that mainly contribute towards a prediction to probe the\nmodel by finding the activated features. Experimental results show that the\nproposed method can reduce the execution time up to 30% while enhancing\ncompetitive interpretability without compromising the quality of explanation\ngenerated.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 19:10:00 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Sudhakar", "Mahesh", ""], ["Sattarzadeh", "Sam", ""], ["Plataniotis", "Konstantinos N.", ""], ["Jang", "Jongseong", ""], ["Jeong", "Yeonjeong", ""], ["Kim", "Hyunwoo", ""]]}, {"id": "2102.07805", "submitter": "Sam Sattarzadeh", "authors": "Sam Sattarzadeh, Mahesh Sudhakar, Konstantinos N. Plataniotis,\n  Jongseong Jang, Yeonjeong Jeong, Hyunwoo Kim", "title": "Integrated Grad-CAM: Sensitivity-Aware Visual Explanation of Deep\n  Convolutional Networks via Integrated Gradient-Based Scoring", "comments": "5 pages, 3 figures Accepted in 2021 IEEE International Conference on\n  Acoustics, Speech, and Signal Processing (ICASSP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualizing the features captured by Convolutional Neural Networks (CNNs) is\none of the conventional approaches to interpret the predictions made by these\nmodels in numerous image recognition applications. Grad-CAM is a popular\nsolution that provides such a visualization by combining the activation maps\nobtained from the model. However, the average gradient-based terms deployed in\nthis method underestimates the contribution of the representations discovered\nby the model to its predictions. Addressing this problem, we introduce a\nsolution to tackle this issue by computing the path integral of the\ngradient-based terms in Grad-CAM. We conduct a thorough analysis to demonstrate\nthe improvement achieved by our method in measuring the importance of the\nextracted representations for the CNN's predictions, which yields to our\nmethod's administration in object localization and model interpretation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 19:21:46 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Sattarzadeh", "Sam", ""], ["Sudhakar", "Mahesh", ""], ["Plataniotis", "Konstantinos N.", ""], ["Jang", "Jongseong", ""], ["Jeong", "Yeonjeong", ""], ["Kim", "Hyunwoo", ""]]}, {"id": "2102.07827", "submitter": "Philip Schniter", "authors": "Michael Wharton, Anne M. Pavy, and Philip Schniter", "title": "Phase-Modulated Radar Waveform Classification Using Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of classifying noisy, phase-modulated radar\nwaveforms. While traditionally this has been accomplished by applying classical\nmachine-learning algorithms on hand-crafted features, it has recently been\nshown that better performance can be attained by training deep neural networks\n(DNNs) to classify raw I/Q waveforms. However, existing DNNs assume\ntime-synchronized waveforms and do not exploit complex-valued signal structure,\nand many aspects of the their DNN design and training are suboptimal. We\ndemonstrate that, with an improved DNN architecture and training procedure, it\nis possible to reduce classification error from 18% to 0.14% on asynchronous\nwaveforms from the SIDLE dataset. Unlike past work, we furthermore demonstrate\nthat accurate classification of multiple overlapping waveforms is also\npossible, by achieving 4.0% error with 4 asynchronous SIDLE waveforms.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 20:07:17 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Wharton", "Michael", ""], ["Pavy", "Anne M.", ""], ["Schniter", "Philip", ""]]}, {"id": "2102.07848", "submitter": "Touqeer Ahmad", "authors": "Akshay Raj Dhamija, Touqeer Ahmad, Jonathan Schwan, Mohsen Jafarzadeh,\n  Chunchun Li, Terrance E. Boult", "title": "Self-Supervised Features Improve Open-World Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper identifies the flaws in existing open-world learning approaches\nand attempts to provide a complete picture in the form of \\textbf{True\nOpen-World Learning}. We accomplish this by proposing a comprehensive\ngeneralize-able open-world learning protocol capable of evaluating various\ncomponents of open-world learning in an operational setting. We argue that in\ntrue open-world learning, the underlying feature representation should be\nlearned in a self-supervised manner. Under this self-supervised feature\nrepresentation, we introduce the problem of detecting unknowns as samples\nbelonging to Out-of-Label space. We differentiate between Out-of-Label space\ndetection and the conventional Out-of-Distribution detection depending upon\nwhether the unknowns being detected belong to the native-world (same as feature\nrepresentation) or a new-world, respectively. Our unifying open-world learning\nframework combines three individual research dimensions, which typically have\nbeen explored independently, i.e., Incremental Learning, Out-of-Distribution\ndetection and Open-World Learning. Starting from a self-supervised feature\nspace, an open-world learner has the ability to adapt and specialize its\nfeature space to the classes in each incremental phase and hence perform better\nwithout incurring any significant overhead, as demonstrated by our experimental\nresults. The incremental learning component of our pipeline provides the new\nstate-of-the-art on established ImageNet-100 protocol. We also demonstrate the\nadaptability of our approach by showing how it can work as a plug-in with any\nof the self-supervised feature representation methods.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 21:03:05 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 15:28:55 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Dhamija", "Akshay Raj", ""], ["Ahmad", "Touqeer", ""], ["Schwan", "Jonathan", ""], ["Jafarzadeh", "Mohsen", ""], ["Li", "Chunchun", ""], ["Boult", "Terrance E.", ""]]}, {"id": "2102.07880", "submitter": "Anjali Balagopal", "authors": "Anjali Balagopal, Howard Morgan, Michael Dohopoloski, Ramsey\n  Timmerman, Jie Shan, Daniel F. Heitjan, Wei Liu, Dan Nguyen, Raquibul Hannan,\n  Aurelie Garant, Neil Desai, Steve Jiang", "title": "PSA-Net: Deep Learning based Physician Style-Aware Segmentation Network\n  for Post-Operative Prostate Cancer Clinical Target Volume", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of medical images with DL algorithms has proven to be\nhighly successful. With most of these algorithms, inter-observer variation is\nan acknowledged problem, leading to sub-optimal results. This problem is even\nmore significant in post-operative clinical target volume (post-op CTV)\nsegmentation due to the absence of macroscopic visual tumor in the image. This\nstudy, using post-op CTV segmentation as the test bed, tries to determine if\nphysician styles are consistent and learnable, if there is an impact of\nphysician styles on treatment outcome and toxicity; and how to explicitly deal\nwith physician styles in DL algorithms to facilitate its clinical acceptance. A\nclassifier is trained to identify which physician has contoured the CTV from\njust the contour and corresponding CT scan, to determine if physician styles\nare consistent and learnable. Next, we evaluate if adapting automatic\nsegmentation to physician styles would be clinically feasible based on a lack\nof difference between outcomes. For modeling different physician styles of CTV\nsegmentation, a concept called physician style-aware (PSA) segmentation is\nproposed which is an encoder-multidecoder network trained with perceptual loss.\nWith the proposed physician style-aware network (PSA-Net), Dice similarity\ncoefficient (DSC) accuracy increases on an average of 3.4% for all physicians\nfrom a general model that is not style adapted. We show that stylistic\ncontouring variations also exist between institutions that follow the same\nsegmentation guidelines and show the effectiveness of the proposed method in\nadapting to new institutional styles. We observed an accuracy improvement of 5%\nin terms of DSC when adapting to the style of a separate institution.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 22:42:52 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Balagopal", "Anjali", ""], ["Morgan", "Howard", ""], ["Dohopoloski", "Michael", ""], ["Timmerman", "Ramsey", ""], ["Shan", "Jie", ""], ["Heitjan", "Daniel F.", ""], ["Liu", "Wei", ""], ["Nguyen", "Dan", ""], ["Hannan", "Raquibul", ""], ["Garant", "Aurelie", ""], ["Desai", "Neil", ""], ["Jiang", "Steve", ""]]}, {"id": "2102.07887", "submitter": "Bowen Pan", "authors": "Bowen Pan, Rameswar Panda, Camilo Fosco, Chung-Ching Lin, Alex\n  Andonian, Yue Meng, Kate Saenko, Aude Oliva, Rogerio Feris", "title": "VA-RED$^2$: Video Adaptive Redundancy Reduction", "comments": "Accepted in ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing inference on deep learning models for videos remains a challenge\ndue to the large amount of computational resources required to achieve robust\nrecognition. An inherent property of real-world videos is the high correlation\nof information across frames which can translate into redundancy in either\ntemporal or spatial feature maps of the models, or both. The type of redundant\nfeatures depends on the dynamics and type of events in the video: static videos\nhave more temporal redundancy while videos focusing on objects tend to have\nmore channel redundancy. Here we present a redundancy reduction framework,\ntermed VA-RED$^2$, which is input-dependent. Specifically, our VA-RED$^2$\nframework uses an input-dependent policy to decide how many features need to be\ncomputed for temporal and channel dimensions. To keep the capacity of the\noriginal model, after fully computing the necessary features, we reconstruct\nthe remaining redundant features from those using cheap linear operations. We\nlearn the adaptive policy jointly with the network weights in a differentiable\nway with a shared-weight mechanism, making it highly efficient. Extensive\nexperiments on multiple video datasets and different visual tasks show that our\nframework achieves $20\\% - 40\\%$ reduction in computation (FLOPs) when compared\nto state-of-the-art methods without any performance loss. Project page:\nhttp://people.csail.mit.edu/bpan/va-red/.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 22:57:52 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Pan", "Bowen", ""], ["Panda", "Rameswar", ""], ["Fosco", "Camilo", ""], ["Lin", "Chung-Ching", ""], ["Andonian", "Alex", ""], ["Meng", "Yue", ""], ["Saenko", "Kate", ""], ["Oliva", "Aude", ""], ["Feris", "Rogerio", ""]]}, {"id": "2102.07899", "submitter": "Fanwei Kong", "authors": "Fanwei Kong, Nathan Wilson, Shawn C. Shadden", "title": "A Deep-Learning Approach For Direct Whole-Heart Mesh Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated construction of surface geometries of cardiac structures from\nvolumetric medical images is important for a number of clinical applications.\nWhile deep-learning based approaches have demonstrated promising reconstruction\nprecision, these approaches have mostly focused on voxel-wise segmentation\nfollowed by surface reconstruction and post-processing techniques. However,\nsuch approaches suffer from a number of limitations including disconnected\nregions or incorrect surface topology due to erroneous segmentation and\nstair-case artifacts due to limited segmentation resolution. We propose a novel\ndeep-learning-based approach that directly predicts whole heart surface meshes\nfrom volumetric CT and MR image data. Our approach leverages a graph\nconvolutional neural network to predict deformation on mesh vertices from a\npre-defined mesh template to reconstruct multiple anatomical structures in a 3D\nimage volume. Our method demonstrated promising performance of generating\nhigh-resolution and high-quality whole heart reconstructions and outperformed\nprior deep-learning based methods on both CT and MR data in terms of precision\nand surface quality. Furthermore, our method can more efficiently produce\ntemporally-consistent and feature-corresponding surface mesh predictions for\nheart motion from CT or MR cine sequences, and therefore can potentially be\napplied for efficiently constructing 4D whole heart dynamics.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 00:39:43 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Kong", "Fanwei", ""], ["Wilson", "Nathan", ""], ["Shadden", "Shawn C.", ""]]}, {"id": "2102.07911", "submitter": "Zuohui Chen", "authors": "Zuohui Chen, Qing Yuan, Xujie Song, Cheng Chen, Dan Zhang, Yun Xiang,\n  Ruigang Liu, and Qi Xuan", "title": "MITNet: GAN Enhanced Magnetic Induction Tomography Based on Complex CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic induction tomography (MIT) is an efficient solution for long-term\nbrain disease monitoring, which focuses on reconstructing bio-impedance\ndistribution inside the human brain using non-intrusive electromagnetic fields.\nHowever, high-quality brain image reconstruction remains challenging since\nreconstructing images from the measured weak signals is a highly non-linear and\nill-conditioned problem. In this work, we propose a generative adversarial\nnetwork (GAN) enhanced MIT technique, named MITNet, based on a complex\nconvolutional neural network (CNN). The experimental results on the real-world\ndataset validate the performance of our technique, which outperforms the\nstate-of-art method by 25.27%.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 01:42:31 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Chen", "Zuohui", ""], ["Yuan", "Qing", ""], ["Song", "Xujie", ""], ["Chen", "Cheng", ""], ["Zhang", "Dan", ""], ["Xiang", "Yun", ""], ["Liu", "Ruigang", ""], ["Xuan", "Qi", ""]]}, {"id": "2102.07925", "submitter": "Dingkang Liang", "authors": "Dingkang Liang, Wei Xu, Yingying Zhu, Yu Zhou", "title": "Focal Inverse Distance Transform Maps for Crowd Localization and\n  Counting in Dense Crowd", "comments": "The code and models are available at\n  https://github.com/dk-liang/FIDTM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a novel map for dense crowd localization and crowd\ncounting. Most crowd counting methods utilize convolution neural networks (CNN)\nto regress a density map, achieving significant progress recently. However,\nthese regression-based methods are often unable to provide a precise location\nfor each person, attributed to two crucial reasons: 1) the density map consists\nof a series of blurry Gaussian blobs, 2) severe overlaps exist in the dense\nregion of the density map. To tackle this issue, we propose a novel Focal\nInverse Distance Transform (FIDT) map for crowd localization and counting.\nCompared with the density maps, the FIDT maps accurately describe the people's\nlocation, without overlap between nearby heads in dense regions. We\nsimultaneously implement crowd localization and counting by regressing the FIDT\nmap. Extensive experiments demonstrate that the proposed method outperforms\nstate-of-the-art localization-based methods in crowd localization tasks,\nachieving very competitive performance compared with the regression-based\nmethods in counting tasks. In addition, the proposed method presents strong\nrobustness for the negative samples and extremely dense scenes, which further\nverifies the effectiveness of the FIDT map. The code and models are available\nat https://github.com/dk-liang/FIDTM.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 02:25:55 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 11:45:10 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Liang", "Dingkang", ""], ["Xu", "Wei", ""], ["Zhu", "Yingying", ""], ["Zhou", "Yu", ""]]}, {"id": "2102.07943", "submitter": "Zhao Kang", "authors": "Zhao Kang, Zhiping Lin, Xiaofeng Zhu, Wenbo Xu", "title": "Structured Graph Learning for Scalable Subspace Clustering: From\n  Single-view to Multi-view", "comments": null, "journal-ref": "IEEE Transactions on Cybernetics 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Graph-based subspace clustering methods have exhibited promising performance.\nHowever, they still suffer some of these drawbacks: encounter the expensive\ntime overhead, fail in exploring the explicit clusters, and cannot generalize\nto unseen data points. In this work, we propose a scalable graph learning\nframework, seeking to address the above three challenges simultaneously.\nSpecifically, it is based on the ideas of anchor points and bipartite graph.\nRather than building a $n\\times n$ graph, where $n$ is the number of samples,\nwe construct a bipartite graph to depict the relationship between samples and\nanchor points. Meanwhile, a connectivity constraint is employed to ensure that\nthe connected components indicate clusters directly. We further establish the\nconnection between our method and the K-means clustering. Moreover, a model to\nprocess multi-view data is also proposed, which is linear scaled with respect\nto $n$. Extensive experiments demonstrate the efficiency and effectiveness of\nour approach with respect to many state-of-the-art clustering methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 03:46:11 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Kang", "Zhao", ""], ["Lin", "Zhiping", ""], ["Zhu", "Xiaofeng", ""], ["Xu", "Wenbo", ""]]}, {"id": "2102.07944", "submitter": "Davis Gilton", "authors": "Davis Gilton, Gregory Ongie, Rebecca Willett", "title": "Deep Equilibrium Architectures for Inverse Problems in Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent efforts on solving inverse problems in imaging via deep neural\nnetworks use architectures inspired by a fixed number of iterations of an\noptimization method. The number of iterations is typically quite small due to\ndifficulties in training networks corresponding to more iterations; the\nresulting solvers cannot be run for more iterations at test time without\nincurring significant errors. This paper describes an alternative approach\ncorresponding to an infinite number of iterations, yielding a consistent\nimprovement in reconstruction accuracy above state-of-the-art alternatives and\nwhere the computational budget can be selected at test time to optimize\ncontext-dependent trade-offs between accuracy and computation. The proposed\napproach leverages ideas from Deep Equilibrium Models, where the fixed-point\niteration is constructed to incorporate a known forward model and insights from\nclassical optimization-based reconstruction methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 03:49:58 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 03:14:53 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Gilton", "Davis", ""], ["Ongie", "Gregory", ""], ["Willett", "Rebecca", ""]]}, {"id": "2102.07951", "submitter": "Boulbaba Ben Amor Prof.", "authors": "Boulbaba Ben Amor, Sylvain Arguill\\`ere and Ling Shao", "title": "ResNet-LDDMM: Advancing the LDDMM Framework Using Deep Residual Networks", "comments": "Submitted to T-PAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In deformable registration, the geometric framework - large deformation\ndiffeomorphic metric mapping or LDDMM, in short - has inspired numerous\ntechniques for comparing, deforming, averaging and analyzing shapes or images.\nGrounded in flows, which are akin to the equations of motion used in fluid\ndynamics, LDDMM algorithms solve the flow equation in the space of plausible\ndeformations, i.e. diffeomorphisms. In this work, we make use of deep residual\nneural networks to solve the non-stationary ODE (flow equation) based on a\nEuler's discretization scheme. The central idea is to represent time-dependent\nvelocity fields as fully connected ReLU neural networks (building blocks) and\nderive optimal weights by minimizing a regularized loss function. Computing\nminimizing paths between deformations, thus between shapes, turns to find\noptimal network parameters by back-propagating over the intermediate building\nblocks. Geometrically, at each time step, ResNet-LDDMM searches for an optimal\npartition of the space into multiple polytopes, and then computes optimal\nvelocity vectors as affine transformations on each of these polytopes. As a\nresult, different parts of the shape, even if they are close (such as two\nfingers of a hand), can be made to belong to different polytopes, and therefore\nbe moved in different directions without costing too much energy. Importantly,\nwe show how diffeomorphic transformations, or more precisely bilipshitz\ntransformations, are predicted by our algorithm. We illustrate these ideas on\ndiverse registration problems of 3D shapes under complex topology-preserving\ntransformations. We thus provide essential foundations for more advanced shape\nvariability analysis under a novel joint geometric-neural networks\nRiemannian-like framework, i.e. ResNet-LDDMM.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 04:07:13 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Amor", "Boulbaba Ben", ""], ["Arguill\u00e8re", "Sylvain", ""], ["Shao", "Ling", ""]]}, {"id": "2102.07954", "submitter": "Dilin Wang", "authors": "Dilin Wang, Chengyue Gong, Meng Li, Qiang Liu, Vikas Chandra", "title": "AlphaNet: Improved Training of Supernets with Alpha-Divergence", "comments": "International Conference on Machine Learning (ICML) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Weight-sharing neural architecture search (NAS) is an effective technique for\nautomating efficient neural architecture design. Weight-sharing NAS builds a\nsupernet that assembles all the architectures as its sub-networks and jointly\ntrains the supernet with the sub-networks. The success of weight-sharing NAS\nheavily relies on distilling the knowledge of the supernet to the sub-networks.\nHowever, we find that the widely used distillation divergence, i.e., KL\ndivergence, may lead to student sub-networks that over-estimate or\nunder-estimate the uncertainty of the teacher supernet, leading to inferior\nperformance of the sub-networks. In this work, we propose to improve the\nsupernet training with a more generalized alpha-divergence. By adaptively\nselecting the alpha-divergence, we simultaneously prevent the over-estimation\nor under-estimation of the uncertainty of the teacher model. We apply the\nproposed alpha-divergence based supernets training to both slimmable neural\nnetworks and weight-sharing NAS, and demonstrate significant improvements.\nSpecifically, our discovered model family, AlphaNet, outperforms prior-art\nmodels on a wide range of FLOPs regimes, including BigNAS, Once-for-All\nnetworks, and AttentiveNAS. We achieve ImageNet top-1 accuracy of 80.0% with\nonly 444M FLOPs. Our code and pretrained models are available at\nhttps://github.com/facebookresearch/AlphaNet.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 04:23:55 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 17:19:59 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Wang", "Dilin", ""], ["Gong", "Chengyue", ""], ["Li", "Meng", ""], ["Liu", "Qiang", ""], ["Chandra", "Vikas", ""]]}, {"id": "2102.07968", "submitter": "Lequan Chen", "authors": "Lequan Chen, Wei Xie, Zhigang Tu, Jinglei Guo, Yaping Tao, Xinming\n  Wang", "title": "Multi-Attribute Enhancement Network for Person Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person Search is designed to jointly solve the problems of Person Detection\nand Person Re-identification (Re-ID), in which the target person will be\nlocated in a large number of uncut images. Over the past few years, Person\nSearch based on deep learning has made great progress. Visual character\nattributes play a key role in retrieving the query person, which has been\nexplored in Re-ID but has been ignored in Person Search. So, we introduce\nattribute learning into the model, allowing the use of attribute features for\nretrieval task. Specifically, we propose a simple and effective model called\nMulti-Attribute Enhancement (MAE) which introduces attribute tags to learn\nlocal features. In addition to learning the global representation of\npedestrians, it also learns the local representation, and combines the two\naspects to learn robust features to promote the search performance.\nAdditionally, we verify the effectiveness of our module on the existing\nbenchmark dataset, CUHK-SYSU and PRW. Ultimately, our model achieves\nstate-of-the-art among end-to-end methods, especially reaching 91.8% of mAP and\n93.0% of rank-1 on CUHK-SYSU.Codes and models are available at\nhttps://github.com/chenlq123/MAE.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 05:43:47 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 03:43:34 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Chen", "Lequan", ""], ["Xie", "Wei", ""], ["Tu", "Zhigang", ""], ["Guo", "Jinglei", ""], ["Tao", "Yaping", ""], ["Wang", "Xinming", ""]]}, {"id": "2102.07973", "submitter": "\\c{C}a\\u{g}lar Aytekin", "authors": "Caglar Aytekin, Sakari Alenius, Dmytro Paliy and Juuso Gren", "title": "A Sub-band Approach to Deep Denoising Wavelet Networks and a\n  Frequency-adaptive Loss for Perceptual Quality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we propose two contributions to neural network based\ndenoising. First, we propose applying separate convolutional layers to each\nsub-band of discrete wavelet transform (DWT) as opposed to the common usage of\nDWT which concatenates all sub-bands and applies a single convolution layer. We\nshow that our approach to using DWT in neural networks improves the accuracy\nnotably, due to keeping the sub-band order uncorrupted prior to inverse DWT.\nOur second contribution is a denoising loss based on top k-percent of errors in\nfrequency domain. A neural network trained with this loss, adaptively focuses\non frequencies that it fails to recover the most in each iteration. We show\nthat this loss results into better perceptual quality by providing an image\nthat is more balanced in terms of the errors in frequency components.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 06:35:42 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Aytekin", "Caglar", ""], ["Alenius", "Sakari", ""], ["Paliy", "Dmytro", ""], ["Gren", "Juuso", ""]]}, {"id": "2102.07975", "submitter": "Jayadeva", "authors": "Kartikeya Badola, Sameer Ambekar, Himanshu Pant, Sumit Soman, Anuradha\n  Sural, Rajiv Narang, Suresh Chandra and Jayadeva", "title": "Twin Augmented Architectures for Robust Classification of COVID-19 Chest\n  X-Ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The gold standard for COVID-19 is RT-PCR, testing facilities for which are\nlimited and not always optimally distributed. Test results are delayed, which\nimpacts treatment. Expert radiologists, one of whom is a co-author, are able to\ndiagnose COVID-19 positivity from Chest X-Rays (CXR) and CT scans, that can\nfacilitate timely treatment. Such diagnosis is particularly valuable in\nlocations lacking radiologists with sufficient expertise and familiarity with\nCOVID-19 patients. This paper has two contributions. One, we analyse literature\non CXR based COVID-19 diagnosis. We show that popular choices of dataset\nselection suffer from data homogeneity, leading to misleading results. We\ncompile and analyse a viable benchmark dataset from multiple existing\nheterogeneous sources. Such a benchmark is important for realistically testing\nmodels. Our second contribution relates to learning from imbalanced data.\nDatasets for COVID X-Ray classification face severe class imbalance, since most\nsubjects are COVID -ve. Twin Support Vector Machines (Twin SVM) and Twin Neural\nNetworks (Twin NN) have, in recent years, emerged as effective ways of handling\nskewed data. We introduce a state-of-the-art technique, termed as Twin\nAugmentation, for modifying popular pre-trained deep learning models. Twin\nAugmentation boosts the performance of a pre-trained deep neural network\nwithout requiring re-training. Experiments show, that across a multitude of\nclassifiers, Twin Augmentation is very effective in boosting the performance of\ngiven pre-trained model for classification in imbalanced settings.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 06:50:17 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Badola", "Kartikeya", ""], ["Ambekar", "Sameer", ""], ["Pant", "Himanshu", ""], ["Soman", "Sumit", ""], ["Sural", "Anuradha", ""], ["Narang", "Rajiv", ""], ["Chandra", "Suresh", ""], ["Jayadeva", "", ""]]}, {"id": "2102.07976", "submitter": "Risheng Liu", "authors": "Risheng Liu, Pan Mu, Xiaoming Yuan, Shangzhi Zeng, Jin Zhang", "title": "A Generic Descent Aggregation Framework for Gradient-based Bi-level\n  Optimization", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.DS math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, gradient-based methods for solving bi-level optimization\ntasks have drawn a great deal of interest from the machine learning community.\nHowever, to calculate the gradient of the best response, existing research\nalways relies on the singleton of the lower-level solution set (a.k.a.,\nLower-Level Singleton, LLS). In this work, by formulating bi-level models from\nan optimistic bi-level viewpoint, we first establish a novel Bi-level Descent\nAggregation (BDA) framework, which aggregates hierarchical objectives of both\nupper level and lower level. The flexibility of our framework benefits from the\nembedded replaceable task-tailored iteration dynamics modules, thereby\ncapturing a wide range of bi-level learning tasks. Theoretically, we derive a\nnew methodology to prove the convergence of BDA framework without the LLS\nrestriction. Besides, the new proof recipe we propose is also engaged to\nimprove the convergence results of conventional gradient-based bi-level methods\nunder the LLS simplification. Furthermore, we employ a one-stage technique to\naccelerate the back-propagation calculation in a numerical manner. Extensive\nexperiments justify our theoretical results and demonstrate the superiority of\nthe proposed algorithm for hyper-parameter optimization and meta-learning\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 06:58:12 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Liu", "Risheng", ""], ["Mu", "Pan", ""], ["Yuan", "Xiaoming", ""], ["Zeng", "Shangzhi", ""], ["Zhang", "Jin", ""]]}, {"id": "2102.07978", "submitter": "Ning Li", "authors": "Ning Li, Tao Li, Chunyu Hu, Kai Wang, Hong Kang", "title": "A Benchmark of Ocular Disease Intelligent Recognition: One Shot for\n  Multi-disease Detection", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In ophthalmology, early fundus screening is an economic and effective way to\nprevent blindness caused by ophthalmic diseases. Clinically, due to the lack of\nmedical resources, manual diagnosis is time-consuming and may delay the\ncondition. With the development of deep learning, some researches on ophthalmic\ndiseases have achieved good results, however, most of them are just based on\none disease. During fundus screening, ophthalmologists usually give diagnoses\nof multi-disease on binocular fundus image, so we release a dataset with 8\ndiseases to meet the real medical scene, which contains 10,000 fundus images\nfrom both eyes of 5,000 patients. We did some benchmark experiments on it\nthrough some state-of-the-art deep neural networks. We found simply increasing\nthe scale of network cannot bring good results for multi-disease\nclassification, and a well-structured feature fusion method combines\ncharacteristics of multi-disease is needed. Through this work, we hope to\nadvance the research of related fields.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 07:00:49 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Li", "Ning", ""], ["Li", "Tao", ""], ["Hu", "Chunyu", ""], ["Wang", "Kai", ""], ["Kang", "Hong", ""]]}, {"id": "2102.07981", "submitter": "Mingbao Lin", "authors": "Mingbao Lin, Rongrong Ji, Zihan Xu, Baochang Zhang, Fei Chao,\n  Mingliang Xu, Chia-Wen Lin, Ling Shao", "title": "SiMaN: Sign-to-Magnitude Network Binarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Binary neural networks (BNNs) have attracted broad research interest due to\ntheir efficient storage and computational ability. Nevertheless, a significant\nchallenge of BNNs lies in handling discrete constraints while ensuring bit\nentropy maximization, which typically makes their weight optimization very\ndifficult. Existing methods relax the learning using the sign function, which\nsimply encodes positive weights into +1s, and -1s otherwise. Alternatively, we\nformulate an angle alignment objective to constrain the weight binarization to\n{0,+1} to solve the challenge. In this paper, we show that our weight\nbinarization provides an analytical solution by encoding high-magnitude weights\ninto +1s, and 0s otherwise. Therefore, a high-quality discrete solution is\nestablished in a computationally efficient manner without the sign function. We\nprove that the learned weights of binarized networks roughly follow a Laplacian\ndistribution that does not allow entropy maximization, and further demonstrate\nthat it can be effectively solved by simply removing the $\\ell_2$\nregularization during network training. Our method, dubbed sign-to-magnitude\nnetwork binarization (SiMaN), is evaluated on CIFAR-10 and ImageNet,\ndemonstrating its superiority over the sign-based state-of-the-arts. Our source\ncode, experimental settings, training logs and binary models are available at\nhttps://github.com/lmbxmu/SiMaN.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 07:03:51 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 12:51:21 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Lin", "Mingbao", ""], ["Ji", "Rongrong", ""], ["Xu", "Zihan", ""], ["Zhang", "Baochang", ""], ["Chao", "Fei", ""], ["Xu", "Mingliang", ""], ["Lin", "Chia-Wen", ""], ["Shao", "Ling", ""]]}, {"id": "2102.07989", "submitter": "Wei Li", "authors": "Jianing Zhang, Wei Li, Honggang Gou, Lu Fang, Ruigang Yang", "title": "LEAD: LiDAR Extender for Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D perception using sensors under vehicle industrial standard is the rigid\ndemand in autonomous driving. MEMS LiDAR emerges with irresistible trend due to\nits lower cost, more robust, and meeting the mass-production standards.\nHowever, it suffers small field of view (FoV), slowing down the step of its\npopulation. In this paper, we propose LEAD, i.e., LiDAR Extender for Autonomous\nDriving, to extend the MEMS LiDAR by coupled image w.r.t both FoV and range. We\npropose a multi-stage propagation strategy based on depth distributions and\nuncertainty map, which shows effective propagation ability. Moreover, our depth\noutpainting/propagation network follows a teacher-student training fashion,\nwhich transfers depth estimation ability to depth completion network without\nany scale error passed. To validate the LiDAR extension quality, we utilize a\nhigh-precise laser scanner to generate a ground-truth dataset. Quantitative and\nqualitative evaluations show that our scheme outperforms SOTAs with a large\nmargin. We believe the proposed LEAD along with the dataset would benefit the\ncommunity w.r.t depth researches.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 07:35:34 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Zhang", "Jianing", ""], ["Li", "Wei", ""], ["Gou", "Honggang", ""], ["Fang", "Lu", ""], ["Yang", "Ruigang", ""]]}, {"id": "2102.07997", "submitter": "Li Rui", "authors": "Rui Li, Shunyi Zheng, Ce Zhang, Chenxi Duan, Libo Wang", "title": "A2-FPN for Semantic Segmentation of Fine-Resolution Remotely Sensed\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation using fine-resolution remotely sensed images plays a\ncritical role in many practical applications, such as urban planning,\nenvironmental protection, natural and anthropogenic landscape monitoring, etc.\nHowever, the automation of semantic segmentation, i.e., automatic\ncategorization/labeling and segmentation is still a challenging task,\nparticularly for fine-resolution images with huge spatial and spectral\ncomplexity. Addressing such a problem represents an exciting research field,\nwhich paves the way for scene-level landscape pattern analysis and decision\nmaking. In this paper, we propose an approach for automatic land segmentation\nbased on the Feature Pyramid Network (FPN). As a classic architecture, FPN can\nbuild a feature pyramid with high-level semantics throughout. However,\nintrinsic defects in feature extraction and fusion hinder FPN from further\naggregating more discriminative features. Hence, we propose an Attention\nAggregation Module (AAM) to enhance multi-scale feature learning through\nattention-guided feature aggregation. Based on FPN and AAM, a novel framework\nnamed Attention Aggregation Feature Pyramid Network (A2-FPN) is developed for\nsemantic segmentation of fine-resolution remotely sensed images. Extensive\nexperiments conducted on three datasets demonstrate the effectiveness of our A2\n-FPN in segmentation accuracy. Code is available at\nhttps://github.com/lironui/A2-FPN.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 07:54:19 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 01:00:48 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 09:50:40 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Li", "Rui", ""], ["Zheng", "Shunyi", ""], ["Zhang", "Ce", ""], ["Duan", "Chenxi", ""], ["Wang", "Libo", ""]]}, {"id": "2102.08003", "submitter": "Yi Zhang", "authors": "Tao Wang, Wenjun Xia, Yongqiang Huang, Huaiqiang Sun, Yan Liu, Hu\n  Chen, Jiliu Zhou, Yi Zhang", "title": "DAN-Net: Dual-Domain Adaptive-Scaling Non-local Network for CT Metal\n  Artifact Reduction", "comments": "29 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Metal implants can heavily attenuate X-rays in computed tomography (CT)\nscans, leading to severe artifacts in reconstructed images, which significantly\njeopardize image quality and negatively impact subsequent diagnoses and\ntreatment planning. With the rapid development of deep learning in the field of\nmedical imaging, several network models have been proposed for metal artifact\nreduction (MAR) in CT. Despite the encouraging results achieved by these\nmethods, there is still much room to further improve performance. In this\npaper, a novel Dual-domain Adaptive-scaling Non-local network (DAN-Net) for\nMAR. We correct the corrupted sinogram using adaptive scaling first to preserve\nmore tissue and bone details as a more informative input. Then, an end-to-end\ndual-domain network is adopted to successively process the sinogram and its\ncorresponding reconstructed image generated by the analytical reconstruction\nlayer. In addition, to better suppress the existing artifacts and restrain the\npotential secondary artifacts caused by inaccurate results of the\nsinogram-domain network, a novel residual sinogram learning strategy and\nnonlocal module are leveraged in the proposed network model. In the\nexperiments, the proposed DAN-Net demonstrates performance competitive with\nseveral state-of-the-art MAR methods in both qualitative and quantitative\naspects.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 08:09:16 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Wang", "Tao", ""], ["Xia", "Wenjun", ""], ["Huang", "Yongqiang", ""], ["Sun", "Huaiqiang", ""], ["Liu", "Yan", ""], ["Chen", "Hu", ""], ["Zhou", "Jiliu", ""], ["Zhang", "Yi", ""]]}, {"id": "2102.08005", "submitter": "Yundong Zhang", "authors": "Yundong Zhang, Huiye Liu, and Qiang Hu", "title": "TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation", "comments": "Accepted by MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Medical image segmentation - the prerequisite of numerous clinical needs -\nhas been significantly prospered by recent advances in convolutional neural\nnetworks (CNNs). However, it exhibits general limitations on modeling explicit\nlong-range relation, and existing cures, resorting to building deep encoders\nalong with aggressive downsampling operations, leads to redundant deepened\nnetworks and loss of localized details. Hence, the segmentation task awaits a\nbetter solution to improve the efficiency of modeling global contexts while\nmaintaining a strong grasp of low-level details. In this paper, we propose a\nnovel parallel-in-branch architecture, TransFuse, to address this challenge.\nTransFuse combines Transformers and CNNs in a parallel style, where both global\ndependency and low-level spatial details can be efficiently captured in a much\nshallower manner. Besides, a novel fusion technique - BiFusion module is\ncreated to efficiently fuse the multi-level features from both branches.\nExtensive experiments demonstrate that TransFuse achieves the newest\nstate-of-the-art results on both 2D and 3D medical image sets including polyp,\nskin lesion, hip, and prostate segmentation, with significant parameter\ndecrease and inference speed improvement.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 08:09:45 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 03:55:02 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zhang", "Yundong", ""], ["Liu", "Huiye", ""], ["Hu", "Qiang", ""]]}, {"id": "2102.08009", "submitter": "Kshitij Sirohi", "authors": "Kshitij Sirohi, Rohit Mohan, Daniel B\\\"uscher, Wolfram Burgard,\n  Abhinav Valada", "title": "EfficientLPS: Efficient LiDAR Panoptic Segmentation", "comments": "Ranked #1 on SemanticKITTI panoptic segmentation benchmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Panoptic segmentation of point clouds is a crucial task that enables\nautonomous vehicles to comprehend their vicinity using their highly accurate\nand reliable LiDAR sensors. Existing top-down approaches tackle this problem by\neither combining independent task-specific networks or translating methods from\nthe image domain ignoring the intricacies of LiDAR data and thus often\nresulting in sub-optimal performance. In this paper, we present the novel\ntop-down Efficient LiDAR Panoptic Segmentation (EfficientLPS) architecture that\naddresses multiple challenges in segmenting LiDAR point clouds including\ndistance-dependent sparsity, severe occlusions, large scale-variations, and\nre-projection errors. EfficientLPS comprises of a novel shared backbone that\nencodes with strengthened geometric transformation modeling capacity and\naggregates semantically rich range-aware multi-scale features. It incorporates\nnew scale-invariant semantic and instance segmentation heads along with the\npanoptic fusion module which is supervised by our proposed panoptic periphery\nloss function. Additionally, we formulate a regularized pseudo labeling\nframework to further improve the performance of EfficientLPS by training on\nunlabelled data. We benchmark our proposed model on two large-scale LiDAR\ndatasets: nuScenes, for which we also provide ground truth annotations, and\nSemanticKITTI. Notably, EfficientLPS sets the new state-of-the-art on both\nthese datasets.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 08:14:52 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 15:30:41 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Sirohi", "Kshitij", ""], ["Mohan", "Rohit", ""], ["B\u00fcscher", "Daniel", ""], ["Burgard", "Wolfram", ""], ["Valada", "Abhinav", ""]]}, {"id": "2102.08021", "submitter": "Alexey Chernyavskiy", "authors": "Ekaterina Redekop, Alexey Chernyavskiy", "title": "Uncertainty-based method for improving poorly labeled segmentation\n  datasets", "comments": "Accepted to ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The success of modern deep learning algorithms for image segmentation heavily\ndepends on the availability of large datasets with clean pixel-level\nannotations (masks), where the objects of interest are accurately delineated.\nLack of time and expertise during data annotation leads to incorrect boundaries\nand label noise. It is known that deep convolutional neural networks (DCNNs)\ncan memorize even completely random labels, resulting in poor accuracy. We\npropose a framework to train binary segmentation DCNNs using sets of unreliable\npixel-level annotations. Erroneously labeled pixels are identified based on the\nestimated aleatoric uncertainty of the segmentation and are relabeled to the\ntrue value.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 08:37:19 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Redekop", "Ekaterina", ""], ["Chernyavskiy", "Alexey", ""]]}, {"id": "2102.08041", "submitter": "YuLi Sun", "authors": "Junzheng Wu, Biao Li, Yao Qin, Weiping Ni, Han Zhang and Yuli Sun", "title": "A Multiscale Graph Convolutional Network for Change Detection in\n  Homogeneous and Heterogeneous Remote Sensing Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT eess.IV math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection (CD) in remote sensing images has been an ever-expanding\narea of research. To date, although many methods have been proposed using\nvarious techniques, accurately identifying changes is still a great challenge,\nespecially in the high resolution or heterogeneous situations, due to the\ndifficulties in effectively modeling the features from ground objects with\ndifferent patterns. In this paper, a novel CD method based on the graph\nconvolutional network (GCN) and multiscale object-based technique is proposed\nfor both homogeneous and heterogeneous images. First, the object-wise high\nlevel features are obtained through a pre-trained U-net and the multiscale\nsegmentations. Treating each parcel as a node, the graph representations can be\nformed and then, fed into the proposed multiscale graph convolutional network\nwith each channel corresponding to one scale. The multiscale GCN propagates the\nlabel information from a small number of labeled nodes to the other ones which\nare unlabeled. Further, to comprehensively incorporate the information from the\noutput channels of multiscale GCN, a fusion strategy is designed using the\nfather-child relationships between scales. Extensive Experiments on optical,\nSAR and heterogeneous optical/SAR data sets demonstrate that the proposed\nmethod outperforms some state-of the-art methods in both qualitative and\nquantitative evaluations. Besides, the Influences of some factors are also\ndiscussed.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 09:26:31 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Wu", "Junzheng", ""], ["Li", "Biao", ""], ["Qin", "Yao", ""], ["Ni", "Weiping", ""], ["Zhang", "Han", ""], ["Sun", "Yuli", ""]]}, {"id": "2102.08065", "submitter": "Swathikiran Sudhakaran", "authors": "Swathikiran Sudhakaran and Sergio Escalera and Oswald Lanz", "title": "Learning to Recognize Actions on Objects in Egocentric Video with\n  Attention Dictionaries", "comments": "Accepted to TPAMI", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3058649", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present EgoACO, a deep neural architecture for video action recognition\nthat learns to pool action-context-object descriptors from frame level features\nby leveraging the verb-noun structure of action labels in egocentric video\ndatasets. The core component of EgoACO is class activation pooling (CAP), a\ndifferentiable pooling operation that combines ideas from bilinear pooling for\nfine-grained recognition and from feature learning for discriminative\nlocalization. CAP uses self-attention with a dictionary of learnable weights to\npool from the most relevant feature regions. Through CAP, EgoACO learns to\ndecode object and scene context descriptors from video frame features. For\ntemporal modeling in EgoACO, we design a recurrent version of class activation\npooling termed Long Short-Term Attention (LSTA). LSTA extends convolutional\ngated LSTM with built-in spatial attention and a re-designed output gate.\nAction, object and context descriptors are fused by a multi-head prediction\nthat accounts for the inter-dependencies between noun-verb-action structured\nlabels in egocentric video datasets. EgoACO features built-in visual\nexplanations, helping learning and interpretation. Results on the two largest\negocentric action recognition datasets currently available, EPIC-KITCHENS and\nEGTEA, show that by explicitly decoding action-context-object descriptors,\nEgoACO achieves state-of-the-art recognition performance.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 10:26:04 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Sudhakaran", "Swathikiran", ""], ["Escalera", "Sergio", ""], ["Lanz", "Oswald", ""]]}, {"id": "2102.08068", "submitter": "Jinsheng Wei", "authors": "Jinsheng Wei, Guanming Lu, Jingjie Yan", "title": "A comparative study on movement feature in different directions for\n  micro-expression recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Micro-expression can reflect people's real emotions. Recognizing\nmicro-expressions is difficult because they are small motions and have a short\nduration. As the research is deepening into micro-expression recognition, many\neffective features and methods have been proposed. To determine which direction\nof movement feature is easier for distinguishing micro-expressions, this paper\nselects 18 directions (including three types of horizontal, vertical and\noblique movements) and proposes a new low-dimensional feature called the\nHistogram of Single Direction Gradient (HSDG) to study this topic. In this\npaper, HSDG in every direction is concatenated with LBP-TOP to obtain the LBP\nwith Single Direction Gradient (LBP-SDG) and analyze which direction of\nmovement feature is more discriminative for micro-expression recognition. As\nwith some existing work, Euler Video Magnification (EVM) is employed as a\npreprocessing step. The experiments on the CASME II and SMIC-HS databases\nsummarize the effective and optimal directions and demonstrate that HSDG in an\noptimal direction is discriminative, and the corresponding LBP-SDG achieves\nstate-of-the-art performance using EVM.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 10:38:16 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Wei", "Jinsheng", ""], ["Lu", "Guanming", ""], ["Yan", "Jingjie", ""]]}, {"id": "2102.08078", "submitter": "Eunhye Lee", "authors": "Eunhye Lee, Jeongmu Kim, Jisu Kim, Tae Hyun Kim", "title": "Restore from Restored: Single-image Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent image inpainting methods show promising results due to the power of\ndeep learning, which can explore external information available from a large\ntraining dataset. However, many state-of-the-art inpainting networks are still\nlimited in exploiting internal information available in the given input image\nat test time. To mitigate this problem, we present a novel and efficient\nself-supervised fine-tuning algorithm that can adapt the parameters of fully\npre-trained inpainting networks without using ground-truth target images. We\nupdate the parameters of the pre-trained state-of-the-art inpainting networks\nby utilizing existing self-similar patches within the given input image without\nchanging network architecture and improve the inpainting quality by a large\nmargin. Qualitative and quantitative experimental results demonstrate the\nsuperiority of the proposed algorithm, and we achieve state-of-the-art\ninpainting results on publicly available numerous benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 10:59:28 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 08:19:10 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Lee", "Eunhye", ""], ["Kim", "Jeongmu", ""], ["Kim", "Jisu", ""], ["Kim", "Tae Hyun", ""]]}, {"id": "2102.08079", "submitter": "Adil Kaan Akan", "authors": "Adil Kaan Akan, Emre Akbas, Fatos T. Yarman Vural", "title": "Just Noticeable Difference for Machine Perception and Generation of\n  Regularized Adversarial Images with Minimal Perturbation", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we introduce a measure for machine perception, inspired by the\nconcept of Just Noticeable Difference (JND) of human perception. Based on this\nmeasure, we suggest an adversarial image generation algorithm, which\niteratively distorts an image by an additive noise until the machine learning\nmodel detects the change in the image by outputting a false label. The amount\nof noise added to the original image is defined as the gradient of the cost\nfunction of the machine learning model. This cost function explicitly minimizes\nthe amount of perturbation applied on the input image and it is regularized by\nbounded range and total variation functions to assure perceptual similarity of\nthe adversarial image to the input. We evaluate the adversarial images\ngenerated by our algorithm both qualitatively and quantitatively on CIFAR10,\nImageNet, and MS COCO datasets. Our experiments on image classification and\nobject detection tasks show that adversarial images generated by our method are\nboth more successful in deceiving the recognition/detection model and less\nperturbed compared to the images generated by the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 11:01:55 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 20:00:47 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Akan", "Adil Kaan", ""], ["Akbas", "Emre", ""], ["Vural", "Fatos T. Yarman", ""]]}, {"id": "2102.08085", "submitter": "Fouzia Altaf Ms", "authors": "Fouzia Altaf, Syed M.S. Islam, Naeem K. Janjua, Naveed Akhtar", "title": "Boosting Deep Transfer Learning for COVID-19 Classification", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  COVID-19 classification using chest Computed Tomography (CT) has been found\npragmatically useful by several studies. Due to the lack of annotated samples,\nthese studies recommend transfer learning and explore the choices of\npre-trained models and data augmentation. However, it is still unknown if there\nare better strategies than vanilla transfer learning for more accurate COVID-19\nclassification with limited CT data. This paper provides an affirmative answer,\ndevising a novel `model' augmentation technique that allows a considerable\nperformance boost to transfer learning for the task. Our method systematically\nreduces the distributional shift between the source and target domains and\nconsiders augmenting deep learning with complementary representation learning\ntechniques. We establish the efficacy of our method with publicly available\ndatasets and models, along with identifying contrasting observations in the\nprevious studies.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 11:15:23 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Altaf", "Fouzia", ""], ["Islam", "Syed M. S.", ""], ["Janjua", "Naeem K.", ""], ["Akhtar", "Naveed", ""]]}, {"id": "2102.08092", "submitter": "Vasco Lopes Ferrinho", "authors": "Vasco Lopes, Ant\\'onio Gaspar, Lu\\'is A. Alexandre, Jo\\~ao Cordeiro", "title": "An AutoML-based Approach to Multimodal Image Sentiment Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sentiment analysis is a research topic focused on analysing data to extract\ninformation related to the sentiment that it causes. Applications of sentiment\nanalysis are wide, ranging from recommendation systems, and marketing to\ncustomer satisfaction. Recent approaches evaluate textual content using Machine\nLearning techniques that are trained over large corpora. However, as social\nmedia grown, other data types emerged in large quantities, such as images.\nSentiment analysis in images has shown to be a valuable complement to textual\ndata since it enables the inference of the underlying message polarity by\ncreating context and connections. Multimodal sentiment analysis approaches\nintend to leverage information of both textual and image content to perform an\nevaluation. Despite recent advances, current solutions still flounder in\ncombining both image and textual information to classify social media data,\nmainly due to subjectivity, inter-class homogeneity and fusion data\ndifferences. In this paper, we propose a method that combines both textual and\nimage individual sentiment analysis into a final fused classification based on\nAutoML, that performs a random search to find the best model. Our method\nachieved state-of-the-art performance in the B-T4SA dataset, with 95.19%\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 11:28:50 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Lopes", "Vasco", ""], ["Gaspar", "Ant\u00f3nio", ""], ["Alexandre", "Lu\u00eds A.", ""], ["Cordeiro", "Jo\u00e3o", ""]]}, {"id": "2102.08094", "submitter": "Oier Mees", "authors": "Oier Mees, Wolfram Burgard", "title": "Composing Pick-and-Place Tasks By Grounding Language", "comments": "Accepted at the International Symposium on Experimental Robotics\n  (ISER) 2020. Videos at http://speechrobot.cs.uni-freiburg.de", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controlling robots to perform tasks via natural language is one of the most\nchallenging topics in human-robot interaction. In this work, we present a robot\nsystem that follows unconstrained language instructions to pick and place\narbitrary objects and effectively resolves ambiguities through dialogues. Our\napproach infers objects and their relationships from input images and language\nexpressions and can place objects in accordance with the spatial relations\nexpressed by the user. Unlike previous approaches, we consider grounding not\nonly for the picking but also for the placement of everyday objects from\nlanguage. Specifically, by grounding objects and their spatial relations, we\nallow specification of complex placement instructions, e.g. \"place it behind\nthe middle red bowl\". Our results obtained using a real-world PR2 robot\ndemonstrate the effectiveness of our method in understanding pick-and-place\nlanguage instructions and sequentially composing them to solve tabletop\nmanipulation tasks. Videos are available at\nhttp://speechrobot.cs.uni-freiburg.de\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 11:29:09 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Mees", "Oier", ""], ["Burgard", "Wolfram", ""]]}, {"id": "2102.08096", "submitter": "Andras Kupcsik", "authors": "Andras Kupcsik, Markus Spies, Alexander Klein, Marco Todescato,\n  Nicolai Waniek, Philipp Schillinger, Mathias Buerger", "title": "Supervised Training of Dense Object Nets using Optimal Descriptors for\n  Industrial Robotic Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense Object Nets (DONs) by Florence, Manuelli and Tedrake (2018) introduced\ndense object descriptors as a novel visual object representation for the\nrobotics community. It is suitable for many applications including object\ngrasping, policy learning, etc. DONs map an RGB image depicting an object into\na descriptor space image, which implicitly encodes key features of an object\ninvariant to the relative camera pose. Impressively, the self-supervised\ntraining of DONs can be applied to arbitrary objects and can be evaluated and\ndeployed within hours. However, the training approach relies on accurate depth\nimages and faces challenges with small, reflective objects, typical for\nindustrial settings, when using consumer grade depth cameras. In this paper we\nshow that given a 3D model of an object, we can generate its descriptor space\nimage, which allows for supervised training of DONs. We rely on Laplacian\nEigenmaps (LE) to embed the 3D model of an object into an optimally generated\nspace. While our approach uses more domain knowledge, it can be efficiently\napplied even for smaller and reflective objects, as it does not rely on depth\ninformation. We compare the training methods on generating 6D grasps for\nindustrial objects and show that our novel supervised training approach\nimproves the pick-and-place performance in industry-relevant tasks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 11:40:12 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Kupcsik", "Andras", ""], ["Spies", "Markus", ""], ["Klein", "Alexander", ""], ["Todescato", "Marco", ""], ["Waniek", "Nicolai", ""], ["Schillinger", "Philipp", ""], ["Buerger", "Mathias", ""]]}, {"id": "2102.08098", "submitter": "Chen Zhu", "authors": "Chen Zhu, Renkun Ni, Zheng Xu, Kezhi Kong, W. Ronny Huang, Tom\n  Goldstein", "title": "GradInit: Learning to Initialize Neural Networks for Stable and\n  Efficient Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Changes in neural architectures have fostered significant breakthroughs in\nlanguage modeling and computer vision. Unfortunately, novel architectures often\nrequire re-thinking the choice of hyperparameters (e.g., learning rate, warmup\nschedule, and momentum coefficients) to maintain stability of the optimizer.\nThis optimizer instability is often the result of poor parameter\ninitialization, and can be avoided by architecture-specific initialization\nschemes. In this paper, we present GradInit, an automated and architecture\nagnostic method for initializing neural networks. GradInit is based on a simple\nheuristic; the variance of each network layer is adjusted so that a single step\nof SGD or Adam results in the smallest possible loss value. This adjustment is\ndone by introducing a scalar multiplier variable in front of each parameter\nblock, and then optimizing these variables using a simple numerical scheme.\nGradInit accelerates the convergence and test performance of many convolutional\narchitectures, both with or without skip connections, and even without\nnormalization layers. It also enables training the original Post-LN Transformer\nfor machine translation without learning rate warmup under a wide range of\nlearning rates and momentum coefficients. Code is available at\nhttps://github.com/zhuchen03/gradinit.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 11:45:35 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Zhu", "Chen", ""], ["Ni", "Renkun", ""], ["Xu", "Zheng", ""], ["Kong", "Kezhi", ""], ["Huang", "W. Ronny", ""], ["Goldstein", "Tom", ""]]}, {"id": "2102.08099", "submitter": "Vasco Lopes Ferrinho", "authors": "Vasco Lopes, Saeid Alirezazadeh, Lu\\'is A. Alexandre", "title": "EPE-NAS: Efficient Performance Estimation Without Training for Neural\n  Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural Architecture Search (NAS) has shown excellent results in designing\narchitectures for computer vision problems. NAS alleviates the need for\nhuman-defined settings by automating architecture design and engineering.\nHowever, NAS methods tend to be slow, as they require large amounts of GPU\ncomputation. This bottleneck is mainly due to the performance estimation\nstrategy, which requires the evaluation of the generated architectures, mainly\nby training them, to update the sampler method. In this paper, we propose\nEPE-NAS, an efficient performance estimation strategy, that mitigates the\nproblem of evaluating networks, by scoring untrained networks and creating a\ncorrelation with their trained performance. We perform this process by looking\nat intra and inter-class correlations of an untrained network. We show that\nEPE-NAS can produce a robust correlation and that by incorporating it into a\nsimple random sampling strategy, we are able to search for competitive\nnetworks, without requiring any training, in a matter of seconds using a single\nGPU. Moreover, EPE-NAS is agnostic to the search method, since it focuses on\nthe evaluation of untrained networks, making it easy to integrate into almost\nany NAS method.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 11:47:05 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Lopes", "Vasco", ""], ["Alirezazadeh", "Saeid", ""], ["Alexandre", "Lu\u00eds A.", ""]]}, {"id": "2102.08145", "submitter": "Florian Tschopp", "authors": "Florian Tschopp, Cornelius von Einem, Andrei Cramariuc, David Hug,\n  Andrew William Palmer, Roland Siegwart, Margarita Chli, Juan Nieto", "title": "Hough2Map -- Iterative Event-based Hough Transform for High-Speed\n  Railway Mapping", "comments": "Florian Tschopp, Cornelius von Einem, and Andrei Cramariuc\n  contributed equally to this work", "journal-ref": "IEEE Robotics and Automation Letters ( Volume: 6, Issue: 2, April\n  2021)", "doi": "10.1109/LRA.2021.3061404", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To cope with the growing demand for transportation on the railway system,\naccurate, robust, and high-frequency positioning is required to enable a safe\nand efficient utilization of the existing railway infrastructure. As a basis\nfor a localization system we propose a complete on-board mapping pipeline able\nto map robust meaningful landmarks, such as poles from power lines, in the\nvicinity of the vehicle. Such poles are good candidates for reliable and long\nterm landmarks even through difficult weather conditions or seasonal changes.\nTo address the challenges of motion blur and illumination changes in railway\nscenarios we employ a Dynamic Vision Sensor, a novel event-based camera. Using\na sideways oriented on-board camera, poles appear as vertical lines. To map\nsuch lines in a real-time event stream, we introduce Hough2Map, a novel\nconsecutive iterative event-based Hough transform framework capable of\ndetecting, tracking, and triangulating close-by structures. We demonstrate the\nmapping reliability and accuracy of Hough2Map on real-world data in typical\nusage scenarios and evaluate using surveyed infrastructure ground truth maps.\nHough2Map achieves a detection reliability of up to 92% and a mapping root mean\nsquare error accuracy of 1.1518m.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 13:36:07 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 15:51:51 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Tschopp", "Florian", ""], ["von Einem", "Cornelius", ""], ["Cramariuc", "Andrei", ""], ["Hug", "David", ""], ["Palmer", "Andrew William", ""], ["Siegwart", "Roland", ""], ["Chli", "Margarita", ""], ["Nieto", "Juan", ""]]}, {"id": "2102.08148", "submitter": "Jintai Chen", "authors": "Jintai Chen, Hongyun Yu, Ruiwei Feng, Danny Z. Chen, Jian Wu", "title": "Flow-Mixup: Classifying Multi-labeled Medical Images with Corrupted\n  Labels", "comments": null, "journal-ref": "2020 IEEE International Conference on Bioinformatics and\n  Biomedicine", "doi": "10.1109/BIBM49941.2020.9313408", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In clinical practice, medical image interpretation often involves\nmulti-labeled classification, since the affected parts of a patient tend to\npresent multiple symptoms or comorbidities. Recently, deep learning based\nframeworks have attained expert-level performance on medical image\ninterpretation, which can be attributed partially to large amounts of accurate\nannotations. However, manually annotating massive amounts of medical images is\nimpractical, while automatic annotation is fast but imprecise (possibly\nintroducing corrupted labels). In this work, we propose a new regularization\napproach, called Flow-Mixup, for multi-labeled medical image classification\nwith corrupted labels. Flow-Mixup guides the models to capture robust features\nfor each abnormality, thus helping handle corrupted labels effectively and\nmaking it possible to apply automatic annotation. Specifically, Flow-Mixup\ndecouples the extracted features by adding constraints to the hidden states of\nthe models. Also, Flow-Mixup is more stable and effective comparing to other\nknown regularization methods, as shown by theoretical and empirical analyses.\nExperiments on two electrocardiogram datasets and a chest X-ray dataset\ncontaining corrupted labels verify that Flow-Mixup is effective and insensitive\nto corrupted labels.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 16:04:26 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Chen", "Jintai", ""], ["Yu", "Hongyun", ""], ["Feng", "Ruiwei", ""], ["Chen", "Danny Z.", ""], ["Wu", "Jian", ""]]}, {"id": "2102.08155", "submitter": "Benedikt Hosp", "authors": "Benedikt Hosp, Myat Su Yin, Peter Haddawy, Paphon Sa-Ngasoongsong, and\n  Enkelejda Kasneci", "title": "Differentiating Surgeon Expertise Solely by Eye Movement Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Developments in computer science in recent years are moving into hospitals.\nSurgeons are faced with ever new technical challenges. Visual perception plays\na key role in most of these. Diagnostic and training models are needed to\noptimize the training of young surgeons. In this study, we present a model for\nclassifying experts, 4th-year residents and 3rd-year residents, using only eye\nmovements. We show a model that uses a minimal set of features and still\nachieve a robust accuracy of 76.46 % to classify eye movements into the correct\nclass. Likewise, in this study, we address the evolutionary steps of visual\nperception between three expertise classes, forming a first step towards a\ndiagnostic model for expertise.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 13:43:40 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Hosp", "Benedikt", ""], ["Yin", "Myat Su", ""], ["Haddawy", "Peter", ""], ["Sa-Ngasoongsong", "Paphon", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "2102.08168", "submitter": "Jian Jin", "authors": "Jian Jin, Xingxing Zhang, Xin Fu, Huan Zhang, Weisi Lin, Jian Lou, Yao\n  Zhao", "title": "Does deep machine vision have just noticeable difference (JND)?", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an important perceptual characteristic of the Human Visual System (HVS),\nthe Just Noticeable Difference (JND) has been studied for decades with\nimage/video processing (e.g., perceptual image/video coding). However, there is\nlittle exploration on the existence of JND for AI, like Deep Machine Vision\n(DMV), although the DMV has made great strides in many machine vision tasks. In\nthis paper, we take an initial attempt, and demonstrate that DMV does have the\nJND, termed as DMVJND. Besides, we propose a JND model for the classification\ntask in DMV. It has been discovered that DMV can tolerate distorted images with\naverage PSNR of only 9.56dB (the lower the better), by generating JND via\nunsupervised learning with our DMVJND-NET. In particular, a semantic-guided\nredundancy assessment strategy is designed to constrain the magnitude and\nspatial distribution of the JND. Experimental results on classification tasks\ndemonstrate that we successfully find and model the JND for deep machine\nvision. Meanwhile, our DMV-JND paves a possible direction for DMV oriented\nimage/video compression, watermarking, quality assessment, deep neural network\nsecurity, and so on.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 14:19:35 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Jin", "Jian", ""], ["Zhang", "Xingxing", ""], ["Fu", "Xin", ""], ["Zhang", "Huan", ""], ["Lin", "Weisi", ""], ["Lou", "Jian", ""], ["Zhao", "Yao", ""]]}, {"id": "2102.08175", "submitter": "Ashesh Ashesh", "authors": "Ashesh, Buo-Fu Chen, Treng-Shi Huang, Boyo Chen, Chia-Tung Chang,\n  Hsuan-Tien Lin", "title": "Accurate and Clear Precipitation Nowcasting with Consecutive Attention\n  and Rain-map Discrimination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Precipitation nowcasting is an important task for weather forecasting. Many\nrecent works aim to predict the high rainfall events more accurately with the\nhelp of deep learning techniques, but such events are relatively rare. The\nrarity is often addressed by formulations that re-weight the rare events.\nSomehow such a formulation carries a side effect of making \"blurry\" predictions\nin low rainfall regions and cannot convince meteorologists to trust its\npractical usability. We fix the trust issue by introducing a discriminator that\nencourages the prediction model to generate realistic rain-maps without\nsacrificing predictive accuracy. Furthermore, we extend the nowcasting time\nframe from one hour to three hours to further address the needs from\nmeteorologists. The extension is based on consecutive attentions across\ndifferent hours. We propose a new deep learning model for precipitation\nnowcasting that includes both the discrimination and attention techniques. The\nmodel is examined on a newly-built benchmark dataset that contains both radar\ndata and actual rain data. The benchmark, which will be publicly released, not\nonly establishes the superiority of the proposed model, but also is expected to\nencourage future research on precipitation nowcasting.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 14:22:54 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Ashesh", "", ""], ["Chen", "Buo-Fu", ""], ["Huang", "Treng-Shi", ""], ["Chen", "Boyo", ""], ["Chang", "Chia-Tung", ""], ["Lin", "Hsuan-Tien", ""]]}, {"id": "2102.08239", "submitter": "Qingyu Zhao", "authors": "Zixuan Liu and Ehsan Adeli and Kilian M. Pohl and Qingyu Zhao", "title": "Going Beyond Saliency Maps: Training Deep Models to Interpret Deep\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretability is a critical factor in applying complex deep learning\nmodels to advance the understanding of brain disorders in neuroimaging studies.\nTo interpret the decision process of a trained classifier, existing techniques\ntypically rely on saliency maps to quantify the voxel-wise or feature-level\nimportance for classification through partial derivatives. Despite providing\nsome level of localization, these maps are not human-understandable from the\nneuroscience perspective as they do not inform the specific meaning of the\nalteration linked to the brain disorder. Inspired by the image-to-image\ntranslation scheme, we propose to train simulator networks that can warp a\ngiven image to inject or remove patterns of the disease. These networks are\ntrained such that the classifier produces consistently increased or decreased\nprediction logits for the simulated images. Moreover, we propose to couple all\nthe simulators into a unified model based on conditional convolution. We\napplied our approach to interpreting classifiers trained on a synthetic dataset\nand two neuroimaging datasets to visualize the effect of the Alzheimer's\ndisease and alcohol use disorder. Compared to the saliency maps generated by\nbaseline approaches, our simulations and visualizations based on the Jacobian\ndeterminants of the warping field reveal meaningful and understandable patterns\nrelated to the diseases.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 15:57:37 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 01:19:14 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Liu", "Zixuan", ""], ["Adeli", "Ehsan", ""], ["Pohl", "Kilian M.", ""], ["Zhao", "Qingyu", ""]]}, {"id": "2102.08248", "submitter": "Jakob Drachmann Havtorn Mr", "authors": "Jakob D. Havtorn, Jes Frellsen, S{\\o}ren Hauberg, Lars Maal{\\o}e", "title": "Hierarchical VAEs Know What They Don't Know", "comments": "Appeared in Proceedings of the 38th International Conference on\n  Machine Learning (ICML 2021). 18 pages, source code available at\n  https://github.com/JakobHavtorn/hvae-oodd,\n  https://github.com/vlievin/biva-pytorch and\n  https://github.com/larsmaaloee/BIVA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models have been demonstrated as state-of-the-art density\nestimators. Yet, recent work has found that they often assign a higher\nlikelihood to data from outside the training distribution. This seemingly\nparadoxical behavior has caused concerns over the quality of the attained\ndensity estimates. In the context of hierarchical variational autoencoders, we\nprovide evidence to explain this behavior by out-of-distribution data having\nin-distribution low-level features. We argue that this is both expected and\ndesirable behavior. With this insight in hand, we develop a fast, scalable and\nfully unsupervised likelihood-ratio score for OOD detection that requires data\nto be in-distribution across all feature-levels. We benchmark the method on a\nvast set of data and model combinations and achieve state-of-the-art results on\nout-of-distribution detection.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 16:08:04 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 09:35:30 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 09:54:43 GMT"}, {"version": "v4", "created": "Thu, 10 Jun 2021 07:44:50 GMT"}, {"version": "v5", "created": "Fri, 11 Jun 2021 11:55:39 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Havtorn", "Jakob D.", ""], ["Frellsen", "Jes", ""], ["Hauberg", "S\u00f8ren", ""], ["Maal\u00f8e", "Lars", ""]]}, {"id": "2102.08259", "submitter": "Bo Zhao", "authors": "Bo Zhao, Hakan Bilen", "title": "Dataset Condensation with Differentiable Siamese Augmentation", "comments": null, "journal-ref": "International Conference on Machine Learning 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In many machine learning problems, large-scale datasets have become the\nde-facto standard to train state-of-the-art deep networks at the price of heavy\ncomputation load. In this paper, we focus on condensing large training sets\ninto significantly smaller synthetic sets which can be used to train deep\nneural networks from scratch with minimum drop in performance. Inspired from\nthe recent training set synthesis methods, we propose Differentiable Siamese\nAugmentation that enables effective use of data augmentation to synthesize more\ninformative synthetic images and thus achieves better performance when training\nnetworks with augmentations. Experiments on multiple image classification\nbenchmarks demonstrate that the proposed method obtains substantial gains over\nthe state-of-the-art, 7% improvements on CIFAR10 and CIFAR100 datasets. We show\nwith only less than 1% data that our method achieves 99.6%, 94.9%, 88.5%, 71.5%\nrelative performance on MNIST, FashionMNIST, SVHN, CIFAR10 respectively. We\nalso explore the use of our method in continual learning and neural\narchitecture search, and show promising results.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 16:32:21 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 08:04:29 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Zhao", "Bo", ""], ["Bilen", "Hakan", ""]]}, {"id": "2102.08312", "submitter": "AmirAbbas Davari", "authors": "Amirabbas Davari, Saahil Islam, Thorsten Seehaus, Matthias Braun,\n  Andreas Maier, Vincent Christlein", "title": "On Mathews Correlation Coefficient and Improved Distance Map Loss for\n  Automatic Glacier Calving Front Segmentation in SAR Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The vast majority of the outlet glaciers and ice streams of the polar ice\nsheets end in the ocean. Ice mass loss via calving of the glaciers into the\nocean has increased over the last few decades. Information on the temporal\nvariability of the calving front position provides fundamental information on\nthe state of the glacier and ice stream, which can be exploited as calibration\nand validation data to enhance ice dynamics modeling. To identify the calving\nfront position automatically, deep neural network-based semantic segmentation\npipelines can be used to delineate the acquired SAR imagery. However, the\nextreme class imbalance is highly challenging for the accurate calving front\nsegmentation in these images. Therefore, we propose the use of the Mathews\ncorrelation coefficient (MCC) as an early stopping criterion because of its\nsymmetrical properties and its invariance towards class imbalance. Moreover, we\npropose an improvement to the distance map-based binary cross-entropy (BCE)\nloss function. The distance map adds context to the loss function about the\nimportant regions for segmentation and helps accounting for the imbalanced\ndata. Using Mathews correlation coefficient as early stopping demonstrates an\naverage 15% dice coefficient improvement compared to the commonly used BCE. The\nmodified distance map loss further improves the segmentation performance by\nanother 2%. These results are encouraging as they support the effectiveness of\nthe proposed methods for segmentation problems suffering from extreme class\nimbalances.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 17:53:34 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 21:09:37 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Davari", "Amirabbas", ""], ["Islam", "Saahil", ""], ["Seehaus", "Thorsten", ""], ["Braun", "Matthias", ""], ["Maier", "Andreas", ""], ["Christlein", "Vincent", ""]]}, {"id": "2102.08318", "submitter": "Ceyuan Yang", "authors": "Ceyuan Yang, Zhirong Wu, Bolei Zhou, Stephen Lin", "title": "Instance Localization for Self-supervised Detection Pretraining", "comments": "To appear in CVPR 2021. Code is available at\n  https://github.com/limbo0000/InstanceLoc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior research on self-supervised learning has led to considerable progress\non image classification, but often with degraded transfer performance on object\ndetection. The objective of this paper is to advance self-supervised pretrained\nmodels specifically for object detection. Based on the inherent difference\nbetween classification and detection, we propose a new self-supervised pretext\ntask, called instance localization. Image instances are pasted at various\nlocations and scales onto background images. The pretext task is to predict the\ninstance category given the composited images as well as the foreground\nbounding boxes. We show that integration of bounding boxes into pretraining\npromotes better task alignment and architecture alignment for transfer\nlearning. In addition, we propose an augmentation method on the bounding boxes\nto further enhance the feature alignment. As a result, our model becomes weaker\nat Imagenet semantic classification but stronger at image patch localization,\nwith an overall stronger pretrained model for object detection. Experimental\nresults demonstrate that our approach yields state-of-the-art transfer learning\nresults for object detection on PASCAL VOC and MSCOCO.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 17:58:57 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 15:41:52 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Yang", "Ceyuan", ""], ["Wu", "Zhirong", ""], ["Zhou", "Bolei", ""], ["Lin", "Stephen", ""]]}, {"id": "2102.08343", "submitter": "Aditya Kumar Akash", "authors": "Aditya Kumar Akash, Vishnu Suresh Lokhande, Sathya N. Ravi, Vikas\n  Singh", "title": "Learning Invariant Representations using Inverse Contrastive Loss", "comments": "Accepted to AAAI-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning invariant representations is a critical first step in a number of\nmachine learning tasks. A common approach corresponds to the so-called\ninformation bottleneck principle in which an application dependent function of\nmutual information is carefully chosen and optimized. Unfortunately, in\npractice, these functions are not suitable for optimization purposes since\nthese losses are agnostic of the metric structure of the parameters of the\nmodel. We introduce a class of losses for learning representations that are\ninvariant to some extraneous variable of interest by inverting the class of\ncontrastive losses, i.e., inverse contrastive loss (ICL). We show that if the\nextraneous variable is binary, then optimizing ICL is equivalent to optimizing\na regularized MMD divergence. More generally, we also show that if we are\nprovided a metric on the sample space, our formulation of ICL can be decomposed\ninto a sum of convex functions of the given distance metric. Our experimental\nresults indicate that models obtained by optimizing ICL achieve significantly\nbetter invariance to the extraneous variable for a fixed desired level of\naccuracy. In a variety of experimental settings, we show applicability of ICL\nfor learning invariant representations for both continuous and discrete\nextraneous variables.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 18:29:28 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Akash", "Aditya Kumar", ""], ["Lokhande", "Vishnu Suresh", ""], ["Ravi", "Sathya N.", ""], ["Singh", "Vikas", ""]]}, {"id": "2102.08360", "submitter": "Anirudh Som", "authors": "Ella Y. Wang, Anirudh Som, Ankita Shukla, Hongjun Choi, Pavan Turaga", "title": "Interpretable COVID-19 Chest X-Ray Classification via Orthogonality\n  Constraint", "comments": "Accepted in the 2021 ACM CHIL Workshop track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks have increasingly been used as an auxiliary tool in\nhealthcare applications, due to their ability to improve performance of several\ndiagnosis tasks. However, these methods are not widely adopted in clinical\nsettings due to the practical limitations in the reliability, generalizability,\nand interpretability of deep learning based systems. As a result, methods have\nbeen developed that impose additional constraints during network training to\ngain more control as well as improve interpretabilty, facilitating their\nacceptance in healthcare community. In this work, we investigate the benefit of\nusing Orthogonal Spheres (OS) constraint for classification of COVID-19 cases\nfrom chest X-ray images. The OS constraint can be written as a simple\northonormality term which is used in conjunction with the standard\ncross-entropy loss during classification network training. Previous studies\nhave demonstrated significant benefits in applying such constraints to deep\nlearning models. Our findings corroborate these observations, indicating that\nthe orthonormality loss function effectively produces improved semantic\nlocalization via GradCAM visualizations, enhanced classification performance,\nand reduced model calibration error. Our approach achieves an improvement in\naccuracy of 1.6% and 4.8% for two- and three-class classification,\nrespectively; similar results are found for models with data augmentation\napplied. In addition to these findings, our work also presents a new\napplication of the OS regularizer in healthcare, increasing the post-hoc\ninterpretability and performance of deep learning models for COVID-19\nclassification to facilitate adoption of these methods in clinical settings. We\nalso identify the limitations of our strategy that can be explored for further\nresearch in future.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 11:35:28 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 22:40:17 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Wang", "Ella Y.", ""], ["Som", "Anirudh", ""], ["Shukla", "Ankita", ""], ["Choi", "Hongjun", ""], ["Turaga", "Pavan", ""]]}, {"id": "2102.08414", "submitter": "Mike Walmsley", "authors": "Mike Walmsley, Chris Lintott, Tobias Geron, Sandor Kruk, Coleman\n  Krawczyk, Kyle W. Willett, Steven Bamford, William Keel, Lee S. Kelvin, Lucy\n  Fortson, Karen L. Masters, Vihang Mehta, Brooke D. Simmons, Rebecca\n  Smethurst, Elisabeth M. Baeten, Christine Macmillan", "title": "Galaxy Zoo DECaLS: Detailed Visual Morphology Measurements from\n  Volunteers and Deep Learning for 314,000 Galaxies", "comments": "First review received from MNRAS. Data at\n  https://zenodo.org/record/4196267. Temporary interactive viewer at\n  https://share.streamlit.io/mwalmsley/galaxy-poster/gz_decals_mike_walmsley.py", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.GA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Galaxy Zoo DECaLS: detailed visual morphological classifications\nfor Dark Energy Camera Legacy Survey images of galaxies within the SDSS DR8\nfootprint. Deeper DECaLS images (r=23.6 vs. r=22.2 from SDSS) reveal spiral\narms, weak bars, and tidal features not previously visible in SDSS imaging. To\nbest exploit the greater depth of DECaLS images, volunteers select from a new\nset of answers designed to improve our sensitivity to mergers and bars. Galaxy\nZoo volunteers provide 7.5 million individual classifications over 314,000\ngalaxies. 140,000 galaxies receive at least 30 classifications, sufficient to\naccurately measure detailed morphology like bars, and the remainder receive\napproximately 5. All classifications are used to train an ensemble of Bayesian\nconvolutional neural networks (a state-of-the-art deep learning method) to\npredict posteriors for the detailed morphology of all 314,000 galaxies. When\nmeasured against confident volunteer classifications, the networks are\napproximately 99% accurate on every question. Morphology is a fundamental\nfeature of every galaxy; our human and machine classifications are an accurate\nand detailed resource for understanding how galaxies evolve.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 19:09:36 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Walmsley", "Mike", ""], ["Lintott", "Chris", ""], ["Geron", "Tobias", ""], ["Kruk", "Sandor", ""], ["Krawczyk", "Coleman", ""], ["Willett", "Kyle W.", ""], ["Bamford", "Steven", ""], ["Keel", "William", ""], ["Kelvin", "Lee S.", ""], ["Fortson", "Lucy", ""], ["Masters", "Karen L.", ""], ["Mehta", "Vihang", ""], ["Simmons", "Brooke D.", ""], ["Smethurst", "Rebecca", ""], ["Baeten", "Elisabeth M.", ""], ["Macmillan", "Christine", ""]]}, {"id": "2102.08416", "submitter": "Shoaib Ehsan", "authors": "Maria Waheed, Michael Milford, Klaus D. McDonald-Maier, Shoaib Ehsan", "title": "Improving Visual Place Recognition Performance by Maximising\n  Complementarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual place recognition (VPR) is the problem of recognising a previously\nvisited location using visual information. Many attempts to improve the\nperformance of VPR methods have been made in the literature. One approach that\nhas received attention recently is the multi-process fusion where different VPR\nmethods run in parallel and their outputs are combined in an effort to achieve\nbetter performance. The multi-process fusion, however, does not have a\nwell-defined criterion for selecting and combining different VPR methods from a\nwide range of available options. To the best of our knowledge, this paper\ninvestigates the complementarity of state-of-the-art VPR methods systematically\nfor the first time and identifies those combinations which can result in better\nperformance. The paper presents a well-defined framework which acts as a sanity\ncheck to find the complementarity between two techniques by utilising a\nMcNemar's test-like approach. The framework allows estimation of upper and\nlower complementarity bounds for the VPR techniques to be combined, along with\nan estimate of maximum VPR performance that may be achieved. Based on this\nframework, results are presented for eight state-of-the-art VPR methods on ten\nwidely-used VPR datasets showing the potential of different combinations of\ntechniques for achieving better performance.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 19:18:33 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Waheed", "Maria", ""], ["Milford", "Michael", ""], ["McDonald-Maier", "Klaus D.", ""], ["Ehsan", "Shoaib", ""]]}, {"id": "2102.08423", "submitter": "Sohaib Syed Dr", "authors": "Hannan Adeel and Syed Sohaib Ali and Muhammad Mohsin Riaz and Syed\n  Abdul Mannan Kirmani and Muhammad Imran Qureshi and Junaid Imtiaz", "title": "Learning deep multiresolution representations for pansharpening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retaining spatial characteristics of panchromatic image and spectral\ninformation of multispectral bands is a critical issue in pansharpening. This\npaper proposes a pyramid based deep fusion framework that preserves spectral\nand spatial characteristics at different scales. The spectral information is\npreserved by passing the corresponding low resolution multispectral image as\nresidual component of the network at each scale. The spatial information is\npreserved by training the network at each scale with the high frequencies of\npanchromatic image alongside the corresponding low resolution multispectral\nimage. The parameters of different networks are shared across the pyramid in\norder to add spatial details consistently across scales. The parameters are\nalso shared across fusion layers within a network at a specific scale.\nExperiments suggest that the proposed architecture outperforms state of the art\npansharpening models. The proposed model, code and dataset is publicly\navailable at https://github.com/sohaibali01/deep_pyramid_fusion.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 19:41:57 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Adeel", "Hannan", ""], ["Ali", "Syed Sohaib", ""], ["Riaz", "Muhammad Mohsin", ""], ["Kirmani", "Syed Abdul Mannan", ""], ["Qureshi", "Muhammad Imran", ""], ["Imtiaz", "Junaid", ""]]}, {"id": "2102.08449", "submitter": "Juan Tapia Dr.", "authors": "Juan Tapia, Marta Gomez-Barrero, Rodrigo Lara, Andres Valenzuela,\n  Christoph Busch", "title": "Selfie Periocular Verification using an Efficient Super-Resolution\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Selfie-based biometrics has great potential for a wide range of applications\nfrom marketing to higher security environments like online banking. This is now\nespecially relevant since e.g. periocular verification is contactless, and\nthereby safe to use in pandemics such as COVID-19. However, selfie-based\nbiometrics faces some challenges since there is limited control over the data\nacquisition conditions. Therefore, super-resolution has to be used to increase\nthe quality of the captured images. Most of the state of the art\nsuper-resolution methods use deep networks with large filters, thereby needing\nto train and store a correspondingly large number of parameters, and making\ntheir use difficult for mobile devices commonly used for selfie-based.\n  In order to achieve an efficient super-resolution method, we propose an\nEfficient Single Image Super-Resolution (ESISR) algorithm, which takes into\naccount a trade-off between the efficiency of the deep neural network and the\nsize of its filters. To that end, the method implements a novel loss function\nbased on the Sharpness metric. This metric turns out to be more suitable for\nincreasing the quality of the eye images. Our method drastically reduces the\nnumber of parameters when compared with Deep CNNs with Skip Connection and\nNetwork (DCSCN): from 2,170,142 to 28,654 parameters when the image size is\nincreased by a factor of x3. Furthermore, the proposed method keeps the sharp\nquality of the images, which is highly relevant for biometric recognition\npurposes. The results on remote verification systems with raw images reached an\nEqual Error Rate (EER) of 8.7% for FaceNet and 10.05% for VGGFace. Where\nembedding vectors were used from periocular images the best results reached an\nEER of 8.9% (x3) for FaceNet and 9.90% (x4) for VGGFace.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 21:01:12 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Tapia", "Juan", ""], ["Gomez-Barrero", "Marta", ""], ["Lara", "Rodrigo", ""], ["Valenzuela", "Andres", ""], ["Busch", "Christoph", ""]]}, {"id": "2102.08460", "submitter": "Michael Hoss", "authors": "Michael Hoss, Maike Scholtes, Lutz Eckstein", "title": "A Review of Testing Object-Based Environment Perception for Safe\n  Automated Driving", "comments": "23 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Safety assurance of automated driving systems must consider uncertain\nenvironment perception. This paper reviews literature addressing how perception\ntesting is realized as part of safety assurance. We focus on testing for\nverification and validation purposes at the interface between perception and\nplanning, and structure our analysis along the three axes 1) test criteria and\nmetrics, 2) test scenarios, and 3) reference data. Furthermore, the analyzed\nliterature includes related safety standards, safety-independent perception\nalgorithm benchmarking, and sensor modeling. We find that the realization of\nsafety-aware perception testing remains an open issue since challenges\nconcerning the three testing axes and their interdependencies currently do not\nappear to be sufficiently solved.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 21:40:39 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Hoss", "Michael", ""], ["Scholtes", "Maike", ""], ["Eckstein", "Lutz", ""]]}, {"id": "2102.08497", "submitter": "Naeemullah Khan", "authors": "Naeemullah Khan, Angira Sharma, Ganesh Sundaramoorthi, Philip H. S.\n  Torr", "title": "Shape-Tailored Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Shape-Tailored Deep Neural Networks (ST-DNN). ST-DNN extend\nconvolutional networks (CNN), which aggregate data from fixed shape (square)\nneighborhoods, to compute descriptors defined on arbitrarily shaped regions.\nThis is natural for segmentation, where descriptors should describe regions\n(e.g., of objects) that have diverse shape. We formulate these descriptors\nthrough the Poisson partial differential equation (PDE), which can be used to\ngeneralize convolution to arbitrary regions. We stack multiple PDE layers to\ngeneralize a deep CNN to arbitrary regions, and apply it to segmentation. We\nshow that ST-DNN are covariant to translations and rotations and robust to\ndomain deformations, natural for segmentation, which existing CNN based methods\nlack. ST-DNN are 3-4 orders of magnitude smaller then CNNs used for\nsegmentation. We show that they exceed segmentation performance compared to\nstate-of-the-art CNN-based descriptors using 2-3 orders smaller training sets\non the texture segmentation problem.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 23:32:14 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Khan", "Naeemullah", ""], ["Sharma", "Angira", ""], ["Sundaramoorthi", "Ganesh", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "2102.08542", "submitter": "Nagashri Lakshminarayana", "authors": "Nagashri Lakshminarayana, Yifang Liu, Karthik Dantu, Venu Govindaraju,\n  Nils Napp", "title": "Active Face Frontalization using Commodity Unmanned Aerial Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a system by which Unmanned Aerial Vehicles (UAVs) can\ngather high-quality face images that can be used in biometric identification\ntasks. Success in face-based identification depends in large part on the image\nquality, and a major factor is how frontal the view is. Face recognition\nsoftware pipelines can improve identification rates by synthesizing frontal\nviews from non-frontal views by a process call {\\em frontalization}. Here we\nexploit the high mobility of UAVs to actively gather frontal images using\ncomponents of a synthetic frontalization pipeline. We define a frontalization\nerror and show that it can be used to guide an UAVs to capture frontal views.\nFurther, we show that the resulting image stream improves matching quality of a\ntypical face recognition similarity metric. The system is implemented using an\noff-the-shelf hardware and software components and can be easily transfered to\nany ROS enabled UAVs.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 02:55:57 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Lakshminarayana", "Nagashri", ""], ["Liu", "Yifang", ""], ["Dantu", "Karthik", ""], ["Govindaraju", "Venu", ""], ["Napp", "Nils", ""]]}, {"id": "2102.08556", "submitter": "Jue Jiang Dr.", "authors": "Jue Jiang, Sadegh Riyahi Alam, Ishita Chen, Perry Zhang, Andreas\n  Rimner, Joseph O. Deasy, Harini Veeraraghavan", "title": "Deep cross-modality (MR-CT) educed distillation learning for cone beam\n  CT lung tumor segmentation", "comments": "The paper has been accepted to Medical Physics", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the widespread availability of in-treatment room cone beam computed\ntomography (CBCT) imaging, due to the lack of reliable segmentation methods,\nCBCT is only used for gross set up corrections in lung radiotherapies. Accurate\nand reliable auto-segmentation tools could potentiate volumetric response\nassessment and geometry-guided adaptive radiation therapies. Therefore, we\ndeveloped a new deep learning CBCT lung tumor segmentation method. Methods: The\nkey idea of our approach called cross modality educed distillation (CMEDL) is\nto use magnetic resonance imaging (MRI) to guide a CBCT segmentation network\ntraining to extract more informative features during training. We accomplish\nthis by training an end-to-end network comprised of unpaired domain adaptation\n(UDA) and cross-domain segmentation distillation networks (SDN) using unpaired\nCBCT and MRI datasets. Feature distillation regularizes the student network to\nextract CBCT features that match the statistical distribution of MRI features\nextracted by the teacher network and obtain better differentiation of tumor\nfrom background.} We also compared against an alternative framework that used\nUDA with MR segmentation network, whereby segmentation was done on the\nsynthesized pseudo MRI representation. All networks were trained with 216\nweekly CBCTs and 82 T2-weighted turbo spin echo MRI acquired from different\npatient cohorts. Validation was done on 20 weekly CBCTs from patients not used\nin training. Independent testing was done on 38 weekly CBCTs from patients not\nused in training or validation. Segmentation accuracy was measured using\nsurface Dice similarity coefficient (SDSC) and Hausdroff distance at 95th\npercentile (HD95) metrics.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 03:52:02 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 15:40:20 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 02:57:47 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Jiang", "Jue", ""], ["Alam", "Sadegh Riyahi", ""], ["Chen", "Ishita", ""], ["Zhang", "Perry", ""], ["Rimner", "Andreas", ""], ["Deasy", "Joseph O.", ""], ["Veeraraghavan", "Harini", ""]]}, {"id": "2102.08567", "submitter": "Chulhong Kim", "authors": "Sampa Misra, Seungwan Jeon, Ravi Managuli, Seiyon Lee, Gyuwon Kim,\n  Seungchul Lee, Richard G Barr, and Chulhong Kim", "title": "Ensemble Transfer Learning of Elastography and B-mode Breast Ultrasound\n  Images", "comments": "17 pages, 10 figures, 6 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computer-aided detection (CAD) of benign and malignant breast lesions becomes\nincreasingly essential in breast ultrasound (US) imaging. The CAD systems rely\non imaging features identified by the medical experts for their performance,\nwhereas deep learning (DL) methods automatically extract features from the\ndata. The challenge of the DL is the insufficiency of breast US images\navailable to train the DL models. Here, we present an ensemble transfer\nlearning model to classify benign and malignant breast tumors using B-mode\nbreast US (B-US) and strain elastography breast US (SE-US) images. This model\ncombines semantic features from AlexNet & ResNet models to classify benign from\nmalignant tumors. We use both B-US and SE-US images to train the model and\nclassify the tumors. We retrospectively gathered 85 patients' data, with 42\nbenign and 43 malignant cases confirmed with the biopsy. Each patient had\nmultiple B-US and their corresponding SE-US images, and the total dataset\ncontained 261 B-US images and 261 SE-US images. Experimental results show that\nour ensemble model achieves a sensitivity of 88.89% and specificity of 91.10%.\nThese diagnostic performances of the proposed method are equivalent to or\nbetter than manual identification. Thus, our proposed ensemble learning method\nwould facilitate detecting early breast cancer, reliably improving patient\ncare.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 04:23:30 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Misra", "Sampa", ""], ["Jeon", "Seungwan", ""], ["Managuli", "Ravi", ""], ["Lee", "Seiyon", ""], ["Kim", "Gyuwon", ""], ["Lee", "Seungchul", ""], ["Barr", "Richard G", ""], ["Kim", "Chulhong", ""]]}, {"id": "2102.08578", "submitter": "Santiago Gonzalez", "authors": "Santiago Gonzalez and Mohak Kant and Risto Miikkulainen", "title": "Evolving GAN Formulations for Higher Quality Image Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have extended deep learning to complex\ngeneration and translation tasks across different data modalities. However,\nGANs are notoriously difficult to train: Mode collapse and other instabilities\nin the training process often degrade the quality of the generated results,\nsuch as images. This paper presents a new technique called TaylorGAN for\nimproving GANs by discovering customized loss functions for each of its two\nnetworks. The loss functions are parameterized as Taylor expansions and\noptimized through multiobjective evolution. On an image-to-image translation\nbenchmark task, this approach qualitatively improves generated image quality\nand quantitatively improves two independent GAN performance metrics. It\ntherefore forms a promising approach for applying GANs to more challenging\ntasks in the future.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 05:11:21 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Gonzalez", "Santiago", ""], ["Kant", "Mohak", ""], ["Miikkulainen", "Risto", ""]]}, {"id": "2102.08597", "submitter": "Aston Zhang", "authors": "Aston Zhang, Yi Tay, Shuai Zhang, Alvin Chan, Anh Tuan Luu, Siu Cheung\n  Hui, Jie Fu", "title": "Beyond Fully-Connected Layers with Quaternions: Parameterization of\n  Hypercomplex Multiplications with $1/n$ Parameters", "comments": "Published as a conference paper at the 9th International Conference\n  on Learning Representations (ICLR 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have demonstrated reasonable success of representation learning\nin hypercomplex space. Specifically, \"fully-connected layers with Quaternions\"\n(4D hypercomplex numbers), which replace real-valued matrix multiplications in\nfully-connected layers with Hamilton products of Quaternions, both enjoy\nparameter savings with only 1/4 learnable parameters and achieve comparable\nperformance in various applications. However, one key caveat is that\nhypercomplex space only exists at very few predefined dimensions (4D, 8D, and\n16D). This restricts the flexibility of models that leverage hypercomplex\nmultiplications. To this end, we propose parameterizing hypercomplex\nmultiplications, allowing models to learn multiplication rules from data\nregardless of whether such rules are predefined. As a result, our method not\nonly subsumes the Hamilton product, but also learns to operate on any arbitrary\nnD hypercomplex space, providing more architectural flexibility using\narbitrarily $1/n$ learnable parameters compared with the fully-connected layer\ncounterpart. Experiments of applications to the LSTM and Transformer models on\nnatural language inference, machine translation, text style transfer, and\nsubject verb agreement demonstrate architectural flexibility and effectiveness\nof the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 06:16:58 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Zhang", "Aston", ""], ["Tay", "Yi", ""], ["Zhang", "Shuai", ""], ["Chan", "Alvin", ""], ["Luu", "Anh Tuan", ""], ["Hui", "Siu Cheung", ""], ["Fu", "Jie", ""]]}, {"id": "2102.08602", "submitter": "Irwan Bello", "authors": "Irwan Bello", "title": "LambdaNetworks: Modeling Long-Range Interactions Without Attention", "comments": "Accepted for publication at the International Conference in Learning\n  Representations 2021 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present lambda layers -- an alternative framework to self-attention -- for\ncapturing long-range interactions between an input and structured contextual\ninformation (e.g. a pixel surrounded by other pixels). Lambda layers capture\nsuch interactions by transforming available contexts into linear functions,\ntermed lambdas, and applying these linear functions to each input separately.\nSimilar to linear attention, lambda layers bypass expensive attention maps, but\nin contrast, they model both content and position-based interactions which\nenables their application to large structured inputs such as images. The\nresulting neural network architectures, LambdaNetworks, significantly\noutperform their convolutional and attentional counterparts on ImageNet\nclassification, COCO object detection and COCO instance segmentation, while\nbeing more computationally efficient. Additionally, we design LambdaResNets, a\nfamily of hybrid architectures across different scales, that considerably\nimproves the speed-accuracy tradeoff of image classification models.\nLambdaResNets reach excellent accuracies on ImageNet while being 3.2 - 4.4x\nfaster than the popular EfficientNets on modern machine learning accelerators.\nWhen training with an additional 130M pseudo-labeled images, LambdaResNets\nachieve up to a 9.5x speed-up over the corresponding EfficientNet checkpoints.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 06:33:47 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Bello", "Irwan", ""]]}, {"id": "2102.08604", "submitter": "Junbum Cha", "authors": "Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun\n  Park, Yunsung Lee, Sungrae Park", "title": "SWAD: Domain Generalization by Seeking Flat Minima", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain generalization (DG) methods aim to achieve generalizability to an\nunseen target domain by using only training data from the source domains.\nAlthough a variety of DG methods have been proposed, a recent study shows that\nunder a fair evaluation protocol, called DomainBed, the simple empirical risk\nminimization (ERM) approach works comparable to or even outperforms previous\nmethods. Unfortunately, simply solving ERM on a complex, non-convex loss\nfunction can easily lead to sub-optimal generalizability by seeking sharp\nminima. In this paper, we theoretically show that finding flat minima results\nin a smaller domain generalization gap. We also propose a simple yet effective\nmethod, named Stochastic Weight Averaging Densely (SWAD), to find flat minima.\nSWAD finds flatter minima and suffers less from overfitting than does the\nvanilla SWA by a dense and overfit-aware stochastic weight sampling strategy.\nSWAD shows state-of-the-art performances on five DG benchmarks, namely PACS,\nVLCS, OfficeHome, TerraIncognita, and DomainNet, with consistent and large\nmargins of +1.6% averagely on out-of-domain accuracy. We also compare SWAD with\nconventional generalization methods, such as data augmentation and consistency\nregularization methods, to verify that the remarkable performance improvements\nare originated from by seeking flat minima, not from better in-domain\ngeneralizability. Last but not least, SWAD is readily adaptable to existing DG\nmethods without modification; the combination of SWAD and an existing DG method\nfurther improves DG performances.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 06:42:09 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 03:47:52 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Cha", "Junbum", ""], ["Chun", "Sanghyuk", ""], ["Lee", "Kyungjae", ""], ["Cho", "Han-Cheol", ""], ["Park", "Seunghyun", ""], ["Lee", "Yunsung", ""], ["Park", "Sungrae", ""]]}, {"id": "2102.08634", "submitter": "Javier Del Ser Dr.", "authors": "Alejandro Barredo Arrieta, Sergio Gil-Lopez, Ibai La\\~na, Miren Nekane\n  Bilbao, Javier Del Ser", "title": "On the Post-hoc Explainability of Deep Echo State Networks for Time\n  Series Forecasting, Image and Video Classification", "comments": "22 pages, 9 figures, 3 tables. Currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since their inception, learning techniques under the Reservoir Computing\nparadigm have shown a great modeling capability for recurrent systems without\nthe computing overheads required for other approaches. Among them, different\nflavors of echo state networks have attracted many stares through time, mainly\ndue to the simplicity and computational efficiency of their learning algorithm.\nHowever, these advantages do not compensate for the fact that echo state\nnetworks remain as black-box models whose decisions cannot be easily explained\nto the general audience. This work addresses this issue by conducting an\nexplainability study of Echo State Networks when applied to learning tasks with\ntime series, image and video data. Specifically, the study proposes three\ndifferent techniques capable of eliciting understandable information about the\nknowledge grasped by these recurrent models, namely, potential memory, temporal\npatterns and pixel absence effect. Potential memory addresses questions related\nto the effect of the reservoir size in the capability of the model to store\ntemporal information, whereas temporal patterns unveils the recurrent\nrelationships captured by the model over time. Finally, pixel absence effect\nattempts at evaluating the effect of the absence of a given pixel when the echo\nstate network model is used for image and video classification. We showcase the\nbenefits of our proposed suite of techniques over three different domains of\napplicability: time series modeling, image and, for the first time in the\nrelated literature, video classification. Our results reveal that the proposed\ntechniques not only allow for a informed understanding of the way these models\nwork, but also serve as diagnostic tools capable of detecting issues inherited\nfrom data (e.g. presence of hidden bias).\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 08:56:33 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Arrieta", "Alejandro Barredo", ""], ["Gil-Lopez", "Sergio", ""], ["La\u00f1a", "Ibai", ""], ["Bilbao", "Miren Nekane", ""], ["Del Ser", "Javier", ""]]}, {"id": "2102.08641", "submitter": "Farshad Ghorbani Veshki", "authors": "Farshad G. Veshki, Nora Ouzir, Sergiy A. Vorobyov, Esa Ollila", "title": "Coupled Feature Learning for Multimodal Medical Image Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal image fusion aims to combine relevant information from images\nacquired with different sensors. In medical imaging, fused images play an\nessential role in both standard and automated diagnosis. In this paper, we\npropose a novel multimodal image fusion method based on coupled dictionary\nlearning. The proposed method is general and can be employed for different\nmedical imaging modalities. Unlike many current medical fusion methods, the\nproposed approach does not suffer from intensity attenuation nor loss of\ncritical information. Specifically, the images to be fused are decomposed into\ncoupled and independent components estimated using sparse representations with\nidentical supports and a Pearson correlation constraint, respectively. An\nalternating minimization algorithm is designed to solve the resulting\noptimization problem. The final fusion step uses the max-absolute-value rule.\nExperiments are conducted using various pairs of multimodal inputs, including\nreal MR-CT and MR-PET images. The resulting performance and execution times\nshow the competitiveness of the proposed method in comparison with\nstate-of-the-art medical image fusion methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 09:13:28 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Veshki", "Farshad G.", ""], ["Ouzir", "Nora", ""], ["Vorobyov", "Sergiy A.", ""], ["Ollila", "Esa", ""]]}, {"id": "2102.08643", "submitter": "Hao Wang", "authors": "Hao Wang, Weining Wang, Jing Liu", "title": "Temporal Memory Attention for Video Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video semantic segmentation requires to utilize the complex temporal\nrelations between frames of the video sequence. Previous works usually exploit\naccurate optical flow to leverage the temporal relations, which suffer much\nfrom heavy computational cost. In this paper, we propose a Temporal Memory\nAttention Network (TMANet) to adaptively integrate the long-range temporal\nrelations over the video sequence based on the self-attention mechanism without\nexhaustive optical flow prediction. Specially, we construct a memory using\nseveral past frames to store the temporal information of the current frame. We\nthen propose a temporal memory attention module to capture the relation between\nthe current frame and the memory to enhance the representation of the current\nframe. Our method achieves new state-of-the-art performances on two challenging\nvideo semantic segmentation datasets, particularly 80.3% mIoU on Cityscapes and\n76.5% mIoU on CamVid with ResNet-50.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 09:18:57 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Wang", "Hao", ""], ["Wang", "Weining", ""], ["Liu", "Jing", ""]]}, {"id": "2102.08660", "submitter": "Pranav Rajpurkar", "authors": "Pranav Rajpurkar, Anirudh Joshi, Anuj Pareek, Andrew Y. Ng, Matthew P.\n  Lungren", "title": "CheXternal: Generalization of Deep Learning Models for Chest X-ray\n  Interpretation to Photos of Chest X-rays and External Clinical Settings", "comments": "Accepted to ACM Conference on Health, Inference, and Learning\n  (ACM-CHIL) 2021. arXiv admin note: substantial text overlap with\n  arXiv:2011.06129", "journal-ref": null, "doi": "10.1145/3450439.3451876", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in training deep learning models have demonstrated the\npotential to provide accurate chest X-ray interpretation and increase access to\nradiology expertise. However, poor generalization due to data distribution\nshifts in clinical settings is a key barrier to implementation. In this study,\nwe measured the diagnostic performance for 8 different chest X-ray models when\napplied to (1) smartphone photos of chest X-rays and (2) external datasets\nwithout any finetuning. All models were developed by different groups and\nsubmitted to the CheXpert challenge, and re-applied to test datasets without\nfurther tuning. We found that (1) on photos of chest X-rays, all 8 models\nexperienced a statistically significant drop in task performance, but only 3\nperformed significantly worse than radiologists on average, and (2) on the\nexternal set, none of the models performed statistically significantly worse\nthan radiologists, and five models performed statistically significantly better\nthan radiologists. Our results demonstrate that some chest X-ray models, under\nclinically relevant distribution shifts, were comparable to radiologists while\nother models were not. Future work should investigate aspects of model training\nprocedures and dataset collection that influence generalization in the presence\nof data distribution shifts.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 09:58:14 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2021 01:59:37 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Rajpurkar", "Pranav", ""], ["Joshi", "Anirudh", ""], ["Pareek", "Anuj", ""], ["Ng", "Andrew Y.", ""], ["Lungren", "Matthew P.", ""]]}, {"id": "2102.08663", "submitter": "Yuhta Takida", "authors": "Yuhta Takida, Wei-Hsiang Liao, Toshimitsu Uesaka, Shusuke Takahashi\n  and Yuki Mitsufuji", "title": "Preventing Posterior Collapse Induced by Oversmoothing in Gaussian VAE", "comments": "21 pages with 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Variational autoencoders (VAEs) often suffer from posterior collapse, which\nis a phenomenon in which the learned latent space becomes uninformative. This\nis often related to a hyperparameter resembling the data variance. It can be\nshown that an inappropriate choice of this parameter causes oversmoothness and\nleads to posterior collapse in the linearly approximated case and can be\nempirically verified for the general cases. Therefore, we propose AR-ELBO\n(Adaptively Regularized Evidence Lower BOund), which controls the smoothness of\nthe model by adapting this variance parameter. In addition, we extend VAE with\nalternative parameterizations on the variance parameter to deal with\nnon-uniform or conditional data variance. The proposed VAE extensions trained\nwith AR-ELBO show improved Fr\\'echet inception distance (FID) on images\ngenerated from the MNIST and CelebA datasets.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 10:00:49 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Takida", "Yuhta", ""], ["Liao", "Wei-Hsiang", ""], ["Uesaka", "Toshimitsu", ""], ["Takahashi", "Shusuke", ""], ["Mitsufuji", "Yuki", ""]]}, {"id": "2102.08665", "submitter": "Nicolas Guigui", "authors": "Nicolas Guigui (UCA, EPIONE), Pamela Moceri (URRIS UR2CA), Maxime\n  Sermesant (UCA, EPIONE), Xavier Pennec (UCA, EPIONE)", "title": "Cardiac Motion Modeling with Parallel Transport and Shape Splines", "comments": null, "journal-ref": "International Symposium on Biological Imaging, Apr 2021, Nice,\n  France", "doi": null, "report-no": null, "categories": "cs.CV math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cases of pressure or volume overload, probing cardiac function may be\ndifficult because of the interactions between shape and deformations.In this\nwork, we use the LDDMM framework and parallel transport to estimate and\nreorient deformations of the right ventricle. We then propose a normalization\nprocedure for the amplitude of the deformation, and a second-order spline model\nto represent the full cardiac contraction. The method is applied to 3D meshes\nof the right ventricle extracted from echocardiographic sequences of 314\npatients divided into three disease categories and a control group. We find\nsignificant differences between pathologies in the model parameters, revealing\ninsights into the dynamics of each disease.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 10:03:32 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Guigui", "Nicolas", "", "UCA, EPIONE"], ["Moceri", "Pamela", "", "URRIS UR2CA"], ["Sermesant", "Maxime", "", "UCA, EPIONE"], ["Pennec", "Xavier", "", "UCA, EPIONE"]]}, {"id": "2102.08708", "submitter": "Waqas Sultani", "authors": "Qazi Ammar Arshad, Mohsen Ali, Saeed-ul Hassan, Chen Chen, Ayisha\n  Imran, Ghulam Rasul, Waqas Sultani", "title": "A Dataset and Benchmark for Malaria Life-Cycle Classification in Thin\n  Blood Smear Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Malaria microscopy, microscopic examination of stained blood slides to detect\nparasite Plasmodium, is considered to be a gold-standard for detecting\nlife-threatening disease malaria. Detecting the plasmodium parasite requires a\nskilled examiner and may take up to 10 to 15 minutes to completely go through\nthe whole slide. Due to a lack of skilled medical professionals in the\nunderdeveloped or resource deficient regions, many cases go misdiagnosed;\nresulting in unavoidable complications and/or undue medication. We propose to\ncomplement the medical professionals by creating a deep learning-based method\nto automatically detect (localize) the plasmodium parasites in the photograph\nof stained film. To handle the unbalanced nature of the dataset, we adopt a\ntwo-stage approach. Where the first stage is trained to detect blood cells and\nclassify them into just healthy or infected. The second stage is trained to\nclassify each detected cell further into the life-cycle stage. To facilitate\nthe research in machine learning-based malaria microscopy, we introduce a new\nlarge scale microscopic image malaria dataset. Thirty-eight thousand cells are\ntagged from the 345 microscopic images of different Giemsa-stained slides of\nblood samples. Extensive experimentation is performed using different CNN\nbackbones including VGG, DenseNet, and ResNet on this dataset. Our experiments\nand analysis reveal that the two-stage approach works better than the one-stage\napproach for malaria detection. To ensure the usability of our approach, we\nhave also developed a mobile app that will be used by local hospitals for\ninvestigation and educational purposes. The dataset, its annotations, and\nimplementation codes will be released upon publication of the paper.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 11:44:52 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Arshad", "Qazi Ammar", ""], ["Ali", "Mohsen", ""], ["Hassan", "Saeed-ul", ""], ["Chen", "Chen", ""], ["Imran", "Ayisha", ""], ["Rasul", "Ghulam", ""], ["Sultani", "Waqas", ""]]}, {"id": "2102.08742", "submitter": "Denis Coquenet", "authors": "Denis Coquenet, Cl\\'ement Chatelain, Thierry Paquet", "title": "SPAN: a Simple Predict & Align Network for Handwritten Paragraph\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unconstrained handwriting recognition is an essential task in document\nanalysis. It is usually carried out in two steps. First, the document is\nsegmented into text lines. Second, an Optical Character Recognition model is\napplied on these line images. We propose the Simple Predict & Align Network: an\nend-to-end recurrence-free Fully Convolutional Network performing OCR at\nparagraph level without any prior segmentation stage. The framework is as\nsimple as the one used for the recognition of isolated lines and we achieve\ncompetitive results on three popular datasets: RIMES, IAM and READ 2016. The\nproposed model does not require any dataset adaptation, it can be trained from\nscratch, without segmentation labels, and it does not require line breaks in\nthe transcription labels. Our code and trained model weights are available at\nhttps://github.com/FactoDeepLearning/SPAN.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 13:12:45 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Coquenet", "Denis", ""], ["Chatelain", "Cl\u00e9ment", ""], ["Paquet", "Thierry", ""]]}, {"id": "2102.08747", "submitter": "Sebastian Monka", "authors": "Sebastian Monka, Lavdim Halilaj, Stefan Schmid, Achim Rettinger", "title": "Learning Visual Models using a Knowledge Graph as a Trainer", "comments": "ISWC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional computer vision approaches, based on neural networks (NN), are\ntypically trained on a large amount of image data. By minimizing the\ncross-entropy loss between a prediction and a given class label, the NN and its\nvisual embedding space are learned to fulfill a given task. However, due to the\nsole dependence on the image data distribution of the training domain, these\nmodels tend to fail when applied to a target domain that differs from their\nsource domain. To learn a more robust NN to domain shifts, we propose the\nknowledge graph neural network (KG-NN), a neuro-symbolic approach that\nsupervises the training using image-data-invariant auxiliary knowledge. The\nauxiliary knowledge is first encoded in a knowledge graph with respective\nconcepts and their relationships, which is then transformed into a dense vector\nrepresentation via an embedding method. Using a contrastive loss function,\nKG-NN learns to adapt its visual embedding space and thus its weights according\nto the image-data invariant knowledge graph embedding space. We evaluate KG-NN\non visual transfer learning tasks for classification using the mini-ImageNet\ndataset and its derivatives, as well as road sign recognition datasets from\nGermany and China. The results show that a visual model trained with a\nknowledge graph as a trainer outperforms a model trained with cross-entropy in\nall experiments, in particular when the domain gap increases. Besides better\nperformance and stronger robustness to domain shifts, these KG-NN adapts to\nmultiple datasets and classes without suffering heavily from catastrophic\nforgetting.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 13:24:41 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 13:21:17 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Monka", "Sebastian", ""], ["Halilaj", "Lavdim", ""], ["Schmid", "Stefan", ""], ["Rettinger", "Achim", ""]]}, {"id": "2102.08820", "submitter": "Mehmet Ozgur Turkoglu", "authors": "Mehmet Ozgur Turkoglu, Stefano D'Aronco, Gregor Perich, Frank\n  Liebisch, Constantin Streit, Konrad Schindler, Jan Dirk Wegner", "title": "Crop mapping from image time series: deep learning with multi-scale\n  label hierarchies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The aim of this paper is to map agricultural crops by classifying satellite\nimage time series. Domain experts in agriculture work with crop type labels\nthat are organised in a hierarchical tree structure, where coarse classes (like\norchards) are subdivided into finer ones (like apples, pears, vines, etc.). We\ndevelop a crop classification method that exploits this expert knowledge and\nsignificantly improves the mapping of rare crop types. The three-level label\nhierarchy is encoded in a convolutional, recurrent neural network (convRNN),\nsuch that for each pixel the model predicts three labels at different level of\ngranularity. This end-to-end trainable, hierarchical network architecture\nallows the model to learn joint feature representations of rare classes (e.g.,\napples, pears) at a coarser level (e.g., orchard), thereby boosting\nclassification performance at the fine-grained level. Additionally, labelling\nat different granularity also makes it possible to adjust the output according\nto the classification scores; as coarser labels with high confidence are\nsometimes more useful for agricultural practice than fine-grained but very\nuncertain labels. We validate the proposed method on a new, large dataset that\nwe make public. ZueriCrop covers an area of 50 km x 48 km in the Swiss cantons\nof Zurich and Thurgau with a total of 116'000 individual fields spanning 48\ncrop classes, and 28,000 (multi-temporal) image patches from Sentinel-2. We\ncompare our proposed hierarchical convRNN model with several baselines,\nincluding methods designed for imbalanced class distributions. The hierarchical\napproach performs superior by at least 9.9 percentage points in F1-score.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 15:27:49 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Turkoglu", "Mehmet Ozgur", ""], ["D'Aronco", "Stefano", ""], ["Perich", "Gregor", ""], ["Liebisch", "Frank", ""], ["Streit", "Constantin", ""], ["Schindler", "Konrad", ""], ["Wegner", "Jan Dirk", ""]]}, {"id": "2102.08850", "submitter": "Roland Zimmermann", "authors": "Roland S. Zimmermann, Yash Sharma, Steffen Schneider, Matthias Bethge,\n  Wieland Brendel", "title": "Contrastive Learning Inverts the Data Generating Process", "comments": "Presented at ICML 2021. The first three authors, as well as the last\n  two authors, contributed equally. Code is available at\n  https://brendel-group.github.io/cl-ica", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive learning has recently seen tremendous success in self-supervised\nlearning. So far, however, it is largely unclear why the learned\nrepresentations generalize so effectively to a large variety of downstream\ntasks. We here prove that feedforward models trained with objectives belonging\nto the commonly used InfoNCE family learn to implicitly invert the underlying\ngenerative model of the observed data. While the proofs make certain\nstatistical assumptions about the generative model, we observe empirically that\nour findings hold even if these assumptions are severely violated. Our theory\nhighlights a fundamental connection between contrastive learning, generative\nmodeling, and nonlinear independent component analysis, thereby furthering our\nunderstanding of the learned representations as well as providing a theoretical\nfoundation to derive more effective contrastive losses.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 16:21:54 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 16:01:36 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 16:36:09 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Zimmermann", "Roland S.", ""], ["Sharma", "Yash", ""], ["Schneider", "Steffen", ""], ["Bethge", "Matthias", ""], ["Brendel", "Wieland", ""]]}, {"id": "2102.08860", "submitter": "Konstantinos Rematas", "authors": "Konstantinos Rematas, Ricardo Martin-Brualla, Vittorio Ferrari", "title": "ShaRF: Shape-conditioned Radiance Fields from a Single View", "comments": "Project page: http://www.krematas.com/sharf/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for estimating neural scenes representations of objects\ngiven only a single image. The core of our method is the estimation of a\ngeometric scaffold for the object and its use as a guide for the reconstruction\nof the underlying radiance field. Our formulation is based on a generative\nprocess that first maps a latent code to a voxelized shape, and then renders it\nto an image, with the object appearance being controlled by a second latent\ncode. During inference, we optimize both the latent codes and the networks to\nfit a test image of a new object. The explicit disentanglement of shape and\nappearance allows our model to be fine-tuned given a single image. We can then\nrender new views in a geometrically consistent manner and they represent\nfaithfully the input object. Additionally, our method is able to generalize to\nimages outside of the training domain (more realistic renderings and even real\nphotographs). Finally, the inferred geometric scaffold is itself an accurate\nestimate of the object's 3D shape. We demonstrate in several experiments the\neffectiveness of our approach in both synthetic and real images.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 16:40:28 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 09:53:38 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Rematas", "Konstantinos", ""], ["Martin-Brualla", "Ricardo", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "2102.08868", "submitter": "Fartash Faghri", "authors": "Fartash Faghri, Sven Gowal, Cristina Vasconcelos, David J. Fleet,\n  Fabian Pedregosa, Nicolas Le Roux", "title": "Bridging the Gap Between Adversarial Robustness and Optimization Bias", "comments": "New CIFAR-10 experiments and Fourier attack variations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that the choice of optimizer, neural network architecture, and\nregularizer significantly affect the adversarial robustness of linear neural\nnetworks, providing guarantees without the need for adversarial training. To\nthis end, we revisit a known result linking maximally robust classifiers and\nminimum norm solutions, and combine it with recent results on the implicit bias\nof optimizers. First, we show that, under certain conditions, it is possible to\nachieve both perfect standard accuracy and a certain degree of robustness,\nsimply by training an overparametrized model using the implicit bias of the\noptimization. In that regime, there is a direct relationship between the type\nof the optimizer and the attack to which the model is robust. To the best of\nour knowledge, this work is the first to study the impact of optimization\nmethods such as sign gradient descent and proximal methods on adversarial\nrobustness. Second, we characterize the robustness of linear convolutional\nmodels, showing that they resist attacks subject to a constraint on the\nFourier-$\\ell_\\infty$ norm. To illustrate these findings we design a novel\nFourier-$\\ell_\\infty$ attack that finds adversarial examples with controllable\nfrequencies. We evaluate Fourier-$\\ell_\\infty$ robustness of\nadversarially-trained deep CIFAR-10 models from the standard RobustBench\nbenchmark and visualize adversarial perturbations.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 16:58:04 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 15:27:16 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Faghri", "Fartash", ""], ["Gowal", "Sven", ""], ["Vasconcelos", "Cristina", ""], ["Fleet", "David J.", ""], ["Pedregosa", "Fabian", ""], ["Roux", "Nicolas Le", ""]]}, {"id": "2102.08871", "submitter": "Ivona Tautkute", "authors": "Ivona Tautkute and Tomasz Trzcinski", "title": "I Want This Product but Different : Multimodal Retrieval with Synthetic\n  Query Expansion", "comments": "Major edits", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses the problem of media retrieval using a multimodal query\n(a query which combines visual input with additional semantic information in\nnatural language feedback). We propose a SynthTriplet GAN framework which\nresolves this task by expanding the multimodal query with a synthetically\ngenerated image that captures semantic information from both image and text\ninput. We introduce a novel triplet mining method that uses a synthetic image\nas an anchor to directly optimize for embedding distances of generated and\ntarget images. We demonstrate that apart from the added value of retrieval\nillustration with synthetic image with the focus on customization and user\nfeedback, the proposed method greatly surpasses other multimodal generation\nmethods and achieves state of the art results in the multimodal retrieval task.\nWe also show that in contrast to other retrieval methods, our method provides\nexplainable embeddings.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 17:02:13 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 14:35:23 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Tautkute", "Ivona", ""], ["Trzcinski", "Tomasz", ""]]}, {"id": "2102.08884", "submitter": "Cheng Zhang", "authors": "Cheng Zhang, Tai-Yu Pan, Yandong Li, Hexiang Hu, Dong Xuan, Soravit\n  Changpinyo, Boqing Gong, Wei-Lun Chao", "title": "MosaicOS: A Simple and Effective Use of Object-Centric Images for\n  Long-Tailed Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many objects do not appear frequently enough in complex scenes (e.g., certain\nhandbags in living rooms) for training an accurate object detector, but are\noften found frequently by themselves (e.g., in product images). Yet, these\nobject-centric images are not effectively leveraged for improving object\ndetection in scene-centric images. In this paper, we propose Mosaic of\nObject-centric images as Scene-centric images (MosaicOS), a simple and novel\nframework that is surprisingly effective at tackling the challenges of\nlong-tailed object detection. Keys to our approach are three-fold: (i) pseudo\nscene-centric image construction from object-centric images for mitigating\ndomain differences, (ii) high-quality bounding box imputation using the\nobject-centric images' class labels, and (iii) a multi-stage training\nprocedure. On LVIS object detection (and instance segmentation), MosaicOS leads\nto a massive 60% (and 23%) relative improvement in average precision for rare\nobject categories. We also show that our framework can be compatibly used with\nother existing approaches to achieve even further gains.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 17:27:21 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 17:40:31 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Zhang", "Cheng", ""], ["Pan", "Tai-Yu", ""], ["Li", "Yandong", ""], ["Hu", "Hexiang", ""], ["Xuan", "Dong", ""], ["Changpinyo", "Soravit", ""], ["Gong", "Boqing", ""], ["Chao", "Wei-Lun", ""]]}, {"id": "2102.08941", "submitter": "Joseph Robinson", "authors": "Joseph P Robinson", "title": "Automatic Face Understanding: Recognizing Families in Photos", "comments": "PhD Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We built the largest database for kinship recognition. The data were labeled\nusing a novel clustering algorithm that used label proposals as side\ninformation to guide more accurate clusters. Great savings in time and human\ninput was had. Statistically, FIW shows enormous gains over its predecessors.\nWe have several benchmarks in kinship verification, family classification,\ntri-subject verification, and large-scale search and retrieval. We also trained\nCNNs on FIW and deployed the model on the renowned KinWild I and II to gain\nSOTA. Most recently, we further augmented FIW with MM. Now, video dynamics,\naudio, and text captions can be used in the decision making of kinship\nrecognition systems. We expect FIW will significantly impact research and\nreality. Additionally, we tackled the classic problem of facial landmark\nlocalization. A majority of these networks have objectives based on L1 or L2\nnorms, which inherit several disadvantages. The locations of landmarks are\ndetermined from generated heatmaps from which predicted landmark locations get\npenalized without accounting for the spread: a high scatter corresponds to low\nconfidence and vice-versa. To address this, we introduced an objective that\npenalizes for low confidence. Another issue is a dependency on labeled data,\nwhich is expensive to collect and susceptible to error. We addressed both\nissues by proposing an adversarial training framework that leverages unlabeled\ndata to improve model performance. Our method claims SOTA on renowned\nbenchmarks. Furthermore, our model is robust with a reduced size: 1/8 the\nnumber of channels is comparable to SOTA in real-time on a CPU. Finally, we\nbuilt BFW to serve as a proxy to measure bias across ethnicity and gender\nsubgroups, allowing us to characterize FR performances per subgroup. We show\nperformances are non-optimal when a single threshold is used to determine\nwhether sample pairs are genuine.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 22:37:25 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Robinson", "Joseph P", ""]]}, {"id": "2102.08945", "submitter": "Zan Gojcic", "authors": "Zan Gojcic, Or Litany, Andreas Wieser, Leonidas J. Guibas, Tolga\n  Birdal", "title": "Weakly Supervised Learning of Rigid 3D Scene Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a data-driven scene flow estimation algorithm exploiting the\nobservation that many 3D scenes can be explained by a collection of agents\nmoving as rigid bodies. At the core of our method lies a deep architecture able\nto reason at the \\textbf{object-level} by considering 3D scene flow in\nconjunction with other 3D tasks. This object level abstraction, enables us to\nrelax the requirement for dense scene flow supervision with simpler binary\nbackground segmentation mask and ego-motion annotations. Our mild supervision\nrequirements make our method well suited for recently released massive data\ncollections for autonomous driving, which do not contain dense scene flow\nannotations. As output, our model provides low-level cues like pointwise flow\nand higher-level cues such as holistic scene understanding at the level of\nrigid objects. We further propose a test-time optimization refining the\npredicted rigid scene flow. We showcase the effectiveness and generalization\ncapacity of our method on four different autonomous driving datasets. We\nrelease our source code and pre-trained models under\n\\url{github.com/zgojcic/Rigid3DSceneFlow}.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 18:58:02 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Gojcic", "Zan", ""], ["Litany", "Or", ""], ["Wieser", "Andreas", ""], ["Guibas", "Leonidas J.", ""], ["Birdal", "Tolga", ""]]}, {"id": "2102.08946", "submitter": "Zhiqiang Shen", "authors": "Zhiqiang Shen and Zechun Liu and Jie Qin and Lei Huang and Kwang-Ting\n  Cheng and Marios Savvides", "title": "S2-BNN: Bridging the Gap Between Self-Supervised Real and 1-bit Neural\n  Networks via Guided Distribution Calibration", "comments": "CVPR 2021 camera-ready version. Self-supervised binary neural\n  networks using distillation loss (5.5~15% improvement over contrastive\n  baseline). Code is available at https://github.com/szq0214/S2-BNN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies dominantly target at self-supervised learning on real-valued\nnetworks and have achieved many promising results. However, on the more\nchallenging binary neural networks (BNNs), this task has not yet been fully\nexplored in the community. In this paper, we focus on this more difficult\nscenario: learning networks where both weights and activations are binary,\nmeanwhile, without any human annotated labels. We observe that the commonly\nused contrastive objective is not satisfying on BNNs for competitive accuracy,\nsince the backbone network contains relatively limited capacity and\nrepresentation ability. Hence instead of directly applying existing\nself-supervised methods, which cause a severe decline in performance, we\npresent a novel guided learning paradigm from real-valued to distill binary\nnetworks on the final prediction distribution, to minimize the loss and obtain\ndesirable accuracy. Our proposed method can boost the simple contrastive\nlearning baseline by an absolute gain of 5.5~15% on BNNs. We further reveal\nthat it is difficult for BNNs to recover the similar predictive distributions\nas real-valued models when training without labels. Thus, how to calibrate them\nis key to address the degradation in performance. Extensive experiments are\nconducted on the large-scale ImageNet and downstream datasets. Our method\nachieves substantial improvement over the simple contrastive learning baseline,\nand is even comparable to many mainstream supervised BNN methods. Code is\navailable at https://github.com/szq0214/S2-BNN.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 18:59:28 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 16:10:03 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Shen", "Zhiqiang", ""], ["Liu", "Zechun", ""], ["Qin", "Jie", ""], ["Huang", "Lei", ""], ["Cheng", "Kwang-Ting", ""], ["Savvides", "Marios", ""]]}, {"id": "2102.08981", "submitter": "Soravit Changpinyo", "authors": "Soravit Changpinyo, Piyush Sharma, Nan Ding, Radu Soricut", "title": "Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize\n  Long-Tail Visual Concepts", "comments": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR\n  2021). Our dataset is available at\n  https://github.com/google-research-datasets/conceptual-12m", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The availability of large-scale image captioning and visual question\nanswering datasets has contributed significantly to recent successes in\nvision-and-language pre-training. However, these datasets are often collected\nwith overrestrictive requirements inherited from their original target tasks\n(e.g., image caption generation), which limit the resulting dataset scale and\ndiversity. We take a step further in pushing the limits of vision-and-language\npre-training data by relaxing the data collection pipeline used in Conceptual\nCaptions 3M (CC3M) [Sharma et al. 2018] and introduce the Conceptual 12M\n(CC12M), a dataset with 12 million image-text pairs specifically meant to be\nused for vision-and-language pre-training. We perform an analysis of this\ndataset and benchmark its effectiveness against CC3M on multiple downstream\ntasks with an emphasis on long-tail visual recognition. Our results clearly\nillustrate the benefit of scaling up pre-training data for vision-and-language\ntasks, as indicated by the new state-of-the-art results on both the nocaps and\nConceptual Captions benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 19:15:53 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 08:20:34 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Changpinyo", "Soravit", ""], ["Sharma", "Piyush", ""], ["Ding", "Nan", ""], ["Soricut", "Radu", ""]]}, {"id": "2102.08983", "submitter": "Sofia Broom\\'e", "authors": "Zhenghong Li, Sofia Broom\\'e, Pia Haubro Andersen, Hedvig Kjellstr\\\"om", "title": "Automated Detection of Equine Facial Action Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently developed Equine Facial Action Coding System (EquiFACS) provides\na precise and exhaustive, but laborious, manual labelling method of facial\naction units of the horse. To automate parts of this process, we propose a Deep\nLearning-based method to detect EquiFACS units automatically from images. We\nuse a cascade framework; we firstly train several object detectors to detect\nthe predefined Region-of-Interest (ROI), and secondly apply binary classifiers\nfor each action unit in related regions. We experiment with both regular CNNs\nand a more tailored model transferred from human facial action unit\nrecognition. Promising initial results are presented for nine action units in\nthe eye and lower face regions. Code for the project is publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 19:22:39 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 12:50:13 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Li", "Zhenghong", ""], ["Broom\u00e9", "Sofia", ""], ["Andersen", "Pia Haubro", ""], ["Kjellstr\u00f6m", "Hedvig", ""]]}, {"id": "2102.08990", "submitter": "Xing Li", "authors": "Xing Li, Haichun Yang, Jiaxin He, Aadarsh Jha, Agnes B. Fogo, Lee E.\n  Wheless, Shilin Zhao, Yuankai Huo", "title": "BEDS: Bagging ensemble deep segmentation for nucleus segmentation with\n  testing stage stain augmentation", "comments": "4 pages, 5 figures, ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing outcome variance is an essential task in deep learning based medical\nimage analysis. Bootstrap aggregating, also known as bagging, is a canonical\nensemble algorithm for aggregating weak learners to become a strong learner.\nRandom forest is one of the most powerful machine learning algorithms before\ndeep learning era, whose superior performance is driven by fitting bagged\ndecision trees (weak learners). Inspired by the random forest technique, we\npropose a simple bagging ensemble deep segmentation (BEDs) method to train\nmultiple U-Nets with partial training data to segment dense nuclei on\npathological images. The contributions of this study are three-fold: (1)\ndeveloping a self-ensemble learning framework for nucleus segmentation; (2)\naggregating testing stage augmentation with self-ensemble learning; and (3)\nelucidating the idea that self-ensemble and testing stage stain augmentation\nare complementary strategies for a superior segmentation performance.\nImplementation Detail: https://github.com/xingli1102/BEDs.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 19:34:41 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Li", "Xing", ""], ["Yang", "Haichun", ""], ["He", "Jiaxin", ""], ["Jha", "Aadarsh", ""], ["Fogo", "Agnes B.", ""], ["Wheless", "Lee E.", ""], ["Zhao", "Shilin", ""], ["Huo", "Yuankai", ""]]}, {"id": "2102.08997", "submitter": "Alberto Sabater", "authors": "Alberto Sabater, Laura Santos, Jose Santos-Victor, Alexandre\n  Bernardino, Luis Montesano, Ana C. Murillo", "title": "One-shot action recognition in challenging therapy scenarios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One-shot action recognition aims to recognize new action categories from a\nsingle reference example, typically referred to as the anchor example. This\nwork presents a novel approach for one-shot action recognition in the wild that\ncomputes motion representations robust to variable kinematic conditions.\nOne-shot action recognition is then performed by evaluating anchor and target\nmotion representations. We also develop a set of complementary steps that boost\nthe action recognition performance in the most challenging scenarios. Our\napproach is evaluated on the public NTU-120 one-shot action recognition\nbenchmark, outperforming previous action recognition models. Besides, we\nevaluate our framework on a real use-case of therapy with autistic people.\nThese recordings are particularly challenging due to high-level artifacts from\nthe patient motion. Our results provide not only quantitative but also online\nqualitative measures, essential for the patient evaluation and monitoring\nduring the actual therapy.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 19:41:37 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 09:12:15 GMT"}, {"version": "v3", "created": "Tue, 20 Apr 2021 15:10:21 GMT"}, {"version": "v4", "created": "Thu, 29 Jul 2021 15:25:40 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Sabater", "Alberto", ""], ["Santos", "Laura", ""], ["Santos-Victor", "Jose", ""], ["Bernardino", "Alexandre", ""], ["Montesano", "Luis", ""], ["Murillo", "Ana C.", ""]]}, {"id": "2102.09000", "submitter": "Mauricio Delbracio", "authors": "Mauricio Delbracio, Damien Kelly, Michael S. Brown, Peyman Milanfar", "title": "Mobile Computational Photography: A Tour", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first mobile camera phone was sold only 20 years ago, when taking\npictures with one's phone was an oddity, and sharing pictures online was\nunheard of. Today, the smartphone is more camera than phone. How did this\nhappen? This transformation was enabled by advances in computational\nphotography -the science and engineering of making great images from small form\nfactor, mobile cameras. Modern algorithmic and computing advances, including\nmachine learning, have changed the rules of photography, bringing to it new\nmodes of capture, post-processing, storage, and sharing. In this paper, we give\na brief history of mobile computational photography and describe some of the\nkey technological components, including burst photography, noise reduction, and\nsuper-resolution. At each step, we may draw naive parallels to the human visual\nsystem.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 19:43:28 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 06:32:25 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Delbracio", "Mauricio", ""], ["Kelly", "Damien", ""], ["Brown", "Michael S.", ""], ["Milanfar", "Peyman", ""]]}, {"id": "2102.09003", "submitter": "Vinod Kumar Kurmi", "authors": "Vinod K Kurmi and Venkatesh K Subramanian and Vinay P Namboodiri", "title": "Domain Impression: A Source Data Free Domain Adaptation Method", "comments": "Published- WACV-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Unsupervised Domain adaptation methods solve the adaptation problem for an\nunlabeled target set, assuming that the source dataset is available with all\nlabels. However, the availability of actual source samples is not always\npossible in practical cases. It could be due to memory constraints, privacy\nconcerns, and challenges in sharing data. This practical scenario creates a\nbottleneck in the domain adaptation problem. This paper addresses this\nchallenging scenario by proposing a domain adaptation technique that does not\nneed any source data. Instead of the source data, we are only provided with a\nclassifier that is trained on the source data. Our proposed approach is based\non a generative framework, where the trained classifier is used for generating\nsamples from the source classes. We learn the joint distribution of data by\nusing the energy-based modeling of the trained classifier. At the same time, a\nnew classifier is also adapted for the target domain. We perform various\nablation analysis under different experimental setups and demonstrate that the\nproposed approach achieves better results than the baseline models in this\nextremely novel scenario.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 19:50:49 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Kurmi", "Vinod K", ""], ["Subramanian", "Venkatesh K", ""], ["Namboodiri", "Vinay P", ""]]}, {"id": "2102.09099", "submitter": "Mohamed Amgad", "authors": "Mohamed Amgad (1), Lamees A. Atteya (2), Hagar Hussein (3), Kareem\n  Hosny Mohammed (4), Ehab Hafiz (5), Maha A.T. Elsebaie (6), Ahmed M.\n  Alhusseiny (7), Mohamed Atef AlMoslemany (8), Abdelmagid M. Elmatboly (9),\n  Philip A. Pappalardo (10), Rokia Adel Sakr (11), Pooya Mobadersany (1), Ahmad\n  Rachid (12), Anas M. Saad (13), Ahmad M. Alkashash (14), Inas A. Ruhban (15),\n  Anas Alrefai (12), Nada M. Elgazar (16), Ali Abdulkarim (17), Abo-Alela Farag\n  (12), Amira Etman (8), Ahmed G. Elsaeed (16), Yahya Alagha (17), Yomna A.\n  Amer (8), Ahmed M. Raslan (18), Menatalla K. Nadim (19), Mai A.T. Elsebaie\n  (12), Ahmed Ayad (20), Liza E. Hanna (3), Ahmed Gadallah (12), Mohamed Elkady\n  (21), Bradley Drumheller (22), David Jaye (22), David Manthey (23), David A.\n  Gutman (24), Habiba Elfandy (25, 26), Lee A.D. Cooper (1, 27, 28) ((1)\n  Department of Pathology, Northwestern University, Chicago, IL, USA, (2) Cairo\n  Health Care Administration, Egyptian Ministry of Health, Cairo, Egypt, (3)\n  Department of Pathology, Nasser institute for research and treatment, Cairo,\n  Egypt, (4) Department of Pathology and Laboratory Medicine, University of\n  Pennsylvania, PA, USA, (5) Department of Clinical Laboratory Research,\n  Theodor Bilharz Research Institute, Giza, Egypt, (6) Department of Medicine,\n  Cook County Hospital, Chicago, IL, USA, (7) Department of Pathology, Baystate\n  Medical Center, University of Massachusetts, Springfield, MA, USA, (8)\n  Faculty of Medicine, Menoufia University, Menoufia, Egypt, (9) Faculty of\n  Medicine, Al-Azhar University, Cairo, Egypt, (10) Consultant for The Center\n  for Applied Proteomics and Molecular Medicine (CAPMM), George Mason\n  University, Manassas, VA, USA, (11) Department of Pathology, National Liver\n  Institute, Menoufia University, Menoufia, Egypt, (12) Faculty of Medicine,\n  Ain Shams University, Cairo, Egypt, (13) Cleveland Clinic Foundation,\n  Cleveland, OH, USA, (14) Department of Pathology, Indiana University,\n  Indianapolis, IN, USA, (15) Faculty of Medicine, Damascus University,\n  Damascus, Syria, (16) Faculty of Medicine, Mansoura University, Mansoura,\n  Egypt, (17) Faculty of Medicine, Cairo University, Cairo, Egypt, (18)\n  Department of Anaesthesia and Critical Care, Menoufia University Hospital,\n  Menoufia, Egypt, (19) Department of Clinical Pathology, Ain Shams University,\n  Cairo, Egypt, (20) Research Department, Oncology Consultants, PA, Houston,\n  TX, USA, (21) Siparadigm Diagnostic Informatics, Pine Brook, NJ, USA, (22)\n  Department of Pathology and Laboratory Medicine, Emory University School of\n  Medicine, Atlanta, GA, USA, (23) Kitware Inc., Clifton Park, NY, USA, (24)\n  Department of Neurology, Emory University School of Medicine, Atlanta, GA,\n  USA, (25) Department of Pathology, National Cancer Institute, Cairo, Egypt,\n  (26) Department of Pathology, Children's Cancer Hospital Egypt CCHE 57357,\n  Cairo, Egypt, (27) Lurie Cancer Center, Northwestern University, Chicago, IL,\n  USA, (28) Center for Computational Imaging and Signal Analytics, Northwestern\n  University Feinberg School of Medicine, Chicago, IL, USA)", "title": "NuCLS: A scalable crowdsourcing, deep learning approach and dataset for\n  nucleus classification, localization and segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-resolution mapping of cells and tissue structures provides a foundation\nfor developing interpretable machine-learning models for computational\npathology. Deep learning algorithms can provide accurate mappings given large\nnumbers of labeled instances for training and validation. Generating adequate\nvolume of quality labels has emerged as a critical barrier in computational\npathology given the time and effort required from pathologists. In this paper\nwe describe an approach for engaging crowds of medical students and\npathologists that was used to produce a dataset of over 220,000 annotations of\ncell nuclei in breast cancers. We show how suggested annotations generated by a\nweak algorithm can improve the accuracy of annotations generated by non-experts\nand can yield useful data for training segmentation algorithms without\nlaborious manual tracing. We systematically examine interrater agreement and\ndescribe modifications to the MaskRCNN model to improve cell mapping. We also\ndescribe a technique we call Decision Tree Approximation of Learned Embeddings\n(DTALE) that leverages nucleus segmentations and morphologic features to\nimprove the transparency of nucleus classification models. The annotation data\nproduced in this study are freely available for algorithm development and\nbenchmarking at: https://sites.google.com/view/nucls.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 01:17:17 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Amgad", "Mohamed", ""], ["Atteya", "Lamees A.", ""], ["Hussein", "Hagar", ""], ["Mohammed", "Kareem Hosny", ""], ["Hafiz", "Ehab", ""], ["Elsebaie", "Maha A. T.", ""], ["Alhusseiny", "Ahmed M.", ""], ["AlMoslemany", "Mohamed Atef", ""], ["Elmatboly", "Abdelmagid M.", ""], ["Pappalardo", "Philip A.", ""], ["Sakr", "Rokia Adel", ""], ["Mobadersany", "Pooya", ""], ["Rachid", "Ahmad", ""], ["Saad", "Anas M.", ""], ["Alkashash", "Ahmad M.", ""], ["Ruhban", "Inas A.", ""], ["Alrefai", "Anas", ""], ["Elgazar", "Nada M.", ""], ["Abdulkarim", "Ali", ""], ["Farag", "Abo-Alela", ""], ["Etman", "Amira", ""], ["Elsaeed", "Ahmed G.", ""], ["Alagha", "Yahya", ""], ["Amer", "Yomna A.", ""], ["Raslan", "Ahmed M.", ""], ["Nadim", "Menatalla K.", ""], ["Elsebaie", "Mai A. T.", ""], ["Ayad", "Ahmed", ""], ["Hanna", "Liza E.", ""], ["Gadallah", "Ahmed", ""], ["Elkady", "Mohamed", ""], ["Drumheller", "Bradley", ""], ["Jaye", "David", ""], ["Manthey", "David", ""], ["Gutman", "David A.", ""], ["Elfandy", "Habiba", ""], ["Cooper", "Lee A. D.", ""]]}, {"id": "2102.09105", "submitter": "Minghua Liu", "authors": "Minghua Liu, Minhyuk Sung, Radomir Mech, Hao Su", "title": "DeepMetaHandles: Learning Deformation Meta-Handles of 3D Meshes with\n  Biharmonic Coordinates", "comments": "CVPR2021 oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose DeepMetaHandles, a 3D conditional generative model based on mesh\ndeformation. Given a collection of 3D meshes of a category and their\ndeformation handles (control points), our method learns a set of meta-handles\nfor each shape, which are represented as combinations of the given handles. The\ndisentangled meta-handles factorize all the plausible deformations of the\nshape, while each of them corresponds to an intuitive deformation. A new\ndeformation can then be generated by sampling the coefficients of the\nmeta-handles in a specific range. We employ biharmonic coordinates as the\ndeformation function, which can smoothly propagate the control points'\ntranslations to the entire mesh. To avoid learning zero deformation as\nmeta-handles, we incorporate a target-fitting module which deforms the input\nmesh to match a random target. To enhance deformations' plausibility, we employ\na soft-rasterizer-based discriminator that projects the meshes to a 2D space.\nOur experiments demonstrate the superiority of the generated deformations as\nwell as the interpretability and consistency of the learned meta-handles.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 01:31:26 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 18:22:10 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Liu", "Minghua", ""], ["Sung", "Minhyuk", ""], ["Mech", "Radomir", ""], ["Su", "Hao", ""]]}, {"id": "2102.09109", "submitter": "Eva Cetinic", "authors": "Eva Cetinic and James She", "title": "Understanding and Creating Art with AI: Review and Outlook", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technologies related to artificial intelligence (AI) have a strong impact on\nthe changes of research and creative practices in visual arts. The growing\nnumber of research initiatives and creative applications that emerge in the\nintersection of AI and art, motivates us to examine and discuss the creative\nand explorative potentials of AI technologies in the context of art. This paper\nprovides an integrated review of two facets of AI and art: 1) AI is used for\nart analysis and employed on digitized artwork collections; 2) AI is used for\ncreative purposes and generating novel artworks. In the context of AI-related\nresearch for art understanding, we present a comprehensive overview of artwork\ndatasets and recent works that address a variety of tasks such as\nclassification, object detection, similarity retrieval, multimodal\nrepresentations, computational aesthetics, etc. In relation to the role of AI\nin creating art, we address various practical and theoretical aspects of AI Art\nand consolidate related works that deal with those topics in detail. Finally,\nwe provide a concise outlook on the future progression and potential impact of\nAI technologies on our understanding and creation of art.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 01:38:11 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Cetinic", "Eva", ""], ["She", "James", ""]]}, {"id": "2102.09117", "submitter": "Jiachen Li", "authors": "Jiachen Li and Hengbo Ma and Zhihao Zhang and Jinning Li and Masayoshi\n  Tomizuka", "title": "Spatio-Temporal Graph Dual-Attention Network for Multi-Agent Prediction\n  and Tracking", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An effective understanding of the environment and accurate trajectory\nprediction of surrounding dynamic obstacles are indispensable for intelligent\nmobile systems (e.g. autonomous vehicles and social robots) to achieve safe and\nhigh-quality planning when they navigate in highly interactive and crowded\nscenarios. Due to the existence of frequent interactions and uncertainty in the\nscene evolution, it is desired for the prediction system to enable relational\nreasoning on different entities and provide a distribution of future\ntrajectories for each agent. In this paper, we propose a generic generative\nneural system (called STG-DAT) for multi-agent trajectory prediction involving\nheterogeneous agents. The system takes a step forward to explicit interaction\nmodeling by incorporating relational inductive biases with a dynamic graph\nrepresentation and leverages both trajectory and scene context information. We\nalso employ an efficient kinematic constraint layer applied to vehicle\ntrajectory prediction. The constraint not only ensures physical feasibility but\nalso enhances model performance. Moreover, the proposed prediction model can be\neasily adopted by multi-target tracking frameworks. The tracking accuracy\nproves to be improved by empirical results. The proposed system is evaluated on\nthree public benchmark datasets for trajectory prediction, where the agents\ncover pedestrians, cyclists and on-road vehicles. The experimental results\ndemonstrate that our model achieves better performance than various baseline\napproaches in terms of prediction and tracking accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 02:25:35 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Li", "Jiachen", ""], ["Ma", "Hengbo", ""], ["Zhang", "Zhihao", ""], ["Li", "Jinning", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "2102.09119", "submitter": "Yidan Qin", "authors": "Yidan Qin, Max Allan, Yisong Yue, Joel W. Burdick, Mahdi Azizian", "title": "Learning Invariant Representation of Tasks for Robust Surgical State\n  Estimation", "comments": "Accepted to IEEE Robotics & Automation Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Surgical state estimators in robot-assisted surgery (RAS) - especially those\ntrained via learning techniques - rely heavily on datasets that capture surgeon\nactions in laboratory or real-world surgical tasks. Real-world RAS datasets are\ncostly to acquire, are obtained from multiple surgeons who may use different\nsurgical strategies, and are recorded under uncontrolled conditions in highly\ncomplex environments. The combination of high diversity and limited data calls\nfor new learning methods that are robust and invariant to operating conditions\nand surgical techniques. We propose StiseNet, a Surgical Task Invariance State\nEstimation Network with an invariance induction framework that minimizes the\neffects of variations in surgical technique and operating environments inherent\nto RAS datasets. StiseNet's adversarial architecture learns to separate\nnuisance factors from information needed for surgical state estimation.\nStiseNet is shown to outperform state-of-the-art state estimation methods on\nthree datasets (including a new real-world RAS dataset: HERNIA-20).\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 02:32:50 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Qin", "Yidan", ""], ["Allan", "Max", ""], ["Yue", "Yisong", ""], ["Burdick", "Joel W.", ""], ["Azizian", "Mahdi", ""]]}, {"id": "2102.09133", "submitter": "Chaowei Fang", "authors": "Chaowei Fang, Haibin Tian, Dingwen Zhang, Qiang Zhang, Jungong Han,\n  Junwei Han", "title": "Densely Nested Top-Down Flows for Salient Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the goal of identifying pixel-wise salient object regions from each\ninput image, salient object detection (SOD) has been receiving great attention\nin recent years. One kind of mainstream SOD methods is formed by a bottom-up\nfeature encoding procedure and a top-down information decoding procedure. While\nnumerous approaches have explored the bottom-up feature extraction for this\ntask, the design on top-down flows still remains under-studied. To this end,\nthis paper revisits the role of top-down modeling in salient object detection\nand designs a novel densely nested top-down flows (DNTDF)-based framework. In\nevery stage of DNTDF, features from higher levels are read in via the\nprogressive compression shortcut paths (PCSP). The notable characteristics of\nour proposed method are as follows. 1) The propagation of high-level features\nwhich usually have relatively strong semantic information is enhanced in the\ndecoding procedure; 2) With the help of PCSP, the gradient vanishing issues\ncaused by non-linear operations in top-down information flows can be\nalleviated; 3) Thanks to the full exploration of high-level features, the\ndecoding process of our method is relatively memory efficient compared against\nthose of existing methods. Integrating DNTDF with EfficientNet, we construct a\nhighly light-weighted SOD model, with very low computational complexity. To\ndemonstrate the effectiveness of the proposed model, comprehensive experiments\nare conducted on six widely-used benchmark datasets. The comparisons to the\nmost state-of-the-art methods as well as the carefully-designed baseline models\nverify our insights on the top-down flow modeling for SOD. The code of this\npaper is available at https://github.com/new-stone-object/DNTD.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 03:14:02 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Fang", "Chaowei", ""], ["Tian", "Haibin", ""], ["Zhang", "Dingwen", ""], ["Zhang", "Qiang", ""], ["Han", "Jungong", ""], ["Han", "Junwei", ""]]}, {"id": "2102.09137", "submitter": "Xinhan Di", "authors": "Xinhan Di, Pengqian Yu", "title": "Multi-Agent Reinforcement Learning of 3D Furniture Layout Simulation in\n  Indoor Graphics Scenes", "comments": "8 pages, 3 figures submit to conference. arXiv admin note:\n  substantial text overlap with arXiv:2101.07462", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the industrial interior design process, professional designers plan the\nfurniture layout to achieve a satisfactory 3D design for selling. In this\npaper, we explore the interior graphics scenes design task as a Markov decision\nprocess (MDP) in 3D simulation, which is solved by multi-agent reinforcement\nlearning. The goal is to produce furniture layout in the 3D simulation of the\nindoor graphics scenes. In particular, we firstly transform the 3D interior\ngraphic scenes into two 2D simulated scenes. We then design the simulated\nenvironment and apply two reinforcement learning agents to learn the optimal 3D\nlayout for the MDP formulation in a cooperative way. We conduct our experiments\non a large-scale real-world interior layout dataset that contains industrial\ndesigns from professional designers. Our numerical results demonstrate that the\nproposed model yields higher-quality layouts as compared with the state-of-art\nmodel. The developed simulator and codes are available at\n\\url{https://github.com/CODE-SUBMIT/simulator2}.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 03:20:35 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Di", "Xinhan", ""], ["Yu", "Pengqian", ""]]}, {"id": "2102.09142", "submitter": "Chen Ziwen", "authors": "Chen Ziwen, Zixuan Guo, Jerod Weinman", "title": "Improved Point Transformation Methods For Self-Supervised Depth\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given stereo or egomotion image pairs, a popular and successful method for\nunsupervised learning of monocular depth estimation is to measure the quality\nof image reconstructions resulting from the learned depth predictions.\nContinued research has improved the overall approach in recent years, yet the\ncommon framework still suffers from several important limitations, particularly\nwhen dealing with points occluded after transformation to a novel viewpoint.\nWhile prior work has addressed this problem heuristically, this paper\nintroduces a z-buffering algorithm that correctly and efficiently handles\noccluded points. Because our algorithm is implemented with operators typical of\nmachine learning libraries, it can be incorporated into any existing\nunsupervised depth learning framework with automatic support for\ndifferentiation. Additionally, because points having negative depth after\ntransformation often signify erroneously shallow depth predictions, we\nintroduce a loss function to penalize this undesirable behavior explicitly.\nExperimental results on the KITTI data set show that the z-buffer and negative\ndepth loss both improve the performance of a state of the art depth-prediction\nnetwork.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 03:42:40 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Ziwen", "Chen", ""], ["Guo", "Zixuan", ""], ["Weinman", "Jerod", ""]]}, {"id": "2102.09150", "submitter": "Decky Aspandi", "authors": "Decky Aspandi, Federico Sukno, Bj\\\"orn Schuller and Xavier Binefa", "title": "An Enhanced Adversarial Network with Combined Latent Features for\n  Spatio-Temporal Facial Affect Estimation in the Wild", "comments": "Accepted Version on VISAPP 2021", "journal-ref": null, "doi": "10.5220/0010332001720181", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affective Computing has recently attracted the attention of the research\ncommunity, due to its numerous applications in diverse areas. In this context,\nthe emergence of video-based data allows to enrich the widely used spatial\nfeatures with the inclusion of temporal information. However, such\nspatio-temporal modelling often results in very high-dimensional feature spaces\nand large volumes of data, making training difficult and time consuming. This\npaper addresses these shortcomings by proposing a novel model that efficiently\nextracts both spatial and temporal features of the data by means of its\nenhanced temporal modelling based on latent features. Our proposed model\nconsists of three major networks, coined Generator, Discriminator, and\nCombiner, which are trained in an adversarial setting combined with curriculum\nlearning to enable our adaptive attention modules. In our experiments, we show\nthe effectiveness of our approach by reporting our competitive results on both\nthe AFEW-VA and SEWA datasets, suggesting that temporal modelling improves the\naffect estimates both in qualitative and quantitative terms. Furthermore, we\nfind that the inclusion of attention mechanisms leads to the highest accuracy\nimprovements, as its weights seem to correlate well with the appearance of\nfacial movements, both in terms of temporal localisation and intensity.\nFinally, we observe the sequence length of around 160\\,ms to be the optimum one\nfor temporal modelling, which is consistent with other relevant findings\nutilising similar lengths.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 04:10:12 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Aspandi", "Decky", ""], ["Sukno", "Federico", ""], ["Schuller", "Bj\u00f6rn", ""], ["Binefa", "Xavier", ""]]}, {"id": "2102.09186", "submitter": "Liqi Yan", "authors": "Liqi Yan, Yiming Cui, Yingjie Chen, Dongfang Liu", "title": "Hierarchical Attention Fusion for Geo-Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Geo-localization is a critical task in computer vision. In this work, we cast\nthe geo-localization as a 2D image retrieval task. Current state-of-the-art\nmethods for 2D geo-localization are not robust to locate a scene with drastic\nscale variations because they only exploit features from one semantic level for\nimage representations. To address this limitation, we introduce a hierarchical\nattention fusion network using multi-scale features for geo-localization. We\nextract the hierarchical feature maps from a convolutional neural network (CNN)\nand organically fuse the extracted features for image representations. Our\ntraining is self-supervised using adaptive weights to control the attention of\nfeature emphasis from each hierarchical level. Evaluation results on the image\nretrieval and the large-scale geo-localization benchmarks indicate that our\nmethod outperforms the existing state-of-the-art methods. Code is available\nhere: \\url{https://github.com/YanLiqi/HAF}.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 07:07:03 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Yan", "Liqi", ""], ["Cui", "Yiming", ""], ["Chen", "Yingjie", ""], ["Liu", "Dongfang", ""]]}, {"id": "2102.09199", "submitter": "Ellak Somfai", "authors": "Ell\\'ak Somfai, Benj\\'amin Baffy, Kristian Fenech, Changlu Guo, Rita\n  Hossz\\'u, Dorina Kor\\'ozs, Fabrizio Nunnari, Marcell P\\'olik, Daniel Sonntag,\n  Attila Ulbert, Andr\\'as L\\H{o}rincz", "title": "Minimizing false negative rate in melanoma detection and providing\n  insight into the causes of classification", "comments": "supplementary materials included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to bridge human and machine intelligence in melanoma detection.\nWe develop a classification system exploiting a combination of visual\npre-processing, deep learning, and ensembling for providing explanations to\nexperts and to minimize false negative rate while maintaining high accuracy in\nmelanoma detection. Source images are first automatically segmented using a\nU-net CNN. The result of the segmentation is then used to extract image\nsub-areas and specific parameters relevant in human evaluation, namely center,\nborder, and asymmetry measures. These data are then processed by tailored\nneural networks which include structure searching algorithms. Partial results\nare then ensembled by a committee machine. Our evaluation on the largest skin\nlesion dataset which is publicly available today, ISIC-2019, shows improvement\nin all evaluated metrics over a baseline using the original images only. We\nalso showed that indicative scores computed by the feature classifiers can\nprovide useful insight into the various features on which the decision can be\nbased.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 07:46:34 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 00:16:28 GMT"}, {"version": "v3", "created": "Tue, 9 Mar 2021 08:16:42 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Somfai", "Ell\u00e1k", ""], ["Baffy", "Benj\u00e1min", ""], ["Fenech", "Kristian", ""], ["Guo", "Changlu", ""], ["Hossz\u00fa", "Rita", ""], ["Kor\u00f3zs", "Dorina", ""], ["Nunnari", "Fabrizio", ""], ["P\u00f3lik", "Marcell", ""], ["Sonntag", "Daniel", ""], ["Ulbert", "Attila", ""], ["L\u0151rincz", "Andr\u00e1s", ""]]}, {"id": "2102.09242", "submitter": "Saikat Dutta", "authors": "Sourya Dipta Das, Nisarg A. Shah, Saikat Dutta, Himanshu Kumar", "title": "DSRN: an Efficient Deep Network for Image Relighting", "comments": "Accepted at ICIP 2021. $\\copyright$ IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Custom and natural lighting conditions can be emulated in images of the scene\nduring post-editing. Extraordinary capabilities of the deep learning framework\ncan be utilized for such purpose. Deep image relighting allows automatic photo\nenhancement by illumination-specific retouching. Most of the state-of-the-art\nmethods for relighting are run-time intensive and memory inefficient. In this\npaper, we propose an efficient, real-time framework Deep Stacked Relighting\nNetwork (DSRN) for image relighting by utilizing the aggregated features from\ninput image at different scales. Our model is very lightweight with total size\nof about 42 MB and has an average inference time of about 0.0116s for image of\nresolution $1024 \\times 1024$ which is faster as compared to other multi-scale\nmodels. Our solution is quite robust for translating image color temperature\nfrom input image to target image and also performs moderately for light\ngradient generation with respect to the target image. Additionally, we show\nthat if images illuminated from opposite directions are used as input, the\nqualitative results improve over using a single input image.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 09:54:15 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 13:50:16 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Das", "Sourya Dipta", ""], ["Shah", "Nisarg A.", ""], ["Dutta", "Saikat", ""], ["Kumar", "Himanshu", ""]]}, {"id": "2102.09244", "submitter": "Jun Lv", "authors": "Jun Lv, Wenqiang Xu, Lixin Yang, Sucheng Qian, Chongzhao Mao, Cewu Lu", "title": "HandTailor: Towards High-Precision Monocular 3D Hand Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D hand pose estimation and shape recovery are challenging tasks in computer\nvision. We introduce a novel framework HandTailor, which combines a\nlearning-based hand module and an optimization-based tailor module to achieve\nhigh-precision hand mesh recovery from a monocular RGB image. The proposed hand\nmodule unifies perspective projection and weak perspective projection in a\nsingle network towards accuracy-oriented and in-the-wild scenarios. The\nproposed tailor module then utilizes the coarsely reconstructed mesh model\nprovided by the hand module as initialization, and iteratively optimizes an\nenergy function to obtain better results. The tailor module is time-efficient,\ncosts only 8ms per frame on a modern CPU. We demonstrate that HandTailor can\nget state-of-the-art performance on several public benchmarks, with impressive\nqualitative results on in-the-wild experiments.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 09:55:38 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Lv", "Jun", ""], ["Xu", "Wenqiang", ""], ["Yang", "Lixin", ""], ["Qian", "Sucheng", ""], ["Mao", "Chongzhao", ""], ["Lu", "Cewu", ""]]}, {"id": "2102.09258", "submitter": "Marta Gomez-Barrero", "authors": "Marta Gomez-Barrero, Pawel Drozdowski, Christian Rathgeb, Jose Patino,\n  Massimmiliano Todisco, Andras Nautsch, Naser Damer, Jannis Priesnitz,\n  Nicholas Evans, Christoph Busch", "title": "Biometrics in the Era of COVID-19: Challenges and Opportunities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Since early 2020 the COVID-19 pandemic has had a considerable impact on many\naspects of daily life. A range of different measures have been implemented\nworldwide to reduce the rate of new infections and to manage the pressure on\nnational health services. A primary strategy has been to reduce gatherings and\nthe potential for transmission through the prioritisation of remote working and\neducation. Enhanced hand hygiene and the use of facial masks have decreased the\nspread of pathogens when gatherings are unavoidable. These particular measures\npresent challenges for reliable biometric recognition, e.g. for facial-, voice-\nand hand-based biometrics. At the same time, new challenges create new\nopportunities and research directions, e.g. renewed interest in non-constrained\niris or periocular recognition, touch-less fingerprint- and vein-based\nauthentication and the use of biometric characteristics for disease detection.\nThis article presents an overview of the research carried out to address those\nchallenges and emerging opportunities.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 10:32:59 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Gomez-Barrero", "Marta", ""], ["Drozdowski", "Pawel", ""], ["Rathgeb", "Christian", ""], ["Patino", "Jose", ""], ["Todisco", "Massimmiliano", ""], ["Nautsch", "Andras", ""], ["Damer", "Naser", ""], ["Priesnitz", "Jannis", ""], ["Evans", "Nicholas", ""], ["Busch", "Christoph", ""]]}, {"id": "2102.09262", "submitter": "Raul Murillo", "authors": "Raul Murillo, Alberto A. Del Barrio, Guillermo Botella, Min Soo Kim,\n  HyunJin Kim and Nader Bagherzadeh", "title": "PLAM: a Posit Logarithm-Approximate Multiplier for Power Efficient\n  Posit-based DNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Posit Number System was introduced in 2017 as a replacement for\nfloating-point numbers. Since then, the community has explored its application\nin Neural Network related tasks and produced some unit designs which are still\nfar from being competitive with their floating-point counterparts. This paper\nproposes a Posit Logarithm-Approximate Multiplication (PLAM) scheme to\nsignificantly reduce the complexity of posit multipliers, the most power-hungry\nunits within Deep Neural Network architectures. When comparing with\nstate-of-the-art posit multipliers, experiments show that the proposed\ntechnique reduces the area, power, and delay of hardware multipliers up to\n72.86%, 81.79%, and 17.01%, respectively, without accuracy degradation.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 10:43:07 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Murillo", "Raul", ""], ["Del Barrio", "Alberto A.", ""], ["Botella", "Guillermo", ""], ["Kim", "Min Soo", ""], ["Kim", "HyunJin", ""], ["Bagherzadeh", "Nader", ""]]}, {"id": "2102.09281", "submitter": "Konstantinos Vougioukas", "authors": "Konstantinos Vougioukas, Stavros Petridis and Maja Pantic", "title": "DINO: A Conditional Energy-Based GAN for Domain Translation", "comments": "Accepted to ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain translation is the process of transforming data from one domain to\nanother while preserving the common semantics. Some of the most popular domain\ntranslation systems are based on conditional generative adversarial networks,\nwhich use source domain data to drive the generator and as an input to the\ndiscriminator. However, this approach does not enforce the preservation of\nshared semantics since the conditional input can often be ignored by the\ndiscriminator. We propose an alternative method for conditioning and present a\nnew framework, where two networks are simultaneously trained, in a supervised\nmanner, to perform domain translation in opposite directions. Our method is not\nonly better at capturing the shared information between two domains but is more\ngeneric and can be applied to a broader range of problems. The proposed\nframework performs well even in challenging cross-modal translations, such as\nvideo-driven speech reconstruction, for which other systems struggle to\nmaintain correspondence.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 11:52:45 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Vougioukas", "Konstantinos", ""], ["Petridis", "Stavros", ""], ["Pantic", "Maja", ""]]}, {"id": "2102.09297", "submitter": "Hana Alghamdi", "authors": "Hana Alghamdi and Rozenn Dahyot", "title": "Sliced $\\mathcal{L}_2$ Distance for Colour Grading", "comments": "5 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method with $\\mathcal{L}_2$ distance that maps one\n$N$-dimensional distribution to another, taking into account available\ninformation about correspondences. We solve the high-dimensional problem in 1D\nspace using an iterative projection approach. To show the potentials of this\nmapping, we apply it to colour transfer between two images that exhibit\noverlapped scenes. Experiments show quantitative and qualitative competitive\nresults as compared with the state of the art colour transfer methods.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 12:17:18 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Alghamdi", "Hana", ""], ["Dahyot", "Rozenn", ""]]}, {"id": "2102.09298", "submitter": "Eran Treister", "authors": "Benjamin J. Bodner and Gil Ben Shalom and Eran Treister", "title": "GradFreeBits: Gradient Free Bit Allocation for Dynamic Low Precision\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantized neural networks (QNNs) are among the main approaches for deploying\ndeep neural networks on low resource edge devices. Training QNNs using\ndifferent levels of precision throughout the network (dynamic quantization)\ntypically achieves superior trade-offs between performance and computational\nload. However, optimizing the different precision levels of QNNs can be\ncomplicated, as the values of the bit allocations are discrete and difficult to\ndifferentiate for. Also, adequately accounting for the dependencies between the\nbit allocation of different layers is not straight-forward. To meet these\nchallenges, in this work we propose GradFreeBits: a novel joint optimization\nscheme for training dynamic QNNs, which alternates between gradient-based\noptimization for the weights, and gradient-free optimization for the bit\nallocation. Our method achieves better or on par performance with current state\nof the art low precision neural networks on CIFAR10/100 and ImageNet\nclassification. Furthermore, our approach can be extended to a variety of other\napplications involving neural networks used in conjunction with parameters\nwhich are difficult to optimize for.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 12:18:09 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Bodner", "Benjamin J.", ""], ["Shalom", "Gil Ben", ""], ["Treister", "Eran", ""]]}, {"id": "2102.09310", "submitter": "Alexander Shekhovtsov", "authors": "Dmitrij Schlesinger, Alexander Shekhovtsov, Boris Flach", "title": "VAE Approximation Error: ELBO and Conditional Independence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The importance of Variational Autoencoders reaches far beyond standalone\ngenerative models -- the approach is also used for learning latent\nrepresentations and can be generalized to semi-supervised learning. This\nrequires a thorough analysis of their commonly known shortcomings: posterior\ncollapse and approximation errors. This paper analyzes VAE approximation errors\ncaused by the combination of the ELBO objective with the choice of the encoder\nprobability family, in particular under conditional independence assumptions.\nWe identify the subclass of generative models consistent with the encoder\nfamily. We show that the ELBO optimizer is pulled from the likelihood optimizer\ntowards this consistent subset. Furthermore, this subset can not be enlarged,\nand the respective error cannot be decreased, by only considering deeper\nencoder networks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 12:54:42 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Schlesinger", "Dmitrij", ""], ["Shekhovtsov", "Alexander", ""], ["Flach", "Boris", ""]]}, {"id": "2102.09320", "submitter": "Daniel Gehrig", "authors": "Daniel Gehrig, Michelle R\\\"uegg, Mathias Gehrig, Javier Hidalgo\n  Carrio, Davide Scaramuzza", "title": "Combining Events and Frames using Recurrent Asynchronous Multimodal\n  Networks for Monocular Depth Prediction", "comments": null, "journal-ref": "IEEE Robotics and Automation Letters (RA-L), 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Event cameras are novel vision sensors that report per-pixel brightness\nchanges as a stream of asynchronous \"events\". They offer significant advantages\ncompared to standard cameras due to their high temporal resolution, high\ndynamic range and lack of motion blur. However, events only measure the varying\ncomponent of the visual signal, which limits their ability to encode scene\ncontext. By contrast, standard cameras measure absolute intensity frames, which\ncapture a much richer representation of the scene. Both sensors are thus\ncomplementary. However, due to the asynchronous nature of events, combining\nthem with synchronous images remains challenging, especially for learning-based\nmethods. This is because traditional recurrent neural networks (RNNs) are not\ndesigned for asynchronous and irregular data from additional sensors. To\naddress this challenge, we introduce Recurrent Asynchronous Multimodal (RAM)\nnetworks, which generalize traditional RNNs to handle asynchronous and\nirregular data from multiple sensors. Inspired by traditional RNNs, RAM\nnetworks maintain a hidden state that is updated asynchronously and can be\nqueried at any time to generate a prediction. We apply this novel architecture\nto monocular depth estimation with events and frames where we show an\nimprovement over state-of-the-art methods by up to 30% in terms of mean\nabsolute depth error. To enable further research on multimodal learning with\nevents, we release EventScape, a new dataset with events, intensity frames,\nsemantic labels, and depth maps recorded in the CARLA simulator.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 13:24:35 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Gehrig", "Daniel", ""], ["R\u00fcegg", "Michelle", ""], ["Gehrig", "Mathias", ""], ["Carrio", "Javier Hidalgo", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "2102.09321", "submitter": "Abdallah Benzine", "authors": "Abdallah Benzine, Mohamed El Amine Seddik, Julien Desmarais", "title": "Deep Miner: A Deep and Multi-branch Network which Mines Rich and Diverse\n  Features for Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Most recent person re-identification approaches are based on the use of deep\nconvolutional neural networks (CNNs). These networks, although effective in\nmultiple tasks such as classification or object detection, tend to focus on the\nmost discriminative part of an object rather than retrieving all its relevant\nfeatures. This behavior penalizes the performance of a CNN for the\nre-identification task, since it should identify diverse and fine grained\nfeatures. It is then essential to make the network learn a wide variety of\nfiner characteristics in order to make the re-identification process of people\neffective and robust to finer changes. In this article, we introduce Deep\nMiner, a method that allows CNNs to \"mine\" richer and more diverse features\nabout people for their re-identification. Deep Miner is specifically composed\nof three types of branches: a Global branch (G-branch), a Local branch\n(L-branch) and an Input-Erased branch (IE-branch). G-branch corresponds to the\ninitial backbone which predicts global characteristics, while L-branch\nretrieves part level resolution features. The IE-branch for its part, receives\npartially suppressed feature maps as input thereby allowing the network to\n\"mine\" new features (those ignored by G-branch) as output. For this special\npurpose, a dedicated suppression procedure for identifying and removing\nfeatures within a given CNN is introduced. This suppression procedure has the\nmajor benefit of being simple, while it produces a model that significantly\noutperforms state-of-the-art (SOTA) re-identification methods. Specifically, we\nconduct experiments on four standard person re-identification benchmarks and\nwitness an absolute performance gain up to 6.5% mAP compared to SOTA.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 13:30:23 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Benzine", "Abdallah", ""], ["Seddik", "Mohamed El Amine", ""], ["Desmarais", "Julien", ""]]}, {"id": "2102.09332", "submitter": "Zuohui Chen", "authors": "Zuohui Chen, Tony Zhang, Zhuangzhi Chen, Yun Xiang, Qi Xuan, and\n  Robert P. Dick", "title": "HVAQ: A High-Resolution Vision-Based Air Quality Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Air pollutants, such as particulate matter, strongly impact human health.\nMost existing pollution monitoring techniques use stationary sensors, which are\ntypically sparsely deployed. However, real-world pollution distributions vary\nrapidly in space and the visual effects of air pollutant can be used to\nestimate concentration, potentially at high spatial resolution. Accurate\npollution monitoring requires either densely deployed conventional point\nsensors, at-a-distance vision-based pollution monitoring, or a combination of\nboth.\n  This paper makes the following contributions: (1) we present a high temporal\nand spatial resolution air quality dataset consisting of PM2.5, PM10,\ntemperature, and humidity data; (2) we simultaneously take images covering the\nlocations of the particle counters; and (3) we evaluate several vision-based\nstate-of-art PM concentration prediction algorithms on our dataset and\ndemonstrate that prediction accuracy increases with sensor density and image.\nIt is our intent and belief that this dataset can enable advances by other\nresearch teams working on air quality estimation.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 13:42:34 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Chen", "Zuohui", ""], ["Zhang", "Tony", ""], ["Chen", "Zhuangzhi", ""], ["Xiang", "Yun", ""], ["Xuan", "Qi", ""], ["Dick", "Robert P.", ""]]}, {"id": "2102.09334", "submitter": "Yifei Shi", "authors": "Yifei Shi, Junwen Huang, Xin Xu, Yifan Zhang, Kai Xu", "title": "StablePose: Learning 6D Object Poses from Geometrically Stable Patches", "comments": "CVPR 2021, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the concept of geometric stability to the problem of 6D object\npose estimation and propose to learn pose inference based on geometrically\nstable patches extracted from observed 3D point clouds. According to the theory\nof geometric stability analysis, a minimal set of three planar/cylindrical\npatches are geometrically stable and determine the full 6DoFs of the object\npose. We train a deep neural network to regress 6D object pose based on\ngeometrically stable patch groups via learning both intra-patch geometric\nfeatures and inter-patch contextual features. A subnetwork is jointly trained\nto predict per-patch poses. This auxiliary task is a relaxation of the group\npose prediction: A single patch cannot determine the full 6DoFs but is able to\nimprove pose accuracy in its corresponding DoFs. Working with patch groups\nmakes our method generalize well for random occlusion and unseen instances. The\nmethod is easily amenable to resolve symmetry ambiguities. Our method achieves\nthe state-of-the-art results on public benchmarks compared not only to\ndepth-only but also to RGBD methods. It also performs well in category-level\npose estimation.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 13:46:22 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 07:29:58 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Shi", "Yifei", ""], ["Huang", "Junwen", ""], ["Xu", "Xin", ""], ["Zhang", "Yifan", ""], ["Xu", "Kai", ""]]}, {"id": "2102.09351", "submitter": "Syed Muhammad Arsalan Bashir Mr.", "authors": "Syed Muhammad Arsalan Bashir, Yi Wang, Mahrukh Khan, Yilong Niu", "title": "A Comprehensive Review of Deep Learning-based Single Image\n  Super-resolution", "comments": "56 Pages, 11 Figures, 5 Tables", "journal-ref": "PeerJ Computer Science 7:e621, 2021", "doi": "10.7717/peerj-cs.621", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image super-resolution (SR) is one of the vital image processing methods that\nimprove the resolution of an image in the field of computer vision. In the last\ntwo decades, significant progress has been made in the field of\nsuper-resolution, especially by utilizing deep learning methods. This survey is\nan effort to provide a detailed survey of recent progress in single-image\nsuper-resolution in the perspective of deep learning while also informing about\nthe initial classical methods used for image super-resolution. The survey\nclassifies the image SR methods into four categories, i.e., classical methods,\nsupervised learning-based methods, unsupervised learning-based methods, and\ndomain-specific SR methods. We also introduce the problem of SR to provide\nintuition about image quality metrics, available reference datasets, and SR\nchallenges. Deep learning-based approaches of SR are evaluated using a\nreference dataset. Some of the reviewed state-of-the-art image SR methods\ninclude the enhanced deep SR network (EDSR), cycle-in-cycle GAN (CinCGAN),\nmultiscale residual network (MSRN), meta residual dense network (Meta-RDN),\nrecurrent back-projection network (RBPN), second-order attention network (SAN),\nSR feedback network (SRFBN) and the wavelet-based residual attention network\n(WRAN). Finally, this survey is concluded with future directions and trends in\nSR and open problems in SR to be addressed by the researchers.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 14:04:25 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 09:12:59 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Bashir", "Syed Muhammad Arsalan", ""], ["Wang", "Yi", ""], ["Khan", "Mahrukh", ""], ["Niu", "Yilong", ""]]}, {"id": "2102.09375", "submitter": "Zhe Ma", "authors": "Zhe Ma, Fenghao Liu, Jianfeng Dong, Xiaoye Qu, Yuan He, Shouling Ji", "title": "Hierarchical Similarity Learning for Language-based Product Image\n  Retrieval", "comments": "Accepted by ICASSP 2021. Code and data will be available at\n  https://github.com/liufh1/hsl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims for the language-based product image retrieval task. The\nmajority of previous works have made significant progress by designing network\nstructure, similarity measurement, and loss function. However, they typically\nperform vision-text matching at certain granularity regardless of the intrinsic\nmultiple granularities of images. In this paper, we focus on the cross-modal\nsimilarity measurement, and propose a novel Hierarchical Similarity Learning\n(HSL) network. HSL first learns multi-level representations of input data by\nstacked encoders, and object-granularity similarity and image-granularity\nsimilarity are computed at each level. All the similarities are combined as the\nfinal hierarchical cross-modal similarity. Experiments on a large-scale product\nretrieval dataset demonstrate the effectiveness of our proposed method. Code\nand data are available at https://github.com/liufh1/hsl.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 14:23:16 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Ma", "Zhe", ""], ["Liu", "Fenghao", ""], ["Dong", "Jianfeng", ""], ["Qu", "Xiaoye", ""], ["He", "Yuan", ""], ["Ji", "Shouling", ""]]}, {"id": "2102.09376", "submitter": "Xiaoping Xie", "authors": "Maoyuan Xu and Xiaoping Xie", "title": "NFCNN: Toward a Noise Fusion Convolutional Neural Network for Image\n  Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based methods have achieved the state-of-the-art performance in\nimage denoising. In this paper, a deep learning based denoising method is\nproposed and a module called fusion block is introduced in the convolutional\nneural network. For this so-called Noise Fusion Convolutional Neural Network\n(NFCNN), there are two branches in its multi-stage architecture. One branch\naims to predict the latent clean image, while the other one predicts the\nresidual image. A fusion block is contained between every two stages by taking\nthe predicted clean image and the predicted residual image as a part of inputs,\nand it outputs a fused result to the next stage. NFCNN has an attractive\ntexture preserving ability because of the fusion block. To train NFCNN, a\nstage-wise supervised training strategy is adopted to avoid the vanishing\ngradient and exploding gradient problems. Experimental results show that NFCNN\nis able to perform competitive denoising results when compared with some\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 07:06:46 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Xu", "Maoyuan", ""], ["Xie", "Xiaoping", ""]]}, {"id": "2102.09386", "submitter": "Jonas Denck", "authors": "Jonas Denck, Jens Guehring, Andreas Maier, Eva Rothgang", "title": "Enhanced Magnetic Resonance Image Synthesis with Contrast-Aware\n  Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A Magnetic Resonance Imaging (MRI) exam typically consists of the acquisition\nof multiple MR pulse sequences, which are required for a reliable diagnosis.\nEach sequence can be parameterized through multiple acquisition parameters\naffecting MR image contrast, signal-to-noise ratio, resolution, or scan time.\nWith the rise of generative deep learning models, approaches for the synthesis\nof MR images are developed to either synthesize additional MR contrasts,\ngenerate synthetic data, or augment existing data for AI training. However,\ncurrent generative approaches for the synthesis of MR images are only trained\non images with a specific set of acquisition parameter values, limiting the\nclinical value of these methods as various sets of acquisition parameter\nsettings are used in clinical practice. Therefore, we trained a generative\nadversarial network (GAN) to generate synthetic MR knee images conditioned on\nvarious acquisition parameters (repetition time, echo time, image orientation).\nThis approach enables us to synthesize MR images with adjustable image\ncontrast. In a visual Turing test, two experts mislabeled 40.5% of real and\nsynthetic MR images, demonstrating that the image quality of the generated\nsynthetic and real MR images is comparable. This work can support radiologists\nand technologists during the parameterization of MR sequences by previewing the\nyielded MR contrast, can serve as a valuable tool for radiology training, and\ncan be used for customized data generation to support AI training.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 11:39:36 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 11:52:50 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Denck", "Jonas", ""], ["Guehring", "Jens", ""], ["Maier", "Andreas", ""], ["Rothgang", "Eva", ""]]}, {"id": "2102.09395", "submitter": "Nikolaos Livathinos", "authors": "Nikolaos Livathinos (1), Cesar Berrospi (1), Maksym Lysak (1), Viktor\n  Kuropiatnyk (1), Ahmed Nassar (1), Andre Carvalho (1), Michele Dolfi (1),\n  Christoph Auer (1), Kasper Dinkla (1), Peter Staar (1) ((1) IBM Research)", "title": "Robust PDF Document Conversion Using Recurrent Neural Networks", "comments": "9 pages, 2 tables, 4 figures, uses aaai21.sty. Accepted at the\n  \"Thirty-Third Annual Conference on Innovative Applications of Artificial\n  Intelligence (IAAI-21)\". Received the \"IAAI-21 Innovative Application Award\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of published PDF documents has increased exponentially in recent\ndecades. There is a growing need to make their rich content discoverable to\ninformation retrieval tools. In this paper, we present a novel approach to\ndocument structure recovery in PDF using recurrent neural networks to process\nthe low-level PDF data representation directly, instead of relying on a visual\nre-interpretation of the rendered PDF page, as has been proposed in previous\nliterature. We demonstrate how a sequence of PDF printing commands can be used\nas input into a neural network and how the network can learn to classify each\nprinting command according to its structural function in the page. This\napproach has three advantages: First, it can distinguish among more\nfine-grained labels (typically 10-20 labels as opposed to 1-5 with visual\nmethods), which results in a more accurate and detailed document structure\nresolution. Second, it can take into account the text flow across pages more\nnaturally compared to visual methods because it can concatenate the printing\ncommands of sequential pages. Last, our proposed method needs less memory and\nit is computationally less expensive than visual methods. This allows us to\ndeploy such models in production environments at a much lower cost. Through\nextensive architectural search in combination with advanced feature\nengineering, we were able to implement a model that yields a weighted average\nF1 score of 97% across 17 distinct structural labels. The best model we\nachieved is currently served in production environments on our Corpus\nConversion Service (CCS), which was presented at KDD18 (arXiv:1806.02284). This\nmodel enhances the capabilities of CCS significantly, as it eliminates the need\nfor human annotated label ground-truth for every unseen document layout. This\nproved particularly useful when applied to a huge corpus of PDF articles\nrelated to COVID-19.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 14:39:54 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Livathinos", "Nikolaos", "", "IBM Research"], ["Berrospi", "Cesar", "", "IBM Research"], ["Lysak", "Maksym", "", "IBM Research"], ["Kuropiatnyk", "Viktor", "", "IBM Research"], ["Nassar", "Ahmed", "", "IBM Research"], ["Carvalho", "Andre", "", "IBM Research"], ["Dolfi", "Michele", "", "IBM Research"], ["Auer", "Christoph", "", "IBM Research"], ["Dinkla", "Kasper", "", "IBM Research"], ["Staar", "Peter", "", "IBM Research"]]}, {"id": "2102.09462", "submitter": "Axel Elaldi", "authors": "Axel Elaldi, Neel Dey, Heejong Kim, Guido Gerig", "title": "Equivariant Spherical Deconvolution: Learning Sparse Orientation\n  Distribution Functions from Spherical Data", "comments": "Accepted to Information Processing in Medical Imaging (IPMI) 2021.\n  Code available at\n  https://github.com/AxelElaldi/equivariant-spherical-deconvolution . First two\n  authors contributed equally. 13 pages with 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a rotation-equivariant unsupervised learning framework for the\nsparse deconvolution of non-negative scalar fields defined on the unit sphere.\nSpherical signals with multiple peaks naturally arise in Diffusion MRI (dMRI),\nwhere each voxel consists of one or more signal sources corresponding to\nanisotropic tissue structure such as white matter. Due to spatial and spectral\npartial voluming, clinically-feasible dMRI struggles to resolve crossing-fiber\nwhite matter configurations, leading to extensive development in spherical\ndeconvolution methodology to recover underlying fiber directions. However,\nthese methods are typically linear and struggle with small crossing-angles and\npartial volume fraction estimation. In this work, we improve on current\nmethodologies by nonlinearly estimating fiber structures via unsupervised\nspherical convolutional networks with guaranteed equivariance to spherical\nrotation. Experimentally, we first validate our proposition via extensive\nsingle and multi-shell synthetic benchmarks demonstrating competitive\nperformance against common baselines. We then show improved downstream\nperformance on fiber tractography measures on the Tractometer benchmark\ndataset. Finally, we show downstream improvements in terms of tractography and\npartial volume estimation on a multi-shell dataset of human subjects.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 16:04:35 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Elaldi", "Axel", ""], ["Dey", "Neel", ""], ["Kim", "Heejong", ""], ["Gerig", "Guido", ""]]}, {"id": "2102.09471", "submitter": "Liming Jiang", "authors": "Liming Jiang, Zhengkui Guo, Wayne Wu, Zhaoyang Liu, Ziwei Liu, Chen\n  Change Loy, Shuo Yang, Yuanjun Xiong, Wei Xia, Baoying Chen, Peiyu Zhuang,\n  Sili Li, Shen Chen, Taiping Yao, Shouhong Ding, Jilin Li, Feiyue Huang,\n  Liujuan Cao, Rongrong Ji, Changlei Lu, Ganchao Tan", "title": "DeeperForensics Challenge 2020 on Real-World Face Forgery Detection:\n  Methods and Results", "comments": "Technical report. Challenge website:\n  https://competitions.codalab.org/competitions/25228", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports methods and results in the DeeperForensics Challenge 2020\non real-world face forgery detection. The challenge employs the\nDeeperForensics-1.0 dataset, one of the most extensive publicly available\nreal-world face forgery detection datasets, with 60,000 videos constituted by a\ntotal of 17.6 million frames. The model evaluation is conducted online on a\nhigh-quality hidden test set with multiple sources and diverse distortions. A\ntotal of 115 participants registered for the competition, and 25 teams made\nvalid submissions. We will summarize the winning solutions and present some\ndiscussions on potential research directions.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 16:48:57 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Jiang", "Liming", ""], ["Guo", "Zhengkui", ""], ["Wu", "Wayne", ""], ["Liu", "Zhaoyang", ""], ["Liu", "Ziwei", ""], ["Loy", "Chen Change", ""], ["Yang", "Shuo", ""], ["Xiong", "Yuanjun", ""], ["Xia", "Wei", ""], ["Chen", "Baoying", ""], ["Zhuang", "Peiyu", ""], ["Li", "Sili", ""], ["Chen", "Shen", ""], ["Yao", "Taiping", ""], ["Ding", "Shouhong", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""], ["Cao", "Liujuan", ""], ["Ji", "Rongrong", ""], ["Lu", "Changlei", ""], ["Tan", "Ganchao", ""]]}, {"id": "2102.09475", "submitter": "Joseph Paul Cohen", "authors": "Joseph Paul Cohen, Rupert Brooks, Sovann En, Evan Zucker, Anuj Pareek,\n  Matthew P. Lungren, Akshay Chaudhari", "title": "Gifsplanation via Latent Shift: A Simple Autoencoder Approach to\n  Counterfactual Generation for Chest X-rays", "comments": "Full paper at MIDL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivation: Traditional image attribution methods struggle to satisfactorily\nexplain predictions of neural networks. Prediction explanation is important,\nespecially in medical imaging, for avoiding the unintended consequences of\ndeploying AI systems when false positive predictions can impact patient care.\nThus, there is a pressing need to develop improved models for model\nexplainability and introspection. Specific problem: A new approach is to\ntransform input images to increase or decrease features which cause the\nprediction. However, current approaches are difficult to implement as they are\nmonolithic or rely on GANs. These hurdles prevent wide adoption. Our approach:\nGiven an arbitrary classifier, we propose a simple autoencoder and gradient\nupdate (Latent Shift) that can transform the latent representation of a\nspecific input image to exaggerate or curtail the features used for prediction.\nWe use this method to study chest X-ray classifiers and evaluate their\nperformance. We conduct a reader study with two radiologists assessing 240\nchest X-ray predictions to identify which ones are false positives (half are)\nusing traditional attribution maps or our proposed method. Results: We found\nlow overlap with ground truth pathology masks for models with reasonably high\naccuracy. However, the results from our reader study indicate that these models\nare generally looking at the correct features. We also found that the Latent\nShift explanation allows a user to have more confidence in true positive\npredictions compared to traditional approaches (0.15$\\pm$0.95 in a 5 point\nscale with p=0.01) with only a small increase in false positive predictions\n(0.04$\\pm$1.06 with p=0.57).\n  Accompanying webpage: https://mlmed.org/gifsplanation\n  Source code: https://github.com/mlmed/gifsplanation\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 16:55:03 GMT"}, {"version": "v2", "created": "Sat, 24 Apr 2021 21:07:01 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Cohen", "Joseph Paul", ""], ["Brooks", "Rupert", ""], ["En", "Sovann", ""], ["Zucker", "Evan", ""], ["Pareek", "Anuj", ""], ["Lungren", "Matthew P.", ""], ["Chaudhari", "Akshay", ""]]}, {"id": "2102.09480", "submitter": "Yen-Cheng Liu", "authors": "Yen-Cheng Liu, Chih-Yao Ma, Zijian He, Chia-Wen Kuo, Kan Chen, Peizhao\n  Zhang, Bichen Wu, Zsolt Kira, Peter Vajda", "title": "Unbiased Teacher for Semi-Supervised Object Detection", "comments": "Accepted to ICLR 2021; Code is available at\n  https://github.com/facebookresearch/unbiased-teacher", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semi-supervised learning, i.e., training networks with both labeled and\nunlabeled data, has made significant progress recently. However, existing works\nhave primarily focused on image classification tasks and neglected object\ndetection which requires more annotation effort. In this work, we revisit the\nSemi-Supervised Object Detection (SS-OD) and identify the pseudo-labeling bias\nissue in SS-OD. To address this, we introduce Unbiased Teacher, a simple yet\neffective approach that jointly trains a student and a gradually progressing\nteacher in a mutually-beneficial manner. Together with a class-balance loss to\ndownweight overly confident pseudo-labels, Unbiased Teacher consistently\nimproved state-of-the-art methods by significant margins on COCO-standard,\nCOCO-additional, and VOC datasets. Specifically, Unbiased Teacher achieves 6.8\nabsolute mAP improvements against state-of-the-art method when using 1% of\nlabeled data on MS-COCO, achieves around 10 mAP improvements against the\nsupervised baseline when using only 0.5, 1, 2% of labeled data on MS-COCO.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 17:02:57 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Liu", "Yen-Cheng", ""], ["Ma", "Chih-Yao", ""], ["He", "Zijian", ""], ["Kuo", "Chia-Wen", ""], ["Chen", "Kan", ""], ["Zhang", "Peizhao", ""], ["Wu", "Bichen", ""], ["Kira", "Zsolt", ""], ["Vajda", "Peter", ""]]}, {"id": "2102.09508", "submitter": "Hao Guan", "authors": "Hao Guan, Mingxia Liu", "title": "Domain Adaptation for Medical Image Analysis: A Survey", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning techniques used in computer-aided medical image analysis\nusually suffer from the domain shift problem caused by different distributions\nbetween source/reference data and target data. As a promising solution, domain\nadaptation has attracted considerable attention in recent years. The aim of\nthis paper is to survey the recent advances of domain adaptation methods in\nmedical image analysis. We first present the motivation of introducing domain\nadaptation techniques to tackle domain heterogeneity issues for medical image\nanalysis. Then we provide a review of recent domain adaptation models in\nvarious medical image analysis tasks. We categorize the existing methods into\nshallow and deep models, and each of them is further divided into supervised,\nsemi-supervised and unsupervised methods. We also provide a brief summary of\nthe benchmark medical image datasets that support current domain adaptation\nresearch. This survey will enable researchers to gain a better understanding of\nthe current status, challenges.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 17:49:08 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Guan", "Hao", ""], ["Liu", "Mingxia", ""]]}, {"id": "2102.09517", "submitter": "Sudhanshu Mittal", "authors": "Sudhanshu Mittal and Silvio Galesso and Thomas Brox", "title": "Essentials for Class Incremental Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary neural networks are limited in their ability to learn from\nevolving streams of training data. When trained sequentially on new or evolving\ntasks, their accuracy drops sharply, making them unsuitable for many real-world\napplications. In this work, we shed light on the causes of this well-known yet\nunsolved phenomenon - often referred to as catastrophic forgetting - in a\nclass-incremental setup. We show that a combination of simple components and a\nloss that balances intra-task and inter-task learning can already resolve\nforgetting to the same extent as more complex measures proposed in literature.\nMoreover, we identify poor quality of the learned representation as another\nreason for catastrophic forgetting in class-IL. We show that performance is\ncorrelated with secondary class information (dark knowledge) learned by the\nmodel and it can be improved by an appropriate regularizer. With these lessons\nlearned, class-incremental learning results on CIFAR-100 and ImageNet improve\nover the state-of-the-art by a large margin, while keeping the approach simple.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 18:01:06 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Mittal", "Sudhanshu", ""], ["Galesso", "Silvio", ""], ["Brox", "Thomas", ""]]}, {"id": "2102.09527", "submitter": "Ahmed Alkhateeb", "authors": "Gouranga Charan, Muhammad Alrabeiah, and Ahmed Alkhateeb", "title": "Vision-Aided 6G Wireless Communications: Blockage Prediction and\n  Proactive Handoff", "comments": "Submitted to IEEE, 30 pages, 11 figures. The dataset will be\n  available soon on the ViWi website https://viwi-dataset.net/", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The sensitivity to blockages is a key challenge for the high-frequency (5G\nmillimeter wave and 6G sub-terahertz) wireless networks. Since these networks\nmainly rely on line-of-sight (LOS) links, sudden link blockages highly threaten\nthe reliability of the networks. Further, when the LOS link is blocked, the\nnetwork typically needs to hand off the user to another LOS basestation, which\nmay incur critical time latency, especially if a search over a large codebook\nof narrow beams is needed. A promising way to tackle the reliability and\nlatency challenges lies in enabling proaction in wireless networks. Proaction\nbasically allows the network to anticipate blockages, especially dynamic\nblockages, and initiate user hand-off beforehand. This paper presents a\ncomplete machine learning framework for enabling proaction in wireless networks\nrelying on visual data captured, for example, by RGB cameras deployed at the\nbase stations. In particular, the paper proposes a vision-aided wireless\ncommunication solution that utilizes bimodal machine learning to perform\nproactive blockage prediction and user hand-off. The bedrock of this solution\nis a deep learning algorithm that learns from visual and wireless data how to\npredict incoming blockages. The predictions of this algorithm are used by the\nwireless network to proactively initiate hand-off decisions and avoid any\nunnecessary latency. The algorithm is developed on a vision-wireless dataset\ngenerated using the ViWi data-generation framework. Experimental results on two\nbasestations with different cameras indicate that the algorithm is capable of\naccurately detecting incoming blockages more than $\\sim 90\\%$ of the time. Such\nblockage prediction ability is directly reflected in the accuracy of proactive\nhand-off, which also approaches $87\\%$. This highlights a promising direction\nfor enabling high reliability and low latency in future wireless networks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 18:12:20 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 00:28:23 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Charan", "Gouranga", ""], ["Alrabeiah", "Muhammad", ""], ["Alkhateeb", "Ahmed", ""]]}, {"id": "2102.09528", "submitter": "Luis Carlos Garcia-Peraza-Herrera", "authors": "Luis C. Garcia-Peraza-Herrera, Lucas Fidon, Claudia D'Ettorre, Danail\n  Stoyanov, Tom Vercauteren, Sebastien Ourselin", "title": "Image Compositing for Segmentation of Surgical Tools without Manual\n  Annotations", "comments": "Accepted by IEEE TMI", "journal-ref": null, "doi": "10.1109/tmi.2021.3057884", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Producing manual, pixel-accurate, image segmentation labels is tedious and\ntime-consuming. This is often a rate-limiting factor when large amounts of\nlabeled images are required, such as for training deep convolutional networks\nfor instrument-background segmentation in surgical scenes. No large datasets\ncomparable to industry standards in the computer vision community are available\nfor this task. To circumvent this problem, we propose to automate the creation\nof a realistic training dataset by exploiting techniques stemming from special\neffects and harnessing them to target training performance rather than visual\nappeal. Foreground data is captured by placing sample surgical instruments over\na chroma key (a.k.a. green screen) in a controlled environment, thereby making\nextraction of the relevant image segment straightforward. Multiple lighting\nconditions and viewpoints can be captured and introduced in the simulation by\nmoving the instruments and camera and modulating the light source. Background\ndata is captured by collecting videos that do not contain instruments. In the\nabsence of pre-existing instrument-free background videos, minimal labeling\neffort is required, just to select frames that do not contain surgical\ninstruments from videos of surgical interventions freely available online. We\ncompare different methods to blend instruments over tissue and propose a novel\ndata augmentation approach that takes advantage of the plurality of options. We\nshow that by training a vanilla U-Net on semi-synthetic data only and applying\na simple post-processing, we are able to match the results of the same network\ntrained on a publicly available manually labeled real dataset.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 18:14:43 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Garcia-Peraza-Herrera", "Luis C.", ""], ["Fidon", "Lucas", ""], ["D'Ettorre", "Claudia", ""], ["Stoyanov", "Danail", ""], ["Vercauteren", "Tom", ""], ["Ourselin", "Sebastien", ""]]}, {"id": "2102.09532", "submitter": "Danijar Hafner", "authors": "Vaibhav Saxena, Jimmy Ba, Danijar Hafner", "title": "Clockwork Variational Autoencoders", "comments": "17 pages, 12 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has enabled algorithms to generate realistic images. However,\naccurately predicting long video sequences requires understanding long-term\ndependencies and remains an open challenge. While existing video prediction\nmodels succeed at generating sharp images, they tend to fail at accurately\npredicting far into the future. We introduce the Clockwork VAE (CW-VAE), a\nvideo prediction model that leverages a hierarchy of latent sequences, where\nhigher levels tick at slower intervals. We demonstrate the benefits of both\nhierarchical latents and temporal abstraction on 4 diverse video prediction\ndatasets with sequences of up to 1000 frames, where CW-VAE outperforms top\nvideo prediction models. Additionally, we propose a Minecraft benchmark for\nlong-term video prediction. We conduct several experiments to gain insights\ninto CW-VAE and confirm that slower levels learn to represent objects that\nchange more slowly in the video, and faster levels learn to represent faster\nobjects.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 18:23:04 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 21:33:21 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Saxena", "Vaibhav", ""], ["Ba", "Jimmy", ""], ["Hafner", "Danijar", ""]]}, {"id": "2102.09542", "submitter": "Bo Li", "authors": "Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, Xiao-Ming Wu", "title": "SLAKE: A Semantically-Labeled Knowledge-Enhanced Dataset for Medical\n  Visual Question Answering", "comments": "ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Medical visual question answering (Med-VQA) has tremendous potential in\nhealthcare. However, the development of this technology is hindered by the\nlacking of publicly-available and high-quality labeled datasets for training\nand evaluation. In this paper, we present a large bilingual dataset, SLAKE,\nwith comprehensive semantic labels annotated by experienced physicians and a\nnew structural medical knowledge base for Med-VQA. Besides, SLAKE includes\nricher modalities and covers more human body parts than the currently available\ndataset. We show that SLAKE can be used to facilitate the development and\nevaluation of Med-VQA systems. The dataset can be downloaded from\nhttp://www.med-vqa.com/slake.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 18:44:50 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Liu", "Bo", ""], ["Zhan", "Li-Ming", ""], ["Xu", "Li", ""], ["Ma", "Lin", ""], ["Yang", "Yan", ""], ["Wu", "Xiao-Ming", ""]]}, {"id": "2102.09546", "submitter": "Alireza Sepas-Moghaddam", "authors": "Alireza Sepas-Moghaddam, Ali Etemad", "title": "Deep Gait Recognition: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gait recognition is an appealing biometric modality which aims to identify\nindividuals based on the way they walk. Deep learning has reshaped the research\nlandscape in this area since 2015 through the ability to automatically learn\ndiscriminative representations. Gait recognition methods based on deep learning\nnow dominate the state-of-the-art in the field and have fostered real-world\napplications. In this paper, we present a comprehensive overview of\nbreakthroughs and recent developments in gait recognition with deep learning,\nand cover broad topics including datasets, test protocols, state-of-the-art\nsolutions, challenges, and future research directions. We first review the\ncommonly used gait datasets along with the principles designed for evaluating\nthem. We then propose a novel taxonomy made up of four separate dimensions\nnamely body representation, temporal representation, feature representation,\nand neural architecture, to help characterize and organize the research\nlandscape and literature in this area. Following our proposed taxonomy, a\ncomprehensive survey of gait recognition methods using deep learning is\npresented with discussions on their performances, characteristics, advantages,\nand limitations. We conclude this survey with a discussion on current\nchallenges and mention a number of promising directions for future research in\ngait recognition.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 18:49:28 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Sepas-Moghaddam", "Alireza", ""], ["Etemad", "Ali", ""]]}, {"id": "2102.09554", "submitter": "Yuzhe Yang", "authors": "Yuzhe Yang, Kaiwen Zha, Ying-Cong Chen, Hao Wang, Dina Katabi", "title": "Delving into Deep Imbalanced Regression", "comments": "ICML 2021 (Long Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world data often exhibit imbalanced distributions, where certain target\nvalues have significantly fewer observations. Existing techniques for dealing\nwith imbalanced data focus on targets with categorical indices, i.e., different\nclasses. However, many tasks involve continuous targets, where hard boundaries\nbetween classes do not exist. We define Deep Imbalanced Regression (DIR) as\nlearning from such imbalanced data with continuous targets, dealing with\npotential missing data for certain target values, and generalizing to the\nentire target range. Motivated by the intrinsic difference between categorical\nand continuous label space, we propose distribution smoothing for both labels\nand features, which explicitly acknowledges the effects of nearby targets, and\ncalibrates both label and learned feature distributions. We curate and\nbenchmark large-scale DIR datasets from common real-world tasks in computer\nvision, natural language processing, and healthcare domains. Extensive\nexperiments verify the superior performance of our strategies. Our work fills\nthe gap in benchmarks and techniques for practical imbalanced regression\nproblems. Code and data are available at\nhttps://github.com/YyzHarry/imbalanced-regression.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 18:56:03 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 17:58:27 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Yang", "Yuzhe", ""], ["Zha", "Kaiwen", ""], ["Chen", "Ying-Cong", ""], ["Wang", "Hao", ""], ["Katabi", "Dina", ""]]}, {"id": "2102.09559", "submitter": "Chen Wei", "authors": "Chen Wei, Kihyuk Sohn, Clayton Mellina, Alan Yuille, Fan Yang", "title": "CReST: A Class-Rebalancing Self-Training Framework for Imbalanced\n  Semi-Supervised Learning", "comments": "To appear in CVPR 2021. Code release:\n  https://github.com/google-research/crest", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning on class-imbalanced data, although a realistic\nproblem, has been under studied. While existing semi-supervised learning (SSL)\nmethods are known to perform poorly on minority classes, we find that they\nstill generate high precision pseudo-labels on minority classes. By exploiting\nthis property, in this work, we propose Class-Rebalancing Self-Training\n(CReST), a simple yet effective framework to improve existing SSL methods on\nclass-imbalanced data. CReST iteratively retrains a baseline SSL model with a\nlabeled set expanded by adding pseudo-labeled samples from an unlabeled set,\nwhere pseudo-labeled samples from minority classes are selected more frequently\naccording to an estimated class distribution. We also propose a progressive\ndistribution alignment to adaptively adjust the rebalancing strength dubbed\nCReST+. We show that CReST and CReST+ improve state-of-the-art SSL algorithms\non various class-imbalanced datasets and consistently outperform other popular\nrebalancing methods. Code has been made available at\nhttps://github.com/google-research/crest.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 18:59:57 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 17:56:51 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Wei", "Chen", ""], ["Sohn", "Kihyuk", ""], ["Mellina", "Clayton", ""], ["Yuille", "Alan", ""], ["Yang", "Fan", ""]]}, {"id": "2102.09563", "submitter": "Roc\\'io del Amor del Amor", "authors": "del Amor Roc\\'io, Colomer Adri\\'an, Monteagudo Carlos, Naranjo Valery", "title": "A Deep Embedded Refined Clustering Approach for Breast Cancer\n  Distinction based on DNA Methylation", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epigenetic alterations have an important role in the development of several\ntypes of cancer. Epigenetic studies generate a large amount of data, which\nmakes it essential to develop novel models capable of dealing with large-scale\ndata. In this work, we propose a deep embedded refined clustering method for\nbreast cancer differentiation based on DNA methylation. In concrete, the deep\nlearning system presented here uses the levels of CpG island methylation\nbetween 0 and 1. The proposed approach is composed of two main stages. The\nfirst stage consists in the dimensionality reduction of the methylation data\nbased on an autoencoder. The second stage is a clustering algorithm based on\nthe soft-assignment of the latent space provided by the autoencoder. The whole\nmethod is optimized through a weighted loss function composed of two terms:\nreconstruction and classification terms. To the best of the authors' knowledge,\nno previous studies have focused on the dimensionality reduction algorithms\nlinked to classification trained end-to-end for DNA methylation analysis. The\nproposed method achieves an unsupervised clustering accuracy of 0.9927 and an\nerror rate (%) of 0.73 on 137 breast tissue samples. After a second test of the\ndeep-learning-based method using a different methylation database, an accuracy\nof 0.9343 and an error rate (%) of 6.57 on 45 breast tissue samples is\nobtained. Based on these results, the proposed algorithm outperforms other\nstate-of-the-art methods evaluated under the same conditions for breast cancer\nclassification based on DNA methylation data.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 16:46:25 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Roc\u00edo", "del Amor", ""], ["Adri\u00e1n", "Colomer", ""], ["Carlos", "Monteagudo", ""], ["Valery", "Naranjo", ""]]}, {"id": "2102.09582", "submitter": "Andreanne Lemay", "authors": "Andreanne Lemay, Charley Gros, Olivier Vincent, Yaou Liu, Joseph Paul\n  Cohen, Julien Cohen-Adad", "title": "Benefits of Linear Conditioning with Metadata for Image Segmentation", "comments": "Accepted at MIDL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical images are often accompanied by metadata describing the image\n(vendor, acquisition parameters) and the patient (disease type or severity,\ndemographics, genomics). This metadata is usually disregarded by image\nsegmentation methods. In this work, we adapt a linear conditioning method\ncalled FiLM (Feature-wise Linear Modulation) for image segmentation tasks. This\nFiLM adaptation enables integrating metadata into segmentation models for\nbetter performance. We observed an average Dice score increase of 5.1% on\nspinal cord tumor segmentation when incorporating the tumor type with FiLM. The\nmetadata modulates the segmentation process through low-cost affine\ntransformations applied on feature maps which can be included in any neural\nnetwork's architecture. Additionally, we assess the relevance of segmentation\nFiLM layers for tackling common challenges in medical imaging: multi-class\ntraining with missing segmentations, model adaptation to multiple tasks, and\ntraining with a limited or unbalanced number of annotated data. Our results\ndemonstrated the following benefits of FiLM for segmentation: FiLMed U-Net was\nrobust to missing labels and reached higher Dice scores with few labels (up to\n16.7%) compared to single-task U-Net. The code is open-source and available at\nwww.ivadomed.org.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 19:03:58 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 15:16:20 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Lemay", "Andreanne", ""], ["Gros", "Charley", ""], ["Vincent", "Olivier", ""], ["Liu", "Yaou", ""], ["Cohen", "Joseph Paul", ""], ["Cohen-Adad", "Julien", ""]]}, {"id": "2102.09603", "submitter": "Sowmen Das", "authors": "Sowmen Das, Arup Datta, Md. Saiful Islam, Md. Ruhul Amin", "title": "Improving DeepFake Detection Using Dynamic Face Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The creation of altered and manipulated faces has become more common due to\nthe improvement of DeepFake generation methods. Simultaneously, we have seen\ndetection models' development for differentiating between a manipulated and\noriginal face from image or video content. We have observed that most publicly\navailable DeepFake detection datasets have limited variations, where a single\nface is used in many videos, resulting in an oversampled training dataset. Due\nto this, deep neural networks tend to overfit to the facial features instead of\nlearning to detect manipulation features of DeepFake content. As a result, most\ndetection architectures perform poorly when tested on unseen data. In this\npaper, we provide a quantitative analysis to investigate this problem and\npresent a solution to prevent model overfitting due to the high volume of\nsamples generated from a small number of actors. We introduce Face-Cutout, a\ndata augmentation method for training Convolutional Neural Networks (CNN), to\nimprove DeepFake detection. In this method, training images with various\nocclusions are dynamically generated using face landmark information\nirrespective of orientation. Unlike other general-purpose augmentation methods,\nit focuses on the facial information that is crucial for DeepFake detection.\nOur method achieves a reduction in LogLoss of 15.2% to 35.3% on different\ndatasets, compared to other occlusion-based augmentation techniques. We show\nthat Face-Cutout can be easily integrated with any CNN-based recognition model\nand improve detection performance.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 20:25:45 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 02:19:30 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Das", "Sowmen", ""], ["Datta", "Arup", ""], ["Islam", "Md. Saiful", ""], ["Amin", "Md. Ruhul", ""]]}, {"id": "2102.09615", "submitter": "Chuang Niu", "authors": "Chuang Niu, Ge Wang, Pingkun Yan, Juergen Hahn, Youfang Lai, Xun Jia,\n  Arjun Krishna, Klaus Mueller, Andreu Badal, KyleJ. Myers, Rongping Zeng", "title": "Noise Entangled GAN For Low-Dose CT Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a Noise Entangled GAN (NE-GAN) for simulating low-dose computed\ntomography (CT) images from a higher dose CT image. First, we present two\nschemes to generate a clean CT image and a noise image from the high-dose CT\nimage. Then, given these generated images, an NE-GAN is proposed to simulate\ndifferent levels of low-dose CT images, where the level of generated noise can\nbe continuously controlled by a noise factor. NE-GAN consists of a generator\nand a set of discriminators, and the number of discriminators is determined by\nthe number of noise levels during training. Compared with the traditional\nmethods based on the projection data that are usually unavailable in real\napplications, NE-GAN can directly learn from the real and/or simulated CT\nimages and may create low-dose CT images quickly without the need of raw data\nor other proprietary CT scanner information. The experimental results show that\nthe proposed method has the potential to simulate realistic low-dose CT images.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 21:04:32 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Niu", "Chuang", ""], ["Wang", "Ge", ""], ["Yan", "Pingkun", ""], ["Hahn", "Juergen", ""], ["Lai", "Youfang", ""], ["Jia", "Xun", ""], ["Krishna", "Arjun", ""], ["Mueller", "Klaus", ""], ["Badal", "Andreu", ""], ["Myers", "KyleJ.", ""], ["Zeng", "Rongping", ""]]}, {"id": "2102.09717", "submitter": "Weixia Zhang", "authors": "Weixia Zhang and Dingquan Li and Chao Ma and Guangtao Zhai and\n  Xiaokang Yang and Kede Ma", "title": "Continual Learning for Blind Image Quality Assessment", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The explosive growth of image data facilitates the fast development of image\nprocessing and computer vision methods for emerging visual applications,\nmeanwhile introducing novel distortions to the processed images. This poses a\ngrand challenge to existing blind image quality assessment (BIQA) models,\nfailing to continually adapt to such subpopulation shift. Recent work suggests\ntraining BIQA methods on the combination of all available human-rated IQA\ndatasets. However, this type of approach is not scalable to a large number of\ndatasets, and is cumbersome to incorporate a newly created dataset as well. In\nthis paper, we formulate continual learning for BIQA, where a model learns\ncontinually from a stream of IQA datasets, building on what was learned from\npreviously seen data. We first identify five desiderata in the new setting with\na measure to quantify the plasticity-stability trade-off. We then propose a\nsimple yet effective method for learning BIQA models continually. Specifically,\nbased on a shared backbone network, we add a prediction head for a new dataset,\nand enforce a regularizer to allow all prediction heads to evolve with new data\nwhile being resistant to catastrophic forgetting of old data. We compute the\nquality score by an adaptive weighted summation of estimates from all\nprediction heads. Extensive experiments demonstrate the promise of the proposed\ncontinual learning method in comparison to standard training techniques for\nBIQA.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 03:07:01 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Zhang", "Weixia", ""], ["Li", "Dingquan", ""], ["Ma", "Chao", ""], ["Zhai", "Guangtao", ""], ["Yang", "Xiaokang", ""], ["Ma", "Kede", ""]]}, {"id": "2102.09737", "submitter": "Srishti Goel Ms.", "authors": "Neeraj Kumar, Srishti Goel, Ankur Narang, Brejesh Lall, Mujtaba Hasan,\n  Pranshu Agarwal, Dipankar Sarkar", "title": "One Shot Audio to Animated Video Generation", "comments": "arXiv admin note: substantial text overlap with arXiv:2012.07842,\n  arXiv:2012.07304", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We consider the challenging problem of audio to animated video generation. We\npropose a novel method OneShotAu2AV to generate an animated video of arbitrary\nlength using an audio clip and a single unseen image of a person as an input.\nThe proposed method consists of two stages. In the first stage, OneShotAu2AV\ngenerates the talking-head video in the human domain given an audio and a\nperson's image. In the second stage, the talking-head video from the human\ndomain is converted to the animated domain. The model architecture of the first\nstage consists of spatially adaptive normalization based multi-level generator\nand multiple multilevel discriminators along with multiple adversarial and\nnon-adversarial losses. The second stage leverages attention based\nnormalization driven GAN architecture along with temporal predictor based\nrecycle loss and blink loss coupled with lipsync loss, for unsupervised\ngeneration of animated video. In our approach, the input audio clip is not\nrestricted to any specific language, which gives the method multilingual\napplicability. OneShotAu2AV can generate animated videos that have: (a) lip\nmovements that are in sync with the audio, (b) natural facial expressions such\nas blinks and eyebrow movements, (c) head movements. Experimental evaluation\ndemonstrates superior performance of OneShotAu2AV as compared to U-GAT-IT and\nRecycleGan on multiple quantitative metrics including KID(Kernel Inception\nDistance), Word error rate, blinks/sec\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 04:29:17 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Kumar", "Neeraj", ""], ["Goel", "Srishti", ""], ["Narang", "Ankur", ""], ["Lall", "Brejesh", ""], ["Hasan", "Mujtaba", ""], ["Agarwal", "Pranshu", ""], ["Sarkar", "Dipankar", ""]]}, {"id": "2102.09744", "submitter": "Zakria Jamali Dr.", "authors": "Zakria, Jianhua Deng, Muhammad Saddam Khokhar, Muhammad Umar Aftab,\n  Jingye Cai, Rajesh Kumar and Jay Kumar", "title": "Trends in Vehicle Re-identification Past, Present, and Future: A\n  Comprehensive Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle Re-identification (re-id) over surveillance camera network with\nnon-overlapping field of view is an exciting and challenging task in\nintelligent transportation systems (ITS). Due to its versatile applicability in\nmetropolitan cities, it gained significant attention. Vehicle re-id matches\ntargeted vehicle over non-overlapping views in multiple camera network.\nHowever, it becomes more difficult due to inter-class similarity, intra-class\nvariability, viewpoint changes, and spatio-temporal uncertainty. In order to\ndraw a detailed picture of vehicle re-id research, this paper gives a\ncomprehensive description of the various vehicle re-id technologies,\napplicability, datasets, and a brief comparison of different methodologies. Our\npaper specifically focuses on vision-based vehicle re-id approaches, including\nvehicle appearance, license plate, and spatio-temporal characteristics. In\naddition, we explore the main challenges as well as a variety of applications\nin different domains. Lastly, a detailed comparison of current state-of-the-art\nmethods performances over VeRi-776 and VehicleID datasets is summarized with\nfuture directions. We aim to facilitate future research by reviewing the work\nbeing done on vehicle re-id till to date.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 05:02:24 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Zakria", "", ""], ["Deng", "Jianhua", ""], ["Khokhar", "Muhammad Saddam", ""], ["Aftab", "Muhammad Umar", ""], ["Cai", "Jingye", ""], ["Kumar", "Rajesh", ""], ["Kumar", "Jay", ""]]}, {"id": "2102.09754", "submitter": "Ryan Hoque", "authors": "Ryan Hoque, Daniel Seita, Ashwin Balakrishna, Aditya Ganapathi, Ajay\n  Kumar Tanwani, Nawid Jamali, Katsu Yamane, Soshi Iba, Ken Goldberg", "title": "VisuoSpatial Foresight for Physical Sequential Fabric Manipulation", "comments": "Journal extension of prior work on VSF to appear in Autonomous Robots\n  S.I. 207. arXiv admin note: text overlap with arXiv:2003.09044", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic fabric manipulation has applications in home robotics, textiles,\nsenior care and surgery. Existing fabric manipulation techniques, however, are\ndesigned for specific tasks, making it difficult to generalize across different\nbut related tasks. We build upon the Visual Foresight framework to learn fabric\ndynamics that can be efficiently reused to accomplish different sequential\nfabric manipulation tasks with a single goal-conditioned policy. We extend our\nearlier work on VisuoSpatial Foresight (VSF), which learns visual dynamics on\ndomain randomized RGB images and depth maps simultaneously and completely in\nsimulation. In this earlier work, we evaluated VSF on multi-step fabric\nsmoothing and folding tasks against 5 baseline methods in simulation and on the\nda Vinci Research Kit (dVRK) surgical robot without any demonstrations at train\nor test time. A key finding was that depth sensing significantly improves\nperformance: RGBD data yields an 80% improvement in fabric folding success rate\nin simulation over pure RGB data. In this work, we vary 4 components of VSF,\nincluding data generation, visual dynamics model, cost function, and\noptimization procedure. Results suggest that training visual dynamics models\nusing longer, corner-based actions can improve the efficiency of fabric folding\nby 76% and enable a physical sequential fabric folding task that VSF could not\npreviously perform with 90% reliability. Code, data, videos, and supplementary\nmaterial are available at https://sites.google.com/view/fabric-vsf/.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 06:06:49 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 21:40:13 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Hoque", "Ryan", ""], ["Seita", "Daniel", ""], ["Balakrishna", "Ashwin", ""], ["Ganapathi", "Aditya", ""], ["Tanwani", "Ajay Kumar", ""], ["Jamali", "Nawid", ""], ["Yamane", "Katsu", ""], ["Iba", "Soshi", ""], ["Goldberg", "Ken", ""]]}, {"id": "2102.09757", "submitter": "Bin Li", "authors": "Bin Li, Hong Fu, Ruimin Li and Wendi Wang", "title": "Serial-parallel Multi-Scale Feature Fusion for Anatomy-Oriented Hand\n  Joint Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Accurate hand joints detection from images is a fundamental topic which is\nessential for many applications in computer vision and human computer\ninteraction. This paper presents a two stage network for hand joints detection\nfrom single unmarked image by using serial-parallel multi-scale feature fusion.\nIn stage I, the hand regions are located by a pre-trained network, and the\nfeatures of each detected hand region are extracted by a shallow spatial hand\nfeatures representation module. The extracted hand features are then fed into\nstage II, which consists of serially connected feature extraction modules with\nsimilar structures, called \"multi-scale feature fusion\" (MSFF). A MSFF contains\nparallel multi-scale feature extraction branches, which generate initial hand\njoint heatmaps. The initial heatmaps are then mutually reinforced by the\nanatomic relationship between hand joints. The experimental results on five\nhand joints datasets show that the proposed network overperforms the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 06:12:24 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Li", "Bin", ""], ["Fu", "Hong", ""], ["Li", "Ruimin", ""], ["Wang", "Wendi", ""]]}, {"id": "2102.09796", "submitter": "Kan Shichao", "authors": "Shichao Kan, Yue Zhang, Fanghui Zhang and Yigang Cen", "title": "A GAN-Based Input-Size Flexibility Model for Single Image Dehazing", "comments": "Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Image-to-image translation based on generative adversarial network (GAN) has\nachieved state-of-the-art performance in various image restoration\napplications. Single image dehazing is a typical example, which aims to obtain\nthe haze-free image of a haze one. This paper concentrates on the challenging\ntask of single image dehazing. Based on the atmospheric scattering model, we\ndesign a novel model to directly generate the haze-free image. The main\nchallenge of image dehazing is that the atmospheric scattering model has two\nparameters, i.e., transmission map and atmospheric light. When we estimate them\nrespectively, the errors will be accumulated to compromise dehazing quality.\nConsidering this reason and various image sizes, we propose a novel input-size\nflexibility conditional generative adversarial network (cGAN) for single image\ndehazing, which is input-size flexibility at both training and test stages for\nimage-to-image translation with cGAN framework. We propose a simple and\neffective U-type residual network (UR-Net) to combine the generator and adopt\nthe spatial pyramid pooling (SPP) to design the discriminator. Moreover, the\nmodel is trained with multi-loss function, in which the consistency loss is a\nnovel designed loss in this paper. We finally build a multi-scale cGAN fusion\nmodel to realize state-of-the-art single image dehazing performance. The\nproposed models receive a haze image as input and directly output a haze-free\none. Experimental results demonstrate the effectiveness and efficiency of the\nproposed models.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 08:27:17 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Kan", "Shichao", ""], ["Zhang", "Yue", ""], ["Zhang", "Fanghui", ""], ["Cen", "Yigang", ""]]}, {"id": "2102.09808", "submitter": "Michael Mozer", "authors": "Michael L. Iuzzolino, Michael C. Mozer, Samy Bengio", "title": "Improving Anytime Prediction with Parallel Cascaded Networks and a\n  Temporal-Difference Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep feedforward neural networks share some characteristics with the\nprimate visual system, a key distinction is their dynamics. Deep nets typically\noperate in serial stages wherein each layer completes its computation before\nprocessing begins in subsequent layers. In contrast, biological systems have\ncascaded dynamics: information propagates from neurons at all layers in\nparallel but transmission occurs gradually over time, leading to speed-accuracy\ntrade offs even in feedforward architectures. We explore the consequences of\nbiologically inspired parallel hardware by constructing cascaded ResNets in\nwhich each residual block has propagation delays but all blocks update in\nparallel in a stateful manner. Because information transmitted through skip\nconnections avoids delays, the functional depth of the architecture increases\nover time, yielding anytime predictions that improve with internal-processing\ntime. We introduce a temporal-difference training loss that achieves a strictly\nsuperior speed-accuracy profile over standard losses and enables the cascaded\narchitecture to outperform state-of-the-art anytime-prediction methods. The\ncascaded architecture has intriguing properties, including: it classifies\ntypical instances more rapidly than atypical instances; it is more robust to\nboth persistent and transient noise than is a conventional ResNet; and its\ntime-varying output trace provides a signal that can be exploited to improve\ninformation processing and inference.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 08:40:19 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 20:52:25 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 06:15:12 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Iuzzolino", "Michael L.", ""], ["Mozer", "Michael C.", ""], ["Bengio", "Samy", ""]]}, {"id": "2102.09858", "submitter": "Kanggeun Lee", "authors": "Kanggeun Lee and Won-Ki Jeong", "title": "ISCL: Interdependent Self-Cooperative Learning for Unpaired Image\n  Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the advent of advances in self-supervised learning, paired clean-noisy\ndata are no longer required in deep learning-based image denoising. However,\nexisting blind denoising methods still require the assumption with regard to\nnoise characteristics, such as zero-mean noise distribution and pixel-wise\nnoise-signal independence; this hinders wide adaptation of the method in the\nmedical domain. On the other hand, unpaired learning can overcome limitations\nrelated to the assumption on noise characteristics, which makes it more\nfeasible for collecting the training data in real-world scenarios. In this\npaper, we propose a novel image denoising scheme, Interdependent\nSelf-Cooperative Learning (ISCL), that leverages unpaired learning by combining\ncyclic adversarial learning with self-supervised residual learning. Unlike the\nexisting unpaired image denoising methods relying on matching data\ndistributions in different domains, the two architectures in ISCL, designed for\ndifferent tasks, complement each other and boost the learning process. To\nassess the performance of the proposed method, we conducted extensive\nexperiments in various biomedical image degradation scenarios, such as noise\ncaused by physical characteristics of electron microscopy (EM) devices (film\nand charging noise), and structural noise found in low-dose computer tomography\n(CT). We demonstrate that the image quality of our method is superior to\nconventional and current state-of-the-art deep learning-based image denoising\nmethods, including supervised learning.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 10:54:25 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Lee", "Kanggeun", ""], ["Jeong", "Won-Ki", ""]]}, {"id": "2102.09875", "submitter": "Jack Yang", "authors": "Shaokang Yang, Shuai Liu, Cheng Yang, Changhu Wang", "title": "Re-rank Coarse Classification with Local Region Enhanced Features for\n  Fine-Grained Image Recognition", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained image recognition is very challenging due to the difficulty of\ncapturing both semantic global features and discriminative local features.\nMeanwhile, these two features are not easy to be integrated, which are even\nconflicting when used simultaneously. In this paper, a retrieval-based\ncoarse-to-fine framework is proposed, where we re-rank the TopN classification\nresults by using the local region enhanced embedding features to improve the\nTop1 accuracy (based on the observation that the correct category usually\nresides in TopN results). To obtain the discriminative regions for\ndistinguishing the fine-grained images, we introduce a weakly-supervised method\nto train a box generating branch with only image-level labels. In addition, to\nlearn more effective semantic global features, we design a multi-level loss\nover an automatically constructed hierarchical category structure. Experimental\nresults show that our method achieves state-of-the-art performance on three\nbenchmarks: CUB-200-2011, Stanford Cars, and FGVC Aircraft. Also,\nvisualizations and analysis are provided for better understanding.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 11:30:25 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Yang", "Shaokang", ""], ["Liu", "Shuai", ""], ["Yang", "Cheng", ""], ["Wang", "Changhu", ""]]}, {"id": "2102.09883", "submitter": "George Eskandar", "authors": "George Eskandar, Alexander Braun, Martin Meinke, Karim Armanious, Bin\n  Yang", "title": "SLPC: a VRNN-based approach for stochastic lidar prediction and\n  completion in autonomous driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting future 3D LiDAR pointclouds is a challenging task that is useful\nin many applications in autonomous driving such as trajectory prediction, pose\nforecasting and decision making. In this work, we propose a new LiDAR\nprediction framework that is based on generative models namely Variational\nRecurrent Neural Networks (VRNNs), titled Stochastic LiDAR Prediction and\nCompletion (SLPC). Our algorithm is able to address the limitations of previous\nvideo prediction frameworks when dealing with sparse data by spatially\ninpainting the depth maps in the upcoming frames. Our contributions can thus be\nsummarized as follows: we introduce the new task of predicting and completing\ndepth maps from spatially sparse data, we present a sparse version of VRNNs and\nan effective self-supervised training method that does not require any labels.\nExperimental results illustrate the effectiveness of our framework in\ncomparison to the state of the art methods in video prediction.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 11:56:44 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Eskandar", "George", ""], ["Braun", "Alexander", ""], ["Meinke", "Martin", ""], ["Armanious", "Karim", ""], ["Yang", "Bin", ""]]}, {"id": "2102.09895", "submitter": "Behzad Bozorgtabar", "authors": "Antoine Spahr, Behzad Bozorgtabar, Jean-Philippe Thiran", "title": "Self-Taught Semi-Supervised Anomaly Detection on Upper Limb X-rays", "comments": "Accepted by ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Detecting anomalies in musculoskeletal radiographs is of paramount importance\nfor large-scale screening in the radiology workflow. Supervised deep networks\ntake for granted a large number of annotations by radiologists, which is often\nprohibitively very time-consuming to acquire. Moreover, supervised systems are\ntailored to closed set scenarios, e.g., trained models suffer from overfitting\nto previously seen rare anomalies at training. Instead, our approach's\nrationale is to use task agnostic pretext tasks to leverage unlabeled data\nbased on a cross-sample similarity measure. Besides, we formulate a complex\ndistribution of data from normal class within our framework to avoid a\npotential bias on the side of anomalies. Through extensive experiments, we show\nthat our method outperforms baselines across unsupervised and self-supervised\nanomaly detection settings on a real-world medical dataset, the MURA dataset.\nWe also provide rich ablation studies to analyze each training stage's effect\nand loss terms on the final performance.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 12:32:58 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 09:21:09 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Spahr", "Antoine", ""], ["Bozorgtabar", "Behzad", ""], ["Thiran", "Jean-Philippe", ""]]}, {"id": "2102.09896", "submitter": "Peng Jiang Dr.", "authors": "Zhiyi Pan, Peng Jiang, Yunhai Wang, Changhe Tu, Anthony G. Cohn", "title": "Scribble-Supervised Semantic Segmentation by Uncertainty Reduction on\n  Neural Representation and Self-Supervision on Neural Eigenspace", "comments": "arXiv admin note: substantial text overlap with arXiv:2011.05621", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scribble-supervised semantic segmentation has gained much attention recently\nfor its promising performance without high-quality annotations. Due to the lack\nof supervision, confident and consistent predictions are usually hard to\nobtain. Typically, people handle these problems to either adopt an auxiliary\ntask with the well-labeled dataset or incorporate the graphical model with\nadditional requirements on scribble annotations. Instead, this work aims to\nachieve semantic segmentation by scribble annotations directly without extra\ninformation and other limitations. Specifically, we propose holistic\noperations, including minimizing entropy and a network embedded random walk on\nneural representation to reduce uncertainty. Given the probabilistic transition\nmatrix of a random walk, we further train the network with self-supervision on\nits neural eigenspace to impose consistency on predictions between related\nimages. Comprehensive experiments and ablation studies verify the proposed\napproach, which demonstrates superiority over others; it is even comparable to\nsome full-label supervised ones and works well when scribbles are randomly\nshrunk or dropped.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 12:33:57 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Pan", "Zhiyi", ""], ["Jiang", "Peng", ""], ["Wang", "Yunhai", ""], ["Tu", "Changhe", ""], ["Cohn", "Anthony G.", ""]]}, {"id": "2102.09961", "submitter": "Simon Brenner", "authors": "Simon Brenner and Robert Sablatnig", "title": "Subjective Assessments of Legibility in Ancient Manuscript Images -- The\n  SALAMI Dataset", "comments": "This is an extended version of a paper presented at the PatReCH 2020\n  (ICPR) workshop and currently in the publication process for Springer LNCS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research field concerned with the digital restoration of degraded written\nheritage lacks a quantitative metric for evaluating its results, which prevents\nthe comparison of relevant methods on large datasets. Thus, we introduce a\nnovel dataset of Subjective Assessments of Legibility in Ancient Manuscript\nImages (SALAMI) to serve as a ground truth for the development of quantitative\nevaluation metrics in the field of digital text restoration. This dataset\nconsists of 250 images of 50 manuscript regions with corresponding spatial maps\nof mean legibility and uncertainty, which are based on a study conducted with\n20 experts of philology and paleography. As this study is the first of its\nkind, the validity and reliability of its design and the results obtained are\nmotivated statistically: we report a high intra- and inter-rater agreement and\nshow that the bulk of variation in the scores is introduced by the images\nregions observed and not by controlled or uncontrolled properties of\nparticipants and test environments, thus concluding that the legibility scores\nmeasured are valid attributes of the underlying images.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 14:53:19 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Brenner", "Simon", ""], ["Sablatnig", "Robert", ""]]}, {"id": "2102.09963", "submitter": "Luis Carlos Garcia-Peraza-Herrera", "authors": "Luis C. Garcia-Peraza-Herrera, Martin Everson, Laurence Lovat, Hsiu-Po\n  Wang, Wen Lun Wang, Rehan Haidry, Danail Stoyanov, Sebastien Ourselin, Tom\n  Vercauteren", "title": "Intrapapillary Capillary Loop Classification in Magnification Endoscopy:\n  Open Dataset and Baseline Methodology", "comments": "Accepted by IJCARS", "journal-ref": null, "doi": "10.1007/s11548-020-02127-w", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose. Early squamous cell neoplasia (ESCN) in the oesophagus is a highly\ntreatable condition. Lesions confined to the mucosal layer can be curatively\ntreated endoscopically. We build a computer-assisted detection (CADe) system\nthat can classify still images or video frames as normal or abnormal with high\ndiagnostic accuracy. Methods. We present a new benchmark dataset containing 68K\nbinary labeled frames extracted from 114 patient videos whose imaged areas have\nbeen resected and correlated to histopathology. Our novel convolutional network\n(CNN) architecture solves the binary classification task and explains what\nfeatures of the input domain drive the decision-making process of the network.\nResults. The proposed method achieved an average accuracy of 91.7 % compared to\nthe 94.7 % achieved by a group of 12 senior clinicians. Our novel network\narchitecture produces deeply supervised activation heatmaps that suggest the\nnetwork is looking at intrapapillary capillary loop (IPCL) patterns when\npredicting abnormality. Conclusion. We believe that this dataset and baseline\nmethod may serve as a reference for future benchmarks on both video frame\nclassification and explainability in the context of ESCN detection. A future\nwork path of high clinical relevance is the extension of the classification to\nESCN types.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 14:55:21 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Garcia-Peraza-Herrera", "Luis C.", ""], ["Everson", "Martin", ""], ["Lovat", "Laurence", ""], ["Wang", "Hsiu-Po", ""], ["Wang", "Wen Lun", ""], ["Haidry", "Rehan", ""], ["Stoyanov", "Danail", ""], ["Ourselin", "Sebastien", ""], ["Vercauteren", "Tom", ""]]}, {"id": "2102.09973", "submitter": "Naoya Takeishi", "authors": "Naoya Takeishi, Keisuke Fujii, Koh Takeuchi, Yoshinobu Kawahara", "title": "Discriminant Dynamic Mode Decomposition for Labeled Spatio-Temporal Data\n  Collections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NA math.DS math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extracting coherent patterns is one of the standard approaches towards\nunderstanding spatio-temporal data. Dynamic mode decomposition (DMD) is a\npowerful tool for extracting coherent patterns, but the original DMD and most\nof its variants do not consider label information, which is often available as\nside information of spatio-temporal data. In this work, we propose a new method\nfor extracting distinctive coherent patterns from labeled spatio-temporal data\ncollections, such that they contribute to major differences in a labeled set of\ndynamics. We achieve such pattern extraction by incorporating discriminant\nanalysis into DMD. To this end, we define a kernel function on subspaces\nspanned by sets of dynamic modes and develop an objective to take both\nreconstruction goodness as DMD and class-separation goodness as discriminant\nanalysis into account. We illustrate our method using a synthetic dataset and\nseveral real-world datasets. The proposed method can be a useful tool for\nexploratory data analysis for understanding spatio-temporal data.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 15:12:59 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Takeishi", "Naoya", ""], ["Fujii", "Keisuke", ""], ["Takeuchi", "Koh", ""], ["Kawahara", "Yoshinobu", ""]]}, {"id": "2102.10033", "submitter": "Ting-Yao Hu", "authors": "Ting-Yao Hu, Alexander G. Hauptmann", "title": "Pose Guided Person Image Generation with Hidden p-Norm Regression", "comments": null, "journal-ref": "ICIP 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a novel approach to solve the pose guided person\nimage generation task. We assume that the relation between pose and appearance\ninformation can be described by a simple matrix operation in hidden space.\nBased on this assumption, our method estimates a pose-invariant feature matrix\nfor each identity, and uses it to predict the target appearance conditioned on\nthe target pose. The estimation process is formulated as a p-norm regression\nproblem in hidden space. By utilizing the differentiation of the solution of\nthis regression problem, the parameters of the whole framework can be trained\nin an end-to-end manner. While most previous works are only applicable to the\nsupervised training and single-shot generation scenario, our method can be\neasily adapted to unsupervised training and multi-shot generation. Extensive\nexperiments on the challenging Market-1501 dataset show that our method yields\ncompetitive performance in all the aforementioned variant scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 17:03:54 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Hu", "Ting-Yao", ""], ["Hauptmann", "Alexander G.", ""]]}, {"id": "2102.10055", "submitter": "Jindong Gu", "authors": "Jindong Gu, Baoyuan Wu, Volker Tresp", "title": "Effective and Efficient Vote Attack on Capsule Networks", "comments": null, "journal-ref": "International Conference on Learning Representations (ICLR), 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard Convolutional Neural Networks (CNNs) can be easily fooled by images\nwith small quasi-imperceptible artificial perturbations. As alternatives to\nCNNs, the recently proposed Capsule Networks (CapsNets) are shown to be more\nrobust to white-box attacks than CNNs under popular attack protocols. Besides,\nthe class-conditional reconstruction part of CapsNets is also used to detect\nadversarial examples. In this work, we investigate the adversarial robustness\nof CapsNets, especially how the inner workings of CapsNets change when the\noutput capsules are attacked. The first observation is that adversarial\nexamples misled CapsNets by manipulating the votes from primary capsules.\nAnother observation is the high computational cost, when we directly apply\nmulti-step attack methods designed for CNNs to attack CapsNets, due to the\ncomputationally expensive routing mechanism. Motivated by these two\nobservations, we propose a novel vote attack where we attack votes of CapsNets\ndirectly. Our vote attack is not only effective but also efficient by\ncircumventing the routing process. Furthermore, we integrate our vote attack\ninto the detection-aware attack paradigm, which can successfully bypass the\nclass-conditional reconstruction based detection method. Extensive experiments\ndemonstrate the superior attack performance of our vote attack on CapsNets.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 17:35:07 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Gu", "Jindong", ""], ["Wu", "Baoyuan", ""], ["Tresp", "Volker", ""]]}, {"id": "2102.10086", "submitter": "Julia Navarro", "authors": "Julia Navarro and Neus Sabater", "title": "Compact and adaptive multiplane images for view synthesis", "comments": "ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, learning methods have been designed to create Multiplane Images\n(MPIs) for view synthesis. While MPIs are extremely powerful and facilitate\nhigh quality renderings, a great amount of memory is required, making them\nimpractical for many applications. In this paper, we propose a learning method\nthat optimizes the available memory to render compact and adaptive MPIs. Our\nMPIs avoid redundant information and take into account the scene geometry to\ndetermine the depth sampling.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 18:39:43 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 13:28:41 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Navarro", "Julia", ""], ["Sabater", "Neus", ""]]}, {"id": "2102.10130", "submitter": "Hamza Ali", "authors": "Abdul Azeem Sikander, Hamza Ali", "title": "Image Classification using CNN for Traffic Signs in Pakistan", "comments": "Image classification, Convolutional Neural networks, Traffic Signs,\n  Pakistan, CNN, Traffic Signs in Pakistan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The autonomous automotive industry is one of the largest and most\nconventional projects worldwide, with many technology companies effectively\ndesigning and orienting their products towards automobile safety and accuracy.\nThese products are performing very well over the roads in developed countries.\nBut can fail in the first minute in an underdeveloped country because there is\nmuch difference between a developed country environment and an underdeveloped\ncountry environment. The following study proposed to train these Artificial\nintelligence models in environment space in an underdeveloped country like\nPakistan. The proposed approach on image classification uses convolutional\nneural networks for image classification for the model. For model pre-training\nGerman traffic signs data set was selected then fine-tuned on Pakistan's\ndataset. The experimental setup showed the best results and accuracy from the\npreviously conducted experiments. In this work to increase the accuracy, more\ndataset was collected to increase the size of images in every class in the data\nset. In the future, a low number of classes are required to be further\nincreased where more images for traffic signs are required to be collected to\nget more accuracy on the training of the model over traffic signs of Pakistan's\nmost used and popular roads motorway and national highway, whose traffic signs\ncolor, size, and shapes are different from common traffic signs.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 19:16:22 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Sikander", "Abdul Azeem", ""], ["Ali", "Hamza", ""]]}, {"id": "2102.10191", "submitter": "Cl\\'ement Playout", "authors": "Cl\\'ement Playout, Ola Ahmad, Freddy Lecue and Farida Cheriet", "title": "Adaptable Deformable Convolutions for Semantic Segmentation of Fisheye\n  Images in Autonomous Driving Systems", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advanced Driver-Assistance Systems rely heavily on perception tasks such as\nsemantic segmentation where images are captured from large field of view (FoV)\ncameras. State-of-the-art works have made considerable progress toward applying\nConvolutional Neural Network (CNN) to standard (rectilinear) images. However,\nthe large FoV cameras used in autonomous vehicles produce fisheye images\ncharacterized by strong geometric distortion. This work demonstrates that a CNN\ntrained on standard images can be readily adapted to fisheye images, which is\ncrucial in real-world applications where time-consuming real-time data\ntransformation must be avoided. Our adaptation protocol mainly relies on\nmodifying the support of the convolutions by using their deformable equivalents\non top of pre-existing layers. We prove that tuning an optimal support only\nrequires a limited amount of labeled fisheye images, as a small number of\ntraining samples is sufficient to significantly improve an existing model's\nperformance on wide-angle images. Furthermore, we show that finetuning the\nweights of the network is not necessary to achieve high performance once the\ndeformable components are learned. Finally, we provide an in-depth analysis of\nthe effect of the deformable convolutions, bringing elements of discussion on\nthe behavior of CNN models.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 22:47:44 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Playout", "Cl\u00e9ment", ""], ["Ahmad", "Ola", ""], ["Lecue", "Freddy", ""], ["Cheriet", "Farida", ""]]}, {"id": "2102.10202", "submitter": "Yuzhuo Ren", "authors": "Yuzhuo Ren, Feng Hu", "title": "Camera Calibration with Pose Guidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera calibration plays a critical role in various computer vision tasks\nsuch as autonomous driving or augmented reality. Widely used camera calibration\ntools utilize plane pattern based methodology, such as using a chessboard or\nAprilTag board, user's calibration expertise level significantly affects\ncalibration accuracy and consistency when without clear instruction.\nFurthermore, calibration is a recurring task that has to be performed each time\nthe camera is changed or moved. It's also a great burden to calibrate huge\namounts of cameras such as Driver Monitoring System (DMS) cameras in a\nproduction line with millions of vehicles. To resolve above issues, we propose\na calibration system called Calibration with Pose Guidance to improve\ncalibration accuracy, reduce calibration variance among different users or\ndifferent trials of the same person. Experiment result shows that our proposed\nmethod achieves more accurate and consistent calibration than traditional\ncalibration tools.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 23:23:54 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Ren", "Yuzhuo", ""], ["Hu", "Feng", ""]]}, {"id": "2102.10212", "submitter": "Athanasios Papadopoulos", "authors": "Athanasios Papadopoulos, Pawe{\\l} Korus, Nasir Memon", "title": "Hard-Attention for Scalable Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep neural networks (DNNs) are typically optimized for a specific input\nresolution (e.g. $224 \\times 224$ px) and their adoption to inputs of higher\nresolution (e.g., satellite or medical images) remains challenging, as it leads\nto excessive computation and memory overhead, and may require substantial\nengineering effort (e.g., streaming). We show that multi-scale hard-attention\ncan be an effective solution to this problem. We propose a novel architecture,\nTNet, which traverses an image pyramid in a top-down fashion, visiting only the\nmost informative regions along the way. We compare our model against strong\nhard-attention baselines, achieving a better trade-off between resources and\naccuracy on ImageNet. We further verify the efficacy of our model on satellite\nimages (fMoW dataset) of size up to $896 \\times 896$ px. In addition, our\nhard-attention mechanism guarantees predictions with a degree of\ninterpretability, without extra cost beyond inference. We also show that we can\nreduce data acquisition and annotation cost, since our model attends only to a\nfraction of the highest resolution content, while using only image-level labels\nwithout bounding boxes.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 00:21:28 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Papadopoulos", "Athanasios", ""], ["Korus", "Pawe\u0142", ""], ["Memon", "Nasir", ""]]}, {"id": "2102.10244", "submitter": "Fangneng Zhan", "authors": "Fangneng Zhan, Yingchen Yu, Rongliang Wu, Changgong Zhang, Shijian Lu,\n  Ling Shao, Feiying Ma, Xuansong Xie", "title": "GMLight: Lighting Estimation via Geometric Distribution Approximation", "comments": "12 pages, 11 figures. arXiv admin note: text overlap with\n  arXiv:2012.11116", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lighting estimation from a single image is an essential yet challenging task\nin computer vision and computer graphics. Existing works estimate lighting by\nregressing representative illumination parameters or generating illumination\nmaps directly. However, these methods often suffer from poor accuracy and\ngeneralization. This paper presents Geometric Mover's Light (GMLight), a\nlighting estimation framework that employs a regression network and a\ngenerative projector for effective illumination estimation. We parameterize\nillumination scenes in terms of the geometric light distribution, light\nintensity, ambient term, and auxiliary depth, and estimate them as a pure\nregression task. Inspired by the earth mover's distance, we design a novel\ngeometric mover's loss to guide the accurate regression of light distribution\nparameters. With the estimated lighting parameters, the generative projector\nsynthesizes panoramic illumination maps with realistic appearance and\nfrequency. Extensive experiments show that GMLight achieves accurate\nillumination estimation and superior fidelity in relighting for 3D object\ninsertion.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 03:31:52 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Zhan", "Fangneng", ""], ["Yu", "Yingchen", ""], ["Wu", "Rongliang", ""], ["Zhang", "Changgong", ""], ["Lu", "Shijian", ""], ["Shao", "Ling", ""], ["Ma", "Feiying", ""], ["Xie", "Xuansong", ""]]}, {"id": "2102.10274", "submitter": "Deng-Ping Fan", "authors": "Deng-Ping Fan, Ge-Peng Ji, Ming-Ming Cheng, Ling Shao", "title": "Concealed Object Detection", "comments": "17 pages, 27 figures, Code: https://github.com/GewelsJI/SINet-V2", "journal-ref": "TPAMI 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the first systematic study on concealed object detection (COD),\nwhich aims to identify objects that are \"perfectly\" embedded in their\nbackground. The high intrinsic similarities between the concealed objects and\ntheir background make COD far more challenging than traditional object\ndetection/segmentation. To better understand this task, we collect a\nlarge-scale dataset, called COD10K, which consists of 10,000 images covering\nconcealed objects in diverse real-world scenarios from 78 object categories.\nFurther, we provide rich annotations including object categories, object\nboundaries, challenging attributes, object-level labels, and instance-level\nannotations. Our COD10K is the largest COD dataset to date, with the richest\nannotations, which enables comprehensive concealed object understanding and can\neven be used to help progress several other vision tasks, such as detection,\nsegmentation, classification, etc. Motivated by how animals hunt in the wild,\nwe also design a simple but strong baseline for COD, termed the Search\nIdentification Network (SINet). Without any bells and whistles, SINet\noutperforms 12 cutting-edge baselines on all datasets tested, making them\nrobust, general architectures that could serve as catalysts for future research\nin COD. Finally, we provide some interesting findings and highlight several\npotential applications and future directions. To spark research in this new\nfield, our code, dataset, and online demo are available on our project page:\nhttp://mmcheng.net/cod.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 06:49:53 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 05:36:12 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Fan", "Deng-Ping", ""], ["Ji", "Ge-Peng", ""], ["Cheng", "Ming-Ming", ""], ["Shao", "Ling", ""]]}, {"id": "2102.10301", "submitter": "Yong Guo", "authors": "Yong Guo, Yin Zheng, Mingkui Tan, Qi Chen, Zhipeng Li, Jian Chen,\n  Peilin Zhao, Junzhou Huang", "title": "Towards Accurate and Compact Architectures via Neural Architecture\n  Transformer", "comments": "Extension of NAT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing effective architectures is one of the key factors behind the\nsuccess of deep neural networks. Existing deep architectures are either\nmanually designed or automatically searched by some Neural Architecture Search\n(NAS) methods. However, even a well-designed/searched architecture may still\ncontain many nonsignificant or redundant modules/operations. Thus, it is\nnecessary to optimize the operations inside an architecture to improve the\nperformance without introducing extra computational cost. To this end, we have\nproposed a Neural Architecture Transformer (NAT) method which casts the\noptimization problem into a Markov Decision Process (MDP) and seeks to replace\nthe redundant operations with more efficient operations, such as skip or null\nconnection. Note that NAT only considers a small number of possible transitions\nand thus comes with a limited search/transition space. As a result, such a\nsmall search space may hamper the performance of architecture optimization. To\naddress this issue, we propose a Neural Architecture Transformer++ (NAT++)\nmethod which further enlarges the set of candidate transitions to improve the\nperformance of architecture optimization. Specifically, we present a two-level\ntransition rule to obtain valid transitions, i.e., allowing operations to have\nmore efficient types (e.g., convolution->separable convolution) or smaller\nkernel sizes (e.g., 5x5->3x3). Note that different operations may have\ndifferent valid transitions. We further propose a Binary-Masked Softmax\n(BMSoftmax) layer to omit the possible invalid transitions. Extensive\nexperiments on several benchmark datasets show that the transformed\narchitecture significantly outperforms both its original counterpart and the\narchitectures optimized by existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 09:38:10 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Guo", "Yong", ""], ["Zheng", "Yin", ""], ["Tan", "Mingkui", ""], ["Chen", "Qi", ""], ["Li", "Zhipeng", ""], ["Chen", "Jian", ""], ["Zhao", "Peilin", ""], ["Huang", "Junzhou", ""]]}, {"id": "2102.10303", "submitter": "Tao Yang", "authors": "Tao Yang, Xuanchi Ren, Yuwang Wang, Wenjun Zeng, Nanning Zheng, Pengju\n  Ren", "title": "GroupifyVAE: from Group-based Definition to VAE-based Unsupervised\n  Representation Disentanglement", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key idea of the state-of-the-art VAE-based unsupervised representation\ndisentanglement methods is to minimize the total correlation of the latent\nvariable distributions. However, it has been proved that VAE-based unsupervised\ndisentanglement can not be achieved without introducing other inductive bias.\nIn this paper, we address VAE-based unsupervised disentanglement by leveraging\nthe constraints derived from the Group Theory based definition as the\nnon-probabilistic inductive bias. More specifically, inspired by the nth\ndihedral group (the permutation group for regular polygons), we propose a\nspecific form of the definition and prove its two equivalent conditions:\nisomorphism and \"the constancy of permutations\". We further provide an\nimplementation of isomorphism based on two Group constraints: the Abel\nconstraint for the exchangeability and Order constraint for the cyclicity. We\nthen convert them into a self-supervised training loss that can be incorporated\ninto VAE-based models to bridge their gaps from the Group Theory based\ndefinition. We train 1800 models covering the most prominent VAE-based models\non five datasets to verify the effectiveness of our method. Compared to the\noriginal models, the Groupidied VAEs consistently achieve better mean\nperformance with smaller variances, and make meaningful dimensions\ncontrollable.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 09:49:51 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Yang", "Tao", ""], ["Ren", "Xuanchi", ""], ["Wang", "Yuwang", ""], ["Zeng", "Wenjun", ""], ["Zheng", "Nanning", ""], ["Ren", "Pengju", ""]]}, {"id": "2102.10335", "submitter": "Nibaran Das", "authors": "Shuvayan Ghosh Dastidar, Kalpita Dutta, Nibaran Das, Mahantapas Kundu\n  and Mita Nasipuri", "title": "Exploring Knowledge Distillation of a Deep Neural Network for\n  Multi-Script identification", "comments": "14 pages, 6 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-lingual script identification is a difficult task consisting of\ndifferent language with complex backgrounds in scene text images. According to\nthe current research scenario, deep neural networks are employed as teacher\nmodels to train a smaller student network by utilizing the teacher model's\npredictions. This process is known as dark knowledge transfer. It has been\nquite successful in many domains where the final result obtained is\nunachievable through directly training the student network with a simple\narchitecture. In this paper, we explore dark knowledge transfer approach using\nlong short-term memory(LSTM) and CNN based assistant model and various deep\nneural networks as the teacher model, with a simple CNN based student network,\nin this domain of multi-script identification from natural scene text images.\nWe explore the performance of different teacher models and their ability to\ntransfer knowledge to a student network. Although the small student network's\nlimited size, our approach obtains satisfactory results on a well-known script\nidentification dataset CVSI-2015.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 12:54:07 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Dastidar", "Shuvayan Ghosh", ""], ["Dutta", "Kalpita", ""], ["Das", "Nibaran", ""], ["Kundu", "Mahantapas", ""], ["Nasipuri", "Mita", ""]]}, {"id": "2102.10338", "submitter": "Haimin Zhang", "authors": "Haimin Zhang, Min Xu, Guoqiang Zhang, and Kenta Niwa", "title": "SSFG: Stochastically Scaling Features and Gradients for Regularizing\n  Graph Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Graph convolutional networks have been successfully applied in various\ngraph-based tasks. In a typical graph convolutional layer, node features are\nupdated by aggregating neighborhood information. Repeatedly applying graph\nconvolutions can cause the oversmoothing issue, i.e., node features at deep\nlayers converge to similar values. Previous studies have suggested that\noversmoothing is one of the major issues that restrict the performance of graph\nconvolutional networks. In this paper, we propose a stochastic regularization\nmethod to tackle the oversmoothing problem. In the proposed method, we\nstochastically scale features and gradients (SSFG) by a factor sampled from a\nprobability distribution in the training procedure. By explicitly applying a\nscaling factor to break feature convergence, the oversmoothing issue is\nalleviated. We show that applying stochastic scaling at the gradient level is\ncomplementary to that applied at the feature level to improve the overall\nperformance. Our method does not increase the number of trainable parameters.\nWhen used together with ReLU, our SSFG can be seen as a stochastic ReLU\nactivation function. We experimentally validate our SSFG regularization method\non three commonly used types of graph networks. Extensive experimental results\non seven benchmark datasets for four graph-based tasks demonstrate that our\nSSFG regularization is effective in improving the overall performance of the\nbaseline graph networks.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 12:59:48 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 12:23:22 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zhang", "Haimin", ""], ["Xu", "Min", ""], ["Zhang", "Guoqiang", ""], ["Niwa", "Kenta", ""]]}, {"id": "2102.10365", "submitter": "Ben Glocker", "authors": "Zeju Li, Konstantinos Kamnitsas, Ben Glocker", "title": "Analyzing Overfitting under Class Imbalance in Neural Networks for Image\n  Segmentation", "comments": "Published in IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": "10.1109/TMI.2020.3046692", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class imbalance poses a challenge for developing unbiased, accurate\npredictive models. In particular, in image segmentation neural networks may\noverfit to the foreground samples from small structures, which are often\nheavily under-represented in the training set, leading to poor generalization.\nIn this study, we provide new insights on the problem of overfitting under\nclass imbalance by inspecting the network behavior. We find empirically that\nwhen training with limited data and strong class imbalance, at test time the\ndistribution of logit activations may shift across the decision boundary, while\nsamples of the well-represented class seem unaffected. This bias leads to a\nsystematic under-segmentation of small structures. This phenomenon is\nconsistently observed for different databases, tasks and network architectures.\nTo tackle this problem, we introduce new asymmetric variants of popular loss\nfunctions and regularization techniques including a large margin loss, focal\nloss, adversarial training, mixup and data augmentation, which are explicitly\ndesigned to counter logit shift of the under-represented classes. Extensive\nexperiments are conducted on several challenging segmentation tasks. Our\nresults demonstrate that the proposed modifications to the objective function\ncan lead to significantly improved segmentation accuracy compared to baselines\nand alternative approaches.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 14:57:58 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Li", "Zeju", ""], ["Kamnitsas", "Konstantinos", ""], ["Glocker", "Ben", ""]]}, {"id": "2102.10369", "submitter": "Tuan Anh Nguyen", "authors": "Anh Nguyen, Anh Tran", "title": "WaNet -- Imperceptible Warping-based Backdoor Attack", "comments": "Accepted to ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the thriving of deep learning and the widespread practice of using\npre-trained networks, backdoor attacks have become an increasing security\nthreat drawing many research interests in recent years. A third-party model can\nbe poisoned in training to work well in normal conditions but behave\nmaliciously when a trigger pattern appears. However, the existing backdoor\nattacks are all built on noise perturbation triggers, making them noticeable to\nhumans. In this paper, we instead propose using warping-based triggers. The\nproposed backdoor outperforms the previous methods in a human inspection test\nby a wide margin, proving its stealthiness. To make such models undetectable by\nmachine defenders, we propose a novel training mode, called the ``noise mode.\nThe trained networks successfully attack and bypass the state-of-the-art\ndefense methods on standard classification datasets, including MNIST, CIFAR-10,\nGTSRB, and CelebA. Behavior analyses show that our backdoors are transparent to\nnetwork inspection, further proving this novel attack mechanism's efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 15:25:36 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 04:08:35 GMT"}, {"version": "v3", "created": "Wed, 24 Feb 2021 15:15:13 GMT"}, {"version": "v4", "created": "Thu, 4 Mar 2021 04:09:38 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Nguyen", "Anh", ""], ["Tran", "Anh", ""]]}, {"id": "2102.10370", "submitter": "Zihan Li", "authors": "Zihan Li, Chen Li, Yudong Yao, Jinghua Zhang, Md Mamunur Rahaman, Hao\n  Xu, Frank Kulwa, Bolin Lu, Xuemin Zhu, Tao Jiang", "title": "EMDS-5: Environmental Microorganism Image Dataset Fifth Version for\n  Multiple Image Analysis Tasks", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0250631", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Environmental Microorganism Data Set Fifth Version (EMDS-5) is a microscopic\nimage dataset including original Environmental Microorganism (EM) images and\ntwo sets of Ground Truth (GT) images. The GT image sets include a single-object\nGT image set and a multi-object GT image set. The EMDS-5 dataset has 21 types\nof EMs, each of which contains 20 original EM images, 20 single-object GT\nimages and 20 multi-object GT images. EMDS-5 can realize to evaluate image\npreprocessing, image segmentation, feature extraction, image classification and\nimage retrieval functions. In order to prove the effectiveness of EMDS-5, for\neach function, we select the most representative algorithms and price\nindicators for testing and evaluation. The image preprocessing functions\ncontain two parts: image denoising and image edge detection. Image denoising\nuses nine kinds of filters to denoise 13 kinds of noises, respectively. In the\naspect of edge detection, six edge detection operators are used to detect the\nedges of the images, and two evaluation indicators, peak-signal to noise ratio\nand mean structural similarity, are used for evaluation. Image segmentation\nincludes single-object image segmentation and multi-object image segmentation.\nSix methods are used for single-object image segmentation, while k-means and\nU-net are used for multi-object segmentation.We extract nine features from the\nimages in EMDS-5 and use the Support Vector Machine classifier for testing. In\nterms of image classification, we select the VGG16 feature to test different\nclassifiers. We test two types of retrieval approaches: texture feature\nretrieval and deep learning feature retrieval. We select the last layer of\nfeatures of these two deep learning networks as feature vectors. We use mean\naverage precision as the evaluation index for retrieval.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 15:27:05 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Li", "Zihan", ""], ["Li", "Chen", ""], ["Yao", "Yudong", ""], ["Zhang", "Jinghua", ""], ["Rahaman", "Md Mamunur", ""], ["Xu", "Hao", ""], ["Kulwa", "Frank", ""], ["Lu", "Bolin", ""], ["Zhu", "Xuemin", ""], ["Jiang", "Tao", ""]]}, {"id": "2102.10377", "submitter": "Chen Yuqian", "authors": "Yuqian Chen, Yang Song, Chaoyi Zhang, Fan Zhang, Lauren O'Donnell,\n  Wojciech Chrzanowski, Weidong Cai", "title": "CellTrack R-CNN: A Novel End-To-End Deep Neural Network for Cell\n  Segmentation and Tracking in Microscopy Images", "comments": "4 pages,3 figures, to be published in The IEEE International\n  Symposium on Biomedical Imaging (ISBI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cell segmentation and tracking in microscopy images are of great significance\nto new discoveries in biology and medicine. In this study, we propose a novel\napproach to combine cell segmentation and cell tracking into a unified\nend-to-end deep learning based framework, where cell detection and segmentation\nare performed with a current instance segmentation pipeline and cell tracking\nis implemented by integrating Siamese Network with the pipeline. Besides,\ntracking performance is improved by incorporating spatial information into the\nnetwork and fusing spatial and visual prediction. Our approach was evaluated on\nthe DeepCell benchmark dataset. Despite being simple and efficient, our method\noutperforms state-of-the-art algorithms in terms of both cell segmentation and\ncell tracking accuracies.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 15:55:40 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Chen", "Yuqian", ""], ["Song", "Yang", ""], ["Zhang", "Chaoyi", ""], ["Zhang", "Fan", ""], ["O'Donnell", "Lauren", ""], ["Chrzanowski", "Wojciech", ""], ["Cai", "Weidong", ""]]}, {"id": "2102.10378", "submitter": "Quang Vu Duc", "authors": "Duc Quang Vu, Ngan T.H.Le and Jia-Ching Wang", "title": "Self-Supervised Learning via multi-Transformation Classification for\n  Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised tasks have been utilized to build useful representations that\ncan be used in downstream tasks when the annotation is unavailable. In this\npaper, we introduce a self-supervised video representation learning method\nbased on the multi-transformation classification to efficiently classify human\nactions. Self-supervised learning on various transformations not only provides\nricher contextual information but also enables the visual representation more\nrobust to the transforms. The spatio-temporal representation of the video is\nlearned in a self-supervised manner by classifying seven different\ntransformations i.e. rotation, clip inversion, permutation, split, join\ntransformation, color switch, frame replacement, noise addition. First, seven\ndifferent video transformations are applied to video clips. Then the 3D\nconvolutional neural networks are utilized to extract features for clips and\nthese features are processed to classify the pseudo-labels. We use the learned\nmodels in pretext tasks as the pre-trained models and fine-tune them to\nrecognize human actions in the downstream task. We have conducted the\nexperiments on UCF101 and HMDB51 datasets together with C3D and 3D Resnet-18 as\nbackbone networks. The experimental results have shown that our proposed\nframework is outperformed other SOTA self-supervised action recognition\napproaches. The code will be made publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 16:11:26 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Vu", "Duc Quang", ""], ["Le", "Ngan T. H.", ""], ["Wang", "Jia-Ching", ""]]}, {"id": "2102.10407", "submitter": "Jun Chen", "authors": "Jun Chen, Han Guo, Kai Yi, Boyang Li, Mohamed Elhoseiny", "title": "VisualGPT: Data-efficient Adaptation of Pretrained Language Models for\n  Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The ability to quickly learn from a small quantity oftraining data widens the\nrange of machine learning applications. In this paper, we propose a\ndata-efficient image captioning model, VisualGPT, which leverages the\nlinguistic knowledge from a large pretrained language model(LM). A crucial\nchallenge is to balance between the use of visual information in the image and\nprior linguistic knowledge acquired from pretraining. We designed a novel\nself-resurrecting encoder-decoder attention mechanism to quickly adapt the\npretrained LM as the language decoder ona small amount of in-domain training\ndata. The proposed self-resurrecting activation unit produces sparse\nactivations but has reduced susceptibility to zero gradients. We train the\nproposed model, VisualGPT, on 0.1%, 0.5% and 1% of MSCOCO and Conceptual\nCaptions training data. Under these conditions, we outperform the best baseline\nmodel by up to 10.8% CIDEr on MS COCO and upto 5.4% CIDEr on Conceptual\nCaptions. Further, Visual-GPT achieves the state-of-the-art result on IU X-ray,\na medical report generation dataset. To the best of our knowledge, this is the\nfirst work that improves data efficiency of image captioning by utilizing LM\npretrained on unimodal data. Our code is available at:\nhttps://github.com/Vision-CAIR/VisualGPT.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 18:02:42 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 18:03:11 GMT"}, {"version": "v3", "created": "Sat, 17 Apr 2021 07:14:40 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chen", "Jun", ""], ["Guo", "Han", ""], ["Yi", "Kai", ""], ["Li", "Boyang", ""], ["Elhoseiny", "Mohamed", ""]]}, {"id": "2102.10438", "submitter": "Radu Tudor Ionescu", "authors": "Mihail Burduja, Radu Tudor Ionescu", "title": "Unsupervised Medical Image Alignment with Curriculum Learning", "comments": "Accepted at ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore different curriculum learning methods for training convolutional\nneural networks on the task of deformable pairwise 3D medical image\nregistration. To the best of our knowledge, we are the first to attempt to\nimprove performance by training medical image registration models using\ncurriculum learning, starting from an easy training setup in the first training\nstages, and gradually increasing the complexity of the setup. On the one hand,\nwe consider two existing curriculum learning approaches, namely curriculum\ndropout and curriculum by smoothing. On the other hand, we propose a novel and\nsimple strategy to achieve curriculum, namely to use purposely blurred images\nat the beginning, then gradually transit to sharper images in the later\ntraining stages. Our experiments with an underlying state-of-the-art deep\nlearning model show that curriculum learning can lead to superior results\ncompared to conventional training. Additionally, we show that curriculum by\ninput blur has the best accuracy versus speed trade-off among the compared\ncurriculum learning approaches.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 20:26:01 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 17:17:00 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Burduja", "Mihail", ""], ["Ionescu", "Radu Tudor", ""]]}, {"id": "2102.10446", "submitter": "Andrei Iantsen", "authors": "Andrei Iantsen, Dimitris Visvikis, Mathieu Hatt", "title": "Squeeze-and-Excitation Normalization for Automated Delineation of Head\n  and Neck Primary Tumors in Combined PET and CT Images", "comments": "7 pages, 2 figures, 2 tables", "journal-ref": "In: Andrearczyk V., Oreiller V., Depeursinge A. (eds) Head and\n  Neck Tumor Segmentation. HECKTOR 2020. Lecture Notes in Computer Science, vol\n  12603. Springer, Cham", "doi": "10.1007/978-3-030-67194-5_4", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Development of robust and accurate fully automated methods for medical image\nsegmentation is crucial in clinical practice and radiomics studies. In this\nwork, we contributed an automated approach for Head and Neck (H&N) primary\ntumor segmentation in combined positron emission tomography / computed\ntomography (PET/CT) images in the context of the MICCAI 2020 Head and Neck\nTumor segmentation challenge (HECKTOR). Our model was designed on the U-Net\narchitecture with residual layers and supplemented with Squeeze-and-Excitation\nNormalization. The described method achieved competitive results in\ncross-validation (DSC 0.745, precision 0.760, recall 0.789) performed on\ndifferent centers, as well as on the test set (DSC 0.759, precision 0.833,\nrecall 0.740) that allowed us to win first prize in the HECKTOR challenge among\n21 participating teams. The full implementation based on PyTorch and the\ntrained models are available at https://github.com/iantsen/hecktor\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 21:06:59 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Iantsen", "Andrei", ""], ["Visvikis", "Dimitris", ""], ["Hatt", "Mathieu", ""]]}, {"id": "2102.10454", "submitter": "Ren Wang", "authors": "Ren Wang, Kaidi Xu, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Chuang Gan,\n  Meng Wang", "title": "On Fast Adversarial Robustness Adaptation in Model-Agnostic\n  Meta-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-agnostic meta-learning (MAML) has emerged as one of the most successful\nmeta-learning techniques in few-shot learning. It enables us to learn a\nmeta-initialization} of model parameters (that we call meta-model) to rapidly\nadapt to new tasks using a small amount of labeled training data. Despite the\ngeneralization power of the meta-model, it remains elusive that how adversarial\nrobustness can be maintained by MAML in few-shot learning. In addition to\ngeneralization, robustness is also desired for a meta-model to defend\nadversarial examples (attacks). Toward promoting adversarial robustness in\nMAML, we first study WHEN a robustness-promoting regularization should be\nincorporated, given the fact that MAML adopts a bi-level (fine-tuning vs.\nmeta-update) learning procedure. We show that robustifying the meta-update\nstage is sufficient to make robustness adapted to the task-specific fine-tuning\nstage even if the latter uses a standard training protocol. We also make\nadditional justification on the acquired robustness adaptation by peering into\nthe interpretability of neurons' activation maps. Furthermore, we investigate\nHOW robust regularization can efficiently be designed in MAML. We propose a\ngeneral but easily-optimized robustness-regularized meta-learning framework,\nwhich allows the use of unlabeled data augmentation, fast adversarial attack\ngeneration, and computationally-light fine-tuning. In particular, we for the\nfirst time show that the auxiliary contrastive learning task can enhance the\nadversarial robustness of MAML. Finally, extensive experiments are conducted to\ndemonstrate the effectiveness of our proposed methods in robust few-shot\nlearning.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 22:03:04 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Wang", "Ren", ""], ["Xu", "Kaidi", ""], ["Liu", "Sijia", ""], ["Chen", "Pin-Yu", ""], ["Weng", "Tsui-Wei", ""], ["Gan", "Chuang", ""], ["Wang", "Meng", ""]]}, {"id": "2102.10462", "submitter": "Huanrui Yang", "authors": "Huanrui Yang, Lin Duan, Yiran Chen, Hai Li", "title": "BSQ: Exploring Bit-Level Sparsity for Mixed-Precision Neural Network\n  Quantization", "comments": "Published as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mixed-precision quantization can potentially achieve the optimal tradeoff\nbetween performance and compression rate of deep neural networks, and thus,\nhave been widely investigated. However, it lacks a systematic method to\ndetermine the exact quantization scheme. Previous methods either examine only a\nsmall manually-designed search space or utilize a cumbersome neural\narchitecture search to explore the vast search space. These approaches cannot\nlead to an optimal quantization scheme efficiently. This work proposes\nbit-level sparsity quantization (BSQ) to tackle the mixed-precision\nquantization from a new angle of inducing bit-level sparsity. We consider each\nbit of quantized weights as an independent trainable variable and introduce a\ndifferentiable bit-sparsity regularizer. BSQ can induce all-zero bits across a\ngroup of weight elements and realize the dynamic precision reduction, leading\nto a mixed-precision quantization scheme of the original model. Our method\nenables the exploration of the full mixed-precision space with a single\ngradient-based optimization process, with only one hyperparameter to tradeoff\nthe performance and compression. BSQ achieves both higher accuracy and higher\nbit reduction on various model architectures on the CIFAR-10 and ImageNet\ndatasets comparing to previous methods.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 22:37:41 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Yang", "Huanrui", ""], ["Duan", "Lin", ""], ["Chen", "Yiran", ""], ["Li", "Hai", ""]]}, {"id": "2102.10472", "submitter": "Mitchell Wortsman", "authors": "Mitchell Wortsman, Maxwell Horton, Carlos Guestrin, Ali Farhadi,\n  Mohammad Rastegari", "title": "Learning Neural Network Subspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent observations have advanced our understanding of the neural network\noptimization landscape, revealing the existence of (1) paths of high accuracy\ncontaining diverse solutions and (2) wider minima offering improved\nperformance. Previous methods observing diverse paths require multiple training\nruns. In contrast we aim to leverage both property (1) and (2) with a single\nmethod and in a single training run. With a similar computational cost as\ntraining one model, we learn lines, curves, and simplexes of high-accuracy\nneural networks. These neural network subspaces contain diverse solutions that\ncan be ensembled, approaching the ensemble performance of independently trained\nnetworks without the training cost. Moreover, using the subspace midpoint\nboosts accuracy, calibration, and robustness to label noise, outperforming\nStochastic Weight Averaging.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 23:26:58 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 05:32:14 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Wortsman", "Mitchell", ""], ["Horton", "Maxwell", ""], ["Guestrin", "Carlos", ""], ["Farhadi", "Ali", ""], ["Rastegari", "Mohammad", ""]]}, {"id": "2102.10480", "submitter": "Chen Zhao", "authors": "Fubao Zhu, Zhengyuan Gao, Chen Zhao, Hanlei Zhu, Yong Dong, Jingfeng\n  Jiang, Neng Dai, Weihua Zhou", "title": "A Deep Learning-based Method to Extract Lumen and Media-Adventitia in\n  Intravascular Ultrasound Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intravascular ultrasound (IVUS) imaging allows direct visualization of the\ncoronary vessel wall and is suitable for the assessment of atherosclerosis and\nthe degree of stenosis. Accurate segmentation and measurements of lumen and\nmedian-adventitia (MA) from IVUS are essential for such a successful clinical\nevaluation. However, current segmentation relies on manual operations, which is\ntime-consuming and user-dependent. In this paper, we aim to develop a deep\nlearning-based method using an encoder-decoder deep architecture to\nautomatically extract both lumen and MA border. Our method named IVUS-U-Net++\nis an extension of the well-known U-Net++ model. More specifically, a feature\npyramid network was added to the U-Net++ model, enabling the utilization of\nfeature maps at different scales. As a result, the accuracy of the probability\nmap and subsequent segmentation have been improved We collected 1746 IVUS\nimages from 18 patients in this study. The whole dataset was split into a\ntraining dataset (1572 images) for the 10-fold cross-validation and a test\ndataset (174 images) for evaluating the performance of models. Our IVUS-U-Net++\nsegmentation model achieved a Jaccard measure (JM) of 0.9412, a Hausdorff\ndistance (HD) of 0.0639 mm for the lumen border, and a JM of 0.9509, an HD of\n0.0867 mm for the MA border, respectively. Moreover, the Pearson correlation\nand Bland-Altman analyses were performed to evaluate the correlations of 12\nclinical parameters measured from our segmentation results and the ground\ntruth, and automatic measurements agreed well with those from the ground truth\n(all Ps<0.01). In conclusion, our preliminary results demonstrate that the\nproposed IVUS-U-Net++ model has great promise for clinical use.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 00:10:05 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Zhu", "Fubao", ""], ["Gao", "Zhengyuan", ""], ["Zhao", "Chen", ""], ["Zhu", "Hanlei", ""], ["Dong", "Yong", ""], ["Jiang", "Jingfeng", ""], ["Dai", "Neng", ""], ["Zhou", "Weihua", ""]]}, {"id": "2102.10484", "submitter": "Pranav Rajpurkar", "authors": "Soham Gadgil, Mark Endo, Emily Wen, Andrew Y. Ng, Pranav Rajpurkar", "title": "CheXseg: Combining Expert Annotations with DNN-generated Saliency Maps\n  for X-ray Segmentation", "comments": "Accepted to Medical Imaging with Deep Learning (MIDL) Conference 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical image segmentation models are typically supervised by expert\nannotations at the pixel-level, which can be expensive to acquire. In this\nwork, we propose a method that combines the high quality of pixel-level expert\nannotations with the scale of coarse DNN-generated saliency maps for training\nmulti-label semantic segmentation models. We demonstrate the application of our\nsemi-supervised method, which we call CheXseg, on multi-label chest X-ray\ninterpretation. We find that CheXseg improves upon the performance (mIoU) of\nfully-supervised methods that use only pixel-level expert annotations by 9.7%\nand weakly-supervised methods that use only DNN-generated saliency maps by\n73.1%. Our best method is able to match radiologist agreement on three out of\nten pathologies and reduces the overall performance gap by 57.2% as compared to\nweakly-supervised methods.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 00:47:30 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 07:02:56 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Gadgil", "Soham", ""], ["Endo", "Mark", ""], ["Wen", "Emily", ""], ["Ng", "Andrew Y.", ""], ["Rajpurkar", "Pranav", ""]]}, {"id": "2102.10485", "submitter": "Massimiliano Lupo Pasini Dr.", "authors": "Massimiliano Lupo Pasini, Vittorio Gabbi, Junqi Yin, Simona Perotto,\n  Nouamane Laanait", "title": "Scalable Balanced Training of Conditional Generative Adversarial Neural\n  Networks on Image Data", "comments": null, "journal-ref": "Journal of Supercomputing, 2021", "doi": "10.1007/s11227-021-03808-2", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a distributed approach to train deep convolutional generative\nadversarial neural network (DC-CGANs) models. Our method reduces the imbalance\nbetween generator and discriminator by partitioning the training data according\nto data labels, and enhances scalability by performing a parallel training\nwhere multiple generators are concurrently trained, each one of them focusing\non a single data label. Performance is assessed in terms of inception score and\nimage quality on MNIST, CIFAR10, CIFAR100, and ImageNet1k datasets, showing a\nsignificant improvement in comparison to state-of-the-art techniques to\ntraining DC-CGANs. Weak scaling is attained on all the four datasets using up\nto 1,000 processes and 2,000 NVIDIA V100 GPUs on the OLCF supercomputer Summit.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 00:48:19 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Pasini", "Massimiliano Lupo", ""], ["Gabbi", "Vittorio", ""], ["Yin", "Junqi", ""], ["Perotto", "Simona", ""], ["Laanait", "Nouamane", ""]]}, {"id": "2102.10490", "submitter": "Junru Wu", "authors": "Junru Wu, Xiyang Dai, Dongdong Chen, Yinpeng Chen, Mengchen Liu, Ye\n  Yu, Zhangyang Wang, Zicheng Liu, Mei Chen, Lu Yuan", "title": "Stronger NAS with Weaker Predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural Architecture Search (NAS) often trains and evaluates a large number of\narchitectures. Recent predictor-based NAS approaches attempt to address such\nheavy computation costs with two key steps: sampling some\narchitecture-performance pairs and fitting a proxy accuracy predictor. Given\nlimited samples, these predictors, however, are far from accurate to locate top\narchitectures due to the difficulty of fitting the huge search space. This\npaper reflects on a simple yet crucial question: if our final goal is to find\nthe best architecture, do we really need to model the whole space well?. We\npropose a paradigm shift from fitting the whole architecture space using one\nstrong predictor, to progressively fitting a search path towards the\nhigh-performance sub-space through a set of weaker predictors. As a key\nproperty of the proposed weak predictors, their probabilities of sampling\nbetter architectures keep increasing. Hence we only sample a few well-performed\narchitectures guided by the previously learned predictor and estimate a new\nbetter weak predictor. This embarrassingly easy framework produces\ncoarse-to-fine iteration to refine the ranking of sampling space gradually.\nExtensive experiments demonstrate that our method costs fewer samples to find\ntop-performance architectures on NAS-Bench-101 and NAS-Bench-201, as well as\nachieves the state-of-the-art ImageNet performance on the NASNet search space.\nIn particular, compared to state-of-the-art (SOTA) predictor-based NAS methods,\nWeakNAS outperforms all of them with notable margins, e.g., requiring at least\n7.5x less samples to find global optimal on NAS-Bench-101; and WeakNAS can also\nabsorb them for further performance boost. We further strike the new SOTA\nresult of 81.3% in the ImageNet MobileNet Search Space. The code is available\nat https://github.com/VITA-Group/WeakNAS.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 01:58:43 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 04:04:59 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Wu", "Junru", ""], ["Dai", "Xiyang", ""], ["Chen", "Dongdong", ""], ["Chen", "Yinpeng", ""], ["Liu", "Mengchen", ""], ["Yu", "Ye", ""], ["Wang", "Zhangyang", ""], ["Liu", "Zicheng", ""], ["Chen", "Mei", ""], ["Yuan", "Lu", ""]]}, {"id": "2102.10493", "submitter": "Shireen Elhabian", "authors": "Praful Agrawal, Ross T. Whitaker, Shireen Y. Elhabian", "title": "Learning Deep Features for Shape Correspondence with Domain Invariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correspondence-based shape models are key to various medical imaging\napplications that rely on a statistical analysis of anatomies. Such shape\nmodels are expected to represent consistent anatomical features across the\npopulation for population-specific shape statistics. Early approaches for\ncorrespondence placement rely on nearest neighbor search for simpler anatomies.\nCoordinate transformations for shape correspondence hold promise to address the\nincreasing anatomical complexities. Nonetheless, due to the inherent\nshape-level geometric complexity and population-level shape variation, the\ncoordinate-wise correspondence often does not translate to the anatomical\ncorrespondence. An alternative, group-wise approach for correspondence\nplacement explicitly models the trade-off between geometric description and the\npopulation's statistical compactness. However, these models achieve limited\nsuccess in resolving nonlinear shape correspondence. Recent works have\naddressed this limitation by adopting an application-specific notion of\ncorrespondence through lifting positional data to a higher dimensional feature\nspace. However, they heavily rely on manual expertise to create domain-specific\nfeatures and consistent landmarks. This paper proposes an automated feature\nlearning approach, using deep convolutional neural networks to extract\ncorrespondence-friendly features from shape ensembles. Further, an unsupervised\ndomain adaptation scheme is introduced to augment the pretrained geometric\nfeatures with new anatomies. Results on anatomical datasets of human scapula,\nfemur, and pelvis bones demonstrate that features learned in supervised fashion\nshow improved performance for correspondence estimation compared to the manual\nfeatures. Further, unsupervised learning is demonstrated to learn complex\nanatomy features using the supervised domain adaptation from features learned\non simpler anatomy.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 02:25:32 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Agrawal", "Praful", ""], ["Whitaker", "Ross T.", ""], ["Elhabian", "Shireen Y.", ""]]}, {"id": "2102.10499", "submitter": "Yixin Li", "authors": "Yixin Li, Xinran Wu, Chen Li, Changhao Sun, Md Rahaman, Haoyuan Chen,\n  Yudong Yao, Xiaoyan Li, Yong Zhang, Tao Jiang", "title": "A Hierarchical Conditional Random Field-based Attention Mechanism\n  Approach for Gastric Histopathology Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Gastric Histopathology Image Classification (GHIC) tasks, which are\nusually weakly supervised learning missions, there is inevitably redundant\ninformation in the images. Therefore, designing networks that can focus on\neffective distinguishing features has become a popular research topic. In this\npaper, to accomplish the tasks of GHIC superiorly and to assist pathologists in\nclinical diagnosis, an intelligent Hierarchical Conditional Random Field based\nAttention Mechanism (HCRF-AM) model is proposed. The HCRF-AM model consists of\nan Attention Mechanism (AM) module and an Image Classification (IC) module. In\nthe AM module, an HCRF model is built to extract attention regions. In the IC\nmodule, a Convolutional Neural Network (CNN) model is trained with the\nattention regions selected and then an algorithm called Classification\nProbability-based Ensemble Learning is applied to obtain the image-level\nresults from patch-level output of the CNN. In the experiment, a classification\nspecificity of 96.67% is achieved on a gastric histopathology dataset with 700\nimages. Our HCRF-AM model demonstrates high classification performance and\nshows its effectiveness and future potential in the GHIC field.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 03:38:51 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 01:23:18 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Li", "Yixin", ""], ["Wu", "Xinran", ""], ["Li", "Chen", ""], ["Sun", "Changhao", ""], ["Rahaman", "Md", ""], ["Chen", "Haoyuan", ""], ["Yao", "Yudong", ""], ["Li", "Xiaoyan", ""], ["Zhang", "Yong", ""], ["Jiang", "Tao", ""]]}, {"id": "2102.10503", "submitter": "Qunxi Dong Dr.", "authors": "J. Zhang, Q. Dong, J. Shi, Q. Li, C.M. Stonnington, B.A. Gutman, K.\n  Chen, E.M. Reiman, R.J. Caselli, P.M. Thompson, J. Ye, Y. Wang", "title": "Predicting Future Cognitive Decline with Hyperbolic Stochastic Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Hyperbolic geometry has been successfully applied in modeling brain cortical\nand subcortical surfaces with general topological structures. However such\napproaches, similar to other surface based brain morphology analysis methods,\nusually generate high dimensional features. It limits their statistical power\nin cognitive decline prediction research, especially in datasets with limited\nsubject numbers. To address the above limitation, we propose a novel framework\ntermed as hyperbolic stochastic coding (HSC). Our preliminary experimental\nresults show that our algorithm achieves superior results on various\nclassification tasks. Our work may enrich surface based brain imaging research\ntools and potentially result in a diagnostic and prognostic indicator to be\nuseful in individualized treatment strategies.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 04:07:27 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Zhang", "J.", ""], ["Dong", "Q.", ""], ["Shi", "J.", ""], ["Li", "Q.", ""], ["Stonnington", "C. M.", ""], ["Gutman", "B. A.", ""], ["Chen", "K.", ""], ["Reiman", "E. M.", ""], ["Caselli", "R. J.", ""], ["Thompson", "P. M.", ""], ["Ye", "J.", ""], ["Wang", "Y.", ""]]}, {"id": "2102.10514", "submitter": "Yudong Liang", "authors": "Yudong Liang, Bin Wang, Jiaying Liu, Deyu Li, Sanping Zhou and Wenqi\n  Ren", "title": "Progressive Depth Learning for Single Image Dehazing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The formulation of the hazy image is mainly dominated by the reflected lights\nand ambient airlight. Existing dehazing methods often ignore the depth cues and\nfail in distant areas where heavier haze disturbs the visibility. However, we\nnote that the guidance of the depth information for transmission estimation\ncould remedy the decreased visibility as distances increase. In turn, the good\ntransmission estimation could facilitate the depth estimation for hazy images.\nIn this paper, a deep end-to-end model that iteratively estimates image depths\nand transmission maps is proposed to perform an effective depth prediction for\nhazy images and improve the dehazing performance with the guidance of depth\ninformation. The image depth and transmission map are progressively refined to\nbetter restore the dehazed image. Our approach benefits from explicitly\nmodeling the inner relationship of image depth and transmission map, which is\nespecially effective for distant hazy areas. Extensive results on the\nbenchmarks demonstrate that our proposed network performs favorably against the\nstate-of-the-art dehazing methods in terms of depth estimation and haze\nremoval.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 05:24:18 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Liang", "Yudong", ""], ["Wang", "Bin", ""], ["Liu", "Jiaying", ""], ["Li", "Deyu", ""], ["Zhou", "Sanping", ""], ["Ren", "Wenqi", ""]]}, {"id": "2102.10526", "submitter": "Yu Fu", "authors": "Yu Fu, Xiao-Jun Wu, Josef Kittler", "title": "A Deep Decomposition Network for Image Processing: A Case Study for\n  Visible and Infrared Image Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Image decomposition is a crucial subject in the field of image processing. It\ncan extract salient features from the source image. We propose a new image\ndecomposition method based on convolutional neural network. This method can be\napplied to many image processing tasks. In this paper, we apply the image\ndecomposition network to the image fusion task. We input infrared image and\nvisible light image and decompose them into three high-frequency feature images\nand a low-frequency feature image respectively. The two sets of feature images\nare fused using a specific fusion strategy to obtain fusion feature images.\nFinally, the feature images are reconstructed to obtain the fused image.\nCompared with the state-of-the-art fusion methods, this method has achieved\nbetter performance in both subjective and objective evaluation.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 06:34:33 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Fu", "Yu", ""], ["Wu", "Xiao-Jun", ""], ["Kittler", "Josef", ""]]}, {"id": "2102.10534", "submitter": "Owen Kunhardt", "authors": "Owen Kunhardt, Arturo Deza, Tomaso Poggio", "title": "The Effects of Image Distribution and Task on Adversarial Robustness", "comments": "Under review at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an adaptation to the area under the curve (AUC)\nmetric to measure the adversarial robustness of a model over a particular\n$\\epsilon$-interval $[\\epsilon_0, \\epsilon_1]$ (interval of adversarial\nperturbation strengths) that facilitates unbiased comparisons across models\nwhen they have different initial $\\epsilon_0$ performance. This can be used to\ndetermine how adversarially robust a model is to different image distributions\nor task (or some other variable); and/or to measure how robust a model is\ncomparatively to other models. We used this adversarial robustness metric on\nmodels of an MNIST, CIFAR-10, and a Fusion dataset (CIFAR-10 + MNIST) where\ntrained models performed either a digit or object recognition task using a\nLeNet, ResNet50, or a fully connected network (FullyConnectedNet) architecture\nand found the following: 1) CIFAR-10 models are inherently less adversarially\nrobust than MNIST models; 2) Both the image distribution and task that a model\nis trained on can affect the adversarial robustness of the resultant model. 3)\nPretraining with a different image distribution and task sometimes carries over\nthe adversarial robustness induced by that image distribution and task in the\nresultant model; Collectively, our results imply non-trivial differences of the\nlearned representation space of one perceptual system over another given its\nexposure to different image statistics or tasks (mainly objects vs digits).\nMoreover, these results hold even when model systems are equalized to have the\nsame level of performance, or when exposed to approximately matched image\nstatistics of fusion images but with different tasks.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 07:15:50 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Kunhardt", "Owen", ""], ["Deza", "Arturo", ""], ["Poggio", "Tomaso", ""]]}, {"id": "2102.10543", "submitter": "Xuanchi Ren", "authors": "Xuanchi Ren, Tao Yang, Yuwang Wang, Wenjun Zeng", "title": "Do Generative Models Know Disentanglement? Contrastive Learning is All\n  You Need", "comments": "Project Page: https://github.com/xrenaa/DisCo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disentangled generative models are typically trained with an extra\nregularization term, which encourages the traversal of each latent factor to\nmake a distinct and independent change at the cost of generation quality. When\ntraversing the latent space of generative models trained without the\ndisentanglement term, the generated samples show semantically meaningful\nchange, raising the question: do generative models know disentanglement? We\npropose an unsupervised and model-agnostic method: Disentanglement via Contrast\n(DisCo) in the Variation Space. DisCo consists of: (i) a Navigator providing\ntraversal directions in the latent space, and (ii) a $\\Delta$-Contrastor\ncomposed of two shared-weight Encoders, which encode image pairs along these\ndirections to disentangled representations respectively, and a difference\noperator to map the encoded representations to the Variation Space. We propose\ntwo more key techniques for DisCo: entropy-based domination loss to make the\nencoded representations more disentangled and the strategy of flipping hard\nnegatives to address directions with the same semantic meaning. By optimizing\nthe Navigator to discover disentangled directions in the latent space and\nEncoders to extract disentangled representations from images with Contrastive\nLearning, DisCo achieves the state-of-the-art disentanglement given pretrained\nnon-disentangled generative models, including GAN, VAE, and Flow. Project page\nat https://github.com/xrenaa/DisCo.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 08:01:20 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Ren", "Xuanchi", ""], ["Yang", "Tao", ""], ["Wang", "Yuwang", ""], ["Zeng", "Wenjun", ""]]}, {"id": "2102.10544", "submitter": "Xuanchi Ren", "authors": "Xuanchi Ren, Tao Yang, Yuwang Wang, Wenjun Zeng", "title": "Rethinking Content and Style: Exploring Bias for Unsupervised\n  Disentanglement", "comments": "Project Page: https://github.com/xrenaa/CS-DisMo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content and style (C-S) disentanglement intends to decompose the underlying\nexplanatory factors of objects into two independent subspaces. From the\nunsupervised disentanglement perspective, we rethink content and style and\npropose a formulation for unsupervised C-S disentanglement based on our\nassumption that different factors are of different importance and popularity\nfor image reconstruction, which serves as a data bias. The corresponding model\ninductive bias is introduced by our proposed C-S disentanglement Module (C-S\nDisMo), which assigns different and independent roles to content and style when\napproximating the real data distributions. Specifically, each content embedding\nfrom the dataset, which encodes the most dominant factors for image\nreconstruction, is assumed to be sampled from a shared distribution across the\ndataset. The style embedding for a particular image, encoding the remaining\nfactors, is used to customize the shared distribution through an affine\ntransformation. The experiments on several popular datasets demonstrate that\nour method achieves the state-of-the-art unsupervised C-S disentanglement,\nwhich is comparable or even better than supervised methods. We verify the\neffectiveness of our method by downstream tasks: domain translation and\nsingle-view 3D reconstruction. Project page at\nhttps://github.com/xrenaa/CS-DisMo.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 08:04:33 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Ren", "Xuanchi", ""], ["Yang", "Tao", ""], ["Wang", "Yuwang", ""], ["Zeng", "Wenjun", ""]]}, {"id": "2102.10553", "submitter": "Xintong Li", "authors": "Chen Li, Xintong Li, Md Rahaman, Xiaoyan Li, Hongzan Sun, Hong Zhang,\n  Yong Zhang, Xiaoqi Li, Jian Wu, Yudong Yao, Marcin Grzegorzek", "title": "A Comprehensive Review of Computer-aided Whole-slide Image Analysis:\n  from Datasets to Feature Extraction, Segmentation, Classification, and\n  Detection Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the development of computer-aided diagnosis (CAD) and image scanning\ntechnology, Whole-slide Image (WSI) scanners are widely used in the field of\npathological diagnosis. Therefore, WSI analysis has become the key to modern\ndigital pathology. Since 2004, WSI has been used more and more in CAD. Since\nmachine vision methods are usually based on semi-automatic or fully automatic\ncomputers, they are highly efficient and labor-saving. The combination of WSI\nand CAD technologies for segmentation, classification, and detection helps\nhistopathologists obtain more stable and quantitative analysis results, save\nlabor costs and improve diagnosis objectivity. This paper reviews the methods\nof WSI analysis based on machine learning. Firstly, the development status of\nWSI and CAD methods are introduced. Secondly, we discuss publicly available WSI\ndatasets and evaluation metrics for segmentation, classification, and detection\ntasks. Then, the latest development of machine learning in WSI segmentation,\nclassification, and detection are reviewed continuously. Finally, the existing\nmethods are studied, the applicabilities of the analysis methods are analyzed,\nand the application prospects of the analysis methods in this field are\nforecasted.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 08:30:48 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Li", "Chen", ""], ["Li", "Xintong", ""], ["Rahaman", "Md", ""], ["Li", "Xiaoyan", ""], ["Sun", "Hongzan", ""], ["Zhang", "Hong", ""], ["Zhang", "Yong", ""], ["Li", "Xiaoqi", ""], ["Wu", "Jian", ""], ["Yao", "Yudong", ""], ["Grzegorzek", "Marcin", ""]]}, {"id": "2102.10555", "submitter": "Md Shafkat Rahman Farabi", "authors": "Shafkat Farabi, Hasibul Haque Himel, Fakhruddin Gazzali, Bakhtiar\n  Hasan, Md. Hasanul Kabir, Moshiur Farazi", "title": "Improving Action Quality Assessment using ResNets and Weighted\n  Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action quality assessment (AQA) aims at automatically judging human action\nbased on a video of the said action and assigning a performance score to it.\nThe majority of works in the existing literature on AQA transform RGB videos to\nhigher-level representations using C3D networks. These higher-level\nrepresentations are used to perform action quality assessment. Due to the\nrelatively shallow nature of C3D, the quality of extracted features is lower\nthan what could be extracted using a deeper convolutional neural network. In\nthis paper, we experiment with deeper convolutional neural networks with\nresidual connections for learning representations for action quality\nassessment. We assess the effects of the depth and the input clip size of the\nconvolutional neural network on the quality of action score predictions. We\nalso look at the effect of using (2+1)D convolutions instead of 3D convolutions\nfor feature extraction. We find that the current clip level feature\nrepresentation aggregation technique of averaging is insufficient to capture\nthe relative importance of features. To overcome this, we propose a\nlearning-based weighted-averaging technique that can perform better. We achieve\na new state-of-the-art Spearman's rank correlation of 0.9315 (an increase of\n0.45%) on the MTL-AQA dataset using a 34 layer (2+1)D convolutional neural\nnetwork with the capability of processing 32 frame clips, using our proposed\naggregation technique.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 08:36:22 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Farabi", "Shafkat", ""], ["Himel", "Hasibul Haque", ""], ["Gazzali", "Fakhruddin", ""], ["Hasan", "Bakhtiar", ""], ["Kabir", "Md. Hasanul", ""], ["Farazi", "Moshiur", ""]]}, {"id": "2102.10557", "submitter": "Nam Nguyen", "authors": "Nam Nguyen and J. Morris Chang", "title": "Contrastive Self-supervised Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a novel cell-based neural architecture search algorithm\n(NAS), which completely alleviates the expensive costs of data labeling\ninherited from supervised learning. Our algorithm capitalizes on the\neffectiveness of self-supervised learning for image representations, which is\nan increasingly crucial topic of computer vision. First, using only a small\namount of unlabeled train data under contrastive self-supervised learning allow\nus to search on a more extensive search space, discovering better neural\narchitectures without surging the computational resources. Second, we entirely\nrelieve the cost for labeled data (by contrastive loss) in the search stage\nwithout compromising architectures' final performance in the evaluation phase.\nFinally, we tackle the inherent discrete search space of the NAS problem by\nsequential model-based optimization via the tree-parzen estimator (SMBO-TPE),\nenabling us to reduce the computational expense response surface significantly.\nAn extensive number of experiments empirically show that our search algorithm\ncan achieve state-of-the-art results with better efficiency in data labeling\ncost, searching time, and accuracy in final validation.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 08:38:28 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 06:09:07 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Nguyen", "Nam", ""], ["Chang", "J. Morris", ""]]}, {"id": "2102.10575", "submitter": "Dalu Guo Mr.", "authors": "Dalu Guo, Dacheng Tao", "title": "Learning Compositional Representation for Few-shot Visual Question\n  Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current methods of Visual Question Answering perform well on the answers with\nan amount of training data but have limited accuracy on the novel ones with few\nexamples. However, humans can quickly adapt to these new categories with just a\nfew glimpses, as they learn to organize the concepts that have been seen before\nto figure the novel class, which are hardly explored by the deep learning\nmethods. Therefore, in this paper, we propose to extract the attributes from\nthe answers with enough data, which are later composed to constrain the\nlearning of the few-shot ones. We generate the few-shot dataset of VQA with a\nvariety of answers and their attributes without any human effort. With this\ndataset, we build our attribute network to disentangle the attributes by\nlearning their features from parts of the image instead of the whole one.\nExperimental results on the VQA v2.0 validation dataset demonstrate the\neffectiveness of our proposed attribute network and the constraint between\nanswers and their corresponding attributes, as well as the ability of our\nmethod to handle the answers with few training examples.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 10:16:24 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Guo", "Dalu", ""], ["Tao", "Dacheng", ""]]}, {"id": "2102.10590", "submitter": "Zahidul Islam", "authors": "Zahidul Islam, Mohammad Rukonuzzaman, Raiyan Ahmed, Md. Hasanul Kabir,\n  Moshiur Farazi", "title": "Efficient Two-Stream Network for Violence Detection Using Separable\n  Convolutional LSTM", "comments": "Accepted by the 2021 International Joint Conference on Neural\n  Networks (IJCNN 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically detecting violence from surveillance footage is a subset of\nactivity recognition that deserves special attention because of its wide\napplicability in unmanned security monitoring systems, internet video\nfiltration, etc. In this work, we propose an efficient two-stream deep learning\narchitecture leveraging Separable Convolutional LSTM (SepConvLSTM) and\npre-trained MobileNet where one stream takes in background suppressed frames as\ninputs and other stream processes difference of adjacent frames. We employed\nsimple and fast input pre-processing techniques that highlight the moving\nobjects in the frames by suppressing non-moving backgrounds and capture the\nmotion in-between frames. As violent actions are mostly characterized by body\nmovements these inputs help produce discriminative features. SepConvLSTM is\nconstructed by replacing convolution operation at each gate of ConvLSTM with a\ndepthwise separable convolution that enables producing robust long-range\nSpatio-temporal features while using substantially fewer parameters. We\nexperimented with three fusion methods to combine the output feature maps of\nthe two streams. Evaluation of the proposed methods was done on three standard\npublic datasets. Our model outperforms the accuracy on the larger and more\nchallenging RWF-2000 dataset by more than a 2% margin while matching\nstate-of-the-art results on the smaller datasets. Our experiments lead us to\nconclude, the proposed models are superior in terms of both computational\nefficiency and detection accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 12:01:48 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 10:14:39 GMT"}, {"version": "v3", "created": "Tue, 20 Apr 2021 15:16:23 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Islam", "Zahidul", ""], ["Rukonuzzaman", "Mohammad", ""], ["Ahmed", "Raiyan", ""], ["Kabir", "Md. Hasanul", ""], ["Farazi", "Moshiur", ""]]}, {"id": "2102.10592", "submitter": "Naoya Muramatsu", "authors": "Naoya Muramatsu and Hai-Tao Yu", "title": "Combining Spiking Neural Network and Artificial Neural Network for\n  Enhanced Image Classification", "comments": "This paper written for DEIM 2021 (https://db-event.jpn.org/deim2021/)\n  has 12 pages, 6 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the continued innovations of deep neural networks, spiking neural\nnetworks (SNNs) that more closely resemble biological brain synapses have\nattracted attention owing to their low power consumption.However, for\ncontinuous data values, they must employ a coding process to convert the values\nto spike trains.Thus, they have not yet exceeded the performance of artificial\nneural networks (ANNs), which handle such values directly.To this end, we\ncombine an ANN and an SNN to build versatile hybrid neural networks (HNNs) that\nimprove the concerned performance.To qualify this performance, MNIST and\nCIFAR-10 image datasets are used for various classification tasks in which the\ntraining and coding methods changes.In addition, we present simultaneous and\nseparate methods to train the artificial and spiking layers, considering the\ncoding methods of each.We find that increasing the number of artificial layers\nat the expense of spiking layers improves the HNN performance.For\nstraightforward datasets such as MNIST, it is easy to achieve the same\nperformance as ANNs by using duplicate coding and separate learning.However,\nfor more complex tasks, the use of Gaussian coding and simultaneous learning is\nfound to improve the accuracy of HNNs while utilizing a smaller number of\nartificial layers.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 12:03:16 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 10:45:36 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Muramatsu", "Naoya", ""], ["Yu", "Hai-Tao", ""]]}, {"id": "2102.10593", "submitter": "Sohail Iqbal Dr", "authors": "Sohail Iqbal, H. Fareed Ahmed, Talha Qaiser, Muhammad Imran Qureshi,\n  Nasir Rajpoot", "title": "Classification of COVID-19 via Homology of CT-SCAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this worldwide spread of SARS-CoV-2 (COVID-19) infection, it is of utmost\nimportance to detect the disease at an early stage especially in the hot spots\nof this epidemic. There are more than 110 Million infected cases on the globe,\nsofar. Due to its promptness and effective results computed tomography\n(CT)-scan image is preferred to the reverse-transcription polymerase chain\nreaction (RT-PCR). Early detection and isolation of the patient is the only\npossible way of controlling the spread of the disease. Automated analysis of\nCT-Scans can provide enormous support in this process. In this article, We\npropose a novel approach to detect SARS-CoV-2 using CT-scan images. Our method\nis based on a very intuitive and natural idea of analyzing shapes, an attempt\nto mimic a professional medic. We mainly trace SARS-CoV-2 features by\nquantifying their topological properties. We primarily use a tool called\npersistent homology, from Topological Data Analysis (TDA), to compute these\ntopological properties. We train and test our model on the \"SARS-CoV-2 CT-scan\ndataset\" \\citep{soares2020sars}, an open-source dataset, containing 2,481\nCT-scans of normal and COVID-19 patients. Our model yielded an overall\nbenchmark F1 score of $99.42\\% $, accuracy $99.416\\%$, precision $99.41\\%$, and\nrecall $99.42\\%$. The TDA techniques have great potential that can be utilized\nfor efficient and prompt detection of COVID-19. The immense potential of TDA\nmay be exploited in clinics for rapid and safe detection of COVID-19 globally,\nin particular in the low and middle-income countries where RT-PCR labs and/or\nkits are in a serious crisis.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 12:18:38 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Iqbal", "Sohail", ""], ["Ahmed", "H. Fareed", ""], ["Qaiser", "Talha", ""], ["Qureshi", "Muhammad Imran", ""], ["Rajpoot", "Nasir", ""]]}, {"id": "2102.10607", "submitter": "Sivaramakrishnan Rajaraman", "authors": "Sivaramakrishnan Rajaraman, Les Folio, Jane Dimperio, Philip Alderson\n  and Sameer Antani", "title": "Improved Semantic Segmentation of Tuberculosis-consistent findings in\n  Chest X-rays Using Augmented Training of Modality-specific U-Net Models with\n  Weak Localizations", "comments": "31 pages, 19 figures, journal publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) has drawn tremendous attention in object localization and\nrecognition for both natural and medical images. U-Net segmentation models have\ndemonstrated superior performance compared to conventional handcrafted\nfeature-based methods. Medical image modality-specific DL models are better at\ntransferring domain knowledge to a relevant target task than those that are\npretrained on stock photography images. This helps improve model adaptation,\ngeneralization, and class-specific region of interest (ROI) localization. In\nthis study, we train chest X-ray (CXR) modality-specific U-Nets and other\nstate-of-the-art U-Net models for semantic segmentation of tuberculosis\n(TB)-consistent findings. Automated segmentation of such manifestations could\nhelp radiologists reduce errors and supplement decision-making while improving\npatient care and productivity. Our approach uses the publicly available TBX11K\nCXR dataset with weak TB annotations, typically provided as bounding boxes, to\ntrain a set of U-Net models. Next, we improve the results by augmenting the\ntraining data with weak localizations, post-processed into an ROI mask, from a\nDL classifier that is trained to classify CXRs as showing normal lungs or\nsuspected TB manifestations. Test data are individually derived from the TBX11K\nCXR training distribution and other cross-institutional collections including\nthe Shenzhen TB and Montgomery TB CXR datasets. We observe that our augmented\ntraining strategy helped the CXR modality-specific U-Net models achieve\nsuperior performance with test data derived from the TBX11K CXR training\ndistribution as well as from cross-institutional collections (p < 0.05).\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 14:03:49 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 18:10:45 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 17:14:27 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Rajaraman", "Sivaramakrishnan", ""], ["Folio", "Les", ""], ["Dimperio", "Jane", ""], ["Alderson", "Philip", ""], ["Antani", "Sameer", ""]]}, {"id": "2102.10640", "submitter": "Harsh Vardhan Singh", "authors": "Ahlad Kumar and Harsh Vardhan Singh", "title": "Tchebichef Transform Domain-based Deep Learning Architecture for Image\n  Super-resolution", "comments": "11 pages, 12 figures, 53 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent outbreak of COVID-19 has motivated researchers to contribute in\nthe area of medical imaging using artificial intelligence and deep learning.\nSuper-resolution (SR), in the past few years, has produced remarkable results\nusing deep learning methods. The ability of deep learning methods to learn the\nnon-linear mapping from low-resolution (LR) images to their corresponding\nhigh-resolution (HR) images leads to compelling results for SR in diverse areas\nof research. In this paper, we propose a deep learning based image\nsuper-resolution architecture in Tchebichef transform domain. This is achieved\nby integrating a transform layer into the proposed architecture through a\ncustomized Tchebichef convolutional layer ($TCL$). The role of TCL is to\nconvert the LR image from the spatial domain to the orthogonal transform domain\nusing Tchebichef basis functions. The inversion of the aforementioned\ntransformation is achieved using another layer known as the Inverse Tchebichef\nconvolutional Layer (ITCL), which converts back the LR images from the\ntransform domain to the spatial domain. It has been observed that using the\nTchebichef transform domain for the task of SR takes the advantage of high and\nlow-frequency representation of images that makes the task of super-resolution\nsimplified. We, further, introduce transfer learning approach to enhance the\nquality of Covid based medical images. It is shown that our architecture\nenhances the quality of X-ray and CT images of COVID-19, providing a better\nimage quality that helps in clinical diagnosis. Experimental results obtained\nusing the proposed Tchebichef transform domain super-resolution (TTDSR)\narchitecture provides competitive results when compared with most of the deep\nlearning methods employed using a fewer number of trainable parameters.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 16:39:20 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 04:21:56 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Kumar", "Ahlad", ""], ["Singh", "Harsh Vardhan", ""]]}, {"id": "2102.10662", "submitter": "Valanarasu Jeya Maria Jose", "authors": "Jeya Maria Jose Valanarasu, Poojan Oza, Ilker Hacihaliloglu, Vishal M.\n  Patel", "title": "Medical Transformer: Gated Axial-Attention for Medical Image\n  Segmentation", "comments": "Accepted at MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, Deep Convolutional Neural Networks have been widely\nadopted for medical image segmentation and shown to achieve adequate\nperformance. However, due to the inherent inductive biases present in the\nconvolutional architectures, they lack understanding of long-range dependencies\nin the image. Recently proposed Transformer-based architectures that leverage\nself-attention mechanism encode long-range dependencies and learn\nrepresentations that are highly expressive. This motivates us to explore\nTransformer-based solutions and study the feasibility of using\nTransformer-based network architectures for medical image segmentation tasks.\nMajority of existing Transformer-based network architectures proposed for\nvision applications require large-scale datasets to train properly. However,\ncompared to the datasets for vision applications, for medical imaging the\nnumber of data samples is relatively low, making it difficult to efficiently\ntrain transformers for medical applications. To this end, we propose a Gated\nAxial-Attention model which extends the existing architectures by introducing\nan additional control mechanism in the self-attention module. Furthermore, to\ntrain the model effectively on medical images, we propose a Local-Global\ntraining strategy (LoGo) which further improves the performance. Specifically,\nwe operate on the whole image and patches to learn global and local features,\nrespectively. The proposed Medical Transformer (MedT) is evaluated on three\ndifferent medical image segmentation datasets and it is shown that it achieves\nbetter performance than the convolutional and other related transformer-based\narchitectures. Code: https://github.com/jeya-maria-jose/Medical-Transformer\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 18:35:14 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 19:01:01 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Valanarasu", "Jeya Maria Jose", ""], ["Oza", "Poojan", ""], ["Hacihaliloglu", "Ilker", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2102.10663", "submitter": "Pranav Rajpurkar", "authors": "Yen Nhi Truong Vu, Richard Wang, Niranjan Balachandar, Can Liu, Andrew\n  Y. Ng, Pranav Rajpurkar", "title": "MedAug: Contrastive learning leveraging patient metadata improves\n  representations for chest X-ray interpretation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-supervised contrastive learning between pairs of multiple views of the\nsame image has been shown to successfully leverage unlabeled data to produce\nmeaningful visual representations for both natural and medical images. However,\nthere has been limited work on determining how to select pairs for medical\nimages, where availability of patient metadata can be leveraged to improve\nrepresentations. In this work, we develop a method to select positive pairs\ncoming from views of possibly different images through the use of patient\nmetadata. We compare strategies for selecting positive pairs for chest X-ray\ninterpretation including requiring them to be from the same patient, imaging\nstudy or laterality. We evaluate downstream task performance by fine-tuning the\nlinear layer on 1% of the labeled dataset for pleural effusion classification.\nOur best performing positive pair selection strategy, which involves using\nimages from the same patient from the same study across all lateralities,\nachieves a performance increase of 3.4% and 14.4% in mean AUC from both a\nprevious contrastive method and ImageNet pretrained baseline respectively. Our\ncontrolled experiments show that the keys to improving downstream performance\non disease classification are (1) using patient metadata to appropriately\ncreate positive pairs from different images with the same underlying\npathologies, and (2) maximizing the number of different images used in query\npairing. In addition, we explore leveraging patient metadata to select hard\nnegative pairs for contrastive learning, but do not find improvement over\nbaselines that do not use metadata. Our method is broadly applicable to medical\nimage interpretation and allows flexibility for incorporating medical insights\nin choosing pairs for contrastive learning.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 18:39:04 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Vu", "Yen Nhi Truong", ""], ["Wang", "Richard", ""], ["Balachandar", "Niranjan", ""], ["Liu", "Can", ""], ["Ng", "Andrew Y.", ""], ["Rajpurkar", "Pranav", ""]]}, {"id": "2102.10680", "submitter": "Fatemeh Haghighi", "authors": "Fatemeh Haghighi, Mohammad Reza Hosseinzadeh Taher, Zongwei Zhou,\n  Michael B. Gotway, Jianming Liang", "title": "Transferable Visual Words: Exploiting the Semantics of Anatomical\n  Patterns for Self-supervised Learning", "comments": "Journal version of arXiv:2007.06959, accepted by IEEE Transactions on\n  Medical Imaging (TMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new concept called \"transferable visual words\"\n(TransVW), aiming to achieve annotation efficiency for deep learning in medical\nimage analysis. Medical imaging--focusing on particular parts of the body for\ndefined clinical purposes--generates images of great similarity in anatomy\nacross patients and yields sophisticated anatomical patterns across images,\nwhich are associated with rich semantics about human anatomy and which are\nnatural visual words. We show that these visual words can be automatically\nharvested according to anatomical consistency via self-discovery, and that the\nself-discovered visual words can serve as strong yet free supervision signals\nfor deep models to learn semantics-enriched generic image representation via\nself-supervision (self-classification and self-restoration). Our extensive\nexperiments demonstrate the annotation efficiency of TransVW by offering higher\nperformance and faster convergence with reduced annotation cost in several\napplications. Our TransVW has several important advantages, including (1)\nTransVW is a fully autodidactic scheme, which exploits the semantics of visual\nwords for self-supervised learning, requiring no expert annotation; (2) visual\nword learning is an add-on strategy, which complements existing self-supervised\nmethods, boosting their performance; and (3) the learned image representation\nis semantics-enriched models, which have proven to be more robust and\ngeneralizable, saving annotation efforts for a variety of applications through\ntransfer learning. Our code, pre-trained models, and curated visual words are\navailable at https://github.com/JLiangLab/TransVW.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 20:44:55 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Haghighi", "Fatemeh", ""], ["Taher", "Mohammad Reza Hosseinzadeh", ""], ["Zhou", "Zongwei", ""], ["Gotway", "Michael B.", ""], ["Liang", "Jianming", ""]]}, {"id": "2102.10681", "submitter": "Max Coenen", "authors": "Max Coenen and Franz Rottensteiner", "title": "Probabilistic Vehicle Reconstruction Using a Multi-Task CNN", "comments": "2019 IEEE/CVF International Conference on Computer Vision Workshop\n  (ICCVW)", "journal-ref": null, "doi": "10.1109/ICCVW.2019.00110", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The retrieval of the 3D pose and shape of objects from images is an ill-posed\nproblem. A common way to object reconstruction is to match entities such as\nkeypoints, edges, or contours of a deformable 3D model, used as shape prior, to\ntheir corresponding entities inferred from the image. However, such approaches\nare highly sensitive to model initialisation, imprecise keypoint localisations\nand/or illumination conditions. In this paper, we present a probabilistic\napproach for shape-aware 3D vehicle reconstruction from stereo images that\nleverages the outputs of a novel multi-task CNN. Specifically, we train a CNN\nthat outputs probability distributions for the vehicle's orientation and for\nboth, vehicle keypoints and wireframe edges. Together with 3D stereo\ninformation we integrate the predicted distributions into a common\nprobabilistic framework. We believe that the CNN-based detection of wireframe\nedges reduces the sensitivity to illumination conditions and object contrast\nand that using the raw probability maps instead of inferring keypoint positions\nreduces the sensitivity to keypoint localisation errors. We show that our\nmethod achieves state-of-the-art results, evaluating our method on the\nchallenging KITTI benchmark and on our own new 'Stereo-Vehicle' dataset.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 20:45:44 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Coenen", "Max", ""], ["Rottensteiner", "Franz", ""]]}, {"id": "2102.10710", "submitter": "Hanlin Niu", "authors": "Hanlin Niu, Ze Ji, Zihang Zhu, Hujun Yin, and Joaquin Carrasco", "title": "3D Vision-guided Pick-and-Place Using Kuka LBR iiwa Robot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the development of a control system for vision-guided\npick-and-place tasks using a robot arm equipped with a 3D camera. The main\nsteps include camera intrinsic and extrinsic calibration, hand-eye calibration,\ninitial object pose registration, objects pose alignment algorithm, and\npick-and-place execution. The proposed system allows the robot be able to to\npick and place object with limited times of registering a new object and the\ndeveloped software can be applied for new object scenario quickly. The\nintegrated system was tested using the hardware combination of kuka iiwa,\nRobotiq grippers (two finger gripper and three finger gripper) and 3D cameras\n(Intel realsense D415 camera, Intel realsense D435 camera, Microsoft Kinect\nV2). The whole system can also be modified for the combination of other robotic\narm, gripper and 3D camera.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 23:26:34 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 02:48:31 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Niu", "Hanlin", ""], ["Ji", "Ze", ""], ["Zhu", "Zihang", ""], ["Yin", "Hujun", ""], ["Carrasco", "Joaquin", ""]]}, {"id": "2102.10731", "submitter": "Sarvesh Kumar Singh Mr.", "authors": "Sarvesh Kumar Singh, Bikram Pratap Banerjee and Simit Raval", "title": "Three dimensional unique identifier based automated georeferencing and\n  coregistration of point clouds in underground environment", "comments": "26 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spatially and geometrically accurate laser scans are essential in modelling\ninfrastructure for applications in civil, mining and transportation. Monitoring\nof underground or indoor environments such as mines or tunnels is challenging\ndue to unavailability of a sensor positioning framework, complicated\nstructurally symmetric layouts, repetitive features and occlusions. Current\npractices largely include a manual selection of discernable reference points\nfor georeferencing and coregistration purpose. This study aims at overcoming\nthese practical challenges in underground or indoor laser scanning. The\ndeveloped approach involves automatically and uniquely identifiable three\ndimensional unique identifiers (3DUIDs) in laser scans, and a 3D registration\n(3DReG) workflow. Field testing of the method in an underground tunnel has been\nfound accurate, effective and efficient. Additionally, a method for\nautomatically extracting roadway tunnel profile has been exhibited. The\ndeveloped 3DUID can be used in roadway profile extraction, guided automation,\nsensor calibration, reference targets for routine survey and deformation\nmonitoring.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 01:47:50 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Singh", "Sarvesh Kumar", ""], ["Banerjee", "Bikram Pratap", ""], ["Raval", "Simit", ""]]}, {"id": "2102.10744", "submitter": "Yudong Chen", "authors": "Yudong Chen, Chaoyu Guan, Zhikun Wei, Xin Wang, Wenwu Zhu", "title": "MetaDelta: A Meta-Learning System for Few-shot Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning aims at learning quickly on novel tasks with limited data by\ntransferring generic experience learned from previous tasks. Naturally,\nfew-shot learning has been one of the most popular applications for\nmeta-learning. However, existing meta-learning algorithms rarely consider the\ntime and resource efficiency or the generalization capacity for unknown\ndatasets, which limits their applicability in real-world scenarios. In this\npaper, we propose MetaDelta, a novel practical meta-learning system for the\nfew-shot image classification. MetaDelta consists of two core components: i)\nmultiple meta-learners supervised by a central controller to ensure efficiency,\nand ii) a meta-ensemble module in charge of integrated inference and better\ngeneralization. In particular, each meta-learner in MetaDelta is composed of a\nunique pretrained encoder fine-tuned by batch training and parameter-free\ndecoder used for prediction. MetaDelta ranks first in the final phase in the\nAAAI 2021 MetaDL\nChallenge\\footnote{https://competitions.codalab.org/competitions/26638},\ndemonstrating the advantages of our proposed system. The codes are publicly\navailable at https://github.com/Frozenmad/MetaDelta.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 02:57:22 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Chen", "Yudong", ""], ["Guan", "Chaoyu", ""], ["Wei", "Zhikun", ""], ["Wang", "Xin", ""], ["Zhu", "Wenwu", ""]]}, {"id": "2102.10765", "submitter": "Renato Hermoza Aragon\\'es", "authors": "Renato Hermoza, Gabriel Maicas, Jacinto C. Nascimento, Gustavo\n  Carneiro", "title": "Post-hoc Overall Survival Time Prediction from Brain MRI", "comments": "5 pages, 2 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Overall survival (OS) time prediction is one of the most common estimates of\nthe prognosis of gliomas and is used to design an appropriate treatment\nplanning. State-of-the-art (SOTA) methods for OS time prediction follow a\npre-hoc approach that require computing the segmentation map of the glioma\ntumor sub-regions (necrotic, edema tumor, enhancing tumor) for estimating OS\ntime. However, the training of the segmentation methods require ground truth\nsegmentation labels which are tedious and expensive to obtain. Given that most\nof the large-scale data sets available from hospitals are unlikely to contain\nsuch precise segmentation, those SOTA methods have limited applicability. In\nthis paper, we introduce a new post-hoc method for OS time prediction that does\nnot require segmentation map annotation for training. Our model uses medical\nimage and patient demographics (represented by age) as inputs to estimate the\nOS time and to estimate a saliency map that localizes the tumor as a way to\nexplain the OS time prediction in a post-hoc manner. It is worth emphasizing\nthat although our model can localize tumors, it uses only the ground truth OS\ntime as training signal, i.e., no segmentation labels are needed. We evaluate\nour post-hoc method on the Multimodal Brain Tumor Segmentation Challenge\n(BraTS) 2019 data set and show that it achieves competitive results compared to\npre-hoc methods with the advantage of not requiring segmentation labels for\ntraining.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 04:27:03 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Hermoza", "Renato", ""], ["Maicas", "Gabriel", ""], ["Nascimento", "Jacinto C.", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "2102.10772", "submitter": "Ronghang Hu", "authors": "Ronghang Hu, Amanpreet Singh", "title": "UniT: Multimodal Multitask Learning with a Unified Transformer", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose UniT, a Unified Transformer model to simultaneously learn the most\nprominent tasks across different domains, ranging from object detection to\nnatural language understanding and multimodal reasoning. Based on the\ntransformer encoder-decoder architecture, our UniT model encodes each input\nmodality with an encoder and makes predictions on each task with a shared\ndecoder over the encoded input representations, followed by task-specific\noutput heads. The entire model is jointly trained end-to-end with losses from\neach task. Compared to previous efforts on multi-task learning with\ntransformers, we share the same model parameters across all tasks instead of\nseparately fine-tuning task-specific models and handle a much higher variety of\ntasks across different domains. In our experiments, we learn 7 tasks jointly\nover 8 datasets, achieving strong performance on each task with 87.5% fewer\nparameters. Code will be released in MMF at https://mmf.sh.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 04:45:06 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 23:23:06 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Hu", "Ronghang", ""], ["Singh", "Amanpreet", ""]]}, {"id": "2102.10777", "submitter": "Tejas Khare", "authors": "Tejas Khare, Vaibhav Bahel and Anuradha C. Phadke", "title": "PCB-Fire: Automated Classification and Fault Detection in PCB", "comments": "6 Pages, 9 Figures, Conference", "journal-ref": "Proceeding Reference - 978-0-7381-4335-4/20/$31.00\n  \\c{opyright}2020 IEEE", "doi": "10.1109/MPCIT51588.2020.9350324", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Printed Circuit Boards are the foundation for the functioning of any\nelectronic device, and therefore are an essential component for various\nindustries such as automobile, communication, computation, etc. However, one of\nthe challenges faced by the PCB manufacturers in the process of manufacturing\nof the PCBs is the faulty placement of its components including missing\ncomponents. In the present scenario the infrastructure required to ensure\nadequate quality of the PCB requires a lot of time and effort. The authors\npresent a novel solution for detecting missing components and classifying them\nin a resourceful manner. The presented algorithm focuses on pixel theory and\nobject detection, which has been used in combination to optimize the results\nfrom the given dataset.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 05:19:22 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Khare", "Tejas", ""], ["Bahel", "Vaibhav", ""], ["Phadke", "Anuradha C.", ""]]}, {"id": "2102.10788", "submitter": "Xu Wang", "authors": "Xu Wang, Yi Jin, Yigang Cen, Tao Wang and Yidong Li", "title": "Attention Models for Point Clouds in Deep Learning: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the advancement of 3D point clouds in deep learning has attracted\nintensive research in different application domains such as computer vision and\nrobotic tasks. However, creating feature representation of robust,\ndiscriminative from unordered and irregular point clouds is challenging. In\nthis paper, our ultimate goal is to provide a comprehensive overview of the\npoint clouds feature representation which uses attention models. More than 75+\nkey contributions in the recent three years are summarized in this survey,\nincluding the 3D objective detection, 3D semantic segmentation, 3D pose\nestimation, point clouds completion etc. We provide a detailed characterization\n(1) the role of attention mechanisms, (2) the usability of attention models\ninto different tasks, (3) the development trend of key technology.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 05:50:22 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Wang", "Xu", ""], ["Jin", "Yi", ""], ["Cen", "Yigang", ""], ["Wang", "Tao", ""], ["Li", "Yidong", ""]]}, {"id": "2102.10795", "submitter": "Chuchu Han", "authors": "Chuchu Han, Zhedong Zheng, Changxin Gao, Nong Sang, Yi Yang", "title": "Decoupled and Memory-Reinforced Networks: Towards Effective Feature\n  Learning for One-Step Person Search", "comments": "8 pages, 6 figures. Accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of person search is to localize and match query persons from scene\nimages. For high efficiency, one-step methods have been developed to jointly\nhandle the pedestrian detection and identification sub-tasks using a single\nnetwork. There are two major challenges in the current one-step approaches. One\nis the mutual interference between the optimization objectives of multiple\nsub-tasks. The other is the sub-optimal identification feature learning caused\nby small batch size when end-to-end training. To overcome these problems, we\npropose a decoupled and memory-reinforced network (DMRNet). Specifically, to\nreconcile the conflicts of multiple objectives, we simplify the standard\ntightly coupled pipelines and establish a deeply decoupled multi-task learning\nframework. Further, we build a memory-reinforced mechanism to boost the\nidentification feature learning. By queuing the identification features of\nrecently accessed instances into a memory bank, the mechanism augments the\nsimilarity pair construction for pairwise metric learning. For better encoding\nconsistency of the stored features, a slow-moving average of the network is\napplied for extracting these features. In this way, the dual networks reinforce\neach other and converge to robust solution states. Experimentally, the proposed\nmethod obtains 93.2% and 46.9% mAP on CUHK-SYSU and PRW datasets, which exceeds\nall the existing one-step methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 06:19:45 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Han", "Chuchu", ""], ["Zheng", "Zhedong", ""], ["Gao", "Changxin", ""], ["Sang", "Nong", ""], ["Yang", "Yi", ""]]}, {"id": "2102.10798", "submitter": "Anlin Sun", "authors": "Yaguan Qian and Anlin Sun", "title": "Person Re-identification based on Robust Features in Open-world", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning technology promotes the rapid development of person\nre-identifica-tion (re-ID). However, some challenges are still existing in the\nopen-world. First, the existing re-ID research usually assumes only one factor\nvariable (view, clothing, pedestrian pose, pedestrian occlusion, image\nresolution, RGB/IR modality) changing, ignoring the complexity of multi-factor\nvariables in the open-world. Second, the existing re-ID methods are over depend\non clothing color and other apparent features of pedestrian, which are easily\ndisguised or changed. In addition, the lack of benchmark datasets containing\nmulti-factor variables is also hindering the practically application of re-ID\nin the open-world. In this paper, we propose a low-cost and high-efficiency\nmethod to solve shortcomings of the existing re-ID research, such as unreliable\nfeature selection, low efficiency of feature extraction, single research\nvariable, etc. Our approach based on pose estimation model improved by group\nconvolution to obtain the continuous key points of pedestrian, and utilize\ndynamic time warping (DTW) to measure the similarity of features between\ndifferent pedestrians. At the same time, to verify the effectiveness of our\nmethod, we provide a miniature dataset which is closer to the real world and\nincludes pedestrian changing clothes and cross-modality factor variables\nfusion. Extensive experiments are conducted and the results show that our\nmethod achieves Rank-1: 60.9%, Rank-5: 78.1%, and mAP: 49.2% on this dataset,\nwhich exceeds most existing state-of-art re-ID models.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 06:49:28 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Qian", "Yaguan", ""], ["Sun", "Anlin", ""]]}, {"id": "2102.10802", "submitter": "Praneeth Vepakomma", "authors": "Praneeth Vepakomma, Julia Balla, Ramesh Raskar", "title": "Differentially Private Supervised Manifold Learning with Applications\n  like Private Image Retrieval", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential Privacy offers strong guarantees such as immutable privacy under\npost processing. Thus it is often looked to as a solution to learning on\nscattered and isolated data. This work focuses on supervised manifold learning,\na paradigm that can generate fine-tuned manifolds for a target use case. Our\ncontributions are two fold. 1) We present a novel differentially private method\n\\textit{PrivateMail} for supervised manifold learning, the first of its kind to\nour knowledge. 2) We provide a novel private geometric embedding scheme for our\nexperimental use case. We experiment on private \"content based image retrieval\"\n- embedding and querying the nearest neighbors of images in a private manner -\nand show extensive privacy-utility tradeoff results, as well as the\ncomputational efficiency and practicality of our methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 06:58:46 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Vepakomma", "Praneeth", ""], ["Balla", "Julia", ""], ["Raskar", "Ramesh", ""]]}, {"id": "2102.10820", "submitter": "Dennis Stumpf", "authors": "Dennis Stumpf, Stephan Krau\\ss, Gerd Reis, Oliver Wasenm\\\"uller,\n  Didier Stricker", "title": "SALT: A Semi-automatic Labeling Tool for RGB-D Video Sequences", "comments": "VISAPP 2021 full paper (9 pages, 6 figures), published by SciTePress:\n  https://www.scitepress.org/PublicationsDetail.aspx?ID=ywQZ3GZrka8=&t=1", "journal-ref": "Proceedings of the 16th International Joint Conference on Computer\n  Vision, Imaging and Computer Graphics Theory and Applications - Volume 4\n  VISAPP: VISAPP (2021) 595-603", "doi": "10.5220/0010303005950603", "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large labeled data sets are one of the essential basics of modern deep\nlearning techniques. Therefore, there is an increasing need for tools that\nallow to label large amounts of data as intuitively as possible. In this paper,\nwe introduce SALT, a tool to semi-automatically annotate RGB-D video sequences\nto generate 3D bounding boxes for full six Degrees of Freedom (DoF) object\nposes, as well as pixel-level instance segmentation masks for both RGB and\ndepth. Besides bounding box propagation through various interpolation\ntechniques, as well as algorithmically guided instance segmentation, our\npipeline also provides built-in pre-processing functionalities to facilitate\nthe data set creation process. By making full use of SALT, annotation time can\nbe reduced by a factor of up to 33.95 for bounding box creation and 8.55 for\nRGB segmentation without compromising the quality of the automatically\ngenerated ground truth.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 08:11:39 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Stumpf", "Dennis", ""], ["Krau\u00df", "Stephan", ""], ["Reis", "Gerd", ""], ["Wasenm\u00fcller", "Oliver", ""], ["Stricker", "Didier", ""]]}, {"id": "2102.10854", "submitter": "Xiaolong Guo", "authors": "Xiaolong Guo, Xiaosong Lan, Kunfeng Wang, Shuxiao Li", "title": "Contour Loss for Instance Segmentation via k-step Distance\n  Transformation Image", "comments": "Under review in IET Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance segmentation aims to locate targets in the image and segment each\ntarget area at pixel level, which is one of the most important tasks in\ncomputer vision. Mask R-CNN is a classic method of instance segmentation, but\nwe find that its predicted masks are unclear and inaccurate near contours. To\ncope with this problem, we draw on the idea of contour matching based on\ndistance transformation image and propose a novel loss function, called contour\nloss. Contour loss is designed to specifically optimize the contour parts of\nthe predicted masks, thus can assure more accurate instance segmentation. In\norder to make the proposed contour loss to be jointly trained under modern\nneural network frameworks, we design a differentiable k-step distance\ntransformation image calculation module, which can approximately compute\ntruncated distance transformation images of the predicted mask and\ncorresponding ground-truth mask online. The proposed contour loss can be\nintegrated into existing instance segmentation methods such as Mask R-CNN, and\ncombined with their original loss functions without modification of the\ninference network structures, thus has strong versatility. Experimental results\non COCO show that contour loss is effective, which can further improve instance\nsegmentation performances.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 09:35:35 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Guo", "Xiaolong", ""], ["Lan", "Xiaosong", ""], ["Wang", "Kunfeng", ""], ["Li", "Shuxiao", ""]]}, {"id": "2102.10882", "submitter": "Xiangxiang Chu", "authors": "Xiangxiang Chu and Zhi Tian and Bo Zhang and Xinlong Wang and Xiaolin\n  Wei and Huaxia Xia and Chunhua Shen", "title": "Conditional Positional Encodings for Vision Transformers", "comments": "A general purpose conditional position encoding for vision\n  transformers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a conditional positional encoding (CPE) scheme for vision\nTransformers. Unlike previous fixed or learnable positional encodings, which\nare pre-defined and independent of input tokens, CPE is dynamically generated\nand conditioned on the local neighborhood of the input tokens. As a result, CPE\ncan easily generalize to the input sequences that are longer than what the\nmodel has ever seen during training. Besides, CPE can keep the desired\ntranslation-invariance in the image classification task, resulting in improved\nclassification accuracy. CPE can be effortlessly implemented with a simple\nPosition Encoding Generator (PEG), and it can be seamlessly incorporated into\nthe current Transformer framework. Built on PEG, we present Conditional\nPosition encoding Vision Transformer (CPVT). We demonstrate that CPVT has\nvisually similar attention maps compared to those with learned positional\nencodings. Benefit from the conditional positional encoding scheme, we obtain\nstate-of-the-art results on the ImageNet classification task compared with\nvision Transformers to date. Our code will be made available at\nhttps://github.com/Meituan-AutoML/CPVT .\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 10:29:55 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 10:59:50 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Chu", "Xiangxiang", ""], ["Tian", "Zhi", ""], ["Zhang", "Bo", ""], ["Wang", "Xinlong", ""], ["Wei", "Xiaolin", ""], ["Xia", "Huaxia", ""], ["Shen", "Chunhua", ""]]}, {"id": "2102.10884", "submitter": "Jun Sun", "authors": "Hongxiang Cai, Jun Sun, Yichao Xiong", "title": "Revisiting Classification Perspective on Scene Text Recognition", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevalent perspectives of scene text recognition are from sequence to\nsequence (seq2seq) and segmentation. Nevertheless, the former is composed of\nmany components which makes implementation and deployment complicated, while\nthe latter requires character level annotations that is expensive. In this\npaper, we revisit classification perspective that models scene text recognition\nas an image classification problem. Classification perspective has a simple\npipeline and only needs word level annotations. We revive classification\nperspective by devising a scene text recognition model named as CSTR, which\nperforms as well as methods from other perspectives. The CSTR model consists of\nCPNet (classification perspective network) and SPPN (separated conv with global\naverage pooling prediction network). CSTR is as simple as image classification\nmodel like ResNet \\cite{he2016deep} which makes it easy to implement and\ndeploy. We demonstrate the effectiveness of the classification perspective on\nscene text recognition with extensive experiments. Futhermore, CSTR achieves\nnearly state-of-the-art performance on six public benchmarks including regular\ntext, irregular text. The code will be available at\nhttps://github.com/Media-Smart/vedastr.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 10:32:07 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 10:08:59 GMT"}, {"version": "v3", "created": "Sat, 12 Jun 2021 13:33:27 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Cai", "Hongxiang", ""], ["Sun", "Jun", ""], ["Xiong", "Yichao", ""]]}, {"id": "2102.10919", "submitter": "Zhiqiang Shen", "authors": "Shaohua Zheng, Zhiqiang Shen, Chenhao Peia, Wangbin Ding, Haojin Lin,\n  Jiepeng Zheng, Lin Pan, Bin Zheng, Liqin Huang", "title": "Interpretative Computer-aided Lung Cancer Diagnosis: from Radiology\n  Analysis to Malignancy Evaluation", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background and Objective:Computer-aided diagnosis (CAD) systems promote\ndiagnosis effectiveness and alleviate pressure of radiologists. A CAD system\nfor lung cancer diagnosis includes nodule candidate detection and nodule\nmalignancy evaluation. Recently, deep learning-based pulmonary nodule detection\nhas reached satisfactory performance ready for clinical application. However,\ndeep learning-based nodule malignancy evaluation depends on heuristic inference\nfrom low-dose computed tomography volume to malignant probability, which lacks\nclinical cognition. Methods:In this paper, we propose a joint radiology\nanalysis and malignancy evaluation network (R2MNet) to evaluate the pulmonary\nnodule malignancy via radiology characteristics analysis. Radiological features\nare extracted as channel descriptor to highlight specific regions of the input\nvolume that are critical for nodule malignancy evaluation. In addition, for\nmodel explanations, we propose channel-dependent activation mapping to\nvisualize the features and shed light on the decision process of deep neural\nnetwork. Results:Experimental results on the LIDC-IDRI dataset demonstrate that\nthe proposed method achieved area under curve of 96.27% on nodule radiology\nanalysis and AUC of 97.52% on nodule malignancy evaluation. In addition,\nexplanations of CDAM features proved that the shape and density of nodule\nregions were two critical factors that influence a nodule to be inferred as\nmalignant, which conforms with the diagnosis cognition of experienced\nradiologists. Conclusion:Incorporating radiology analysis with nodule malignant\nevaluation, the network inference process conforms to the diagnostic procedure\nof radiologists and increases the confidence of evaluation results. Besides,\nmodel interpretation with CDAM features shed light on the regions which DNNs\nfocus on when they estimate nodule malignancy probabilities.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 11:40:43 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Zheng", "Shaohua", ""], ["Shen", "Zhiqiang", ""], ["Peia", "Chenhao", ""], ["Ding", "Wangbin", ""], ["Lin", "Haojin", ""], ["Zheng", "Jiepeng", ""], ["Pan", "Lin", ""], ["Zheng", "Bin", ""], ["Huang", "Liqin", ""]]}, {"id": "2102.10923", "submitter": "Mateus Riva", "authors": "Mateus Riva, Pietro Gori, Florian Yger, Roberto Cesar, Isabelle Bloch", "title": "Approximation of dilation-based spatial relations to add structural\n  constraints in neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial relations between objects in an image have proved useful for\nstructural object recognition. Structural constraints can act as regularization\nin neural network training, improving generalization capability with small\ndatasets. Several relations can be modeled as a morphological dilation of a\nreference object with a structuring element representing the semantics of the\nrelation, from which the degree of satisfaction of the relation between another\nobject and the reference object can be derived. However, dilation is not\ndifferentiable, requiring an approximation to be used in the context of\ngradient-descent training of a network. We propose to approximate dilations\nusing convolutions based on a kernel equal to the structuring element. We show\nthat the proposed approximation, even if slightly less accurate than previous\napproximations, is definitely faster to compute and therefore more suitable for\ncomputationally intensive neural network applications.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 11:44:02 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Riva", "Mateus", ""], ["Gori", "Pietro", ""], ["Yger", "Florian", ""], ["Cesar", "Roberto", ""], ["Bloch", "Isabelle", ""]]}, {"id": "2102.10928", "submitter": "Huu Le", "authors": "Huu Le and Christopher Zach", "title": "Escaping Poor Local Minima in Large Scale Robust Estimation", "comments": "arXiv admin note: substantial text overlap with arXiv:2003.09080", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Robust parameter estimation is a crucial task in several 3D computer vision\npipelines such as Structure from Motion (SfM). State-of-the-art algorithms for\nrobust estimation, however, still suffer from difficulties in converging to\nsatisfactory solutions due to the presence of many poor local minima or flat\nregions in the optimization landscapes. In this paper, we introduce two novel\napproaches for robust parameter estimation. The first algorithm utilizes the\nFilter Method (FM), which is a framework for constrained optimization allowing\ngreat flexibility in algorithmic choices, to derive an adaptive kernel scaling\nstrategy that enjoys a strong ability to escape poor minima and achieves fast\nconvergence rates. Our second algorithm combines a generalized Majorization\nMinimization (GeMM) framework with the half-quadratic lifting formulation to\nobtain a simple yet efficient solver for robust estimation. We empirically show\nthat both proposed approaches show encouraging capability on avoiding poor\nlocal minima and achieve competitive results compared to existing state-of-the\nart robust fitting algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 11:58:29 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Le", "Huu", ""], ["Zach", "Christopher", ""]]}, {"id": "2102.10929", "submitter": "Markus Bosch", "authors": "Markus Bosch", "title": "Deep Learning for Robust Motion Segmentation with Non-Static Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work proposes a new end-to-end DCNN based approach for motion\nsegmentation, especially for video sequences captured with such non-static\ncameras, called MOSNET. While other approaches focus on spatial or temporal\ncontext only, the proposed approach uses 3D convolutions as a key technology to\nfactor in, spatio-temporal features in cohesive video frames. This is done by\ncapturing temporal information in features with a low and also with a high\nlevel of abstraction. The lean network architecture with about 21k trainable\nparameters is mainly based on a pre-trained VGG-16 network. The MOSNET uses a\nnew feature map fusion technique, which enables the network to focus on the\nappropriate level of abstraction, resolution, and the appropriate size of the\nreceptive field regarding the input. Furthermore, the end-to-end deep learning\nbased approach can be extended by feature based image alignment as a\npre-processing step, which brings a gain in performance for some scenes.\nEvaluating the end-to-end deep learning based MOSNET network in a scene\nindependent manner leads to an overall F-measure of 0.803 on the CDNet2014\ndataset. A small temporal window of five input frames, without the need of any\ninitialization is used to obtain this result. Therefore the network is able to\nperform well on scenes captured with non-static cameras where the image content\nchanges significantly during the scene. In order to get robust results in\nscenes captured with a moving camera, feature based image alignment can\nimplemented as pre-processing step. The MOSNET combined with pre-processing\nleads to an F-measure of 0.685 when cross-evaluating with a relabeled LASIESTA\ndataset, which underpins the capability generalise of the MOSNET.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 11:58:41 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Bosch", "Markus", ""]]}, {"id": "2102.10935", "submitter": "Yazhou Yao", "authors": "Tao Chen, Guosen Xie, Yazhou Yao, Qiong Wang, Fumin Shen, Zhenmin\n  Tang, and Jian Zhang", "title": "Semantically Meaningful Class Prototype Learning for One-Shot Image\n  Semantic Segmentation", "comments": null, "journal-ref": "IEEE Transactions on Multimedia, 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-shot semantic image segmentation aims to segment the object regions for\nthe novel class with only one annotated image. Recent works adopt the episodic\ntraining strategy to mimic the expected situation at testing time. However,\nthese existing approaches simulate the test conditions too strictly during the\ntraining process, and thus cannot make full use of the given label information.\nBesides, these approaches mainly focus on the foreground-background target\nclass segmentation setting. They only utilize binary mask labels for training.\nIn this paper, we propose to leverage the multi-class label information during\nthe episodic training. It will encourage the network to generate more\nsemantically meaningful features for each category. After integrating the\ntarget class cues into the query features, we then propose a pyramid feature\nfusion module to mine the fused features for the final classifier. Furthermore,\nto take more advantage of the support image-mask pair, we propose a\nself-prototype guidance branch to support image segmentation. It can constrain\nthe network for generating more compact features and a robust prototype for\neach semantic class. For inference, we propose a fused prototype guidance\nbranch for the segmentation of the query image. Specifically, we leverage the\nprediction of the query image to extract the pseudo-prototype and combine it\nwith the initial prototype. Then we utilize the fused prototype to guide the\nfinal segmentation of the query image. Extensive experiments demonstrate the\nsuperiority of our proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 12:07:35 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Chen", "Tao", ""], ["Xie", "Guosen", ""], ["Yao", "Yazhou", ""], ["Wang", "Qiong", ""], ["Shen", "Fumin", ""], ["Tang", "Zhenmin", ""], ["Zhang", "Jian", ""]]}, {"id": "2102.10951", "submitter": "Alexander Hepburn", "authors": "Alexander Hepburn, Raul Santos-Rodriguez", "title": "Explainers in the Wild: Making Surrogate Explainers Robust to\n  Distortions through Perception", "comments": null, "journal-ref": "2021 IEEE International Conference on Image Processing (ICIP),\n  Anchorage, Alaska, USA", "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explaining the decisions of models is becoming pervasive in the image\nprocessing domain, whether it is by using post-hoc methods or by creating\ninherently interpretable models. While the widespread use of surrogate\nexplainers is a welcome addition to inspect and understand black-box models,\nassessing the robustness and reliability of the explanations is key for their\nsuccess. Additionally, whilst existing work in the explainability field\nproposes various strategies to address this problem, the challenges of working\nwith data in the wild is often overlooked. For instance, in image\nclassification, distortions to images can not only affect the predictions\nassigned by the model, but also the explanation. Given a clean and a distorted\nversion of an image, even if the prediction probabilities are similar, the\nexplanation may still be different. In this paper we propose a methodology to\nevaluate the effect of distortions in explanations by embedding perceptual\ndistances that tailor the neighbourhoods used to training surrogate explainers.\nWe also show that by operating in this way, we can make the explanations more\nrobust to distortions. We generate explanations for images in the Imagenet-C\ndataset and demonstrate how using a perceptual distances in the surrogate\nexplainer creates more coherent explanations for the distorted and reference\nimages.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 12:38:53 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 10:39:04 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Hepburn", "Alexander", ""], ["Santos-Rodriguez", "Raul", ""]]}, {"id": "2102.10955", "submitter": "Chen Wang", "authors": "Yinghui Li, Ruiyang Liu, Chen Wang, Li Yangning, Ning Ding, Hai-Tao\n  Zheng", "title": "Learning Purified Feature Representations from Task-irrelevant Labels", "comments": "arXiv admin note: substantial text overlap with arXiv:2011.08470", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning an empirically effective model with generalization using limited\ndata is a challenging task for deep neural networks. In this paper, we propose\na novel learning framework called PurifiedLearning to exploit task-irrelevant\nfeatures extracted from task-irrelevant labels when training models on\nsmall-scale datasets. Particularly, we purify feature representations by using\nthe expression of task-irrelevant information, thus facilitating the learning\nprocess of classification. Our work is built on solid theoretical analysis and\nextensive experiments, which demonstrate the effectiveness of PurifiedLearning.\nAccording to the theory we proved, PurifiedLearning is model-agnostic and\ndoesn't have any restrictions on the model needed, so it can be combined with\nany existing deep neural networks with ease to achieve better performance. The\nsource code of this paper will be available in the future for reproducibility.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 12:50:49 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Li", "Yinghui", ""], ["Liu", "Ruiyang", ""], ["Wang", "Chen", ""], ["Yangning", "Li", ""], ["Ding", "Ning", ""], ["Zheng", "Hai-Tao", ""]]}, {"id": "2102.11057", "submitter": "Guillaume Jaume", "authors": "Pushpak Pati and Guillaume Jaume and Antonio Foncubierta and Florinda\n  Feroce and Anna Maria Anniciello and Giosu\\`e Scognamiglio and Nadia Brancati\n  and Maryse Fiche and Estelle Dubruc and Daniel Riccio and Maurizio Di Bonito\n  and Giuseppe De Pietro and Gerardo Botti and Jean-Philippe Thiran and Maria\n  Frucci and Orcun Goksel and Maria Gabrani", "title": "Hierarchical Graph Representations in Digital Pathology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Cancer diagnosis, prognosis, and therapy response predictions from tissue\nspecimens highly depend on the phenotype and topological distribution of\nconstituting histological entities. Thus, adequate tissue representations for\nencoding histological entities is imperative for computer aided cancer patient\ncare. To this end, several approaches have leveraged cell-graphs that encode\ncell morphology and organization to denote the tissue information. These allow\nfor utilizing machine learning to map tissue representations to tissue\nfunctionality to help quantify their relationship. Though cellular information\nis crucial, it is incomplete alone to comprehensively characterize complex\ntissue structure. We herein treat the tissue as a hierarchical composition of\nmultiple types of histological entities from fine to coarse level, capturing\nmultivariate tissue information at multiple levels. We propose a novel\nmulti-level hierarchical entity-graph representation of tissue specimens to\nmodel hierarchical compositions that encode histological entities as well as\ntheir intra- and inter-entity level interactions. Subsequently, a graph neural\nnetwork is proposed to operate on the hierarchical entity-graph representation\nto map the tissue structure to tissue functionality. Specifically, for input\nhistology images we utilize well-defined cells and tissue regions to build\nHierArchical Cell-to-Tissue (HACT) graph representations, and devise HACT-Net,\na graph neural network, to classify such HACT representations. As part of this\nwork, we introduce the BReAst Carcinoma Subtyping (BRACS) dataset, a large\ncohort of H&E stained breast tumor images, to evaluate our proposed methodology\nagainst pathologists and state-of-the-art approaches. Through comparative\nassessment and ablation studies, our method is demonstrated to yield superior\nclassification results compared to alternative methods as well as pathologists.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 14:30:57 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 09:09:02 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Pati", "Pushpak", ""], ["Jaume", "Guillaume", ""], ["Foncubierta", "Antonio", ""], ["Feroce", "Florinda", ""], ["Anniciello", "Anna Maria", ""], ["Scognamiglio", "Giosu\u00e8", ""], ["Brancati", "Nadia", ""], ["Fiche", "Maryse", ""], ["Dubruc", "Estelle", ""], ["Riccio", "Daniel", ""], ["Di Bonito", "Maurizio", ""], ["De Pietro", "Giuseppe", ""], ["Botti", "Gerardo", ""], ["Thiran", "Jean-Philippe", ""], ["Frucci", "Maria", ""], ["Goksel", "Orcun", ""], ["Gabrani", "Maria", ""]]}, {"id": "2102.11068", "submitter": "Ning Liu", "authors": "Ning Liu, Geng Yuan, Zhengping Che, Xuan Shen, Xiaolong Ma, Qing Jin,\n  Jian Ren, Jian Tang, Sijia Liu, Yanzhi Wang", "title": "Lottery Ticket Preserves Weight Correlation: Is It Desirable or Not?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In deep model compression, the recent finding \"Lottery Ticket Hypothesis\"\n(LTH) (Frankle & Carbin, 2018) pointed out that there could exist a winning\nticket (i.e., a properly pruned sub-network together with original weight\ninitialization) that can achieve competitive performance than the original\ndense network. However, it is not easy to observe such winning property in many\nscenarios, where for example, a relatively large learning rate is used even if\nit benefits training the original dense model. In this work, we investigate the\nunderlying condition and rationale behind the winning property, and find that\nthe underlying reason is largely attributed to the correlation between\ninitialized weights and final-trained weights when the learning rate is not\nsufficiently large. Thus, the existence of winning property is correlated with\nan insufficient DNN pretraining, and is unlikely to occur for a well-trained\nDNN. To overcome this limitation, we propose the \"pruning & fine-tuning\" method\nthat consistently outperforms lottery ticket sparse training under the same\npruning algorithm and the same total training epochs. Extensive experiments\nover multiple deep models (VGG, ResNet, MobileNet-v2) on different datasets\nhave been conducted to justify our proposals.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 14:49:46 GMT"}, {"version": "v2", "created": "Sun, 18 Jul 2021 08:19:07 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Liu", "Ning", ""], ["Yuan", "Geng", ""], ["Che", "Zhengping", ""], ["Shen", "Xuan", ""], ["Ma", "Xiaolong", ""], ["Jin", "Qing", ""], ["Ren", "Jian", ""], ["Tang", "Jian", ""], ["Liu", "Sijia", ""], ["Wang", "Yanzhi", ""]]}, {"id": "2102.11099", "submitter": "Shunjie Dong", "authors": "Shunjie Dong and Qianqian Yang and Yu Fu and Mei Tian and Cheng Zhuo", "title": "RCoNet: Deformable Mutual Information Maximization and High-order\n  Uncertainty-aware Learning for Robust COVID-19 Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The novel 2019 Coronavirus (COVID-19) infection has spread world widely and\nis currently a major healthcare challenge around the world. Chest Computed\nTomography (CT) and X-ray images have been well recognized to be two effective\ntechniques for clinical COVID-19 disease diagnoses. Due to faster imaging time\nand considerably lower cost than CT, detecting COVID-19 in chest X-ray (CXR)\nimages is preferred for efficient diagnosis, assessment and treatment. However,\nconsidering the similarity between COVID-19 and pneumonia, CXR samples with\ndeep features distributed near category boundaries are easily misclassified by\nthe hyper-planes learned from limited training data. Moreover, most existing\napproaches for COVID-19 detection focus on the accuracy of prediction and\noverlook the uncertainty estimation, which is particularly important when\ndealing with noisy datasets. To alleviate these concerns, we propose a novel\ndeep network named {\\em RCoNet$^k_s$} for robust COVID-19 detection which\nemploys {\\em Deformable Mutual Information Maximization} (DeIM), {\\em Mixed\nHigh-order Moment Feature} (MHMF) and {\\em Multi-expert Uncertainty-aware\nLearning} (MUL). With DeIM, the mutual information (MI) between input data and\nthe corresponding latent representations can be well estimated and maximized to\ncapture compact and disentangled representational characteristics. Meanwhile,\nMHMF can fully explore the benefits of using high-order statistics and extract\ndiscriminative features of complex distributions in medical imaging. Finally,\nMUL creates multiple parallel dropout networks for each CXR image to evaluate\nuncertainty and thus prevent performance degradation caused by the noise in the\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 15:13:42 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Dong", "Shunjie", ""], ["Yang", "Qianqian", ""], ["Fu", "Yu", ""], ["Tian", "Mei", ""], ["Zhuo", "Cheng", ""]]}, {"id": "2102.11115", "submitter": "Adam Dahlgren Lindstr\\\"om", "authors": "Adam Dahlgren Lindstr\\\"om, Suna Bensch, Johanna Bj\\\"orklund, Frank\n  Drewes", "title": "Probing Multimodal Embeddings for Linguistic Properties: the\n  Visual-Semantic Case", "comments": "Submitted July 1 2020, COLING 2020 main conference", "journal-ref": null, "doi": "10.18653/v1/2020.coling-main.64", "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic embeddings have advanced the state of the art for countless natural\nlanguage processing tasks, and various extensions to multimodal domains, such\nas visual-semantic embeddings, have been proposed. While the power of\nvisual-semantic embeddings comes from the distillation and enrichment of\ninformation through machine learning, their inner workings are poorly\nunderstood and there is a shortage of analysis tools. To address this problem,\nwe generalize the notion of probing tasks to the visual-semantic case. To this\nend, we (i) discuss the formalization of probing tasks for embeddings of\nimage-caption pairs, (ii) define three concrete probing tasks within our\ngeneral framework, (iii) train classifiers to probe for those properties, and\n(iv) compare various state-of-the-art embeddings under the lens of the proposed\nprobing tasks. Our experiments reveal an up to 12% increase in accuracy on\nvisual-semantic embeddings compared to the corresponding unimodal embeddings,\nwhich suggest that the text and image dimensions represented in the former do\ncomplement each other.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 15:47:04 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Lindstr\u00f6m", "Adam Dahlgren", ""], ["Bensch", "Suna", ""], ["Bj\u00f6rklund", "Johanna", ""], ["Drewes", "Frank", ""]]}, {"id": "2102.11121", "submitter": "Pedro Felzenszwalb", "authors": "Jeova F. S. Rocha Neto, Pedro Felzenszwalb, Marilyn Vazquez", "title": "Direct Estimation of Appearance Models for Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image segmentation algorithms often depend on appearance models that\ncharacterize the distribution of pixel values in different image regions. We\ndescribe a novel approach for estimating appearance models directly from an\nimage, without explicit consideration of the pixels that make up each region.\nOur approach is based on algebraic expressions that relate local image\nstatistics to the appearance models of spatially coherent regions. We describe\ntwo algorithms that can use the aforementioned algebraic expressions for\nestimating appearance models. The first algorithm is based on solving a system\nof linear and quadratic equations. The second algorithm is a spectral method\nbased on an eigenvector computation. We present experimental results that\ndemonstrate the proposed methods work well in practice and lead to effective\nimage segmentation algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 15:50:39 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Neto", "Jeova F. S. Rocha", ""], ["Felzenszwalb", "Pedro", ""], ["Vazquez", "Marilyn", ""]]}, {"id": "2102.11126", "submitter": "Deressa Wodajo", "authors": "Deressa Wodajo, Solomon Atnafu", "title": "Deepfake Video Detection Using Convolutional Vision Transformer", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapid advancement of deep learning models that can generate and synthesis\nhyper-realistic videos known as Deepfakes and their ease of access to the\ngeneral public have raised concern from all concerned bodies to their possible\nmalicious intent use. Deep learning techniques can now generate faces, swap\nfaces between two subjects in a video, alter facial expressions, change gender,\nand alter facial features, to list a few. These powerful video manipulation\nmethods have potential use in many fields. However, they also pose a looming\nthreat to everyone if used for harmful purposes such as identity theft,\nphishing, and scam. In this work, we propose a Convolutional Vision Transformer\nfor the detection of Deepfakes. The Convolutional Vision Transformer has two\ncomponents: Convolutional Neural Network (CNN) and Vision Transformer (ViT).\nThe CNN extracts learnable features while the ViT takes in the learned features\nas input and categorizes them using an attention mechanism. We trained our\nmodel on the DeepFake Detection Challenge Dataset (DFDC) and have achieved 91.5\npercent accuracy, an AUC value of 0.91, and a loss value of 0.32. Our\ncontribution is that we have added a CNN module to the ViT architecture and\nhave achieved a competitive result on the DFDC dataset.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 15:56:05 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 14:38:28 GMT"}, {"version": "v3", "created": "Thu, 11 Mar 2021 13:45:17 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Wodajo", "Deressa", ""], ["Atnafu", "Solomon", ""]]}, {"id": "2102.11132", "submitter": "Xuehao Liu", "authors": "Xuehao Liu, Sarah Jane Delany, Susan McKeever", "title": "Wider Vision: Enriching Convolutional Neural Networks via Alignment to\n  External Knowledge Bases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Deep learning models suffer from opaqueness. For Convolutional Neural\nNetworks (CNNs), current research strategies for explaining models focus on the\ntarget classes within the associated training dataset. As a result, the\nunderstanding of hidden feature map activations is limited by the\ndiscriminative knowledge gleaned during training. The aim of our work is to\nexplain and expand CNNs models via the mirroring or alignment of CNN to an\nexternal knowledge base. This will allow us to give a semantic context or label\nfor each visual feature. We can match CNN feature activations to nodes in our\nexternal knowledge base. This supports knowledge-based interpretation of the\nfeatures associated with model decisions. To demonstrate our approach, we build\ntwo separate graphs. We use an entity alignment method to align the feature\nnodes in a CNN with the nodes in a ConceptNet based knowledge graph. We then\nmeasure the proximity of CNN graph nodes to semantically meaningful knowledge\nbase nodes. Our results show that in the aligned embedding space, nodes from\nthe knowledge graph are close to the CNN feature nodes that have similar\nmeanings, indicating that nodes from an external knowledge base can act as\nexplanatory semantic references for features in the model. We analyse a variety\nof graph building methods in order to improve the results from our embedding\nspace. We further demonstrate that by using hierarchical relationships from our\nexternal knowledge base, we can locate new unseen classes outside the CNN\ntraining set in our embeddings space, based on visual feature activations. This\nsuggests that we can adapt our approach to identify unseen classes based on CNN\nfeature activations. Our demonstrated approach of aligning a CNN with an\nexternal knowledge base paves the way to reason about and beyond the trained\nmodel, with future adaptations to explainable models and zero-shot learning.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 16:00:03 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Liu", "Xuehao", ""], ["Delany", "Sarah Jane", ""], ["McKeever", "Susan", ""]]}, {"id": "2102.11149", "submitter": "Ruiwen Zhang", "authors": "Ruiwen Zhang and Zhidong Deng and Hongsen Lin and Hongchao Lu", "title": "Phase Space Reconstruction Network for Lane Intrusion Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a complex road traffic scene, illegal lane intrusion of pedestrians or\ncyclists constitutes one of the main safety challenges in autonomous driving\napplication. In this paper, we propose a novel object-level phase space\nreconstruction network (PSRNet) for motion time series classification, aiming\nto recognize lane intrusion actions that occur 150m ahead through a monocular\ncamera fixed on moving vehicle. In the PSRNet, the movement of pedestrians and\ncyclists, specifically viewed as an observable object-level dynamic process,\ncan be reconstructed as trajectories of state vectors in a latent phase space\nand further characterized by a learnable Lyapunov exponent-like classifier that\nindicates discrimination in terms of average exponential divergence of state\ntrajectories. Additionally, in order to first transform video inputs into\none-dimensional motion time series of each object, a lane width normalization\nbased on visual object tracking-by-detection is presented. Extensive\nexperiments are conducted on the THU-IntrudBehavior dataset collected from real\nurban roads. The results show that our PSRNet could reach the best accuracy of\n98.0%, which remarkably exceeds existing action recognition approaches by more\nthan 30%.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 16:18:35 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Zhang", "Ruiwen", ""], ["Deng", "Zhidong", ""], ["Lin", "Hongsen", ""], ["Lu", "Hongchao", ""]]}, {"id": "2102.11158", "submitter": "Shuxiao Chen", "authors": "Qinqing Zheng, Shuxiao Chen, Qi Long, Weijie J. Su", "title": "Federated $f$-Differential Privacy", "comments": "Accepted to AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is a training paradigm where the clients\ncollaboratively learn models by repeatedly sharing information without\ncompromising much on the privacy of their local sensitive data. In this paper,\nwe introduce federated $f$-differential privacy, a new notion specifically\ntailored to the federated setting, based on the framework of Gaussian\ndifferential privacy. Federated $f$-differential privacy operates on record\nlevel: it provides the privacy guarantee on each individual record of one\nclient's data against adversaries. We then propose a generic private federated\nlearning framework {PriFedSync} that accommodates a large family of\nstate-of-the-art FL algorithms, which provably achieves federated\n$f$-differential privacy. Finally, we empirically demonstrate the trade-off\nbetween privacy guarantee and prediction performance for models trained by\n{PriFedSync} in computer vision tasks.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 16:28:21 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Zheng", "Qinqing", ""], ["Chen", "Shuxiao", ""], ["Long", "Qi", ""], ["Su", "Weijie J.", ""]]}, {"id": "2102.11163", "submitter": "Niklas Smedemark-Margulies", "authors": "Niklas Smedemark-Margulies, Jung Yeon Park, Max Daniels, Rose Yu,\n  Jan-Willem van de Meent, Paul Hand", "title": "Generator Surgery for Compressed Sensing", "comments": "Code available at: https://github.com/nik-sm/generator-surgery", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image recovery from compressive measurements requires a signal prior for the\nimages being reconstructed. Recent work has explored the use of deep generative\nmodels with low latent dimension as signal priors for such problems. However,\ntheir recovery performance is limited by high representation error. We\nintroduce a method for achieving low representation error using generators as\nsignal priors. Using a pre-trained generator, we remove one or more initial\nblocks at test time and optimize over the new, higher-dimensional latent space\nto recover a target image. Experiments demonstrate significantly improved\nreconstruction quality for a variety of network architectures. This approach\nalso works well for out-of-training-distribution images and is competitive with\nother state-of-the-art methods. Our experiments show that test-time\narchitectural modifications can greatly improve the recovery quality of\ngenerator signal priors for compressed sensing.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 16:38:57 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 01:57:30 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Smedemark-Margulies", "Niklas", ""], ["Park", "Jung Yeon", ""], ["Daniels", "Max", ""], ["Yu", "Rose", ""], ["van de Meent", "Jan-Willem", ""], ["Hand", "Paul", ""]]}, {"id": "2102.11228", "submitter": "Juan Marcos Ramirez Rond\\'on", "authors": "Juan Ram\\'irez, H\\'ector Vargas, Jos\\'e Ignacio Mart\\'inez, Henry\n  Arguello", "title": "Subspace-Based Feature Fusion From Hyperspectral And Multispectral Image\n  For Land Cover Classification", "comments": "4 pages, 2 figures, 1 table, and 2 algorithms. Submitted to the\n  International Geoscience and Remote Sensing Symposium (2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In remote sensing, hyperspectral (HS) and multispectral (MS) image fusion\nhave emerged as a synthesis tool to improve the data set resolution. However,\nconventional image fusion methods typically degrade the performance of the land\ncover classification. In this paper, a feature fusion method from HS and MS\nimages for pixel-based classification is proposed. More precisely, the proposed\nmethod first extracts spatial features from the MS image using morphological\nprofiles. Then, the feature fusion model assumes that both the extracted\nmorphological profiles and the HS image can be described as a feature matrix\nlying in different subspaces. An algorithm based on combining alternating\noptimization (AO) and the alternating direction method of multipliers (ADMM) is\ndeveloped to solve efficiently the feature fusion problem. Finally, extensive\nsimulations were run to evaluate the performance of the proposed feature fusion\napproach for two data sets. In general, the proposed approach exhibits a\ncompetitive performance compared to other feature extraction methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 17:59:18 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Ram\u00edrez", "Juan", ""], ["Vargas", "H\u00e9ctor", ""], ["Mart\u00ednez", "Jos\u00e9 Ignacio", ""], ["Arguello", "Henry", ""]]}, {"id": "2102.11237", "submitter": "Sulabh Katiyar", "authors": "Sulabh Katiyar, Samir Kumar Borgohain", "title": "Image Captioning using Deep Stacked LSTMs, Contextual Word Embeddings\n  and Data Augmentation", "comments": "Accepted for publication in Springer Book Series: Advances in\n  Intelligent Systems and Computing - ISSN 2194-5357. Upon publication, this\n  article will point to the published one", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Image Captioning, or the automatic generation of descriptions for images, is\none of the core problems in Computer Vision and has seen considerable progress\nusing Deep Learning Techniques. We propose to use Inception-ResNet\nConvolutional Neural Network as encoder to extract features from images,\nHierarchical Context based Word Embeddings for word representations and a Deep\nStacked Long Short Term Memory network as decoder, in addition to using Image\nData Augmentation to avoid over-fitting. For data Augmentation, we use\nHorizontal and Vertical Flipping in addition to Perspective Transformations on\nthe images. We evaluate our proposed methods with two image captioning\nframeworks- Encoder-Decoder and Soft Attention. Evaluation on widely used\nmetrics have shown that our approach leads to considerable improvement in model\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 18:15:39 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Katiyar", "Sulabh", ""], ["Borgohain", "Samir Kumar", ""]]}, {"id": "2102.11262", "submitter": "Lei Ding", "authors": "Lei Ding, Hao Tang, Yahui Liu, Yilei Shi, Xiao Xiang Zhu and Lorenzo\n  Bruzzone", "title": "Adversarial Shape Learning for Building Extraction in VHR Remote Sensing\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building extraction in VHR RSIs remains a challenging task due to occlusion\nand boundary ambiguity problems. Although conventional convolutional neural\nnetworks (CNNs) based methods are capable of exploiting local texture and\ncontext information, they fail to capture the shape patterns of buildings,\nwhich is a necessary constraint in the human recognition. To address this\nissue, we propose an adversarial shape learning network (ASLNet) to model the\nbuilding shape patterns that improve the accuracy of building segmentation. In\nthe proposed ASLNet, we introduce the adversarial learning strategy to\nexplicitly model the shape constraints, as well as a CNN shape regularizer to\nstrengthen the embedding of shape features. To assess the geometric accuracy of\nbuilding segmentation results, we introduced several object-based quality\nassessment metrics. Experiments on two open benchmark datasets show that the\nproposed ASLNet improves both the pixel-based accuracy and the object-based\nquality measurements by a large margin. The code is available at:\nhttps://github.com/ggsDing/ASLNet\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 18:49:43 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 13:58:51 GMT"}, {"version": "v3", "created": "Tue, 9 Mar 2021 20:59:18 GMT"}, {"version": "v4", "created": "Wed, 17 Mar 2021 10:16:18 GMT"}, {"version": "v5", "created": "Tue, 30 Mar 2021 22:12:26 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Ding", "Lei", ""], ["Tang", "Hao", ""], ["Liu", "Yahui", ""], ["Shi", "Yilei", ""], ["Zhu", "Xiao Xiang", ""], ["Bruzzone", "Lorenzo", ""]]}, {"id": "2102.11263", "submitter": "Kripasindhu Sarkar", "authors": "Kripasindhu Sarkar and Vladislav Golyanik and Lingjie Liu and\n  Christian Theobalt", "title": "Style and Pose Control for Image Synthesis of Humans from a Single\n  Monocular View", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photo-realistic re-rendering of a human from a single image with explicit\ncontrol over body pose, shape and appearance enables a wide range of\napplications, such as human appearance transfer, virtual try-on, motion\nimitation, and novel view synthesis. While significant progress has been made\nin this direction using learning-based image generation tools, such as GANs,\nexisting approaches yield noticeable artefacts such as blurring of fine\ndetails, unrealistic distortions of the body parts and garments as well as\nsevere changes of the textures. We, therefore, propose a new method for\nsynthesising photo-realistic human images with explicit control over pose and\npart-based appearance, i.e., StylePoseGAN, where we extend a non-controllable\ngenerator to accept conditioning of pose and appearance separately. Our network\ncan be trained in a fully supervised way with human images to disentangle pose,\nappearance and body parts, and it significantly outperforms existing single\nimage re-rendering methods. Our disentangled representation opens up further\napplications such as garment transfer, motion transfer, virtual try-on, head\n(identity) swap and appearance interpolation. StylePoseGAN achieves\nstate-of-the-art image generation fidelity on common perceptual metrics\ncompared to the current best-performing methods and convinces in a\ncomprehensive user study.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 18:50:47 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Sarkar", "Kripasindhu", ""], ["Golyanik", "Vladislav", ""], ["Liu", "Lingjie", ""], ["Theobalt", "Christian", ""]]}, {"id": "2102.11273", "submitter": "Eric Mintun", "authors": "Eric Mintun, Alexander Kirillov, and Saining Xie", "title": "On Interaction Between Augmentations and Corruptions in Natural\n  Corruption Robustness", "comments": "8+13 pages, 6+10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Invariance to a broad array of image corruptions, such as warping, noise, or\ncolor shifts, is an important aspect of building robust models in computer\nvision. Recently, several new data augmentations have been proposed that\nsignificantly improve performance on ImageNet-C, a benchmark of such\ncorruptions. However, there is still a lack of basic understanding on the\nrelationship between data augmentations and test-time corruptions. To this end,\nwe develop a feature space for image transforms, and then use a new measure in\nthis space between augmentations and corruptions called the Minimal Sample\nDistance to demonstrate there is a strong correlation between similarity and\nperformance. We then investigate recent data augmentations and observe a\nsignificant degradation in corruption robustness when the test-time corruptions\nare sampled to be perceptually dissimilar from ImageNet-C in this feature\nspace. Our results suggest that test error can be improved by training on\nperceptually similar augmentations, and data augmentations may not generalize\nwell beyond the existing benchmark. We hope our results and tools will allow\nfor more robust progress towards improving robustness to image corruptions.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 18:58:39 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Mintun", "Eric", ""], ["Kirillov", "Alexander", ""], ["Xie", "Saining", ""]]}, {"id": "2102.11318", "submitter": "Falguni Laljibhai Patel", "authors": "Falguni Patel, NirmalKumar Patel, Santosh Kumar Bharti", "title": "Lie-Sensor: A Live Emotion Verifier or a Licensor for Chat Applications\n  using Emotional Intelligence", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Veracity is an essential key in research and development of innovative\nproducts. Live Emotion analysis and verification nullify deceit made to\ncomplainers on live chat, corroborate messages of both ends in messaging apps\nand promote an honest conversation between users. The main concept behind this\nemotion artificial intelligent verifier is to license or decline message\naccountability by comparing variegated emotions of chat app users recognized\nthrough facial expressions and text prediction. In this paper, a proposed\nemotion intelligent live detector acts as an honest arbiter who distributes\nfacial emotions into labels namely, Happiness, Sadness, Surprise, and Hate.\nFurther, it separately predicts a label of messages through text\nclassification. Finally, it compares both labels and declares the message as a\nfraud or a bonafide. For emotion detection, we deployed Convolutional Neural\nNetwork (CNN) using a miniXception model and for text prediction, we selected\nSupport Vector Machine (SVM) natural language processing probability classifier\ndue to receiving the best accuracy on training dataset after applying Support\nVector Machine (SVM), Random Forest Classifier, Naive Bayes Classifier, and\nLogistic regression.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 02:47:30 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Patel", "Falguni", ""], ["Patel", "NirmalKumar", ""], ["Bharti", "Santosh Kumar", ""]]}, {"id": "2102.11343", "submitter": "Prakhar Kaushik", "authors": "Prakhar Kaushik, Alex Gain, Adam Kortylewski and Alan Yuille", "title": "Understanding Catastrophic Forgetting and Remembering in Continual\n  Learning with Optimal Relevance Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Catastrophic forgetting in neural networks is a significant problem for\ncontinual learning. A majority of the current methods replay previous data\nduring training, which violates the constraints of an ideal continual learning\nsystem. Additionally, current approaches that deal with forgetting ignore the\nproblem of catastrophic remembering, i.e. the worsening ability to discriminate\nbetween data from different tasks. In our work, we introduce Relevance Mapping\nNetworks (RMNs) which are inspired by the Optimal Overlap Hypothesis. The\nmappings reflects the relevance of the weights for the task at hand by\nassigning large weights to essential parameters. We show that RMNs learn an\noptimized representational overlap that overcomes the twin problem of\ncatastrophic forgetting and remembering. Our approach achieves state-of-the-art\nperformance across all common continual learning datasets, even significantly\noutperforming data replay methods while not violating the constraints for an\nideal continual learning system. Moreover, RMNs retain the ability to detect\ndata from new tasks in an unsupervised manner, thus proving their resilience\nagainst catastrophic remembering.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 20:34:00 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Kaushik", "Prakhar", ""], ["Gain", "Alex", ""], ["Kortylewski", "Adam", ""], ["Yuille", "Alan", ""]]}, {"id": "2102.11356", "submitter": "Raid Al-Nima", "authors": "Raid R. Al-Nima, Ali N. Hamoodi, Radhwan Y. Al-Jawadi and Ziad S.\n  Mohammad", "title": "Shadow Image Enlargement Distortion Removal", "comments": "7 pages, 6 figures and 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This project aims to adopt preprocessing operations to get less distortions\nfor shadow image enlargement. The preprocessing operations consists of three\nmain steps: first enlarge the original shadow image by using any kind of\ninterpolation methods, second apply average filter to the enlargement image and\nfinally apply the unsharp filter to the previous averaged image. These\npreprocessing operations leads to get an enlargement image very close to the\noriginal enlarge image for the same shadow image. Then comparisons established\nbetween the adopted image and original image by using different types of\ninterpolation and different alfa values for unsharp filter to reach the best\nway which have less different errors between the two images.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 20:54:57 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Al-Nima", "Raid R.", ""], ["Hamoodi", "Ali N.", ""], ["Al-Jawadi", "Radhwan Y.", ""], ["Mohammad", "Ziad S.", ""]]}, {"id": "2102.11361", "submitter": "Xavier Gonzalez I", "authors": "Xavier Ignacio Gonz\\'alez", "title": "The FaCells. An Exploratory Study about LSTM Layers on Face Sketches\n  Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Lines are human mental abstractions. A bunch of lines may form a drawing. A\nset of drawings can feed an LSTM network input layer, considering each draw as\na list of lines and a line a list of points. This paper proposes the pointless\nmotive to classify the gender of celebrities' portraits as an excuse for\nexploration in a broad, more artistic sense. Investigation results drove\ncompelling ideas here discussed. The experiments compared different ways to\nrepresent draws to be input in a network and showed that an absolute format of\ncoordinates (x, y) was a better performer than a relative one (Dx, Dy) with\nrespect to prior points, most frequent in the reviewed literature. Experiments\nalso showed that, due to the recurrent nature of LSTMs, the order of lines\nforming a drawing is a relevant factor for input in an LSTM classifier not\nstudied before. A minimum 'pencil' traveled length criteria for line ordering\nproved suitable, possible by reducing it to a TSP particular instance. The best\nconfiguration for gender classification appears with an LSTM layer that returns\nthe hidden state value for each input point step, followed by a global average\nlayer along the sequence, before the output dense layer. That result guided the\nidea of removing the average in the network pipeline and return a per-point\nattribute score just by adjusting tensors dimensions. With this trick, the\nmodel detects an attribute in a drawing and also recognizes the points linked\nto it. Moreover, by overlapping filtered lines of portraits, an attribute's\nvisual essence is depicted. Meet the FaCells.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 21:05:57 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Gonz\u00e1lez", "Xavier Ignacio", ""]]}, {"id": "2102.11382", "submitter": "Xinyu Gong", "authors": "Xinyu Gong, Wuyang Chen, Tianlong Chen and Zhangyang Wang", "title": "Sandwich Batch Normalization", "comments": "Codes are available at\n  https://github.com/VITA-Group/Sandwich-Batch-Normalization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Sandwich Batch Normalization (SaBN), an embarrassingly easy\nimprovement of Batch Normalization (BN) with only a few lines of code changes.\nSaBN is motivated by addressing the inherent feature distribution heterogeneity\nthat one can be identified in many tasks, which can arise from data\nheterogeneity (multiple input domains) or model heterogeneity (dynamic\narchitectures, model conditioning, etc.). Our SaBN factorizes the BN affine\nlayer into one shared sandwich affine layer, cascaded by several parallel\nindependent affine layers. Concrete analysis reveals that, during optimization,\nSaBN promotes balanced gradient norms while still preserving diverse gradient\ndirections: a property that many application tasks seem to favor. We\ndemonstrate the prevailing effectiveness of SaBN as a drop-in replacement in\nfour tasks: $\\textbf{conditional image generation}$, $\\textbf{neural\narchitecture search}$ (NAS), $\\textbf{adversarial training}$, and\n$\\textbf{arbitrary style transfer}$. Leveraging SaBN immediately achieves\nbetter Inception Score and FID on CIFAR-10 and ImageNet conditional image\ngeneration with three state-of-the-art GANs; boosts the performance of a\nstate-of-the-art weight-sharing NAS algorithm significantly on NAS-Bench-201;\nsubstantially improves the robust and standard accuracies for adversarial\ndefense; and produces superior arbitrary stylized results. We also provide\nvisualizations and analysis to help understand why SaBN works. Codes are\navailable at https://github.com/VITA-Group/Sandwich-Batch-Normalization.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 22:09:43 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Gong", "Xinyu", ""], ["Chen", "Wuyang", ""], ["Chen", "Tianlong", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2102.11385", "submitter": "Masuda Tonima", "authors": "Masuda Akter Tonima, Fatemeh Esfahani, Austin Dehart, Youmin Zhang", "title": "Lightweight Combinational Machine Learning Algorithm for Sorting Canine\n  Torso Radiographs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The veterinary field lacks automation in contrast to the tremendous\ntechnological advances made in the human medical field. Implementation of\nmachine learning technology can shorten any step of the automation process.\nThis paper explores these core concepts and starts with automation in sorting\nradiographs for canines by view and anatomy. This is achieved by developing a\nnew lightweight algorithm inspired by AlexNet, Inception, and SqueezeNet. The\nproposed module proves to be lighter than SqueezeNet while maintaining accuracy\nhigher than that of AlexNet, ResNet, DenseNet, and SqueezeNet.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 22:17:33 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Tonima", "Masuda Akter", ""], ["Esfahani", "Fatemeh", ""], ["Dehart", "Austin", ""], ["Zhang", "Youmin", ""]]}, {"id": "2102.11393", "submitter": "Wei Zhou", "authors": "Wei Zhou, Jiahua Xu, Qiuping Jiang, Zhibo Chen", "title": "No-Reference Quality Assessment for 360-degree Images by Analysis of\n  Multi-frequency Information and Local-global Naturalness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  360-degree/omnidirectional images (OIs) have achieved remarkable attentions\ndue to the increasing applications of virtual reality (VR). Compared to\nconventional 2D images, OIs can provide more immersive experience to consumers,\nbenefitting from the higher resolution and plentiful field of views (FoVs).\nMoreover, observing OIs is usually in the head mounted display (HMD) without\nreferences. Therefore, an efficient blind quality assessment method, which is\nspecifically designed for 360-degree images, is urgently desired. In this\npaper, motivated by the characteristics of the human visual system (HVS) and\nthe viewing process of VR visual contents, we propose a novel and effective\nno-reference omnidirectional image quality assessment (NR OIQA) algorithm by\nMulti-Frequency Information and Local-Global Naturalness (MFILGN).\nSpecifically, inspired by the frequency-dependent property of visual cortex, we\nfirst decompose the projected equirectangular projection (ERP) maps into\nwavelet subbands. Then, the entropy intensities of low and high frequency\nsubbands are exploited to measure the multi-frequency information of OIs.\nBesides, except for considering the global naturalness of ERP maps, owing to\nthe browsed FoVs, we extract the natural scene statistics features from each\nviewport image as the measure of local naturalness. With the proposed\nmulti-frequency information measurement and local-global naturalness\nmeasurement, we utilize support vector regression as the final image quality\nregressor to train the quality evaluation model from visual quality-related\nfeatures to human ratings. To our knowledge, the proposed model is the first\nno-reference quality assessment method for 360-degreee images that combines\nmulti-frequency information and image naturalness. Experimental results on two\npublicly available OIQA databases demonstrate that our proposed MFILGN\noutperforms state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 22:52:35 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Zhou", "Wei", ""], ["Xu", "Jiahua", ""], ["Jiang", "Qiuping", ""], ["Chen", "Zhibo", ""]]}, {"id": "2102.11395", "submitter": "Ghani Lawal Mr.", "authors": "Ghani O. Lawal and Michael Greenspan", "title": "Procam Calibration from a Single Pose of a Planar Target", "comments": "11 pages, 9 figures, 10 tables. Submitted to the VISAPP Conference.\n  Stored in the SciTepress Digital Library:\n  https://www.scitepress.org/PublicationsDetail.aspx?ID=rGG70YCQyOs=&t=1", "journal-ref": "In Proceedings of the 16th International Joint Conference on\n  Computer Vision, Imaging and Computer Graphics Theory and Applications\n  (VISIGRAPP 2021) - Volume 5: VISAPP, pages 817-827", "doi": "10.5220/0010327708170827", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel user friendly method is proposed for calibrating a procam system from\na single pose of a planar chessboard target. The user simply needs to orient\nthe chessboard in a single appropriate pose. A sequence of Gray Code patterns\nare projected onto the chessboard, which allows correspondences between the\ncamera, projector and the chessboard to be automatically extracted. These\ncorrespondences are fed as input to a nonlinear optimization method that models\nthe projector of the principle points onto the chessboard, and accurately\ncalculates the intrinsic and extrinsic parameters of both the camera and the\nprojector, as well as the camera's distortion coefficients. The method is\nexperimentally validated on the procam system, which is shown to be comparable\nin accuracy with existing multi-pose approaches. The impact of the orientation\nof the chessboard with respect to the procam imaging places is also explored\nthrough extensive simulation.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 22:53:29 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Lawal", "Ghani O.", ""], ["Greenspan", "Michael", ""]]}, {"id": "2102.11396", "submitter": "Donghui Yan", "authors": "Donghui Yan, Jian Zou, Zhenpeng Li", "title": "Learning Low-dimensional Manifolds for Scoring of Tissue Microarray\n  Images", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Tissue microarray (TMA) images have emerged as an important high-throughput\ntool for cancer study and the validation of biomarkers. Efforts have been\ndedicated to further improve the accuracy of TACOMA, a cutting-edge automatic\nscoring algorithm for TMA images. One major advance is due to deepTacoma, an\nalgorithm that incorporates suitable deep representations of a group nature.\nInspired by the recent advance in semi-supervised learning and deep learning,\nwe propose mfTacoma to learn alternative deep representations in the context of\nTMA image scoring. In particular, mfTacoma learns the low-dimensional\nmanifolds, a common latent structure in high dimensional data. Deep\nrepresentation learning and manifold learning typically requires large data. By\nencoding deep representation of the manifolds as regularizing features,\nmfTacoma effectively leverages the manifold information that is potentially\ncrude due to small data. Our experiments show that deep features by manifolds\noutperforms two alternatives -- deep features by linear manifolds with\nprincipal component analysis or by leveraging the group property.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 22:55:04 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Yan", "Donghui", ""], ["Zou", "Jian", ""], ["Li", "Zhenpeng", ""]]}, {"id": "2102.11456", "submitter": "Jiahong Ouyang", "authors": "Jiahong Ouyang, Ehsan Adeli, Kilian M. Pohl, Qingyu Zhao, Greg\n  Zaharchuk", "title": "Representation Disentanglement for Multi-modal brain MR Analysis", "comments": "Accepted by Information Processing in Medical Imaging (IPMI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modal MRIs are widely used in neuroimaging applications since different\nMR sequences provide complementary information about brain structures. Recent\nworks have suggested that multi-modal deep learning analysis can benefit from\nexplicitly disentangling anatomical (shape) and modality (appearance)\ninformation into separate image presentations. In this work, we challenge\nmainstream strategies by showing that they do not naturally lead to\nrepresentation disentanglement both in theory and in practice. To address this\nissue, we propose a margin loss that regularizes the similarity in\nrelationships of the representations across subjects and modalities. To enable\nrobust training, we further use a conditional convolution to design a single\nmodel for encoding images of all modalities. Lastly, we propose a fusion\nfunction to combine the disentangled anatomical representations as a set of\nmodality-invariant features for downstream tasks. We evaluate the proposed\nmethod on three multi-modal neuroimaging datasets. Experiments show that our\nproposed method can achieve superior disentangled representations compared to\nexisting disentanglement strategies. Results also indicate that the fused\nanatomical representation has potential in the downstream task of zero-dose PET\nreconstruction and brain tumor segmentation. The code is available at\n\\url{https://github.com/ouyangjiahong/representation-disentanglement}.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 02:08:38 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 06:41:49 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Ouyang", "Jiahong", ""], ["Adeli", "Ehsan", ""], ["Pohl", "Kilian M.", ""], ["Zhao", "Qingyu", ""], ["Zaharchuk", "Greg", ""]]}, {"id": "2102.11464", "submitter": "Zhiliang Xu", "authors": "Zhiliang Xu, Xiyu Yu, Zhibin Hong, Zhen Zhu, Junyu Han, Jingtuo Liu,\n  Errui Ding, Xiang Bai", "title": "FaceController: Controllable Attribute Editing for Face in the Wild", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face attribute editing aims to generate faces with one or multiple desired\nface attributes manipulated while other details are preserved. Unlike prior\nworks such as GAN inversion, which has an expensive reverse mapping process, we\npropose a simple feed-forward network to generate high-fidelity manipulated\nfaces. By simply employing some existing and easy-obtainable prior information,\nour method can control, transfer, and edit diverse attributes of faces in the\nwild. The proposed method can consequently be applied to various applications\nsuch as face swapping, face relighting, and makeup transfer. In our method, we\ndecouple identity, expression, pose, and illumination using 3D priors; separate\ntexture and colors by using region-wise style codes. All the information is\nembedded into adversarial learning by our identity-style normalization module.\nDisentanglement losses are proposed to enhance the generator to extract\ninformation independently from each attribute. Comprehensive quantitative and\nqualitative evaluations have been conducted. In a single framework, our method\nachieves the best or competitive scores on a variety of face applications.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 02:47:28 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Xu", "Zhiliang", ""], ["Yu", "Xiyu", ""], ["Hong", "Zhibin", ""], ["Zhu", "Zhen", ""], ["Han", "Junyu", ""], ["Liu", "Jingtuo", ""], ["Ding", "Errui", ""], ["Bai", "Xiang", ""]]}, {"id": "2102.11467", "submitter": "Saahil Jain", "authors": "Saahil Jain, Akshay Smit, Steven QH Truong, Chanh DT Nguyen,\n  Minh-Thanh Huynh, Mudit Jain, Victoria A. Young, Andrew Y. Ng, Matthew P.\n  Lungren, Pranav Rajpurkar", "title": "VisualCheXbert: Addressing the Discrepancy Between Radiology Report\n  Labels and Image Labels", "comments": "Accepted to ACM Conference on Health, Inference, and Learning\n  (ACM-CHIL) 2021", "journal-ref": null, "doi": "10.1145/3450439.3451862", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic extraction of medical conditions from free-text radiology reports\nis critical for supervising computer vision models to interpret medical images.\nIn this work, we show that radiologists labeling reports significantly disagree\nwith radiologists labeling corresponding chest X-ray images, which reduces the\nquality of report labels as proxies for image labels. We develop and evaluate\nmethods to produce labels from radiology reports that have better agreement\nwith radiologists labeling images. Our best performing method, called\nVisualCheXbert, uses a biomedically-pretrained BERT model to directly map from\na radiology report to the image labels, with a supervisory signal determined by\na computer vision model trained to detect medical conditions from chest X-ray\nimages. We find that VisualCheXbert outperforms an approach using an existing\nradiology report labeler by an average F1 score of 0.14 (95% CI 0.12, 0.17). We\nalso find that VisualCheXbert better agrees with radiologists labeling chest\nX-ray images than do radiologists labeling the corresponding radiology reports\nby an average F1 score across several medical conditions of between 0.12 (95%\nCI 0.09, 0.15) and 0.21 (95% CI 0.18, 0.24).\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 03:02:36 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 07:06:44 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Jain", "Saahil", ""], ["Smit", "Akshay", ""], ["Truong", "Steven QH", ""], ["Nguyen", "Chanh DT", ""], ["Huynh", "Minh-Thanh", ""], ["Jain", "Mudit", ""], ["Young", "Victoria A.", ""], ["Ng", "Andrew Y.", ""], ["Lungren", "Matthew P.", ""], ["Rajpurkar", "Pranav", ""]]}, {"id": "2102.11506", "submitter": "Sulabh Katiyar", "authors": "Sulabh Katiyar, Samir Kumar Borgohain", "title": "Comparative evaluation of CNN architectures for Image Caption Generation", "comments": "Article Published in International Journal of Advanced Computer\n  Science and Applications(IJACSA), Volume 11 Issue 12, 2020", "journal-ref": "in International Journal of Advanced Computer Science and\n  Applications, 11(12), 2020", "doi": "10.14569/IJACSA.2020.0111291", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Aided by recent advances in Deep Learning, Image Caption Generation has seen\ntremendous progress over the last few years. Most methods use transfer learning\nto extract visual information, in the form of image features, with the help of\npre-trained Convolutional Neural Network models followed by transformation of\nthe visual information using a Caption Generator module to generate the output\nsentences. Different methods have used different Convolutional Neural Network\nArchitectures and, to the best of our knowledge, there is no systematic study\nwhich compares the relative efficacy of different Convolutional Neural Network\narchitectures for extracting the visual information. In this work, we have\nevaluated 17 different Convolutional Neural Networks on two popular Image\nCaption Generation frameworks: the first based on Neural Image Caption (NIC)\ngeneration model and the second based on Soft-Attention framework. We observe\nthat model complexity of Convolutional Neural Network, as measured by number of\nparameters, and the accuracy of the model on Object Recognition task does not\nnecessarily co-relate with its efficacy on feature extraction for Image Caption\nGeneration task.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 05:43:54 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Katiyar", "Sulabh", ""], ["Borgohain", "Samir Kumar", ""]]}, {"id": "2102.11519", "submitter": "Ata Jodeiri", "authors": "Elham Yousef Kalaf, Ata Jodeiri, Seyed Kamaledin Setarehdan, Ng Wei\n  Lin, Kartini Binti Rahman, Nur Aishah Taib, Sarinder Kaur Dhillon", "title": "Classification of Breast Cancer Lesions in Ultrasound Images by using\n  Attention Layer and loss Ensembles in Deep Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reliable classification of benign and malignant lesions in breast ultrasound\nimages can provide an effective and relatively low cost method for early\ndiagnosis of breast cancer. The accuracy of the diagnosis is however highly\ndependent on the quality of the ultrasound systems and the experience of the\nusers (radiologists). The leverage in deep convolutional neural network\napproaches provided solutions in efficient analysis of breast ultrasound\nimages. In this study, we proposed a new framework for classification of breast\ncancer lesions by use of an attention module in modified VGG16 architecture. We\nalso proposed new ensembled loss function which is the combination of binary\ncross-entropy and logarithm of the hyperbolic cosine loss to improve the model\ndiscrepancy between classified lesions and its labels. Networks trained from\npretrained ImageNet weights, and subsequently fine-tuned with ultrasound\ndatasets. The proposed model in this study outperformed other modified VGG16\narchitectures with the accuracy of 93% and also the results are competitive\nwith other state of the art frameworks for classification of breast cancer\nlesions. In this study, we employed transfer learning approaches with the\npre-trained VGG16 architecture. Different CNN models for classification task\nwere trained to predict benign or malignant lesions in breast ultrasound\nimages. Our Experimental results show that the choice of loss function is\nhighly important in classification task and by adding an attention block we\ncould empower the performance our model.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 06:49:12 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Kalaf", "Elham Yousef", ""], ["Jodeiri", "Ata", ""], ["Setarehdan", "Seyed Kamaledin", ""], ["Lin", "Ng Wei", ""], ["Rahman", "Kartini Binti", ""], ["Taib", "Nur Aishah", ""], ["Dhillon", "Sarinder Kaur", ""]]}, {"id": "2102.11520", "submitter": "Ata Jodeiri", "authors": "Sadegh Soleimani Pour, Ata Jodeiri, Hossein Rashidi, Seyed Mostafa\n  Mirhassani, Hoda Kheradfallah, Hadi Seyedarabi", "title": "Automatic Ship Classification Utilizing Bag of Deep Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detection and classification of ships based on their silhouette profiles in\nnatural imagery is an important undertaking in computer science. This problem\ncan be viewed from a variety of perspectives, including security, traffic\ncontrol, and even militarism. Therefore, in each of the aforementioned\napplications, specific processing is required. In this paper, by applying the\n\"bag of words\" (BoW), a new method is presented that its words are the features\nthat are obtained using pre-trained models of deep convolutional networks. ,\nThree VGG models are utilized which provide superior accuracy in identifying\nobjects. The regions of the image that are selected as the initial proposals\nare derived from a greedy algorithm on the key points generated by the Scale\nInvariant Feature Transform (SIFT) method. Using the deep features in the BOW\nmethod provides a good improvement in the recognition and classification of\nships. Eventually, we obtained an accuracy of 91.8% in the classification of\nthe ships which shows the improvement of about 5% compared to previous methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 07:01:08 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Pour", "Sadegh Soleimani", ""], ["Jodeiri", "Ata", ""], ["Rashidi", "Hossein", ""], ["Mirhassani", "Seyed Mostafa", ""], ["Kheradfallah", "Hoda", ""], ["Seyedarabi", "Hadi", ""]]}, {"id": "2102.11526", "submitter": "Ziwei Wang", "authors": "Ziwei Wang, Yadan Luo and Zi Huang", "title": "Enhanced Modality Transition for Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image captioning model is a cross-modality knowledge discovery task, which\ntargets at automatically describing an image with an informative and coherent\nsentence. To generate the captions, the previous encoder-decoder frameworks\ndirectly forward the visual vectors to the recurrent language model, forcing\nthe recurrent units to generate a sentence based on the visual features.\nAlthough these sentences are generally readable, they still suffer from the\nlack of details and highlights, due to the fact that the substantial gap\nbetween the image and text modalities is not sufficiently addressed. In this\nwork, we explicitly build a Modality Transition Module (MTM) to transfer visual\nfeatures into semantic representations before forwarding them to the language\nmodel. During the training phase, the modality transition network is optimised\nby the proposed modality loss, which compares the generated preliminary textual\nencodings with the target sentence vectors from a pre-trained text\nauto-encoder. In this way, the visual vectors are transited into the textual\nsubspace for more contextual and precise language generation. The novel MTM can\nbe incorporated into most of the existing methods. Extensive experiments have\nbeen conducted on the MS-COCO dataset demonstrating the effectiveness of the\nproposed framework, improving the performance by 3.4% comparing to the\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 07:20:12 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Wang", "Ziwei", ""], ["Luo", "Yadan", ""], ["Huang", "Zi", ""]]}, {"id": "2102.11530", "submitter": "Kanji Tanaka", "authors": "Kanji Tanaka", "title": "Domain-invariant NBV Planner for Active Cross-domain Self-localization", "comments": "5 pages, 5 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pole-like landmark has received increasing attention as a domain-invariant\nvisual cue for visual robot self-localization across domains (e.g., seasons,\ntimes of day, weathers). However, self-localization using pole-like landmarks\ncan be ill-posed for a passive observer, as many viewpoints may not provide any\npole-like landmark view. To alleviate this problem, we consider an active\nobserver and explore a novel \"domain-invariant\" next-best-view (NBV) planner\nthat attains consistent performance over different domains (i.e.,\nmaintenance-free), without requiring the expensive task of training data\ncollection and retraining. In our approach, a novel multi-encoder deep\nconvolutional neural network enables to detect domain invariant pole-like\nlandmarks, which are then used as the sole input to a model-free deep\nreinforcement learning -based domain-invariant NBV planner. Further, we develop\na practical system for active self-localization using sparse invariant\nlandmarks and dense discriminative landmarks. In experiments, we demonstrate\nthat the proposed method is effective both in efficient landmark detection and\nin discriminative self-localization.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 07:36:45 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Tanaka", "Kanji", ""]]}, {"id": "2102.11535", "submitter": "Wuyang Chen", "authors": "Wuyang Chen, Xinyu Gong, Zhangyang Wang", "title": "Neural Architecture Search on ImageNet in Four GPU Hours: A\n  Theoretically Inspired Perspective", "comments": "accepted as ICLR 2021 poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Neural Architecture Search (NAS) has been explosively studied to automate the\ndiscovery of top-performer neural networks. Current works require heavy\ntraining of supernet or intensive architecture evaluations, thus suffering from\nheavy resource consumption and often incurring search bias due to truncated\ntraining or approximations. Can we select the best neural architectures without\ninvolving any training and eliminate a drastic portion of the search cost? We\nprovide an affirmative answer, by proposing a novel framework called\ntraining-free neural architecture search (TE-NAS). TE-NAS ranks architectures\nby analyzing the spectrum of the neural tangent kernel (NTK) and the number of\nlinear regions in the input space. Both are motivated by recent theory advances\nin deep networks and can be computed without any training and any label. We\nshow that: (1) these two measurements imply the trainability and expressivity\nof a neural network; (2) they strongly correlate with the network's test\naccuracy. Further on, we design a pruning-based NAS mechanism to achieve a more\nflexible and superior trade-off between the trainability and expressivity\nduring the search. In NAS-Bench-201 and DARTS search spaces, TE-NAS completes\nhigh-quality search but only costs 0.5 and 4 GPU hours with one 1080Ti on\nCIFAR-10 and ImageNet, respectively. We hope our work inspires more attempts in\nbridging the theoretical findings of deep networks and practical impacts in\nreal NAS applications. Code is available at:\nhttps://github.com/VITA-Group/TENAS.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 07:50:44 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 08:27:58 GMT"}, {"version": "v3", "created": "Wed, 3 Mar 2021 16:17:27 GMT"}, {"version": "v4", "created": "Tue, 16 Mar 2021 00:59:19 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Chen", "Wuyang", ""], ["Gong", "Xinyu", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2102.11541", "submitter": "Lan Chen", "authors": "Lan Chen, Lin Gao, Jie Yang, Shibiao Xu, Juntao Ye, Xiaopeng Zhang,\n  Yu-Kun Lai", "title": "Deep Deformation Detail Synthesis for Thin Shell Models", "comments": "15 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In physics-based cloth animation, rich folds and detailed wrinkles are\nachieved at the cost of expensive computational resources and huge labor\ntuning. Data-driven techniques make efforts to reduce the computation\nsignificantly by a database. One type of methods relies on human poses to\nsynthesize fitted garments which cannot be applied to general cloth. Another\ntype of methods adds details to the coarse meshes without such restrictions.\nHowever, existing works usually utilize coordinate-based representations which\ncannot cope with large-scale deformation, and requires dense vertex\ncorrespondences between coarse and fine meshes. Moreover, as such methods only\nadd details, they require coarse meshes to be close to fine meshes, which can\nbe either impossible, or require unrealistic constraints when generating fine\nmeshes. To address these challenges, we develop a temporally and spatially\nas-consistent-as-possible deformation representation (named TS-ACAP) and a\nDeformTransformer network to learn the mapping from low-resolution meshes to\ndetailed ones. This TS-ACAP representation is designed to ensure both spatial\nand temporal consistency for sequential large-scale deformations from cloth\nanimations. With this representation, our DeformTransformer network first\nutilizes two mesh-based encoders to extract the coarse and fine features,\nrespectively. To transduct the coarse features to the fine ones, we leverage\nthe Transformer network that consists of frame-level attention mechanisms to\nensure temporal coherence of the prediction. Experimental results show that our\nmethod is able to produce reliable and realistic animations in various datasets\nat high frame rates: 10 ~ 35 times faster than physics-based simulation, with\nsuperior detail synthesis abilities than existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 08:09:11 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Chen", "Lan", ""], ["Gao", "Lin", ""], ["Yang", "Jie", ""], ["Xu", "Shibiao", ""], ["Ye", "Juntao", ""], ["Zhang", "Xiaopeng", ""], ["Lai", "Yu-Kun", ""]]}, {"id": "2102.11558", "submitter": "Andreas Kamilaris", "authors": "Andreas Kamilaris, Jean-Baptiste Filippi, Chirag Padubidri, Jesper\n  Provoost, Savvas Karatsiolis, Ian Cole, Wouter Couwenbergh and Evi Demetriou", "title": "EscapeWildFire: Assisting People to Escape Wildfires in Real-Time", "comments": "6th IEEE International Workshop on Pervasive Context-Aware Smart\n  Cities and Intelligent Transport System (PerAwareCity), Proc. of PerCom 2021,\n  Kassel, Germany, March, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the past couple of decades, the number of wildfires and area of land\nburned around the world has been steadily increasing, partly due to climatic\nchanges and global warming. Therefore, there is a high probability that more\npeople will be exposed to and endangered by forest fires. Hence there is an\nurgent need to design pervasive systems that effectively assist people and\nguide them to safety during wildfires. This paper presents EscapeWildFire, a\nmobile application connected to a backend system which models and predicts\nwildfire geographical progression, assisting citizens to escape wildfires in\nreal-time. A small pilot indicates the correctness of the system. The code is\nopen-source; fire authorities around the world are encouraged to adopt this\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 08:58:37 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Kamilaris", "Andreas", ""], ["Filippi", "Jean-Baptiste", ""], ["Padubidri", "Chirag", ""], ["Provoost", "Jesper", ""], ["Karatsiolis", "Savvas", ""], ["Cole", "Ian", ""], ["Couwenbergh", "Wouter", ""], ["Demetriou", "Evi", ""]]}, {"id": "2102.11560", "submitter": "Sohini Roychowdhury", "authors": "Sohini Roychowdhury, Kwok Sun Tang, Mohith Ashok, Anoop Sanka", "title": "SISE-PC: Semi-supervised Image Subsampling for Explainable Pathology", "comments": "4 pages, 6 images, 2 tables, submitted to IEEE EMBC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although automated pathology classification using deep learning (DL) has\nproved to be predictively efficient, DL methods are found to be data and\ncompute cost intensive. In this work, we aim to reduce DL training costs by\npre-training a Resnet feature extractor using SimCLR contrastive loss for\nlatent encoding of OCT images. We propose a novel active learning framework\nthat identifies a minimal sub-sampled dataset containing the most uncertain OCT\nimage samples using label propagation on the SimCLR latent encodings. The\npre-trained Resnet model is then fine-tuned with the labelled minimal\nsub-sampled data and the underlying pathological sites are visually explained.\nOur framework identifies upto 2% of OCT images to be most uncertain that need\nprioritized specialist attention and that can fine-tune a Resnet model to\nachieve upto 97% classification accuracy. The proposed method can be extended\nto other medical images to minimize prediction costs.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 09:00:15 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 07:45:25 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Roychowdhury", "Sohini", ""], ["Tang", "Kwok Sun", ""], ["Ashok", "Mohith", ""], ["Sanka", "Anoop", ""]]}, {"id": "2102.11566", "submitter": "Hongxin Xiang", "authors": "Hongxin Xiang, Cheng Xie, Ting Zeng, Yun Yang", "title": "Multi-Knowledge Fusion for New Feature Generation in Generalized\n  Zero-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suffering from the semantic insufficiency and domain-shift problems, most of\nexisting state-of-the-art methods fail to achieve satisfactory results for\nZero-Shot Learning (ZSL). In order to alleviate these problems, we propose a\nnovel generative ZSL method to learn more generalized features from\nmulti-knowledge with continuously generated new semantics in semantic-to-visual\nembedding. In our approach, the proposed Multi-Knowledge Fusion Network\n(MKFNet) takes different semantic features from multi-knowledge as input, which\nenables more relevant semantic features to be trained for semantic-to-visual\nembedding, and finally generates more generalized visual features by adaptively\nfusing visual features from different knowledge domain. The proposed New\nFeature Generator (NFG) with adaptive genetic strategy is used to enrich\nsemantic information on the one hand, and on the other hand it greatly improves\nthe intersection of visual feature generated by MKFNet and unseen visual\nfaetures. Empirically, we show that our approach can achieve significantly\nbetter performance compared to existing state-of-the-art methods on a large\nnumber of benchmarks for several ZSL tasks, including traditional ZSL,\ngeneralized ZSL and zero-shot retrieval.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 09:11:05 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Xiang", "Hongxin", ""], ["Xie", "Cheng", ""], ["Zeng", "Ting", ""], ["Yang", "Yun", ""]]}, {"id": "2102.11585", "submitter": "Gurkirt Singh", "authors": "Gurkirt Singh, Stephen Akrigg, Manuele Di Maio, Valentina Fontana,\n  Reza Javanmard Alitappeh, Suman Saha, Kossar Jeddisaravi, Farzad Yousefi,\n  Jacob Culley, Tom Nicholson, Jordan Omokeowa, Salman Khan, Stanislao\n  Grazioso, Andrew Bradley, Giuseppe Di Gironimo, Fabio Cuzzolin", "title": "ROAD: The ROad event Awareness Dataset for Autonomous Driving", "comments": "21 pages, dataset paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans approach driving in a holistic fashion which entails, in particular,\nunderstanding road events and their evolution. Injecting these capabilities in\nan autonomous vehicle has thus the potential to take situational awareness and\ndecision making closer to human-level performance. To this purpose, we\nintroduce the ROad event Awareness Dataset (ROAD) for Autonomous Driving, to\nour knowledge the first of its kind. ROAD is designed to test an autonomous\nvehicle's ability to detect road events, defined as triplets composed by a\nmoving agent, the action(s) it performs and the corresponding scene locations.\nROAD comprises 22 videos, originally from the Oxford RobotCar Dataset,\nannotated with bounding boxes showing the location in the image plane of each\nroad event. We also provide as baseline a new incremental algorithm for online\nroad event awareness, based on inflating RetinaNet along time, which achieves a\nmean average precision of 16.8% and 6.1% for frame-level and video-level event\ndetection, respectively, at 50% overlap. Though promising, these figures\nhighlight the challenges faced by situation awareness in autonomous driving.\nFinally, ROAD allows scholars to investigate exciting tasks such as complex\n(road) activity detection, future road event anticipation and the modelling of\nsentient road agents in terms of mental states. Dataset can be obtained from\nhttps://github.com/gurkirt/road-dataset and baseline code from\nhttps://github.com/gurkirt/3D-RetinaNet.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 09:48:56 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 10:07:31 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Singh", "Gurkirt", ""], ["Akrigg", "Stephen", ""], ["Di Maio", "Manuele", ""], ["Fontana", "Valentina", ""], ["Alitappeh", "Reza Javanmard", ""], ["Saha", "Suman", ""], ["Jeddisaravi", "Kossar", ""], ["Yousefi", "Farzad", ""], ["Culley", "Jacob", ""], ["Nicholson", "Tom", ""], ["Omokeowa", "Jordan", ""], ["Khan", "Salman", ""], ["Grazioso", "Stanislao", ""], ["Bradley", "Andrew", ""], ["Di Gironimo", "Giuseppe", ""], ["Cuzzolin", "Fabio", ""]]}, {"id": "2102.11586", "submitter": "YueFeng Chen", "authors": "Kejiang Chen, Yuefeng Chen, Hang Zhou, Chuan Qin, Xiaofeng Mao,\n  Weiming Zhang, Nenghai Yu", "title": "Adversarial Examples Detection beyond Image Space", "comments": "To appear in ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been proved that they are vulnerable to adversarial\nexamples, which are generated by adding human-imperceptible perturbations to\nimages. To defend these adversarial examples, various detection based methods\nhave been proposed. However, most of them perform poorly on detecting\nadversarial examples with extremely slight perturbations. By exploring these\nadversarial examples, we find that there exists compliance between\nperturbations and prediction confidence, which guides us to detect\nfew-perturbation attacks from the aspect of prediction confidence. To detect\nboth few-perturbation attacks and large-perturbation attacks, we propose a\nmethod beyond image space by a two-stream architecture, in which the image\nstream focuses on the pixel artifacts and the gradient stream copes with the\nconfidence artifacts. The experimental results show that the proposed method\noutperforms the existing methods under oblivious attacks and is verified\neffective to defend omniscient attacks as well.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 09:55:03 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Chen", "Kejiang", ""], ["Chen", "Yuefeng", ""], ["Zhou", "Hang", ""], ["Qin", "Chuan", ""], ["Mao", "Xiaofeng", ""], ["Zhang", "Weiming", ""], ["Yu", "Nenghai", ""]]}, {"id": "2102.11588", "submitter": "Julio Wissing", "authors": "Julio Wissing, Benedikt Boenninghoff, Dorothea Kolossa, Tsubasa\n  Ochiai, Marc Delcroix, Keisuke Kinoshita, Tomohiro Nakatani, Shoko Araki,\n  Christopher Schymura", "title": "Data Fusion for Audiovisual Speaker Localization: Extending Dynamic\n  Stream Weights to the Spatial Domain", "comments": "4 pages, 6 figures, ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.CL cs.CV cs.LG eess.AS eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimating the positions of multiple speakers can be helpful for tasks like\nautomatic speech recognition or speaker diarization. Both applications benefit\nfrom a known speaker position when, for instance, applying beamforming or\nassigning unique speaker identities. Recently, several approaches utilizing\nacoustic signals augmented with visual data have been proposed for this task.\nHowever, both the acoustic and the visual modality may be corrupted in specific\nspatial regions, for instance due to poor lighting conditions or to the\npresence of background noise. This paper proposes a novel audiovisual data\nfusion framework for speaker localization by assigning individual dynamic\nstream weights to specific regions in the localization space. This fusion is\nachieved via a neural network, which combines the predictions of individual\naudio and video trackers based on their time- and location-dependent\nreliability. A performance evaluation using audiovisual recordings yields\npromising results, with the proposed fusion approach outperforming all baseline\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 09:59:31 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 07:57:47 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Wissing", "Julio", ""], ["Boenninghoff", "Benedikt", ""], ["Kolossa", "Dorothea", ""], ["Ochiai", "Tsubasa", ""], ["Delcroix", "Marc", ""], ["Kinoshita", "Keisuke", ""], ["Nakatani", "Tomohiro", ""], ["Araki", "Shoko", ""], ["Schymura", "Christopher", ""]]}, {"id": "2102.11603", "submitter": "Sourav Garg", "authors": "Sourav Garg and Michael Milford", "title": "SeqNet: Learning Descriptors for Sequence-based Hierarchical Place\n  Recognition", "comments": "Accepted for publication in IEEE RA-L 2021; includes supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Place Recognition (VPR) is the task of matching current visual imagery\nfrom a camera to images stored in a reference map of the environment. While\ninitial VPR systems used simple direct image methods or hand-crafted visual\nfeatures, recent work has focused on learning more powerful visual features and\nfurther improving performance through either some form of sequential matcher /\nfilter or a hierarchical matching process. In both cases the performance of the\ninitial single-image based system is still far from perfect, putting\nsignificant pressure on the sequence matching or (in the case of hierarchical\nsystems) pose refinement stages. In this paper we present a novel hybrid system\nthat creates a high performance initial match hypothesis generator using short\nlearnt sequential descriptors, which enable selective control sequential score\naggregation using single image learnt descriptors. Sequential descriptors are\ngenerated using a temporal convolutional network dubbed SeqNet, encoding short\nimage sequences using 1-D convolutions, which are then matched against the\ncorresponding temporal descriptors from the reference dataset to provide an\nordered list of place match hypotheses. We then perform selective sequential\nscore aggregation using shortlisted single image learnt descriptors from a\nseparate pipeline to produce an overall place match hypothesis. Comprehensive\nexperiments on challenging benchmark datasets demonstrate the proposed method\noutperforming recent state-of-the-art methods using the same amount of\nsequential information. Source code and supplementary material can be found at\nhttps://github.com/oravus/seqNet.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 10:32:10 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 01:52:08 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Garg", "Sourav", ""], ["Milford", "Michael", ""]]}, {"id": "2102.11614", "submitter": "Di Xie", "authors": "Weijie Chen and Luojun Lin and Shicai Yang and Di Xie and Shiliang Pu\n  and Yueting Zhuang and Wenqi Ren", "title": "Self-Supervised Noisy Label Learning for Source-Free Unsupervised Domain\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is a strong prerequisite to access source data freely in many existing\nunsupervised domain adaptation approaches. However, source data is agnostic in\nmany practical scenarios due to the constraints of expensive data transmission\nand data privacy protection. Usually, the given source domain pre-trained model\nis expected to optimize with only unlabeled target data, which is termed as\nsource-free unsupervised domain adaptation. In this paper, we solve this\nproblem from the perspective of noisy label learning, since the given\npre-trained model can pre-generate noisy label for unlabeled target data via\ndirectly network inference. Under this problem modeling, incorporating\nself-supervised learning, we propose a novel Self-Supervised Noisy Label\nLearning method, which can effectively fine-tune the pre-trained model with\npre-generated label as well as selfgenerated label on the fly. Extensive\nexperiments had been conducted to validate its effectiveness. Our method can\neasily achieve state-of-the-art results and surpass other methods by a very\nlarge margin. Code will be released.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 10:51:45 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Chen", "Weijie", ""], ["Lin", "Luojun", ""], ["Yang", "Shicai", ""], ["Xie", "Di", ""], ["Pu", "Shiliang", ""], ["Zhuang", "Yueting", ""], ["Ren", "Wenqi", ""]]}, {"id": "2102.11646", "submitter": "Niv Nayman", "authors": "Niv Nayman, Yonathan Aflalo, Asaf Noy, Lihi Zelnik-Manor", "title": "HardCoRe-NAS: Hard Constrained diffeRentiable Neural Architecture Search", "comments": "Niv Nayman and Yonathan Aflalo contributed equally. An implementation\n  of HardCoRe-NAS is available at: https://github.com/Alibaba-MIIL/HardCoReNAS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realistic use of neural networks often requires adhering to multiple\nconstraints on latency, energy and memory among others. A popular approach to\nfind fitting networks is through constrained Neural Architecture Search (NAS),\nhowever, previous methods enforce the constraint only softly. Therefore, the\nresulting networks do not exactly adhere to the resource constraint and their\naccuracy is harmed. In this work we resolve this by introducing Hard\nConstrained diffeRentiable NAS (HardCoRe-NAS), that is based on an accurate\nformulation of the expected resource requirement and a scalable search method\nthat satisfies the hard constraint throughout the search. Our experiments show\nthat HardCoRe-NAS generates state-of-the-art architectures, surpassing other\nNAS methods, while strictly satisfying the hard resource constraints without\nany tuning required.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 11:56:30 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Nayman", "Niv", ""], ["Aflalo", "Yonathan", ""], ["Noy", "Asaf", ""], ["Zelnik-Manor", "Lihi", ""]]}, {"id": "2102.11650", "submitter": "Walter H. L. Pinaya", "authors": "Walter Hugo Lopez Pinaya, Petru-Daniel Tudosiu, Robert Gray, Geraint\n  Rees, Parashkev Nachev, Sebastien Ourselin, M. Jorge Cardoso", "title": "Unsupervised Brain Anomaly Detection and Segmentation with Transformers", "comments": "22 pages, 9 figures, submitted to MIDL 2021, OpenReview\n  https://openreview.net/forum?id=Z1tlNqbCpp_", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pathological brain appearances may be so heterogeneous as to be intelligible\nonly as anomalies, defined by their deviation from normality rather than any\nspecific pathological characteristic. Amongst the hardest tasks in medical\nimaging, detecting such anomalies requires models of the normal brain that\ncombine compactness with the expressivity of the complex, long-range\ninteractions that characterise its structural organisation. These are\nrequirements transformers have arguably greater potential to satisfy than other\ncurrent candidate architectures, but their application has been inhibited by\ntheir demands on data and computational resource. Here we combine the latent\nrepresentation of vector quantised variational autoencoders with an ensemble of\nautoregressive transformers to enable unsupervised anomaly detection and\nsegmentation defined by deviation from healthy brain imaging data, achievable\nat low computational cost, within relative modest data regimes. We compare our\nmethod to current state-of-the-art approaches across a series of experiments\ninvolving synthetic and real pathological lesions. On real lesions, we train\nour models on 15,000 radiologically normal participants from UK Biobank, and\nevaluate performance on four different brain MR datasets with small vessel\ndisease, demyelinating lesions, and tumours. We demonstrate superior anomaly\ndetection performance both image-wise and pixel-wise, achievable without\npost-processing. These results draw attention to the potential of transformers\nin this most challenging of imaging tasks.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 12:10:58 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Pinaya", "Walter Hugo Lopez", ""], ["Tudosiu", "Petru-Daniel", ""], ["Gray", "Robert", ""], ["Rees", "Geraint", ""], ["Nachev", "Parashkev", ""], ["Ourselin", "Sebastien", ""], ["Cardoso", "M. Jorge", ""]]}, {"id": "2102.11677", "submitter": "Yeman Brhane Hagos", "authors": "Yeman Brhane Hagos, Catherine SY Lecat, Dominic Patel, Lydia Lee,\n  Thien-An Tran, Manuel Rodriguez- Justo, Kwee Yong, Yinyin Yuan", "title": "Cell abundance aware deep learning for cell detection on highly\n  imbalanced pathological data", "comments": "Accepted at The IEEE International Symposium on Biomedical Imaging\n  (ISBI) 2021, 5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated analysis of tissue sections allows a better understanding of\ndisease biology and may reveal biomarkers that could guide prognosis or\ntreatment selection. In digital pathology, less abundant cell types can be of\nbiological significance, but their scarcity can result in biased and\nsub-optimal cell detection model. To minimize the effect of cell imbalance on\ncell detection, we proposed a deep learning pipeline that considers the\nabundance of cell types during model training. Cell weight images were\ngenerated, which assign larger weights to less abundant cells and used the\nweights to regularize Dice overlap loss function. The model was trained and\nevaluated on myeloma bone marrow trephine samples. Our model obtained a cell\ndetection F1-score of 0.78, a 2% increase compared to baseline models, and it\noutperformed baseline models at detecting rare cell types. We found that\nscaling deep learning loss function by the abundance of cells improves cell\ndetection performance. Our results demonstrate the importance of incorporating\ndomain knowledge on deep learning methods for pathological data with class\nimbalance.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 13:07:52 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Hagos", "Yeman Brhane", ""], ["Lecat", "Catherine SY", ""], ["Patel", "Dominic", ""], ["Lee", "Lydia", ""], ["Tran", "Thien-An", ""], ["Justo", "Manuel Rodriguez-", ""], ["Yong", "Kwee", ""], ["Yuan", "Yinyin", ""]]}, {"id": "2102.11684", "submitter": "Andrew Willis", "authors": "Pengcheng Liu, Nathan Hewitt, Waseem Shadid, Andrew Willis", "title": "A System for 3D Reconstruction Of Comminuted Tibial Plafond Bone\n  Fractures", "comments": "Elsevier Journal of Computerized Medical Imaging and Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High energy impacts at joint locations often generate highly fragmented, or\ncomminuted, bone fractures. Current approaches for treatment require physicians\nto decide how to classify the fracture within a hierarchy fracture severity\ncategories. Each category then provides a best-practice treatment scenario to\nobtain the best possible prognosis for the patient. This article identifies\nshortcomings associated with qualitative-only evaluation of fracture severity\nand provides new quantitative metrics that serve to address these shortcomings.\nWe propose a system to semi-automatically extract quantitative metrics that are\nmajor indicators of fracture severity. These include: (i) fracture surface\narea, i.e., how much surface area was generated when the bone broke apart, and\n(ii) dispersion, i.e., how far the fragments have rotated and translated from\ntheir original anatomic positions. This article describes new computational\ntools to extract these metrics by computationally reconstructing 3D bone\nanatomy from CT images with a focus on tibial plafond fracture cases where\ndifficult qualitative fracture severity cases are more prevalent.\nReconstruction is accomplished within a single system that integrates several\nnovel algorithms that identify, extract and piece-together fractured fragments\nin a virtual environment. Doing so provides objective quantitative measures for\nthese fracture severity indicators. The availability of such measures provides\nnew tools for fracture severity assessment which may lead to improved fracture\ntreatment. This paper describes the system, the underlying algorithms and the\nmetrics of the reconstruction results by quantitatively analyzing six clinical\ntibial plafond fracture cases.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 13:26:55 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Liu", "Pengcheng", ""], ["Hewitt", "Nathan", ""], ["Shadid", "Waseem", ""], ["Willis", "Andrew", ""]]}, {"id": "2102.11720", "submitter": "Benjamin Naoto Chiche", "authors": "Benjamin Naoto Chiche, Arnaud Woiselle, Joana Frontera-Pons and\n  Jean-Luc Starck", "title": "Deep Unrolled Network for Video Super-Resolution", "comments": "6 pages. 3 figures. Published in: 2020 Tenth International Conference\n  on Image Processing Theory, Tools and Applications (IPTA)", "journal-ref": null, "doi": "10.1109/IPTA50016.2020.9286636", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video super-resolution (VSR) aims to reconstruct a sequence of\nhigh-resolution (HR) images from their corresponding low-resolution (LR)\nversions. Traditionally, solving a VSR problem has been based on iterative\nalgorithms that can exploit prior knowledge on image formation and assumptions\non the motion. However, these classical methods struggle at incorporating\ncomplex statistics from natural images. Furthermore, VSR has recently benefited\nfrom the improvement brought by deep learning (DL) algorithms. These techniques\ncan efficiently learn spatial patterns from large collections of images. Yet,\nthey fail to incorporate some knowledge about the image formation model, which\nlimits their flexibility. Unrolled optimization algorithms, developed for\ninverse problems resolution, allow to include prior information into deep\nlearning architectures. They have been used mainly for single image restoration\ntasks. Adapting an unrolled neural network structure can bring the following\nbenefits. First, this may increase performance of the super-resolution task.\nThen, this gives neural networks better interpretability. Finally, this allows\nflexibility in learning a single model to nonblindly deal with multiple\ndegradations. In this paper, we propose a new VSR neural network based on\nunrolled optimization techniques and discuss its performance.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 14:35:09 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Chiche", "Benjamin Naoto", ""], ["Woiselle", "Arnaud", ""], ["Frontera-Pons", "Joana", ""], ["Starck", "Jean-Luc", ""]]}, {"id": "2102.11730", "submitter": "Marco Wallner", "authors": "Marco Wallner, Daniel Steininger, Verena Widhalm, Matthias\n  Sch\\\"orghuber, Csaba Beleznai", "title": "RGB-D Railway Platform Monitoring and Scene Understanding for Enhanced\n  Passenger Safety", "comments": "The final authenticated version is available online at\n  https://doi.org/10.1007/978-3-030-68787-8_47", "journal-ref": "Pattern Recognition. ICPR International Workshops and Challenges.\n  ICPR 2021. Lecture Notes in Computer Science, vol 12667. Springer, Cham", "doi": "10.1007/978-3-030-68787-8_47", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated monitoring and analysis of passenger movement in safety-critical\nparts of transport infrastructures represent a relevant visual surveillance\ntask. Recent breakthroughs in visual representation learning and spatial\nsensing opened up new possibilities for detecting and tracking humans and\nobjects within a 3D spatial context. This paper proposes a flexible analysis\nscheme and a thorough evaluation of various processing pipelines to detect and\ntrack humans on a ground plane, calibrated automatically via stereo depth and\npedestrian detection. We consider multiple combinations within a set of RGB-\nand depth-based detection and tracking modalities. We exploit the modular\nconcepts of Meshroom [2] and demonstrate its use as a generic vision processing\npipeline and scalable evaluation framework. Furthermore, we introduce a novel\nopen RGB-D railway platform dataset with annotations to support research\nactivities in automated RGB-D surveillance. We present quantitative results for\nmultiple object detection and tracking for various algorithmic combinations on\nour dataset. Results indicate that the combined use of depth-based spatial\ninformation and learned representations yields substantially enhanced detection\nand tracking accuracies. As demonstrated, these enhancements are especially\npronounced in adverse situations when occlusions and objects not captured by\nlearned representations are present.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 14:44:34 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Wallner", "Marco", ""], ["Steininger", "Daniel", ""], ["Widhalm", "Verena", ""], ["Sch\u00f6rghuber", "Matthias", ""], ["Beleznai", "Csaba", ""]]}, {"id": "2102.11731", "submitter": "Xiao Li", "authors": "Xiao Li, Jianmin Li, Ting Dai, Jie Shi, Jun Zhu, Xiaolin Hu", "title": "Rethinking Natural Adversarial Examples for Classification Models", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, it was found that many real-world examples without intentional\nmodifications can fool machine learning models, and such examples are called\n\"natural adversarial examples\". ImageNet-A is a famous dataset of natural\nadversarial examples. By analyzing this dataset, we hypothesized that large,\ncluttered and/or unusual background is an important reason why the images in\nthis dataset are difficult to be classified. We validated the hypothesis by\nreducing the background influence in ImageNet-A examples with object detection\ntechniques. Experiments showed that the object detection models with various\nclassification models as backbones obtained much higher accuracy than their\ncorresponding classification models. A detection model based on the\nclassification model EfficientNet-B7 achieved a top-1 accuracy of 53.95%,\nsurpassing previous state-of-the-art classification models trained on ImageNet,\nsuggesting that accurate localization information can significantly boost the\nperformance of classification models on ImageNet-A. We then manually cropped\nthe objects in images from ImageNet-A and created a new dataset, named\nImageNet-A-Plus. A human test on the new dataset showed that the deep\nlearning-based classifiers still performed quite poorly compared with humans.\nTherefore, the new dataset can be used to study the robustness of\nclassification models to the internal variance of objects without considering\nthe background disturbance.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 14:46:48 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Li", "Xiao", ""], ["Li", "Jianmin", ""], ["Dai", "Ting", ""], ["Shi", "Jie", ""], ["Zhu", "Jun", ""], ["Hu", "Xiaolin", ""]]}, {"id": "2102.11743", "submitter": "Kyle Mills", "authors": "Kyle Mills and Isaac Tamblyn", "title": "Weakly-supervised multi-class object localization using only object\n  counts as labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate the use of an extensive deep neural network to localize\ninstances of objects in images. The EDNN is naturally able to accurately\nperform multi-class counting using only ground truth count values as labels.\nWithout providing any conceptual information, object annotations, or pixel\nsegmentation information, the neural network is able to formulate its own\nconceptual representation of the items in the image. Using images labelled with\nonly the counts of the objects present,the structure of the extensive deep\nneural network can be exploited to perform localization of the objects within\nthe visual field. We demonstrate that a trained EDNN can be used to count\nobjects in images much larger than those on which it was trained. In order to\ndemonstrate our technique, we introduce seven new data sets: five progressively\nharder MNIST digit-counting data sets, and two datasets of 3d-rendered rubber\nducks in various situations. On most of these datasets, the EDNN achieves\ngreater than 99% test set accuracy in counting objects.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 15:14:46 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Mills", "Kyle", ""], ["Tamblyn", "Isaac", ""]]}, {"id": "2102.11747", "submitter": "Uddeshya Upadhyay", "authors": "Uddeshya Upadhyay, Yanbei Chen, Zeynep Akata", "title": "Uncertainty-aware Generalized Adaptive CycleGAN", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unpaired image-to-image translation refers to learning inter-image-domain\nmapping in an unsupervised manner. Existing methods often learn deterministic\nmappings without explicitly modelling the robustness to outliers or predictive\nuncertainty, leading to performance degradation when encountering unseen\nout-of-distribution (OOD) patterns at test time. To address this limitation, we\npropose a novel probabilistic method called Uncertainty-aware Generalized\nAdaptive Cycle Consistency (UGAC), which models the per-pixel residual by\ngeneralized Gaussian distribution, capable of modelling heavy-tailed\ndistributions. We compare our model with a wide variety of state-of-the-art\nmethods on two challenging tasks: unpaired image denoising in the natural image\nand unpaired modality prorogation in medical image domains. Experimental\nresults demonstrate that our model offers superior image generation quality\ncompared to recent methods in terms of quantitative metrics such as\nsignal-to-noise ratio and structural similarity. Our model also exhibits\nstronger robustness towards OOD test data.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 15:22:35 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Upadhyay", "Uddeshya", ""], ["Chen", "Yanbei", ""], ["Akata", "Zeynep", ""]]}, {"id": "2102.11811", "submitter": "Meng Zhang", "authors": "Meng Zhang, Duygu Ceylan, Tuanfeng Wang, Niloy J. Mitra", "title": "Dynamic Neural Garments", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vital task of the wider digital human effort is the creation of realistic\ngarments on digital avatars, both in the form of characteristic fold patterns\nand wrinkles in static frames as well as richness of garment dynamics under\navatars' motion. Existing workflow of modeling, simulation, and rendering\nclosely replicates the physics behind real garments, but is tedious and\nrequires repeating most of the workflow under changes to characters' motion,\ncamera angle, or garment resizing. Although data-driven solutions exist, they\neither focus on static scenarios or only handle dynamics of tight garments. We\npresent a solution that, at test time, takes in body joint motion to directly\nproduce realistic dynamic garment image sequences. Specifically, given the\ntarget joint motion sequence of an avatar, we propose dynamic neural garments\nto jointly simulate and render plausible dynamic garment appearance from an\nunseen viewpoint. Technically, our solution generates a coarse garment proxy\nsequence, learns deep dynamic features attached to this template, and neurally\nrenders the features to produce appearance changes such as folds, wrinkles, and\nsilhouettes. We demonstrate generalization behavior to both unseen motion and\nunseen camera views. Further, our network can be fine-tuned to adopt to new\nbody shape and/or background images. We also provide comparisons against\nexisting neural rendering and image sequence translation approaches, and report\nclear quantitative improvements.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 17:21:21 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Zhang", "Meng", ""], ["Ceylan", "Duygu", ""], ["Wang", "Tuanfeng", ""], ["Mitra", "Niloy J.", ""]]}, {"id": "2102.11838", "submitter": "Old\\v{r}ich Kodym", "authors": "Old\\v{r}ich Kodym, Michal Hradi\\v{s}", "title": "Page Layout Analysis System for Unconstrained Historic Documents", "comments": "Submitted to ICDAR2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extraction of text regions and individual text lines from historic documents\nis necessary for automatic transcription. We propose extending a CNN-based text\nbaseline detection system by adding line height and text block boundary\npredictions to the model output, allowing the system to extract more\ncomprehensive layout information. We also show that pixel-wise text orientation\nprediction can be used for processing documents with multiple text\norientations. We demonstrate that the proposed method performs well on the cBAD\nbaseline detection dataset. Additionally, we benchmark the method on newly\nintroduced PERO layout dataset which we also make public.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 18:13:36 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Kodym", "Old\u0159ich", ""], ["Hradi\u0161", "Michal", ""]]}, {"id": "2102.11856", "submitter": "Vinay Verma Kumar", "authors": "Vinay Kumar Verma, Kevin Liang, Nikhil Mehta, Lawrence Carin", "title": "Meta-Learned Attribute Self-Gating for Continual Generalized Zero-Shot\n  Learning", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Zero-shot learning (ZSL) has been shown to be a promising approach to\ngeneralizing a model to categories unseen during training by leveraging class\nattributes, but challenges still remain. Recently, methods using generative\nmodels to combat bias towards classes seen during training have pushed the\nstate of the art of ZSL, but these generative models can be slow or\ncomputationally expensive to train. Additionally, while many previous ZSL\nmethods assume a one-time adaptation to unseen classes, in reality, the world\nis always changing, necessitating a constant adjustment for deployed models.\nModels unprepared to handle a sequential stream of data are likely to\nexperience catastrophic forgetting. We propose a meta-continual zero-shot\nlearning (MCZSL) approach to address both these issues. In particular, by\npairing self-gating of attributes and scaled class normalization with\nmeta-learning based training, we are able to outperform state-of-the-art\nresults while being able to train our models substantially faster\n($>100\\times$) than expensive generative-based approaches. We demonstrate this\nby performing experiments on five standard ZSL datasets (CUB, aPY, AWA1, AWA2\nand SUN) in both generalized zero-shot learning and generalized continual\nzero-shot learning settings.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 18:36:14 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Verma", "Vinay Kumar", ""], ["Liang", "Kevin", ""], ["Mehta", "Nikhil", ""], ["Carin", "Lawrence", ""]]}, {"id": "2102.11859", "submitter": "Mark Weber", "authors": "Mark Weber, Jun Xie, Maxwell Collins, Yukun Zhu, Paul Voigtlaender,\n  Hartwig Adam, Bradley Green, Andreas Geiger, Bastian Leibe, Daniel Cremers,\n  Aljosa Osep, Laura Leal-Taixe, Liang-Chieh Chen", "title": "STEP: Segmenting and Tracking Every Pixel", "comments": "Datasets, metric, and baselines will be made publicly available soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle video panoptic segmentation, a task that requires\nassigning semantic classes and track identities to all pixels in a video. To\nstudy this important problem in a setting that requires a continuous\ninterpretation of sensory data, we present a new benchmark: Segmenting and\nTracking Every Pixel (STEP), encompassing two datasets, KITTI-STEP, and\nMOTChallenge-STEP together with a new evaluation metric. Our work is the first\nthat targets this task in a real-world setting that requires dense\ninterpretation in both spatial and temporal domains. As the ground-truth for\nthis task is difficult and expensive to obtain, existing datasets are either\nconstructed synthetically or only sparsely annotated within short video clips.\nBy contrast, our datasets contain long video sequences, providing challenging\nexamples and a test-bed for studying long-term pixel-precise segmentation and\ntracking. For measuring the performance, we propose a novel evaluation metric\nSegmentation and Tracking Quality (STQ) that fairly balances semantic and\ntracking aspects of this task and is suitable for evaluating sequences of\narbitrary length. We will make our datasets, metric, and baselines publicly\navailable.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 18:43:02 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Weber", "Mark", ""], ["Xie", "Jun", ""], ["Collins", "Maxwell", ""], ["Zhu", "Yukun", ""], ["Voigtlaender", "Paul", ""], ["Adam", "Hartwig", ""], ["Green", "Bradley", ""], ["Geiger", "Andreas", ""], ["Leibe", "Bastian", ""], ["Cremers", "Daniel", ""], ["Osep", "Aljosa", ""], ["Leal-Taixe", "Laura", ""], ["Chen", "Liang-Chieh", ""]]}, {"id": "2102.11860", "submitter": "Chengyuan Yao", "authors": "Chengyuan Yao, Pavol Bielik, Petar Tsankov, Martin Vechev", "title": "Automated Discovery of Adaptive Attacks on Adversarial Defenses", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable evaluation of adversarial defenses is a challenging task, currently\nlimited to an expert who manually crafts attacks that exploit the defense's\ninner workings, or to approaches based on ensemble of fixed attacks, none of\nwhich may be effective for the specific defense at hand. Our key observation is\nthat custom attacks are composed from a set of reusable building blocks, such\nas fine-tuning relevant attack parameters, network transformations, and custom\nloss functions. Based on this observation, we present an extensible framework\nthat defines a search space over these reusable building blocks and\nautomatically discovers an effective attack on a given model with an unknown\ndefense by searching over suitable combinations of these blocks. We evaluated\nour framework on 23 adversarial defenses and showed it outperforms AutoAttack,\nthe current state-of-the-art tool for reliable evaluation of adversarial\ndefenses: our discovered attacks are either stronger, producing 3.0%-50.8%\nadditional adversarial examples (10 cases), or are typically 2x faster while\nenjoying similar adversarial robustness (13 cases).\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 18:43:24 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 18:46:50 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Yao", "Chengyuan", ""], ["Bielik", "Pavol", ""], ["Tsankov", "Petar", ""], ["Vechev", "Martin", ""]]}, {"id": "2102.11861", "submitter": "Philipp Henzler", "authors": "Philipp Henzler, Valentin Deschaintre, Niloy J. Mitra, Tobias Ritschel", "title": "Generative Modelling of BRDF Textures from Flash Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We learn a latent space for easy capture, semantic editing, consistent\ninterpolation, and efficient reproduction of visual material appearance. When\nusers provide a photo of a stationary natural material captured under flash\nlight illumination, it is converted in milliseconds into a latent material\ncode. In a second step, conditioned on the material code, our method, again in\nmilliseconds, produces an infinite and diverse spatial field of BRDF model\nparameters (diffuse albedo, specular albedo, roughness, normals) that allows\nrendering in complex scenes and illuminations, matching the appearance of the\ninput picture. Technically, we jointly embed all flash images into a latent\nspace using a convolutional encoder, and -- conditioned on these latent codes\n-- convert random spatial fields into fields of BRDF parameters using a\nconvolutional neural network (CNN). We condition these BRDF parameters to match\nthe visual characteristics (statistics and spectra of visual features) of the\ninput under matching light. A user study confirms that the semantics of the\nlatent material space agree with user expectations and compares our approach\nfavorably to previous work.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 18:45:18 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Henzler", "Philipp", ""], ["Deschaintre", "Valentin", ""], ["Mitra", "Niloy J.", ""], ["Ritschel", "Tobias", ""]]}, {"id": "2102.11865", "submitter": "Alvaro Gomariz", "authors": "Alvaro Gomariz, Tiziano Portenier, C\\'esar Nombela-Arrieta, Orcun\n  Goksel", "title": "Probabilistic Spatial Analysis in Quantitative Microscopy with\n  Uncertainty-Aware Cell Detection using Deep Bayesian Regression of Density\n  Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D microscopy is key in the investigation of diverse biological systems, and\nthe ever increasing availability of large datasets demands automatic cell\nidentification methods that not only are accurate, but also can imply the\nuncertainty in their predictions to inform about potential errors and hence\nconfidence in conclusions using them. While conventional deep learning methods\noften yield deterministic results, advances in deep Bayesian learning allow for\naccurate predictions with a probabilistic interpretation in numerous image\nclassification and segmentation tasks. It is however nontrivial to extend such\nBayesian methods to cell detection, which requires specialized learning\nframeworks. In particular, regression of density maps is a popular successful\napproach for extracting cell coordinates from local peaks in a postprocessing\nstep, which hinders any meaningful probabilistic output. We herein propose a\ndeep learning-based cell detection framework that can operate on large\nmicroscopy images and outputs desired probabilistic predictions by (i)\nintegrating Bayesian techniques for the regression of uncertainty-aware density\nmaps, where peak detection can be applied to generate cell proposals, and (ii)\nlearning a mapping from the numerous proposals to a probabilistic space that is\ncalibrated, i.e. accurately represents the chances of a successful prediction.\nUtilizing such calibrated predictions, we propose a probabilistic spatial\nanalysis with Monte-Carlo sampling. We demonstrate this in revising an existing\ndescription of the distribution of a mesenchymal stromal cell type within the\nbone marrow, where our proposed methods allow us to reveal spatial patterns\nthat are otherwise undetectable. Introducing such probabilistic analysis in\nquantitative microscopy pipelines will allow for reporting confidence intervals\nfor testing biological hypotheses of spatial distributions.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 18:52:16 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Gomariz", "Alvaro", ""], ["Portenier", "Tiziano", ""], ["Nombela-Arrieta", "C\u00e9sar", ""], ["Goksel", "Orcun", ""]]}, {"id": "2102.11870", "submitter": "Mohamed El Banani", "authors": "Mohamed El Banani, Luya Gao, Justin Johnson", "title": "UnsupervisedR&R: Unsupervised Point Cloud Registration via\n  Differentiable Rendering", "comments": "11 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aligning partial views of a scene into a single whole is essential to\nunderstanding one's environment and is a key component of numerous robotics\ntasks such as SLAM and SfM. Recent approaches have proposed end-to-end systems\nthat can outperform traditional methods by leveraging pose supervision.\nHowever, with the rising prevalence of cameras with depth sensors, we can\nexpect a new stream of raw RGB-D data without the annotations needed for\nsupervision. We propose UnsupervisedR&R: an end-to-end unsupervised approach to\nlearning point cloud registration from raw RGB-D video. The key idea is to\nleverage differentiable alignment and rendering to enforce photometric and\ngeometric consistency between frames. We evaluate our approach on indoor scene\ndatasets and find that we outperform existing traditional approaches with\nclassic and learned descriptors while being competitive with supervised\ngeometric point cloud registration approaches.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 18:59:10 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Banani", "Mohamed El", ""], ["Gao", "Luya", ""], ["Johnson", "Justin", ""]]}, {"id": "2102.11916", "submitter": "Pramod Abichandani Dr", "authors": "Himanshu Patel, Craig Iaboni, Deepan Lobo, Ji-won Choi, Pramod\n  Abichandani", "title": "Event Camera Based Real-Time Detection and Tracking of Indoor Ground\n  Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a real-time method to detect and track multiple mobile\nground robots using event cameras. The method uses density-based spatial\nclustering of applications with noise (DBSCAN) to detect the robots and a\nsingle k-dimensional (k-d) tree to accurately keep track of them as they move\nin an indoor arena. Robust detections and tracks are maintained in the face of\nevent camera noise and lack of events (due to robots moving slowly or\nstopping). An off-the-shelf RGB camera-based tracking system was used to\nprovide ground truth. Experiments including up to 4 robots are performed to\nstudy the effect of i) varying DBSCAN parameters, ii) the event accumulation\ntime, iii) the number of robots in the arena, and iv) the speed of the robots\non the detection and tracking performance. The experimental results showed 100%\ndetection and tracking fidelity in the face of event camera noise and robots\nstopping for tests involving up to 3 robots (and upwards of 93% for 4 robots).\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 19:50:17 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Patel", "Himanshu", ""], ["Iaboni", "Craig", ""], ["Lobo", "Deepan", ""], ["Choi", "Ji-won", ""], ["Abichandani", "Pramod", ""]]}, {"id": "2102.11944", "submitter": "Sebastian Stabinger BSc MSc", "authors": "Sebastian Stabinger, David Peer, and Antonio Rodr\\'iguez-S\\'anchez", "title": "Arguments for the Unsuitability of Convolutional Neural Networks for\n  Non--Local Tasks", "comments": "Under review at Neural Networks Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have established themselves over the past years\nas the state of the art method for image classification, and for many datasets,\nthey even surpass humans in categorizing images. Unfortunately, the same\narchitectures perform much worse when they have to compare parts of an image to\neach other to correctly classify this image.\n  Until now, no well-formed theoretical argument has been presented to explain\nthis deficiency. In this paper, we will argue that convolutional layers are of\nlittle use for such problems, since comparison tasks are global by nature, but\nconvolutional layers are local by design. We will use this insight to\nreformulate a comparison task into a sorting task and use findings on sorting\nnetworks to propose a lower bound for the number of parameters a neural network\nneeds to solve comparison tasks in a generalizable way. We will use this lower\nbound to argue that attention, as well as iterative/recurrent processing, is\nneeded to prevent a combinatorial explosion.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 21:13:49 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Stabinger", "Sebastian", ""], ["Peer", "David", ""], ["Rodr\u00edguez-S\u00e1nchez", "Antonio", ""]]}, {"id": "2102.11952", "submitter": "Kazuto Nakashima", "authors": "Kazuto Nakashima and Ryo Kurazume", "title": "Learning to Drop Points for LiDAR Scan Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative modeling of 3D scenes is a crucial topic for aiding mobile robots\nto improve unreliable observations. However, despite the rapid progress in the\nnatural image domain, building generative models is still challenging for 3D\ndata, such as point clouds. Most existing studies on point clouds have focused\non small and uniform-density data. In contrast, 3D LiDAR point clouds widely\nused in mobile robots are non-trivial to be handled because of the large number\nof points and varying-density. To circumvent this issue, 3D-to-2D projected\nrepresentation such as a cylindrical depth map has been studied in existing\nLiDAR processing tasks but susceptible to discrete lossy pixels caused by\nfailures of laser reflection. This paper proposes a novel framework based on\ngenerative adversarial networks to synthesize realistic LiDAR data as an\nimproved 2D representation. Our generative architectures are designed to learn\na distribution of inverse depth maps and simultaneously simulate the lossy\npixels, which enables us to decompose an underlying smooth geometry and the\ncorresponding uncertainty of laser reflection. To simulate the lossy pixels, we\npropose a differentiable framework to learn to produce sample-dependent binary\nmasks using the Gumbel-Sigmoid reparametrization trick. We demonstrate the\neffectiveness of our approach in synthesis and reconstruction tasks on two\nLiDAR datasets. We further showcase potential applications by recovering\nvarious corruptions in LiDAR data.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 21:53:14 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Nakashima", "Kazuto", ""], ["Kurazume", "Ryo", ""]]}, {"id": "2102.11958", "submitter": "Adam Van Etten", "authors": "Adam Van Etten, Daniel Hogan", "title": "The SpaceNet Multi-Temporal Urban Development Challenge", "comments": "13 pages, 11 figures. To appear in PMLR Post Proceedings for the\n  Competition Track at NeurIPS 2020. arXiv admin note: text overlap with\n  arXiv:2102.04420", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Building footprints provide a useful proxy for a great many humanitarian\napplications. For example, building footprints are useful for high fidelity\npopulation estimates, and quantifying population statistics is fundamental to\n~1/4 of the United Nations Sustainable Development Goals Indicators. In this\npaper we (the SpaceNet Partners) discuss efforts to develop techniques for\nprecise building footprint localization, tracking, and change detection via the\nSpaceNet Multi-Temporal Urban Development Challenge (also known as SpaceNet 7).\nIn this NeurIPS 2020 competition, participants were asked identify and track\nbuildings in satellite imagery time series collected over rapidly urbanizing\nareas. The competition centered around a brand new open source dataset of\nPlanet Labs satellite imagery mosaics at 4m resolution, which includes 24\nimages (one per month) covering ~100 unique geographies. Tracking individual\nbuildings at this resolution is quite challenging, yet the winning participants\ndemonstrated impressive performance with the newly developed SpaceNet Change\nand Object Tracking (SCOT) metric. This paper details the top-5 winning\napproaches, as well as analysis of results that yielded a handful of\ninteresting anecdotes such as decreasing performance with latitude.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 22:01:22 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 19:12:39 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Van Etten", "Adam", ""], ["Hogan", "Daniel", ""]]}, {"id": "2102.11994", "submitter": "Md Asifuzzaman Jishan", "authors": "M. A. Jishan, M. S. Alam, Afrida Islam, I. R. Mazumder, K. R. Mahmud\n  and A. K. Al Azad", "title": "Characterization and recognition of handwritten digits using Julia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic image and digit recognition is a computationally challenging task\nfor image processing and pattern recognition, requiring an adequate\nappreciation of the syntactic and semantic importance of the image for the\nidentification ofthe handwritten digits. Image and Pattern Recognition has been\nidentified as one of the driving forces in the research areas because of its\nshifting of different types of applications, such as safety frameworks,\nclinical frameworks, diversion, and so on.In this study, for recognition, we\nimplemented a hybrid neural network model that is capable of recognizing the\ndigit of MNISTdataset and achieved a remarkable result. The proposed neural\nmodel network can extract features from the image and recognize the features in\nthe layer by layer. To expand, it is so important for the neural network to\nrecognize how the proposed modelcan work in each layer, how it can generate\noutput, and so on. Besides, it also can recognize the auto-encoding system and\nthe variational auto-encoding system of the MNIST dataset. This study will\nexplore those issues that are discussed above, and the explanation for them,\nand how this phenomenon can be overcome.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 00:30:41 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Jishan", "M. A.", ""], ["Alam", "M. S.", ""], ["Islam", "Afrida", ""], ["Mazumder", "I. R.", ""], ["Mahmud", "K. R.", ""], ["Azad", "A. K. Al", ""]]}, {"id": "2102.11996", "submitter": "Ji Zhao", "authors": "Ji Zhao, Banglei Guan", "title": "On Relative Pose Recovery for Multi-Camera Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The point correspondence (PC) and affine correspondence (AC) are widely used\nfor relative pose estimation. An AC consists of a PC across two views and an\naffine transformation between the small patches around this PC. Previous work\ndemonstrates that one AC generally provides three independent constraints for\nrelative pose estimation. For multi-camera systems, there is still not any\nAC-based minimal solver for general relative pose estimation. To deal with this\nproblem, we propose a complete solution to relative pose estimation from two\nACs for multi-camera systems, consisting of a series of minimal solvers. The\nsolver generation in our solution is based on Cayley or quaternion\nparameterization for rotation and hidden variable technique to eliminate\ntranslation. This solver generation method is also naturally applied to\nrelative pose estimation from PCs, resulting in a new six-point method for\nmulti-camera systems. A few extensions are made, including relative pose\nestimation with known rotation angle and/or with unknown focal lengths.\nExtensive experiments demonstrate that the proposed AC-based solvers and\nPC-based solvers are effective and efficient on synthetic and real-world\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 00:39:57 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Zhao", "Ji", ""], ["Guan", "Banglei", ""]]}, {"id": "2102.12010", "submitter": "Pierre Merriaux", "authors": "Jean-Luc D\\'eziel, Pierre Merriaux, Francis Tremblay, Dave Lessard,\n  Dominique Plourde, Julien Stanguennec, Pierre Goulet and Pierre Olivier", "title": "PixSet : An Opportunity for 3D Computer Vision to Go Beyond Point Clouds\n  With a Full-Waveform LiDAR Dataset", "comments": "7 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Leddar PixSet is a new publicly available dataset (dataset.leddartech.com)\nfor autonomous driving research and development. One key novelty of this\ndataset is the presence of full-waveform data from the Leddar Pixell sensor, a\nsolid-state flash LiDAR. Full-waveform data has been shown to improve the\nperformance of perception algorithms in airborne applications but is yet to be\ndemonstrated for terrestrial applications such as autonomous driving. The\nPixSet dataset contains approximately 29k frames from 97 sequences recorded in\nhigh-density urban areas, using a set of various sensors (cameras, LiDARs,\nradar, IMU, etc.) Each frame has been manually annotated with 3D bounding\nboxes.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 01:13:17 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 00:25:11 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["D\u00e9ziel", "Jean-Luc", ""], ["Merriaux", "Pierre", ""], ["Tremblay", "Francis", ""], ["Lessard", "Dave", ""], ["Plourde", "Dominique", ""], ["Stanguennec", "Julien", ""], ["Goulet", "Pierre", ""], ["Olivier", "Pierre", ""]]}, {"id": "2102.12037", "submitter": "William Harvey", "authors": "William Harvey, Saeid Naderiparizi, Frank Wood", "title": "Image Completion via Inference in Deep Generative Models", "comments": "21 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider image completion from the perspective of amortized inference in\nan image generative model. We leverage recent state of the art variational\nauto-encoder architectures that have been shown to produce photo-realistic\nnatural images at non-trivial resolutions. Through amortized inference in such\na model we can train neural artifacts that produce diverse, realistic image\ncompletions even when the vast majority of an image is missing. We demonstrate\nsuperior sample quality and diversity compared to prior art on the CIFAR-10 and\nFFHQ-256 datasets. We conclude by describing and demonstrating an application\nthat requires an in-painting model with the capabilities ours exhibits: the use\nof Bayesian optimal experimental design to select the most informative sequence\nof small field of view x-rays for chest pathology detection.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 02:59:43 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 22:18:32 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Harvey", "William", ""], ["Naderiparizi", "Saeid", ""], ["Wood", "Frank", ""]]}, {"id": "2102.12040", "submitter": "Min Xu", "authors": "Xuefeng Du, Haohan Wang, Zhenxi Zhu, Xiangrui Zeng, Yi-Wei Chang, Jing\n  Zhang, Min Xu", "title": "Active Learning to Classify Macromolecular Structures in situ for Less\n  Supervision in Cryo-Electron Tomography", "comments": "Statement on authorship changes: Dr. Eric Xing was an academic\n  advisor of Mr. Haohan Wang. Dr. Xing was not directly involved in this work\n  and has no direct interaction or collaboration with any other authors on this\n  work. Therefore, Dr. Xing is removed from the author list according to his\n  request. Mr. Zhenxi Zhu's affiliation is updated to his current affiliation", "journal-ref": null, "doi": "10.1093/bioinformatics/btab123", "report-no": null, "categories": "q-bio.QM cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivation: Cryo-Electron Tomography (cryo-ET) is a 3D bioimaging tool that\nvisualizes the structural and spatial organization of macromolecules at a\nnear-native state in single cells, which has broad applications in life\nscience. However, the systematic structural recognition and recovery of\nmacromolecules captured by cryo-ET are difficult due to high structural\ncomplexity and imaging limits. Deep learning based subtomogram classification\nhave played critical roles for such tasks. As supervised approaches, however,\ntheir performance relies on sufficient and laborious annotation on a large\ntraining dataset.\n  Results: To alleviate this major labeling burden, we proposed a Hybrid Active\nLearning (HAL) framework for querying subtomograms for labelling from a large\nunlabeled subtomogram pool. Firstly, HAL adopts uncertainty sampling to select\nthe subtomograms that have the most uncertain predictions. Moreover, to\nmitigate the sampling bias caused by such strategy, a discriminator is\nintroduced to judge if a certain subtomogram is labeled or unlabeled and\nsubsequently the model queries the subtomogram that have higher probabilities\nto be unlabeled. Additionally, HAL introduces a subset sampling strategy to\nimprove the diversity of the query set, so that the information overlap is\ndecreased between the queried batches and the algorithmic efficiency is\nimproved. Our experiments on subtomogram classification tasks using both\nsimulated and real data demonstrate that we can achieve comparable testing\nperformance (on average only 3% accuracy drop) by using less than 30% of the\nlabeled subtomograms, which shows a very promising result for subtomogram\nclassification task with limited labeling resources.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 03:10:32 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 17:21:01 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Du", "Xuefeng", ""], ["Wang", "Haohan", ""], ["Zhu", "Zhenxi", ""], ["Zeng", "Xiangrui", ""], ["Chang", "Yi-Wei", ""], ["Zhang", "Jing", ""], ["Xu", "Min", ""]]}, {"id": "2102.12056", "submitter": "Changfa Shi", "authors": "Changfa Shi, Min Xian, Xiancheng Zhou, Haotian Wang, Heng-Da Cheng", "title": "Multi-Slice Low-Rank Tensor Decomposition Based Multi-Atlas\n  Segmentation: Application to Automatic Pathological Liver CT Segmentation", "comments": "9 Figures, 41 pages, accepted by Medical Image Analysis,\n  DOI:10.1016/j.media.2021.102152", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Liver segmentation from abdominal CT images is an essential step for liver\ncancer computer-aided diagnosis and surgical planning. However, both the\naccuracy and robustness of existing liver segmentation methods cannot meet the\nrequirements of clinical applications. In particular, for the common clinical\ncases where the liver tissue contains major pathology, current segmentation\nmethods show poor performance. In this paper, we propose a novel low-rank\ntensor decomposition (LRTD) based multi-atlas segmentation (MAS) framework that\nachieves accurate and robust pathological liver segmentation of CT images.\nFirstly, we propose a multi-slice LRTD scheme to recover the underlying\nlow-rank structure embedded in 3D medical images. It performs the LRTD on small\nimage segments consisting of multiple consecutive image slices. Then, we\npresent an LRTD-based atlas construction method to generate tumor-free liver\natlases that mitigates the performance degradation of liver segmentation due to\nthe presence of tumors. Finally, we introduce an LRTD-based MAS algorithm to\nderive patient-specific liver atlases for each test image, and to achieve\naccurate pairwise image registration and label propagation. Extensive\nexperiments on three public databases of pathological liver cases validate the\neffectiveness of the proposed method. Both qualitative and quantitative results\ndemonstrate that, in the presence of major pathology, the proposed method is\nmore accurate and robust than state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 04:09:39 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 01:07:51 GMT"}, {"version": "v3", "created": "Sat, 17 Jul 2021 03:36:45 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Shi", "Changfa", ""], ["Xian", "Min", ""], ["Zhou", "Xiancheng", ""], ["Wang", "Haotian", ""], ["Cheng", "Heng-Da", ""]]}, {"id": "2102.12061", "submitter": "Zhen Zeng", "authors": "Zhen Zeng, Tucker Balch, Manuela Veloso", "title": "Deep Video Prediction for Time Series Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series forecasting is essential for decision making in many domains. In\nthis work, we address the challenge of predicting prices evolution among\nmultiple potentially interacting financial assets. A solution to this problem\nhas obvious importance for governments, banks, and investors. Statistical\nmethods such as Auto Regressive Integrated Moving Average (ARIMA) are widely\napplied to these problems. In this paper, we propose to approach economic time\nseries forecasting of multiple financial assets in a novel way via video\nprediction. Given past prices of multiple potentially interacting financial\nassets, we aim to predict the prices evolution in the future. Instead of\ntreating the snapshot of prices at each time point as a vector, we spatially\nlayout these prices in 2D as an image, such that we can harness the power of\nCNNs in learning a latent representation for these financial assets. Thus, the\nhistory of these prices becomes a sequence of images, and our goal becomes\npredicting future images. We build on a state-of-the-art video prediction\nmethod for forecasting future images. Our experiments involve the prediction\ntask of the price evolution of nine financial assets traded in U.S. stock\nmarkets. The proposed method outperforms baselines including ARIMA, Prophet,\nand variations of the proposed method, demonstrating the benefits of harnessing\nthe power of CNNs in the problem of economic time series forecasting.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 04:27:23 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Zeng", "Zhen", ""], ["Balch", "Tucker", ""], ["Veloso", "Manuela", ""]]}, {"id": "2102.12092", "submitter": "Aditya Ramesh", "authors": "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss,\n  Alec Radford, Mark Chen, Ilya Sutskever", "title": "Zero-Shot Text-to-Image Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text-to-image generation has traditionally focused on finding better modeling\nassumptions for training on a fixed dataset. These assumptions might involve\ncomplex architectures, auxiliary losses, or side information such as object\npart labels or segmentation masks supplied during training. We describe a\nsimple approach for this task based on a transformer that autoregressively\nmodels the text and image tokens as a single stream of data. With sufficient\ndata and scale, our approach is competitive with previous domain-specific\nmodels when evaluated in a zero-shot fashion.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 06:42:31 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 23:26:05 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Ramesh", "Aditya", ""], ["Pavlov", "Mikhail", ""], ["Goh", "Gabriel", ""], ["Gray", "Scott", ""], ["Voss", "Chelsea", ""], ["Radford", "Alec", ""], ["Chen", "Mark", ""], ["Sutskever", "Ilya", ""]]}, {"id": "2102.12093", "submitter": "Yang You", "authors": "Yang You, Yujing Lou, Ruoxi Shi, Qi Liu, Yu-Wing Tai, Lizhuang Ma,\n  Weiming Wang, Cewu Lu", "title": "PRIN/SPRIN: On Extracting Point-wise Rotation Invariant Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud analysis without pose priors is very challenging in real\napplications, as the orientations of point clouds are often unknown. In this\npaper, we propose a brand new point-set learning framework PRIN, namely,\nPoint-wise Rotation Invariant Network, focusing on rotation invariant feature\nextraction in point clouds analysis. We construct spherical signals by Density\nAware Adaptive Sampling to deal with distorted point distributions in spherical\nspace. Spherical Voxel Convolution and Point Re-sampling are proposed to\nextract rotation invariant features for each point. In addition, we extend PRIN\nto a sparse version called SPRIN, which directly operates on sparse point\nclouds. Both PRIN and SPRIN can be applied to tasks ranging from object\nclassification, part segmentation, to 3D feature matching and label alignment.\nResults show that, on the dataset with randomly rotated point clouds, SPRIN\ndemonstrates better performance than state-of-the-art methods without any data\naugmentation. We also provide thorough theoretical proof and analysis for\npoint-wise rotation invariance achieved by our methods. Our code is available\non https://github.com/qq456cvb/SPRIN.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 06:44:09 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["You", "Yang", ""], ["Lou", "Yujing", ""], ["Shi", "Ruoxi", ""], ["Liu", "Qi", ""], ["Tai", "Yu-Wing", ""], ["Ma", "Lizhuang", ""], ["Wang", "Weiming", ""], ["Lu", "Cewu", ""]]}, {"id": "2102.12095", "submitter": "Shunxin Xu", "authors": "Shunxin Xu, Ke Sun, Dong Liu, Zhiwei Xiong, Zheng-Jun Zha", "title": "Synergy Between Semantic Segmentation and Image Denoising via Alternate\n  Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The capability of image semantic segmentation may be deteriorated due to\nnoisy input image, where image denoising prior to segmentation helps. Both\nimage denoising and semantic segmentation have been developed significantly\nwith the advance of deep learning. Thus, we are interested in the synergy\nbetween them by using a holistic deep model. We observe that not only denoising\nhelps combat the drop of segmentation accuracy due to noise, but also\npixel-wise semantic information boosts the capability of denoising. We then\npropose a boosting network to perform denoising and segmentation alternately.\nThe proposed network is composed of multiple segmentation and denoising blocks\n(SDBs), each of which estimates semantic map then uses the map to regularize\ndenoising. Experimental results show that the denoised image quality is\nimproved substantially and the segmentation accuracy is improved to close to\nthat of clean images. Our code and models will be made publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 06:48:45 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Xu", "Shunxin", ""], ["Sun", "Ke", ""], ["Liu", "Dong", ""], ["Xiong", "Zhiwei", ""], ["Zha", "Zheng-Jun", ""]]}, {"id": "2102.12096", "submitter": "Jianzhun Shao", "authors": "Jianzhun Shao, Yuhang Jiang, Gu Wang, Zhigang Li, Xiangyang Ji", "title": "PFRL: Pose-Free Reinforcement Learning for 6D Pose Estimation", "comments": null, "journal-ref": "In Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition, pp. 11454-11463. 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  6D pose estimation from a single RGB image is a challenging and vital task in\ncomputer vision. The current mainstream deep model methods resort to 2D images\nannotated with real-world ground-truth 6D object poses, whose collection is\nfairly cumbersome and expensive, even unavailable in many cases. In this work,\nto get rid of the burden of 6D annotations, we formulate the 6D pose refinement\nas a Markov Decision Process and impose on the reinforcement learning approach\nwith only 2D image annotations as weakly-supervised 6D pose information, via a\ndelicate reward definition and a composite reinforced optimization method for\nefficient and effective policy training. Experiments on LINEMOD and T-LESS\ndatasets demonstrate that our Pose-Free approach is able to achieve\nstate-of-the-art performance compared with the methods without using real-world\nground-truth 6D pose labels.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 06:49:41 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Shao", "Jianzhun", ""], ["Jiang", "Yuhang", ""], ["Wang", "Gu", ""], ["Li", "Zhigang", ""], ["Ji", "Xiangyang", ""]]}, {"id": "2102.12122", "submitter": "Wenhai Wang", "authors": "Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding\n  Liang, Tong Lu, Ping Luo, Ling Shao", "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction\n  without Convolutions", "comments": "13 pages, 6 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although using convolutional neural networks (CNNs) as backbones achieves\ngreat successes in computer vision, this work investigates a simple backbone\nnetwork useful for many dense prediction tasks without convolutions. Unlike the\nrecently-proposed Transformer model (e.g., ViT) that is specially designed for\nimage classification, we propose Pyramid Vision Transformer~(PVT), which\novercomes the difficulties of porting Transformer to various dense prediction\ntasks. PVT has several merits compared to prior arts. (1) Different from ViT\nthat typically has low-resolution outputs and high computational and memory\ncost, PVT can be not only trained on dense partitions of the image to achieve\nhigh output resolution, which is important for dense predictions but also using\na progressive shrinking pyramid to reduce computations of large feature maps.\n(2) PVT inherits the advantages from both CNN and Transformer, making it a\nunified backbone in various vision tasks without convolutions by simply\nreplacing CNN backbones. (3) We validate PVT by conducting extensive\nexperiments, showing that it boosts the performance of many downstream tasks,\ne.g., object detection, semantic, and instance segmentation. For example, with\na comparable number of parameters, RetinaNet+PVT achieves 40.4 AP on the COCO\ndataset, surpassing RetinNet+ResNet50 (36.3 AP) by 4.1 absolute AP. We hope PVT\ncould serve as an alternative and useful backbone for pixel-level predictions\nand facilitate future researches. Code is available at\nhttps://github.com/whai362/PVT.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 08:33:55 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Wang", "Wenhai", ""], ["Xie", "Enze", ""], ["Li", "Xiang", ""], ["Fan", "Deng-Ping", ""], ["Song", "Kaitao", ""], ["Liang", "Ding", ""], ["Lu", "Tong", ""], ["Luo", "Ping", ""], ["Shao", "Ling", ""]]}, {"id": "2102.12127", "submitter": "Ngoc Tran", "authors": "Toan Pham Van, Son Trung Nguyen, Linh Bao Doan, Ngoc N. Tran and Ta\n  Minh Thanh", "title": "Efficient Palm-Line Segmentation with U-Net Context Fusion Module", "comments": "Published in 2020 International Conference on Advanced Computing and\n  Applications (ACOMP)", "journal-ref": "2020 International Conference on Advanced Computing and\n  Applications (ACOMP), Quy Nhon, Vietnam, 2020, pp. 23-28", "doi": "10.1109/ACOMP50827.2020.00011", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Many cultures around the world believe that palm reading can be used to\npredict the future life of a person. Palmistry uses features of the hand such\nas palm lines, hand shape, or fingertip position. However, the research on\npalm-line detection is still scarce, many of them applied traditional image\nprocessing techniques. In most real-world scenarios, images usually are not in\nwell-conditioned, causing these methods to severely under-perform. In this\npaper, we propose an algorithm to extract principle palm lines from an image of\na person's hand. Our method applies deep learning networks (DNNs) to improve\nperformance. Another challenge of this problem is the lack of training data. To\ndeal with this issue, we handcrafted a dataset from scratch. From this dataset,\nwe compare the performance of readily available methods with ours. Furthermore,\nbased on the UNet segmentation neural network architecture and the knowledge of\nattention mechanism, we propose a highly efficient architecture to detect\npalm-lines. We proposed the Context Fusion Module to capture the most important\ncontext feature, which aims to improve segmentation accuracy. The experimental\nresults show that it outperforms the other methods with the highest F1 Score\nabout 99.42% and mIoU is 0.584 for the same dataset.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 08:42:52 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Van", "Toan Pham", ""], ["Nguyen", "Son Trung", ""], ["Doan", "Linh Bao", ""], ["Tran", "Ngoc N.", ""], ["Thanh", "Ta Minh", ""]]}, {"id": "2102.12129", "submitter": "Jingjing Wang", "authors": "Jingjing Wang, Jingyi Zhang, Ying Bian, Youyi Cai, Chunmao Wang,\n  Shiliang Pu", "title": "Self-Domain Adaptation for Face Anti-Spoofing", "comments": "Camera Ready, AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Although current face anti-spoofing methods achieve promising results under\nintra-dataset testing, they suffer from poor generalization to unseen attacks.\nMost existing works adopt domain adaptation (DA) or domain generalization (DG)\ntechniques to address this problem. However, the target domain is often unknown\nduring training which limits the utilization of DA methods. DG methods can\nconquer this by learning domain invariant features without seeing any target\ndata. However, they fail in utilizing the information of target data. In this\npaper, we propose a self-domain adaptation framework to leverage the unlabeled\ntest domain data at inference. Specifically, a domain adaptor is designed to\nadapt the model for test domain. In order to learn a better adaptor, a\nmeta-learning based adaptor learning algorithm is proposed using the data of\nmultiple source domains at the training step. At test time, the adaptor is\nupdated using only the test domain data according to the proposed unsupervised\nadaptor loss to further improve the performance. Extensive experiments on four\npublic datasets validate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 08:46:39 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Wang", "Jingjing", ""], ["Zhang", "Jingyi", ""], ["Bian", "Ying", ""], ["Cai", "Youyi", ""], ["Wang", "Chunmao", ""], ["Pu", "Shiliang", ""]]}, {"id": "2102.12135", "submitter": "Qiaosi Yi", "authors": "Qiaosi Yi, Juncheng Li, Faming Fang, Aiwen Jiang, Guixu Zhang", "title": "Efficient and Accurate Multi-scale Topological Network for Single Image\n  Dehazing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Single image dehazing is a challenging ill-posed problem that has drawn\nsignificant attention in the last few years. Recently, convolutional neural\nnetworks have achieved great success in image dehazing. However, it is still\ndifficult for these increasingly complex models to recover accurate details\nfrom the hazy image. In this paper, we pay attention to the feature extraction\nand utilization of the input image itself. To achieve this, we propose a\nMulti-scale Topological Network (MSTN) to fully explore the features at\ndifferent scales. Meanwhile, we design a Multi-scale Feature Fusion Module\n(MFFM) and an Adaptive Feature Selection Module (AFSM) to achieve the selection\nand fusion of features at different scales, so as to achieve progressive image\ndehazing. This topological network provides a large number of search paths that\nenable the network to extract abundant image features as well as strong fault\ntolerance and robustness. In addition, ASFM and MFFM can adaptively select\nimportant features and ignore interference information when fusing different\nscale representations. Extensive experiments are conducted to demonstrate the\nsuperiority of our method compared with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 08:53:14 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Yi", "Qiaosi", ""], ["Li", "Juncheng", ""], ["Fang", "Faming", ""], ["Jiang", "Aiwen", ""], ["Zhang", "Guixu", ""]]}, {"id": "2102.12137", "submitter": "Daoxun Xia", "authors": "Haojie Liu, Shun Ma, Daoxun Xia, and Shaozi Li", "title": "SFANet: A Spectrum-aware Feature Augmentation Network for\n  Visible-Infrared Person Re-Identification", "comments": "This paper has been submitted to the journal of IEEE Transactions on\n  Neural Networks and Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visible-Infrared person re-identification (VI-ReID) is a challenging matching\nproblem due to large modality varitions between visible and infrared images.\nExisting approaches usually bridge the modality gap with only feature-level\nconstraints, ignoring pixel-level variations. Some methods employ GAN to\ngenerate style-consistent images, but it destroys the structure information and\nincurs a considerable level of noise. In this paper, we explicitly consider\nthese challenges and formulate a novel spectrum-aware feature augementation\nnetwork named SFANet for cross-modality matching problem. Specifically, we put\nforward to employ grayscale-spectrum images to fully replace RGB images for\nfeature learning. Learning with the grayscale-spectrum images, our model can\napparently reduce modality discrepancy and detect inner structure relations\nacross the different modalities, making it robust to color variations. In\nfeature-level, we improve the conventional two-stream network through balancing\nthe number of specific and sharable convolutional blocks, which preserve the\nspatial structure information of features. Additionally, a bi-directional\ntri-constrained top-push ranking loss (BTTR) is embedded in the proposed\nnetwork to improve the discriminability, which efficiently further boosts the\nmatching accuracy. Meanwhile, we further introduce an effective dual-linear\nwith batch normalization ID embedding method to model the identity-specific\ninformation and assits BTTR loss in magnitude stabilizing. On SYSU-MM01 and\nRegDB datasets, we conducted extensively experiments to demonstrate that our\nproposed framework contributes indispensably and achieves a very competitive\nVI-ReID performance.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 08:57:32 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Liu", "Haojie", ""], ["Ma", "Shun", ""], ["Xia", "Daoxun", ""], ["Li", "Shaozi", ""]]}, {"id": "2102.12139", "submitter": "Ngoc Tran", "authors": "Toan Pham Van, Tam Minh Nguyen, Ngoc N. Tran, Hoai Viet Nguyen, Linh\n  Bao Doan, Huy Quang Dao and Thanh Ta Minh", "title": "Interpreting the Latent Space of Generative Adversarial Networks using\n  Supervised Learning", "comments": "Published in 2020 International Conference on Advanced Computing and\n  Applications (ACOMP)", "journal-ref": "2020 International Conference on Advanced Computing and\n  Applications (ACOMP), Quy Nhon, Vietnam, 2020, pp. 49-54", "doi": "10.1109/ACOMP50827.2020.00015", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  With great progress in the development of Generative Adversarial Networks\n(GANs), in recent years, the quest for insights in understanding and\nmanipulating the latent space of GAN has gained more and more attention due to\nits wide range of applications. While most of the researches on this task have\nfocused on unsupervised learning method, which induces difficulties in training\nand limitation in results, our work approaches another direction, encoding\nhuman's prior knowledge to discover more about the hidden space of GAN. With\nthis supervised manner, we produce promising results, demonstrated by accurate\nmanipulation of generated images. Even though our model is more suitable for\ntask-specific problems, we hope that its ease in implementation, preciseness,\nrobustness, and the allowance of richer set of properties (compared to other\napproaches) for image manipulation can enhance the result of many current\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 09:00:18 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Van", "Toan Pham", ""], ["Nguyen", "Tam Minh", ""], ["Tran", "Ngoc N.", ""], ["Nguyen", "Hoai Viet", ""], ["Doan", "Linh Bao", ""], ["Dao", "Huy Quang", ""], ["Minh", "Thanh Ta", ""]]}, {"id": "2102.12145", "submitter": "Gu Wang", "authors": "Gu Wang, Fabian Manhardt, Federico Tombari, Xiangyang Ji", "title": "GDR-Net: Geometry-Guided Direct Regression Network for Monocular 6D\n  Object Pose Estimation", "comments": "CVPR 2021 camera ready, typo fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  6D pose estimation from a single RGB image is a fundamental task in computer\nvision. The current top-performing deep learning-based methods rely on an\nindirect strategy, i.e., first establishing 2D-3D correspondences between the\ncoordinates in the image plane and object coordinate system, and then applying\na variant of the P$n$P/RANSAC algorithm. However, this two-stage pipeline is\nnot end-to-end trainable, thus is hard to be employed for many tasks requiring\ndifferentiable poses. On the other hand, methods based on direct regression are\ncurrently inferior to geometry-based methods. In this work, we perform an\nin-depth investigation on both direct and indirect methods, and propose a\nsimple yet effective Geometry-guided Direct Regression Network (GDR-Net) to\nlearn the 6D pose in an end-to-end manner from dense correspondence-based\nintermediate geometric representations. Extensive experiments show that our\napproach remarkably outperforms state-of-the-art methods on LM, LM-O and YCB-V\ndatasets. Code is available at https://git.io/GDR-Net.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 09:11:31 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 14:17:41 GMT"}, {"version": "v3", "created": "Tue, 9 Mar 2021 05:26:14 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Wang", "Gu", ""], ["Manhardt", "Fabian", ""], ["Tombari", "Federico", ""], ["Ji", "Xiangyang", ""]]}, {"id": "2102.12147", "submitter": "Jinghua Zhang", "authors": "Frank Kulwa, Chen Li, Jinghua Zhang, Kimiaki Shirahama, Sergey Kosov,\n  Xin Zhao, Hongzan Sun, Tao Jiang, Marcin Grzegorzek", "title": "A New Pairwise Deep Learning Feature For Environmental Microorganism\n  Image Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environmental microorganism (EM) offers a high-efficient, harmless, and\nlow-cost solution to environmental pollution. They are used in sanitation,\nmonitoring, and decomposition of environmental pollutants. However, this\ndepends on the proper identification of suitable microorganisms. In order to\nfasten, low the cost, increase consistency and accuracy of identification, we\npropose the novel pairwise deep learning features to analyze microorganisms.\nThe pairwise deep learning features technique combines the capability of\nhandcrafted and deep learning features. In this technique we, leverage the Shi\nand Tomasi interest points by extracting deep learning features from patches\nwhich are centered at interest points locations. Then, to increase the number\nof potential features that have intermediate spatial characteristics between\nnearby interest points, we use Delaunay triangulation theorem and straight-line\ngeometric theorem to pair the nearby deep learning features. The potential of\npairwise features is justified on the classification of EMs using SVMs, k-NN,\nand Random Forest classifier. The pairwise features obtain outstanding results\nof 99.17%, 91.34%, 91.32%, 91.48%, and 99.56%, which are the increase of about\n5.95%, 62.40%, 62.37%, 61.84%, and 3.23% in accuracy, F1-score, recall,\nprecision, and specificity respectively, compared to non-paired deep learning\nfeatures.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 09:14:06 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Kulwa", "Frank", ""], ["Li", "Chen", ""], ["Zhang", "Jinghua", ""], ["Shirahama", "Kimiaki", ""], ["Kosov", "Sergey", ""], ["Zhao", "Xin", ""], ["Sun", "Hongzan", ""], ["Jiang", "Tao", ""], ["Grzegorzek", "Marcin", ""]]}, {"id": "2102.12152", "submitter": "Tung-I Chen", "authors": "Tung-I Chen, Yueh-Cheng Liu, Hung-Ting Su, Yu-Cheng Chang, Yu-Hsiang\n  Lin, Jia-Fong Yeh, Winston H. Hsu", "title": "Dual-Awareness Attention for Few-Shot Object Detection", "comments": null, "journal-ref": "IEEE Transactions on Multimedia 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recent progress has significantly boosted few-shot classification (FSC)\nperformance, few-shot object detection (FSOD) remains challenging for modern\nlearning systems. Existing FSOD systems follow FSC approaches, ignoring the\nissues of spatial misalignment and vagueness in class representations, and\nconsequently result in low performance. Observing this, we propose a novel\nDual-Awareness Attention (DAnA) mechanism that can adaptively generate\nquery-position-aware (QPA) support features and guide the detection networks\nprecisely. The generated QPA features represent local information of a support\nimage conditioned on a given region of the query. By taking the spatial\nrelationships across different images into consideration, our approach\nconspicuously outperforms previous FSOD methods (+6.9 AP relatively) and\nachieves remarkable results even under a challenging cross-dataset evaluation\nsetting. Furthermore, the proposed DAnA component is flexible and adaptable to\nmultiple existing object detection frameworks. By equipping DAnA, conventional\nobject detection models, Faster R-CNN and RetinaNet, which are not designed\nexplicitly for few-shot learning, reach state-of-the-art performance in FSOD\ntasks.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 09:17:27 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 08:40:00 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Chen", "Tung-I", ""], ["Liu", "Yueh-Cheng", ""], ["Su", "Hung-Ting", ""], ["Chang", "Yu-Cheng", ""], ["Lin", "Yu-Hsiang", ""], ["Yeh", "Jia-Fong", ""], ["Hsu", "Winston H.", ""]]}, {"id": "2102.12154", "submitter": "Jingjing Wang", "authors": "Jingwei Yan, Boyuan Jiang, Jingjing Wang, Qiang Li, Chunmao Wang,\n  Shiliang Pu", "title": "Multi-Level Adaptive Region of Interest and Graph Learning for Facial\n  Action Unit Recognition", "comments": "Camera Ready, ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In facial action unit (AU) recognition tasks, regional feature learning and\nAU relation modeling are two effective aspects which are worth exploring.\nHowever, the limited representation capacity of regional features makes it\ndifficult for relation models to embed AU relationship knowledge. In this\npaper, we propose a novel multi-level adaptive ROI and graph learning (MARGL)\nframework to tackle this problem. Specifically, an adaptive ROI learning module\nis designed to automatically adjust the location and size of the predefined AU\nregions. Meanwhile, besides relationship between AUs, there exists strong\nrelevance between regional features across multiple levels of the backbone\nnetwork as level-wise features focus on different aspects of representation. In\norder to incorporate the intra-level AU relation and inter-level AU regional\nrelevance simultaneously, a multi-level AU relation graph is constructed and\ngraph convolution is performed to further enhance AU regional features of each\nlevel. Experiments on BP4D and DISFA demonstrate the proposed MARGL\nsignificantly outperforms the previous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 09:22:45 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Yan", "Jingwei", ""], ["Jiang", "Boyuan", ""], ["Wang", "Jingjing", ""], ["Li", "Qiang", ""], ["Wang", "Chunmao", ""], ["Pu", "Shiliang", ""]]}, {"id": "2102.12191", "submitter": "Md Mamunur Rahaman", "authors": "Md Mamunur Rahaman, Chen Li, Yudong Yao, Frank Kulwa, Xiangchen Wu,\n  Xiaoyan Li, Qian Wang", "title": "DeepCervix: A Deep Learning-based Framework for the Classification of\n  Cervical Cells Using Hybrid Deep Feature Fusion Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cervical cancer, one of the most common fatal cancers among women, can be\nprevented by regular screening to detect any precancerous lesions at early\nstages and treat them. Pap smear test is a widely performed screening technique\nfor early detection of cervical cancer, whereas this manual screening method\nsuffers from high false-positive results because of human errors. To improve\nthe manual screening practice, machine learning (ML) and deep learning (DL)\nbased computer-aided diagnostic (CAD) systems have been investigated widely to\nclassify cervical pap cells. Most of the existing researches require\npre-segmented images to obtain good classification results, whereas accurate\ncervical cell segmentation is challenging because of cell clustering. Some\nstudies rely on handcrafted features, which cannot guarantee the classification\nstage's optimality. Moreover, DL provides poor performance for a multiclass\nclassification task when there is an uneven distribution of data, which is\nprevalent in the cervical cell dataset. This investigation has addressed those\nlimitations by proposing DeepCervix, a hybrid deep feature fusion (HDFF)\ntechnique based on DL to classify the cervical cells accurately. Our proposed\nmethod uses various DL models to capture more potential information to enhance\nclassification performance. Our proposed HDFF method is tested on the publicly\navailable SIPAKMED dataset and compared the performance with base DL models and\nthe LF method. For the SIPAKMED dataset, we have obtained the state-of-the-art\nclassification accuracy of 99.85%, 99.38%, and 99.14% for 2-class, 3-class, and\n5-class classification. Moreover, our method is tested on the Herlev dataset\nand achieves an accuracy of 98.32% for binary class and 90.32% for 7-class\nclassification.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 10:34:51 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Rahaman", "Md Mamunur", ""], ["Li", "Chen", ""], ["Yao", "Yudong", ""], ["Kulwa", "Frank", ""], ["Wu", "Xiangchen", ""], ["Li", "Xiaoyan", ""], ["Wang", "Qian", ""]]}, {"id": "2102.12205", "submitter": "Zhuoling Li", "authors": "Zhuoling Li, Haohan Wang, Tymoteusz Swistek, Weixin Chen, Yuanzheng\n  Li, Haoqian Wang", "title": "Enabling the Network to Surf the Internet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning is challenging due to the limited data and labels. Existing\nalgorithms usually resolve this problem by pre-training the model with a\nconsiderable amount of annotated data which shares knowledge with the target\ndomain. Nevertheless, large quantities of homogenous data samples are not\nalways available. To tackle this issue, we develop a framework that enables the\nmodel to surf the Internet, which implies that the model can collect and\nannotate data without manual effort. Since the online data is virtually\nlimitless and continues to be generated, the model can thus be empowered to\nconstantly obtain up-to-date knowledge from the Internet. Additionally, we\nobserve that the generalization ability of the learned representation is\ncrucial for self-supervised learning. To present its importance, a naive yet\nefficient normalization strategy is proposed. Consequentially, this strategy\nboosts the accuracy of the model significantly (20.46% at most). We demonstrate\nthe superiority of the proposed framework with experiments on miniImageNet,\ntieredImageNet and Omniglot. The results indicate that our method has surpassed\nprevious unsupervised counterparts by a large margin (more than 10%) and\nobtained performance comparable with the supervised ones.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 11:00:29 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Li", "Zhuoling", ""], ["Wang", "Haohan", ""], ["Swistek", "Tymoteusz", ""], ["Chen", "Weixin", ""], ["Li", "Yuanzheng", ""], ["Wang", "Haoqian", ""]]}, {"id": "2102.12213", "submitter": "Francesco Pelosin", "authors": "Francesco Pelosin, Andrea Gasparetto, Andrea Albarelli, Andrea\n  Torsello", "title": "Unsupervised semantic discovery through visual patterns detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new fast fully unsupervised method to discover semantic\npatterns. Our algorithm is able to hierarchically find visual categories and\nproduce a segmentation mask where previous methods fail. Through the modeling\nof what is a visual pattern in an image, we introduce the notion of \"semantic\nlevels\" and devise a conceptual framework along with measures and a dedicated\nbenchmark dataset for future comparisons. Our algorithm is composed by two\nphases. A filtering phase, which selects semantical hotsposts by means of an\naccumulator space, then a clustering phase which propagates the semantic\nproperties of the hotspots on a superpixels basis. We provide both qualitative\nand quantitative experimental validation, achieving optimal results in terms of\nrobustness to noise and semantic consistency. We also made code and dataset\npublicly available.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 11:13:15 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Pelosin", "Francesco", ""], ["Gasparetto", "Andrea", ""], ["Albarelli", "Andrea", ""], ["Torsello", "Andrea", ""]]}, {"id": "2102.12218", "submitter": "Sanat Ramesh", "authors": "Sanat Ramesh, Diego Dall'Alba, Cristians Gonzalez, Tong Yu, Pietro\n  Mascagni, Didier Mutter, Jacques Marescaux, Paolo Fiorini, Nicolas Padoy", "title": "Multi-Task Temporal Convolutional Networks for Joint Recognition of\n  Surgical Phases and Steps in Gastric Bypass Procedures", "comments": "Accepted to IPCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Purpose: Automatic segmentation and classification of surgical activity is\ncrucial for providing advanced support in computer-assisted interventions and\nautonomous functionalities in robot-assisted surgeries. Prior works have\nfocused on recognizing either coarse activities, such as phases, or\nfine-grained activities, such as gestures. This work aims at jointly\nrecognizing two complementary levels of granularity directly from videos,\nnamely phases and steps. Method: We introduce two correlated surgical\nactivities, phases and steps, for the laparoscopic gastric bypass procedure. We\npropose a Multi-task Multi-Stage Temporal Convolutional Network (MTMS-TCN)\nalong with a multi-task Convolutional Neural Network (CNN) training setup to\njointly predict the phases and steps and benefit from their complementarity to\nbetter evaluate the execution of the procedure. We evaluate the proposed method\non a large video dataset consisting of 40 surgical procedures (Bypass40).\nResults: We present experimental results from several baseline models for both\nphase and step recognition on the Bypass40 dataset. The proposed MTMS-TCN\nmethod outperforms in both phase and step recognition by 1-2% in accuracy,\nprecision and recall, compared to single-task methods. Furthermore, for step\nrecognition, MTMS-TCN achieves a superior performance of 3-6% compared to LSTM\nbased models in accuracy, precision, and recall. Conclusion: In this work, we\npresent a multi-task multi-stage temporal convolutional network for surgical\nactivity recognition, which shows improved results compared to single-task\nmodels on the Bypass40 gastric bypass dataset with multi-level annotations. The\nproposed method shows that the joint modeling of phases and steps is beneficial\nto improve the overall recognition of each type of activity.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 11:18:19 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Ramesh", "Sanat", ""], ["Dall'Alba", "Diego", ""], ["Gonzalez", "Cristians", ""], ["Yu", "Tong", ""], ["Mascagni", "Pietro", ""], ["Mutter", "Didier", ""], ["Marescaux", "Jacques", ""], ["Fiorini", "Paolo", ""], ["Padoy", "Nicolas", ""]]}, {"id": "2102.12219", "submitter": "Gui-Song Xia", "authors": "Jian Ding, Nan Xue, Gui-Song Xia, Xiang Bai, Wen Yang, Micheal Ying\n  Yang, Serge Belongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, Liangpei\n  Zhang", "title": "Object Detection in Aerial Images: A Large-Scale Benchmark and\n  Challenges", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade, object detection has achieved significant progress in\nnatural images but not in aerial images, due to the massive variations in the\nscale and orientation of objects caused by the bird's-eye view of aerial\nimages. More importantly, the lack of large-scale benchmarks becomes a major\nobstacle to the development of object detection in aerial images (ODAI). In\nthis paper, we present a large-scale Dataset of Object deTection in Aerial\nimages (DOTA) and comprehensive baselines for ODAI. The proposed DOTA dataset\ncontains 1,793,658 object instances of 18 categories of oriented-bounding-box\nannotations collected from 11,268 aerial images. Based on this large-scale and\nwell-annotated dataset, we build baselines covering 10 state-of-the-art\nalgorithms with over 70 configurations, where the speed and accuracy\nperformances of each model have been evaluated. Furthermore, we provide a\nuniform code library for ODAI and build a website for testing and evaluating\ndifferent algorithms. Previous challenges run on DOTA have attracted more than\n1300 teams worldwide. We believe that the expanded large-scale DOTA dataset,\nthe extensive baselines, the code library and the challenges can facilitate the\ndesigns of robust algorithms and reproducible research on the problem of object\ndetection in aerial images.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 11:20:55 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Ding", "Jian", ""], ["Xue", "Nan", ""], ["Xia", "Gui-Song", ""], ["Bai", "Xiang", ""], ["Yang", "Wen", ""], ["Yang", "Micheal Ying", ""], ["Belongie", "Serge", ""], ["Luo", "Jiebo", ""], ["Datcu", "Mihai", ""], ["Pelillo", "Marcello", ""], ["Zhang", "Liangpei", ""]]}, {"id": "2102.12239", "submitter": "Matthias K\\\"ummerer", "authors": "Matthias K\\\"ummerer, Matthias Bethge", "title": "State-of-the-Art in Human Scanpath Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last years have seen a surge in models predicting the scanpaths of\nfixations made by humans when viewing images. However, the field is lacking a\nprincipled comparison of those models with respect to their predictive power.\nIn the past, models have usually been evaluated based on comparing human\nscanpaths to scanpaths generated from the model. Here, instead we evaluate\nmodels based on how well they predict each fixation in a scanpath given the\nprevious scanpath history. This makes model evaluation closely aligned with the\nbiological processes thought to underly scanpath generation and allows to apply\nestablished saliency metrics like AUC and NSS in an intuitive and interpretable\nway. We evaluate many existing models of scanpath prediction on the datasets\nMIT1003, MIT300, CAT2000 train and CAT200 test, for the first time giving a\ndetailed picture of the current state of the art of human scanpath prediction.\nWe also show that the discussed method of model benchmarking allows for more\ndetailed analyses leading to interesting insights about where and when models\nfail to predict human behaviour. The MIT/Tuebingen Saliency Benchmark will\nimplement the evaluation of scanpath models as detailed here, allowing\nresearchers to score their models on the established benchmark datasets MIT300\nand CAT2000.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 12:01:28 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["K\u00fcmmerer", "Matthias", ""], ["Bethge", "Matthias", ""]]}, {"id": "2102.12252", "submitter": "Dongwei Ren", "authors": "Zhaohui Zheng and Rongguang Ye and Ping Wang and Jun Wang and Dongwei\n  Ren and Wangmeng Zuo", "title": "Localization Distillation for Object Detection", "comments": "Our source code and trained models are publicly available at\n  https://github.com/HikariTJU/LD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge distillation (KD) has witnessed its powerful ability in learning\ncompact models in deep learning field, but it is still limited in distilling\nlocalization information for object detection. Existing KD methods for object\ndetection mainly focus on mimicking deep features between teacher model and\nstudent model, which not only is restricted by specific model architectures,\nbut also cannot distill localization ambiguity. In this paper, we first propose\nlocalization distillation (LD) for object detection. In particular, our LD can\nbe formulated as standard KD by adopting the general localization\nrepresentation of bounding box. Our LD is very flexible, and is applicable to\ndistill localization ambiguity for arbitrary architecture of teacher model and\nstudent model. Moreover, it is interesting to find that Self-LD, i.e.,\ndistilling teacher model itself, can further boost state-of-the-art\nperformance. Second, we suggest a teacher assistant (TA) strategy to fill the\npossible gap between teacher model and student model, by which the distillation\neffectiveness can be guaranteed even the selected teacher model is not optimal.\nOn benchmark datasets PASCAL VOC and MS COCO, our LD can consistently improve\nthe performance for student detectors, and also boosts state-of-the-art\ndetectors notably. Our source code and trained models are publicly available at\nhttps://github.com/HikariTJU/LD\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 12:26:21 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 07:23:17 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Zheng", "Zhaohui", ""], ["Ye", "Rongguang", ""], ["Wang", "Ping", ""], ["Wang", "Jun", ""], ["Ren", "Dongwei", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "2102.12256", "submitter": "Tianze Rong", "authors": "Tianze Rong, Hongxiang Cai, Yichao Xiong", "title": "An Enhanced Prohibited Items Recognition Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposed a new modeling method to promote the performance of prohibited\nitems recognition via X-ray image. We analyzed the characteristics of\nprohibited items and X-ray images. We found the fact that the scales of some\nitems are too small to be recognized which encumber the model performance. Then\nwe adopted a set of data augmentation and modified the model to adapt the field\nof prohibited items recognition. The Convolutional Block Attention Module(CBAM)\nand rescoring mechanism has been assembled into the model. By the modification,\nour model achieved a mAP of 89.9% on SIXray10, mAP of 74.8%.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 12:38:15 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Rong", "Tianze", ""], ["Cai", "Hongxiang", ""], ["Xiong", "Yichao", ""]]}, {"id": "2102.12281", "submitter": "Aydogan Ozcan", "authors": "Luzhe Huang, Tairan Liu, Xilin Yang, Yi Luo, Yair Rivenson, Aydogan\n  Ozcan", "title": "Holographic image reconstruction with phase recovery and autofocusing\n  using recurrent neural networks", "comments": "18 Pages, 7 Figures, 1 Table", "journal-ref": "ACS Photonics (2021)", "doi": "10.1021/acsphotonics.1c00337", "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital holography is one of the most widely used label-free microscopy\ntechniques in biomedical imaging. Recovery of the missing phase information of\na hologram is an important step in holographic image reconstruction. Here we\ndemonstrate a convolutional recurrent neural network (RNN) based phase recovery\napproach that uses multiple holograms, captured at different sample-to-sensor\ndistances to rapidly reconstruct the phase and amplitude information of a\nsample, while also performing autofocusing through the same network. We\ndemonstrated the success of this deep learning-enabled holography method by\nimaging microscopic features of human tissue samples and Papanicolaou (Pap)\nsmears. These results constitute the first demonstration of the use of\nrecurrent neural networks for holographic imaging and phase recovery, and\ncompared with existing methods, the presented approach improves the\nreconstructed image quality, while also increasing the depth-of-field and\ninference speed.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 01:51:43 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Huang", "Luzhe", ""], ["Liu", "Tairan", ""], ["Yang", "Xilin", ""], ["Luo", "Yi", ""], ["Rivenson", "Yair", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "2102.12295", "submitter": "Sergey Nesteruk", "authors": "Sergey Nesteruk, Dmitrii Shadrin, Mariia Pukalchik", "title": "Image Augmentation for Multitask Few-Shot Learning: Agricultural Domain\n  Use-Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large datasets' availability is catalyzing a rapid expansion of deep learning\nin general and computer vision in particular. At the same time, in many\ndomains, a sufficient amount of training data is lacking, which may become an\nobstacle to the practical application of computer vision techniques. This paper\nchallenges small and imbalanced datasets based on the example of a plant\nphenomics domain. We introduce an image augmentation framework, which enables\nus to extremely enlarge the number of training samples while providing the data\nfor such tasks as object detection, semantic segmentation, instance\nsegmentation, object counting, image denoising, and classification. We prove\nthat our augmentation method increases model performance when only a few\ntraining samples are available. In our experiment, we use the DeepLabV3 model\non semantic segmentation tasks with Arabidopsis and Nicotiana tabacum image\ndataset. The obtained result shows a 9% relative increase in model performance\ncompared to the basic image augmentation techniques.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 14:08:34 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Nesteruk", "Sergey", ""], ["Shadrin", "Dmitrii", ""], ["Pukalchik", "Mariia", ""]]}, {"id": "2102.12308", "submitter": "Omri Bar", "authors": "Daniel Neimark, Omri Bar, Maya Zohar, Gregory D. Hager, Dotan\n  Asselmann", "title": "\"Train one, Classify one, Teach one\" -- Cross-surgery transfer learning\n  for surgical step recognition", "comments": "MIDL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work demonstrated the ability of machine learning to automatically\nrecognize surgical workflow steps from videos. However, these studies focused\non only a single type of procedure. In this work, we analyze, for the first\ntime, surgical step recognition on four different laparoscopic surgeries:\nCholecystectomy, Right Hemicolectomy, Sleeve Gastrectomy, and Appendectomy.\nInspired by the traditional apprenticeship model, in which surgical training is\nbased on the Halstedian method, we paraphrase the \"see one, do one, teach one\"\napproach for the surgical intelligence domain as \"train one, classify one,\nteach one\". In machine learning, this approach is often referred to as transfer\nlearning. To analyze the impact of transfer learning across different\nlaparoscopic procedures, we explore various time-series architectures and\nexamine their performance on each target domain. We introduce a new\narchitecture, the Time-Series Adaptation Network (TSAN), an architecture\noptimized for transfer learning of surgical step recognition, and we show how\nTSAN can be pre-trained using self-supervised learning on a Sequence Sorting\ntask. Such pre-training enables TSAN to learn workflow steps of a new\nlaparoscopic procedure type from only a small number of labeled samples from\nthe target procedure. Our proposed architecture leads to better performance\ncompared to other possible architectures, reaching over 90% accuracy when\ntransferring from laparoscopic Cholecystectomy to the other three procedure\ntypes.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 14:36:18 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 06:55:23 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Neimark", "Daniel", ""], ["Bar", "Omri", ""], ["Zohar", "Maya", ""], ["Hager", "Gregory D.", ""], ["Asselmann", "Dotan", ""]]}, {"id": "2102.12310", "submitter": "Xi-Le Zhao", "authors": "Yu-Chun Miao, Xi-Le Zhao, Xiao Fu, Jian-Li Wang, and Yu-Bang Zheng", "title": "Hyperspectral Denoising Using Unsupervised Disentangled Spatio-Spectral\n  Deep Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image denoising is often empowered by accurate prior information. In recent\nyears, data-driven neural network priors have shown promising performance for\nRGB natural image denoising. Compared to classic handcrafted priors (e.g.,\nsparsity and total variation), the \"deep priors\" are learned using a large\nnumber of training samples -- which can accurately model the complex image\ngenerating process. However, data-driven priors are hard to acquire for\nhyperspectral images (HSIs) due to the lack of training data. A remedy is to\nuse the so-called unsupervised deep image prior (DIP). Under the unsupervised\nDIP framework, it is hypothesized and empirically demonstrated that proper\nneural network structures are reasonable priors of certain types of images, and\nthe network weights can be learned without training data. Nonetheless, the most\neffective unsupervised DIP structures were proposed for natural images instead\nof HSIs. The performance of unsupervised DIP-based HSI denoising is limited by\na couple of serious challenges, namely, network structure design and network\ncomplexity. This work puts forth an unsupervised DIP framework that is based on\nthe classic spatio-spectral decomposition of HSIs. Utilizing the so-called\nlinear mixture model of HSIs, two types of unsupervised DIPs, i.e., U-Net-like\nnetwork and fully-connected networks, are employed to model the abundance maps\nand endmembers contained in the HSIs, respectively. This way, empirically\nvalidated unsupervised DIP structures for natural images can be easily\nincorporated for HSI denoising. Besides, the decomposition also substantially\nreduces network complexity. An efficient alternating optimization algorithm is\nproposed to handle the formulated denoising problem. Semi-real and real data\nexperiments are employed to showcase the effectiveness of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 14:38:51 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Miao", "Yu-Chun", ""], ["Zhao", "Xi-Le", ""], ["Fu", "Xiao", ""], ["Wang", "Jian-Li", ""], ["Zheng", "Yu-Bang", ""]]}, {"id": "2102.12319", "submitter": "Osama Mazhar", "authors": "Osama Mazhar, Robert Babuska and Jens Kober", "title": "GEM: Glare or Gloom, I Can Still See You -- End-to-End Multimodal Object\n  Detection", "comments": "IEEE Robotics and Automation Letters (RA-L)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks designed for vision tasks are often prone to failure\nwhen they encounter environmental conditions not covered by the training data.\nSingle-modal strategies are insufficient when the sensor fails to acquire\ninformation due to malfunction or its design limitations. Multi-sensor\nconfigurations are known to provide redundancy, increase reliability, and are\ncrucial in achieving robustness against asymmetric sensor failures. To address\nthe issue of changing lighting conditions and asymmetric sensor degradation in\nobject detection, we develop a multi-modal 2D object detector, and propose\ndeterministic and stochastic sensor-aware feature fusion strategies. The\nproposed fusion mechanisms are driven by the estimated sensor measurement\nreliability values/weights. Reliable object detection in harsh lighting\nconditions is essential for applications such as self-driving vehicles and\nhuman-robot interaction. We also propose a new \"r-blended\" hybrid depth\nmodality for RGB-D sensors. Through extensive experimentation, we show that the\nproposed strategies outperform the existing state-of-the-art methods on the\nFLIR-Thermal dataset, and obtain promising results on the SUNRGB-D dataset. We\nadditionally record a new RGB-Infra indoor dataset, namely L515-Indoors, and\ndemonstrate that the proposed object detection methodologies are highly\neffective for a variety of lighting conditions.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 14:56:37 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 16:42:00 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 10:33:26 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Mazhar", "Osama", ""], ["Babuska", "Robert", ""], ["Kober", "Jens", ""]]}, {"id": "2102.12321", "submitter": "Tianmin Shu", "authors": "Tianmin Shu, Abhishek Bhandwaldar, Chuang Gan, Kevin A. Smith, Shari\n  Liu, Dan Gutfreund, Elizabeth Spelke, Joshua B. Tenenbaum, Tomer D. Ullman", "title": "AGENT: A Benchmark for Core Psychological Reasoning", "comments": "ICML 2021, 12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For machine agents to successfully interact with humans in real-world\nsettings, they will need to develop an understanding of human mental life.\nIntuitive psychology, the ability to reason about hidden mental variables that\ndrive observable actions, comes naturally to people: even pre-verbal infants\ncan tell agents from objects, expecting agents to act efficiently to achieve\ngoals given constraints. Despite recent interest in machine agents that reason\nabout other agents, it is not clear if such agents learn or hold the core\npsychology principles that drive human reasoning. Inspired by cognitive\ndevelopment studies on intuitive psychology, we present a benchmark consisting\nof a large dataset of procedurally generated 3D animations, AGENT (Action,\nGoal, Efficiency, coNstraint, uTility), structured around four scenarios (goal\npreferences, action efficiency, unobserved constraints, and cost-reward\ntrade-offs) that probe key concepts of core intuitive psychology. We validate\nAGENT with human-ratings, propose an evaluation protocol emphasizing\ngeneralization, and compare two strong baselines built on Bayesian inverse\nplanning and a Theory of Mind neural network. Our results suggest that to pass\nthe designed tests of core intuitive psychology at human levels, a model must\nacquire or have built-in representations of how agents plan, combining utility\ncomputations and core knowledge of objects and physics.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 14:58:23 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 18:11:01 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 03:41:55 GMT"}, {"version": "v4", "created": "Mon, 26 Jul 2021 03:13:11 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Shu", "Tianmin", ""], ["Bhandwaldar", "Abhishek", ""], ["Gan", "Chuang", ""], ["Smith", "Kevin A.", ""], ["Liu", "Shari", ""], ["Gutfreund", "Dan", ""], ["Spelke", "Elizabeth", ""], ["Tenenbaum", "Joshua B.", ""], ["Ullman", "Tomer D.", ""]]}, {"id": "2102.12354", "submitter": "Fl\\'avio Santos", "authors": "Flavio Santos, Cleber Zanchettin, Leonardo Matos, and Paulo Novais", "title": "On the Impact of Interpretability Methods in Active Image Augmentation\n  Method", "comments": "published in Logic Journal of the IGPL (2021)", "journal-ref": "Logic Journal of the IGPL, 2021, jzab006", "doi": "10.1093/jigpal/jzab006", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robustness is a significant constraint in machine learning models. The\nperformance of the algorithms must not deteriorate when training and testing\nwith slightly different data. Deep neural network models achieve awe-inspiring\nresults in a wide range of applications of computer vision. Still, in the\npresence of noise or region occlusion, some models exhibit inaccurate\nperformance even with data handled in training. Besides, some experiments\nsuggest deep learning models sometimes use incorrect parts of the input\ninformation to perform inference. Activate Image Augmentation (ADA) is an\naugmentation method that uses interpretability methods to augment the training\ndata and improve its robustness to face the described problems. Although ADA\npresented interesting results, its original version only used the Vanilla\nBackpropagation interpretability to train the U-Net model. In this work, we\npropose an extensive experimental analysis of the interpretability method's\nimpact on ADA. We use five interpretability methods: Vanilla Backpropagation,\nGuided Backpropagation, GradCam, Guided GradCam, and InputXGradient. The\nresults show that all methods achieve similar performance at the ending of\ntraining, but when combining ADA with GradCam, the U-Net model presented an\nimpressive fast convergence.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 15:40:54 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Santos", "Flavio", ""], ["Zanchettin", "Cleber", ""], ["Matos", "Leonardo", ""], ["Novais", "Paulo", ""]]}, {"id": "2102.12418", "submitter": "Jennifer Maier", "authors": "Jennifer Maier, Marlies Nitschke, Jang-Hwan Choi, Garry Gold, Rebecca\n  Fahrig, Bjoern M. Eskofier, Andreas Maier", "title": "Rigid and non-rigid motion compensation in weight-bearing cone-beam CT\n  of the knee using (noisy) inertial measurements", "comments": "16 pages, 6 figures, submitted to Elsevier Medical Image Analysis on\n  Feb 11, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Involuntary subject motion is the main source of artifacts in weight-bearing\ncone-beam CT of the knee. To achieve image quality for clinical diagnosis, the\nmotion needs to be compensated. We propose to use inertial measurement units\n(IMUs) attached to the leg for motion estimation. We perform a simulation study\nusing real motion recorded with an optical tracking system. Three IMU-based\ncorrection approaches are evaluated, namely rigid motion correction, non-rigid\n2D projection deformation and non-rigid 3D dynamic reconstruction. We present\nan initialization process based on the system geometry. With an IMU noise\nsimulation, we investigate the applicability of the proposed methods in real\napplications. All proposed IMU-based approaches correct motion at least as good\nas a state-of-the-art marker-based approach. The structural similarity index\nand the root mean squared error between motion-free and motion corrected\nvolumes are improved by 24-35% and 78-85%, respectively, compared with the\nuncorrected case. The noise analysis shows that the noise levels of\ncommercially available IMUs need to be improved by a factor of $10^5$ which is\ncurrently only achieved by specialized hardware not robust enough for the\napplication. The presented study confirms the feasibility of this novel\napproach and defines improvements necessary for a real application.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 17:19:32 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Maier", "Jennifer", ""], ["Nitschke", "Marlies", ""], ["Choi", "Jang-Hwan", ""], ["Gold", "Garry", ""], ["Fahrig", "Rebecca", ""], ["Eskofier", "Bjoern M.", ""], ["Maier", "Andreas", ""]]}, {"id": "2102.12443", "submitter": "Jes\\'us Andr\\'es Portillo Quintero", "authors": "Jes\\'us Andr\\'es Portillo-Quintero, Jos\\'e Carlos Ortiz-Bayliss, Hugo\n  Terashima-Mar\\'in", "title": "A Straightforward Framework For Video Retrieval Using CLIP", "comments": "10 pages, 1 figure, submitted to Mexican Conference for Pattern\n  Recognition (MCPR 2021); corrected results section and added model\n  specifications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video Retrieval is a challenging task where a text query is matched to a\nvideo or vice versa. Most of the existing approaches for addressing such a\nproblem rely on annotations made by the users. Although simple, this approach\nis not always feasible in practice. In this work, we explore the application of\nthe language-image model, CLIP, to obtain video representations without the\nneed for said annotations. This model was explicitly trained to learn a common\nspace where images and text can be compared. Using various techniques described\nin this document, we extended its application to videos, obtaining\nstate-of-the-art results on the MSR-VTT and MSVD benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 18:15:12 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 17:55:07 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Portillo-Quintero", "Jes\u00fas Andr\u00e9s", ""], ["Ortiz-Bayliss", "Jos\u00e9 Carlos", ""], ["Terashima-Mar\u00edn", "Hugo", ""]]}, {"id": "2102.12472", "submitter": "Mehmet Ayg\\\"un", "authors": "Mehmet Ayg\\\"un, Aljo\\v{s}a O\\v{s}ep, Mark Weber, Maxim Maximov, Cyrill\n  Stachniss, Jens Behley, Laura Leal-Taix\\'e", "title": "4D Panoptic LiDAR Segmentation", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Temporal semantic scene understanding is critical for self-driving cars or\nrobots operating in dynamic environments. In this paper, we propose 4D panoptic\nLiDAR segmentation to assign a semantic class and a temporally-consistent\ninstance ID to a sequence of 3D points. To this end, we present an approach and\na point-centric evaluation metric. Our approach determines a semantic class for\nevery point while modeling object instances as probability distributions in the\n4D spatio-temporal domain. We process multiple point clouds in parallel and\nresolve point-to-instance associations, effectively alleviating the need for\nexplicit temporal data association. Inspired by recent advances in benchmarking\nof multi-object tracking, we propose to adopt a new evaluation metric that\nseparates the semantic and point-to-instance association aspects of the task.\nWith this work, we aim at paving the road for future developments of temporal\nLiDAR panoptic perception.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 18:56:16 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 17:58:34 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Ayg\u00fcn", "Mehmet", ""], ["O\u0161ep", "Aljo\u0161a", ""], ["Weber", "Mark", ""], ["Maximov", "Maxim", ""], ["Stachniss", "Cyrill", ""], ["Behley", "Jens", ""], ["Leal-Taix\u00e9", "Laura", ""]]}, {"id": "2102.12505", "submitter": "Utako Yamamoto", "authors": "Utako Yamamoto, Megumi Nakao, Masayuki Ohzeki, Junko Tokuno, Toyofumi\n  Fengshi Chen-Yoshikawa, and Tetsuya Matsuda", "title": "Kernel-based framework to estimate deformations of pneumothorax lung\n  using relative position of anatomical landmarks", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In video-assisted thoracoscopic surgeries, successful procedures of nodule\nresection are highly dependent on the precise estimation of lung deformation\nbetween the inflated lung in the computed tomography (CT) images during\npreoperative planning and the deflated lung in the treatment views during\nsurgery. Lungs in the pneumothorax state during surgery have a large volume\nchange from normal lungs, making it difficult to build a mechanical model. The\npurpose of this study is to develop a deformation estimation method of the 3D\nsurface of a deflated lung from a few partial observations. To estimate\ndeformations for a largely deformed lung, a kernel regression-based solution\nwas introduced. The proposed method used a few landmarks to capture the partial\ndeformation between the 3D surface mesh obtained from preoperative CT and the\nintraoperative anatomical positions. The deformation for each vertex of the\nentire mesh model was estimated per-vertex as a relative position from the\nlandmarks. The landmarks were placed in the anatomical position of the lung's\nouter contour. The method was applied on nine datasets of the left lungs of\nlive Beagle dogs. Contrast-enhanced CT images of the lungs were acquired. The\nproposed method achieved a local positional error of vertices of 2.74 mm,\nHausdorff distance of 6.11 mm, and Dice similarity coefficient of 0.94.\nMoreover, the proposed method could estimate lung deformations from a small\nnumber of training cases and a small observation area. This study contributes\nto the data-driven modeling of pneumothorax deformation of the lung.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 19:00:17 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Yamamoto", "Utako", ""], ["Nakao", "Megumi", ""], ["Ohzeki", "Masayuki", ""], ["Tokuno", "Junko", ""], ["Chen-Yoshikawa", "Toyofumi Fengshi", ""], ["Matsuda", "Tetsuya", ""]]}, {"id": "2102.12525", "submitter": "Varun Kelkar", "authors": "Varun A. Kelkar, Mark A. Anastasio", "title": "Prior Image-Constrained Reconstruction using Style-Based Generative\n  Models", "comments": "Accepted for publication at the International Conference on Machine\n  Learning (ICML) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining a useful estimate of an object from highly incomplete imaging\nmeasurements remains a holy grail of imaging science. Deep learning methods\nhave shown promise in learning object priors or constraints to improve the\nconditioning of an ill-posed imaging inverse problem. In this study, a\nframework for estimating an object of interest that is semantically related to\na known prior image, is proposed. An optimization problem is formulated in the\ndisentangled latent space of a style-based generative model, and semantically\nmeaningful constraints are imposed using the disentangled latent representation\nof the prior image. Stable recovery from incomplete measurements with the help\nof a prior image is theoretically analyzed. Numerical experiments demonstrating\nthe superior performance of our approach as compared to related methods are\npresented.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 19:36:55 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 23:22:13 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Kelkar", "Varun A.", ""], ["Anastasio", "Mark A.", ""]]}, {"id": "2102.12544", "submitter": "Masuda Tonima", "authors": "Masuda Akter Tonima, F M Anim Hossain, Austin DeHart and Youmin Zhang", "title": "Auto-Detection of Tibial Plateau Angle in Canine Radiographs Using a\n  Deep Learning Approach", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stifle joint issues are a major cause of lameness in dogs and it can be a\nsignificant marker for various forms of diseases or injuries. A known Tibial\nPlateau Angle (TPA) helps in the reduction of the diagnosis time of the cause.\nWith the state of the art object detection algorithm YOLO, and its variants,\nthis paper delves into identifying joints, their centroids and other regions of\ninterest to draw multiple line axes and finally calculating the TPA. The\nmethods investigated predicts successfully the TPA within the normal range for\n80 percent of the images.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 20:22:44 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Tonima", "Masuda Akter", ""], ["Hossain", "F M Anim", ""], ["DeHart", "Austin", ""], ["Zhang", "Youmin", ""]]}, {"id": "2102.12555", "submitter": "Yigit Alparslan", "authors": "Yigit Alparslan and Edward Kim", "title": "Robust SleepNets", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  State-of-the-art convolutional neural networks excel in machine learning\ntasks such as face recognition, and object classification but suffer\nsignificantly when adversarial attacks are present. It is crucial that machine\ncritical systems, where machine learning models are deployed, utilize robust\nmodels to handle a wide range of variability in the real world and malicious\nactors that may use adversarial attacks. In this study, we investigate eye\nclosedness detection to prevent vehicle accidents related to driver\ndisengagements and driver drowsiness. Specifically, we focus on adversarial\nattacks in this application domain, but emphasize that the methodology can be\napplied to many other domains. We develop two models to detect eye closedness:\nfirst model on eye images and a second model on face images. We adversarially\nattack the models with Projected Gradient Descent, Fast Gradient Sign and\nDeepFool methods and report adversarial success rate. We also study the effect\nof training data augmentation. Finally, we adversarially train the same models\non perturbed images and report the success rate for the defense against these\nattacks. We hope our study sets up the work to prevent potential vehicle\naccidents by capturing drivers' face images and alerting them in case driver's\neyes are closed due to drowsiness.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 20:48:13 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Alparslan", "Yigit", ""], ["Kim", "Edward", ""]]}, {"id": "2102.12570", "submitter": "Bedirhan Uzun", "authors": "Hakan Cevikalp, Bedirhan Uzun, Okan K\\\"op\\\"ukl\\\"u, Gurkan Ozturk", "title": "Deep Compact Polyhedral Conic Classifier for Open and Closed Set\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a new deep neural network classifier that\nsimultaneously maximizes the inter-class separation and minimizes the\nintra-class variation by using the polyhedral conic classification function.\nThe proposed method has one loss term that allows the margin maximization to\nmaximize the inter-class separation and another loss term that controls the\ncompactness of the class acceptance regions. Our proposed method has a nice\ngeometric interpretation using polyhedral conic function geometry. We tested\nthe proposed method on various visual classification problems including\nclosed/open set recognition and anomaly detection. The experimental results\nshow that the proposed method typically outperforms other state-of-the art\nmethods, and becomes a better choice compared to other tested methods\nespecially for open set recognition type problems.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 21:38:31 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Cevikalp", "Hakan", ""], ["Uzun", "Bedirhan", ""], ["K\u00f6p\u00fckl\u00fc", "Okan", ""], ["Ozturk", "Gurkan", ""]]}, {"id": "2102.12593", "submitter": "Bing Li", "authors": "Bing Li, Yuanlue Zhu, Yitong Wang, Chia-Wen Lin, Bernard Ghanem,\n  Linlin Shen", "title": "AniGAN: Style-Guided Generative Adversarial Networks for Unsupervised\n  Anime Face Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel framework to translate a portrait\nphoto-face into an anime appearance. Our aim is to synthesize anime-faces which\nare style-consistent with a given reference anime-face. However, unlike typical\ntranslation tasks, such anime-face translation is challenging due to complex\nvariations of appearances among anime-faces. Existing methods often fail to\ntransfer the styles of reference anime-faces, or introduce noticeable\nartifacts/distortions in the local shapes of their generated faces. We propose\nAniGAN, a novel GAN-based translator that synthesizes high-quality anime-faces.\nSpecifically, a new generator architecture is proposed to simultaneously\ntransfer color/texture styles and transform local facial shapes into anime-like\ncounterparts based on the style of a reference anime-face, while preserving the\nglobal structure of the source photo-face. We propose a double-branch\ndiscriminator to learn both domain-specific distributions and domain-shared\ndistributions, helping generate visually pleasing anime-faces and effectively\nmitigate artifacts. Extensive experiments on selfie2anime and a new face2anime\ndataset qualitatively and quantitatively demonstrate the superiority of our\nmethod over state-of-the-art methods. The new dataset is available at\nhttps://github.com/bing-li-ai/AniGAN .\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 22:47:38 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 11:30:50 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Li", "Bing", ""], ["Zhu", "Yuanlue", ""], ["Wang", "Yitong", ""], ["Lin", "Chia-Wen", ""], ["Ghanem", "Bernard", ""], ["Shen", "Linlin", ""]]}, {"id": "2102.12595", "submitter": "Takuro Hoshi", "authors": "Takuro Hoshi, Yohei Baba and Gaurang Gavai", "title": "Railway Anomaly detection model using synthetic defect images generated\n  by CycleGAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although training data is essential for machine learning, railway companies\nare facing difficulties in gathering adequate images of defective equipment due\nto their proactive replacement of would be defective equipment. Nevertheless,\nproactive replacement is indispensable for safe and undisturbed operation of\npublic transport. In this research, we have developed a model using CycleGAN to\ngenerate artificial images of defective equipment instead of real images. By\nadopting these generated images as training data, we verified that these images\nare indistinguishable from real images and they play a vital role in enhancing\nthe accuracy of the defect detection models.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 22:59:43 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Hoshi", "Takuro", ""], ["Baba", "Yohei", ""], ["Gavai", "Gaurang", ""]]}, {"id": "2102.12627", "submitter": "Geoffrey Hinton", "authors": "Geoffrey Hinton", "title": "How to represent part-whole hierarchies in a neural network", "comments": "43 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper does not describe a working system. Instead, it presents a single\nidea about representation which allows advances made by several different\ngroups to be combined into an imaginary system called GLOM. The advances\ninclude transformers, neural fields, contrastive representation learning,\ndistillation and capsules. GLOM answers the question: How can a neural network\nwith a fixed architecture parse an image into a part-whole hierarchy which has\na different structure for each image? The idea is simply to use islands of\nidentical vectors to represent the nodes in the parse tree. If GLOM can be made\nto work, it should significantly improve the interpretability of the\nrepresentations produced by transformer-like systems when applied to vision or\nlanguage\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 01:51:22 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Hinton", "Geoffrey", ""]]}, {"id": "2102.12642", "submitter": "Yuanhan Zhang", "authors": "Yuanhan Zhang, Zhenfei Yin, Jing Shao, Ziwei Liu, Shuo Yang, Yuanjun\n  Xiong, Wei Xia, Yan Xu, Man Luo, Jian Liu, Jianshu Li, Zhijun Chen, Mingyu\n  Guo, Hui Li, Junfu Liu, Pengfei Gao, Tianqi Hong, Hao Han, Shijie Liu, Xinhua\n  Chen, Di Qiu, Cheng Zhen, Dashuang Liang, Yufeng Jin, Zhanlong Hao", "title": "CelebA-Spoof Challenge 2020 on Face Anti-Spoofing: Methods and Results", "comments": "Technical report. Challenge website:\n  https://competitions.codalab.org/competitions/26210", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As facial interaction systems are prevalently deployed, security and\nreliability of these systems become a critical issue, with substantial research\nefforts devoted. Among them, face anti-spoofing emerges as an important area,\nwhose objective is to identify whether a presented face is live or spoof.\nRecently, a large-scale face anti-spoofing dataset, CelebA-Spoof which\ncomprised of 625,537 pictures of 10,177 subjects has been released. It is the\nlargest face anti-spoofing dataset in terms of the numbers of the data and the\nsubjects. This paper reports methods and results in the CelebA-Spoof Challenge\n2020 on Face AntiSpoofing which employs the CelebA-Spoof dataset. The model\nevaluation is conducted online on the hidden test set. A total of 134\nparticipants registered for the competition, and 19 teams made valid\nsubmissions. We will analyze the top ranked solutions and present some\ndiscussion on future work directions.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 02:31:41 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 02:33:52 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Zhang", "Yuanhan", ""], ["Yin", "Zhenfei", ""], ["Shao", "Jing", ""], ["Liu", "Ziwei", ""], ["Yang", "Shuo", ""], ["Xiong", "Yuanjun", ""], ["Xia", "Wei", ""], ["Xu", "Yan", ""], ["Luo", "Man", ""], ["Liu", "Jian", ""], ["Li", "Jianshu", ""], ["Chen", "Zhijun", ""], ["Guo", "Mingyu", ""], ["Li", "Hui", ""], ["Liu", "Junfu", ""], ["Gao", "Pengfei", ""], ["Hong", "Tianqi", ""], ["Han", "Hao", ""], ["Liu", "Shijie", ""], ["Chen", "Xinhua", ""], ["Qiu", "Di", ""], ["Zhen", "Cheng", ""], ["Liang", "Dashuang", ""], ["Jin", "Yufeng", ""], ["Hao", "Zhanlong", ""]]}, {"id": "2102.12670", "submitter": "Azarakhsh Keipour", "authors": "Azarakhsh Keipour and Guilherme A. S. Pereira and Sebastian Scherer", "title": "Real-Time Ellipse Detection for Robotics Applications", "comments": "Accepted to RA-L and IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for real-time detection and tracking of elliptic\npatterns suitable for real-world robotics applications. The method fits\nellipses to each contour in the image frame and rejects ellipses that do not\nyield a good fit. The resulting detection and tracking method is lightweight\nenough to be used on robots' resource-limited onboard computers, can deal with\nlighting variations and detect the pattern even when the view is partial. The\nmethod is tested on an example application of an autonomous UAV landing on a\nfast-moving vehicle to show its performance indoors, outdoors, and in\nsimulation on a real-world robotics task. The comparison with other well-known\nellipse detection methods shows that our proposed algorithm outperforms other\nmethods with the F1 score of 0.981 on a dataset with over 1500 frames. The\nvideos of experiments, the source codes, and the collected dataset are provided\nwith the paper at https://theairlab.org/landing-on-vehicle .\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 03:53:59 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 06:17:41 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Keipour", "Azarakhsh", ""], ["Pereira", "Guilherme A. S.", ""], ["Scherer", "Sebastian", ""]]}, {"id": "2102.12728", "submitter": "William Smith", "authors": "William H. B. Smith, Michael Milford, Klaus D. McDonald-Maier, Shoaib\n  Ehsan", "title": "Scene Retrieval for Contextual Visual Mapping", "comments": "8 page paper on visual place recogniton and scene classification", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual navigation localizes a query place image against a reference database\nof place images, also known as a `visual map'. Localization accuracy\nrequirements for specific areas of the visual map, `scene classes', vary\naccording to the context of the environment and task. State-of-the-art visual\nmapping is unable to reflect these requirements by explicitly targetting scene\nclasses for inclusion in the map. Four different scene classes, including\npedestrian crossings and stations, are identified in each of the Nordland and\nSt. Lucia datasets. Instead of re-training separate scene classifiers which\nstruggle with these overlapping scene classes we make our first contribution:\ndefining the problem of `scene retrieval'. Scene retrieval extends image\nretrieval to classification of scenes defined at test time by associating a\nsingle query image to reference images of scene classes. Our second\ncontribution is a triplet-trained convolutional neural network (CNN) to address\nthis problem which increases scene classification accuracy by up to 7% against\nstate-of-the-art networks pre-trained for scene recognition. The second\ncontribution is an algorithm `DMC' that combines our scene classification with\ndistance and memorability for visual mapping. Our analysis shows that DMC\nincludes 64% more images of our chosen scene classes in a visual map than just\nusing distance interval mapping. State-of-the-art visual place descriptors\nAMOS-Net, Hybrid-Net and NetVLAD are finally used to show that DMC improves\nscene class localization accuracy by a mean of 3% and localization accuracy of\nthe remaining map images by a mean of 10% across both datasets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 08:23:19 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Smith", "William H. B.", ""], ["Milford", "Michael", ""], ["McDonald-Maier", "Klaus D.", ""], ["Ehsan", "Shoaib", ""]]}, {"id": "2102.12755", "submitter": "Rongda Fu", "authors": "Jinquan Guo, Rongda Fu, Lin Pan, Shaohua Zheng, Liqin Huang, Bin\n  Zheng, Bingwei He", "title": "Coarse-to-fine Airway Segmentation Using Multi information Fusion\n  Network and CNN-based Region Growing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic airway segmentation from chest computed tomography (CT) scans plays\nan important role in pulmonary disease diagnosis and computer-assisted therapy.\nHowever, low contrast at peripheral branches and complex tree-like structures\nremain as two mainly challenges for airway segmentation. Recent research has\nillustrated that deep learning methods perform well in segmentation tasks.\nMotivated by these works, a coarse-to-fine segmentation framework is proposed\nto obtain a complete airway tree. Our framework segments the overall airway and\nsmall branches via the multi-information fusion convolution neural network\n(Mif-CNN) and the CNN-based region growing, respectively. In Mif-CNN, atrous\nspatial pyramid pooling (ASPP) is integrated into a u-shaped network, and it\ncan expend the receptive field and capture multi-scale information. Meanwhile,\nboundary and location information are incorporated into semantic information.\nThese information are fused to help Mif-CNN utilize additional context\nknowledge and useful features. To improve the performance of the segmentation\nresult, the CNN-based region growing method is designed to focus on obtaining\nsmall branches. A voxel classification network (VCN), which can entirely\ncapture the rich information around each voxel, is applied to classify the\nvoxels into airway and non-airway. In addition, a shape reconstruction method\nis used to refine the airway tree.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 09:51:30 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Guo", "Jinquan", ""], ["Fu", "Rongda", ""], ["Pan", "Lin", ""], ["Zheng", "Shaohua", ""], ["Huang", "Liqin", ""], ["Zheng", "Bin", ""], ["He", "Bingwei", ""]]}, {"id": "2102.12759", "submitter": "Georg Muntingh PhD", "authors": "Oliver J.D. Barrowclough, Georg Muntingh, Varatharajan Nainamalai,\n  Ivar Stangeby", "title": "Binary segmentation of medical images using implicit spline\n  representations and deep learning", "comments": "17 pages, 5 figures", "journal-ref": "Computer Aided Geometric Design, Volume 85, 2021", "doi": "10.1016/j.cagd.2021.101972", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to image segmentation based on combining implicit\nspline representations with deep convolutional neural networks. This is done by\npredicting the control points of a bivariate spline function whose zero-set\nrepresents the segmentation boundary. We adapt several existing neural network\narchitectures and design novel loss functions that are tailored towards\nproviding implicit spline curve approximations. The method is evaluated on a\ncongenital heart disease computed tomography medical imaging dataset.\nExperiments are carried out by measuring performance in various standard\nmetrics for different networks and loss functions. We determine that splines of\nbidegree $(1,1)$ with $128\\times128$ coefficient resolution performed optimally\nfor $512\\times 512$ resolution CT images. For our best network, we achieve an\naverage volumetric test Dice score of almost 92%, which reaches the state of\nthe art for this congenital heart disease dataset.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 10:04:25 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 08:50:53 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Barrowclough", "Oliver J. D.", ""], ["Muntingh", "Georg", ""], ["Nainamalai", "Varatharajan", ""], ["Stangeby", "Ivar", ""]]}, {"id": "2102.12764", "submitter": "Viraj Kulkarni", "authors": "Jitesh Seth, Rohit Lokwani, Viraj Kulkarni, Aniruddha Pant, Amit\n  Kharat", "title": "Reducing Labelled Data Requirement for Pneumonia Segmentation using\n  Image Augmentations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning semantic segmentation algorithms can localise abnormalities or\nopacities from chest radiographs. However, the task of collecting and\nannotating training data is expensive and requires expertise which remains a\nbottleneck for algorithm performance. We investigate the effect of image\naugmentations on reducing the requirement of labelled data in the semantic\nsegmentation of chest X-rays for pneumonia detection. We train fully\nconvolutional network models on subsets of different sizes from the total\ntraining data. We apply a different image augmentation while training each\nmodel and compare it to the baseline trained on the entire dataset without\naugmentations. We find that rotate and mixup are the best augmentations amongst\nrotate, mixup, translate, gamma and horizontal flip, wherein they reduce the\nlabelled data requirement by 70% while performing comparably to the baseline in\nterms of AUC and mean IoU in our experiments.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 10:11:30 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Seth", "Jitesh", ""], ["Lokwani", "Rohit", ""], ["Kulkarni", "Viraj", ""], ["Pant", "Aniruddha", ""], ["Kharat", "Amit", ""]]}, {"id": "2102.12765", "submitter": "Wei-Chen Chiu", "authors": "Chun-Chih Teng and Pin-Yu Chen and Wei-Chen Chiu", "title": "Domain Adaptation for Learning Generator from Paired Few-Shot Data", "comments": "accepted in ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a Paired Few-shot GAN (PFS-GAN) model for learning generators with\nsufficient source data and a few target data. While generative model learning\ntypically needs large-scale training data, our PFS-GAN not only uses the\nconcept of few-shot learning but also domain shift to transfer the knowledge\nacross domains, which alleviates the issue of obtaining low-quality generator\nwhen only trained with target domain data. The cross-domain datasets are\nassumed to have two properties: (1) each target-domain sample has its\nsource-domain correspondence and (2) two domains share similar content\ninformation but different appearance. Our PFS-GAN aims to learn the\ndisentangled representation from images, which composed of domain-invariant\ncontent features and domain-specific appearance features. Furthermore, a\nrelation loss is introduced on the content features while shifting the\nappearance features to increase the structural diversity. Extensive experiments\nshow that our method has better quantitative and qualitative results on the\ngenerated target-domain data with higher diversity in comparison to several\nbaselines.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 10:11:44 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Teng", "Chun-Chih", ""], ["Chen", "Pin-Yu", ""], ["Chiu", "Wei-Chen", ""]]}, {"id": "2102.12781", "submitter": "Harshay Shah", "authors": "Harshay Shah, Prateek Jain, Praneeth Netrapalli", "title": "Do Input Gradients Highlight Discriminative Features?", "comments": "Code: https://github.com/harshays/inputgradients", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Post-hoc gradient-based interpretability methods [Simonyan et al., 2013,\nSmilkov et al., 2017] that provide instance-specific explanations of model\npredictions are often based on assumption (A): magnitude of input gradients --\ngradients of logits with respect to input -- noisily highlight discriminative\ntask-relevant features. In this work, we test the validity of assumption (A)\nusing a three-pronged approach. First, we develop an evaluation framework,\nDiffROAR, to test assumption (A) on four image classification benchmarks. Our\nresults suggest that (i) input gradients of standard models (i.e., trained on\noriginal data) may grossly violate (A), whereas (ii) input gradients of\nadversarially robust models satisfy (A). Second, we then introduce BlockMNIST,\nan MNIST-based semi-real dataset, that by design encodes a priori knowledge of\ndiscriminative features. Our analysis on BlockMNIST leverages this information\nto validate as well as characterize differences between input gradient\nattributions of standard and robust models. Finally, we theoretically prove\nthat our empirical findings hold on a simplified version of the BlockMNIST\ndataset. Specifically, we prove that input gradients of standard\none-hidden-layer MLPs trained on this dataset do not highlight\ninstance-specific signal coordinates, thus grossly violating assumption (A).\nOur findings motivate the need to formalize and test common assumptions in\ninterpretability in a falsifiable manner [Leavitt and Morcos, 2020].\nAdditionally, we believe that the DiffROAR evaluation framework and\nBlockMNIST-based datasets can serve as sanity checks to audit instance-specific\ninterpretability methods.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 11:04:38 GMT"}, {"version": "v2", "created": "Sun, 20 Jun 2021 15:30:11 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Shah", "Harshay", ""], ["Jain", "Prateek", ""], ["Netrapalli", "Praneeth", ""]]}, {"id": "2102.12808", "submitter": "Jinrong Yang", "authors": "Jinrong Yang, Shengkai Wu, Lijun Gou, Hangcheng Yu, Chenxi Lin,\n  Jiazhuo Wang, Minxuan Li, Xiaoping Li", "title": "SCD: A Stacked Carton Dataset for Detection and Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Carton detection is an important technique in the automatic logistics system\nand can be applied to many applications such as the stacking and unstacking of\ncartons, the unloading of cartons in the containers. However, there is no\npublic large-scale carton dataset for the research community to train and\nevaluate the carton detection models up to now, which hinders the development\nof carton detection. In this paper, we present a large-scale carton dataset\nnamed Stacked Carton Dataset(SCD) with the goal of advancing the\nstate-of-the-art in carton detection. Images are collected from the internet\nand several warehourses, and objects are labeled using per-instance\nsegmentation for precise localization. There are totally 250,000 instance masks\nfrom 16,136 images. In addition, we design a carton detector based on RetinaNet\nby embedding Offset Prediction between Classification and Localization\nmodule(OPCL) and Boundary Guided Supervision module(BGS). OPCL alleviates the\nimbalance problem between classification and localization quality which boosts\nAP by 3.1% - 4.7% on SCD while BGS guides the detector to pay more attention to\nboundary information of cartons and decouple repeated carton textures. To\ndemonstrate the generalization of OPCL to other datasets, we conduct extensive\nexperiments on MS COCO and PASCAL VOC. The improvement of AP on MS COCO and\nPASCAL VOC is 1.8% - 2.2% and 3.4% - 4.3% respectively.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 12:22:12 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Yang", "Jinrong", ""], ["Wu", "Shengkai", ""], ["Gou", "Lijun", ""], ["Yu", "Hangcheng", ""], ["Lin", "Chenxi", ""], ["Wang", "Jiazhuo", ""], ["Li", "Minxuan", ""], ["Li", "Xiaoping", ""]]}, {"id": "2102.12823", "submitter": "Julien Dupeyroux", "authors": "Raoul Dinaux, Nikhil Wessendorp, Julien Dupeyroux, Guido de Croon", "title": "FAITH: Fast iterative half-plane focus of expansion estimation using\n  event-based optic flow", "comments": "8 pages, 11 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Course estimation is a key component for the development of autonomous\nnavigation systems for robots. While state-of-the-art methods widely use\nvisual-based algorithms, it is worth noting that they all fail to deal with the\ncomplexity of the real world by being computationally greedy and sometimes too\nslow. They often require obstacles to be highly textured to improve the overall\nperformance, particularly when the obstacle is located within the focus of\nexpansion (FOE) where the optic flow (OF) is almost null. This study proposes\nthe FAst ITerative Half-plane (FAITH) method to determine the course of a micro\nair vehicle (MAV). This is achieved by means of an event-based camera, along\nwith a fast RANSAC-based algorithm that uses event-based OF to determine the\nFOE. The performance is validated by means of a benchmark on a simulated\nenvironment and then tested on a dataset collected for indoor obstacle\navoidance. Our results show that the computational efficiency of our solution\noutperforms state-of-the-art methods while keeping a high level of accuracy.\nThis has been further demonstrated onboard an MAV equipped with an event-based\ncamera, showing that our event-based FOE estimation can be achieved online\nonboard tiny drones, thus opening the path towards fully neuromorphic solutions\nfor autonomous obstacle avoidance and navigation onboard MAVs.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 12:49:02 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Dinaux", "Raoul", ""], ["Wessendorp", "Nikhil", ""], ["Dupeyroux", "Julien", ""], ["de Croon", "Guido", ""]]}, {"id": "2102.12827", "submitter": "Maura Pintor", "authors": "Maura Pintor, Fabio Roli, Wieland Brendel, Battista Biggio", "title": "Fast Minimum-norm Adversarial Attacks through Adaptive Norm Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating adversarial robustness amounts to finding the minimum perturbation\nneeded to have an input sample misclassified. The inherent complexity of the\nunderlying optimization requires current gradient-based attacks to be carefully\ntuned, initialized, and possibly executed for many computationally-demanding\niterations, even if specialized to a given perturbation model. In this work, we\novercome these limitations by proposing a fast minimum-norm (FMN) attack that\nworks with different $\\ell_p$-norm perturbation models ($p=0, 1, 2, \\infty$),\nis robust to hyperparameter choices, does not require adversarial starting\npoints, and converges within few lightweight steps. It works by iteratively\nfinding the sample misclassified with maximum confidence within an\n$\\ell_p$-norm constraint of size $\\epsilon$, while adapting $\\epsilon$ to\nminimize the distance of the current sample to the decision boundary. Extensive\nexperiments show that FMN significantly outperforms existing attacks in terms\nof convergence speed and computation time, while reporting comparable or even\nsmaller perturbation sizes.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 12:56:26 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 14:33:12 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Pintor", "Maura", ""], ["Roli", "Fabio", ""], ["Brendel", "Wieland", ""], ["Biggio", "Battista", ""]]}, {"id": "2102.12839", "submitter": "Maurice Quach", "authors": "Maurice Quach, Aladine Chetouani, Giuseppe Valenzise and Frederic\n  Dufaux", "title": "A deep perceptual metric for 3D point clouds", "comments": "Presented at IS&T Electronic Imaging: Image Quality and System\n  Performance, January 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds are essential for storage and transmission of 3D content. As\nthey can entail significant volumes of data, point cloud compression is crucial\nfor practical usage. Recently, point cloud geometry compression approaches\nbased on deep neural networks have been explored. In this paper, we evaluate\nthe ability to predict perceptual quality of typical voxel-based loss functions\nemployed to train these networks. We find that the commonly used focal loss and\nweighted binary cross entropy are poorly correlated with human perception. We\nthus propose a perceptual loss function for 3D point clouds which outperforms\nexisting loss functions on the ICIP2020 subjective dataset. In addition, we\npropose a novel truncated distance field voxel grid representation and find\nthat it leads to sparser latent spaces and loss functions that are more\ncorrelated with perceived visual quality compared to a binary representation.\nThe source code is available at\nhttps://github.com/mauriceqch/2021_pc_perceptual_loss.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 13:24:59 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Quach", "Maurice", ""], ["Chetouani", "Aladine", ""], ["Valenzise", "Giuseppe", ""], ["Dufaux", "Frederic", ""]]}, {"id": "2102.12853", "submitter": "M. Alex O. Vasilescu", "authors": "M. Alex O. Vasilescu, Eric Kim, and Xiao S. Zeng", "title": "CausalX: Causal Explanations and Block Multilinear Factor Analysis", "comments": "arXiv admin note: text overlap with arXiv:1911.04180", "journal-ref": "2020 25th International Conference on Pattern Recognition (ICPR),\n  Milan, Italy, pp. 10736-10743", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By adhering to the dictum, \"No causation without manipulation (treatment,\nintervention)\", cause and effect data analysis represents changes in observed\ndata in terms of changes in the causal factors. When causal factors are not\namenable for active manipulation in the real world due to current technological\nlimitations or ethical considerations, a counterfactual approach performs an\nintervention on the model of data formation. In the case of object\nrepresentation or activity (temporal object) representation, varying object\nparts is generally unfeasible whether they be spatial and/or temporal.\nMultilinear algebra, the algebra of higher-order tensors, is a suitable and\ntransparent framework for disentangling the causal factors of data formation.\nLearning a part-based intrinsic causal factor representations in a multilinear\nframework requires applying a set of interventions on a part-based multilinear\nmodel. We propose a unified multilinear model of wholes and parts. We derive a\nhierarchical block multilinear factorization, the M-mode Block SVD, that\ncomputes a disentangled representation of the causal factors by optimizing\nsimultaneously across the entire object hierarchy. Given computational\nefficiency considerations, we introduce an incremental bottom-up computational\nalternative, the Incremental M-mode Block SVD, that employs the lower-level\nabstractions, the part representations, to represent the higher level of\nabstractions, the parent wholes. This incremental computational approach may\nalso be employed to update the causal model parameters when data becomes\navailable incrementally. The resulting object representation is an\ninterpretable combinatorial choice of intrinsic causal factor representations\nrelated to an object's recursive hierarchy of wholes and parts that renders\nobject recognition robust to occlusion and reduces training data requirements.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 13:49:01 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 12:03:44 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Vasilescu", "M. Alex O.", ""], ["Kim", "Eric", ""], ["Zeng", "Xiao S.", ""]]}, {"id": "2102.12867", "submitter": "Yuhang Zang", "authors": "Yuhang Zang, Chen Huang, Chen Change Loy", "title": "FASA: Feature Augmentation and Sampling Adaptation for Long-Tailed\n  Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent methods for long-tailed instance segmentation still struggle on rare\nobject classes with few training data. We propose a simple yet effective\nmethod, Feature Augmentation and Sampling Adaptation (FASA), that addresses the\ndata scarcity issue by augmenting the feature space especially for rare\nclasses. Both the Feature Augmentation (FA) and feature sampling components are\nadaptive to the actual training status -- FA is informed by the feature mean\nand variance of observed real samples from past iterations, and we sample the\ngenerated virtual features in a loss-adapted manner to avoid over-fitting. FASA\ndoes not require any elaborate loss design, and removes the need for\ninter-class transfer learning that often involves large cost and\nmanually-defined head/tail class groups. We show FASA is a fast, generic method\nthat can be easily plugged into standard or long-tailed segmentation\nframeworks, with consistent performance gains and little added cost. FASA is\nalso applicable to other tasks like long-tailed classification with\nstate-of-the-art performance. Code will be released.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 14:07:23 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Zang", "Yuhang", ""], ["Huang", "Chen", ""], ["Loy", "Chen Change", ""]]}, {"id": "2102.12898", "submitter": "Soumick Chatterjee", "authors": "Soumick Chatterjee, Alessandro Sciarra, Max D\\\"unnwald, Raghava\n  Vinaykanth Mushunuri, Ranadheer Podishetti, Rajatha Nagaraja Rao, Geetha\n  Doddapaneni Gopinath, Steffen Oeltze-Jafra, Oliver Speck and Andreas\n  N\\\"urnberger", "title": "ShuffleUNet: Super resolution of diffusion-weighted MRIs using deep\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion-weighted magnetic resonance imaging (DW-MRI) can be used to\ncharacterise the microstructure of the nervous tissue, e.g. to delineate brain\nwhite matter connections in a non-invasive manner via fibre tracking. Magnetic\nResonance Imaging (MRI) in high spatial resolution would play an important role\nin visualising such fibre tracts in a superior manner. However, obtaining an\nimage of such resolution comes at the expense of longer scan time. Longer scan\ntime can be associated with the increase of motion artefacts, due to the\npatient's psychological and physical conditions. Single Image Super-Resolution\n(SISR), a technique aimed to obtain high-resolution (HR) details from one\nsingle low-resolution (LR) input image, achieved with Deep Learning, is the\nfocus of this study. Compared to interpolation techniques or sparse-coding\nalgorithms, deep learning extracts prior knowledge from big datasets and\nproduces superior MRI images from the low-resolution counterparts. In this\nresearch, a deep learning based super-resolution technique is proposed and has\nbeen applied for DW-MRI. Images from the IXI dataset have been used as the\nground-truth and were artificially downsampled to simulate the low-resolution\nimages. The proposed method has shown statistically significant improvement\nover the baselines and achieved an SSIM of $0.913\\pm0.045$.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 14:52:23 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Chatterjee", "Soumick", ""], ["Sciarra", "Alessandro", ""], ["D\u00fcnnwald", "Max", ""], ["Mushunuri", "Raghava Vinaykanth", ""], ["Podishetti", "Ranadheer", ""], ["Rao", "Rajatha Nagaraja", ""], ["Gopinath", "Geetha Doddapaneni", ""], ["Oeltze-Jafra", "Steffen", ""], ["Speck", "Oliver", ""], ["N\u00fcrnberger", "Andreas", ""]]}, {"id": "2102.12911", "submitter": "Markus D. Solbach", "authors": "Markus D. Solbach, John K. Tsotsos", "title": "Blocks World Revisited: The Effect of Self-Occlusion on Classification\n  by Convolutional Neural Networks", "comments": "12 pages, 19 Figures, in submission,\n  https://nvision2.data.eecs.yorku.ca/TEOS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the recent successes in computer vision, there remain new avenues to\nexplore. In this work, we propose a new dataset to investigate the effect of\nself-occlusion on deep neural networks. With TEOS (The Effect of\nSelf-Occlusion), we propose a 3D blocks world dataset that focuses on the\ngeometric shape of 3D objects and their omnipresent challenge of\nself-occlusion. We designed TEOS to investigate the role of self-occlusion in\nthe context of object classification. Even though remarkable progress has been\nseen in object classification, self-occlusion is a challenge. In the\nreal-world, self-occlusion of 3D objects still presents significant challenges\nfor deep learning approaches. However, humans deal with this by deploying\ncomplex strategies, for instance, by changing the viewpoint or manipulating the\nscene to gather necessary information. With TEOS, we present a dataset of two\ndifficulty levels (L1 and L2 ), containing 36 and 12 objects, respectively. We\nprovide 738 uniformly sampled views of each object, their mask, object and\ncamera position, orientation, amount of self-occlusion, as well as the CAD\nmodel of each object. We present baseline evaluations with five well-known\nclassification deep neural networks and show that TEOS poses a significant\nchallenge for all of them. The dataset, as well as the pre-trained models, are\nmade publicly available for the scientific community under\nhttps://nvision2.data.eecs.yorku.ca/TEOS.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 15:02:47 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Solbach", "Markus D.", ""], ["Tsotsos", "John K.", ""]]}, {"id": "2102.12926", "submitter": "Mustafa Hajij", "authors": "Mustafa Hajij, Ghada Zamzmi, Xuanting Cai", "title": "Persistent Homology and Graphs Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG cs.CV math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article aims to study the topological invariant properties encoded in\nnode graph representational embeddings by utilizing tools available in\npersistent homology. Specifically, given a node embedding representation\nalgorithm, we consider the case when these embeddings are real-valued. By\nviewing these embeddings as scalar functions on a domain of interest, we can\nutilize the tools available in persistent homology to study the topological\ninformation encoded in these representations. Our construction effectively\ndefines a unique persistence-based graph descriptor, on both the graph and node\nlevels, for every node representation algorithm. To demonstrate the\neffectiveness of the proposed method, we study the topological descriptors\ninduced by DeepWalk, Node2Vec and Diff2Vec.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 15:26:21 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 15:07:37 GMT"}, {"version": "v3", "created": "Tue, 2 Mar 2021 04:59:27 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Hajij", "Mustafa", ""], ["Zamzmi", "Ghada", ""], ["Cai", "Xuanting", ""]]}, {"id": "2102.12982", "submitter": "Nils Rethmeier", "authors": "Nils Rethmeier and Isabelle Augenstein", "title": "A Primer on Contrastive Pretraining in Language Processing: Methods,\n  Lessons Learned and Perspectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern natural language processing (NLP) methods employ self-supervised\npretraining objectives such as masked language modeling to boost the\nperformance of various application tasks. These pretraining methods are\nfrequently extended with recurrence, adversarial or linguistic property\nmasking, and more recently with contrastive learning objectives. Contrastive\nself-supervised training objectives enabled recent successes in image\nrepresentation pretraining by learning to contrast input-input pairs of\naugmented images as either similar or dissimilar. However, in NLP, automated\ncreation of text input augmentations is still very challenging because a single\ntoken can invert the meaning of a sentence. For this reason, some contrastive\nNLP pretraining methods contrast over input-label pairs, rather than over\ninput-input pairs, using methods from Metric Learning and Energy Based Models.\nIn this survey, we summarize recent self-supervised and supervised contrastive\nNLP pretraining methods and describe where they are used to improve language\nmodeling, few or zero-shot learning, pretraining data-efficiency and specific\nNLP end-tasks. We introduce key contrastive learning concepts with lessons\nlearned from prior research and structure works by applications and cross-field\nrelations. Finally, we point to open challenges and future directions for\ncontrastive NLP to encourage bringing contrastive NLP pretraining closer to\nrecent successes in image representation pretraining.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 16:35:07 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Rethmeier", "Nils", ""], ["Augenstein", "Isabelle", ""]]}, {"id": "2102.13002", "submitter": "Inseop Chung", "authors": "Inseop Chung, Daesik Kim, Nojun Kwak", "title": "Maximizing Cosine Similarity Between Spatial Features for Unsupervised\n  Domain Adaptation in Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method that tackles the problem of unsupervised domain\nadaptation for semantic segmentation by maximizing the cosine similarity\nbetween the source and the target domain at the feature level. A segmentation\nnetwork mainly consists of two parts, a feature extractor and a classification\nhead. We expect that if we can make the two domains have small domain gap at\nthe feature level, they would also have small domain discrepancy at the\nclassification head. Our method computes a cosine similarity matrix between the\nsource feature map and the target feature map, then we maximize the elements\nexceeding a threshold to guide the target features to have high similarity with\nthe most similar source feature. Moreover, we use a class-wise source feature\ndictionary which stores the latest features of the source domain to prevent the\nunmatching problem when computing the cosine similarity matrix and be able to\ncompare a target feature with various source features from various images.\nThrough extensive experiments, we verify that our method gains performance on\ntwo unsupervised domain adaptation tasks (GTA5$\\to$ Cityscaspes and\nSYNTHIA$\\to$ Cityscapes).\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 17:05:46 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 04:13:39 GMT"}, {"version": "v3", "created": "Thu, 18 Mar 2021 02:50:30 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Chung", "Inseop", ""], ["Kim", "Daesik", ""], ["Kwak", "Nojun", ""]]}, {"id": "2102.13011", "submitter": "Zhihao Shi", "authors": "Zhihao Shi, Chengqi Li, Linhui Dai, Xiaohong Liu, Jun Chen, Timothy N.\n  Davidson", "title": "Learning for Unconstrained Space-Time Video Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen considerable research activities devoted to video\nenhancement that simultaneously increases temporal frame rate and spatial\nresolution. However, the existing methods either fail to explore the intrinsic\nrelationship between temporal and spatial information or lack flexibility in\nthe choice of final temporal/spatial resolution. In this work, we propose an\nunconstrained space-time video super-resolution network, which can effectively\nexploit space-time correlation to boost performance. Moreover, it has complete\nfreedom in adjusting the temporal frame rate and spatial resolution through the\nuse of the optical flow technique and a generalized pixelshuffle operation. Our\nextensive experiments demonstrate that the proposed method not only outperforms\nthe state-of-the-art, but also requires far fewer parameters and less running\ntime.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 17:15:05 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Shi", "Zhihao", ""], ["Li", "Chengqi", ""], ["Dai", "Linhui", ""], ["Liu", "Xiaohong", ""], ["Chen", "Jun", ""], ["Davidson", "Timothy N.", ""]]}, {"id": "2102.13030", "submitter": "Rita Ramos", "authors": "Rita Parada Ramos, Patr\\'icia Pereira, Helena Moniz, Joao Paulo\n  Carvalho, Bruno Martins", "title": "Retrieval Augmentation for Deep Neural Networks", "comments": "Accepted at IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks have achieved state-of-the-art results in various vision\nand/or language tasks. Despite the use of large training datasets, most models\nare trained by iterating over single input-output pairs, discarding the\nremaining examples for the current prediction. In this work, we actively\nexploit the training data, using the information from nearest training examples\nto aid the prediction both during training and testing. Specifically, our\napproach uses the target of the most similar training example to initialize the\nmemory state of an LSTM model, or to guide attention mechanisms. We apply this\napproach to image captioning and sentiment analysis, respectively through image\nand text retrieval. Results confirm the effectiveness of the proposed approach\nfor the two tasks, on the widely used Flickr8 and IMDB datasets. Our code is\npublicly available at http://github.com/RitaRamo/retrieval-augmentation-nn.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 17:38:31 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 09:14:47 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Ramos", "Rita Parada", ""], ["Pereira", "Patr\u00edcia", ""], ["Moniz", "Helena", ""], ["Carvalho", "Joao Paulo", ""], ["Martins", "Bruno", ""]]}, {"id": "2102.13042", "submitter": "Andrew Wilson", "authors": "Gregory W. Benton, Wesley J. Maddox, Sanae Lotfi, Andrew Gordon Wilson", "title": "Loss Surface Simplexes for Mode Connecting Volumes and Fast Ensembling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a better understanding of the loss surfaces for multilayer networks, we\ncan build more robust and accurate training procedures. Recently it was\ndiscovered that independently trained SGD solutions can be connected along\none-dimensional paths of near-constant training loss. In this paper, we show\nthat there are mode-connecting simplicial complexes that form multi-dimensional\nmanifolds of low loss, connecting many independently trained models. Inspired\nby this discovery, we show how to efficiently build simplicial complexes for\nfast ensembling, outperforming independently trained deep ensembles in\naccuracy, calibration, and robustness to dataset shift. Notably, our approach\nonly requires a few training epochs to discover a low-loss simplex, starting\nfrom a pre-trained solution. Code is available at\nhttps://github.com/g-benton/loss-surface-simplexes.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 17:53:24 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Benton", "Gregory W.", ""], ["Maddox", "Wesley J.", ""], ["Lotfi", "Sanae", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "2102.13066", "submitter": "Burhaneddin Yaman", "authors": "Chi Zhang, Jinghan Jia, Burhaneddin Yaman, Steen Moeller, Sijia Liu,\n  Mingyi Hong, Mehmet Ak\\c{c}akaya", "title": "On Instabilities of Conventional Multi-Coil MRI Reconstruction to Small\n  Adverserial Perturbations", "comments": "To appear in Proceedings of the 29th Annual Meeting of ISMRM, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning (DL) has received much attention in accelerated MRI,\nrecent studies suggest small perturbations may lead to instabilities in\nDL-based reconstructions, leading to concern for their clinical application.\nHowever, these works focus on single-coil acquisitions, which is not practical.\nWe investigate instabilities caused by small adversarial attacks for multi-coil\nacquisitions. Our results suggest that, parallel imaging and multi-coil CS\nexhibit considerable instabilities against small adversarial perturbations.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 18:27:51 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Zhang", "Chi", ""], ["Jia", "Jinghan", ""], ["Yaman", "Burhaneddin", ""], ["Moeller", "Steen", ""], ["Liu", "Sijia", ""], ["Hong", "Mingyi", ""], ["Ak\u00e7akaya", "Mehmet", ""]]}, {"id": "2102.13086", "submitter": "Xingyi Zhou", "authors": "Xingyi Zhou, Vladlen Koltun, Philipp Kr\\\"ahenb\\\"uhl", "title": "Simple multi-dataset detection", "comments": "code is available at https://github.com/xingyizhou/UniDet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How do we build a general and broad object detection system? We use all\nlabels of all concepts ever annotated. These labels span diverse datasets with\npotentially inconsistent taxonomies. In this paper, we present a simple method\nfor training a unified detector on multiple large-scale datasets. We use\ndataset-specific training protocols and losses, but share a common detection\narchitecture with dataset-specific outputs. We show how to automatically\nintegrate these dataset-specific outputs into a common semantic taxonomy. In\ncontrast to prior work, our approach does not require manual taxonomy\nreconciliation. Our multi-dataset detector performs as well as dataset-specific\nmodels on each training domain, but generalizes much better to new unseen\ndomains. Entries based on the presented methodology ranked first in the object\ndetection and instance segmentation tracks of the ECCV 2020 Robust Vision\nChallenge.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 18:55:58 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Zhou", "Xingyi", ""], ["Koltun", "Vladlen", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""]]}, {"id": "2102.13090", "submitter": "Qianqian Wang", "authors": "Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard\n  Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah Snavely, Thomas\n  Funkhouser", "title": "IBRNet: Learning Multi-View Image-Based Rendering", "comments": "CVPR 2021. Project page: https://ibrnet.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a method that synthesizes novel views of complex scenes by\ninterpolating a sparse set of nearby views. The core of our method is a network\narchitecture that includes a multilayer perceptron and a ray transformer that\nestimates radiance and volume density at continuous 5D locations (3D spatial\nlocations and 2D viewing directions), drawing appearance information on the fly\nfrom multiple source views. By drawing on source views at render time, our\nmethod hearkens back to classic work on image-based rendering (IBR), and allows\nus to render high-resolution imagery. Unlike neural scene representation work\nthat optimizes per-scene functions for rendering, we learn a generic view\ninterpolation function that generalizes to novel scenes. We render images using\nclassic volume rendering, which is fully differentiable and allows us to train\nusing only multi-view posed images as supervision. Experiments show that our\nmethod outperforms recent novel view synthesis methods that also seek to\ngeneralize to novel scenes. Further, if fine-tuned on each scene, our method is\ncompetitive with state-of-the-art single-scene neural rendering methods.\nProject page: https://ibrnet.github.io/\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 18:56:21 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 23:19:33 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Wang", "Qianqian", ""], ["Wang", "Zhicheng", ""], ["Genova", "Kyle", ""], ["Srinivasan", "Pratul", ""], ["Zhou", "Howard", ""], ["Barron", "Jonathan T.", ""], ["Martin-Brualla", "Ricardo", ""], ["Snavely", "Noah", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "2102.13123", "submitter": "Shubhendu Trivedi", "authors": "Zhen Lin, Nicholas Huang, Camille Avestruz, W. L. Kimmy Wu, Shubhendu\n  Trivedi, Jo\\~ao Caldeira, Brian Nord", "title": "DeepSZ: Identification of Sunyaev-Zel'dovich Galaxy Clusters using Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": "FERMILAB-PUB-21-077-SCD", "categories": "astro-ph.CO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Galaxy clusters identified from the Sunyaev Zel'dovich (SZ) effect are a key\ningredient in multi-wavelength cluster-based cosmology. We present a comparison\nbetween two methods of cluster identification: the standard Matched Filter (MF)\nmethod in SZ cluster finding and a method using Convolutional Neural Networks\n(CNN). We further implement and show results for a `combined' identifier. We\napply the methods to simulated millimeter maps for several observing\nfrequencies for an SPT-3G-like survey. There are some key differences between\nthe methods. The MF method requires image pre-processing to remove point\nsources and a model for the noise, while the CNN method requires very little\npre-processing of images. Additionally, the CNN requires tuning of\nhyperparameters in the model and takes as input, cutout images of the sky.\nSpecifically, we use the CNN to classify whether or not an 8 arcmin $\\times$ 8\narcmin cutout of the sky contains a cluster. We compare differences in purity\nand completeness. The MF signal-to-noise ratio depends on both mass and\nredshift. Our CNN, trained for a given mass threshold, captures a different set\nof clusters than the MF, some of which have SNR below the MF detection\nthreshold. However, the CNN tends to mis-classify cutouts whose clusters are\nlocated near the edge of the cutout, which can be mitigated with staggered\ncutouts. We leverage the complementarity of the two methods, combining the\nscores from each method for identification. The purity and completeness of the\nMF alone are both 0.61, assuming a standard detection threshold. The purity and\ncompleteness of the CNN alone are 0.59 and 0.61. The combined classification\nmethod yields 0.60 and 0.77, a significant increase for completeness with a\nmodest decrease in purity. We advocate for combined methods that increase the\nconfidence of many lower signal-to-noise clusters.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 19:01:00 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 21:01:34 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Lin", "Zhen", ""], ["Huang", "Nicholas", ""], ["Avestruz", "Camille", ""], ["Wu", "W. L. Kimmy", ""], ["Trivedi", "Shubhendu", ""], ["Caldeira", "Jo\u00e3o", ""], ["Nord", "Brian", ""]]}, {"id": "2102.13143", "submitter": "Jaideep Murkute", "authors": "Jaideep Murkute", "title": "Robust Pollen Imagery Classification with Generative Modeling and Mixup\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning approaches have shown great success in image classification\ntasks and can aid greatly towards the fast and reliable classification of\npollen grain aerial imagery. However, often-times deep learning methods in the\nsetting of natural images can suffer generalization problems and yield poor\nperformance on unseen test distribution. In this work, we present and a robust\ndeep learning framework that can generalize well for pollen grain\naerobiological imagery classification. We develop a convolutional neural\nnetwork-based pollen grain classification approach and combine some of the best\npractices in deep learning for better generalization. In addition to\ncommonplace approaches like data-augmentation and weight regularization, we\nutilize implicit regularization methods like manifold mixup to allow learning\nof smoother decision boundaries. We also make use of proven state-of-the-art\narchitectural choices like EfficientNet convolutional neural networks. Inspired\nby the success of generative modeling with variational autoencoders, we train\nmodels with a richer learning objective which can allow the model to focus on\nthe relevant parts of the image. Finally, we create an ensemble of neural\nnetworks, for the robustness of the test set predictions. Based on our\nexperiments, we show improved generalization performance as measured with a\nweighted F1-score with the aforementioned approaches. The proposed approach\nearned a fourth-place in the final rankings in the ICPR-2020 Pollen Grain\nClassification Challenge; with a 0.972578 weighted F1 score,0.950828 macro\naverage F1 scores, and 0.972877 recognition accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 19:39:24 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Murkute", "Jaideep", ""]]}, {"id": "2102.13147", "submitter": "Seong Jae Hwang", "authors": "Anthony Sicilia, Xingchen Zhao, Davneet Minhas, Erin O'Connor, Howard\n  Aizenstein, William Klunk, Dana Tudorascu, Seong Jae Hwang", "title": "Multi-Domain Learning by Meta-Learning: Taking Optimal Steps in\n  Multi-Domain Loss Landscapes by Inner-Loop Learning", "comments": "IEEE International Symposium on Biomedical Imaging 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider a model-agnostic solution to the problem of Multi-Domain Learning\n(MDL) for multi-modal applications. Many existing MDL techniques are\nmodel-dependent solutions which explicitly require nontrivial architectural\nchanges to construct domain-specific modules. Thus, properly applying these MDL\ntechniques for new problems with well-established models, e.g. U-Net for\nsemantic segmentation, may demand various low-level implementation efforts. In\nthis paper, given emerging multi-modal data (e.g., various structural\nneuroimaging modalities), we aim to enable MDL purely algorithmically so that\nwidely used neural networks can trivially achieve MDL in a model-independent\nmanner. To this end, we consider a weighted loss function and extend it to an\neffective procedure by employing techniques from the recently active area of\nlearning-to-learn (meta-learning). Specifically, we take inner-loop gradient\nsteps to dynamically estimate posterior distributions over the hyperparameters\nof our loss function. Thus, our method is model-agnostic, requiring no\nadditional model parameters and no network architecture changes; instead, only\na few efficient algorithmic modifications are needed to improve performance in\nMDL. We demonstrate our solution to a fitting problem in medical imaging,\nspecifically, in the automatic segmentation of white matter hyperintensity\n(WMH). We look at two neuroimaging modalities (T1-MR and FLAIR) with\ncomplementary information fitting for our problem.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 19:54:44 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Sicilia", "Anthony", ""], ["Zhao", "Xingchen", ""], ["Minhas", "Davneet", ""], ["O'Connor", "Erin", ""], ["Aizenstein", "Howard", ""], ["Klunk", "William", ""], ["Tudorascu", "Dana", ""], ["Hwang", "Seong Jae", ""]]}, {"id": "2102.13184", "submitter": "Huichen Li", "authors": "Huichen Li and Linyi Li and Xiaojun Xu and Xiaolu Zhang and Shuang\n  Yang and Bo Li", "title": "Nonlinear Projection Based Gradient Estimation for Query Efficient\n  Blackbox Attacks", "comments": "Accepted by AISTATS 2021; 9 pages excluding references and appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Gradient estimation and vector space projection have been studied as two\ndistinct topics. We aim to bridge the gap between the two by investigating how\nto efficiently estimate gradient based on a projected low-dimensional space. We\nfirst provide lower and upper bounds for gradient estimation under both linear\nand nonlinear projections, and outline checkable sufficient conditions under\nwhich one is better than the other. Moreover, we analyze the query complexity\nfor the projection-based gradient estimation and present a sufficient condition\nfor query-efficient estimators. Built upon our theoretic analysis, we propose a\nnovel query-efficient Nonlinear Gradient Projection-based Boundary Blackbox\nAttack (NonLinear-BA). We conduct extensive experiments on four image datasets:\nImageNet, CelebA, CIFAR-10, and MNIST, and show the superiority of the proposed\nmethods compared with the state-of-the-art baselines. In particular, we show\nthat the projection-based boundary blackbox attacks are able to achieve much\nsmaller magnitude of perturbations with 100% attack success rate based on\nefficient queries. Both linear and nonlinear projections demonstrate their\nadvantages under different conditions. We also evaluate NonLinear-BA against\nthe commercial online API MEGVII Face++, and demonstrate the high blackbox\nattack performance both quantitatively and qualitatively. The code is publicly\navailable at https://github.com/AI-secure/NonLinear-BA.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 21:32:19 GMT"}, {"version": "v2", "created": "Sat, 24 Apr 2021 16:22:40 GMT"}, {"version": "v3", "created": "Wed, 28 Apr 2021 19:39:40 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Li", "Huichen", ""], ["Li", "Linyi", ""], ["Xu", "Xiaojun", ""], ["Zhang", "Xiaolu", ""], ["Yang", "Shuang", ""], ["Li", "Bo", ""]]}, {"id": "2102.13190", "submitter": "George Papakostas Prof.", "authors": "G.K. Sidiropoulos, G.A. Papakostas", "title": "Machine Biometrics -- Towards Identifying Machines in a Smart City\n  Environment", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper deals with the identification of machines in a smart city\nenvironment. The concept of machine biometrics is proposed in this work for the\nfirst time, as a way to authenticate machine identities interacting with humans\nin everyday life. This definition is imposed in modern years where autonomous\nvehicles, social robots, etc. are considered active members of contemporary\nsocieties. In this context, the case of car identification from the engine\nbehavioral biometrics is examined. For this purpose, 22 sound features were\nextracted and their discrimination capabilities were tested in combination with\n9 different machine learning classifiers, towards identifying 5 car\nmanufacturers. The experimental results revealed the ability of the proposed\nbiometrics to identify cars with high accuracy up to 98% for the case of the\nMultilayer Perceptron (MLP) neural network model.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 21:49:20 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Sidiropoulos", "G. K.", ""], ["Papakostas", "G. A.", ""]]}, {"id": "2102.13258", "submitter": "Feng Xue", "authors": "Feng Xue and Junfeng Cao and Yu Zhou and Fei Sheng and Yankai Wang and\n  Anlong Ming", "title": "Boundary-induced and scene-aggregated network for monocular depth\n  prediction", "comments": "Accepted by Pattern Recognition 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular depth prediction is an important task in scene understanding. It\naims to predict the dense depth of a single RGB image. With the development of\ndeep learning, the performance of this task has made great improvements.\nHowever, two issues remain unresolved: (1) The deep feature encodes the wrong\nfarthest region in a scene, which leads to a distorted 3D structure of the\npredicted depth; (2) The low-level features are insufficient utilized, which\nmakes it even harder to estimate the depth near the edge with sudden depth\nchange. To tackle these two issues, we propose the Boundary-induced and\nScene-aggregated network (BS-Net). In this network, the Depth Correlation\nEncoder (DCE) is first designed to obtain the contextual correlations between\nthe regions in an image, and perceive the farthest region by considering the\ncorrelations. Meanwhile, the Bottom-Up Boundary Fusion (BUBF) module is\ndesigned to extract accurate boundary that indicates depth change. Finally, the\nStripe Refinement module (SRM) is designed to refine the dense depth induced by\nthe boundary cue, which improves the boundary accuracy of the predicted depth.\nSeveral experimental results on the NYUD v2 dataset and \\xff{the iBims-1\ndataset} illustrate the state-of-the-art performance of the proposed approach.\nAnd the SUN-RGBD dataset is employed to evaluate the generalization of our\nmethod. Code is available at https://github.com/XuefengBUPT/BS-Net.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 01:43:17 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 15:09:46 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Xue", "Feng", ""], ["Cao", "Junfeng", ""], ["Zhou", "Yu", ""], ["Sheng", "Fei", ""], ["Wang", "Yankai", ""], ["Ming", "Anlong", ""]]}, {"id": "2102.13262", "submitter": "Yu Shen", "authors": "Yu Shen, Laura Zheng, Manli Shu, Weizi Li, Tom Goldstein, Ming C. Lin", "title": "Improving Robustness of Learning-based Autonomous Steering Using\n  Adversarial Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  For safety of autonomous driving, vehicles need to be able to drive under\nvarious lighting, weather, and visibility conditions in different environments.\nThese external and environmental factors, along with internal factors\nassociated with sensors, can pose significant challenges to perceptual data\nprocessing, hence affecting the decision-making and control of the vehicle. In\nthis work, we address this critical issue by introducing a framework for\nanalyzing robustness of the learning algorithm w.r.t varying quality in the\nimage input for autonomous driving. Using the results of sensitivity analysis,\nwe further propose an algorithm to improve the overall performance of the task\nof \"learning to steer\". The results show that our approach is able to enhance\nthe learning outcomes up to 48%. A comparative study drawn between our approach\nand other related techniques, such as data augmentation and adversarial\ntraining, confirms the effectiveness of our algorithm as a way to improve the\nrobustness and generalization of neural network training for autonomous\ndriving.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 02:08:07 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Shen", "Yu", ""], ["Zheng", "Laura", ""], ["Shu", "Manli", ""], ["Li", "Weizi", ""], ["Goldstein", "Tom", ""], ["Lin", "Ming C.", ""]]}, {"id": "2102.13269", "submitter": "Yi Zhou", "authors": "Yi Zhou, Lei Huang, Tianfei Zhou, Ling Shao", "title": "Many-to-One Distribution Learning and K-Nearest Neighbor Smoothing for\n  Thoracic Disease Identification", "comments": "Accepted to AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Chest X-rays are an important and accessible clinical imaging tool for the\ndetection of many thoracic diseases. Over the past decade, deep learning, with\na focus on the convolutional neural network (CNN), has become the most powerful\ncomputer-aided diagnosis technology for improving disease identification\nperformance. However, training an effective and robust deep CNN usually\nrequires a large amount of data with high annotation quality. For chest X-ray\nimaging, annotating large-scale data requires professional domain knowledge and\nis time-consuming. Thus, existing public chest X-ray datasets usually adopt\nlanguage pattern based methods to automatically mine labels from reports.\nHowever, this results in label uncertainty and inconsistency. In this paper, we\npropose many-to-one distribution learning (MODL) and K-nearest neighbor\nsmoothing (KNNS) methods from two perspectives to improve a single model's\ndisease identification performance, rather than focusing on an ensemble of\nmodels. MODL integrates multiple models to obtain a soft label distribution for\noptimizing the single target model, which can reduce the effects of original\nlabel uncertainty. Moreover, KNNS aims to enhance the robustness of the target\nmodel to provide consistent predictions on images with similar medical\nfindings. Extensive experiments on the public NIH Chest X-ray and CheXpert\ndatasets show that our model achieves consistent improvements over the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 02:29:30 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Zhou", "Yi", ""], ["Huang", "Lei", ""], ["Zhou", "Tianfei", ""], ["Shao", "Ling", ""]]}, {"id": "2102.13272", "submitter": "Jingbo Jiang", "authors": "Jingbo Jiang, Xizi Chen, Chi-Ying Tsui", "title": "A Reconfigurable Winograd CNN Accelerator with Nesting Decomposition\n  Algorithm for Computing Convolution with Large Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent literature found that convolutional neural networks (CNN) with large\nfilters perform well in some applications such as image semantic segmentation.\nWinograd transformation helps to reduce the number of multiplications in a\nconvolution but suffers from numerical instability when the convolution filter\nsize gets large. This work proposes a nested Winograd algorithm to iteratively\ndecompose a large filter into a sequence of 3x3 tiles which can then be\naccelerated with a 3x3 Winograd algorithm. Compared with the state-of-art\nOLA-Winograd algorithm, the proposed algorithm reduces the multiplications by\n1.41 to 3.29 times for computing 5x5 to 9x9 convolutions.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 02:42:42 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Jiang", "Jingbo", ""], ["Chen", "Xizi", ""], ["Tsui", "Chi-Ying", ""]]}, {"id": "2102.13280", "submitter": "Luyan Liu", "authors": "Luyan Liu, Zhiwei Wen, Songwei Liu, Hong-Yu Zhou, Hongwei Zhu,\n  Weicheng Xie, Linlin Shen, Kai Ma and Yefeng Zheng", "title": "MixSearch: Searching for Domain Generalized Medical Image Segmentation\n  Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Considering the scarcity of medical data, most datasets in medical image\nanalysis are an order of magnitude smaller than those of natural images.\nHowever, most Network Architecture Search (NAS) approaches in medical images\nfocused on specific datasets and did not take into account the generalization\nability of the learned architectures on unseen datasets as well as different\ndomains. In this paper, we address this point by proposing to search for\ngeneralizable U-shape architectures on a composited dataset that mixes medical\nimages from multiple segmentation tasks and domains creatively, which is named\nMixSearch. Specifically, we propose a novel approach to mix multiple\nsmall-scale datasets from multiple domains and segmentation tasks to produce a\nlarge-scale dataset. Then, a novel weaved encoder-decoder structure is designed\nto search for a generalized segmentation network in both cell-level and\nnetwork-level. The network produced by the proposed MixSearch framework\nachieves state-of-the-art results compared with advanced encoder-decoder\nnetworks across various datasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 02:55:28 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Liu", "Luyan", ""], ["Wen", "Zhiwei", ""], ["Liu", "Songwei", ""], ["Zhou", "Hong-Yu", ""], ["Zhu", "Hongwei", ""], ["Xie", "Weicheng", ""], ["Shen", "Linlin", ""], ["Ma", "Kai", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2102.13318", "submitter": "Seogkyu Jeon", "authors": "Seogkyu Jeon, Pilhyeon Lee, Kibeom Hong, Hyeran Byun", "title": "Continuous Face Aging Generative Adversarial Networks", "comments": "To appear at ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face aging is the task aiming to translate the faces in input images to\ndesignated ages. To simplify the problem, previous methods have limited\nthemselves only able to produce discrete age groups, each of which consists of\nten years. Consequently, the exact ages of the translated results are unknown\nand it is unable to obtain the faces of different ages within groups. To this\nend, we propose the continuous face aging generative adversarial networks\n(CFA-GAN). Specifically, to make the continuous aging feasible, we propose to\ndecompose image features into two orthogonal features: the identity and the age\nbasis features. Moreover, we introduce the novel loss function for identity\npreservation which maximizes the cosine similarity between the original and the\ngenerated identity basis features. With the qualitative and quantitative\nevaluations on MORPH, we demonstrate the realistic and continuous aging ability\nof our model, validating its superiority against existing models. To the best\nof our knowledge, this work is the first attempt to handle continuous target\nages.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 06:22:25 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Jeon", "Seogkyu", ""], ["Lee", "Pilhyeon", ""], ["Hong", "Kibeom", ""], ["Byun", "Hyeran", ""]]}, {"id": "2102.13319", "submitter": "Chun-Hsien Lin", "authors": "Chun-Hsien Lin and Bing-Fei Wu", "title": "Domain Adapting Ability of Self-Supervised Learning for Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep convolutional networks have achieved great performance in face\nrecognition tasks, the challenge of domain discrepancy still exists in real\nworld applications. Lack of domain coverage of training data (source domain)\nmakes the learned models degenerate in a testing scenario (target domain). In\nface recognition tasks, classes in two domains are usually different, so\nclassical domain adaptation approaches, assuming there are shared classes in\ndomains, may not be reasonable solutions for this problem. In this paper,\nself-supervised learning is adopted to learn a better embedding space where the\nsubjects in target domain are more distinguishable. The learning goal is\nmaximizing the similarity between the embeddings of each image and its mirror\nin both domains. The experiments show its competitive results compared with\nprior works. To know the reason why it can achieve such performance, we further\ndiscuss how this approach affects the learning of embeddings.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 06:23:14 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Lin", "Chun-Hsien", ""], ["Wu", "Bing-Fei", ""]]}, {"id": "2102.13322", "submitter": "Hongxin Xiang", "authors": "Cheng Xie, Ting Zeng, Hongxin Xiang, Keqin Li, Yun Yang, Qing Liu", "title": "Class Knowledge Overlay to Visual Feature Learning for Zero-Shot Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New categories can be discovered by transforming semantic features into\nsynthesized visual features without corresponding training samples in zero-shot\nimage classification. Although significant progress has been made in generating\nhigh-quality synthesized visual features using generative adversarial networks,\nguaranteeing semantic consistency between the semantic features and visual\nfeatures remains very challenging. In this paper, we propose a novel zero-shot\nlearning approach, GAN-CST, based on class knowledge to visual feature learning\nto tackle the problem. The approach consists of three parts, class knowledge\noverlay, semi-supervised learning and triplet loss. It applies class knowledge\noverlay (CKO) to obtain knowledge not only from the corresponding class but\nalso from other classes that have the knowledge overlay. It ensures that the\nknowledge-to-visual learning process has adequate information to generate\nsynthesized visual features. The approach also applies a semi-supervised\nlearning process to re-train knowledge-to-visual model. It contributes to\nreinforcing synthesized visual features generation as well as new category\nprediction. We tabulate results on a number of benchmark datasets demonstrating\nthat the proposed model delivers superior performance over state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 06:34:35 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Xie", "Cheng", ""], ["Zeng", "Ting", ""], ["Xiang", "Hongxin", ""], ["Li", "Keqin", ""], ["Yang", "Yun", ""], ["Liu", "Qing", ""]]}, {"id": "2102.13323", "submitter": "Eli Shlizerman", "authors": "Jinlin Xiang, Shane Colburn, Arka Majumdar, Eli Shlizerman", "title": "Knowledge Distillation Circumvents Nonlinearity for Optical\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.ET cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Convolutional Neural Networks (CNNs) have enabled ubiquitous\nimage processing applications. As such, CNNs require fast runtime (forward\npropagation) to process high-resolution visual streams in real time. This is\nstill a challenging task even with state-of-the-art graphics and tensor\nprocessing units. The bottleneck in computational efficiency primarily occurs\nin the convolutional layers. Performing operations in the Fourier domain is a\npromising way to accelerate forward propagation since it transforms\nconvolutions into elementwise multiplications, which are considerably faster to\ncompute for large kernels. Furthermore, such computation could be implemented\nusing an optical 4f system with orders of magnitude faster operation. However,\na major challenge in using this spectral approach, as well as in an optical\nimplementation of CNNs, is the inclusion of a nonlinearity between each\nconvolutional layer, without which CNN performance drops dramatically. Here, we\npropose a Spectral CNN Linear Counterpart (SCLC) network architecture and\ndevelop a Knowledge Distillation (KD) approach to circumvent the need for a\nnonlinearity and successfully train such networks. While the KD approach is\nknown in machine learning as an effective process for network pruning, we adapt\nthe approach to transfer the knowledge from a nonlinear network (teacher) to a\nlinear counterpart (student). We show that the KD approach can achieve\nperformance that easily surpasses the standard linear version of a CNN and\ncould approach the performance of the nonlinear network. Our simulations show\nthat the possibility of increasing the resolution of the input image allows our\nproposed 4f optical linear network to perform more efficiently than a nonlinear\nnetwork with the same accuracy on two fundamental image processing tasks: (i)\nobject classification and (ii) semantic segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 06:35:34 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Xiang", "Jinlin", ""], ["Colburn", "Shane", ""], ["Majumdar", "Arka", ""], ["Shlizerman", "Eli", ""]]}, {"id": "2102.13326", "submitter": "Hongxin Xiang", "authors": "Zeng Ting, Xiang Hongxin, Xie Cheng, Yang Yun, Liu Qing", "title": "Zero-Shot Learning Based on Knowledge Sharing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-Shot Learning (ZSL) is an emerging research that aims to solve the\nclassification problems with very few training data. The present works on ZSL\nmainly focus on the mapping of learning semantic space to visual space. It\nencounters many challenges that obstruct the progress of ZSL research. First,\nthe representation of the semantic feature is inadequate to represent all\nfeatures of the categories. Second, the domain drift problem still exists\nduring the transfer from semantic space to visual space. In this paper, we\nintroduce knowledge sharing (KS) to enrich the representation of semantic\nfeatures. Based on KS, we apply a generative adversarial network to generate\npseudo visual features from semantic features that are very close to the real\nvisual features. Abundant experimental results from two benchmark datasets of\nZSL show that the proposed approach has a consistent improvement.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 06:43:29 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Ting", "Zeng", ""], ["Hongxin", "Xiang", ""], ["Cheng", "Xie", ""], ["Yun", "Yang", ""], ["Qing", "Liu", ""]]}, {"id": "2102.13327", "submitter": "Chun-Hsien Lin", "authors": "Chun-Hsien Lin and Bing-Fei Wu", "title": "Mitigating Domain Mismatch in Face Recognition Using Style Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite outstanding performance on public benchmarks, face recognition still\nsuffers due to domain mismatch between training (source) and testing (target)\ndata. Furthermore, these domains are not shared classes, which complicates\ndomain adaptation. Since this is also a fine-grained classification problem\nwhich does not strictly follow the low-density separation principle,\nconventional domain adaptation approaches do not resolve these problems. In\nthis paper, we formulate domain mismatch in face recognition as a style\nmismatch problem for which we propose two methods. First, we design a domain\ndiscriminator with human-level judgment to mine target-like images in the\ntraining data to mitigate the domain gap. Second, we extract style\nrepresentations in low-level feature maps of the backbone model, and match the\nstyle distributions of the two domains to find a common style representation.\nEvaluations on verification and open-set and closed-set identification\nprotocols show that both methods yield good improvements, and that performance\nis more robust if they are combined. Our approach is competitive with related\nwork, and its effectiveness is verified in a practical application.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 06:43:50 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 03:51:06 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Lin", "Chun-Hsien", ""], ["Wu", "Bing-Fei", ""]]}, {"id": "2102.13329", "submitter": "Fu-En Yang", "authors": "Fu-En Yang, Jing-Cheng Chang, Yuan-Hao Lee, Yu-Chiang Frank Wang", "title": "Dual-MTGAN: Stochastic and Deterministic Motion Transfer for\n  Image-to-Video Synthesis", "comments": "Accepted to ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating videos with content and motion variations is a challenging task in\ncomputer vision. While the recent development of GAN allows video generation\nfrom latent representations, it is not easy to produce videos with particular\ncontent of motion patterns of interest. In this paper, we propose Dual Motion\nTransfer GAN (Dual-MTGAN), which takes image and video data as inputs while\nlearning disentangled content and motion representations. Our Dual-MTGAN is\nable to perform deterministic motion transfer and stochastic motion generation.\nBased on a given image, the former preserves the input content and transfers\nmotion patterns observed from another video sequence, and the latter directly\nproduces videos with plausible yet diverse motion patterns based on the input\nimage. The proposed model is trained in an end-to-end manner, without the need\nto utilize pre-defined motion features like pose or facial landmarks. Our\nquantitative and qualitative results would confirm the effectiveness and\nrobustness of our model in addressing such conditioned image-to-video tasks.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 06:54:48 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Yang", "Fu-En", ""], ["Chang", "Jing-Cheng", ""], ["Lee", "Yuan-Hao", ""], ["Wang", "Yu-Chiang Frank", ""]]}, {"id": "2102.13360", "submitter": "Zun Li", "authors": "Zun Li, Congyan Lang, Liqian Liang, Tao Wang, Songhe Feng, Jun Wu, and\n  Yidong Li", "title": "A Universal Model for Cross Modality Mapping by Relational Reasoning", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the aim of matching a pair of instances from two different modalities,\ncross modality mapping has attracted growing attention in the computer vision\ncommunity. Existing methods usually formulate the mapping function as the\nsimilarity measure between the pair of instance features, which are embedded to\na common space. However, we observe that the relationships among the instances\nwithin a single modality (intra relations) and those between the pair of\nheterogeneous instances (inter relations) are insufficiently explored in\nprevious approaches. Motivated by this, we redefine the mapping function with\nrelational reasoning via graph modeling, and further propose a GCN-based\nRelational Reasoning Network (RR-Net) in which inter and intra relations are\nefficiently computed to universally resolve the cross modality mapping problem.\nConcretely, we first construct two kinds of graph, i.e., Intra Graph and Inter\nGraph, to respectively model intra relations and inter relations. Then RR-Net\nupdates all the node features and edge features in an iterative manner for\nlearning intra and inter relations simultaneously. Last, RR-Net outputs the\nprobabilities over the edges which link a pair of heterogeneous instances to\nestimate the mapping results. Extensive experiments on three example tasks,\ni.e., image classification, social recommendation and sound recognition,\nclearly demonstrate the superiority and universality of our proposed model.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 08:56:24 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Li", "Zun", ""], ["Lang", "Congyan", ""], ["Liang", "Liqian", ""], ["Wang", "Tao", ""], ["Feng", "Songhe", ""], ["Wu", "Jun", ""], ["Li", "Yidong", ""]]}, {"id": "2102.13378", "submitter": "Alexandre Bruckert", "authors": "Alexandre Bruckert, Marc Christie, Olivier Le Meur", "title": "Where to look at the movies : Analyzing visual attention to understand\n  movie editing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the process of making a movie, directors constantly care about where the\nspectator will look on the screen. Shot composition, framing, camera movements\nor editing are tools commonly used to direct attention. In order to provide a\nquantitative analysis of the relationship between those tools and gaze\npatterns, we propose a new eye-tracking database, containing gaze pattern\ninformation on movie sequences, as well as editing annotations, and we show how\nstate-of-the-art computational saliency techniques behave on this dataset. In\nthis work, we expose strong links between movie editing and spectators\nscanpaths, and open several leads on how the knowledge of editing information\ncould improve human visual attention modeling for cinematic content. The\ndataset generated and analysed during the current study is available at\nhttps://github.com/abruckert/eye_tracking_filmmaking\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 09:54:58 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Bruckert", "Alexandre", ""], ["Christie", "Marc", ""], ["Meur", "Olivier Le", ""]]}, {"id": "2102.13391", "submitter": "Rajat Sharma", "authors": "Rajat Sharma, Tobias Schwandt, Christian Kunert, Steffen Urban and\n  Wolfgang Broll", "title": "Point Cloud Upsampling and Normal Estimation using Deep Learning for\n  Robust Surface Reconstruction", "comments": null, "journal-ref": "In Proceedings of the 16th International Joint Conference on\n  Computer Vision, Imaging and Computer Graphics Theory and Applications\n  (VISIGRAPP 2021) - Volume 5: VISAPP, pages 70-79", "doi": "10.5220/0010211600700079", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The reconstruction of real-world surfaces is on high demand in various\napplications. Most existing reconstruction approaches apply 3D scanners for\ncreating point clouds which are generally sparse and of low density. These\npoints clouds will be triangulated and used for visualization in combination\nwith surface normals estimated by geometrical approaches. However, the quality\nof the reconstruction depends on the density of the point cloud and the\nestimation of the surface normals. In this paper, we present a novel deep\nlearning architecture for point cloud upsampling that enables subsequent stable\nand smooth surface reconstruction. A noisy point cloud of low density with\ncorresponding point normals is used to estimate a point cloud with higher\ndensity and appendant point normals. To this end, we propose a compound loss\nfunction that encourages the network to estimate points that lie on a surface\nincluding normals accurately predicting the orientation of the surface. Our\nresults show the benefit of estimating normals together with point positions.\nThe resulting point cloud is smoother, more complete, and the final surface\nreconstruction is much closer to ground truth.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 10:58:26 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Sharma", "Rajat", ""], ["Schwandt", "Tobias", ""], ["Kunert", "Christian", ""], ["Urban", "Steffen", ""], ["Broll", "Wolfgang", ""]]}, {"id": "2102.13392", "submitter": "Dimitri Gominski", "authors": "Dimitri Gominski, Val\\'erie Gouet-Brunet, Liming Chen", "title": "Unifying Remote Sensing Image Retrieval and Classification with Robust\n  Fine-tuning", "comments": "Classification methodology is not correct (using all labels at test\n  time). Further experiments with retrieval results show error in reported\n  results (mean+confidence intervals do not show significant improvement)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Advances in high resolution remote sensing image analysis are currently\nhampered by the difficulty of gathering enough annotated data for training deep\nlearning methods, giving rise to a variety of small datasets and associated\ndataset-specific methods. Moreover, typical tasks such as classification and\nretrieval lack a systematic evaluation on standard benchmarks and training\ndatasets, which make it hard to identify durable and generalizable scientific\ncontributions. We aim at unifying remote sensing image retrieval and\nclassification with a new large-scale training and testing dataset, SF300,\nincluding both vertical and oblique aerial images and made available to the\nresearch community, and an associated fine-tuning method. We additionally\npropose a new adversarial fine-tuning method for global descriptors. We show\nthat our framework systematically achieves a boost of retrieval and\nclassification performance on nine different datasets compared to an ImageNet\npretrained baseline, with currently no other method to compare to.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 11:01:30 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 15:56:56 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Gominski", "Dimitri", ""], ["Gouet-Brunet", "Val\u00e9rie", ""], ["Chen", "Liming", ""]]}, {"id": "2102.13400", "submitter": "Kailun Yang", "authors": "Hao Chen, Weijian Hu, Kailun Yang, Jian Bai, Kaiwei Wang", "title": "Panoramic annular SLAM with loop closure and global optimization", "comments": "Accepted to Applied Optics. 12 pages, 11 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose panoramic annular simultaneous localization and\nmapping (PA-SLAM), a visual SLAM system based on panoramic annular lens. A\nhybrid point selection strategy is put forward in the tracking front-end, which\nensures repeatability of keypoints and enables loop closure detection based on\nthe bag-of-words approach. Every detected loop candidate is verified\ngeometrically and the $Sim(3)$ relative pose constraint is estimated to perform\npose graph optimization and global bundle adjustment in the back-end. A\ncomprehensive set of experiments on real-world datasets demonstrates that the\nhybrid point selection strategy allows reliable loop closure detection, and the\naccumulated error and scale drift have been significantly reduced via global\noptimization, enabling PA-SLAM to reach state-of-the-art accuracy while\nmaintaining high robustness and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 11:22:40 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 09:09:44 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Chen", "Hao", ""], ["Hu", "Weijian", ""], ["Yang", "Kailun", ""], ["Bai", "Jian", ""], ["Wang", "Kaiwei", ""]]}, {"id": "2102.13423", "submitter": "Roger Mar\\'i", "authors": "Roland Akiki, Roger Mar\\'i, Carlo de Franchis, Jean-Michel Morel,\n  Gabriele Facciolo", "title": "Robust Rational Polynomial Camera Modelling for SAR and Pushbroom\n  Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The Rational Polynomial Camera (RPC) model can be used to describe a variety\nof image acquisition systems in remote sensing, notably optical and Synthetic\nAperture Radar (SAR) sensors. RPC functions relate 3D to 2D coordinates and\nvice versa, regardless of physical sensor specificities, which has made them an\nessential tool to harness satellite images in a generic way. This article\ndescribes a terrain-independent algorithm to accurately derive a RPC model from\na set of 3D-2D point correspondences based on a regularized least squares fit.\nThe performance of the method is assessed by varying the point correspondences\nand the size of the area that they cover. We test the algorithm on SAR and\noptical data, to derive RPCs from physical sensor models or from other RPC\nmodels after composition with corrective functions.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 12:16:35 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Akiki", "Roland", ""], ["Mar\u00ed", "Roger", ""], ["de Franchis", "Carlo", ""], ["Morel", "Jean-Michel", ""], ["Facciolo", "Gabriele", ""]]}, {"id": "2102.13438", "submitter": "Xiongfeng Peng", "authors": "Xiongfeng Peng, Zhihua Liu, Qiang Wang, Yun-Tae Kim, Myungjae Jeon", "title": "Accurate Visual-Inertial SLAM by Feature Re-identification", "comments": "7 pages, 4 figures, Submitted to ICRA2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel feature re-identification method for real-time\nvisual-inertial SLAM. The front-end module of the state-of-the-art\nvisual-inertial SLAM methods (e.g. visual feature extraction and matching\nschemes) relies on feature tracks across image frames, which are easily broken\nin challenging scenarios, resulting in insufficient visual measurement and\naccumulated error in pose estimation. In this paper, we propose an efficient\ndrift-less SLAM method by re-identifying existing features from a\nspatial-temporal sensitive sub-global map. The re-identified features over a\nlong time span serve as augmented visual measurements and are incorporated into\nthe optimization module which can gradually decrease the accumulative error in\nthe long run, and further build a drift-less global map in the system.\nExtensive experiments show that our feature re-identification method is both\neffective and efficient. Specifically, when combining the feature\nre-identification with the state-of-the-art SLAM method [11], our method\nachieves 67.3% and 87.5% absolute translation error reduction with only a small\nadditional computational cost on two public SLAM benchmark DBs: EuRoC and\nTUM-VI respectively.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 12:54:33 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Peng", "Xiongfeng", ""], ["Liu", "Zhihua", ""], ["Wang", "Qiang", ""], ["Kim", "Yun-Tae", ""], ["Jeon", "Myungjae", ""]]}, {"id": "2102.13493", "submitter": "Dom Ginhac", "authors": "Yu Liu, Fan Yang and Dominique Ginhac", "title": "ACDnet: An action detection network for real-time edge computing based\n  on flow-guided feature approximation and memory aggregation", "comments": "Accepted for publication in Pattern Recognition Letters", "journal-ref": "Pattern Recognition Letters, 145 , 118-126, 2021", "doi": "10.1016/j.patrec.2021.02.001", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Interpreting human actions requires understanding the spatial and temporal\ncontext of the scenes. State-of-the-art action detectors based on Convolutional\nNeural Network (CNN) have demonstrated remarkable results by adopting\ntwo-stream or 3D CNN architectures. However, these methods typically operate in\na non-real-time, ofline fashion due to system complexity to reason\nspatio-temporal information. Consequently, their high computational cost is not\ncompliant with emerging real-world scenarios such as service robots or public\nsurveillance where detection needs to take place at resource-limited edge\ndevices. In this paper, we propose ACDnet, a compact action detection network\ntargeting real-time edge computing which addresses both efficiency and\naccuracy. It intelligently exploits the temporal coherence between successive\nvideo frames to approximate their CNN features rather than naively extracting\nthem. It also integrates memory feature aggregation from past video frames to\nenhance current detection stability, implicitly modeling long temporal cues\nover time. Experiments conducted on the public benchmark datasets UCF-24 and\nJHMDB-21 demonstrate that ACDnet, when integrated with the SSD detector, can\nrobustly achieve detection well above real-time (75 FPS). At the same time, it\nretains reasonable accuracy (70.92 and 49.53 frame mAP) compared to other\ntop-performing methods using far heavier configurations. Codes will be\navailable at https://github.com/dginhac/ACDnet.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 14:06:31 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Liu", "Yu", ""], ["Yang", "Fan", ""], ["Ginhac", "Dominique", ""]]}, {"id": "2102.13517", "submitter": "Kuo Yang", "authors": "Kuo Yang, Emad A. Mohammed, Behrouz H. Far", "title": "Detection of Alzheimer's Disease Using Graph-Regularized Convolutional\n  Neural Network Based on Structural Similarity Learning of Brain Magnetic\n  Resonance Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: This paper presents an Alzheimer's disease (AD) detection method\nbased on learning structural similarity between Magnetic Resonance Images\n(MRIs) and representing this similarity as a graph. Methods: We construct the\nsimilarity graph using embedded features of the input image (i.e., Non-Demented\n(ND), Very Mild Demented (VMD), Mild Demented (MD), and Moderated Demented\n(MDTD)). We experiment and compare different dimension-reduction and clustering\nalgorithms to construct the best similarity graph to capture the similarity\nbetween the same class images using the cosine distance as a similarity\nmeasure. We utilize the similarity graph to present (sample) the training data\nto a convolutional neural network (CNN). We use the similarity graph as a\nregularizer in the loss function of a CNN model to minimize the distance\nbetween the input images and their k-nearest neighbours in the similarity graph\nwhile minimizing the categorical cross-entropy loss between the training image\npredictions and the actual image class labels. Results: We conduct extensive\nexperiments with several pre-trained CNN models and compare the results to\nother recent methods. Conclusion: Our method achieves superior performance on\nthe testing dataset (accuracy = 0.986, area under receiver operating\ncharacteristics curve = 0.998, F1 measure = 0.987). Significance: The\nclassification results show an improvement in the prediction accuracy compared\nto the other methods. We release all the code used in our experiments to\nencourage reproducible research in this area\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 14:49:50 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Yang", "Kuo", ""], ["Mohammed", "Emad A.", ""], ["Far", "Behrouz H.", ""]]}, {"id": "2102.13520", "submitter": "Duolikun Danier", "authors": "Duolikun Danier and David Bull", "title": "Texture-aware Video Frame Interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Temporal interpolation has the potential to be a powerful tool for video\ncompression. Existing methods for frame interpolation do not discriminate\nbetween video textures and generally invoke a single general model capable of\ninterpolating a wide range of video content. However, past work on video\ntexture analysis and synthesis has shown that different textures exhibit vastly\ndifferent motion characteristics and they can be divided into three classes\n(static, dynamic continuous and dynamic discrete). In this work, we study the\nimpact of video textures on video frame interpolation, and propose a novel\nframework where, given an interpolation algorithm, separate models are trained\non different textures. Our study shows that video texture has significant\nimpact on the performance of frame interpolation models and it is beneficial to\nhave separate models specifically adapted to these texture classes, instead of\ntraining a single model that tries to learn generic motion. Our results\ndemonstrate that models fine-tuned using our framework achieve, on average, a\n0.3dB gain in PSNR on the test set used.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 14:46:56 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Danier", "Duolikun", ""], ["Bull", "David", ""]]}, {"id": "2102.13541", "submitter": "Jue Jiang Dr.", "authors": "Harini Veeraraghavan, Jue Jiang, Sharif Elguindi, Sean L. Berry,\n  Ifeanyirochukwu Onochie, Aditya Apte, Laura Cervino, Joseph O. Deasy", "title": "Nested-block self-attention for robust radiotherapy planning\n  segmentation", "comments": "Under review at Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep convolutional networks have been widely studied for head and\nneck (HN) organs at risk (OAR) segmentation, their use for routine clinical\ntreatment planning is limited by a lack of robustness to imaging artifacts, low\nsoft tissue contrast on CT, and the presence of abnormal anatomy. In order to\naddress these challenges, we developed a computationally efficient nested block\nself-attention (NBSA) method that can be combined with any convolutional\nnetwork. Our method achieves computational efficiency by performing non-local\ncalculations within memory blocks of fixed spatial extent. Contextual\ndependencies are captured by passing information in a raster scan order between\nblocks, as well as through a second attention layer that causes bi-directional\nattention flow. We implemented our approach on three different networks to\ndemonstrate feasibility. Following training using 200 cases, we performed\ncomprehensive evaluations using conventional and clinical metrics on a separate\nset of 172 test scans sourced from external and internal institution datasets\nwithout any exclusion criteria. NBSA required a similar number of computations\n(15.7 gflops) as the most efficient criss-cross attention (CCA) method and\ngenerated significantly more accurate segmentations for brain stem (Dice of\n0.89 vs. 0.86) and parotid glands (0.86 vs. 0.84) than CCA. NBSA's\nsegmentations were less variable than multiple 3D methods, including for small\norgans with low soft-tissue contrast such as the submandibular glands (surface\nDice of 0.90).\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 15:28:47 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Veeraraghavan", "Harini", ""], ["Jiang", "Jue", ""], ["Elguindi", "Sharif", ""], ["Berry", "Sean L.", ""], ["Onochie", "Ifeanyirochukwu", ""], ["Apte", "Aditya", ""], ["Cervino", "Laura", ""], ["Deasy", "Joseph O.", ""]]}, {"id": "2102.13558", "submitter": "Hao Zhang", "authors": "Hao Zhang, Aixin Sun, Wei Jing, Liangli Zhen, Joey Tianyi Zhou, Rick\n  Siow Mong Goh", "title": "Natural Language Video Localization: A Revisit in Span-based Question\n  Answering Framework", "comments": "15 pages, 18 figures, and 10 tables. Accepted by IEEE Transactions on\n  Pattern Analysis and Machine Intelligence (TPAMI). arXiv admin note:\n  substantial text overlap with arXiv:2004.13931", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2021", "doi": "10.1109/TPAMI.2021.3060449", "report-no": "TPAMI-2020-09-1337.R1", "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural Language Video Localization (NLVL) aims to locate a target moment\nfrom an untrimmed video that semantically corresponds to a text query. Existing\napproaches mainly solve the NLVL problem from the perspective of computer\nvision by formulating it as ranking, anchor, or regression tasks. These methods\nsuffer from large performance degradation when localizing on long videos. In\nthis work, we address the NLVL from a new perspective, i.e., span-based\nquestion answering (QA), by treating the input video as a text passage. We\npropose a video span localizing network (VSLNet), on top of the standard\nspan-based QA framework (named VSLBase), to address NLVL. VSLNet tackles the\ndifferences between NLVL and span-based QA through a simple yet effective\nquery-guided highlighting (QGH) strategy. QGH guides VSLNet to search for the\nmatching video span within a highlighted region. To address the performance\ndegradation on long videos, we further extend VSLNet to VSLNet-L by applying a\nmulti-scale split-and-concatenation strategy. VSLNet-L first splits the\nuntrimmed video into short clip segments; then, it predicts which clip segment\ncontains the target moment and suppresses the importance of other segments.\nFinally, the clip segments are concatenated, with different confidences, to\nlocate the target moment accurately. Extensive experiments on three benchmark\ndatasets show that the proposed VSLNet and VSLNet-L outperform the\nstate-of-the-art methods; VSLNet-L addresses the issue of performance\ndegradation on long videos. Our study suggests that the span-based QA framework\nis an effective strategy to solve the NLVL problem.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 15:57:59 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 07:58:49 GMT"}, {"version": "v3", "created": "Tue, 2 Mar 2021 09:42:19 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Zhang", "Hao", ""], ["Sun", "Aixin", ""], ["Jing", "Wei", ""], ["Zhen", "Liangli", ""], ["Zhou", "Joey Tianyi", ""], ["Goh", "Rick Siow Mong", ""]]}, {"id": "2102.13588", "submitter": "Shuai Yu", "authors": "Shuai Yu, Jianyang Xie, Jinkui Hao, Yalin Zheng, Jiong Zhang, Yan Hu,\n  Jiang Liu, Yitian Zhao", "title": "3D Vessel Reconstruction in OCT-Angiography via Depth Map Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optical Coherence Tomography Angiography (OCTA) has been increasingly used in\nthe management of eye and systemic diseases in recent years. Manual or\nautomatic analysis of blood vessel in 2D OCTA images (en face angiograms) is\ncommonly used in clinical practice, however it may lose rich 3D spatial\ndistribution information of blood vessels or capillaries that are useful for\nclinical decision-making. In this paper, we introduce a novel 3D vessel\nreconstruction framework based on the estimation of vessel depth maps from OCTA\nimages. First, we design a network with structural constraints to predict the\ndepth of blood vessels in OCTA images. In order to promote the accuracy of the\npredicted depth map at both the overall structure- and pixel- level, we combine\nMSE and SSIM loss as the training loss function. Finally, the 3D vessel\nreconstruction is achieved by utilizing the estimated depth map and 2D vessel\nsegmentation results. Experimental results demonstrate that our method is\neffective in the depth prediction and 3D vessel reconstruction for OCTA\nimages.% results may be used to guide subsequent vascular analysis\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 16:53:39 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Yu", "Shuai", ""], ["Xie", "Jianyang", ""], ["Hao", "Jinkui", ""], ["Zheng", "Yalin", ""], ["Zhang", "Jiong", ""], ["Hu", "Yan", ""], ["Liu", "Jiang", ""], ["Zhao", "Yitian", ""]]}, {"id": "2102.13624", "submitter": "Jonas Geiping", "authors": "Jonas Geiping, Liam Fowl, Gowthami Somepalli, Micah Goldblum, Michael\n  Moeller, Tom Goldstein", "title": "What Doesn't Kill You Makes You Robust(er): Adversarial Training against\n  Poisons and Backdoors", "comments": "17 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data poisoning is a threat model in which a malicious actor tampers with\ntraining data to manipulate outcomes at inference time. A variety of defenses\nagainst this threat model have been proposed, but each suffers from at least\none of the following flaws: they are easily overcome by adaptive attacks, they\nseverely reduce testing performance, or they cannot generalize to diverse data\npoisoning threat models. Adversarial training, and its variants, is currently\nconsidered the only empirically strong defense against (inference-time)\nadversarial attacks. In this work, we extend the adversarial training framework\nto instead defend against (training-time) poisoning and backdoor attacks. Our\nmethod desensitizes networks to the effects of poisoning by creating poisons\nduring training and injecting them into training batches. We show that this\ndefense withstands adaptive attacks, generalizes to diverse threat models, and\nincurs a better performance trade-off than previous defenses.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 17:54:36 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Geiping", "Jonas", ""], ["Fowl", "Liam", ""], ["Somepalli", "Gowthami", ""], ["Goldblum", "Micah", ""], ["Moeller", "Michael", ""], ["Goldstein", "Tom", ""]]}, {"id": "2102.13635", "submitter": "Issam Hammad", "authors": "Issam Hammad, Ryan Simpson, Hippolyte Djonon Tsague, and Sarah Hall", "title": "Using Deep Learning to Automate the Detection of Flaws in Nuclear Fuel\n  Channel UT Scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nuclear reactor inspections are critical to ensure the safety and reliability\nof plants operation. Inspections occur during planned outages and include the\ninspection of the reactor's fuel channels. In Canada, Ultrasonic Testing (UT)\nis used to inspect the health of fuel channels in Canada's Deuterium Uranium\n(CANDU) reactors. Currently, analysis of the UT scans is performed by manual\nvisualization and measurement to locate, characterize, and disposition flaws.\nTherefore, there is a motivation to develop an automated method that is fast\nand accurate. In this paper, a proof of concept (PoC) that automates the\ndetection of flaws in nuclear fuel channel UT scans using a convolutional\nneural network (CNN) is presented. This industry research was conducted at\nAlithya Digital Technology Corporation in Pickering, Ontario, Canada. The CNN\nmodel was trained after constructing a dataset using historical UT scans and\nthe corresponding inspection results. This data was obtained from a large\nnuclear power generation company in Ontario. The requirement for this prototype\nwas to identify the location of at least a portion of each flaw in fuel channel\nscans while minimizing false positives (FPs). This allows for automatic\ndetection of the location of each flaw where further manual analysis is\nperformed to identify the extent and the type of the flaw. Based on the defined\nrequirement, the proposed model was able to achieve 100% accuracy for UT scans\nwith minor chatter and a 100% sensitivity with minimal FPs for complicated UT\nscans with severe chatter using 18 UT full test scans.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 18:19:07 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Hammad", "Issam", ""], ["Simpson", "Ryan", ""], ["Tsague", "Hippolyte Djonon", ""], ["Hall", "Sarah", ""]]}, {"id": "2102.13644", "submitter": "Aneeq Zia", "authors": "Aneeq Zia, Kiran Bhattacharyya, Xi Liu, Ziheng Wang, Satoshi Kondo,\n  Emanuele Colleoni, Beatrice van Amsterdam, Razeen Hussain, Raabid Hussain,\n  Lena Maier-Hein, Danail Stoyanov, Stefanie Speidel, and Anthony Jarc", "title": "Surgical Visual Domain Adaptation: Results from the MICCAI 2020\n  SurgVisDom Challenge", "comments": "Results from SurgVisDom 2020 challenge held at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Surgical data science is revolutionizing minimally invasive surgery by\nenabling context-aware applications. However, many challenges exist around\nsurgical data (and health data, more generally) needed to develop context-aware\nmodels. This work - presented as part of the Endoscopic Vision (EndoVis)\nchallenge at the Medical Image Computing and Computer Assisted Intervention\n(MICCAI) 2020 conference - seeks to explore the potential for visual domain\nadaptation in surgery to overcome data privacy concerns. In particular, we\npropose to use video from virtual reality (VR) simulations of surgical\nexercises in robotic-assisted surgery to develop algorithms to recognize tasks\nin a clinical-like setting. We present the performance of the different\napproaches to solve visual domain adaptation developed by challenge\nparticipants. Our analysis shows that the presented models were unable to learn\nmeaningful motion based features form VR data alone, but did significantly\nbetter when small amount of clinical-like data was also made available. Based\non these results, we discuss promising methods and further work to address the\nproblem of visual domain adaptation in surgical data science. We also release\nthe challenge dataset publicly at https://www.synapse.org/surgvisdom2020.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 18:45:28 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Zia", "Aneeq", ""], ["Bhattacharyya", "Kiran", ""], ["Liu", "Xi", ""], ["Wang", "Ziheng", ""], ["Kondo", "Satoshi", ""], ["Colleoni", "Emanuele", ""], ["van Amsterdam", "Beatrice", ""], ["Hussain", "Razeen", ""], ["Hussain", "Raabid", ""], ["Maier-Hein", "Lena", ""], ["Stoyanov", "Danail", ""], ["Speidel", "Stefanie", ""], ["Jarc", "Anthony", ""]]}, {"id": "2102.13645", "submitter": "Davood Karimi", "authors": "Davood Karimi, Serge Vasylechko, Ali Gholipour", "title": "Convolution-Free Medical Image Segmentation using Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Like other applications in computer vision, medical image segmentation has\nbeen most successfully addressed using deep learning models that rely on the\nconvolution operation as their main building block. Convolutions enjoy\nimportant properties such as sparse interactions, weight sharing, and\ntranslation equivariance. These properties give convolutional neural networks\n(CNNs) a strong and useful inductive bias for vision tasks. In this work we\nshow that a different method, based entirely on self-attention between\nneighboring image patches and without any convolution operations, can achieve\ncompetitive or better results. Given a 3D image block, our network divides it\ninto $n^3$ 3D patches, where $n=3 \\text{ or } 5$ and computes a 1D embedding\nfor each patch. The network predicts the segmentation map for the center patch\nof the block based on the self-attention between these patch embeddings. We\nshow that the proposed model can achieve segmentation accuracies that are\nbetter than the state of the art CNNs on three datasets. We also propose\nmethods for pre-training this model on large corpora of unlabeled images. Our\nexperiments show that with pre-training the advantage of our proposed network\nover CNNs can be significant when labeled training data is small.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 18:49:13 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Karimi", "Davood", ""], ["Vasylechko", "Serge", ""], ["Gholipour", "Ali", ""]]}]