[{"id": "2008.00072", "submitter": "Francois Grondin", "authors": "Jonathan Vincent, Mathieu Labb\\'e, Jean-Samuel Lauzon, Fran\\c{c}ois\n  Grondin, Pier-Marc Comtois-Rivet, Fran\\c{c}ois Michaud", "title": "Dynamic Object Tracking and Masking for Visual SLAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In dynamic environments, performance of visual SLAM techniques can be\nimpaired by visual features taken from moving objects. One solution is to\nidentify those objects so that their visual features can be removed for\nlocalization and mapping. This paper presents a simple and fast pipeline that\nuses deep neural networks, extended Kalman filters and visual SLAM to improve\nboth localization and mapping in dynamic environments (around 14 fps on a GTX\n1080). Results on the dynamic sequences from the TUM dataset using RTAB-Map as\nvisual SLAM suggest that the approach achieves similar localization performance\ncompared to other state-of-the-art methods, while also providing the position\nof the tracked dynamic objects, a 3D map free of those dynamic objects, better\nloop closure detection with the whole pipeline able to run on a robot moving at\nmoderate speed.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 20:37:14 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Vincent", "Jonathan", ""], ["Labb\u00e9", "Mathieu", ""], ["Lauzon", "Jean-Samuel", ""], ["Grondin", "Fran\u00e7ois", ""], ["Comtois-Rivet", "Pier-Marc", ""], ["Michaud", "Fran\u00e7ois", ""]]}, {"id": "2008.00078", "submitter": "Minghan Li", "authors": "Minghan Li, Xialei Liu, Joost van de Weijer, Bogdan Raducanu", "title": "Learning to Rank for Active Learning: A Listwise Approach", "comments": "Accepted at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning emerged as an alternative to alleviate the effort to label\nhuge amount of data for data hungry applications (such as image/video indexing\nand retrieval, autonomous driving, etc.). The goal of active learning is to\nautomatically select a number of unlabeled samples for annotation (according to\na budget), based on an acquisition function, which indicates how valuable a\nsample is for training the model. The learning loss method is a task-agnostic\napproach which attaches a module to learn to predict the target loss of\nunlabeled data, and select data with the highest loss for labeling. In this\nwork, we follow this strategy but we define the acquisition function as a\nlearning to rank problem and rethink the structure of the loss prediction\nmodule, using a simple but effective listwise approach. Experimental results on\nfour datasets demonstrate that our method outperforms recent state-of-the-art\nactive learning approaches for both image classification and regression tasks.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 21:05:16 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 21:47:34 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Li", "Minghan", ""], ["Liu", "Xialei", ""], ["van de Weijer", "Joost", ""], ["Raducanu", "Bogdan", ""]]}, {"id": "2008.00092", "submitter": "Tien Do", "authors": "Kourosh Sartipi, Tien Do, Tong Ke, Khiem Vuong, Stergios I.\n  Roumeliotis", "title": "Deep Depth Estimation from Visual-Inertial SLAM", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of learning to complete a scene's depth from\nsparse depth points and images of indoor scenes. Specifically, we study the\ncase in which the sparse depth is computed from a visual-inertial simultaneous\nlocalization and mapping (VI-SLAM) system. The resulting point cloud has low\ndensity, it is noisy, and has non-uniform spatial distribution, as compared to\nthe input from active depth sensors, e.g., LiDAR or Kinect. Since the VI-SLAM\nproduces point clouds only over textured areas, we compensate for the missing\ndepth of the low-texture surfaces by leveraging their planar structures and\ntheir surface normals which is an important intermediate representation. The\npre-trained surface normal network, however, suffers from large performance\ndegradation when there is a significant difference in the viewing direction\n(especially the roll angle) of the test image as compared to the trained ones.\nTo address this limitation, we use the available gravity estimate from the\nVI-SLAM to warp the input image to the orientation prevailing in the training\ndataset. This results in a significant performance gain for the surface normal\nestimate, and thus the dense depth estimates. Finally, we show that our method\noutperforms other state-of-the-art approaches both on training (ScanNet and\nNYUv2) and testing (collected with Azure Kinect) datasets.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 21:28:25 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 22:00:36 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Sartipi", "Kourosh", ""], ["Do", "Tien", ""], ["Ke", "Tong", ""], ["Vuong", "Khiem", ""], ["Roumeliotis", "Stergios I.", ""]]}, {"id": "2008.00096", "submitter": "Ian Cherabier", "authors": "Audrey Richard, Ian Cherabier, Martin R. Oswald, Marc Pollefeys,\n  Konrad Schindler", "title": "KAPLAN: A 3D Point Descriptor for Shape Completion", "comments": "18 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel 3D shape completion method that operates directly on\nunstructured point clouds, thus avoiding resource-intensive data structures\nlike voxel grids. To this end, we introduce KAPLAN, a 3D point descriptor that\naggregates local shape information via a series of 2D convolutions. The key\nidea is to project the points in a local neighborhood onto multiple planes with\ndifferent orientations. In each of those planes, point properties like normals\nor point-to-plane distances are aggregated into a 2D grid and abstracted into a\nfeature representation with an efficient 2D convolutional encoder. Since all\nplanes are encoded jointly, the resulting representation nevertheless can\ncapture their correlations and retains knowledge about the underlying 3D shape,\nwithout expensive 3D convolutions. Experiments on public datasets show that\nKAPLAN achieves state-of-the-art performance for 3D shape completion.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 21:56:08 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 11:21:57 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Richard", "Audrey", ""], ["Cherabier", "Ian", ""], ["Oswald", "Martin R.", ""], ["Pollefeys", "Marc", ""], ["Schindler", "Konrad", ""]]}, {"id": "2008.00103", "submitter": "Peter Christen", "authors": "David J. Hand, Peter Christen, Nishadi Kirielle", "title": "F*: An Interpretable Transformation of the F-measure", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The F-measure, also known as the F1-score, is widely used to assess the\nperformance of classification algorithms. However, some researchers find it\nlacking in intuitive interpretation, questioning the appropriateness of\ncombining two aspects of performance as conceptually distinct as precision and\nrecall, and also questioning whether the harmonic mean is the best way to\ncombine them. To ease this concern, we describe a simple transformation of the\nF-measure, which we call F* (F-star), which has an immediate practical\ninterpretation.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 22:37:08 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 22:26:20 GMT"}, {"version": "v3", "created": "Thu, 18 Mar 2021 02:03:47 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Hand", "David J.", ""], ["Christen", "Peter", ""], ["Kirielle", "Nishadi", ""]]}, {"id": "2008.00106", "submitter": "Feiyan Hu", "authors": "Feiyan Hu, Venkatesh G M, Noel E. O'Connor, Alan F. Smeaton and\n  Suzanne Little", "title": "Utilising Visual Attention Cues for Vehicle Detection and Tracking", "comments": "Accepted in ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced Driver-Assistance Systems (ADAS) have been attracting attention from\nmany researchers. Vision-based sensors are the closest way to emulate human\ndriver visual behavior while driving. In this paper, we explore possible ways\nto use visual attention (saliency) for object detection and tracking. We\ninvestigate: 1) How a visual attention map such as a \\emph{subjectness}\nattention or saliency map and an \\emph{objectness} attention map can facilitate\nregion proposal generation in a 2-stage object detector; 2) How a visual\nattention map can be used for tracking multiple objects. We propose a neural\nnetwork that can simultaneously detect objects as and generate objectness and\nsubjectness maps to save computational power. We further exploit the visual\nattention map during tracking using a sequential Monte Carlo probability\nhypothesis density (PHD) filter. The experiments are conducted on KITTI and\nDETRAC datasets. The use of visual attention and hierarchical features has\nshown a considerable improvement of $\\approx$8\\% in object detection which\neffectively increased tracking performance by $\\approx$4\\% on KITTI dataset.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 23:00:13 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Hu", "Feiyan", ""], ["M", "Venkatesh G", ""], ["O'Connor", "Noel E.", ""], ["Smeaton", "Alan F.", ""], ["Little", "Suzanne", ""]]}, {"id": "2008.00119", "submitter": "Indrani Bhattacharya", "authors": "Indrani Bhattacharya and Arun Seetharaman and Wei Shao and Rewa Sood\n  and Christian A. Kunder and Richard E. Fan and Simon John Christoph Soerensen\n  and Jeffrey B. Wang and Pejman Ghanouni and Nikola C. Teslovich and James D.\n  Brooks and Geoffrey A. Sonn and Mirabela Rusu", "title": "CorrSigNet: Learning CORRelated Prostate Cancer SIGnatures from\n  Radiology and Pathology Images for Improved Computer Aided Diagnosis", "comments": "Accepted to MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance Imaging (MRI) is widely used for screening and staging\nprostate cancer. However, many prostate cancers have subtle features which are\nnot easily identifiable on MRI, resulting in missed diagnoses and alarming\nvariability in radiologist interpretation. Machine learning models have been\ndeveloped in an effort to improve cancer identification, but current models\nlocalize cancer using MRI-derived features, while failing to consider the\ndisease pathology characteristics observed on resected tissue. In this paper,\nwe propose CorrSigNet, an automated two-step model that localizes prostate\ncancer on MRI by capturing the pathology features of cancer. First, the model\nlearns MRI signatures of cancer that are correlated with corresponding\nhistopathology features using Common Representation Learning. Second, the model\nuses the learned correlated MRI features to train a Convolutional Neural\nNetwork to localize prostate cancer. The histopathology images are used only in\nthe first step to learn the correlated features. Once learned, these correlated\nfeatures can be extracted from MRI of new patients (without histopathology or\nsurgery) to localize cancer. We trained and validated our framework on a unique\ndataset of 75 patients with 806 slices who underwent MRI followed by\nprostatectomy surgery. We tested our method on an independent test set of 20\nprostatectomy patients (139 slices, 24 cancerous lesions, 1.12M pixels) and\nachieved a per-pixel sensitivity of 0.81, specificity of 0.71, AUC of 0.86 and\na per-lesion AUC of $0.96 \\pm 0.07$, outperforming the current state-of-the-art\naccuracy in predicting prostate cancer using MRI.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 23:44:25 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Bhattacharya", "Indrani", ""], ["Seetharaman", "Arun", ""], ["Shao", "Wei", ""], ["Sood", "Rewa", ""], ["Kunder", "Christian A.", ""], ["Fan", "Richard E.", ""], ["Soerensen", "Simon John Christoph", ""], ["Wang", "Jeffrey B.", ""], ["Ghanouni", "Pejman", ""], ["Teslovich", "Nikola C.", ""], ["Brooks", "James D.", ""], ["Sonn", "Geoffrey A.", ""], ["Rusu", "Mirabela", ""]]}, {"id": "2008.00128", "submitter": "Steven Grosz Mr.", "authors": "Steven A. Grosz, Joshua J. Engelsma, Anil K. Jain", "title": "White-Box Evaluation of Fingerprint Recognition Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical evaluations of fingerprint recognition systems consist of end-to-end\nblack-box evaluations, which assess performance in terms of overall\nidentification or authentication accuracy. However, these black-box tests of\nsystem performance do not reveal insights into the performance of the\nindividual modules, including image acquisition, feature extraction, and\nmatching. On the other hand, white-box evaluations, the topic of this paper,\nmeasure the individual performance of each constituent module in isolation.\nWhile a few studies have conducted white-box evaluations of the fingerprint\nreader, feature extractor, and matching components, no existing study has\nprovided a full system, white-box analysis of the uncertainty introduced at\neach stage of a fingerprint recognition system. In this work, we extend\nprevious white-box evaluations of fingerprint recognition system components and\nprovide a unified, in-depth analysis of fingerprint recognition system\nperformance based on the aggregated white-box evaluation results. In\nparticular, we analyze the uncertainty introduced at each stage of the\nfingerprint recognition system due to adverse capture conditions (i.e., varying\nillumination, moisture, and pressure) at the time of acquisition. Our\nexperiments show that a system that performs better overall, in terms of\nblack-box recognition performance, does not necessarily perform best at each\nmodule in the fingerprint recognition system pipeline, which can only be seen\nwith white-box analysis of each sub-module. Findings such as these enable\nresearchers to better focus their efforts in improving fingerprint recognition\nsystems.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 00:14:37 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Grosz", "Steven A.", ""], ["Engelsma", "Joshua J.", ""], ["Jain", "Anil K.", ""]]}, {"id": "2008.00141", "submitter": "Jing Shi", "authors": "Jing Shi, Zhiheng Li, Haitian Zheng, Yihang Xu, Tianyou Xiao, Weitao\n  Tan, Xiaoning Guo, Sizhe Li, Bin Yang, Zhexin Xu, Ruitao Lin, Zhongkai\n  Shangguan, Yue Zhao, Jingwen Wang, Rohan Sharma, Surya Iyer, Ajinkya\n  Deshmukh, Raunak Mahalik, Srishti Singh, Jayant G Rohra, Yipeng Zhang, Tongyu\n  Yang, Xuan Wen, Ethan Fahnestock, Bryce Ikeda, Ian Lawson, Alan Finkelstein,\n  Kehao Guo, Richard Magnotti, Andrew Sexton, Jeet Ketan Thaker, Yiyang Su,\n  Chenliang Xu", "title": "Actor-Action Video Classification CSC 249/449 Spring 2020 Challenge\n  Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report summarizes submissions and compiles from Actor-Action\nvideo classification challenge held as a final project in CSC 249/449 Machine\nVision course (Spring 2020) at University of Rochester\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 01:07:41 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 16:19:49 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Shi", "Jing", ""], ["Li", "Zhiheng", ""], ["Zheng", "Haitian", ""], ["Xu", "Yihang", ""], ["Xiao", "Tianyou", ""], ["Tan", "Weitao", ""], ["Guo", "Xiaoning", ""], ["Li", "Sizhe", ""], ["Yang", "Bin", ""], ["Xu", "Zhexin", ""], ["Lin", "Ruitao", ""], ["Shangguan", "Zhongkai", ""], ["Zhao", "Yue", ""], ["Wang", "Jingwen", ""], ["Sharma", "Rohan", ""], ["Iyer", "Surya", ""], ["Deshmukh", "Ajinkya", ""], ["Mahalik", "Raunak", ""], ["Singh", "Srishti", ""], ["Rohra", "Jayant G", ""], ["Zhang", "Yipeng", ""], ["Yang", "Tongyu", ""], ["Wen", "Xuan", ""], ["Fahnestock", "Ethan", ""], ["Ikeda", "Bryce", ""], ["Lawson", "Ian", ""], ["Finkelstein", "Alan", ""], ["Guo", "Kehao", ""], ["Magnotti", "Richard", ""], ["Sexton", "Andrew", ""], ["Thaker", "Jeet Ketan", ""], ["Su", "Yiyang", ""], ["Xu", "Chenliang", ""]]}, {"id": "2008.00148", "submitter": "Mohammed Hamzah Abed", "authors": "Mohammed hamzah abed, Lamia Abed Noor Muhammed, Sarah Hussein Toman", "title": "Diabetic Retinopathy Diagnosis based on Convolutional Neural Network", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic Retinopathy DR is a popular disease for many people as a result of\nage or the diabetic, as a result, it can cause blindness. therefore, diagnosis\nof this disease especially in the early time can prevent its effect for a lot\nof patients. To achieve this diagnosis, eye retina must be examined\ncontinuously. Therefore, computer-aided tools can be used in the field based on\ncomputer vision techniques. Different works have been performed using various\nmachine learning techniques. Convolutional Neural Network is one of the promise\nmethods, so it was for Diabetic Retinopathy detection in this paper. Also, the\nproposed work contains visual enhancement in the pre-processing phase, then the\nCNN model is trained to be able for recognition and classification phase, to\ndiagnosis the healthy and unhealthy retina image. Three public dataset\nDiaretDB0, DiaretDB1 and DrimDB were used in practical testing. The\nimplementation of this work based on Matlab- R2019a, deep learning toolbox and\ndeep network designer to design the architecture of the convolutional neural\nnetwork and train it. The results were evaluated to different metrics; accuracy\nis one of them. The best accuracy that was achieved: for DiaretDB0 is 100%,\nDiaretDB1 is 99.495% and DrimDB is 97.55%.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 01:56:04 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["abed", "Mohammed hamzah", ""], ["Muhammed", "Lamia Abed Noor", ""], ["Toman", "Sarah Hussein", ""]]}, {"id": "2008.00157", "submitter": "Flavio de Barros Vidal", "authors": "Ana Paula G. S. de Almeida and Flavio de Barros Vidal", "title": "L-CNN: A Lattice cross-fusion strategy for multistream convolutional\n  neural networks", "comments": "5 pages, 3 figures", "journal-ref": "Electronics Letters, vol. 55, no. 22, pp. 1180-1182, 2029", "doi": "10.1049/el.2019.2631", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a fusion strategy for multistream convolutional networks,\nthe Lattice Cross Fusion. This approach crosses signals from convolution layers\nperforming mathematical operation-based fusions right before pooling layers.\nResults on a purposely worsened CIFAR-10, a popular image classification data\nset, with a modified AlexNet-LCNN version show that this novel method\noutperforms by 46% the baseline single stream network, with faster convergence,\nstability, and robustness.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 03:08:28 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["de Almeida", "Ana Paula G. S.", ""], ["Vidal", "Flavio de Barros", ""]]}, {"id": "2008.00158", "submitter": "Tiancheng Zhi", "authors": "Tiancheng Zhi, Christoph Lassner, Tony Tung, Carsten Stoll, Srinivasa\n  G. Narasimhan and Minh Vo", "title": "TexMesh: Reconstructing Detailed Human Texture and Geometry from RGB-D\n  Video", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present TexMesh, a novel approach to reconstruct detailed human meshes\nwith high-resolution full-body texture from RGB-D video. TexMesh enables high\nquality free-viewpoint rendering of humans. Given the RGB frames, the captured\nenvironment map, and the coarse per-frame human mesh from RGB-D tracking, our\nmethod reconstructs spatiotemporally consistent and detailed per-frame meshes\nalong with a high-resolution albedo texture. By using the incident illumination\nwe are able to accurately estimate local surface geometry and albedo, which\nallows us to further use photometric constraints to adapt a synthetically\ntrained model to real-world sequences in a self-supervised manner for detailed\nsurface geometry and high-resolution texture estimation. In practice, we train\nour models on a short example sequence for self-adaptation and the model runs\nat interactive framerate afterwards. We validate TexMesh on synthetic and\nreal-world data, and show it outperforms the state of art quantitatively and\nqualitatively.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 03:17:23 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2020 05:41:16 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2020 03:14:17 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Zhi", "Tiancheng", ""], ["Lassner", "Christoph", ""], ["Tung", "Tony", ""], ["Stoll", "Carsten", ""], ["Narasimhan", "Srinivasa G.", ""], ["Vo", "Minh", ""]]}, {"id": "2008.00168", "submitter": "Li Rui", "authors": "Rui Li, Shunyi Zheng, Chenxi Duan and Ce Zhang", "title": "Land Cover Classification from Remote Sensing Images Based on\n  Multi-Scale Fully Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a Multi-Scale Fully Convolutional Network (MSFCN) with\nmulti-scale convolutional kernel is proposed to exploit discriminative\nrepresentations from two-dimensional (2D) satellite images.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 04:31:11 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 11:10:31 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Li", "Rui", ""], ["Zheng", "Shunyi", ""], ["Duan", "Chenxi", ""], ["Zhang", "Ce", ""]]}, {"id": "2008.00175", "submitter": "Ajoy Mondal Dr.", "authors": "Ajoy Mondal and Kuntal Ghosh", "title": "State-of-The-Art Fuzzy Active Contour Models for Image Segmentation", "comments": null, "journal-ref": "Soft Computing, 1-17 (2020)", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Image segmentation is the initial step for every image analysis task. A large\nvariety of segmentation algorithm has been proposed in the literature during\nseveral decades with some mixed success. Among them, the fuzzy energy based\nactive contour models get attention to the researchers during last decade which\nresults in development of various methods. A good segmentation algorithm should\nperform well in a large number of images containing noise, blur, low contrast,\nregion in-homogeneity, etc. However, the performances of the most of the\nexisting fuzzy energy based active contour models have been evaluated typically\non the limited number of images. In this article, our aim is to review the\nexisting fuzzy active contour models from the theoretical point of view and\nalso evaluate them experimentally on a large set of images under the various\nconditions. The analysis under a large variety of images provides objective\ninsight into the strengths and weaknesses of various fuzzy active contour\nmodels. Finally, we discuss several issues and future research direction on\nthis particular topic.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 05:42:37 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Mondal", "Ajoy", ""], ["Ghosh", "Kuntal", ""]]}, {"id": "2008.00178", "submitter": "Mohit Prabhushankar", "authors": "Mohit Prabhushankar, Gukyeong Kwon, Dogancan Temel, and Ghassan\n  AlRegib", "title": "Contrastive Explanations in Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual explanations are logical arguments based on visual features that\njustify the predictions made by neural networks. Current modes of visual\nexplanations answer questions of the form $`Why \\text{ } P?'$. These $Why$\nquestions operate under broad contexts thereby providing answers that are\nirrelevant in some cases. We propose to constrain these $Why$ questions based\non some context $Q$ so that our explanations answer contrastive questions of\nthe form $`Why \\text{ } P, \\text{} rather \\text{ } than \\text{ } Q?'$. In this\npaper, we formalize the structure of contrastive visual explanations for neural\nnetworks. We define contrast based on neural networks and propose a methodology\nto extract defined contrasts. We then use the extracted contrasts as a plug-in\non top of existing $`Why \\text{ } P?'$ techniques, specifically Grad-CAM. We\ndemonstrate their value in analyzing both networks and data in applications of\nlarge-scale recognition, fine-grained recognition, subsurface seismic analysis,\nand image quality assessment.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 05:50:01 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Prabhushankar", "Mohit", ""], ["Kwon", "Gukyeong", ""], ["Temel", "Dogancan", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "2008.00188", "submitter": "Shihao Xu", "authors": "Haocong Rao, Shihao Xu, Xiping Hu, Jun Cheng, Bin Hu", "title": "Augmented Skeleton Based Contrastive Action Learning with Momentum LSTM\n  for Unsupervised Action Recognition", "comments": "Accepted by Information Sciences. Our codes are available at\n  https://github.com/Mikexu007/AS-CAL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition via 3D skeleton data is an emerging important topic in\nthese years. Most existing methods either extract hand-crafted descriptors or\nlearn action representations by supervised learning paradigms that require\nmassive labeled data. In this paper, we for the first time propose a\ncontrastive action learning paradigm named AS-CAL that can leverage different\naugmentations of unlabeled skeleton data to learn action representations in an\nunsupervised manner. Specifically, we first propose to contrast similarity\nbetween augmented instances (query and key) of the input skeleton sequence,\nwhich are transformed by multiple novel augmentation strategies, to learn\ninherent action patterns (\"pattern-invariance\") of different skeleton\ntransformations. Second, to encourage learning the pattern-invariance with more\nconsistent action representations, we propose a momentum LSTM, which is\nimplemented as the momentum-based moving average of LSTM based query encoder,\nto encode long-term action dynamics of the key sequence. Third, we introduce a\nqueue to store the encoded keys, which allows our model to flexibly reuse\nproceeding keys and build a more consistent dictionary to improve contrastive\nlearning. Last, by temporally averaging the hidden states of action learned by\nthe query encoder, a novel representation named Contrastive Action Encoding\n(CAE) is proposed to represent human's action effectively. Extensive\nexperiments show that our approach typically improves existing hand-crafted\nmethods by 10-50% top-1 accuracy, and it can achieve comparable or even\nsuperior performance to numerous supervised learning methods.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 06:37:57 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 01:32:35 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2020 13:14:59 GMT"}, {"version": "v4", "created": "Fri, 2 Apr 2021 08:14:45 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Rao", "Haocong", ""], ["Xu", "Shihao", ""], ["Hu", "Xiping", ""], ["Cheng", "Jun", ""], ["Hu", "Bin", ""]]}, {"id": "2008.00192", "submitter": "Xia Chen", "authors": "Xia Chen, Jianren Wang, Martial Hebert", "title": "PanoNet: Real-time Panoptic Segmentation through Position-Sensitive\n  Feature Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We propose a simple, fast, and flexible framework to generate simultaneously\nsemantic and instance masks for panoptic segmentation. Our method, called\nPanoNet, incorporates a clean and natural structure design that tackles the\nproblem purely as a segmentation task without the time-consuming detection\nprocess. We also introduce position-sensitive embedding for instance grouping\nby accounting for both object's appearance and its spatial location. Overall,\nPanoNet yields high panoptic quality results of high-resolution Cityscapes\nimages in real-time, significantly faster than all other methods with\ncomparable performance. Our approach well satisfies the practical speed and\nmemory requirement for many applications like autonomous driving and augmented\nreality.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 06:58:35 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Chen", "Xia", ""], ["Wang", "Jianren", ""], ["Hebert", "Martial", ""]]}, {"id": "2008.00195", "submitter": "Guanghao Yin", "authors": "Guanghao Yin, Shouqian Sun, Chao Li, Xin Min", "title": "Joint Generative Learning and Super-Resolution For Real-World\n  Camera-Screen Degradation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world single image super-resolution (SISR) task, the low-resolution\nimage suffers more complicated degradations, not only downsampled by unknown\nkernels. However, existing SISR methods are generally studied with the\nsynthetic low-resolution generation such as bicubic interpolation (BI), which\ngreatly limits their performance. Recently, some researchers investigate\nreal-world SISR from the perspective of the camera and smartphone. However,\nexcept the acquisition equipment, the display device also involves more\ncomplicated degradations. In this paper, we focus on the camera-screen\ndegradation and build a real-world dataset (Cam-ScreenSR), where HR images are\noriginal ground truths from the previous DIV2K dataset and corresponding LR\nimages are camera-captured versions of HRs displayed on the screen. We conduct\nextensive experiments to demonstrate that involving more real degradations is\npositive to improve the generalization of SISR models. Moreover, we propose a\njoint two-stage model. Firstly, the downsampling degradation GAN(DD-GAN) is\ntrained to model the degradation and produces more various of LR images, which\nis validated to be efficient for data augmentation. Then the dual residual\nchannel attention network (DuRCAN) learns to recover the SR image. The weighted\ncombination of L1 loss and proposed Laplacian loss are applied to sharpen the\nhigh-frequency edges. Extensive experimental results in both typical synthetic\nand complicated real-world degradations validate the proposed method\noutperforms than existing SOTA models with less parameters, faster speed and\nbetter visual results. Moreover, in real captured photographs, our model also\ndelivers best visual quality with sharper edge, less artifacts, especially\nappropriate color enhancement, which has not been accomplished by previous\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 07:10:13 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 14:21:40 GMT"}, {"version": "v3", "created": "Mon, 14 Sep 2020 09:22:34 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Yin", "Guanghao", ""], ["Sun", "Shouqian", ""], ["Li", "Chao", ""], ["Min", "Xin", ""]]}, {"id": "2008.00206", "submitter": "Jiefeng Li", "authors": "Jiefeng Li, Can Wang, Wentao Liu, Chen Qian, Cewu Lu", "title": "HMOR: Hierarchical Multi-Person Ordinal Relations for Monocular\n  Multi-Person 3D Pose Estimation", "comments": "To appear on ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remarkable progress has been made in 3D human pose estimation from a\nmonocular RGB camera. However, only a few studies explored 3D multi-person\ncases. In this paper, we attempt to address the lack of a global perspective of\nthe top-down approaches by introducing a novel form of supervision -\nHierarchical Multi-person Ordinal Relations (HMOR). The HMOR encodes\ninteraction information as the ordinal relations of depths and angles\nhierarchically, which captures the body-part and joint level semantic and\nmaintains global consistency at the same time. In our approach, an integrated\ntop-down model is designed to leverage these ordinal relations in the learning\nprocess. The integrated model estimates human bounding boxes, human depths, and\nroot-relative 3D poses simultaneously, with a coarse-to-fine architecture to\nimprove the accuracy of depth estimation. The proposed method significantly\noutperforms state-of-the-art methods on publicly available multi-person 3D pose\ndatasets. In addition to superior performance, our method costs lower\ncomputation complexity and fewer model parameters.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 07:53:27 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 11:55:12 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Li", "Jiefeng", ""], ["Wang", "Can", ""], ["Liu", "Wentao", ""], ["Qian", "Chen", ""], ["Lu", "Cewu", ""]]}, {"id": "2008.00217", "submitter": "Siyuan Liang", "authors": "Siyuan Liang, Xingxing Wei, Siyuan Yao and Xiaochun Cao", "title": "Efficient Adversarial Attacks for Visual Object Tracking", "comments": null, "journal-ref": "eccv 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual object tracking is an important task that requires the tracker to find\nthe objects quickly and accurately. The existing state-ofthe-art object\ntrackers, i.e., Siamese based trackers, use DNNs to attain high accuracy.\nHowever, the robustness of visual tracking models is seldom explored. In this\npaper, we analyze the weakness of object trackers based on the Siamese network\nand then extend adversarial examples to visual object tracking. We present an\nend-to-end network FAN (Fast Attack Network) that uses a novel drift loss\ncombined with the embedded feature loss to attack the Siamese network based\ntrackers. Under a single GPU, FAN is efficient in the training speed and has a\nstrong attack performance. The FAN can generate an adversarial example at 10ms,\nachieve effective targeted attack (at least 40% drop rate on OTB) and\nuntargeted attack (at least 70% drop rate on OTB).\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 08:47:58 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Liang", "Siyuan", ""], ["Wei", "Xingxing", ""], ["Yao", "Siyuan", ""], ["Cao", "Xiaochun", ""]]}, {"id": "2008.00223", "submitter": "Tuan N.A. Hoang", "authors": "Tuan Hoang and Thanh-Toan Do and Tam V. Nguyen and Ngai-Man Cheung", "title": "Unsupervised Deep Cross-modality Spectral Hashing", "comments": "Accepted to IEEE Transaction on Image Processing (TIP) Add\n  Acknowledgement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel framework, namely Deep Cross-modality Spectral\nHashing (DCSH), to tackle the unsupervised learning problem of binary hash\ncodes for efficient cross-modal retrieval. The framework is a two-step hashing\napproach which decouples the optimization into (1) binary optimization and (2)\nhashing function learning. In the first step, we propose a novel spectral\nembedding-based algorithm to simultaneously learn single-modality and binary\ncross-modality representations. While the former is capable of well preserving\nthe local structure of each modality, the latter reveals the hidden patterns\nfrom all modalities. In the second step, to learn mapping functions from\ninformative data inputs (images and word embeddings) to binary codes obtained\nfrom the first step, we leverage the powerful CNN for images and propose a\nCNN-based deep architecture to learn text modality. Quantitative evaluations on\nthree standard benchmark datasets demonstrate that the proposed DCSH method\nconsistently outperforms other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 09:20:11 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 05:15:00 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2020 09:23:55 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Hoang", "Tuan", ""], ["Do", "Thanh-Toan", ""], ["Nguyen", "Tam V.", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "2008.00226", "submitter": "Regev Cohen", "authors": "Regev Cohen, Michael Elad and Peyman Milanfar", "title": "Regularization by Denoising via Fixed-Point Projection (RED-PRO)", "comments": "33 Pages, 6 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse problems in image processing are typically cast as optimization\ntasks, consisting of data-fidelity and stabilizing regularization terms. A\nrecent regularization strategy of great interest utilizes the power of\ndenoising engines. Two such methods are the Plug-and-Play Prior (PnP) and\nRegularization by Denoising (RED). While both have shown state-of-the-art\nresults in various recovery tasks, their theoretical justification is\nincomplete. In this paper, we aim to bridge between RED and PnP, enriching the\nunderstanding of both frameworks. Towards that end, we reformulate RED as a\nconvex optimization problem utilizing a projection (RED-PRO) onto the\nfixed-point set of demicontractive denoisers. We offer a simple iterative\nsolution to this problem, by which we show that PnP proximal gradient method is\na special case of RED-PRO, while providing guarantees for the convergence of\nboth frameworks to globally optimal solutions. In addition, we present\nrelaxations of RED-PRO that allow for handling denoisers with limited\nfixed-point sets. Finally, we demonstrate RED-PRO for the tasks of image\ndeblurring and super-resolution, showing improved results with respect to the\noriginal RED framework.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 09:35:22 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 20:22:33 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Cohen", "Regev", ""], ["Elad", "Michael", ""], ["Milanfar", "Peyman", ""]]}, {"id": "2008.00230", "submitter": "Tao Zhou", "authors": "Tao Zhou, Deng-Ping Fan, Ming-Ming Cheng, Jianbing Shen, and Ling Shao", "title": "RGB-D Salient Object Detection: A Survey", "comments": "24 pages, 12 figures. Has been accepted by Computational Visual Media", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salient object detection (SOD), which simulates the human visual perception\nsystem to locate the most attractive object(s) in a scene, has been widely\napplied to various computer vision tasks. Now, with the advent of depth\nsensors, depth maps with affluent spatial information that can be beneficial in\nboosting the performance of SOD, can easily be captured. Although various RGB-D\nbased SOD models with promising performance have been proposed over the past\nseveral years, an in-depth understanding of these models and challenges in this\ntopic remains lacking. In this paper, we provide a comprehensive survey of\nRGB-D based SOD models from various perspectives, and review related benchmark\ndatasets in detail. Further, considering that the light field can also provide\ndepth maps, we review SOD models and popular benchmark datasets from this\ndomain as well. Moreover, to investigate the SOD ability of existing models, we\ncarry out a comprehensive evaluation, as well as attribute-based evaluation of\nseveral representative RGB-D based SOD models. Finally, we discuss several\nchallenges and open directions of RGB-D based SOD for future research. All\ncollected models, benchmark datasets, source code links, datasets constructed\nfor attribute-based evaluation, and codes for evaluation will be made publicly\navailable at https://github.com/taozh2017/RGBDSODsurvey\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 10:01:32 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 11:18:49 GMT"}, {"version": "v3", "created": "Sun, 29 Nov 2020 21:08:09 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Zhou", "Tao", ""], ["Fan", "Deng-Ping", ""], ["Cheng", "Ming-Ming", ""], ["Shen", "Jianbing", ""], ["Shao", "Ling", ""]]}, {"id": "2008.00238", "submitter": "Pavan Magesh", "authors": "Pavan Rajkumar Magesh, Richard Delwin Myloth, Rijo Jackson Tom", "title": "An Explainable Machine Learning Model for Early Detection of Parkinson's\n  Disease using LIME on DaTscan Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parkinson's disease (PD) is a degenerative and progressive neurological\ncondition. Early diagnosis can improve treatment for patients and is performed\nthrough dopaminergic imaging techniques like the SPECT DaTscan. In this study,\nwe propose a machine learning model that accurately classifies any given\nDaTscan as having Parkinson's disease or not, in addition to providing a\nplausible reason for the prediction. This is kind of reasoning is done through\nthe use of visual indicators generated using Local Interpretable Model-Agnostic\nExplainer (LIME) methods. DaTscans were drawn from the Parkinson's Progression\nMarkers Initiative database and trained on a CNN (VGG16) using transfer\nlearning, yielding an accuracy of 95.2%, a sensitivity of 97.5%, and a\nspecificity of 90.9%. Keeping model interpretability of paramount importance,\nespecially in the healthcare field, this study utilises LIME explanations to\ndistinguish PD from non-PD, using visual superpixels on the DaTscans. It could\nbe concluded that the proposed system, in union with its measured\ninterpretability and accuracy may effectively aid medical workers in the early\ndiagnosis of Parkinson's Disease.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 10:44:03 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Magesh", "Pavan Rajkumar", ""], ["Myloth", "Richard Delwin", ""], ["Tom", "Rijo Jackson", ""]]}, {"id": "2008.00239", "submitter": "Ruicheng Feng", "authors": "Ruicheng Feng, Weipeng Guan, Yu Qiao, Chao Dong", "title": "Exploring Multi-Scale Feature Propagation and Communication for Image\n  Super Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-scale techniques have achieved great success in a wide range of\ncomputer vision tasks. However, while this technique is incorporated in\nexisting works, there still lacks a comprehensive investigation on variants of\nmulti-scale convolution in image super resolution. In this work, we present a\nunified formulation over widely-used multi-scale structures. With this\nframework, we systematically explore the two factors of multi-scale convolution\n-- feature propagation and cross-scale communication. Based on the\ninvestigation, we propose a generic and efficient multi-scale convolution unit\n-- Multi-Scale cross-Scale Share-weights convolution (MS$^3$-Conv). Extensive\nexperiments demonstrate that the proposed MS$^3$-Conv can achieve better SR\nperformance than the standard convolution with less parameters and\ncomputational cost. Beyond quantitative analysis, we comprehensively study the\nvisual quality, which shows that MS$^3$-Conv behave better to recover\nhigh-frequency details.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 10:44:06 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 08:03:17 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Feng", "Ruicheng", ""], ["Guan", "Weipeng", ""], ["Qiao", "Yu", ""], ["Dong", "Chao", ""]]}, {"id": "2008.00247", "submitter": "Atmadeep Banerjee", "authors": "Atmadeep Banerjee", "title": "Meta-DRN: Meta-Learning for 1-Shot Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern deep learning models have revolutionized the field of computer vision.\nBut, a significant drawback of most of these models is that they require a\nlarge number of labelled examples to generalize properly. Recent developments\nin few-shot learning aim to alleviate this requirement. In this paper, we\npropose a novel lightweight CNN architecture for 1-shot image segmentation. The\nproposed model is created by taking inspiration from well-performing\narchitectures for semantic segmentation and adapting it to the 1-shot domain.\nWe train our model using 4 meta-learning algorithms that have worked well for\nimage classification and compare the results. For the chosen dataset, our\nproposed model has a 70% lower parameter count than the benchmark, while having\nbetter or comparable mean IoU scores using all 4 of the meta-learning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 11:23:37 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Banerjee", "Atmadeep", ""]]}, {"id": "2008.00261", "submitter": "Bingchen Zhao", "authors": "Bingchen Zhao, Xin Wen", "title": "Distilling Visual Priors from Self-Supervised Learning", "comments": "This is the 2nd place tech report for VIPriors Image Classification\n  Challenge ECCVW2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are prone to overfit small training\ndatasets. We present a novel two-phase pipeline that leverages self-supervised\nlearning and knowledge distillation to improve the generalization ability of\nCNN models for image classification under the data-deficient setting. The first\nphase is to learn a teacher model which possesses rich and generalizable visual\nrepresentations via self-supervised learning, and the second phase is to\ndistill the representations into a student model in a self-distillation manner,\nand meanwhile fine-tune the student model for the image classification task. We\nalso propose a novel margin loss for the self-supervised contrastive learning\nproxy task to better learn the representation under the data-deficient\nscenario. Together with other tricks, we achieve competitive performance in the\nVIPriors image classification challenge.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 13:07:18 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Zhao", "Bingchen", ""], ["Wen", "Xin", ""]]}, {"id": "2008.00267", "submitter": "Hieu Le", "authors": "Hieu Le and Dimitris Samaras", "title": "From Shadow Segmentation to Shadow Removal", "comments": "Accepted at ECCV 2020. All code, trained models, and data are\n  available (soon) at: https://www3.cs.stonybrook.\n  edu/~cvl/projects/FSS2SR/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The requirement for paired shadow and shadow-free images limits the size and\ndiversity of shadow removal datasets and hinders the possibility of training\nlarge-scale, robust shadow removal algorithms. We propose a shadow removal\nmethod that can be trained using only shadow and non-shadow patches cropped\nfrom the shadow images themselves. Our method is trained via an adversarial\nframework, following a physical model of shadow formation. Our central\ncontribution is a set of physics-based constraints that enables this\nadversarial training. Our method achieves competitive shadow removal results\ncompared to state-of-the-art methods that are trained with fully paired shadow\nand shadow-free images. The advantages of our training regime are even more\npronounced in shadow removal for videos. Our method can be fine-tuned on a\ntesting video with only the shadow masks generated by a pre-trained shadow\ndetector and outperforms state-of-the-art methods on this challenging test. We\nillustrate the advantages of our method on our proposed video shadow removal\ndataset.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 14:00:10 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Le", "Hieu", ""], ["Samaras", "Dimitris", ""]]}, {"id": "2008.00299", "submitter": "Mohammed Bany Muhammad", "authors": "Mohammed Bany Muhammad, Mohammed Yeasin", "title": "Eigen-CAM: Class Activation Map using Principal Components", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": "10.1109/IJCNN48605.2020.9206626", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are ubiquitous due to the ease of developing models and\ntheir influence on other domains. At the heart of this progress is\nconvolutional neural networks (CNNs) that are capable of learning\nrepresentations or features given a set of data. Making sense of such complex\nmodels (i.e., millions of parameters and hundreds of layers) remains\nchallenging for developers as well as the end-users. This is partially due to\nthe lack of tools or interfaces capable of providing interpretability and\ntransparency. A growing body of literature, for example, class activation map\n(CAM), focuses on making sense of what a model learns from the data or why it\nbehaves poorly in a given task. This paper builds on previous ideas to cope\nwith the increasing demand for interpretable, robust, and transparent models.\nOur approach provides a simpler and intuitive (or familiar) way of generating\nCAM. The proposed Eigen-CAM computes and visualizes the principle components of\nthe learned features/representations from the convolutional layers. Empirical\nstudies were performed to compare the Eigen-CAM with the state-of-the-art\nmethods (such as Grad-CAM, Grad-CAM++, CNN-fixations) by evaluating on\nbenchmark datasets such as weakly-supervised localization and localizing\nobjects in the presence of adversarial noise. Eigen-CAM was found to be robust\nagainst classification errors made by fully connected layers in CNNs, does not\nrely on the backpropagation of gradients, class relevance score, maximum\nactivation locations, or any other form of weighting features. In addition, it\nworks with all CNN models without the need to modify layers or retrain models.\nEmpirical results show up to 12% improvement over the best method among the\nmethods compared on weakly supervised object localization.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 17:14:13 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Muhammad", "Mohammed Bany", ""], ["Yeasin", "Mohammed", ""]]}, {"id": "2008.00302", "submitter": "Radu Tudor Ionescu", "authors": "Mihail Burduja, Radu Tudor Ionescu and Nicolae Verga", "title": "Accurate and Efficient Intracranial Hemorrhage Detection and Subtype\n  Classification in 3D CT Scans with Convolutional and Long Short-Term Memory\n  Neural Networks", "comments": "Accepted at Sensors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present our system for the RSNA Intracranial Hemorrhage\nDetection challenge. The proposed system is based on a lightweight deep neural\nnetwork architecture composed of a convolutional neural network (CNN) that\ntakes as input individual CT slices, and a Long Short-Term Memory (LSTM)\nnetwork that takes as input feature embeddings provided by the CNN. For\nefficient processing, we consider various feature selection methods to produce\na subset of useful CNN features for the LSTM. Furthermore, we reduce the CT\nslices by a factor of 2x, allowing ourselves to train the model faster. Even if\nour model is designed to balance speed and accuracy, we report a weighted mean\nlog loss of 0.04989 on the final test set, which places us in the top 30\nranking (2%) from a total of 1345 participants. Although our computing\ninfrastructure does not allow it, processing CT slices at their original scale\nis likely to improve performance. In order to enable others to reproduce our\nresults, we provide our code as open source at\nhttps://github.com/warchildmd/ihd. After the challenge, we conducted a\nsubjective intracranial hemorrhage detection assessment by radiologists,\nindicating that the performance of our deep model is on par with that of\ndoctors specialized in reading CT scans. Another contribution of our work is to\nintegrate Grad-CAM visualizations in our system, providing useful explanations\nfor its predictions. We therefore consider our system as a viable option when a\nfast diagnosis or a second opinion on intracranial hemorrhage detection are\nneeded.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 17:28:25 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 08:05:21 GMT"}, {"version": "v3", "created": "Tue, 29 Sep 2020 14:55:07 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Burduja", "Mihail", ""], ["Ionescu", "Radu Tudor", ""], ["Verga", "Nicolae", ""]]}, {"id": "2008.00305", "submitter": "Omid Poursaeed", "authors": "Omid Poursaeed, Tianxing Jiang, Han Qiao, Nayun Xu, Vladimir G. Kim", "title": "Self-supervised Learning of Point Clouds via Orientation Estimation", "comments": "3DV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds provide a compact and efficient representation of 3D shapes.\nWhile deep neural networks have achieved impressive results on point cloud\nlearning tasks, they require massive amounts of manually labeled data, which\ncan be costly and time-consuming to collect. In this paper, we leverage 3D\nself-supervision for learning downstream tasks on point clouds with fewer\nlabels. A point cloud can be rotated in infinitely many ways, which provides a\nrich label-free source for self-supervision. We consider the auxiliary task of\npredicting rotations that in turn leads to useful features for other tasks such\nas shape classification and 3D keypoint prediction. Using experiments on\nShapeNet and ModelNet, we demonstrate that our approach outperforms the\nstate-of-the-art. Moreover, features learned by our model are complementary to\nother self-supervised methods and combining them leads to further performance\nimprovement.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 17:49:45 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 01:46:11 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Poursaeed", "Omid", ""], ["Jiang", "Tianxing", ""], ["Qiao", "Han", ""], ["Xu", "Nayun", ""], ["Kim", "Vladimir G.", ""]]}, {"id": "2008.00324", "submitter": "Zeshi Yang", "authors": "Zeshi Yang and Kangkang Yin", "title": "Improving Skeleton-based Action Recognitionwith Robust Spatial and\n  Temporal Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently skeleton-based action recognition has made signif-icant progresses\nin the computer vision community. Most state-of-the-art algorithms are based on\nGraph Convolutional Networks (GCN), andtarget at improving the network\nstructure of the backbone GCN lay-ers. In this paper, we propose a novel\nmechanism to learn more robustdiscriminative features in space and time. More\nspecifically, we add aDiscriminative Feature Learning (DFL) branch to the last\nlayers of thenetwork to extract discriminative spatial and temporal features to\nhelpregularize the learning. We also formally advocate the use of\nDirection-Invariant Features (DIF) as input to the neural networks. We show\nthataction recognition accuracy can be improved when these robust featuresare\nlearned and used. We compare our results with those of ST-GCNand related\nmethods on four datasets: NTU-RGBD60, NTU-RGBD120,SYSU 3DHOI and\nSkeleton-Kinetics.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 19:29:53 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Yang", "Zeshi", ""], ["Yin", "Kangkang", ""]]}, {"id": "2008.00326", "submitter": "Aditya Agarwal", "authors": "Aditya Agarwal, Yupeng Han, Maxim Likhachev", "title": "PERCH 2.0 : Fast and Accurate GPU-based Perception via Search for Object\n  Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pose estimation of known objects is fundamental to tasks such as robotic\ngrasping and manipulation. The need for reliable grasping imposes stringent\naccuracy requirements on pose estimation in cluttered, occluded scenes in\ndynamic environments. Modern methods employ large sets of training data to\nlearn features in order to find correspondence between 3D models and observed\ndata. However these methods require extensive annotation of ground truth poses.\nAn alternative is to use algorithms that search for the best explanation of the\nobserved scene in a space of possible rendered scenes. A recently developed\nalgorithm, PERCH (PErception Via SeaRCH) does so by using depth data to\nconverge to a globally optimum solution using a search over a specially\nconstructed tree. While PERCH offers strong guarantees on accuracy, the current\nformulation suffers from low scalability owing to its high runtime. In\naddition, the sole reliance on depth data for pose estimation restricts the\nalgorithm to scenes where no two objects have the same shape. In this work, we\npropose PERCH 2.0, a novel perception via search strategy that takes advantage\nof GPU acceleration and RGB data. We show that our approach can achieve a\nspeedup of 100x over PERCH, as well as better accuracy than the\nstate-of-the-art data-driven approaches on 6-DoF pose estimation without the\nneed for annotating ground truth poses in the training data. Our code and video\nare available at https://sbpl-cruz.github.io/perception/.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 19:42:56 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Agarwal", "Aditya", ""], ["Han", "Yupeng", ""], ["Likhachev", "Maxim", ""]]}, {"id": "2008.00334", "submitter": "Wentao Bao", "authors": "Wentao Bao and Qi Yu and Yu Kong", "title": "Uncertainty-based Traffic Accident Anticipation with Spatio-Temporal\n  Relational Learning", "comments": "Accepted by ACM MM 2020", "journal-ref": null, "doi": "10.1145/3394171.3413827", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic accident anticipation aims to predict accidents from dashcam videos\nas early as possible, which is critical to safety-guaranteed self-driving\nsystems. With cluttered traffic scenes and limited visual cues, it is of great\nchallenge to predict how long there will be an accident from early observed\nframes. Most existing approaches are developed to learn features of\naccident-relevant agents for accident anticipation, while ignoring the features\nof their spatial and temporal relations. Besides, current deterministic deep\nneural networks could be overconfident in false predictions, leading to high\nrisk of traffic accidents caused by self-driving systems. In this paper, we\npropose an uncertainty-based accident anticipation model with spatio-temporal\nrelational learning. It sequentially predicts the probability of traffic\naccident occurrence with dashcam videos. Specifically, we propose to take\nadvantage of graph convolution and recurrent networks for relational feature\nlearning, and leverage Bayesian neural networks to address the intrinsic\nvariability of latent relational representations. The derived uncertainty-based\nranking loss is found to significantly boost model performance by improving the\nquality of relational features. In addition, we collect a new Car Crash Dataset\n(CCD) for traffic accident anticipation which contains environmental attributes\nand accident reasons annotations. Experimental results on both public and the\nnewly-compiled datasets show state-of-the-art performance of our model. Our\ncode and CCD dataset are available at https://github.com/Cogito2012/UString.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 20:21:48 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Bao", "Wentao", ""], ["Yu", "Qi", ""], ["Kong", "Yu", ""]]}, {"id": "2008.00348", "submitter": "Donghyun Kim", "authors": "Donghyun Kim, Kuniaki Saito, Kate Saenko, Stan Sclaroff, Bryan A\n  Plummer", "title": "Self-supervised Visual Attribute Learning for Fashion Compatibility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many self-supervised learning (SSL) methods have been successful in learning\nsemantically meaningful visual representations by solving pretext tasks.\nHowever, state-of-the-art SSL methods focus on object recognition or detection\ntasks, which aim to learn object shapes, but ignore visual attributes such as\ncolor and texture via color distortion augmentation. However, learning these\nvisual attributes could be more important than learning object shapes for other\nvision tasks, such as fashion compatibility. To address this deficiency, we\npropose Self-supervised Tasks for Outfit Compatibility (STOC) without any\nsupervision. Specifically, STOC aims to learn colors and textures of fashion\nitems and embed similar items nearby. STOC outperforms state-of-the-art SSL by\n9.5% and a supervised Siamese Network by 3% on a fill-in-the-blank outfit\ncompletion task on our unsupervised benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 21:53:22 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Kim", "Donghyun", ""], ["Saito", "Kuniaki", ""], ["Saenko", "Kate", ""], ["Sclaroff", "Stan", ""], ["Plummer", "Bryan A", ""]]}, {"id": "2008.00362", "submitter": "Zili Yi", "authors": "Zili Yi, Qiang Tang, Vishnu Sanjay Ramiya Srinivasan, Zhan Xu", "title": "Animating Through Warping: an Efficient Method for High-Quality Facial\n  Expression Animation", "comments": "18 pages, 13 figures, Accepted to ACM Multimedia 2020", "journal-ref": null, "doi": "10.1145/3394171.3413926", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in deep neural networks have considerably improved the art of\nanimating a still image without operating in 3D domain. Whereas, prior arts can\nonly animate small images (typically no larger than 512x512) due to memory\nlimitations, difficulty of training and lack of high-resolution (HD) training\ndatasets, which significantly reduce their potential for applications in movie\nproduction and interactive systems. Motivated by the idea that HD images can be\ngenerated by adding high-frequency residuals to low-resolution results produced\nby a neural network, we propose a novel framework known as Animating Through\nWarping (ATW) to enable efficient animation of HD images.\n  Specifically, the proposed framework consists of two modules, a novel\ntwo-stage neural-network generator and a novel post-processing module known as\nAnimating Through Warping (ATW). It only requires the generator to be trained\non small images and can do inference on an image of any size. During inference,\nan HD input image is decomposed into a low-resolution component(128x128) and\nits corresponding high-frequency residuals. The generator predicts the\nlow-resolution result as well as the motion field that warps the input face to\nthe desired status (e.g., expressions categories or action units). Finally, the\nResWarp module warps the residuals based on the motion field and adding the\nwarped residuals to generates the final HD results from the naively up-sampled\nlow-resolution results. Experiments show the effectiveness and efficiency of\nour method in generating high-resolution animations. Our proposed framework\nsuccessfully animates a 4K facial image, which has never been achieved by prior\nneural models. In addition, our method generally guarantee the temporal\ncoherency of the generated animations. Source codes will be made publicly\navailable.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 23:52:33 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Yi", "Zili", ""], ["Tang", "Qiang", ""], ["Srinivasan", "Vishnu Sanjay Ramiya", ""], ["Xu", "Zhan", ""]]}, {"id": "2008.00363", "submitter": "Satyananda Kashyap", "authors": "Satyananda Kashyap, Alexandros Karargyris, Joy Wu, Yaniv Gur, Arjun\n  Sharma, Ken C. L. Wong, Mehdi Moradi, Tanveer Syeda-Mahmood", "title": "Looking in the Right place for Anomalies: Explainable AI through\n  Automatic Location Learning", "comments": "5 pages, Paper presented as a poster at the International Symposium\n  on Biomedical Imaging, 2020, Paper Number 655", "journal-ref": "2020 IEEE 17th International Symposium on Biomedical Imaging\n  (ISBI)", "doi": "10.1109/ISBI45749.2020.9098370", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has now become the de facto approach to the recognition of\nanomalies in medical imaging. Their 'black box' way of classifying medical\nimages into anomaly labels poses problems for their acceptance, particularly\nwith clinicians. Current explainable AI methods offer justifications through\nvisualizations such as heat maps but cannot guarantee that the network is\nfocusing on the relevant image region fully containing the anomaly. In this\npaper, we develop an approach to explainable AI in which the anomaly is assured\nto be overlapping the expected location when present. This is made possible by\nautomatically extracting location-specific labels from textual reports and\nlearning the association of expected locations to labels using a hybrid\ncombination of Bi-Directional Long Short-Term Memory Recurrent Neural Networks\n(Bi-LSTM) and DenseNet-121. Use of this expected location to bias the\nsubsequent attention-guided inference network based on ResNet101 results in the\nisolation of the anomaly at the expected location when present. The method is\nevaluated on a large chest X-ray dataset.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 00:02:37 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Kashyap", "Satyananda", ""], ["Karargyris", "Alexandros", ""], ["Wu", "Joy", ""], ["Gur", "Yaniv", ""], ["Sharma", "Arjun", ""], ["Wong", "Ken C. L.", ""], ["Moradi", "Mehdi", ""], ["Syeda-Mahmood", "Tanveer", ""]]}, {"id": "2008.00380", "submitter": "Sharmin Majumder", "authors": "Sharmin Majumder, Nasser Kehtarnavaz", "title": "Vision and Inertial Sensing Fusion for Human Action Recognition : A\n  Review", "comments": "14 pages,4 figures,2 tables. Submitted to IEEE Sensors Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human action recognition is used in many applications such as video\nsurveillance, human computer interaction, assistive living, and gaming. Many\npapers have appeared in the literature showing that the fusion of vision and\ninertial sensing improves recognition accuracies compared to the situations\nwhen each sensing modality is used individually. This paper provides a survey\nof the papers in which vision and inertial sensing are used simultaneously\nwithin a fusion framework in order to perform human action recognition. The\nsurveyed papers are categorized in terms of fusion approaches, features,\nclassifiers, as well as multimodality datasets considered. Challenges as well\nas possible future directions are also stated for deploying the fusion of these\ntwo sensing modalities under realistic conditions.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 02:06:44 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Majumder", "Sharmin", ""], ["Kehtarnavaz", "Nasser", ""]]}, {"id": "2008.00394", "submitter": "Xiaogang Wang", "authors": "Xiaogang Wang, Marcelo H Ang Jr and Gim Hee Lee", "title": "Point Cloud Completion by Learning Shape Priors", "comments": "IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In view of the difficulty in reconstructing object details in point cloud\ncompletion, we propose a shape prior learning method for object completion. The\nshape priors include geometric information in both complete and the partial\npoint clouds. We design a feature alignment strategy to learn the shape prior\nfrom complete points, and a coarse to fine strategy to incorporate partial\nprior in the fine stage. To learn the complete objects prior, we first train a\npoint cloud auto-encoder to extract the latent embeddings from complete points.\nThen we learn a mapping to transfer the point features from partial points to\nthat of the complete points by optimizing feature alignment losses. The feature\nalignment losses consist of a L2 distance and an adversarial loss obtained by\nMaximum Mean Discrepancy Generative Adversarial Network (MMD-GAN). The L2\ndistance optimizes the partial features towards the complete ones in the\nfeature space, and MMD-GAN decreases the statistical distance of two point\nfeatures in a Reproducing Kernel Hilbert Space. We achieve state-of-the-art\nperformances on the point cloud completion task. Our code is available at\nhttps://github.com/xiaogangw/point-cloud-completion-shape-prior.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 04:00:32 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 08:07:03 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Wang", "Xiaogang", ""], ["Ang", "Marcelo H", "Jr"], ["Lee", "Gim Hee", ""]]}, {"id": "2008.00397", "submitter": "Liu Yang", "authors": "Liu Yang, Fanqi Meng, Ming-Kuang Daniel Wu, Vicent Ying, Xianchao Xu", "title": "SeqDialN: Sequential Visual Dialog Networks in Joint Visual-Linguistic\n  Representation Space", "comments": "18 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we formulate a visual dialog as an information flow in which\neach piece of information is encoded with the joint visual-linguistic\nrepresentation of a single dialog round. Based on this formulation, we consider\nthe visual dialog task as a sequence problem consisting of ordered\nvisual-linguistic vectors. For featurization, we use a Dense Symmetric\nCo-Attention network as a lightweight vison-language joint representation\ngenerator to fuse multimodal features (i.e., image and text), yielding better\ncomputation and data efficiencies. For inference, we propose two Sequential\nDialog Networks (SeqDialN): the first uses LSTM for information propagation\n(IP) and the second uses a modified Transformer for multi-step reasoning (MR).\nOur architecture separates the complexity of multimodal feature fusion from\nthat of inference, which allows simpler design of the inference engine. IP\nbased SeqDialN is our baseline with a simple 2-layer LSTM design that achieves\ndecent performance. MR based SeqDialN, on the other hand, recurrently refines\nthe semantic question/history representations through the self-attention stack\nof Transformer and produces promising results on the visual dialog task. On\nVisDial v1.0 test-std dataset, our best single generative SeqDialN achieves\n62.54% NDCG and 48.63% MRR; our ensemble generative SeqDialN achieves 63.78%\nNDCG and 49.98% MRR, which set a new state-of-the-art generative visual dialog\nmodel. We fine-tune discriminative SeqDialN with dense annotations and boost\nthe performance up to 72.41% NDCG and 55.11% MRR. In this work, we discuss the\nextensive experiments we have conducted to demonstrate the effectiveness of our\nmodel components. We also provide visualization for the reasoning process from\nthe relevant conversation rounds and discuss our fine-tuning methods. Our code\nis available at https://github.com/xiaoxiaoheimei/SeqDialN\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 04:57:54 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Yang", "Liu", ""], ["Meng", "Fanqi", ""], ["Wu", "Ming-Kuang Daniel", ""], ["Ying", "Vicent", ""], ["Xu", "Xianchao", ""]]}, {"id": "2008.00407", "submitter": "Xuankai Liu", "authors": "Xuankai Liu, Fengting Li, Bihan Wen, Qi Li", "title": "Removing Backdoor-Based Watermarks in Neural Networks with Limited Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been widely applied and achieved great success in\nvarious fields. As training deep models usually consumes massive data and\ncomputational resources, trading the trained deep models is highly demanded and\nlucrative nowadays. Unfortunately, the naive trading schemes typically involves\npotential risks related to copyright and trustworthiness issues, e.g., a sold\nmodel can be illegally resold to others without further authorization to reap\nhuge profits. To tackle this problem, various watermarking techniques are\nproposed to protect the model intellectual property, amongst which the\nbackdoor-based watermarking is the most commonly-used one. However, the\nrobustness of these watermarking approaches is not well evaluated under\nrealistic settings, such as limited in-distribution data availability and\nagnostic of watermarking patterns. In this paper, we benchmark the robustness\nof watermarking, and propose a novel backdoor-based watermark removal framework\nusing limited data, dubbed WILD. The proposed WILD removes the watermarks of\ndeep models with only a small portion of training data, and the output model\ncan perform the same as models trained from scratch without watermarks\ninjected. In particular, a novel data augmentation method is utilized to mimic\nthe behavior of watermark triggers. Combining with the distribution alignment\nbetween the normal and perturbed (e.g., occluded) data in the feature space,\nour approach generalizes well on all typical types of trigger contents. The\nexperimental results demonstrate that our approach can effectively remove the\nwatermarks without compromising the deep model performance for the original\ntask with the limited access to training data.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 06:25:26 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 03:31:14 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Liu", "Xuankai", ""], ["Li", "Fengting", ""], ["Wen", "Bihan", ""], ["Li", "Qi", ""]]}, {"id": "2008.00418", "submitter": "Xiaoming Li", "authors": "Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo,\n  Lei Zhang", "title": "Blind Face Restoration via Deep Multi-scale Component Dictionaries", "comments": "In ECCV 2020. Code is available at:\n  https://github.com/csxmli2016/DFDNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent reference-based face restoration methods have received considerable\nattention due to their great capability in recovering high-frequency details on\nreal low-quality images. However, most of these methods require a high-quality\nreference image of the same identity, making them only applicable in limited\nscenes. To address this issue, this paper suggests a deep face dictionary\nnetwork (termed as DFDNet) to guide the restoration process of degraded\nobservations. To begin with, we use K-means to generate deep dictionaries for\nperceptually significant face components (\\ie, left/right eyes, nose and mouth)\nfrom high-quality images. Next, with the degraded input, we match and select\nthe most similar component features from their corresponding dictionaries and\ntransfer the high-quality details to the input via the proposed dictionary\nfeature transfer (DFT) block. In particular, component AdaIN is leveraged to\neliminate the style diversity between the input and dictionary features (\\eg,\nillumination), and a confidence score is proposed to adaptively fuse the\ndictionary feature to the input. Finally, multi-scale dictionaries are adopted\nin a progressive manner to enable the coarse-to-fine restoration. Experiments\nshow that our proposed method can achieve plausible performance in both\nquantitative and qualitative evaluation, and more importantly, can generate\nrealistic and promising results on real degraded images without requiring an\nidentity-belonging reference. The source code and models are available at\n\\url{https://github.com/csxmli2016/DFDNet}.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 07:02:07 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Li", "Xiaoming", ""], ["Chen", "Chaofeng", ""], ["Zhou", "Shangchen", ""], ["Lin", "Xianhui", ""], ["Zuo", "Wangmeng", ""], ["Zhang", "Lei", ""]]}, {"id": "2008.00446", "submitter": "Lei Zhou", "authors": "Lei Zhou, Zixin Luo, Mingmin Zhen, Tianwei Shen, Shiwei Li, Zhuofei\n  Huang, Tian Fang, Long Quan", "title": "Stochastic Bundle Adjustment for Efficient and Scalable 3D\n  Reconstruction", "comments": "Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current bundle adjustment solvers such as the Levenberg-Marquardt (LM)\nalgorithm are limited by the bottleneck in solving the Reduced Camera System\n(RCS) whose dimension is proportional to the camera number. When the problem is\nscaled up, this step is neither efficient in computation nor manageable for a\nsingle compute node. In this work, we propose a stochastic bundle adjustment\nalgorithm which seeks to decompose the RCS approximately inside the LM\niterations to improve the efficiency and scalability. It first reformulates the\nquadratic programming problem of an LM iteration based on the clustering of the\nvisibility graph by introducing the equality constraints across clusters. Then,\nwe propose to relax it into a chance constrained problem and solve it through\nsampled convex program. The relaxation is intended to eliminate the\ninterdependence between clusters embodied by the constraints, so that a large\nRCS can be decomposed into independent linear sub-problems. Numerical\nexperiments on unordered Internet image sets and sequential SLAM image sets, as\nwell as distributed experiments on large-scale datasets, have demonstrated the\nhigh efficiency and scalability of the proposed approach. Codes are released at\nhttps://github.com/zlthinker/STBA.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 10:26:09 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Zhou", "Lei", ""], ["Luo", "Zixin", ""], ["Zhen", "Mingmin", ""], ["Shen", "Tianwei", ""], ["Li", "Shiwei", ""], ["Huang", "Zhuofei", ""], ["Fang", "Tian", ""], ["Quan", "Long", ""]]}, {"id": "2008.00455", "submitter": "Takashi Isobe", "authors": "Takashi Isobe, Xu Jia, Shuhang Gu, Songjiang Li, Shengjin Wang, Qi\n  Tian", "title": "Video Super-Resolution with Recurrent Structure-Detail Network", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most video super-resolution methods super-resolve a single reference frame\nwith the help of neighboring frames in a temporal sliding window. They are less\nefficient compared to the recurrent-based methods. In this work, we propose a\nnovel recurrent video super-resolution method which is both effective and\nefficient in exploiting previous frames to super-resolve the current frame. It\ndivides the input into structure and detail components which are fed to a\nrecurrent unit composed of several proposed two-stream structure-detail blocks.\nIn addition, a hidden state adaptation module that allows the current frame to\nselectively use information from hidden state is introduced to enhance its\nrobustness to appearance change and error accumulation. Extensive ablation\nstudy validate the effectiveness of the proposed modules. Experiments on\nseveral benchmark datasets demonstrate the superior performance of the proposed\nmethod compared to state-of-the-art methods on video super-resolution.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 11:01:19 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Isobe", "Takashi", ""], ["Jia", "Xu", ""], ["Gu", "Shuhang", ""], ["Li", "Songjiang", ""], ["Wang", "Shengjin", ""], ["Tian", "Qi", ""]]}, {"id": "2008.00456", "submitter": "Iman Nematollahi", "authors": "Iman Nematollahi and Oier Mees and Lukas Hermann and Wolfram Burgard", "title": "Hindsight for Foresight: Unsupervised Structured Dynamics Models from\n  Physical Interaction", "comments": "Accepted at the 2020 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge for an agent learning to interact with the world is to reason\nabout physical properties of objects and to foresee their dynamics under the\neffect of applied forces. In order to scale learning through interaction to\nmany objects and scenes, robots should be able to improve their own performance\nfrom real-world experience without requiring human supervision. To this end, we\npropose a novel approach for modeling the dynamics of a robot's interactions\ndirectly from unlabeled 3D point clouds and images. Unlike previous approaches,\nour method does not require ground-truth data associations provided by a\ntracker or any pre-trained perception network. To learn from unlabeled\nreal-world interaction data, we enforce consistency of estimated 3D clouds,\nactions and 2D images with observed ones. Our joint forward and inverse network\nlearns to segment a scene into salient object parts and predicts their 3D\nmotion under the effect of applied actions. Moreover, our object-centric model\noutputs action-conditioned 3D scene flow, object masks and 2D optical flow as\nemergent properties. Our extensive evaluation both in simulation and with\nreal-world data demonstrates that our formulation leads to effective,\ninterpretable models that can be used for visuomotor control and planning.\nVideos, code and dataset are available at http://hind4sight.cs.uni-freiburg.de\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 11:04:49 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Nematollahi", "Iman", ""], ["Mees", "Oier", ""], ["Hermann", "Lukas", ""], ["Burgard", "Wolfram", ""]]}, {"id": "2008.00460", "submitter": "Wenchao Zhang", "authors": "Wenchao Zhang, Chong Fu, Mai Zhu", "title": "Joint Object Contour Points and Semantics for Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The attributes of object contours has great significance for instance\nsegmentation task. However, most of the current popular deep neural networks do\nnot pay much attention to the object edge information. Inspired by the human\nannotation process when making instance segmentation datasets, in this paper,\nwe propose Mask Point R-CNN aiming at promoting the neural network's attention\nto the object boundary. Specifically, we innovatively extend the original human\nkeypoint detection task to the contour point detection of any object. Based on\nthis analogy, we present an contour point detection auxiliary task to Mask\nR-CNN, which can boost the gradient flow between different tasks by effectively\nusing feature fusion strategies and multi-task joint training. As a\nconsequence, the model will be more sensitive to the edges of the object and\ncan capture more geometric features. Quantitatively, the experimental results\nshow that our approach outperforms vanilla Mask R-CNN by 3.8\\% on Cityscapes\ndataset and 0.8\\% on COCO dataset.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 11:11:28 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 04:02:09 GMT"}, {"version": "v3", "created": "Sun, 4 Jul 2021 02:21:19 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Zhang", "Wenchao", ""], ["Fu", "Chong", ""], ["Zhu", "Mai", ""]]}, {"id": "2008.00485", "submitter": "Yifei Shi", "authors": "Yifei Shi, Junwen Huang, Hongjia Zhang, Xin Xu, Szymon Rusinkiewicz,\n  Kai Xu", "title": "SymmetryNet: Learning to Predict Reflectional and Rotational Symmetries\n  of 3D Shapes from Single-View RGB-D Images", "comments": "15 pages", "journal-ref": "ACM Transactions on Graphics (Proceeding of SIGGRAPH Asia), 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of symmetry detection of 3D shapes from single-view\nRGB-D images, where severely missing data renders geometric detection approach\ninfeasible. We propose an end-to-end deep neural network which is able to\npredict both reflectional and rotational symmetries of 3D objects present in\nthe input RGB-D image. Directly training a deep model for symmetry prediction,\nhowever, can quickly run into the issue of overfitting. We adopt a multi-task\nlearning approach. Aside from symmetry axis prediction, our network is also\ntrained to predict symmetry correspondences. In particular, given the 3D points\npresent in the RGB-D image, our network outputs for each 3D point its symmetric\ncounterpart corresponding to a specific predicted symmetry. In addition, our\nnetwork is able to detect for a given shape multiple symmetries of different\ntypes. We also contribute a benchmark of 3D symmetry detection based on\nsingle-view RGB-D images. Extensive evaluation on the benchmark demonstrates\nthe strong generalization ability of our method, in terms of high accuracy of\nboth symmetry axis prediction and counterpart estimation. In particular, our\nmethod is robust in handling unseen object instances with large variation in\nshape, multi-symmetry composition, as well as novel object categories.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 14:10:09 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 07:54:41 GMT"}, {"version": "v3", "created": "Sun, 30 Aug 2020 23:56:39 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Shi", "Yifei", ""], ["Huang", "Junwen", ""], ["Zhang", "Hongjia", ""], ["Xu", "Xin", ""], ["Rusinkiewicz", "Szymon", ""], ["Xu", "Kai", ""]]}, {"id": "2008.00490", "submitter": "Xinge Zhu", "authors": "Wanli Chen, Xinge Zhu, Ruoqi Sun, Junjun He, Ruiyu Li, Xiaoyong Shen,\n  and Bei Yu", "title": "Tensor Low-Rank Reconstruction for Semantic Segmentation", "comments": "ECCV2020. Top-1 performance on PASCAL-VOC12; Source code at\n  https://github.com/CWanli/RecoNet.git", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Context information plays an indispensable role in the success of semantic\nsegmentation. Recently, non-local self-attention based methods are proved to be\neffective for context information collection. Since the desired context\nconsists of spatial-wise and channel-wise attentions, 3D representation is an\nappropriate formulation. However, these non-local methods describe 3D context\ninformation based on a 2D similarity matrix, where space compression may lead\nto channel-wise attention missing. An alternative is to model the contextual\ninformation directly without compression. However, this effort confronts a\nfundamental difficulty, namely the high-rank property of context information.\nIn this paper, we propose a new approach to model the 3D context\nrepresentations, which not only avoids the space compression but also tackles\nthe high-rank difficulty. Here, inspired by tensor canonical-polyadic\ndecomposition theory (i.e, a high-rank tensor can be expressed as a combination\nof rank-1 tensors.), we design a low-rank-to-high-rank context reconstruction\nframework (i.e, RecoNet). Specifically, we first introduce the tensor\ngeneration module (TGM), which generates a number of rank-1 tensors to capture\nfragments of context feature. Then we use these rank-1 tensors to recover the\nhigh-rank context features through our proposed tensor reconstruction module\n(TRM). Extensive experiments show that our method achieves state-of-the-art on\nvarious public datasets. Additionally, our proposed method has more than 100\ntimes less computational cost compared with conventional non-local-based\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 14:29:28 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Chen", "Wanli", ""], ["Zhu", "Xinge", ""], ["Sun", "Ruoqi", ""], ["He", "Junjun", ""], ["Li", "Ruiyu", ""], ["Shen", "Xiaoyong", ""], ["Yu", "Bei", ""]]}, {"id": "2008.00498", "submitter": "Zhicheng Cao", "authors": "Zhicheng Cao, Xi Cen and Liaojun Pang", "title": "HyperFaceNet: A Hyperspectral Face Recognition Method Based on Deep\n  Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition has already been well studied under the visible light and\nthe infrared,in both intra-spectral and cross-spectral cases. However, how to\nfuse different light bands, i.e., hyperspectral face recognition, is still an\nopen research problem, which has the advantages of richer information retaining\nand all-weather functionality over single band face recognition. Among the very\nfew works for hyperspectral face recognition, traditional non-deep learning\ntechniques are largely used. Thus, we in this paper bring deep learning into\nthe topic of hyperspectral face recognition, and propose a new fusion model\n(termed HyperFaceNet) especially for hyperspectral faces. The proposed fusion\nmodel is characterized by residual dense learning, a feedback style encoder and\na recognition-oriented loss function. During the experiments, our method is\nproved to be of higher recognition rates than face recognition using either\nvisible light or the infrared. Moreover, our fusion model is shown to be\nsuperior to other general-purposed image fusion methods including\nstate-of-the-arts, in terms of both image quality and recognition performance.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 14:59:24 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 09:46:33 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Cao", "Zhicheng", ""], ["Cen", "Xi", ""], ["Pang", "Liaojun", ""]]}, {"id": "2008.00499", "submitter": "Jianyi Wang", "authors": "Jianyi Wang, Xin Deng, Mai Xu, Congyong Chen, Yuhang Song", "title": "Multi-level Wavelet-based Generative Adversarial Network for Perceptual\n  Quality Enhancement of Compressed Video", "comments": null, "journal-ref": "16th European conference on computer vision. 2020 Aug 23", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past few years have witnessed fast development in video quality\nenhancement via deep learning. Existing methods mainly focus on enhancing the\nobjective quality of compressed video while ignoring its perceptual quality. In\nthis paper, we focus on enhancing the perceptual quality of compressed video.\nOur main observation is that enhancing the perceptual quality mostly relies on\nrecovering high-frequency sub-bands in wavelet domain. Accordingly, we propose\na novel generative adversarial network (GAN) based on multi-level wavelet\npacket transform (WPT) to enhance the perceptual quality of compressed video,\nwhich is called multi-level wavelet-based GAN (MW-GAN). In MW-GAN, we first\napply motion compensation with a pyramid architecture to obtain temporal\ninformation. Then, we propose a wavelet reconstruction network with\nwavelet-dense residual blocks (WDRB) to recover the high-frequency details. In\naddition, the adversarial loss of MW-GAN is added via WPT to further encourage\nhigh-frequency details recovery for video frames. Experimental results\ndemonstrate the superiority of our method.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 15:01:38 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Wang", "Jianyi", ""], ["Deng", "Xin", ""], ["Xu", "Mai", ""], ["Chen", "Congyong", ""], ["Song", "Yuhang", ""]]}, {"id": "2008.00506", "submitter": "Yushuo Guan", "authors": "Yushuo Guan, Pengyu Zhao, Bingxuan Wang, Yuanxing Zhang, Cong Yao,\n  Kaigui Bian, Jian Tang", "title": "Differentiable Feature Aggregation Search for Knowledge Distillation", "comments": "A feature distillation method via differentiable architecture search", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation has become increasingly important in model\ncompression. It boosts the performance of a miniaturized student network with\nthe supervision of the output distribution and feature maps from a\nsophisticated teacher network. Some recent works introduce multi-teacher\ndistillation to provide more supervision to the student network. However, the\neffectiveness of multi-teacher distillation methods are accompanied by costly\ncomputation resources. To tackle with both the efficiency and the effectiveness\nof knowledge distillation, we introduce the feature aggregation to imitate the\nmulti-teacher distillation in the single-teacher distillation framework by\nextracting informative supervision from multiple teacher feature maps.\nSpecifically, we introduce DFA, a two-stage Differentiable Feature Aggregation\nsearch method that motivated by DARTS in neural architecture search, to\nefficiently find the aggregations. In the first stage, DFA formulates the\nsearching problem as a bi-level optimization and leverages a novel bridge loss,\nwhich consists of a student-to-teacher path and a teacher-to-student path, to\nfind appropriate feature aggregations. The two paths act as two players against\neach other, trying to optimize the unified architecture parameters to the\nopposite directions while guaranteeing both expressivity and learnability of\nthe feature aggregation simultaneously. In the second stage, DFA performs\nknowledge distillation with the derived feature aggregation. Experimental\nresults show that DFA outperforms existing methods on CIFAR-100 and CINIC-10\ndatasets under various teacher-student settings, verifying the effectiveness\nand robustness of the design.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 15:42:29 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Guan", "Yushuo", ""], ["Zhao", "Pengyu", ""], ["Wang", "Bingxuan", ""], ["Zhang", "Yuanxing", ""], ["Yao", "Cong", ""], ["Bian", "Kaigui", ""], ["Tang", "Jian", ""]]}, {"id": "2008.00512", "submitter": "Manu Tom", "authors": "Manu Tom, Melanie Suetterlin, Damien Bouffard, Mathias Rothermel,\n  Stefan Wunderle, Emmanuel Baltsavias", "title": "Integrated monitoring of ice in selected Swiss lakes. Final project\n  report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various lake observables, including lake ice, are related to climate and\nclimate change and provide a good opportunity for long-term monitoring. Lakes\n(and as part of them lake ice) is therefore considered an Essential Climate\nVariable (ECV) of the Global Climate Observing System (GCOS). Following the\nneed for an integrated multi-temporal monitoring of lake ice in Switzerland,\nMeteoSwiss in the framework of GCOS Switzerland supported this 2-year project\nto explore not only the use of satellite images but also the possibilities of\nWebcams and in-situ measurements. The aim of this project is to monitor some\ntarget lakes and detect the extent of ice and especially the ice-on/off dates,\nwith focus on the integration of various input data and processing methods. The\ntarget lakes are: St. Moritz, Silvaplana, Sils, Sihl, Greifen and Aegeri,\nwhereby only the first four were mainly frozen during the observation period\nand thus processed. The observation period was mainly the winter 2016-17.\nDuring the project, various approaches were developed, implemented, tested and\ncompared. Firstly, low spatial resolution (250 - 1000 m) but high temporal\nresolution (1 day) satellite images from the optical sensors MODIS and VIIRS\nwere used. Secondly, and as a pilot project, the use of existing public Webcams\nwas investigated for (a) validation of results from satellite data, and (b)\nindependent estimation of lake ice, especially for small lakes like St. Moritz,\nthat could not be possibly monitored in the satellite images. Thirdly, in-situ\nmeasurements were made in order to characterize the development of the\ntemperature profiles and partly pressure before freezing and under the\nice-cover until melting. This report presents the results of the project work.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 16:18:51 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Tom", "Manu", ""], ["Suetterlin", "Melanie", ""], ["Bouffard", "Damien", ""], ["Rothermel", "Mathias", ""], ["Wunderle", "Stefan", ""], ["Baltsavias", "Emmanuel", ""]]}, {"id": "2008.00528", "submitter": "Yujie He", "authors": "Yujie He, Changhong Fu, Fuling Lin, Yiming Li, Peng Lu", "title": "Towards Robust Visual Tracking for Unmanned Aerial Vehicle with\n  Tri-Attentional Correlation Filters", "comments": "IROS'20 accepted, 8 pages, 6 figures, and 2 tables", "journal-ref": "IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS 2020), Las Vegas, USA", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object tracking has been broadly applied in unmanned aerial vehicle (UAV)\ntasks in recent years. However, existing algorithms still face difficulties\nsuch as partial occlusion, clutter background, and other challenging visual\nfactors. Inspired by the cutting-edge attention mechanisms, a novel object\ntracking framework is proposed to leverage multi-level visual attention. Three\nprimary attention, i.e., contextual attention, dimensional attention, and\nspatiotemporal attention, are integrated into the training and detection stages\nof correlation filter-based tracking pipeline. Therefore, the proposed tracker\nis equipped with robust discriminative power against challenging factors while\nmaintaining high operational efficiency in UAV scenarios. Quantitative and\nqualitative experiments on two well-known benchmarks with 173 challenging UAV\nvideo sequences demonstrate the effectiveness of the proposed framework. The\nproposed tracking algorithm favorably outperforms 12 state-of-the-art methods,\nyielding 4.8% relative gain in UAVDT and 8.2% relative gain in UAV123@10fps\nagainst the baseline tracker while operating at the speed of $\\sim$ 28 frames\nper second.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 17:36:25 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 13:39:21 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["He", "Yujie", ""], ["Fu", "Changhong", ""], ["Lin", "Fuling", ""], ["Li", "Yiming", ""], ["Lu", "Peng", ""]]}, {"id": "2008.00542", "submitter": "Yu Shen", "authors": "Yu Shen, Sijie Zhu, Chen Chen, Qian Du, Liang Xiao, Jianyu Chen, Delu\n  Pan", "title": "Efficient Deep Learning of Non-local Features for Hyperspectral Image\n  Classification", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2020.3014286", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based methods, such as Convolution Neural Network (CNN), have\ndemonstrated their efficiency in hyperspectral image (HSI) classification.\nThese methods can automatically learn spectral-spatial discriminative features\nwithin local patches. However, for each pixel in an HSI, it is not only related\nto its nearby pixels but also has connections to pixels far away from itself.\nTherefore, to incorporate the long-range contextual information, a deep fully\nconvolutional network (FCN) with an efficient non-local module, named ENL-FCN,\nis proposed for HSI classification. In the proposed framework, a deep FCN\nconsiders an entire HSI as input and extracts spectral-spatial information in a\nlocal receptive field. The efficient non-local module is embedded in the\nnetwork as a learning unit to capture the long-range contextual information.\nDifferent from the traditional non-local neural networks, the long-range\ncontextual information is extracted in a specially designed criss-cross path\nfor computation efficiency. Furthermore, by using a recurrent operation, each\npixel's response is aggregated from all pixels of HSI. The benefits of our\nproposed ENL-FCN are threefold: 1) the long-range contextual information is\nincorporated effectively, 2) the efficient module can be freely embedded in a\ndeep neural network in a plug-and-play fashion, and 3) it has much fewer\nlearning parameters and requires less computational resources. The experiments\nconducted on three popular HSI datasets demonstrate that the proposed method\nachieves state-of-the-art classification performance with lower computational\ncost in comparison with several leading deep neural networks for HSI.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 19:13:22 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Shen", "Yu", ""], ["Zhu", "Sijie", ""], ["Chen", "Chen", ""], ["Du", "Qian", ""], ["Xiao", "Liang", ""], ["Chen", "Jianyu", ""], ["Pan", "Delu", ""]]}, {"id": "2008.00544", "submitter": "Wentian Zhao", "authors": "Wentian Zhao, Seokhwan Kim, Ning Xu, Hailin Jin", "title": "Video Question Answering on Screencast Tutorials", "comments": null, "journal-ref": null, "doi": "10.24963/ijcai.2020/148", "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new video question answering task on screencast\ntutorials. We introduce a dataset including question, answer and context\ntriples from the tutorial videos for a software. Unlike other video question\nanswering works, all the answers in our dataset are grounded to the domain\nknowledge base. An one-shot recognition algorithm is designed to extract the\nvisual cues, which helps enhance the performance of video question answering.\nWe also propose several baseline neural network architectures based on various\naspects of video contexts from the dataset. The experimental results\ndemonstrate that our proposed models significantly improve the question\nanswering performances by incorporating multi-modal contexts and domain\nknowledge.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 19:27:42 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Zhao", "Wentian", ""], ["Kim", "Seokhwan", ""], ["Xu", "Ning", ""], ["Jin", "Hailin", ""]]}, {"id": "2008.00549", "submitter": "Ruimin Ke", "authors": "Ruimin Ke, Zhiyong Cui, Yanlong Chen, Meixin Zhu, Hao (Frank) Yang,\n  Yinhai Wang", "title": "Edge Computing for Real-Time Near-Crash Detection for Smart\n  Transportation Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic near-crash events serve as critical data sources for various smart\ntransportation applications, such as being surrogate safety measures for\ntraffic safety research and corner case data for automated vehicle testing.\nHowever, there are several key challenges for near-crash detection. First,\nextracting near-crashes from original data sources requires significant\ncomputing, communication, and storage resources. Also, existing methods lack\nefficiency and transferability, which bottlenecks prospective large-scale\napplications. To this end, this paper leverages the power of edge computing to\naddress these challenges by processing the video streams from existing dashcams\nonboard in a real-time manner. We design a multi-thread system architecture\nthat operates on edge devices and model the bounding boxes generated by object\ndetection and tracking in linear complexity. The method is insensitive to\ncamera parameters and backward compatible with different vehicles. The edge\ncomputing system has been evaluated with recorded videos and real-world tests\non two cars and four buses for over ten thousand hours. It filters out\nirrelevant videos in real-time thereby saving labor cost, processing time,\nnetwork bandwidth, and data storage. It collects not only event videos but also\nother valuable data such as road user type, event location, time to collision,\nvehicle trajectory, vehicle speed, brake switch, and throttle. The experiments\ndemonstrate the promising performance of the system regarding efficiency,\naccuracy, reliability, and transferability. It is among the first efforts in\napplying edge computing for real-time traffic video analytics and is expected\nto benefit multiple sub-fields in smart transportation research and\napplications.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 19:39:14 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 05:02:43 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Ke", "Ruimin", "", "Frank"], ["Cui", "Zhiyong", "", "Frank"], ["Chen", "Yanlong", "", "Frank"], ["Zhu", "Meixin", "", "Frank"], ["Hao", "", "", "Frank"], ["Yang", "", ""], ["Wang", "Yinhai", ""]]}, {"id": "2008.00558", "submitter": "Barbara Benato", "authors": "Barbara Caroline Benato and Jancarlo Ferreira Gomes and Alexandru\n  Cristian Telea and Alexandre Xavier Falc\\~ao", "title": "Semi-supervised deep learning based on label propagation in a 2D\n  embedded space", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While convolutional neural networks need large labeled sets for training\nimages, expert human supervision of such datasets can be very laborious.\nProposed solutions propagate labels from a small set of supervised images to a\nlarge set of unsupervised ones to obtain sufficient truly-and-artificially\nlabeled samples to train a deep neural network model. Yet, such solutions need\nmany supervised images for validation. We present a loop in which a deep neural\nnetwork (VGG-16) is trained from a set with more correctly labeled samples\nalong iterations, created by using t-SNE to project the features of its last\nmax-pooling layer into a 2D embedded space in which labels are propagated using\nthe Optimum-Path Forest semi-supervised classifier. As the labeled set improves\nalong iterations, it improves the features of the neural network. We show that\nthis can significantly improve classification results on test data (using only\n1\\% to 5\\% of supervised samples) of three private challenging datasets and two\npublic ones.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 20:08:54 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 14:30:27 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Benato", "Barbara Caroline", ""], ["Gomes", "Jancarlo Ferreira", ""], ["Telea", "Alexandru Cristian", ""], ["Falc\u00e3o", "Alexandre Xavier", ""]]}, {"id": "2008.00605", "submitter": "Xiyang Luo", "authors": "Xiyang Luo, Hossein Talebi, Feng Yang, Michael Elad, Peyman Milanfar", "title": "The Rate-Distortion-Accuracy Tradeoff: JPEG Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handling digital images is almost always accompanied by a lossy compression\nin order to facilitate efficient transmission and storage. This introduces an\nunavoidable tension between the allocated bit-budget (rate) and the\nfaithfulness of the resulting image to the original one (distortion). An\nadditional complicating consideration is the effect of the compression on\nrecognition performance by given classifiers (accuracy). This work aims to\nexplore this rate-distortion-accuracy tradeoff. As a case study, we focus on\nthe design of the quantization tables in the JPEG compression standard. We\noffer a novel optimal tuning of these tables via continuous optimization,\nleveraging a differential implementation of both the JPEG encoder-decoder and\nan entropy estimator. This enables us to offer a unified framework that\nconsiders the interplay between rate, distortion and classification accuracy.\nIn all these fronts, we report a substantial boost in performance by a simple\nand easily implemented modification of these tables.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 01:39:01 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Luo", "Xiyang", ""], ["Talebi", "Hossein", ""], ["Yang", "Feng", ""], ["Elad", "Michael", ""], ["Milanfar", "Peyman", ""]]}, {"id": "2008.00610", "submitter": "Yehui Yang", "authors": "Yehui Yang, Fangxin Shang, Binghong Wu, Dalu Yang, Lei Wang, Yanwu Xu,\n  Wensheng Zhang, Tianzhu Zhang", "title": "Robust Collaborative Learning of Patch-level and Image-level Annotations\n  for Diabetic Retinopathy Grading from Fundus Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic retinopathy (DR) grading from fundus images has attracted increasing\ninterest in both academic and industrial communities. Most convolutional neural\nnetwork (CNN) based algorithms treat DR grading as a classification task via\nimage-level annotations. However, these algorithms have not fully explored the\nvaluable information in the DR-related lesions. In this paper, we present a\nrobust framework, which collaboratively utilizes patch-level and image-level\nannotations, for DR severity grading. By an end-to-end optimization, this\nframework can bi-directionally exchange the fine-grained lesion and image-level\ngrade information. As a result, it exploits more discriminative features for DR\ngrading. The proposed framework shows better performance than the recent\nstate-of-the-art algorithms and three clinical ophthalmologists with over nine\nyears of experience. By testing on datasets of different distributions (such as\nlabel and camera), we prove that our algorithm is robust when facing image\nquality and distribution variations that commonly exist in real-world practice.\nWe inspect the proposed framework through extensive ablation studies to\nindicate the effectiveness and necessity of each motivation. The code and some\nvaluable annotations are now publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 02:17:42 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 07:35:45 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Yang", "Yehui", ""], ["Shang", "Fangxin", ""], ["Wu", "Binghong", ""], ["Yang", "Dalu", ""], ["Wang", "Lei", ""], ["Xu", "Yanwu", ""], ["Zhang", "Wensheng", ""], ["Zhang", "Tianzhu", ""]]}, {"id": "2008.00627", "submitter": "Yichen Wu", "authors": "Yichen Wu, Jun Shu, Qi Xie, Qian Zhao and Deyu Meng", "title": "Learning to Purify Noisy Labels via Meta Soft Label Corrector", "comments": "12 pages,6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep neural networks (DNNs) can easily overfit to biased training data\nwith noisy labels. Label correction strategy is commonly used to alleviate this\nissue by designing a method to identity suspected noisy labels and then correct\nthem. Current approaches to correcting corrupted labels usually need certain\npre-defined label correction rules or manually preset hyper-parameters. These\nfixed settings make it hard to apply in practice since the accurate label\ncorrection usually related with the concrete problem, training data and the\ntemporal information hidden in dynamic iterations of training process. To\naddress this issue, we propose a meta-learning model which could estimate soft\nlabels through meta-gradient descent step under the guidance of noise-free meta\ndata. By viewing the label correction procedure as a meta-process and using a\nmeta-learner to automatically correct labels, we could adaptively obtain\nrectified soft labels iteratively according to current training problems\nwithout manually preset hyper-parameters. Besides, our method is model-agnostic\nand we can combine it with any other existing model with ease. Comprehensive\nexperiments substantiate the superiority of our method in both synthetic and\nreal-world problems with noisy labels compared with current SOTA label\ncorrection strategies.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 03:25:17 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Wu", "Yichen", ""], ["Shu", "Jun", ""], ["Xie", "Qi", ""], ["Zhao", "Qian", ""], ["Meng", "Deyu", ""]]}, {"id": "2008.00634", "submitter": "Amir Mazaheri", "authors": "Aaron Ott, Amir Mazaheri, Niels D. Lobo, Mubarak Shah", "title": "Deep Photo Cropper and Enhancer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new type of image enhancement problem. Compared to\ntraditional image enhancement methods, which mostly deal with pixel-wise\nmodifications of a given photo, our proposed task is to crop an image which is\nembedded within a photo and enhance the quality of the cropped image. We split\nour proposed approach into two deep networks: deep photo cropper and deep image\nenhancer. In the photo cropper network, we employ a spatial transformer to\nextract the embedded image. In the photo enhancer, we employ super-resolution\nto increase the number of pixels in the embedded image and reduce the effect of\nstretching and distortion of pixels. We use cosine distance loss between image\nfeatures and ground truth for the cropper and the mean square loss for the\nenhancer. Furthermore, we propose a new dataset to train and test the proposed\nmethod. Finally, we analyze the proposed method with respect to qualitative and\nquantitative evaluations.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 03:50:20 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Ott", "Aaron", ""], ["Mazaheri", "Amir", ""], ["Lobo", "Niels D.", ""], ["Shah", "Mubarak", ""]]}, {"id": "2008.00637", "submitter": "Weihao Yuan", "authors": "Weihao Yuan, Michael Yu Wang, Qifeng Chen", "title": "Self-supervised Object Tracking with Cycle-consistent Siamese Networks", "comments": "2020 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning for visual object tracking possesses valuable\nadvantages compared to supervised learning, such as the non-necessity of\nlaborious human annotations and online training. In this work, we exploit an\nend-to-end Siamese network in a cycle-consistent self-supervised framework for\nobject tracking. Self-supervision can be performed by taking advantage of the\ncycle consistency in the forward and backward tracking. To better leverage the\nend-to-end learning of deep networks, we propose to integrate a Siamese region\nproposal and mask regression network in our tracking framework so that a fast\nand more accurate tracker can be learned without the annotation of each frame.\nThe experiments on the VOT dataset for visual object tracking and on the DAVIS\ndataset for video object segmentation propagation show that our method\noutperforms prior approaches on both tasks.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 04:10:38 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Yuan", "Weihao", ""], ["Wang", "Michael Yu", ""], ["Chen", "Qifeng", ""]]}, {"id": "2008.00638", "submitter": "Dibakar Gope", "authors": "Dibakar Gope, Jesse Beu, Matthew Mattina", "title": "High Throughput Matrix-Matrix Multiplication between Asymmetric\n  Bit-Width Operands", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix multiplications between asymmetric bit-width operands, especially\nbetween 8- and 4-bit operands are likely to become a fundamental kernel of many\nimportant workloads including neural networks and machine learning. While\nexisting SIMD matrix multiplication instructions for symmetric bit-width\noperands can support operands of mixed precision by zero- or sign-extending the\nnarrow operand to match the size of the other operands, they cannot exploit the\nbenefit of narrow bit-width of one of the operands. We propose a new SIMD\nmatrix multiplication instruction that uses mixed precision on its inputs (8-\nand 4-bit operands) and accumulates product values into narrower 16-bit output\naccumulators, in turn allowing the SIMD operation at 128-bit vector width to\nprocess a greater number of data elements per instruction to improve processing\nthroughput and memory bandwidth utilization without increasing the register\nread- and write-port bandwidth in CPUs. The proposed asymmetric-operand-size\nSIMD instruction offers 2x improvement in throughput of matrix multiplication\nin comparison to throughput obtained using existing symmetric-operand-size\ninstructions while causing negligible (0.05%) overflow from 16-bit accumulators\nfor representative machine learning workloads. The asymmetric-operand-size\ninstruction not only can improve matrix multiplication throughput in CPUs, but\nalso can be effective to support multiply-and-accumulate (MAC) operation\nbetween 8- and 4-bit operands in state-of-the-art DNN hardware accelerators\n(e.g., systolic array microarchitecture in Google TPU, etc.) and offer similar\nimprovement in matrix multiply performance seamlessly without violating the\nvarious implementation constraints. We demonstrate how a systolic array\narchitecture designed for symmetric-operand-size instructions could be modified\nto support an asymmetric-operand-sized instruction.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 04:12:31 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Gope", "Dibakar", ""], ["Beu", "Jesse", ""], ["Mattina", "Matthew", ""]]}, {"id": "2008.00658", "submitter": "Yuheng Lu", "authors": "Yuheng Lu, Fan Yang, Fangping Chen, Don Xie", "title": "PIC-Net: Point Cloud and Image Collaboration Network for Large-Scale\n  Place Recognition", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Place recognition is one of the hot research fields in automation technology\nand is still an open issue, Camera and Lidar are two mainstream sensors used in\nthis task, Camera-based methods are easily affected by illumination and season\nchanges, LIDAR cannot get the rich data as the image could , In this paper, we\npropose the PIC-Net (Point cloud and Image Collaboration Network), which use\nattention mechanism to fuse the features of image and point cloud, and mine the\ncomplementary information between the two. Furthermore, in order to improve the\nrecognition performance at night, we transform the night image into the daytime\nstyle. Comparison results show that the collaboration of image and point cloud\noutperform both image-based and point cloud-based method, the attention\nstrategy and day-night-transform could further improve the performance.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 05:58:00 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Lu", "Yuheng", ""], ["Yang", "Fan", ""], ["Chen", "Fangping", ""], ["Xie", "Don", ""]]}, {"id": "2008.00665", "submitter": "Savvas Karatsiolis", "authors": "Savvas Karatsiolis and Andreas Kamilaris", "title": "The pursuit of beauty: Converting image labels to meaningful vectors", "comments": "20 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A challenge of the computer vision community is to understand the semantics\nof an image, in order to allow image reconstruction based on existing\nhigh-level features or to better analyze (semi-)labelled datasets. Towards\naddressing this challenge, this paper introduces a method, called\nOcclusion-based Latent Representations (OLR), for converting image labels to\nmeaningful representations that capture a significant amount of data semantics.\nBesides being informational rich, these representations compose a disentangled\nlow-dimensional latent space where each image label is encoded into a separate\nvector. We evaluate the quality of these representations in a series of\nexperiments whose results suggest that the proposed model can capture data\nconcepts and discover data interrelations.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 06:33:11 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Karatsiolis", "Savvas", ""], ["Kamilaris", "Andreas", ""]]}, {"id": "2008.00697", "submitter": "Yanrui Bin", "authors": "Yanrui Bin, Xuan Cao, Xinya Chen, Yanhao Ge, Ying Tai, Chengjie Wang,\n  Jilin Li, Feiyue Huang, Changxin Gao, Nong Sang", "title": "Adversarial Semantic Data Augmentation for Human Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose estimation is the task of localizing body keypoints from still\nimages. The state-of-the-art methods suffer from insufficient examples of\nchallenging cases such as symmetric appearance, heavy occlusion and nearby\nperson. To enlarge the amounts of challenging cases, previous methods augmented\nimages by cropping and pasting image patches with weak semantics, which leads\nto unrealistic appearance and limited diversity. We instead propose Semantic\nData Augmentation (SDA), a method that augments images by pasting segmented\nbody parts with various semantic granularity. Furthermore, we propose\nAdversarial Semantic Data Augmentation (ASDA), which exploits a generative\nnetwork to dynamiclly predict tailored pasting configuration. Given\noff-the-shelf pose estimation network as discriminator, the generator seeks the\nmost confusing transformation to increase the loss of the discriminator while\nthe discriminator takes the generated sample as input and learns from it. The\nwhole pipeline is optimized in an adversarial manner. State-of-the-art results\nare achieved on challenging benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 07:56:04 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Bin", "Yanrui", ""], ["Cao", "Xuan", ""], ["Chen", "Xinya", ""], ["Ge", "Yanhao", ""], ["Tai", "Ying", ""], ["Wang", "Chengjie", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""], ["Gao", "Changxin", ""], ["Sang", "Nong", ""]]}, {"id": "2008.00698", "submitter": "Hanlin Chen", "authors": "Hanlin Chen, Baochang Zhang, Song Xue, Xuan Gong, Hong Liu, Rongrong\n  Ji, David Doermann", "title": "Anti-Bandit Neural Architecture Search for Model Defense", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (DCNNs) have dominated as the best\nperformers in machine learning, but can be challenged by adversarial attacks.\nIn this paper, we defend against adversarial attacks using neural architecture\nsearch (NAS) which is based on a comprehensive search of denoising blocks,\nweight-free operations, Gabor filters and convolutions. The resulting\nanti-bandit NAS (ABanditNAS) incorporates a new operation evaluation measure\nand search process based on the lower and upper confidence bounds (LCB and\nUCB). Unlike the conventional bandit algorithm using UCB for evaluation only,\nwe use UCB to abandon arms for search efficiency and LCB for a fair competition\nbetween arms. Extensive experiments demonstrate that ABanditNAS is faster than\nother NAS methods, while achieving an $8.73\\%$ improvement over prior arts on\nCIFAR-10 under PGD-$7$.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 07:59:39 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 08:33:48 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Chen", "Hanlin", ""], ["Zhang", "Baochang", ""], ["Xue", "Song", ""], ["Gong", "Xuan", ""], ["Liu", "Hong", ""], ["Ji", "Rongrong", ""], ["Doermann", "David", ""]]}, {"id": "2008.00710", "submitter": "Yuting He", "authors": "Yuting He, Tiantian Li, Guanyu Yang, Youyong Kong, Yang Chen, Huazhong\n  Shu, Jean-Louis Coatrieux, Jean-Louis Dillenseger, Shuo Li", "title": "Deep Complementary Joint Model for Complex Scene Registration and\n  Few-shot Segmentation on Medical Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based medical image registration and segmentation joint models\nutilize the complementarity (augmentation data or weakly supervised data from\nregistration, region constraints from segmentation) to bring mutual improvement\nin complex scene and few-shot situation. However, further adoption of the joint\nmodels are hindered: 1) the diversity of augmentation data is reduced limiting\nthe further enhancement of segmentation, 2) misaligned regions in weakly\nsupervised data disturb the training process, 3) lack of label-based region\nconstraints in few-shot situation limits the registration performance. We\npropose a novel Deep Complementary Joint Model (DeepRS) for complex scene\nregistration and few-shot segmentation. We embed a perturbation factor in the\nregistration to increase the activity of deformation thus maintaining the\naugmentation data diversity. We take a pixel-wise discriminator to extract\nalignment confidence maps which highlight aligned regions in weakly supervised\ndata so the misaligned regions' disturbance will be suppressed via weighting.\nThe outputs from segmentation model are utilized to implement deep-based region\nconstraints thus relieving the label requirements and bringing fine\nregistration. Extensive experiments on the CT dataset of MM-WHS 2017 Challenge\nshow great advantages of our DeepRS that outperforms the existing\nstate-of-the-art models.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 08:25:59 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["He", "Yuting", ""], ["Li", "Tiantian", ""], ["Yang", "Guanyu", ""], ["Kong", "Youyong", ""], ["Chen", "Yang", ""], ["Shu", "Huazhong", ""], ["Coatrieux", "Jean-Louis", ""], ["Dillenseger", "Jean-Louis", ""], ["Li", "Shuo", ""]]}, {"id": "2008.00714", "submitter": "Wenhai Wang", "authors": "Wenhai Wang, Xuebo Liu, Xiaozhong Ji, Enze Xie, Ding Liang, Zhibo\n  Yang, Tong Lu, Chunhua Shen, Ping Luo", "title": "AE TextSpotter: Learning Visual and Linguistic Representation for\n  Ambiguous Text Spotting", "comments": "Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scene text spotting aims to detect and recognize the entire word or sentence\nwith multiple characters in natural images. It is still challenging because\nambiguity often occurs when the spacing between characters is large or the\ncharacters are evenly spread in multiple rows and columns, making many visually\nplausible groupings of the characters (e.g. \"BERLIN\" is incorrectly detected as\n\"BERL\" and \"IN\" in Fig. 1(c)). Unlike previous works that merely employed\nvisual features for text detection, this work proposes a novel text spotter,\nnamed Ambiguity Eliminating Text Spotter (AE TextSpotter), which learns both\nvisual and linguistic features to significantly reduce ambiguity in text\ndetection. The proposed AE TextSpotter has three important benefits. 1) The\nlinguistic representation is learned together with the visual representation in\na framework. To our knowledge, it is the first time to improve text detection\nby using a language model. 2) A carefully designed language module is utilized\nto reduce the detection confidence of incorrect text lines, making them easily\npruned in the detection stage. 3) Extensive experiments show that AE\nTextSpotter outperforms other state-of-the-art methods by a large margin. For\nexample, we carefully select a validation set of extremely ambiguous samples\nfrom the IC19-ReCTS dataset, where our approach surpasses other methods by more\nthan 4%. The code has been released at\nhttps://github.com/whai362/AE_TextSpotter. The image list and evaluation\nscripts of the validation set have been released at\nhttps://github.com/whai362/TDA-ReCTS.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 08:40:01 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 17:16:23 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2020 06:36:02 GMT"}, {"version": "v4", "created": "Wed, 21 Oct 2020 11:59:59 GMT"}, {"version": "v5", "created": "Tue, 6 Jul 2021 14:06:06 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Wang", "Wenhai", ""], ["Liu", "Xuebo", ""], ["Ji", "Xiaozhong", ""], ["Xie", "Enze", ""], ["Liang", "Ding", ""], ["Yang", "Zhibo", ""], ["Lu", "Tong", ""], ["Shen", "Chunhua", ""], ["Luo", "Ping", ""]]}, {"id": "2008.00744", "submitter": "Samuel Albanie", "authors": "Samuel Albanie, Yang Liu, Arsha Nagrani, Antoine Miech, Ernesto Coto,\n  Ivan Laptev, Rahul Sukthankar, Bernard Ghanem, Andrew Zisserman, Valentin\n  Gabeur, Chen Sun, Karteek Alahari, Cordelia Schmid, Shizhe Chen, Yida Zhao,\n  Qin Jin, Kaixu Cui, Hui Liu, Chen Wang, Yudong Jiang, Xiaoshuai Hao", "title": "The End-of-End-to-End: A Video Understanding Pentathlon Challenge (2020)", "comments": "Individual reports, dataset information, rules, and released source\n  code can be found at the competition webpage\n  (https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new video understanding pentathlon challenge, an open\ncompetition held in conjunction with the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) 2020. The objective of the challenge was to explore\nand evaluate new methods for text-to-video retrieval-the task of searching for\ncontent within a corpus of videos using natural language queries. This report\nsummarizes the results of the first edition of the challenge together with the\nfindings of the participants.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 09:55:26 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Albanie", "Samuel", ""], ["Liu", "Yang", ""], ["Nagrani", "Arsha", ""], ["Miech", "Antoine", ""], ["Coto", "Ernesto", ""], ["Laptev", "Ivan", ""], ["Sukthankar", "Rahul", ""], ["Ghanem", "Bernard", ""], ["Zisserman", "Andrew", ""], ["Gabeur", "Valentin", ""], ["Sun", "Chen", ""], ["Alahari", "Karteek", ""], ["Schmid", "Cordelia", ""], ["Chen", "Shizhe", ""], ["Zhao", "Yida", ""], ["Jin", "Qin", ""], ["Cui", "Kaixu", ""], ["Liu", "Hui", ""], ["Wang", "Chen", ""], ["Jiang", "Yudong", ""], ["Hao", "Xiaoshuai", ""]]}, {"id": "2008.00752", "submitter": "Liping Zhang", "authors": "Liping Zhang, Weijun Li, Lina Yu, Xiaoli Dong, Linjun Sun, Xin Ning,\n  Jian Xu, and Hong Qin", "title": "GmFace: A Mathematical Model for Face Image Representation Using\n  Multi-Gaussian", "comments": "12 pages, 12 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Establishing mathematical models is a ubiquitous and effective method to\nunderstand the objective world. Due to complex physiological structures and\ndynamic behaviors, mathematical representation of the human face is an\nespecially challenging task. A mathematical model for face image representation\ncalled GmFace is proposed in the form of a multi-Gaussian function in this\npaper. The model utilizes the advantages of two-dimensional Gaussian function\nwhich provides a symmetric bell surface with a shape that can be controlled by\nparameters. The GmNet is then designed using Gaussian functions as neurons,\nwith parameters that correspond to each of the parameters of GmFace in order to\ntransform the problem of GmFace parameter solving into a network optimization\nproblem of GmNet. The face modeling process can be described by the following\nsteps: (1) GmNet initialization; (2) feeding GmNet with face image(s); (3)\ntraining GmNet until convergence; (4) drawing out the parameters of GmNet (as\nthe same as GmFace); (5) recording the face model GmFace. Furthermore, using\nGmFace, several face image transformation operations can be realized\nmathematically through simple parameter computation.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 10:11:10 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Zhang", "Liping", ""], ["Li", "Weijun", ""], ["Yu", "Lina", ""], ["Dong", "Xiaoli", ""], ["Sun", "Linjun", ""], ["Ning", "Xin", ""], ["Xu", "Jian", ""], ["Qin", "Hong", ""]]}, {"id": "2008.00760", "submitter": "Marco Maggipinto", "authors": "Marco Maggipinto and Matteo Terzi and Gian Antonio Susto", "title": "IntroVAC: Introspective Variational Classifiers for Learning\n  Interpretable Latent Subspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning useful representations of complex data has been the subject of\nextensive research for many years. With the diffusion of Deep Neural Networks,\nVariational Autoencoders have gained lots of attention since they provide an\nexplicit model of the data distribution based on an encoder/decoder\narchitecture which is able to both generate images and encode them in a\nlow-dimensional subspace. However, the latent space is not easily interpretable\nand the generation capabilities show some limitations since images typically\nlook blurry and lack details. In this paper, we propose the Introspective\nVariational Classifier (IntroVAC), a model that learns interpretable latent\nsubspaces by exploiting information from an additional label and provides\nimproved image quality thanks to an adversarial training strategy.We show that\nIntroVAC is able to learn meaningful directions in the latent space enabling\nfine-grained manipulation of image attributes. We validate our approach on the\nCelebA dataset.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 10:21:41 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 07:23:10 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Maggipinto", "Marco", ""], ["Terzi", "Matteo", ""], ["Susto", "Gian Antonio", ""]]}, {"id": "2008.00767", "submitter": "Cong Wang", "authors": "Cong Wang, Xiaoying Xing, Zhixun Su, Junyang Chen", "title": "DCSFN: Deep Cross-scale Fusion Network for Single Image Rain Removal", "comments": "Accepted to ACM International Conference on Multimedia (MM'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rain removal is an important but challenging computer vision task as rain\nstreaks can severely degrade the visibility of images that may make other\nvisions or multimedia tasks fail to work. Previous works mainly focused on\nfeature extraction and processing or neural network structure, while the\ncurrent rain removal methods can already achieve remarkable results, training\nbased on single network structure without considering the cross-scale\nrelationship may cause information drop-out. In this paper, we explore the\ncross-scale manner between networks and inner-scale fusion operation to solve\nthe image rain removal task. Specifically, to learn features with different\nscales, we propose a multi-sub-networks structure, where these sub-networks are\nfused via a crossscale manner by Gate Recurrent Unit to inner-learn and make\nfull use of information at different scales in these sub-networks. Further, we\ndesign an inner-scale connection block to utilize the multi-scale information\nand features fusion way between different scales to improve rain representation\nability and we introduce the dense block with skip connection to inner-connect\nthese blocks. Experimental results on both synthetic and real-world datasets\nhave demonstrated the superiority of our proposed method, which outperforms\nover the state-of-the-art methods. The source code will be available at\nhttps://supercong94.wixsite.com/supercong94.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 10:34:45 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Wang", "Cong", ""], ["Xing", "Xiaoying", ""], ["Su", "Zhixun", ""], ["Chen", "Junyang", ""]]}, {"id": "2008.00777", "submitter": "Chaofan Tao", "authors": "Chaofan Tao, Qinhong Jiang, Lixin Duan, Ping Luo", "title": "Dynamic and Static Context-aware LSTM for Multi-agent Motion Prediction", "comments": "17 pages, 6 figures", "journal-ref": "ECCV 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent motion prediction is challenging because it aims to foresee the\nfuture trajectories of multiple agents (\\textit{e.g.} pedestrians)\nsimultaneously in a complicated scene. Existing work addressed this challenge\nby either learning social spatial interactions represented by the positions of\na group of pedestrians, while ignoring their temporal coherence (\\textit{i.e.}\ndependencies between different long trajectories), or by understanding the\ncomplicated scene layout (\\textit{e.g.} scene segmentation) to ensure safe\nnavigation. However, unlike previous work that isolated the spatial\ninteraction, temporal coherence, and scene layout, this paper designs a new\nmechanism, \\textit{i.e.}, Dynamic and Static Context-aware Motion Predictor\n(DSCMP), to integrates these rich information into the long-short-term-memory\n(LSTM). It has three appealing benefits. (1) DSCMP models the dynamic\ninteractions between agents by learning both their spatial positions and\ntemporal coherence, as well as understanding the contextual scene layout.(2)\nDifferent from previous LSTM models that predict motions by propagating hidden\nfeatures frame by frame, limiting the capacity to learn correlations between\nlong trajectories, we carefully design a differentiable queue mechanism in\nDSCMP, which is able to explicitly memorize and learn the correlations between\nlong trajectories. (3) DSCMP captures the context of scene by inferring latent\nvariable, which enables multimodal predictions with meaningful semantic scene\nlayout. Extensive experiments show that DSCMP outperforms state-of-the-art\nmethods by large margins, such as 9.05\\% and 7.62\\% relative improvements on\nthe ETH-UCY and SDD datasets respectively.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 11:03:57 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Tao", "Chaofan", ""], ["Jiang", "Qinhong", ""], ["Duan", "Lixin", ""], ["Luo", "Ping", ""]]}, {"id": "2008.00801", "submitter": "Laurent Kloeker", "authors": "Laurent Kloeker, Christian Kotulla, Lutz Eckstein", "title": "Real-Time Point Cloud Fusion of Multi-LiDAR Infrastructure Sensor Setups\n  with Unknown Spatial Location and Orientation", "comments": "Accepted to be published as part of the 23rd IEEE International\n  Conference on Intelligent Transportation Systems (ITSC), Rhodes, Greece,\n  September 20-23, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of infrastructure sensor technology for traffic detection has already\nbeen proven several times. However, extrinsic sensor calibration is still a\nchallenge for the operator. While previous approaches are unable to calibrate\nthe sensors without the use of reference objects in the sensor field of view\n(FOV), we present an algorithm that is completely detached from external\nassistance and runs fully automatically. Our method focuses on the\nhigh-precision fusion of LiDAR point clouds and is evaluated in simulation as\nwell as on real measurements. We set the LiDARs in a continuous pendulum motion\nin order to simulate real-world operation as closely as possible and to\nincrease the demands on the algorithm. However, it does not receive any\ninformation about the initial spatial location and orientation of the LiDARs\nthroughout the entire measurement period. Experiments in simulation as well as\nwith real measurements have shown that our algorithm performs a continuous\npoint cloud registration of up to four 64-layer LiDARs in real-time. The\naveraged resulting translational error is within a few centimeters and the\naveraged error in rotation is below 0.15 degrees.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 08:43:39 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Kloeker", "Laurent", ""], ["Kotulla", "Christian", ""], ["Eckstein", "Lutz", ""]]}, {"id": "2008.00802", "submitter": "Thuong Nguyen Canh", "authors": "Thuong Nguyen Canh, Byeungwoo Jeon", "title": "Multi-Scale Deep Compressive Imaging", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning-based compressive imaging (DCI) has surpassed the\nconventional compressive imaging in reconstruction quality and faster running\ntime. While multi-scale has shown superior performance over single-scale,\nresearch in DCI has been limited to single-scale sampling. Despite training\nwith single-scale images, DCI tends to favor low-frequency components similar\nto the conventional multi-scale sampling, especially at low subrate. From this\nperspective, it would be easier for the network to learn multi-scale features\nwith a multi-scale sampling architecture. In this work, we proposed a\nmulti-scale deep compressive imaging (MS-DCI) framework which jointly learns to\ndecompose, sample, and reconstruct images at multi-scale. A three-phase\nend-to-end training scheme was introduced with an initial and two enhance\nreconstruction phases to demonstrate the efficiency of multi-scale sampling and\nfurther improve the reconstruction performance. We analyzed the decomposition\nmethods (including Pyramid, Wavelet, and Scale-space), sampling matrices, and\nmeasurements and showed the empirical benefit of MS-DCI which consistently\noutperforms both conventional and deep learning-based approaches.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 12:01:47 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Canh", "Thuong Nguyen", ""], ["Jeon", "Byeungwoo", ""]]}, {"id": "2008.00807", "submitter": "Christos Matsoukas", "authors": "Christos Matsoukas, Albert Bou I Hernandez, Yue Liu, Karin Dembrower,\n  Gisele Miranda, Emir Konuk, Johan Fredin Haslum, Athanasios Zouzos, Peter\n  Lindholm, Fredrik Strand, Kevin Smith", "title": "Adding Seemingly Uninformative Labels Helps in Low Data Regimes", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evidence suggests that networks trained on large datasets generalize well not\nsolely because of the numerous training examples, but also class diversity\nwhich encourages learning of enriched features. This raises the question of\nwhether this remains true when data is scarce - is there an advantage to\nlearning with additional labels in low-data regimes? In this work, we consider\na task that requires difficult-to-obtain expert annotations: tumor segmentation\nin mammography images. We show that, in low-data settings, performance can be\nimproved by complementing the expert annotations with seemingly uninformative\nlabels from non-expert annotators, turning the task into a multi-class problem.\nWe reveal that these gains increase when less expert data is available, and\nuncover several interesting properties through further studies. We demonstrate\nour findings on CSAW-S, a new dataset that we introduce here, and confirm them\non two public datasets.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 17:38:59 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 10:52:43 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Matsoukas", "Christos", ""], ["Hernandez", "Albert Bou I", ""], ["Liu", "Yue", ""], ["Dembrower", "Karin", ""], ["Miranda", "Gisele", ""], ["Konuk", "Emir", ""], ["Haslum", "Johan Fredin", ""], ["Zouzos", "Athanasios", ""], ["Lindholm", "Peter", ""], ["Strand", "Fredrik", ""], ["Smith", "Kevin", ""]]}, {"id": "2008.00809", "submitter": "Sumanth Chennupati", "authors": "Sumanth Chennupati, Sai Nooka, Shagan Sah, Raymond W Ptucha", "title": "Adaptive Hierarchical Decomposition of Large Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has recently demonstrated its ability to rival the human brain\nfor visual object recognition. As datasets get larger, a natural question to\nask is if existing deep learning architectures can be extended to handle the\n50+K classes thought to be perceptible by a typical human. Most deep learning\narchitectures concentrate on splitting diverse categories, while ignoring the\nsimilarities amongst them. This paper introduces a framework that automatically\nanalyzes and configures a family of smaller deep networks as a replacement to a\nsingular, larger network. Class similarities guide the creation of a family\nfrom course to fine classifiers which solve categorical problems more\neffectively than a single large classifier. The resulting smaller networks are\nhighly scalable, parallel and more practical to train, and achieve higher\nclassification accuracy. This paper also proposes a method to adaptively select\nthe configuration of the hierarchical family of classifiers using linkage\nstatistics from overall and sub-classification confusion matrices. Depending on\nthe number of classes and the complexity of the problem, a deep learning model\nis selected and the complexity is determined. Numerous experiments on network\nclasses, layers, and architecture configurations validate our results.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 21:04:50 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Chennupati", "Sumanth", ""], ["Nooka", "Sai", ""], ["Sah", "Shagan", ""], ["Ptucha", "Raymond W", ""]]}, {"id": "2008.00810", "submitter": "Xiaolong Liu", "authors": "Xiaolong Liu, Yuqing Hou, Anbang Yao, Yurong Chen, Keqiang Li", "title": "CASNet: Common Attribute Support Network for image instance and panoptic\n  segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance segmentation and panoptic segmentation is being paid more and more\nattention in recent years. In comparison with bounding box based object\ndetection and semantic segmentation, instance segmentation can provide more\nanalytical results at pixel level. Given the insight that pixels belonging to\none instance have one or more common attributes of current instance, we bring\nup an one-stage instance segmentation network named Common Attribute Support\nNetwork (CASNet), which realizes instance segmentation by predicting and\nclustering common attributes. CASNet is designed in the manner of fully\nconvolutional and can implement training and inference from end to end. And\nCASNet manages predicting the instance without overlaps and holes, which\nproblem exists in most of current instance segmentation algorithms.\nFurthermore, it can be easily extended to panoptic segmentation through minor\nmodifications with little computation overhead. CASNet builds a bridge between\nsemantic and instance segmentation from finding pixel class ID to obtaining\nclass and instance ID by operations on common attribute. Through experiment for\ninstance and panoptic segmentation, CASNet gets mAP 32.8% and PQ 59.0% on\nCityscapes validation dataset by joint training, and mAP 36.3% and PQ 66.1% by\nseparated training mode. For panoptic segmentation, CASNet gets\nstate-of-the-art performance on the Cityscapes validation dataset.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 06:23:52 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Liu", "Xiaolong", ""], ["Hou", "Yuqing", ""], ["Yao", "Anbang", ""], ["Chen", "Yurong", ""], ["Li", "Keqiang", ""]]}, {"id": "2008.00813", "submitter": "Jose Escalona", "authors": "Jos\\'e L. Escalona", "title": "Kinematics of motion tracking using computer vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the kinematics of the motion tracking of a rigid body\nusing video recording. The novelty of the paper is on the adaptation of the\nmethods and nomenclature used in Computer Vision to those used in Multibody\nSystem Dynamics. That way, the equations presented here can be used, for\nexample, for inverse-dynamics multibody simulations driven by the motion\ntracking of selected bodies. This paper also adapts the well-known Zhang\ncalibration method to the presented nomenclature.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 18:02:13 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Escalona", "Jos\u00e9 L.", ""]]}, {"id": "2008.00817", "submitter": "Huazhu Fu", "authors": "Shihao Zhang, Huazhu Fu, Yanwu Xu, Yanxia Liu, Mingkui Tan", "title": "Retinal Image Segmentation with a Structure-Texture Demixing Network", "comments": "Accepted to MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinal image segmentation plays an important role in automatic disease\ndiagnosis. This task is very challenging because the complex structure and\ntexture information are mixed in a retinal image, and distinguishing the\ninformation is difficult. Existing methods handle texture and structure\njointly, which may lead biased models toward recognizing textures and thus\nresults in inferior segmentation performance. To address it, we propose a\nsegmentation strategy that seeks to separate structure and texture components\nand significantly improve the performance. To this end, we design a\nstructure-texture demixing network (STD-Net) that can process structures and\ntextures differently and better. Extensive experiments on two retinal image\nsegmentation tasks (i.e., blood vessel segmentation, optic disc and cup\nsegmentation) demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 12:19:03 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Zhang", "Shihao", ""], ["Fu", "Huazhu", ""], ["Xu", "Yanwu", ""], ["Liu", "Yanxia", ""], ["Tan", "Mingkui", ""]]}, {"id": "2008.00818", "submitter": "Ya Lu", "authors": "Ya Lu and Thomai Stathopoulou and Stavroula Mougiakakou", "title": "Partially Supervised Multi-Task Network for Single-View Dietary\n  Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Food volume estimation is an essential step in the pipeline of dietary\nassessment and demands the precise depth estimation of the food surface and\ntable plane. Existing methods based on computer vision require either\nmulti-image input or additional depth maps, reducing convenience of\nimplementation and practical significance. Despite the recent advances in\nunsupervised depth estimation from a single image, the achieved performance in\nthe case of large texture-less areas needs to be improved. In this paper, we\npropose a network architecture that jointly performs geometric understanding\n(i.e., depth prediction and 3D plane estimation) and semantic prediction on a\nsingle food image, enabling a robust and accurate food volume estimation\nregardless of the texture characteristics of the target plane. For the training\nof the network, only monocular videos with semantic ground truth are required,\nwhile the depth map and 3D plane ground truth are no longer needed.\nExperimental results on two separate food image databases demonstrate that our\nmethod performs robustly on texture-less scenarios and is superior to\nunsupervised networks and structure from motion based approaches, while it\nachieves comparable performance to fully-supervised methods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 08:53:05 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Lu", "Ya", ""], ["Stathopoulou", "Thomai", ""], ["Mougiakakou", "Stavroula", ""]]}, {"id": "2008.00819", "submitter": "Ali Ayub", "authors": "Ali Ayub, Alan R. Wagner", "title": "Tell me what this is: Few-Shot Incremental Object Learning by a Robot", "comments": "Accepted at IEEE IROS 2020", "journal-ref": null, "doi": "10.1109/IROS45743.2020.9341140", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many applications, robots will need to be incrementally trained to\nrecognize the specific objects needed for an application. This paper presents a\npractical system for incrementally training a robot to recognize different\nobject categories using only a small set of visual examples provided by a\nhuman. The paper uses a recently developed state-of-the-art method for few-shot\nincremental learning of objects. After learning the object classes\nincrementally, the robot performs a table cleaning task organizing objects into\ncategories specified by the human. We also demonstrate the system's ability to\nlearn arrangements of objects and predict missing or incorrectly placed\nobjects. Experimental evaluations demonstrate that our approach achieves nearly\nthe same performance as a system trained with all examples at one time (batch\ntraining), which constitutes a theoretical upper bound.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 04:42:14 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Ayub", "Ali", ""], ["Wagner", "Alan R.", ""]]}, {"id": "2008.00820", "submitter": "Mingkui Tan", "authors": "Peihao Chen, Yang Zhang, Mingkui Tan, Hongdong Xiao, Deng Huang,\n  Chuang Gan", "title": "Generating Visually Aligned Sound from Videos", "comments": "Published in IEEE Transactions on Image Processing, 2020. Code,\n  pre-trained models and demo video: https://github.com/PeihaoChen/regnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the task of generating sound from natural videos, and the sound\nshould be both temporally and content-wise aligned with visual signals. This\ntask is extremely challenging because some sounds generated \\emph{outside} a\ncamera can not be inferred from video content. The model may be forced to learn\nan incorrect mapping between visual content and these irrelevant sounds. To\naddress this challenge, we propose a framework named REGNET. In this framework,\nwe first extract appearance and motion features from video frames to better\ndistinguish the object that emits sound from complex background information. We\nthen introduce an innovative audio forwarding regularizer that directly\nconsiders the real sound as input and outputs bottlenecked sound features.\nUsing both visual and bottlenecked sound features for sound prediction during\ntraining provides stronger supervision for the sound prediction. The audio\nforwarding regularizer can control the irrelevant sound component and thus\nprevent the model from learning an incorrect mapping between video frames and\nsound emitted by the object that is out of the screen. During testing, the\naudio forwarding regularizer is removed to ensure that REGNET can produce\npurely aligned sound only from visual features. Extensive evaluations based on\nAmazon Mechanical Turk demonstrate that our method significantly improves both\ntemporal and content-wise alignment. Remarkably, our generated sound can fool\nthe human with a 68.12% success rate. Code and pre-trained models are publicly\navailable at https://github.com/PeihaoChen/regnet\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 07:51:06 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Chen", "Peihao", ""], ["Zhang", "Yang", ""], ["Tan", "Mingkui", ""], ["Xiao", "Hongdong", ""], ["Huang", "Deng", ""], ["Gan", "Chuang", ""]]}, {"id": "2008.00821", "submitter": "Mohanad Abukmeil", "authors": "Mohanad Abukmeil and Gian Luca Marcialis", "title": "Experimental results on palmvein-based personal recognition by\n  multi-snapshot fusion of textural features", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we investigate multiple snapshot fusion of textural features\nfor palmvein recognition including identification and verification. Although\nthe literature proposed several approaches for palmvein recognition, the\npalmvein performance is still affected by identification and verification\nerrors. As well-known, palmveins are usually described by line-based methods\nwhich enhance the vein flow. This is claimed to be unique from person to\nperson. However, palmvein images are also characterized by texture that can be\npointed out by textural features, which relies on recent and efficient\nhand-crafted algorithms such as Local Binary Patterns, Local Phase\nQuantization, Local Tera Pattern, Local directional Pattern, and Binarized\nStatistical Image Features (LBP, LPQ, LTP, LDP and BSIF, respectively), among\nothers. Finally, they can be easily managed at feature-level fusion, when more\nthan one sample can be acquired for recognition. Therefore, multi-snapshot\nfusion can be adopted for exploiting these features complementarity. Our goal\nin this paper is to show that this is confirmed for palmvein recognition, thus\nallowing to achieve very high recognition rates on a well-known benchmark data\nset.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 11:34:46 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 12:00:52 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Abukmeil", "Mohanad", ""], ["Marcialis", "Gian Luca", ""]]}, {"id": "2008.00823", "submitter": "Yibing Song", "authors": "Yinglong Wang, Yibing Song, Chao Ma, and Bing Zeng", "title": "Rethinking Image Deraining via Rain Streaks and Vapors", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image deraining regards an input image as a fusion of a background\nimage, a transmission map, rain streaks, and atmosphere light. While advanced\nmodels are proposed for image restoration (i.e., background image generation),\nthey regard rain streaks with the same properties as background rather than\ntransmission medium. As vapors (i.e., rain streaks accumulation or fog-like\nrain) are conveyed in the transmission map to model the veiling effect, the\nfusion of rain streaks and vapors do not naturally reflect the rain image\nformation. In this work, we reformulate rain streaks as transmission medium\ntogether with vapors to model rain imaging. We propose an encoder-decoder CNN\nnamed as SNet to learn the transmission map of rain streaks. As rain streaks\nappear with various shapes and directions, we use ShuffleNet units within SNet\nto capture their anisotropic representations. As vapors are brought by rain\nstreaks, we propose a VNet containing spatial pyramid pooling (SSP) to predict\nthe transmission map of vapors in multi-scales based on that of rain streaks.\nMeanwhile, we use an encoder CNN named ANet to estimate atmosphere light. The\nSNet, VNet, and ANet are jointly trained to predict transmission maps and\natmosphere light for rain image restoration. Extensive experiments on the\nbenchmark datasets demonstrate the effectiveness of the proposed visual model\nto predict rain streaks and vapors. The proposed deraining method performs\nfavorably against state-of-the-art deraining approaches.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 12:15:07 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Wang", "Yinglong", ""], ["Song", "Yibing", ""], ["Ma", "Chao", ""], ["Zeng", "Bing", ""]]}, {"id": "2008.00825", "submitter": "Pradyumna Gupta", "authors": "Pradyumna Gupta, Himanshu Gupta, Aman Sinha", "title": "DSC IIT-ISM at SemEval-2020 Task 8: Bi-Fusion Techniques for Deep Meme\n  Emotion Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Memes have become an ubiquitous social media entity and the processing and\nanalysis of suchmultimodal data is currently an active area of research. This\npaper presents our work on theMemotion Analysis shared task of SemEval 2020,\nwhich involves the sentiment and humoranalysis of memes. We propose a system\nwhich uses different bimodal fusion techniques toleverage the inter-modal\ndependency for sentiment and humor classification tasks. Out of all\nourexperiments, the best system improved the baseline with macro F1 scores of\n0.357 on SentimentClassification (Task A), 0.510 on Humor Classification (Task\nB) and 0.312 on Scales of SemanticClasses (Task C).\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 17:23:35 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Gupta", "Pradyumna", ""], ["Gupta", "Himanshu", ""], ["Sinha", "Aman", ""]]}, {"id": "2008.00827", "submitter": "K Naveen Kumar", "authors": "Debaditya Roy, K. Naveen Kumar, C. Krishna Mohan", "title": "Defining Traffic States using Spatio-temporal Traffic Graphs", "comments": "Accepted in 23rd IEEE International Conference on Intelligent\n  Transportation Systems September 20 to 23, 2020. 6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intersections are one of the main sources of congestion and hence, it is\nimportant to understand traffic behavior at intersections. Particularly, in\ndeveloping countries with high vehicle density, mixed traffic type, and\nlane-less driving behavior, it is difficult to distinguish between congested\nand normal traffic behavior. In this work, we propose a way to understand the\ntraffic state of smaller spatial regions at intersections using traffic graphs.\nThe way these traffic graphs evolve over time reveals different traffic states\n- a) a congestion is forming (clumping), the congestion is dispersing\n(unclumping), or c) the traffic is flowing normally (neutral). We train a\nspatio-temporal deep network to identify these changes. Also, we introduce a\nlarge dataset called EyeonTraffic (EoT) containing 3 hours of aerial videos\ncollected at 3 busy intersections in Ahmedabad, India. Our experiments on the\nEoT dataset show that the traffic graphs can help in correctly identifying\ncongestion-prone behavior in different spatial regions of an intersection.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 17:27:52 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Roy", "Debaditya", ""], ["Kumar", "K. Naveen", ""], ["Mohan", "C. Krishna", ""]]}, {"id": "2008.00829", "submitter": "Abdul Mueed Hafiz Dr.", "authors": "Abdul Mueed Hafiz and Ghulam Mohiuddin Bhat", "title": "Deep Network Ensemble Learning applied to Image Classification using CNN\n  Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional machine learning approaches may fail to perform satisfactorily\nwhen dealing with complex data. In this context, the importance of data mining\nevolves w.r.t. building an efficient knowledge discovery and mining framework.\nEnsemble learning is aimed at integration of fusion, modeling and mining of\ndata into a unified model. However, traditional ensemble learning methods are\ncomplex and have optimization or tuning problems. In this paper, we propose a\nsimple, sequential, efficient, ensemble learning approach using multiple deep\nnetworks. The deep network used in the ensembles is ResNet50. The model draws\ninspiration from binary decision/classification trees. The proposed approach is\ncompared against the baseline viz. the single classifier approach i.e. using a\nsingle multiclass ResNet50 on the ImageNet and Natural Images datasets. Our\napproach outperforms the baseline on all experiments on the ImageNet dataset.\nCode is available in https://github.com/mueedhafiz1982/CNNTreeEnsemble.git\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 07:58:25 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Hafiz", "Abdul Mueed", ""], ["Bhat", "Ghulam Mohiuddin", ""]]}, {"id": "2008.00836", "submitter": "Qiao Liu", "authors": "Qiao Liu, Xin Li, Zhenyu He, Chenglong Li, Jun Li, Zikun Zhou, Di\n  Yuan, Jing Li, Kai Yang, Nana Fan, Feng Zheng", "title": "LSOTB-TIR:A Large-Scale High-Diversity Thermal Infrared Object Tracking\n  Benchmark", "comments": "accepted by ACM Mutlimedia Conference, 2020", "journal-ref": null, "doi": "10.1145/3394171.3413922", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we present a Large-Scale and high-diversity general Thermal\nInfraRed (TIR) Object Tracking Benchmark, called LSOTBTIR, which consists of an\nevaluation dataset and a training dataset with a total of 1,400 TIR sequences\nand more than 600K frames. We annotate the bounding box of objects in every\nframe of all sequences and generate over 730K bounding boxes in total. To the\nbest of our knowledge, LSOTB-TIR is the largest and most diverse TIR object\ntracking benchmark to date. To evaluate a tracker on different attributes, we\ndefine 4 scenario attributes and 12 challenge attributes in the evaluation\ndataset. By releasing LSOTB-TIR, we encourage the community to develop deep\nlearning based TIR trackers and evaluate them fairly and comprehensively. We\nevaluate and analyze more than 30 trackers on LSOTB-TIR to provide a series of\nbaselines, and the results show that deep trackers achieve promising\nperformance. Furthermore, we re-train several representative deep trackers on\nLSOTB-TIR, and their results demonstrate that the proposed training dataset\nsignificantly improves the performance of deep TIR trackers. Codes and dataset\nare available at https://github.com/QiaoLiuHit/LSOTB-TIR.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 12:36:06 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Liu", "Qiao", ""], ["Li", "Xin", ""], ["He", "Zhenyu", ""], ["Li", "Chenglong", ""], ["Li", "Jun", ""], ["Zhou", "Zikun", ""], ["Yuan", "Di", ""], ["Li", "Jing", ""], ["Yang", "Kai", ""], ["Fan", "Nana", ""], ["Zheng", "Feng", ""]]}, {"id": "2008.00859", "submitter": "Tianshui Chen", "authors": "Yuan Xie, Tianshui Chen, Tao Pu, Hefeng Wu, Liang Lin", "title": "Adversarial Graph Representation Adaptation for Cross-Domain Facial\n  Expression Recognition", "comments": "Accepted at ACM MM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data inconsistency and bias are inevitable among different facial expression\nrecognition (FER) datasets due to subjective annotating process and different\ncollecting conditions. Recent works resort to adversarial mechanisms that learn\ndomain-invariant features to mitigate domain shift. However, most of these\nworks focus on holistic feature adaptation, and they ignore local features that\nare more transferable across different datasets. Moreover, local features carry\nmore detailed and discriminative content for expression recognition, and thus\nintegrating local features may enable fine-grained adaptation. In this work, we\npropose a novel Adversarial Graph Representation Adaptation (AGRA) framework\nthat unifies graph representation propagation with adversarial learning for\ncross-domain holistic-local feature co-adaptation. To achieve this, we first\nbuild a graph to correlate holistic and local regions within each domain and\nanother graph to correlate these regions across different domains. Then, we\nlearn the per-class statistical distribution of each domain and extract\nholistic-local features from the input image to initialize the corresponding\ngraph nodes. Finally, we introduce two stacked graph convolution networks to\npropagate holistic-local feature within each domain to explore their\ninteraction and across different domains for holistic-local feature\nco-adaptation. In this way, the AGRA framework can adaptively learn\nfine-grained domain-invariant features and thus facilitate cross-domain\nexpression recognition. We conduct extensive and fair experiments on several\npopular benchmarks and show that the proposed AGRA framework achieves superior\nperformance over previous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 13:27:24 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 09:50:05 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Xie", "Yuan", ""], ["Chen", "Tianshui", ""], ["Pu", "Tao", ""], ["Wu", "Hefeng", ""], ["Lin", "Liang", ""]]}, {"id": "2008.00878", "submitter": "Gaurav Kumar Nayak", "authors": "Gaurav Kumar Nayak, Saksham Jain, R Venkatesh Babu, Anirban\n  Chakraborty", "title": "Fusion of Deep and Non-Deep Methods for Fast Super-Resolution of\n  Satellite Images", "comments": "Accepted in IEEE BigMM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the emerging commercial space industry there is a drastic increase in\naccess to low cost satellite imagery. The price for satellite images depends on\nthe sensor quality and revisit rate. This work proposes to bridge the gap\nbetween image quality and the price by improving the image quality via\nsuper-resolution (SR). Recently, a number of deep SR techniques have been\nproposed to enhance satellite images. However, none of these methods utilize\nthe region-level context information, giving equal importance to each region in\nthe image. This, along with the fact that most state-of-the-art SR methods are\ncomplex and cumbersome deep models, the time taken to process very large\nsatellite images can be impractically high. We, propose to handle this\nchallenge by designing an SR framework that analyzes the regional information\ncontent on each patch of the low-resolution image and judiciously chooses to\nuse more computationally complex deep models to super-resolve more\nstructure-rich regions on the image, while using less resource-intensive\nnon-deep methods on non-salient regions. Through extensive experiments on a\nlarge satellite image, we show substantial decrease in inference time while\nachieving similar performance to that of existing deep SR methods over several\nevaluation measures like PSNR, MSE and SSIM.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 13:55:39 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Nayak", "Gaurav Kumar", ""], ["Jain", "Saksham", ""], ["Babu", "R Venkatesh", ""], ["Chakraborty", "Anirban", ""]]}, {"id": "2008.00892", "submitter": "Shikun Liu", "authors": "Shikun Liu, Zhe Lin, Yilin Wang, Jianming Zhang, Federico Perazzi,\n  Edward Johns", "title": "Shape Adaptor: A Learnable Resizing Module", "comments": "Published at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a novel resizing module for neural networks: shape adaptor, a\ndrop-in enhancement built on top of traditional resizing layers, such as\npooling, bilinear sampling, and strided convolution. Whilst traditional\nresizing layers have fixed and deterministic reshaping factors, our module\nallows for a learnable reshaping factor. Our implementation enables shape\nadaptors to be trained end-to-end without any additional supervision, through\nwhich network architectures can be optimised for each individual task, in a\nfully automated way. We performed experiments across seven image classification\ndatasets, and results show that by simply using a set of our shape adaptors\ninstead of the original resizing layers, performance increases consistently\nover human-designed networks, across all datasets. Additionally, we show the\neffectiveness of shape adaptors on two other applications: network compression\nand transfer learning. The source code is available at:\nhttps://github.com/lorenmt/shape-adaptor.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 14:15:52 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 13:10:50 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Liu", "Shikun", ""], ["Lin", "Zhe", ""], ["Wang", "Yilin", ""], ["Zhang", "Jianming", ""], ["Perazzi", "Federico", ""], ["Johns", "Edward", ""]]}, {"id": "2008.00901", "submitter": "Zhiyang Liu", "authors": "Chao Chai, Pengchong Qiao, Bin Zhao, Huiying Wang, Guohua Liu, Hong\n  Wu, E Mark Haacke, Wen Shen, Chen Cao, Xinchen Ye, Zhiyang Liu, Shuang Xia", "title": "Automated Segmentation of Brain Gray Matter Nuclei on Quantitative\n  Susceptibility Mapping Using Deep Convolutional Neural Network", "comments": "submitted to IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abnormal iron accumulation in the brain subcortical nuclei has been reported\nto be correlated to various neurodegenerative diseases, which can be measured\nthrough the magnetic susceptibility from the quantitative susceptibility\nmapping (QSM). To quantitively measure the magnetic susceptibility, the nuclei\nshould be accurately segmented, which is a tedious task for clinicians. In this\npaper, we proposed a double-branch residual-structured U-Net (DB-ResUNet) based\non 3D convolutional neural network (CNN) to automatically segment such brain\ngray matter nuclei. To better tradeoff between segmentation accuracy and the\nmemory efficiency, the proposed DB-ResUNet fed image patches with high\nresolution and the patches with low resolution but larger field of view into\nthe local and global branches, respectively. Experimental results revealed that\nby jointly using QSM and T$_\\text{1}$ weighted imaging (T$_\\text{1}$WI) as\ninputs, the proposed method was able to achieve better segmentation accuracy\nover its single-branch counterpart, as well as the conventional atlas-based\nmethod and the classical 3D-UNet structure. The susceptibility values and the\nvolumes were also measured, which indicated that the measurements from the\nproposed DB-ResUNet are able to present high correlation with values from the\nmanually annotated regions of interest.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 14:32:30 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Chai", "Chao", ""], ["Qiao", "Pengchong", ""], ["Zhao", "Bin", ""], ["Wang", "Huiying", ""], ["Liu", "Guohua", ""], ["Wu", "Hong", ""], ["Haacke", "E Mark", ""], ["Shen", "Wen", ""], ["Cao", "Chen", ""], ["Ye", "Xinchen", ""], ["Liu", "Zhiyang", ""], ["Xia", "Shuang", ""]]}, {"id": "2008.00910", "submitter": "Sadegh Etemad", "authors": "Sadegh Etemad, Maryam Amirmazlaghani", "title": "Color Texture Image Retrieval Based on Copula Multivariate Modeling in\n  the Shearlet Domain", "comments": "37 pages, 16 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a color texture image retrieval framework is proposed based on\nShearlet domain modeling using Copula multivariate model. In the proposed\nframework, Gaussian Copula is used to model the dependencies between different\nsub-bands of the Non Subsample Shearlet Transform (NSST) and non-Gaussian\nmodels are used for marginal modeling of the coefficients. Six different\nschemes are proposed for modeling NSST coefficients based on the four types of\nneighboring defined; moreover, Kullback Leibler Divergence(KLD) close form is\ncalculated in different situations for the two Gaussian Copula and non Gaussian\nfunctions in order to investigate the similarities in the proposed retrieval\nframework. The Jeffery divergence (JD) criterion, which is a symmetrical\nversion of KLD, is used for investigating similarities in the proposed\nframework. We have implemented our experiments on four texture image retrieval\nbenchmark datasets, the results of which show the superiority of the proposed\nframework over the existing state-of-the-art methods. In addition, the\nretrieval time of the proposed framework is also analyzed in the two steps of\nfeature extraction and similarity matching, which also shows that the proposed\nframework enjoys an appropriate retrieval time.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 14:40:27 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Etemad", "Sadegh", ""], ["Amirmazlaghani", "Maryam", ""]]}, {"id": "2008.00916", "submitter": "Jonathan Williford", "authors": "Jonathan R. Williford, Brandon B. May, Jeffrey Byrne", "title": "Explainable Face Recognition", "comments": "To appear in the Proceedings of ECCV 2020. Project page at\n  https://stresearch.github.io/xfr", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Explainable face recognition is the problem of explaining why a facial\nmatcher matches faces. In this paper, we provide the first comprehensive\nbenchmark and baseline evaluation for explainable face recognition. We define a\nnew evaluation protocol called the ``inpainting game'', which is a curated set\nof 3648 triplets (probe, mate, nonmate) of 95 subjects, which differ by\nsynthetically inpainting a chosen facial characteristic like the nose, eyebrows\nor mouth creating an inpainted nonmate. An explainable face matcher is tasked\nwith generating a network attention map which best explains which regions in a\nprobe image match with a mated image, and not with an inpainted nonmate for\neach triplet. This provides ground truth for quantifying what image regions\ncontribute to face matching. Furthermore, we provide a comprehensive benchmark\non this dataset comparing five state of the art methods for network attention\nin face recognition on three facial matchers. This benchmark includes two new\nalgorithms for network attention called subtree EBP and Density-based Input\nSampling for Explanation (DISE) which outperform the state of the art by a wide\nmargin. Finally, we show qualitative visualization of these network attention\ntechniques on novel images, and explore how these explainable face recognition\nmodels can improve transparency and trust for facial matchers.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 14:47:51 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Williford", "Jonathan R.", ""], ["May", "Brandon B.", ""], ["Byrne", "Jeffrey", ""]]}, {"id": "2008.00923", "submitter": "Tianshui Chen", "authors": "Tianshui Chen, Tao Pu, Hefeng Wu, Yuan Xie, Lingbo Liu, Liang Lin", "title": "Cross-Domain Facial Expression Recognition: A Unified Evaluation\n  Benchmark and Adversarial Graph Learning", "comments": "Extension of our ACM MM 2020 paper. arXiv admin note: substantial\n  text overlap with arXiv:2008.00859", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address the problem of data inconsistencies among different facial\nexpression recognition (FER) datasets, many cross-domain FER methods (CD-FERs)\nhave been extensively devised in recent years. Although each declares to\nachieve superior performance, fair comparisons are lacking due to the\ninconsistent choices of the source/target datasets and feature extractors. In\nthis work, we first analyze the performance effect caused by these inconsistent\nchoices, and then re-implement some well-performing CD-FER and recently\npublished domain adaptation algorithms. We ensure that all these algorithms\nadopt the same source datasets and feature extractors for fair CD-FER\nevaluations. We find that most of the current leading algorithms use\nadversarial learning to learn holistic domain-invariant features to mitigate\ndomain shifts. However, these algorithms ignore local features, which are more\ntransferable across different datasets and carry more detailed content for\nfine-grained adaptation. To address these issues, we integrate graph\nrepresentation propagation with adversarial learning for cross-domain\nholistic-local feature co-adaptation by developing a novel adversarial graph\nrepresentation adaptation (AGRA) framework. Specifically, it first builds two\ngraphs to correlate holistic and local regions within each domain and across\ndifferent domains, respectively. Then, it extracts holistic-local features from\nthe input image and uses learnable per-class statistical distributions to\ninitialize the corresponding graph nodes. Finally, two stacked graph\nconvolution networks (GCNs) are adopted to propagate holistic-local features\nwithin each domain to explore their interaction and across different domains\nfor holistic-local feature co-adaptation. We conduct extensive and fair\nevaluations on several popular benchmarks and show that the proposed AGRA\nframework outperforms previous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 15:00:31 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 09:39:43 GMT"}, {"version": "v3", "created": "Thu, 27 Aug 2020 06:40:29 GMT"}, {"version": "v4", "created": "Wed, 23 Sep 2020 09:05:32 GMT"}, {"version": "v5", "created": "Sat, 24 Oct 2020 03:10:00 GMT"}, {"version": "v6", "created": "Thu, 29 Oct 2020 08:19:33 GMT"}, {"version": "v7", "created": "Tue, 15 Jun 2021 09:59:21 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Chen", "Tianshui", ""], ["Pu", "Tao", ""], ["Wu", "Hefeng", ""], ["Xie", "Yuan", ""], ["Liu", "Lingbo", ""], ["Lin", "Liang", ""]]}, {"id": "2008.00928", "submitter": "Piyush Yadav", "authors": "Piyush Yadav, Dipto Sarkar, Dhaval Salwala, Edward Curry", "title": "Traffic Prediction Framework for OpenStreetMap using Deep Learning based\n  Complex Event Processing and Open Traffic Cameras", "comments": "16 pages, 9 Figures, 3 Tables, Paper accepted in GIScience 2020 (now\n  postponed to 2021)", "journal-ref": null, "doi": "10.4230/LIPIcs.GIScience.2021.I.17", "report-no": null, "categories": "cs.CV cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Displaying near-real-time traffic information is a useful feature of digital\nnavigation maps. However, most commercial providers rely on\nprivacy-compromising measures such as deriving location information from\ncellphones to estimate traffic. The lack of an open-source traffic estimation\nmethod using open data platforms is a bottleneck for building sophisticated\nnavigation services on top of OpenStreetMap (OSM). We propose a deep\nlearning-based Complex Event Processing (CEP) method that relies on publicly\navailable video camera streams for traffic estimation. The proposed framework\nperforms near-real-time object detection and objects property extraction across\ncamera clusters in parallel to derive multiple measures related to traffic with\nthe results visualized on OpenStreetMap. The estimation of object properties\n(e.g. vehicle speed, count, direction) provides multidimensional data that can\nbe leveraged to create metrics and visualization for congestion beyond commonly\nused density-based measures. Our approach couples both flow and count measures\nduring interpolation by considering each vehicle as a sample point and their\nspeed as weight. We demonstrate multidimensional traffic metrics (e.g. flow\nrate, congestion estimation) over OSM by processing 22 traffic cameras from\nLondon streets. The system achieves a near-real-time performance of 1.42\nseconds median latency and an average F-score of 0.80.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 17:10:43 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Yadav", "Piyush", ""], ["Sarkar", "Dipto", ""], ["Salwala", "Dhaval", ""], ["Curry", "Edward", ""]]}, {"id": "2008.00930", "submitter": "YangQuan Chen Prof.", "authors": "Jairo Viola, YangQuan Chen and Jing Wang", "title": "FaultFace: Deep Convolutional Generative Adversarial Network (DCGAN)\n  based Ball-Bearing Failure Detection Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Failure detection is employed in the industry to improve system performance\nand reduce costs due to unexpected malfunction events. So, a good dataset of\nthe system is desirable for designing an automated failure detection system.\nHowever, industrial process datasets are unbalanced and contain little\ninformation about failure behavior due to the uniqueness of these events and\nthe high cost for running the system just to get information about the\nundesired behaviors. For this reason, performing correct training and\nvalidation of automated failure detection methods is challenging. This paper\nproposes a methodology called FaultFace for failure detection on Ball-Bearing\njoints for rotational shafts using deep learning techniques to create balanced\ndatasets. The FaultFace methodology uses 2D representations of vibration\nsignals denominated faceportraits obtained by time-frequency transformation\ntechniques. From the obtained faceportraits, a Deep Convolutional Generative\nAdversarial Network is employed to produce new faceportraits of the nominal and\nfailure behaviors to get a balanced dataset. A Convolutional Neural Network is\ntrained for fault detection employing the balanced dataset. The FaultFace\nmethodology is compared with other deep learning techniques to evaluate its\nperformance in for fault detection with unbalanced datasets. Obtained results\nshow that FaultFace methodology has a good performance for failure detection\nfor unbalanced datasets.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 06:37:53 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Viola", "Jairo", ""], ["Chen", "YangQuan", ""], ["Wang", "Jing", ""]]}, {"id": "2008.00932", "submitter": "Hacer Yalim Keles", "authors": "Ozge Mercanoglu Sincan and Hacer Yalim Keles", "title": "AUTSL: A Large Scale Multi-modal Turkish Sign Language Dataset and\n  Baseline Methods", "comments": "Preprint of the accepted paper at IEEE Access Journal. The revised\n  version contains empirical results with Montalbano dataset, in addition to\n  AUTSL. The abstract is revised accordingly", "journal-ref": "IEEE Access (2020), vol. 8, pp. 181340-181355", "doi": "10.1109/ACCESS.2020.3028072", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign language recognition is a challenging problem where signs are identified\nby simultaneous local and global articulations of multiple sources, i.e. hand\nshape and orientation, hand movements, body posture, and facial expressions.\nSolving this problem computationally for a large vocabulary of signs in real\nlife settings is still a challenge, even with the state-of-the-art models. In\nthis study, we present a new largescale multi-modal Turkish Sign Language\ndataset (AUTSL) with a benchmark and provide baseline models for performance\nevaluations. Our dataset consists of 226 signs performed by 43 different\nsigners and 38,336 isolated sign video samples in total. Samples contain a wide\nvariety of backgrounds recorded in indoor and outdoor environments. Moreover,\nspatial positions and the postures of signers also vary in the recordings. Each\nsample is recorded with Microsoft Kinect v2 and contains RGB, depth, and\nskeleton modalities. We prepared benchmark training and test sets for user\nindependent assessments of the models. We trained several deep learning based\nmodels and provide empirical evaluations using the benchmark; we used CNNs to\nextract features, unidirectional and bidirectional LSTM models to characterize\ntemporal information. We also incorporated feature pooling modules and temporal\nattention to our models to improve the performances. We evaluated our baseline\nmodels on AUTSL and Montalbano datasets. Our models achieved competitive\nresults with the state-of-the-art methods on Montalbano dataset, i.e. 96.11%\naccuracy. In AUTSL random train-test splits, our models performed up to 95.95%\naccuracy. In the proposed user-independent benchmark dataset our best baseline\nmodel achieved 62.02% accuracy. The gaps in the performances of the same\nbaseline models show the challenges inherent in our benchmark dataset. AUTSL\nbenchmark dataset is publicly available at https://cvml.ankara.edu.tr.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 15:12:05 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 10:31:48 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Sincan", "Ozge Mercanoglu", ""], ["Keles", "Hacer Yalim", ""]]}, {"id": "2008.00936", "submitter": "Duygu Sarikaya", "authors": "Duygu Sarikaya, Jason J. Corso and Khurshid A. Guru", "title": "Detection and Localization of Robotic Tools in Robot-Assisted Surgery\n  Videos Using Deep Neural Networks for Region Proposal and Detection", "comments": null, "journal-ref": "IEEE Transactions on Medical Imaging 36 (2017) 1542-1549", "doi": "10.1109/TMI.2017.2665671", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video understanding of robot-assisted surgery (RAS) videos is an active\nresearch area. Modeling the gestures and skill level of surgeons presents an\ninteresting problem. The insights drawn may be applied in effective skill\nacquisition, objective skill assessment, real-time feedback, and human-robot\ncollaborative surgeries. We propose a solution to the tool detection and\nlocalization open problem in RAS video understanding, using a strictly computer\nvision approach and the recent advances of deep learning. We propose an\narchitecture using multimodal convolutional neural networks for fast detection\nand localization of tools in RAS videos. To our knowledge, this approach will\nbe the first to incorporate deep neural networks for tool detection and\nlocalization in RAS videos. Our architecture applies a Region Proposal Network\n(RPN), and a multi-modal two stream convolutional network for object detection,\nto jointly predict objectness and localization on a fusion of image and\ntemporal motion cues. Our results with an Average Precision (AP) of 91% and a\nmean computation time of 0.1 seconds per test frame detection indicate that our\nstudy is superior to conventionally used methods for medical imaging while also\nemphasizing the benefits of using RPN for precision and efficiency. We also\nintroduce a new dataset, ATLAS Dione, for RAS video understanding. Our dataset\nprovides video data of ten surgeons from Roswell Park Cancer Institute (RPCI)\n(Buffalo, NY) performing six different surgical tasks on the daVinci Surgical\nSystem (dVSS R ) with annotations of robotic tools per frame.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 10:59:15 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Sarikaya", "Duygu", ""], ["Corso", "Jason J.", ""], ["Guru", "Khurshid A.", ""]]}, {"id": "2008.00942", "submitter": "Mingkui Tan", "authors": "Jiezhang Cao, Yong Guo, Qingyao Wu, Chunhua Shen, Junzhou Huang,\n  Mingkui Tan", "title": "Improving Generative Adversarial Networks with Local Coordinate Coding", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have shown remarkable success in\ngenerating realistic data from some predefined prior distribution (e.g.,\nGaussian noises). However, such prior distribution is often independent of real\ndata and thus may lose semantic information (e.g., geometric structure or\ncontent in images) of data. In practice, the semantic information might be\nrepresented by some latent distribution learned from data. However, such latent\ndistribution may incur difficulties in data sampling for GANs. In this paper,\nrather than sampling from the predefined prior distribution, we propose an\nLCCGAN model with local coordinate coding (LCC) to improve the performance of\ngenerating data. First, we propose an LCC sampling method in LCCGAN to sample\nmeaningful points from the latent manifold. With the LCC sampling method, we\ncan exploit the local information on the latent manifold and thus produce new\ndata with promising quality. Second, we propose an improved version, namely\nLCCGAN++, by introducing a higher-order term in the generator approximation.\nThis term is able to achieve better approximation and thus further improve the\nperformance. More critically, we derive the generalization bound for both\nLCCGAN and LCCGAN++ and prove that a low-dimensional input is sufficient to\nachieve good generalization performance. Extensive experiments on four\nbenchmark datasets demonstrate the superiority of the proposed method over\nexisting GANs.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 09:17:50 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Cao", "Jiezhang", ""], ["Guo", "Yong", ""], ["Wu", "Qingyao", ""], ["Shen", "Chunhua", ""], ["Huang", "Junzhou", ""], ["Tan", "Mingkui", ""]]}, {"id": "2008.00947", "submitter": "Ting Yao", "authors": "Yingwei Pan and Jun Xu and Yehao Li and Ting Yao and Tao Mei", "title": "Pre-training for Video Captioning Challenge 2020 Summary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Pre-training for Video Captioning Challenge 2020 Summary: results and\nchallenge participants' technical reports.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 15:31:27 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Pan", "Yingwei", ""], ["Xu", "Jun", ""], ["Li", "Yehao", ""], ["Yao", "Ting", ""], ["Mei", "Tao", ""]]}, {"id": "2008.00948", "submitter": "Manuel Rebol", "authors": "Manuel Rebol, Patrick Kn\\\"obelreiter", "title": "Frame-To-Frame Consistent Semantic Segmentation", "comments": "ACVRW20", "journal-ref": null, "doi": "10.3217/978-3-85125-752-6-18", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we aim for temporally consistent semantic segmentation\nthroughout frames in a video. Many semantic segmentation algorithms process\nimages individually which leads to an inconsistent scene interpretation due to\nillumination changes, occlusions and other variations over time. To achieve a\ntemporally consistent prediction, we train a convolutional neural network (CNN)\nwhich propagates features through consecutive frames in a video using a\nconvolutional long short term memory (ConvLSTM) cell. Besides the temporal\nfeature propagation, we penalize inconsistencies in our loss function. We show\nin our experiments that the performance improves when utilizing video\ninformation compared to single frame prediction. The mean intersection over\nunion (mIoU) metric on the Cityscapes validation set increases from 45.2 % for\nthe single frames to 57.9 % for video data after implementing the ConvLSTM to\npropagate features trough time on the ESPNet. Most importantly, inconsistency\ndecreases from 4.5 % to 1.3 % which is a reduction by 71.1 %. Our results\nindicate that the added temporal information produces a frame-to-frame\nconsistent and more accurate image understanding compared to single frame\nprocessing. Code and videos are available at\nhttps://github.com/mrebol/f2f-consistent-semantic-segmentation\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 15:28:40 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 22:24:56 GMT"}, {"version": "v3", "created": "Thu, 27 Aug 2020 18:14:38 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Rebol", "Manuel", ""], ["Kn\u00f6belreiter", "Patrick", ""]]}, {"id": "2008.00951", "submitter": "Elad Richardson", "authors": "Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar,\n  Stav Shapiro, Daniel Cohen-Or", "title": "Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation", "comments": "Accepted to CVPR 2021, project page available at\n  https://eladrich.github.io/pixel2style2pixel/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generic image-to-image translation framework, pixel2style2pixel\n(pSp). Our pSp framework is based on a novel encoder network that directly\ngenerates a series of style vectors which are fed into a pretrained StyleGAN\ngenerator, forming the extended W+ latent space. We first show that our encoder\ncan directly embed real images into W+, with no additional optimization. Next,\nwe propose utilizing our encoder to directly solve image-to-image translation\ntasks, defining them as encoding problems from some input domain into the\nlatent domain. By deviating from the standard invert first, edit later\nmethodology used with previous StyleGAN encoders, our approach can handle a\nvariety of tasks even when the input image is not represented in the StyleGAN\ndomain. We show that solving translation tasks through StyleGAN significantly\nsimplifies the training process, as no adversary is required, has better\nsupport for solving tasks without pixel-to-pixel correspondence, and inherently\nsupports multi-modal synthesis via the resampling of styles. Finally, we\ndemonstrate the potential of our framework on a variety of facial\nimage-to-image translation tasks, even when compared to state-of-the-art\nsolutions designed specifically for a single task, and further show that it can\nbe extended beyond the human facial domain.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 15:30:38 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 12:53:36 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Richardson", "Elad", ""], ["Alaluf", "Yuval", ""], ["Patashnik", "Or", ""], ["Nitzan", "Yotam", ""], ["Azar", "Yaniv", ""], ["Shapiro", "Stav", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2008.00962", "submitter": "Lucas Tabelini Torres", "authors": "Lucas Tabelini, Rodrigo Berriel, Thiago M. Paix\\~ao, Alberto F. De\n  Souza, Claudine Badue, Nicu Sebe and Thiago Oliveira-Santos", "title": "Deep Traffic Sign Detection and Recognition Without Target Domain Real\n  Images", "comments": "arXiv admin note: text overlap with arXiv:1907.09679", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been successfully applied to several problems related to\nautonomous driving, often relying on large databases of real target-domain\nimages for proper training. The acquisition of such real-world data is not\nalways possible in the self-driving context, and sometimes their annotation is\nnot feasible. Moreover, in many tasks, there is an intrinsic data imbalance\nthat most learning-based methods struggle to cope with. Particularly, traffic\nsign detection is a challenging problem in which these three issues are seen\naltogether. To address these challenges, we propose a novel database generation\nmethod that requires only (i) arbitrary natural images, i.e., requires no real\nimage from the target-domain, and (ii) templates of the traffic signs. The\nmethod does not aim at overcoming the training with real data, but to be a\ncompatible alternative when the real data is not available. The effortlessly\ngenerated database is shown to be effective for the training of a deep detector\non traffic signs from multiple countries. On large data sets, training with a\nfully synthetic data set almost matches the performance of training with a real\none. When compared to training with a smaller data set of real images, training\nwith synthetic images increased the accuracy by 12.25%. The proposed method\nalso improves the performance of the detector when target-domain data are\navailable.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 21:06:47 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Tabelini", "Lucas", ""], ["Berriel", "Rodrigo", ""], ["Paix\u00e3o", "Thiago M.", ""], ["De Souza", "Alberto F.", ""], ["Badue", "Claudine", ""], ["Sebe", "Nicu", ""], ["Oliveira-Santos", "Thiago", ""]]}, {"id": "2008.00965", "submitter": "Bingyao Huang", "authors": "Bingyao Huang, Tao Sun, Haibin Ling", "title": "End-to-end Full Projector Compensation", "comments": "Source code: https://github.com/BingyaoHuang/CompenNeSt-plusplus.\n  arXiv admin note: text overlap with arXiv:1908.06246, arXiv:1904.04335", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3050124", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Full projector compensation aims to modify a projector input image to\ncompensate for both geometric and photometric disturbance of the projection\nsurface. Traditional methods usually solve the two parts separately and may\nsuffer from suboptimal solutions. In this paper, we propose the first\nend-to-end differentiable solution, named CompenNeSt++, to solve the two\nproblems jointly. First, we propose a novel geometric correction subnet, named\nWarpingNet, which is designed with a cascaded coarse-to-fine structure to learn\nthe sampling grid directly from sampling images. Second, we propose a novel\nphotometric compensation subnet, named CompenNeSt, which is designed with a\nsiamese architecture to capture the photometric interactions between the\nprojection surface and the projected images, and to use such information to\ncompensate the geometrically corrected images. By concatenating WarpingNet with\nCompenNeSt, CompenNeSt++ accomplishes full projector compensation and is\nend-to-end trainable. Third, to improve practicability, we propose a novel\nsynthetic data-based pre-training strategy to significantly reduce the number\nof training images and training time. Moreover, we construct the first\nsetup-independent full compensation benchmark to facilitate future studies. In\nthorough experiments, our method shows clear advantages over prior art with\npromising compensation quality and meanwhile being practically convenient.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 18:23:52 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 01:15:58 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2021 18:49:49 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Huang", "Bingyao", ""], ["Sun", "Tao", ""], ["Ling", "Haibin", ""]]}, {"id": "2008.00975", "submitter": "Ting Yao", "authors": "Ting Yao and Yiheng Zhang and Zhaofan Qiu and Yingwei Pan and Tao Mei", "title": "SeCo: Exploring Sequence Supervision for Unsupervised Representation\n  Learning", "comments": "AAAI 2021; Code is publicly available at:\n  https://github.com/YihengZhang-CV/SeCo-Sequence-Contrastive-Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A steady momentum of innovations and breakthroughs has convincingly pushed\nthe limits of unsupervised image representation learning. Compared to static 2D\nimages, video has one more dimension (time). The inherent supervision existing\nin such sequential structure offers a fertile ground for building unsupervised\nlearning models. In this paper, we compose a trilogy of exploring the basic and\ngeneric supervision in the sequence from spatial, spatiotemporal and sequential\nperspectives. We materialize the supervisory signals through determining\nwhether a pair of samples is from one frame or from one video, and whether a\ntriplet of samples is in the correct temporal order. We uniquely regard the\nsignals as the foundation in contrastive learning and derive a particular form\nnamed Sequence Contrastive Learning (SeCo). SeCo shows superior results under\nthe linear protocol on action recognition (Kinetics), untrimmed activity\nrecognition (ActivityNet) and object tracking (OTB-100). More remarkably, SeCo\ndemonstrates considerable improvements over recent unsupervised pre-training\ntechniques, and leads the accuracy by 2.96% and 6.47% against fully-supervised\nImageNet pre-training in action recognition task on UCF101 and HMDB51,\nrespectively. Source code is available at\n\\url{https://github.com/YihengZhang-CV/SeCo-Sequence-Contrastive-Learning}.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 15:51:35 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 17:25:22 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Yao", "Ting", ""], ["Zhang", "Yiheng", ""], ["Qiu", "Zhaofan", ""], ["Pan", "Yingwei", ""], ["Mei", "Tao", ""]]}, {"id": "2008.00992", "submitter": "Matteo Dunnhofer", "authors": "Matteo Dunnhofer, Niki Martinel, Christian Micheloni", "title": "An Exploration of Target-Conditioned Segmentation Methods for Visual\n  Object Trackers", "comments": "European Conference on Computer Vision (ECCV) 2020, Visual Object\n  Tracking Challenge VOT2020 workshop", "journal-ref": null, "doi": "10.1007/978-3-030-68238-5_41", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual object tracking is the problem of predicting a target object's state\nin a video. Generally, bounding-boxes have been used to represent states, and a\nsurge of effort has been spent by the community to produce efficient causal\nalgorithms capable of locating targets with such representations. As the field\nis moving towards binary segmentation masks to define objects more precisely,\nin this paper we propose to extensively explore target-conditioned segmentation\nmethods available in the computer vision community, in order to transform any\nbounding-box tracker into a segmentation tracker. Our analysis shows that such\nmethods allow trackers to compete with recently proposed segmentation trackers,\nwhile performing quasi real-time.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 16:21:18 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 14:17:19 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Dunnhofer", "Matteo", ""], ["Martinel", "Niki", ""], ["Micheloni", "Christian", ""]]}, {"id": "2008.00997", "submitter": "Miquel Mir\\'o-Nicolau", "authors": "Miquel Mir\\'o-Nicolau, Biel Moy\\`a-Alcover, Manuel Gonz\\'alez-Hidalgo\n  and Antoni Jaume-i-Cap\\'o", "title": "Segmenting overlapped cell clusters in biomedical images by concave\n  point detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a method to detect concave points as a first step to\nsegment overlapped objects on images. Given an image of an object cluster we\ncompute the curvature on each point of its contour. Then, we select regions\nwith the highest probability to contain an interest point, that is, regions\nwith higher curvature. Finally we obtain an interest point from each region and\nwe classify them between convex and concave. In order to evaluate the quality\nof the concave point detection algorithm we constructed a synthetic dataset to\nsimulate overlapping objects, providing the position of the concave points as a\nground truth. As a case study, the performance of a well-known application is\nevaluated, such as the splitting of overlapped cells in images of peripheral\nblood smears samples of patients with sickle cell anaemia. We used the proposed\nmethod to detect the concave points in clusters of cells and then we separate\nthis clusters by ellipse fitting. Experimentally we demonstrate that our\nproposal has a better performance than the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 16:32:49 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 11:29:20 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Mir\u00f3-Nicolau", "Miquel", ""], ["Moy\u00e0-Alcover", "Biel", ""], ["Gonz\u00e1lez-Hidalgo", "Manuel", ""], ["Jaume-i-Cap\u00f3", "Antoni", ""]]}, {"id": "2008.01003", "submitter": "Radu Tudor Ionescu", "authors": "Mariana-Iuliana Georgescu, Radu Tudor Ionescu", "title": "Teacher-Student Training and Triplet Loss for Facial Expression\n  Recognition under Occlusion", "comments": "Accepted at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the task of facial expression recognition under\nstrong occlusion. We are particularly interested in cases where 50% of the face\nis occluded, e.g. when the subject wears a Virtual Reality (VR) headset. While\nprevious studies show that pre-training convolutional neural networks (CNNs) on\nfully-visible (non-occluded) faces improves the accuracy, we propose to employ\nknowledge distillation to achieve further improvements. First of all, we employ\nthe classic teacher-student training strategy, in which the teacher is a CNN\ntrained on fully-visible faces and the student is a CNN trained on occluded\nfaces. Second of all, we propose a new approach for knowledge distillation\nbased on triplet loss. During training, the goal is to reduce the distance\nbetween an anchor embedding, produced by a student CNN that takes occluded\nfaces as input, and a positive embedding (from the same class as the anchor),\nproduced by a teacher CNN trained on fully-visible faces, so that it becomes\nsmaller than the distance between the anchor and a negative embedding (from a\ndifferent class than the anchor), produced by the student CNN. Third of all, we\npropose to combine the distilled embeddings obtained through the classic\nteacher-student strategy and our novel teacher-student strategy based on\ntriplet loss into a single embedding vector. We conduct experiments on two\nbenchmarks, FER+ and AffectNet, with two CNN architectures, VGG-f and VGG-face,\nshowing that knowledge distillation can bring significant improvements over the\nstate-of-the-art methods designed for occluded faces in the VR setting.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 16:41:19 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 18:54:30 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Georgescu", "Mariana-Iuliana", ""], ["Ionescu", "Radu Tudor", ""]]}, {"id": "2008.01018", "submitter": "Antoine Miech", "authors": "Antoine Miech, Jean-Baptiste Alayrac, Ivan Laptev, Josef Sivic, Andrew\n  Zisserman", "title": "RareAct: A video dataset of unusual interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a manually annotated video dataset of unusual actions,\nnamely RareAct, including actions such as \"blend phone\", \"cut keyboard\" and\n\"microwave shoes\". RareAct aims at evaluating the zero-shot and few-shot\ncompositionality of action recognition models for unlikely compositions of\ncommon action verbs and object nouns. It contains 122 different actions which\nwere obtained by combining verbs and nouns rarely co-occurring together in the\nlarge-scale textual corpus from HowTo100M, but that frequently appear\nseparately. We provide benchmarks using a state-of-the-art HowTo100M pretrained\nvideo and text model and show that zero-shot and few-shot compositionality of\nactions remains a challenging and unsolved task.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 16:53:35 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Miech", "Antoine", ""], ["Alayrac", "Jean-Baptiste", ""], ["Laptev", "Ivan", ""], ["Sivic", "Josef", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2008.01023", "submitter": "Yu Han", "authors": "Yu Han, Shuai Yang, Wenjing Wang, Jiaying Liu", "title": "From Design Draft to Real Attire: Unaligned Fashion Image Translation", "comments": "Accepted by ACMMM 2020. Our project website is available at:\n  https://victoriahy.github.io/MM2020/", "journal-ref": null, "doi": "10.1145/3394171.3413953", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion manipulation has attracted growing interest due to its great\napplication value, which inspires many researches towards fashion images.\nHowever, little attention has been paid to fashion design draft. In this paper,\nwe study a new unaligned translation problem between design drafts and real\nfashion items, whose main challenge lies in the huge misalignment between the\ntwo modalities. We first collect paired design drafts and real fashion item\nimages without pixel-wise alignment. To solve the misalignment problem, our\nmain idea is to train a sampling network to adaptively adjust the input to an\nintermediate state with structure alignment to the output. Moreover, built upon\nthe sampling network, we present design draft to real fashion item translation\nnetwork (D2RNet), where two separate translation streams that focus on texture\nand shape, respectively, are combined tactfully to get both benefits. D2RNet is\nable to generate realistic garments with both texture and shape consistency to\ntheir design drafts. We show that this idea can be effectively applied to the\nreverse translation problem and present R2DNet accordingly. Extensive\nexperiments on unaligned fashion design translation demonstrate the superiority\nof our method over state-of-the-art methods. Our project website is available\nat: https://victoriahy.github.io/MM2020/ .\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 17:03:11 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 03:02:40 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2020 12:27:01 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Han", "Yu", ""], ["Yang", "Shuai", ""], ["Wang", "Wenjing", ""], ["Liu", "Jiaying", ""]]}, {"id": "2008.01034", "submitter": "Adrian Lopez Rodriguez", "authors": "Adrian Lopez-Rodriguez and Benjamin Busam and Krystian Mikolajczyk", "title": "Project to Adapt: Domain Adaptation for Depth Completion from Noisy and\n  Sparse Sensor Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth completion aims to predict a dense depth map from a sparse depth input.\nThe acquisition of dense ground truth annotations for depth completion settings\ncan be difficult and, at the same time, a significant domain gap between real\nLiDAR measurements and synthetic data has prevented from successful training of\nmodels in virtual settings. We propose a domain adaptation approach for\nsparse-to-dense depth completion that is trained from synthetic data, without\nannotations in the real domain or additional sensors. Our approach simulates\nthe real sensor noise in an RGB+LiDAR set-up, and consists of three modules:\nsimulating the real LiDAR input in the synthetic domain via projections,\nfiltering the real noisy LiDAR for supervision and adapting the synthetic RGB\nimage using a CycleGAN approach. We extensively evaluate these modules against\nthe state-of-the-art in the KITTI depth completion benchmark, showing\nsignificant improvements.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 17:21:57 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 14:46:40 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Lopez-Rodriguez", "Adrian", ""], ["Busam", "Benjamin", ""], ["Mikolajczyk", "Krystian", ""]]}, {"id": "2008.01053", "submitter": "Jannis Walk", "authors": "Jannis Walk, Niklas K\\\"uhl and Jonathan Sch\\\"afer", "title": "Towards Leveraging End-of-Life Tools as an Asset: Value Co-Creation\n  based on Deep Learning in the Machining Industry", "comments": "Proceedings of the 53rd Hawaii International Conference on System\n  Sciences | 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Sustainability is the key concept in the management of products that reached\ntheir end-of-life. We propose that end-of-life products have -- besides their\nvalue as recyclable assets -- additional value for producer and consumer. We\nargue this is especially true for the machining industry, where we illustrate\nan automatic characterization of worn cutting tools to foster value co-creation\nbetween tool manufacturer and tool user (customer) in the future. In the work\nat hand, we present a deep-learning-based computer vision system for the\nautomatic classification of worn tools regarding flank wear and chipping. The\nresulting Matthews Correlation Coefficient of 0.878 and 0.644 confirms the\nfeasibility of our system based on the VGG-16 network and Gradient Boosting.\nBased on these first results we derive a research agenda which addresses the\nneed for a more holistic tool characterization by semantic segmentation and\nassesses the perceived business impact and usability by different user groups.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 07:06:57 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Walk", "Jannis", ""], ["K\u00fchl", "Niklas", ""], ["Sch\u00e4fer", "Jonathan", ""]]}, {"id": "2008.01057", "submitter": "Jiawei Chen", "authors": "Jiawei Chen, Jenson Hsiao, Chiu Man Ho", "title": "Residual Frames with Efficient Pseudo-3D CNN for Human Action\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human action recognition is regarded as a key cornerstone in domains such as\nsurveillance or video understanding. Despite recent progress in the development\nof end-to-end solutions for video-based action recognition, achieving\nstate-of-the-art performance still requires using auxiliary hand-crafted motion\nrepresentations, e.g., optical flow, which are usually computationally\ndemanding. In this work, we propose to use residual frames (i.e., differences\nbetween adjacent RGB frames) as an alternative \"lightweight\" motion\nrepresentation, which carries salient motion information and is computationally\nefficient. In addition, we develop a new pseudo-3D convolution module which\ndecouples 3D convolution into 2D and 1D convolution. The proposed module\nexploits residual information in the feature space to better structure motions,\nand is equipped with a self-attention mechanism that assists to recalibrate the\nappearance and motion features. Empirical results confirm the efficiency and\neffectiveness of residual frames as well as the proposed pseudo-3D convolution\nmodule.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 17:40:17 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Chen", "Jiawei", ""], ["Hsiao", "Jenson", ""], ["Ho", "Chiu Man", ""]]}, {"id": "2008.01059", "submitter": "Zhengyuan Yang", "authors": "Zhengyuan Yang, Tianlang Chen, Liwei Wang, Jiebo Luo", "title": "Improving One-stage Visual Grounding by Recursive Sub-query Construction", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We improve one-stage visual grounding by addressing current limitations on\ngrounding long and complex queries. Existing one-stage methods encode the\nentire language query as a single sentence embedding vector, e.g., taking the\nembedding from BERT or the hidden state from LSTM. This single vector\nrepresentation is prone to overlooking the detailed descriptions in the query.\nTo address this query modeling deficiency, we propose a recursive sub-query\nconstruction framework, which reasons between image and query for multiple\nrounds and reduces the referring ambiguity step by step. We show our new\none-stage method obtains 5.0%, 4.5%, 7.5%, 12.8% absolute improvements over the\nstate-of-the-art one-stage baseline on ReferItGame, RefCOCO, RefCOCO+, and\nRefCOCOg, respectively. In particular, superior performances on longer and more\ncomplex queries validates the effectiveness of our query modeling.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 17:43:30 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Yang", "Zhengyuan", ""], ["Chen", "Tianlang", ""], ["Wang", "Liwei", ""], ["Luo", "Jiebo", ""]]}, {"id": "2008.01065", "submitter": "Tengda Han", "authors": "Tengda Han, Weidi Xie, Andrew Zisserman", "title": "Memory-augmented Dense Predictive Coding for Video Representation\n  Learning", "comments": "ECCV2020, Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this paper is self-supervised learning from video, in\nparticular for representations for action recognition. We make the following\ncontributions: (i) We propose a new architecture and learning framework\nMemory-augmented Dense Predictive Coding (MemDPC) for the task. It is trained\nwith a predictive attention mechanism over the set of compressed memories, such\nthat any future states can always be constructed by a convex combination of the\ncondense representations, allowing to make multiple hypotheses efficiently.\n(ii) We investigate visual-only self-supervised video representation learning\nfrom RGB frames, or from unsupervised optical flow, or both. (iii) We\nthoroughly evaluate the quality of learnt representation on four different\ndownstream tasks: action recognition, video retrieval, learning with scarce\nannotations, and unintentional action classification. In all cases, we\ndemonstrate state-of-the-art or comparable performance over other approaches\nwith orders of magnitude fewer training data.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 17:57:01 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Han", "Tengda", ""], ["Xie", "Weidi", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2008.01068", "submitter": "Peng-Shuai Wang", "authors": "Peng-Shuai Wang, Yu-Qi Yang, Qian-Fang Zou, Zhirong Wu, Yang Liu, Xin\n  Tong", "title": "Unsupervised 3D Learning for Shape Analysis via Multiresolution Instance\n  Discrimination", "comments": "Accepted by AAAI 2021. Code:\n  https://github.com/microsoft/O-CNN/blob/master/docs/unsupervised.md", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although unsupervised feature learning has demonstrated its advantages to\nreducing the workload of data labeling and network design in many fields,\nexisting unsupervised 3D learning methods still cannot offer a generic network\nfor various shape analysis tasks with competitive performance to supervised\nmethods. In this paper, we propose an unsupervised method for learning a\ngeneric and efficient shape encoding network for different shape analysis\ntasks. The key idea of our method is to jointly encode and learn shape and\npoint features from unlabeled 3D point clouds. For this purpose, we adapt\nHR-Net to octree-based convolutional neural networks for jointly encoding shape\nand point features with fused multiresolution subnetworks and design a\nsimple-yet-efficient Multiresolution Instance Discrimination (MID) loss for\njointly learning the shape and point features. Our network takes a 3D point\ncloud as input and output both shape and point features. After training, the\nnetwork is concatenated with simple task-specific back-end layers and\nfine-tuned for different shape analysis tasks. We evaluate the efficacy and\ngenerality of our method and validate our network and loss design with a set of\nshape analysis tasks, including shape classification, semantic shape\nsegmentation, as well as shape registration tasks. With simple back-ends, our\nnetwork demonstrates the best performance among all unsupervised methods and\nachieves competitive performance to supervised methods, especially in tasks\nwith a small labeled dataset. For fine-grained shape segmentation, our method\neven surpasses existing supervised methods by a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 17:58:46 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 02:49:28 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Wang", "Peng-Shuai", ""], ["Yang", "Yu-Qi", ""], ["Zou", "Qian-Fang", ""], ["Wu", "Zhirong", ""], ["Liu", "Yang", ""], ["Tong", "Xin", ""]]}, {"id": "2008.01078", "submitter": "Hendrik Schr\\\"oter", "authors": "Wei-Cheng Lai, Hendrik Schr\\\"oter", "title": "Ubicomp Digital 2020 -- Handwriting classification using a convolutional\n  recurrent network", "comments": "CRNN, Handwriting recognition, Ubicomp, Stabilo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ubicomp Digital 2020 -- Time Series Classification Challenge from STABILO\nis a challenge about multi-variate time series classification. The data\ncollected from 100 volunteer writers, and contains 15 features measured with\nmultiple sensors on a pen. In this paper,we use a neural network to classify\nthe data into 52 classes, that is lower and upper cases of Arabic letters. The\nproposed architecture of the neural network a is CNN-LSTM network. It combines\nconvolutional neural network (CNN) for short term context with along short term\nmemory layer (LSTM) for also long term dependencies. We reached an accuracy of\n68% on our writer exclusive test set and64.6% on the blind challenge test set\nresulting in the second place.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 11:32:22 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Lai", "Wei-Cheng", ""], ["Schr\u00f6ter", "Hendrik", ""]]}, {"id": "2008.01116", "submitter": "Supratik Banerjee", "authors": "Supratik Banerjee, Cagri Ozcinar, Aakanksha Rana, Aljosa Smolic and\n  Michael Manzke", "title": "Sub-Pixel Back-Projection Network For Lightweight Single Image\n  Super-Resolution", "comments": "To appear in IMVIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN)-based methods have achieved great success\nfor single-image superresolution (SISR). However, most models attempt to\nimprove reconstruction accuracy while increasing the requirement of number of\nmodel parameters. To tackle this problem, in this paper, we study reducing the\nnumber of parameters and computational cost of CNN-based SISR methods while\nmaintaining the accuracy of super-resolution reconstruction performance. To\nthis end, we introduce a novel network architecture for SISR, which strikes a\ngood trade-off between reconstruction quality and low computational complexity.\nSpecifically, we propose an iterative back-projection architecture using\nsub-pixel convolution instead of deconvolution layers. We evaluate the\nperformance of computational and reconstruction accuracy for our proposed model\nwith extensive quantitative and qualitative evaluations. Experimental results\nreveal that our proposed method uses fewer parameters and reduces the\ncomputational cost while maintaining reconstruction accuracy against\nstate-of-the-art SISR methods over well-known four SR benchmark datasets. Code\nis available at\n\"https://github.com/supratikbanerjee/SubPixel-BackProjection_SuperResolution\".\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 18:15:16 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Banerjee", "Supratik", ""], ["Ozcinar", "Cagri", ""], ["Rana", "Aakanksha", ""], ["Smolic", "Aljosa", ""], ["Manzke", "Michael", ""]]}, {"id": "2008.01133", "submitter": "Gabriel Machado", "authors": "Gabriel Machado, Edemir Ferreira, Keiller Nogueira, Hugo Oliveira,\n  Pedro Gama and Jefersson A. dos Santos", "title": "AiRound and CV-BrCT: Novel Multi-View Datasets for Scene Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is undeniable that aerial/satellite images can provide useful information\nfor a large variety of tasks. But, since these images are always looking from\nabove, some applications can benefit from complementary information provided by\nother perspective views of the scene, such as ground-level images. Despite a\nlarge number of public repositories for both georeferenced photographs and\naerial images, there is a lack of benchmark datasets that allow the development\nof approaches that exploit the benefits and complementarity of aerial/ground\nimagery. In this paper, we present two new publicly available datasets named\n\\thedataset~and CV-BrCT. The first one contains triplets of images from the\nsame geographic coordinate with different perspectives of view extracted from\nvarious places around the world. Each triplet is composed of an aerial RGB\nimage, a ground-level perspective image, and a Sentinel-2 sample. The second\ndataset contains pairs of aerial and street-level images extracted from\nsoutheast Brazil. We design an extensive set of experiments concerning\nmulti-view scene classification, using early and late fusion. Such experiments\nwere conducted to show that image classification can be enhanced using\nmulti-view data.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 18:55:46 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Machado", "Gabriel", ""], ["Ferreira", "Edemir", ""], ["Nogueira", "Keiller", ""], ["Oliveira", "Hugo", ""], ["Gama", "Pedro", ""], ["Santos", "Jefersson A. dos", ""]]}, {"id": "2008.01147", "submitter": "Hongliang Li", "authors": "Hongliang Li, Tal Mezheritsky, Liset Vazquez Romaguera, Samuel Kadoury", "title": "3D B-mode ultrasound speckle reduction using deep learning for 3D\n  registration applications", "comments": "10 pages, 3 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound (US) speckles are granular patterns which can impede image\npost-processing tasks, such as image segmentation and registration.\nConventional filtering approaches are commonly used to remove US speckles,\nwhile their main drawback is long run-time in a 3D scenario. Although a few\nstudies were conducted to remove 2D US speckles using deep learning, to our\nknowledge, there is no study to perform speckle reduction of 3D B-mode US using\ndeep learning. In this study, we propose a 3D dense U-Net model to process 3D\nUS B-mode data from a clinical US system. The model's results were applied to\n3D registration. We show that our deep learning framework can obtain similar\nsuppression and mean preservation index (1.066) on speckle reduction when\ncompared to conventional filtering approaches (0.978), while reducing the\nruntime by two orders of magnitude. Moreover, it is found that the speckle\nreduction using our deep learning model contributes to improving the 3D\nregistration performance. The mean square error of 3D registration on 3D data\nusing 3D U-Net speckle reduction is reduced by half compared to that with\nspeckles.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 19:29:59 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Li", "Hongliang", ""], ["Mezheritsky", "Tal", ""], ["Romaguera", "Liset Vazquez", ""], ["Kadoury", "Samuel", ""]]}, {"id": "2008.01148", "submitter": "Tariq Iqbal", "authors": "Md Mofijul Islam and Tariq Iqbal", "title": "HAMLET: A Hierarchical Multimodal Attention-based Human Activity\n  Recognition Algorithm", "comments": "To be published in the IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS) 2020", "journal-ref": "IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS) 2020", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To fluently collaborate with people, robots need the ability to recognize\nhuman activities accurately. Although modern robots are equipped with various\nsensors, robust human activity recognition (HAR) still remains a challenging\ntask for robots due to difficulties related to multimodal data fusion. To\naddress these challenges, in this work, we introduce a deep neural\nnetwork-based multimodal HAR algorithm, HAMLET. HAMLET incorporates a\nhierarchical architecture, where the lower layer encodes spatio-temporal\nfeatures from unimodal data by adopting a multi-head self-attention mechanism.\nWe develop a novel multimodal attention mechanism for disentangling and fusing\nthe salient unimodal features to compute the multimodal features in the upper\nlayer. Finally, multimodal features are used in a fully connect neural-network\nto recognize human activities. We evaluated our algorithm by comparing its\nperformance to several state-of-the-art activity recognition algorithms on\nthree human activity datasets. The results suggest that HAMLET outperformed all\nother evaluated baselines across all datasets and metrics tested, with the\nhighest top-1 accuracy of 95.12% and 97.45% on the UTD-MHAD [1] and the\nUT-Kinect [2] datasets respectively, and F1-score of 81.52% on the UCSD-MIT [3]\ndataset. We further visualize the unimodal and multimodal attention maps, which\nprovide us with a tool to interpret the impact of attention mechanisms\nconcerning HAR.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 19:34:48 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Islam", "Md Mofijul", ""], ["Iqbal", "Tariq", ""]]}, {"id": "2008.01156", "submitter": "Michael Burke Dr", "authors": "Michael Burke, Kartic Subr, Subramanian Ramamoorthy", "title": "Action sequencing using visual permutations", "comments": "This paper has been accepted for publication at IEEE RA-L", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can easily reason about the sequence of high level actions needed to\ncomplete tasks, but it is particularly difficult to instil this ability in\nrobots trained from relatively few examples. This work considers the task of\nneural action sequencing conditioned on a single reference visual state. This\ntask is extremely challenging as it is not only subject to the significant\ncombinatorial complexity that arises from large action sets, but also requires\na model that can perform some form of symbol grounding, mapping high\ndimensional input data to actions, while reasoning about action relationships.\nThis paper takes a permutation perspective and argues that action sequencing\nbenefits from the ability to reason about both permutations and ordering\nconcepts. Empirical analysis shows that neural models trained with latent\npermutations outperform standard neural architectures in constrained action\nsequencing tasks. Results also show that action sequencing using visual\npermutations is an effective mechanism to initialise and speed up traditional\nplanning techniques and successfully scales to far greater action set sizes\nthan models considered previously.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 19:49:06 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 02:34:31 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Burke", "Michael", ""], ["Subr", "Kartic", ""], ["Ramamoorthy", "Subramanian", ""]]}, {"id": "2008.01158", "submitter": "Fakrul Islam Tushar", "authors": "Fakrul Islam Tushar, Vincent M. D'Anniballe, Rui Hou, Maciej A.\n  Mazurowski, Wanyi Fu, Ehsan Samei, Geoffrey D. Rubin, Joseph Y. Lo", "title": "Multi-Disease Classification of 13,667 Body CT Scans Using Weakly\n  Supervised Deep Learning", "comments": "18 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: Training deep learning classifiers typically requires massive\namounts of manual annotation. Weak supervision may leverage existing medical\ndata to classify multiple diseases and organ systems. Purpose: To design\nmulti-disease classifiers for body computed tomography (CT) scans using\nautomatically extracted labels from radiology text reports. Materials &\nMethods: This retrospective study deployed rule-based algorithms to extract\n19,255 disease labels from reports of 13,667 body CT scans of 12,092 subjects\nfor training. Using a 3D DenseVNet, three organ systems were segmented:\nlungs/pleura, liver/gallbladder, and kidneys/ureters. For each organ, a 3D\nconvolutional neural network classified normality versus four common diseases.\nTesting was performed on an additional 2,158 CT volumes relative to 2,875\nmanually derived reference labels. Results: Manual validation of the extracted\nlabels confirmed 91 to 99% accuracy. Performance using the receiver operating\ncharacteristic area under the curve (AUC) for lungs/pleura labels were as\nfollows: atelectasis 0.77 (95% CI: 0.74 to 0.81), nodule 0.65 (0.61 to 0.69),\nemphysema 0.89 (0.86 to 0.92), effusion 0.97 (0.96 to 0.98), and normal 0.89\n(0.87 to 0.91). For liver/gallbladder: stone 0.62 (0.56 to 0.67), lesion 0.73\n(0.69 to 0.77), dilation 0.87 (0.84 to 0.90), fatty 0.89 (0.86 to 0.92), and\nnormal 0.82 (0.78 to 0.85). For kidneys/ureters: stone 0.83 (0.79 to 0.87),\natrophy 0.92 (0.89 to 0.94), lesion 0.68 (0.64 to 0.72), cyst 0.70 (0.66 to\n0.73), and normal 0.79 (0.75 to 0.83). Conclusion: Weakly supervised deep\nlearning classifiers leveraged massive amounts of unannotated body CT data to\nclassify multiple organ systems and diverse diseases.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 19:55:53 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 14:25:08 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Tushar", "Fakrul Islam", ""], ["D'Anniballe", "Vincent M.", ""], ["Hou", "Rui", ""], ["Mazurowski", "Maciej A.", ""], ["Fu", "Wanyi", ""], ["Samei", "Ehsan", ""], ["Rubin", "Geoffrey D.", ""], ["Lo", "Joseph Y.", ""]]}, {"id": "2008.01162", "submitter": "Jun Hayakawa", "authors": "Jun Hayakawa, Behzad Dariush", "title": "Recognition and 3D Localization of Pedestrian Actions from Monocular\n  Video", "comments": null, "journal-ref": "IEEE Intelligent Transportation Systems Conference (ITSC) 2020", "doi": "10.1109/ITSC45102.2020.9294551", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Understanding and predicting pedestrian behavior is an important and\nchallenging area of research for realizing safe and effective navigation\nstrategies in automated and advanced driver assistance technologies in urban\nscenes. This paper focuses on monocular pedestrian action recognition and 3D\nlocalization from an egocentric view for the purpose of predicting intention\nand forecasting future trajectory. A challenge in addressing this problem in\nurban traffic scenes is attributed to the unpredictable behavior of\npedestrians, whereby actions and intentions are constantly in flux and depend\non the pedestrians pose, their 3D spatial relations, and their interaction with\nother agents as well as with the environment. To partially address these\nchallenges, we consider the importance of pose toward recognition and 3D\nlocalization of pedestrian actions. In particular, we propose an action\nrecognition framework using a two-stream temporal relation network with inputs\ncorresponding to the raw RGB image sequence of the tracked pedestrian as well\nas the pedestrian pose. The proposed method outperforms methods using a\nsingle-stream temporal relation network based on evaluations using the JAAD\npublic dataset. The estimated pose and associated body key-points are also used\nas input to a network that estimates the 3D location of the pedestrian using a\nunique loss function. The evaluation of our 3D localization method on the KITTI\ndataset indicates the improvement of the average localization error as compared\nto existing state-of-the-art methods. Finally, we conduct qualitative tests of\naction recognition and 3D localization on HRI's H3D driving dataset.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 19:57:03 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Hayakawa", "Jun", ""], ["Dariush", "Behzad", ""]]}, {"id": "2008.01167", "submitter": "Nermin Samet", "authors": "Nermin Samet, Samet Hicsonmez, Emre Akbas", "title": "Reducing Label Noise in Anchor-Free Object Detection", "comments": "BMVC 2020 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current anchor-free object detectors label all the features that spatially\nfall inside a predefined central region of a ground-truth box as positive. This\napproach causes label noise during training, since some of these positively\nlabeled features may be on the background or an occluder object, or they are\nsimply not discriminative features. In this paper, we propose a new labeling\nstrategy aimed to reduce the label noise in anchor-free detectors. We sum-pool\npredictions stemming from individual features into a single prediction. This\nallows the model to reduce the contributions of non-discriminatory features\nduring training. We develop a new one-stage, anchor-free object detector,\nPPDet, to employ this labeling strategy during training and a similar\nprediction pooling method during inference. On the COCO dataset, PPDet achieves\nthe best performance among anchor-free top-down detectors and performs on-par\nwith the other state-of-the-art methods. It also outperforms all major\none-stage and two-stage methods in small object detection (${AP}_{S}$ $31.4$).\nCode is available at https://github.com/nerminsamet/ppdet\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 20:02:46 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 19:12:23 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Samet", "Nermin", ""], ["Hicsonmez", "Samet", ""], ["Akbas", "Emre", ""]]}, {"id": "2008.01178", "submitter": "Nicolas Gonthier", "authors": "Nicolas Gonthier and Sa\\\"id Ladjal and Yann Gousseau", "title": "Multiple instance learning on deep features for weakly supervised object\n  detection with extreme domain shifts", "comments": "31 pages, 11 figures The paper is under consideration at Computer\n  Vision and Image Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised object detection (WSOD) using only image-level annotations\nhas attracted a growing attention over the past few years. Whereas such task is\ntypically addressed with a domain-specific solution focused on natural images,\nwe show that a simple multiple instance approach applied on pre-trained deep\nfeatures yields excellent performances on non-photographic datasets, possibly\nincluding new classes. The approach does not include any fine-tuning or\ncross-domain learning and is therefore efficient and possibly applicable to\narbitrary datasets and classes. We investigate several flavors of the proposed\napproach, some including multi-layers perceptron and polyhedral classifiers.\nDespite its simplicity, our method shows competitive results on a range of\npublicly available datasets, including paintings (People-Art, IconArt),\nwatercolors, cliparts and comics and allows to quickly learn unseen visual\ncategories.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 20:36:01 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 09:08:58 GMT"}, {"version": "v3", "created": "Sat, 12 Sep 2020 16:02:00 GMT"}, {"version": "v4", "created": "Sun, 25 Oct 2020 12:21:21 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Gonthier", "Nicolas", ""], ["Ladjal", "Sa\u00efd", ""], ["Gousseau", "Yann", ""]]}, {"id": "2008.01179", "submitter": "Kuan-Hui Lee", "authors": "Kuan-Hui Lee, Matthew Kliemann, Adrien Gaidon, Jie Li, Chao Fang,\n  Sudeep Pillai, Wolfram Burgard", "title": "PillarFlow: End-to-end Birds-eye-view Flow Estimation for Autonomous\n  Driving", "comments": "Accepted by IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In autonomous driving, accurately estimating the state of surrounding\nobstacles is critical for safe and robust path planning. However, this\nperception task is difficult, particularly for generic obstacles/objects, due\nto appearance and occlusion changes. To tackle this problem, we propose an\nend-to-end deep learning framework for LIDAR-based flow estimation in bird's\neye view (BeV). Our method takes consecutive point cloud pairs as input and\nproduces a 2-D BeV flow grid describing the dynamic state of each cell. The\nexperimental results show that the proposed method not only estimates 2-D BeV\nflow accurately but also improves tracking performance of both dynamic and\nstatic objects.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 20:36:28 GMT"}, {"version": "v2", "created": "Sun, 23 Aug 2020 00:22:05 GMT"}, {"version": "v3", "created": "Sat, 29 Aug 2020 13:35:09 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Lee", "Kuan-Hui", ""], ["Kliemann", "Matthew", ""], ["Gaidon", "Adrien", ""], ["Li", "Jie", ""], ["Fang", "Chao", ""], ["Pillai", "Sudeep", ""], ["Burgard", "Wolfram", ""]]}, {"id": "2008.01180", "submitter": "Chenyun Wu", "authors": "Chenyun Wu, Mikayla Timm, Subhransu Maji", "title": "Describing Textures using Natural Language", "comments": "ECCV 2020 Oral. Code and dataset are released at:\n  https://people.cs.umass.edu/~chenyun/texture", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Textures in natural images can be characterized by color, shape, periodicity\nof elements within them, and other attributes that can be described using\nnatural language. In this paper, we study the problem of describing visual\nattributes of texture on a novel dataset containing rich descriptions of\ntextures, and conduct a systematic study of current generative and\ndiscriminative models for grounding language to images on this dataset. We find\nthat while these models capture some properties of texture, they fail to\ncapture several compositional properties, such as the colors of dots. We\nprovide critical analysis of existing models by generating synthetic but\nrealistic textures with different descriptions. Our dataset also allows us to\ntrain interpretable models and generate language-based explanations of what\ndiscriminative features are learned by deep networks for fine-grained\ncategorization where texture plays a key role. We present visualizations of\nseveral fine-grained domains and show that texture attributes learned on our\ndataset offer improvements over expert-designed attributes on the Caltech-UCSD\nBirds dataset.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 20:37:35 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Wu", "Chenyun", ""], ["Timm", "Mikayla", ""], ["Maji", "Subhransu", ""]]}, {"id": "2008.01183", "submitter": "Qiaosong Wang", "authors": "Yu-Ting Chang, Qiaosong Wang, Wei-Chih Hung, Robinson Piramuthu,\n  Yi-Hsuan Tsai, Ming-Hsuan Yang", "title": "Weakly-Supervised Semantic Segmentation via Sub-category Exploration", "comments": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition. 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing weakly-supervised semantic segmentation methods using image-level\nannotations typically rely on initial responses to locate object regions.\nHowever, such response maps generated by the classification network usually\nfocus on discriminative object parts, due to the fact that the network does not\nneed the entire object for optimizing the objective function. To enforce the\nnetwork to pay attention to other parts of an object, we propose a simple yet\neffective approach that introduces a self-supervised task by exploiting the\nsub-category information. Specifically, we perform clustering on image features\nto generate pseudo sub-categories labels within each annotated parent class,\nand construct a sub-category objective to assign the network to a more\nchallenging task. By iteratively clustering image features, the training\nprocess does not limit itself to the most discriminative object parts, hence\nimproving the quality of the response maps. We conduct extensive analysis to\nvalidate the proposed method and show that our approach performs favorably\nagainst the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 20:48:31 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Chang", "Yu-Ting", ""], ["Wang", "Qiaosong", ""], ["Hung", "Wei-Chih", ""], ["Piramuthu", "Robinson", ""], ["Tsai", "Yi-Hsuan", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2008.01187", "submitter": "Chenyun Wu", "authors": "Chenyun Wu, Zhe Lin, Scott Cohen, Trung Bui, Subhransu Maji", "title": "PhraseCut: Language-based Image Segmentation in the Wild", "comments": "Published in CVPR 2020. Code and data are released at:\n  https://people.cs.umass.edu/~chenyun/phrasecut", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of segmenting image regions given a natural language\nphrase, and study it on a novel dataset of 77,262 images and 345,486\nphrase-region pairs. Our dataset is collected on top of the Visual Genome\ndataset and uses the existing annotations to generate a challenging set of\nreferring phrases for which the corresponding regions are manually annotated.\nPhrases in our dataset correspond to multiple regions and describe a large\nnumber of object and stuff categories as well as their attributes such as\ncolor, shape, parts, and relationships with other entities in the image. Our\nexperiments show that the scale and diversity of concepts in our dataset poses\nsignificant challenges to the existing state-of-the-art. We systematically\nhandle the long-tail nature of these concepts and present a modular approach to\ncombine category, attribute, and relationship cues that outperforms existing\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 20:58:53 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Wu", "Chenyun", ""], ["Lin", "Zhe", ""], ["Cohen", "Scott", ""], ["Bui", "Trung", ""], ["Maji", "Subhransu", ""]]}, {"id": "2008.01191", "submitter": "Sadaqat Ur Rehman", "authors": "Sadaqat ur Rehman, Muhammad Waqas, Shanshan Tu, Anis Koubaa, Obaid ur\n  Rehman, Jawad Ahmad, Muhammad Hanif, Zhu Han", "title": "Deep Learning Techniques for Future Intelligent Cross-Media Retrieval", "comments": "arXiv admin note: text overlap with arXiv:1804.09539 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the advancement in technology and the expansion of broadcasting,\ncross-media retrieval has gained much attention. It plays a significant role in\nbig data applications and consists in searching and finding data from different\ntypes of media. In this paper, we provide a novel taxonomy according to the\nchallenges faced by multi-modal deep learning approaches in solving cross-media\nretrieval, namely: representation, alignment, and translation. These challenges\nare evaluated on deep learning (DL) based methods, which are categorized into\nfour main groups: 1) unsupervised methods, 2) supervised methods, 3) pairwise\nbased methods, and 4) rank based methods. Then, we present some well-known\ncross-media datasets used for retrieval, considering the importance of these\ndatasets in the context in of deep learning based cross-media retrieval\napproaches. Moreover, we also present an extensive review of the\nstate-of-the-art problems and its corresponding solutions for encouraging deep\nlearning in cross-media retrieval. The fundamental objective of this work is to\nexploit Deep Neural Networks (DNNs) for bridging the \"media gap\", and provide\nresearchers and developers with a better understanding of the underlying\nproblems and the potential solutions of deep learning assisted cross-media\nretrieval. To the best of our knowledge, this is the first comprehensive survey\nto address cross-media retrieval under deep learning methods.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 09:49:33 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Rehman", "Sadaqat ur", ""], ["Waqas", "Muhammad", ""], ["Tu", "Shanshan", ""], ["Koubaa", "Anis", ""], ["Rehman", "Obaid ur", ""], ["Ahmad", "Jawad", ""], ["Hanif", "Muhammad", ""], ["Han", "Zhu", ""]]}, {"id": "2008.01196", "submitter": "Jerome Darmont", "authors": "Abderrazek Azri (ERIC), C\\'ecile Favre (ERIC), Nouria Harbi (ERIC),\n  J\\'er\\^ome Darmont (ERIC)", "title": "Including Images into Message Veracity Assessment in Social Media", "comments": null, "journal-ref": "8th International Conference on Innovation and New Trends in\n  Information Technology (INTIS 2019), Dec 2019, Tangier, Morocco", "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extensive use of social media in the diffusion of information has also\nlaid a fertile ground for the spread of rumors, which could significantly\naffect the credibility of social media. An ever-increasing number of users post\nnews including, in addition to text, multimedia data such as images and videos.\nYet, such multimedia content is easily editable due to the broad availability\nof simple and effective image and video processing tools. The problem of\nassessing the veracity of social network posts has attracted a lot of attention\nfrom researchers in recent years. However, almost all previous works have\nfocused on analyzing textual contents to determine veracity, while visual\ncontents, and more particularly images, remains ignored or little exploited in\nthe literature. In this position paper, we propose a framework that explores\ntwo novel ways to assess the veracity of messages published on social networks\nby analyzing the credibility of both their textual and visual contents.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 08:42:17 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Azri", "Abderrazek", "", "ERIC"], ["Favre", "C\u00e9cile", "", "ERIC"], ["Harbi", "Nouria", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "2008.01201", "submitter": "Qiaosong Wang", "authors": "Yu-Ting Chang, Qiaosong Wang, Wei-Chih Hung, Robinson Piramuthu,\n  Yi-Hsuan Tsai, Ming-Hsuan Yang", "title": "Mixup-CAM: Weakly-supervised Semantic Segmentation via Uncertainty\n  Regularization", "comments": "Accepted at BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Obtaining object response maps is one important step to achieve\nweakly-supervised semantic segmentation using image-level labels. However,\nexisting methods rely on the classification task, which could result in a\nresponse map only attending on discriminative object regions as the network\ndoes not need to see the entire object for optimizing the classification loss.\nTo tackle this issue, we propose a principled and end-to-end train-able\nframework to allow the network to pay attention to other parts of the object,\nwhile producing a more complete and uniform response map. Specifically, we\nintroduce the mixup data augmentation scheme into the classification network\nand design two uncertainty regularization terms to better interact with the\nmixup strategy. In experiments, we conduct extensive analysis to demonstrate\nthe proposed method and show favorable performance against state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 21:19:08 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Chang", "Yu-Ting", ""], ["Wang", "Qiaosong", ""], ["Hung", "Wei-Chih", ""], ["Piramuthu", "Robinson", ""], ["Tsai", "Yi-Hsuan", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2008.01214", "submitter": "Qian Wang", "authors": "Qian Wang, Toby P. Breckon", "title": "Generalized Zero-Shot Domain Adaptation via Coupled Conditional\n  Variational Autoencoders", "comments": "Durham University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation approaches aim to exploit useful information from the\nsource domain where supervised learning examples are easier to obtain to\naddress a learning problem in the target domain where there is no or limited\navailability of such examples. In classification problems, domain adaptation\nhas been studied under varying supervised, unsupervised and semi-supervised\nconditions. However, a common situation when the labelled samples are available\nfor a subset of target domain classes has been overlooked. In this paper, we\nformulate this particular domain adaptation problem within a generalized\nzero-shot learning framework by treating the labelled source domain samples as\nsemantic representations for zero-shot learning. For this particular problem,\nneither conventional domain adaptation approaches nor zero-shot learning\nalgorithms directly apply. To address this generalized zero-shot domain\nadaptation problem, we present a novel Coupled Conditional Variational\nAutoencoder (CCVAE) which can generate synthetic target domain features for\nunseen classes from their source domain counterparts. Extensive experiments\nhave been conducted on three domain adaptation datasets including a bespoke\nX-ray security checkpoint dataset to simulate a real-world application in\naviation security. The results demonstrate the effectiveness of our proposed\napproach both against established benchmarks and in terms of real-world\napplicability.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 21:48:50 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Wang", "Qian", ""], ["Breckon", "Toby P.", ""]]}, {"id": "2008.01216", "submitter": "Hongwei Li", "authors": "Hongwei Li, Jianguo Zhang, and Bjoern Menze", "title": "Generalisable Cardiac Structure Segmentation via Attentional and Stacked\n  Image Adaptation", "comments": "method description of our solution in M&M segmentation challenge,\n  STACOM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tackling domain shifts in multi-centre and multi-vendor data sets remains\nchallenging for cardiac image segmentation. In this paper, we propose a\ngeneralisable segmentation framework for cardiac image segmentation in which\nmulti-centre, multi-vendor, multi-disease datasets are involved. A generative\nadversarial networks with an attention loss was proposed to translate the\nimages from existing source domains to a target domain, thus to generate\ngood-quality synthetic cardiac structure and enlarge the training set. A stack\nof data augmentation techniques was further used to simulate real-world\ntransformation to boost the segmentation performance for unseen domains.We\nachieved an average Dice score of 90.3% for the left ventricle, 85.9% for the\nmyocardium, and 86.5% for the right ventricle on the hidden validation set\nacross four vendors. We show that the domain shifts in heterogeneous cardiac\nimaging datasets can be drastically reduced by two aspects: 1) good-quality\nsynthetic data by learning the underlying target domain distribution, and 2)\nstacked classical image processing techniques for data augmentation.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 21:51:15 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 20:00:16 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Li", "Hongwei", ""], ["Zhang", "Jianguo", ""], ["Menze", "Bjoern", ""]]}, {"id": "2008.01218", "submitter": "Qian Wang", "authors": "Qian Wang, Neelanjan Bhowmik, Toby P. Breckon", "title": "Multi-Class 3D Object Detection Within Volumetric 3D Computed Tomography\n  Baggage Security Screening Imagery", "comments": "Durham University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic detection of prohibited objects within passenger baggage is\nimportant for aviation security. X-ray Computed Tomography (CT) based 3D\nimaging is widely used in airports for aviation security screening whilst prior\nwork on automatic prohibited item detection focus primarily on 2D X-ray\nimagery. These works have proven the possibility of extending deep\nconvolutional neural networks (CNN) based automatic prohibited item detection\nfrom 2D X-ray imagery to volumetric 3D CT baggage security screening imagery.\nHowever, previous work on 3D object detection in baggage security screening\nimagery focused on the detection of one specific type of objects (e.g., either\n{\\it bottles} or {\\it handguns}). As a result, multiple models are needed if\nmore than one type of prohibited item is required to be detected in practice.\nIn this paper, we consider the detection of multiple object categories of\ninterest using one unified framework. To this end, we formulate a more\nchallenging multi-class 3D object detection problem within 3D CT imagery and\npropose a viable solution (3D RetinaNet) to tackle this problem. To enhance the\nperformance of detection we investigate a variety of strategies including data\naugmentation and varying backbone networks. Experimentation carried out to\nprovide both quantitative and qualitative evaluations of the proposed approach\nto multi-class 3D object detection within 3D CT baggage security screening\nimagery. Experimental results demonstrate the combination of the 3D RetinaNet\nand a series of favorable strategies can achieve a mean Average Precision (mAP)\nof 65.3\\% over five object classes (i.e. {\\it bottles, handguns, binoculars,\nglock frames, iPods}). The overall performance is affected by the poor\nperformance on {\\it glock frames} and {\\it iPods} due to the lack of data and\ntheir resemblance with the baggage clutter.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 21:54:14 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Wang", "Qian", ""], ["Bhowmik", "Neelanjan", ""], ["Breckon", "Toby P.", ""]]}, {"id": "2008.01232", "submitter": "M. Esat Kalfaoglu", "authors": "M. Esat Kalfaoglu, Sinan Kalkan, A. Aydin Alatan", "title": "Late Temporal Modeling in 3D CNN Architectures with BERT for Action\n  Recognition", "comments": "Presented on the 2nd Workshop on Video Turing Test: Toward\n  Human-Level Video Story Understanding, ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we combine 3D convolution with late temporal modeling for\naction recognition. For this aim, we replace the conventional Temporal Global\nAverage Pooling (TGAP) layer at the end of 3D convolutional architecture with\nthe Bidirectional Encoder Representations from Transformers (BERT) layer in\norder to better utilize the temporal information with BERT's attention\nmechanism. We show that this replacement improves the performances of many\npopular 3D convolution architectures for action recognition, including ResNeXt,\nI3D, SlowFast and R(2+1)D. Moreover, we provide the-state-of-the-art results on\nboth HMDB51 and UCF101 datasets with 85.10% and 98.69% top-1 accuracy,\nrespectively. The code is publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 22:57:22 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 20:57:38 GMT"}, {"version": "v3", "created": "Thu, 17 Sep 2020 20:25:02 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Kalfaoglu", "M. Esat", ""], ["Kalkan", "Sinan", ""], ["Alatan", "A. Aydin", ""]]}, {"id": "2008.01251", "submitter": "Motohisa Fukuda", "authors": "Motohisa Fukuda, Takashi Okuno, Shinya Yuki", "title": "Central object segmentation by deep learning for fruits and other\n  roundish objects", "comments": "The version 2 contains a new section about the automatic processing\n  of time series photos. All the programs are available at\n  https://github.com/MotohisaFukuda/CROP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CROP (Central Roundish Object Painter), which identifies and\npaints the object at the center of an RGB image. Primarily CROP works for\nroundish fruits in various illumination conditions, but surprisingly, it could\nalso deal with images of other organic or inorganic materials, or ones by\noptical and electron microscopes, although CROP was trained solely by 172\nimages of fruits. The method involves image segmentation by deep learning, and\nthe architecture of the neural network is a deeper version of the original\nU-Net. This technique could provide us with a means of automatically collecting\nstatistical data of fruit growth in farms. As an example, we describe our\nexperiment of processing 510 time series photos automatically to collect the\ndata on the size and the position of the target fruit. Our trained neural\nnetwork CROP and the above automatic programs are available on GitHub with\nuser-friendly interface programs.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 00:13:40 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2020 12:34:06 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Fukuda", "Motohisa", ""], ["Okuno", "Takashi", ""], ["Yuki", "Shinya", ""]]}, {"id": "2008.01254", "submitter": "Seong Hun Lee", "authors": "Seong Hun Lee, Javier Civera", "title": "Geometric Interpretations of the Normalized Epipolar Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we provide geometric interpretations of the normalized epipolar\nerror. Most notably, we show that it is directly related to the following\nquantities: (1) the shortest distance between the two backprojected rays, (2)\nthe dihedral angle between the two bounding epipolar planes, and (3) the\n$L_1$-optimal angular reprojection error.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 00:27:01 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 14:49:10 GMT"}, {"version": "v3", "created": "Sat, 8 Aug 2020 12:21:22 GMT"}, {"version": "v4", "created": "Sat, 17 Oct 2020 03:47:05 GMT"}, {"version": "v5", "created": "Sat, 28 Nov 2020 11:13:32 GMT"}, {"version": "v6", "created": "Thu, 10 Dec 2020 20:10:28 GMT"}, {"version": "v7", "created": "Wed, 30 Dec 2020 21:31:43 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Lee", "Seong Hun", ""], ["Civera", "Javier", ""]]}, {"id": "2008.01258", "submitter": "Seong Hun Lee", "authors": "Seong Hun Lee, Javier Civera", "title": "Robust Uncertainty-Aware Multiview Triangulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a robust and efficient method for multiview triangulation and\nuncertainty estimation. Our contribution is threefold: First, we propose an\noutlier rejection scheme using two-view RANSAC with the midpoint method. By\nprescreening the two-view samples prior to triangulation, we achieve the\nstate-of-the-art efficiency. Second, we compare different local optimization\nmethods for refining the initial solution and the inlier set. With an iterative\nupdate of the inlier set, we show that the optimization provides significant\nimprovement in accuracy and robustness. Third, we model the uncertainty of a\ntriangulated point as a function of three factors: the number of cameras, the\nmean reprojection error and the maximum parallax angle. Learning this model\nallows us to quickly interpolate the uncertainty at test time. We validate our\nmethod through an extensive evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 00:47:42 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 14:52:00 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Lee", "Seong Hun", ""], ["Civera", "Javier", ""]]}, {"id": "2008.01270", "submitter": "Mingmin Zhen", "authors": "Mingmin Zhen, Shiwei Li, Lei Zhou, Jiaxiang Shang, Haoan Feng, Tian\n  Fang, Long Quan", "title": "Learning Discriminative Feature with CRF for Unsupervised Video Object\n  Segmentation", "comments": null, "journal-ref": "European Conference on Computer Vision 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel network, called discriminative feature\nnetwork (DFNet), to address the unsupervised video object segmentation task. To\ncapture the inherent correlation among video frames, we learn discriminative\nfeatures (D-features) from the input images that reveal feature distribution\nfrom a global perspective. The D-features are then used to establish\ncorrespondence with all features of test image under conditional random field\n(CRF) formulation, which is leveraged to enforce consistency between pixels.\nThe experiments verify that DFNet outperforms state-of-the-art methods by a\nlarge margin with a mean IoU score of 83.4% and ranks first on the DAVIS-2016\nleaderboard while using much fewer parameters and achieving much more efficient\nperformance in the inference phase. We further evaluate DFNet on the FBMS\ndataset and the video saliency dataset ViSal, reaching a new state-of-the-art.\nTo further demonstrate the generalizability of our framework, DFNet is also\napplied to the image object co-segmentation task. We perform experiments on a\nchallenging dataset PASCAL-VOC and observe the superiority of DFNet. The\nthorough experiments verify that DFNet is able to capture and mine the\nunderlying relations of images and discover the common foreground objects.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 01:53:56 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Zhen", "Mingmin", ""], ["Li", "Shiwei", ""], ["Zhou", "Lei", ""], ["Shang", "Jiaxiang", ""], ["Feng", "Haoan", ""], ["Fang", "Tian", ""], ["Quan", "Long", ""]]}, {"id": "2008.01286", "submitter": "Manush Bhatt", "authors": "Manush Bhatt, Rajesh Kalyanam, Gen Nishida, Liu He, Christopher May,\n  Dev Niyogi, Daniel Aliaga", "title": "Design and Deployment of Photo2Building: A Cloud-based Procedural\n  Modeling Tool as a Service", "comments": "7 pages, 7 figures, PEARC '20: Practice and Experience in Advanced\n  Research Computing, July 26--30, 2020, Portland, OR, USA", "journal-ref": "ACM, PEARC 2020", "doi": "10.1145/3311790.3396670", "report-no": null, "categories": "cs.DC cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Photo2Building tool to create a plausible 3D model of a building\nfrom only a single photograph. Our tool is based on a prior desktop version\nwhich, as described in this paper, is converted into a client-server model,\nwith job queuing, web-page support, and support of concurrent usage. The\nreported cloud-based web-accessible tool can reconstruct a building in 40\nseconds on average and costing only 0.60 USD with current pricing. This\nprovides for an extremely scalable and possibly widespread tool for creating\nbuilding models for use in urban design and planning applications. With the\ngrowing impact of rapid urbanization on weather and climate and resource\navailability, access to such a service is expected to help a wide variety of\nusers such as city planners, urban meteorologists worldwide in the quest to\nimproved prediction of urban weather and designing climate-resilient cities of\nthe future.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 02:43:33 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Bhatt", "Manush", ""], ["Kalyanam", "Rajesh", ""], ["Nishida", "Gen", ""], ["He", "Liu", ""], ["May", "Christopher", ""], ["Niyogi", "Dev", ""], ["Aliaga", "Daniel", ""]]}, {"id": "2008.01295", "submitter": "Adam Harley", "authors": "Adam W. Harley, Shrinidhi K. Lakshmikanth, Paul Schydlo, Katerina\n  Fragkiadaki", "title": "Tracking Emerges by Looking Around Static Scenes, with Neural 3D Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We hypothesize that an agent that can look around in static scenes can learn\nrich visual representations applicable to 3D object tracking in complex dynamic\nscenes. We are motivated in this pursuit by the fact that the physical world\nitself is mostly static, and multiview correspondence labels are relatively\ncheap to collect in static scenes, e.g., by triangulation. We propose to\nleverage multiview data of \\textit{static points} in arbitrary scenes (static\nor dynamic), to learn a neural 3D mapping module which produces features that\nare correspondable across time. The neural 3D mapper consumes RGB-D data as\ninput, and produces a 3D voxel grid of deep features as output. We train the\nvoxel features to be correspondable across viewpoints, using a contrastive\nloss, and correspondability across time emerges automatically. At test time,\ngiven an RGB-D video with approximate camera poses, and given the 3D box of an\nobject to track, we track the target object by generating a map of each\ntimestep and locating the object's features within each map. In contrast to\nmodels that represent video streams in 2D or 2.5D, our model's 3D scene\nrepresentation is disentangled from projection artifacts, is stable under\ncamera motion, and is robust to partial occlusions. We test the proposed\narchitectures in challenging simulated and real data, and show that our\nunsupervised 3D object trackers outperform prior unsupervised 2D and 2.5D\ntrackers, and approach the accuracy of supervised trackers. This work\ndemonstrates that 3D object trackers can emerge without tracking labels,\nthrough multiview self-supervision on static data.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 02:59:23 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Harley", "Adam W.", ""], ["Lakshmikanth", "Shrinidhi K.", ""], ["Schydlo", "Paul", ""], ["Fragkiadaki", "Katerina", ""]]}, {"id": "2008.01296", "submitter": "Feihu Huang", "authors": "Feihu Huang, Songcan Chen, Heng Huang", "title": "Faster Stochastic Alternating Direction Method of Multipliers for\n  Nonconvex Optimization", "comments": "Published in ICML 2019, 43 pages. arXiv admin note: text overlap with\n  arXiv:1907.13463", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a faster stochastic alternating direction method of\nmultipliers (ADMM) for nonconvex optimization by using a new stochastic\npath-integrated differential estimator (SPIDER), called as SPIDER-ADMM.\nMoreover, we prove that the SPIDER-ADMM achieves a record-breaking incremental\nfirst-order oracle (IFO) complexity of $\\mathcal{O}(n+n^{1/2}\\epsilon^{-1})$\nfor finding an $\\epsilon$-approximate stationary point, which improves the\ndeterministic ADMM by a factor $\\mathcal{O}(n^{1/2})$, where $n$ denotes the\nsample size. As one of major contribution of this paper, we provide a new\ntheoretical analysis framework for nonconvex stochastic ADMM methods with\nproviding the optimal IFO complexity. Based on this new analysis framework, we\nstudy the unsolved optimal IFO complexity of the existing non-convex SVRG-ADMM\nand SAGA-ADMM methods, and prove they have the optimal IFO complexity of\n$\\mathcal{O}(n+n^{2/3}\\epsilon^{-1})$. Thus, the SPIDER-ADMM improves the\nexisting stochastic ADMM methods by a factor of $\\mathcal{O}(n^{1/6})$.\nMoreover, we extend SPIDER-ADMM to the online setting, and propose a faster\nonline SPIDER-ADMM. Our theoretical analysis shows that the online SPIDER-ADMM\nhas the IFO complexity of $\\mathcal{O}(\\epsilon^{-\\frac{3}{2}})$, which\nimproves the existing best results by a factor of\n$\\mathcal{O}(\\epsilon^{-\\frac{1}{2}})$. Finally, the experimental results on\nbenchmark datasets validate that the proposed algorithms have faster\nconvergence rate than the existing ADMM algorithms for nonconvex optimization.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 02:59:42 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 14:44:23 GMT"}, {"version": "v3", "created": "Mon, 10 Aug 2020 03:20:17 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Huang", "Feihu", ""], ["Chen", "Songcan", ""], ["Huang", "Heng", ""]]}, {"id": "2008.01323", "submitter": "Pengqian Yu", "authors": "Xinhan Di, Pengqian Yu, Hong Zhu, Lei Cai, Qiuyan Sheng, Changyu Sun", "title": "Structural Plan of Indoor Scenes with Personalized Preferences", "comments": "Accepted by the 8th International Workshop on Assistive Computer\n  Vision and Robotics (ACVR) in Conjunction with ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an assistive model that supports professional\ninterior designers to produce industrial interior decoration solutions and to\nmeet the personalized preferences of the property owners. The proposed model is\nable to automatically produce the layout of objects of a particular indoor\nscene according to property owners' preferences. In particular, the model\nconsists of the extraction of abstract graph, conditional graph generation, and\nconditional scene instantiation. We provide an interior layout dataset that\ncontains real-world 11000 designs from professional designers. Our numerical\nresults on the dataset demonstrate the effectiveness of the proposed model\ncompared with the state-of-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 04:46:19 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 13:30:45 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Di", "Xinhan", ""], ["Yu", "Pengqian", ""], ["Zhu", "Hong", ""], ["Cai", "Lei", ""], ["Sheng", "Qiuyan", ""], ["Sun", "Changyu", ""]]}, {"id": "2008.01334", "submitter": "Xin Wen", "authors": "Jie Shao, Xin Wen, Bingchen Zhao and Xiangyang Xue", "title": "Temporal Context Aggregation for Video Retrieval with Contrastive\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current research focus on Content-Based Video Retrieval requires\nhigher-level video representation describing the long-range semantic\ndependencies of relevant incidents, events, etc. However, existing methods\ncommonly process the frames of a video as individual images or short clips,\nmaking the modeling of long-range semantic dependencies difficult. In this\npaper, we propose TCA (Temporal Context Aggregation for Video Retrieval), a\nvideo representation learning framework that incorporates long-range temporal\ninformation between frame-level features using the self-attention mechanism. To\ntrain it on video retrieval datasets, we propose a supervised contrastive\nlearning method that performs automatic hard negative mining and utilizes the\nmemory bank mechanism to increase the capacity of negative samples. Extensive\nexperiments are conducted on multiple video retrieval tasks, such as\nCC_WEB_VIDEO, FIVR-200K, and EVVE. The proposed method shows a significant\nperformance advantage (~17% mAP on FIVR-200K) over state-of-the-art methods\nwith video-level features, and deliver competitive results with 22x faster\ninference time comparing with frame-level features.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 05:24:20 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 08:21:08 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Shao", "Jie", ""], ["Wen", "Xin", ""], ["Zhao", "Bingchen", ""], ["Xue", "Xiangyang", ""]]}, {"id": "2008.01338", "submitter": "Zhao-Min Chen", "authors": "Zhao-Min Chen, Xin Jin, Borui Zhao, Xiu-Shen Wei, Yanwen Guo", "title": "Hierarchical Context Embedding for Region-based Object Detection", "comments": "Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art two-stage object detectors apply a classifier to a sparse\nset of object proposals, relying on region-wise features extracted by RoIPool\nor RoIAlign as inputs. The region-wise features, in spite of aligning well with\nthe proposal locations, may still lack the crucial context information which is\nnecessary for filtering out noisy background detections, as well as recognizing\nobjects possessing no distinctive appearances. To address this issue, we\npresent a simple but effective Hierarchical Context Embedding (HCE) framework,\nwhich can be applied as a plug-and-play component, to facilitate the\nclassification ability of a series of region-based detectors by mining\ncontextual cues. Specifically, to advance the recognition of context-dependent\nobject categories, we propose an image-level categorical embedding module which\nleverages the holistic image-level context to learn object-level concepts.\nThen, novel RoI features are generated by exploiting hierarchically embedded\ncontext information beneath both whole images and interested regions, which are\nalso complementary to conventional RoI features. Moreover, to make full use of\nour hierarchical contextual RoI features, we propose the early-and-late fusion\nstrategies (i.e., feature fusion and confidence fusion), which can be combined\nto boost the classification accuracy of region-based detectors. Comprehensive\nexperiments demonstrate that our HCE framework is flexible and generalizable,\nleading to significant and consistent improvements upon various region-based\ndetectors, including FPN, Cascade R-CNN and Mask R-CNN.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 05:33:22 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Chen", "Zhao-Min", ""], ["Jin", "Xin", ""], ["Zhao", "Borui", ""], ["Wei", "Xiu-Shen", ""], ["Guo", "Yanwen", ""]]}, {"id": "2008.01341", "submitter": "Jogendra Nath Kundu", "authors": "Jogendra Nath Kundu, Mugalodi Rakesh, Varun Jampani, Rahul Mysore\n  Venkatesh, R. Venkatesh Babu", "title": "Appearance Consensus Driven Self-Supervised Human Mesh Recovery", "comments": "ECCV 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a self-supervised human mesh recovery framework to infer human\npose and shape from monocular images in the absence of any paired supervision.\nRecent advances have shifted the interest towards directly regressing\nparameters of a parametric human model by supervising them on large-scale\ndatasets with 2D landmark annotations. This limits the generalizability of such\napproaches to operate on images from unlabeled wild environments. Acknowledging\nthis we propose a novel appearance consensus driven self-supervised objective.\nTo effectively disentangle the foreground (FG) human we rely on image pairs\ndepicting the same person (consistent FG) in varied pose and background (BG)\nwhich are obtained from unlabeled wild videos. The proposed FG appearance\nconsistency objective makes use of a novel, differentiable Color-recovery\nmodule to obtain vertex colors without the need for any appearance network; via\nefficient realization of color-picking and reflectional symmetry. We achieve\nstate-of-the-art results on the standard model-based 3D pose estimation\nbenchmarks at comparable supervision levels. Furthermore, the resulting colored\nmesh prediction opens up the usage of our framework for a variety of\nappearance-related tasks beyond the pose and shape estimation, thus\nestablishing our superior generalizability.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 05:40:39 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Kundu", "Jogendra Nath", ""], ["Rakesh", "Mugalodi", ""], ["Jampani", "Varun", ""], ["Venkatesh", "Rahul Mysore", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "2008.01342", "submitter": "Yuwen Xiong", "authors": "Yuwen Xiong, Mengye Ren, Raquel Urtasun", "title": "LoCo: Local Contrastive Representation Learning", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural nets typically perform end-to-end backpropagation to learn the\nweights, a procedure that creates synchronization constraints in the weight\nupdate step across layers and is not biologically plausible. Recent advances in\nunsupervised contrastive representation learning point to the question of\nwhether a learning algorithm can also be made local, that is, the updates of\nlower layers do not directly depend on the computation of upper layers. While\nGreedy InfoMax separately learns each block with a local objective, we found\nthat it consistently hurts readout accuracy in state-of-the-art unsupervised\ncontrastive learning algorithms, possibly due to the greedy objective as well\nas gradient isolation. In this work, we discover that by overlapping local\nblocks stacking on top of each other, we effectively increase the decoder depth\nand allow upper blocks to implicitly send feedbacks to lower blocks. This\nsimple design closes the performance gap between local learning and end-to-end\ncontrastive learning algorithms for the first time. Aside from standard\nImageNet experiments, we also show results on complex downstream tasks such as\nobject detection and instance segmentation directly using readout features.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 05:41:29 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 08:54:06 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Xiong", "Yuwen", ""], ["Ren", "Mengye", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2008.01344", "submitter": "Manish Bhattarai", "authors": "Vamsi Karthik Vadlamani, Manish Bhattarai, Meenu Ajith, Manel\n  Mart{\\i}nez-Ramon", "title": "A Novel Indoor Positioning System for unprepared firefighting scenarios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Situational awareness and Indoor location tracking for firefighters is one of\nthe tasks with paramount importance in search and rescue operations. For Indoor\nPositioning systems (IPS), GPS is not the best possible solution. There are few\nother techniques like dead reckoning, Wifi and bluetooth based triangulation,\nStructure from Motion (SFM) based scene reconstruction for Indoor positioning\nsystem. However due to high temperatures, the rapidly changing environment of\nfires, and low parallax in the thermal images, these techniques are not\nsuitable for relaying the necessary information in a fire fighting environment\nneeded to increase situational awareness in real time. In fire fighting\nenvironments, thermal imaging cameras are used due to smoke and low visibility\nhence obtaining relative orientation from the vanishing point estimation is\nvery difficult. The following technique that is the content of this research\nimplements a novel optical flow based video compass for orientation estimation\nand fused IMU data based activity recognition for IPS. This technique helps\nfirst responders to go into unprepared, unknown environments and still maintain\nsituational awareness like the orientation and, position of the victim fire\nfighters.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 05:46:03 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Vadlamani", "Vamsi Karthik", ""], ["Bhattarai", "Manish", ""], ["Ajith", "Meenu", ""], ["Mart\u0131nez-Ramon", "Manel", ""]]}, {"id": "2008.01362", "submitter": "Jong Chul Ye", "authors": "Hyungjin Chung, Eunju Cha, Leonard Sunwoo, and Jong Chul Ye", "title": "Two-Stage Deep Learning for Accelerated 3D Time-of-Flight MRA without\n  Matched Training Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-of-flight magnetic resonance angiography (TOF-MRA) is one of the most\nwidely used non-contrast MR imaging methods to visualize blood vessels, but due\nto the 3-D volume acquisition highly accelerated acquisition is necessary.\nAccordingly, high quality reconstruction from undersampled TOF-MRA is an\nimportant research topic for deep learning. However, most existing deep\nlearning works require matched reference data for supervised training, which\nare often difficult to obtain. By extending the recent theoretical\nunderstanding of cycleGAN from the optimal transport theory, here we propose a\nnovel two-stage unsupervised deep learning approach, which is composed of the\nmulti-coil reconstruction network along the coronal plane followed by a\nmulti-planar refinement network along the axial plane. Specifically, the first\nnetwork is trained in the square-root of sum of squares (SSoS) domain to\nachieve high quality parallel image reconstruction, whereas the second\nrefinement network is designed to efficiently learn the characteristics of\nhighly-activated blood flow using double-headed max-pool discriminator.\nExtensive experiments demonstrate that the proposed learning process without\nmatched reference exceeds performance of state-of-the-art compressed sensing\n(CS)-based method and provides comparable or even better results than\nsupervised learning approaches.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 06:36:38 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Chung", "Hyungjin", ""], ["Cha", "Eunju", ""], ["Sunwoo", "Leonard", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2008.01365", "submitter": "Zehao Huang", "authors": "Zehao Huang, Zehui Chen, Qiaofei Li, Hongkai Zhang, Naiyan Wang", "title": "1st Place Solutions of Waymo Open Dataset Challenge 2020 -- 2D Object\n  Detection Track", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report, we present our solutions of Waymo Open Dataset\n(WOD) Challenge 2020 - 2D Object Track. We adopt FPN as our basic framework.\nCascade RCNN, stacked PAFPN Neck and Double-Head are used for performance\nimprovements. In order to handle the small object detection problem in WOD, we\nuse very large image scales for both training and testing. Using our methods,\nour team RW-TSDet achieved the 1st place in the 2D Object Detection Track.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 06:46:28 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Huang", "Zehao", ""], ["Chen", "Zehui", ""], ["Li", "Qiaofei", ""], ["Zhang", "Hongkai", ""], ["Wang", "Naiyan", ""]]}, {"id": "2008.01369", "submitter": "Quan Cui", "authors": "Quan Cui, Qing-Yuan Jiang, Xiu-Shen Wei, Wu-Jun Li and Osamu Yoshie", "title": "ExchNet: A Unified Hashing Network for Large-Scale Fine-Grained Image\n  Retrieval", "comments": "Accepted by ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrieving content relevant images from a large-scale fine-grained dataset\ncould suffer from intolerably slow query speed and highly redundant storage\ncost, due to high-dimensional real-valued embeddings which aim to distinguish\nsubtle visual differences of fine-grained objects. In this paper, we study the\nnovel fine-grained hashing topic to generate compact binary codes for\nfine-grained images, leveraging the search and storage efficiency of hash\nlearning to alleviate the aforementioned problems. Specifically, we propose a\nunified end-to-end trainable network, termed as ExchNet. Based on attention\nmechanisms and proposed attention constraints, it can firstly obtain both local\nand global features to represent object parts and whole fine-grained objects,\nrespectively. Furthermore, to ensure the discriminative ability and semantic\nmeaning's consistency of these part-level features across images, we design a\nlocal feature alignment approach by performing a feature exchanging operation.\nLater, an alternative learning algorithm is employed to optimize the whole\nExchNet and then generate the final binary hash codes. Validated by extensive\nexperiments, our proposal consistently outperforms state-of-the-art generic\nhashing methods on five fine-grained datasets, which shows our effectiveness.\nMoreover, compared with other approximate nearest neighbor methods, ExchNet\nachieves the best speed-up and storage reduction, revealing its efficiency and\npracticality.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 07:01:32 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Cui", "Quan", ""], ["Jiang", "Qing-Yuan", ""], ["Wei", "Xiu-Shen", ""], ["Li", "Wu-Jun", ""], ["Yoshie", "Osamu", ""]]}, {"id": "2008.01380", "submitter": "Ata Mahjoubfar", "authors": "Te-Yuan Liu, Ata Mahjoubfar, Daniel Prusinski, Luis Stevens", "title": "Neuromorphic Computing for Content-based Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic computing mimics the neural activity of the brain through\nemulating spiking neural networks. In numerous machine learning tasks,\nneuromorphic chips are expected to provide superior solutions in terms of cost\nand power efficiency. Here, we explore the application of Loihi, a neuromorphic\ncomputing chip developed by Intel, for the computer vision task of image\nretrieval. We evaluated the functionalities and the performance metrics that\nare critical in context-based visual search and recommender systems using\ndeep-learning embeddings. Our results show that the neuromorphic solution is\nabout 3.2 times more energy-efficient compared with an Intel Core i7 CPU and\n12.5 times more energy-efficient compared with Nvidia T4 GPU for inference by a\nlightweight convolutional neural network without batching, while maintaining\nthe same level of matching accuracy. The study validates the longterm potential\nof neuromorphic computing in machine learning, as a complementary paradigm to\nthe existing Von Neumann architectures.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 07:34:07 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Liu", "Te-Yuan", ""], ["Mahjoubfar", "Ata", ""], ["Prusinski", "Daniel", ""], ["Stevens", "Luis", ""]]}, {"id": "2008.01388", "submitter": "Jogendra Nath Kundu", "authors": "Jogendra Nath Kundu, Ambareesh Revanur, Govind Vitthal Waghmare, Rahul\n  Mysore Venkatesh, R. Venkatesh Babu", "title": "Unsupervised Cross-Modal Alignment for Multi-Person 3D Pose Estimation", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deployment friendly, fast bottom-up framework for multi-person\n3D human pose estimation. We adopt a novel neural representation of\nmulti-person 3D pose which unifies the position of person instances with their\ncorresponding 3D pose representation. This is realized by learning a generative\npose embedding which not only ensures plausible 3D pose predictions, but also\neliminates the usual keypoint grouping operation as employed in prior bottom-up\napproaches. Further, we propose a practical deployment paradigm where paired 2D\nor 3D pose annotations are unavailable. In the absence of any paired\nsupervision, we leverage a frozen network, as a teacher model, which is trained\non an auxiliary task of multi-person 2D pose estimation. We cast the learning\nas a cross-modal alignment problem and propose training objectives to realize a\nshared latent space between two diverse modalities. We aim to enhance the\nmodel's ability to perform beyond the limiting teacher network by enriching the\nlatent-to-3D pose mapping using artificially synthesized multi-person 3D scene\nsamples. Our approach not only generalizes to in-the-wild images, but also\nyields a superior trade-off between speed and performance, compared to prior\ntop-down approaches. Our approach also yields state-of-the-art multi-person 3D\npose estimation performance among the bottom-up approaches under consistent\nsupervision levels.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 07:54:25 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Kundu", "Jogendra Nath", ""], ["Revanur", "Ambareesh", ""], ["Waghmare", "Govind Vitthal", ""], ["Venkatesh", "Rahul Mysore", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "2008.01389", "submitter": "Jogendra Nath Kundu", "authors": "Jogendra Nath Kundu, Rahul Mysore Venkatesh, Naveen Venkat, Ambareesh\n  Revanur, R. Venkatesh Babu", "title": "Class-Incremental Domain Adaptation", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a practical Domain Adaptation (DA) paradigm called\nClass-Incremental Domain Adaptation (CIDA). Existing DA methods tackle\ndomain-shift but are unsuitable for learning novel target-domain classes.\nMeanwhile, class-incremental (CI) methods enable learning of new classes in\nabsence of source training data but fail under a domain-shift without labeled\nsupervision. In this work, we effectively identify the limitations of these\napproaches in the CIDA paradigm. Motivated by theoretical and empirical\nobservations, we propose an effective method, inspired by prototypical\nnetworks, that enables classification of target samples into both shared and\nnovel (one-shot) target classes, even under a domain-shift. Our approach yields\nsuperior performance as compared to both DA and CI methods in the CIDA\nparadigm.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 07:55:03 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Kundu", "Jogendra Nath", ""], ["Venkatesh", "Rahul Mysore", ""], ["Venkat", "Naveen", ""], ["Revanur", "Ambareesh", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "2008.01392", "submitter": "Mert B\\\"ulent Sar{\\i}y{\\i}ld{\\i}z", "authors": "Mert Bulent Sariyildiz, Julien Perez, Diane Larlus", "title": "Learning Visual Representations with Caption Annotations", "comments": "Accepted to the 2020 European Conference on Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretraining general-purpose visual features has become a crucial part of\ntackling many computer vision tasks. While one can learn such features on the\nextensively-annotated ImageNet dataset, recent approaches have looked at ways\nto allow for noisy, fewer, or even no annotations to perform such pretraining.\nStarting from the observation that captioned images are easily crawlable, we\nargue that this overlooked source of information can be exploited to supervise\nthe training of visual representations. To do so, motivated by the recent\nprogresses in language models, we introduce {\\em image-conditioned masked\nlanguage modeling} (ICMLM) -- a proxy task to learn visual representations over\nimage-caption pairs. ICMLM consists in predicting masked words in captions by\nrelying on visual cues. To tackle this task, we propose hybrid models, with\ndedicated visual and textual encoders, and we show that the visual\nrepresentations learned as a by-product of solving this task transfer well to a\nvariety of target tasks. Our experiments confirm that image captions can be\nleveraged to inject global and localized semantic information into visual\nrepresentations. Project website: https://europe.naverlabs.com/icmlm.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 08:04:16 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Sariyildiz", "Mert Bulent", ""], ["Perez", "Julien", ""], ["Larlus", "Diane", ""]]}, {"id": "2008.01403", "submitter": "Daizong Liu", "authors": "Daizong Liu, Xiaoye Qu, Xiao-Yang Liu, Jianfeng Dong, Pan Zhou,\n  Zichuan Xu", "title": "Jointly Cross- and Self-Modal Graph Attention Network for Query-Based\n  Moment Localization", "comments": "Accepted by ACM MM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query-based moment localization is a new task that localizes the best matched\nsegment in an untrimmed video according to a given sentence query. In this\nlocalization task, one should pay more attention to thoroughly mine visual and\nlinguistic information. To this end, we propose a novel Cross- and Self-Modal\nGraph Attention Network (CSMGAN) that recasts this task as a process of\niterative messages passing over a joint graph. Specifically, the joint graph\nconsists of Cross-Modal interaction Graph (CMG) and Self-Modal relation Graph\n(SMG), where frames and words are represented as nodes, and the relations\nbetween cross- and self-modal node pairs are described by an attention\nmechanism. Through parametric message passing, CMG highlights relevant\ninstances across video and sentence, and then SMG models the pairwise relation\ninside each modality for frame (word) correlating. With multiple layers of such\na joint graph, our CSMGAN is able to effectively capture high-order\ninteractions between two modalities, thus enabling a further precise\nlocalization. Besides, to better comprehend the contextual details in the\nquery, we develop a hierarchical sentence encoder to enhance the query\nunderstanding. Extensive experiments on four public datasets demonstrate the\neffectiveness of our proposed model, and GCSMAN significantly outperforms the\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 08:25:24 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 01:56:06 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Liu", "Daizong", ""], ["Qu", "Xiaoye", ""], ["Liu", "Xiao-Yang", ""], ["Dong", "Jianfeng", ""], ["Zhou", "Pan", ""], ["Xu", "Zichuan", ""]]}, {"id": "2008.01405", "submitter": "Hyungtae Lim", "authors": "Hyungtae Lim, Hyeonjae Gil and Hyun Myung", "title": "MSDPN: Monocular Depth Prediction with Partial Laser Observation using\n  Multi-stage Neural Networks", "comments": "8 pages, 8 figures, IEEE/RSJ Intelligent Robots and Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a deep-learning-based multi-stage network architecture called\nMulti-Stage Depth Prediction Network (MSDPN) is proposed to predict a dense\ndepth map using a 2D LiDAR and a monocular camera. Our proposed network\nconsists of a multi-stage encoder-decoder architecture and Cross Stage Feature\nAggregation (CSFA). The proposed multi-stage encoder-decoder architecture\nalleviates the partial observation problem caused by the characteristics of a\n2D LiDAR, and CSFA prevents the multi-stage network from diluting the features\nand allows the network to learn the inter-spatial relationship between features\nbetter. Previous works use sub-sampled data from the ground truth as an input\nrather than actual 2D LiDAR data. In contrast, our approach trains the model\nand conducts experiments with a physically-collected 2D LiDAR dataset. To this\nend, we acquired our own dataset called KAIST RGBD-scan dataset and validated\nthe effectiveness and the robustness of MSDPN under realistic conditions. As\nverified experimentally, our network yields promising performance against\nstate-of-the-art methods. Additionally, we analyzed the performance of\ndifferent input methods and confirmed that the reference depth map is robust in\nuntrained scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 08:27:40 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Lim", "Hyungtae", ""], ["Gil", "Hyeonjae", ""], ["Myung", "Hyun", ""]]}, {"id": "2008.01410", "submitter": "Wanyu Bian", "authors": "Wanyu Bian, Yunmei Chen, Xiaojing Ye", "title": "Deep Parallel MRI Reconstruction Network Without Coil Sensitivities", "comments": "Accepted by MICCAI international workshop MLMIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep neural network architecture by mapping the robust\nproximal gradient scheme for fast image reconstruction in parallel MRI (pMRI)\nwith regularization function trained from data. The proposed network learns to\nadaptively combine the multi-coil images from incomplete pMRI data into a\nsingle image with homogeneous contrast, which is then passed to a nonlinear\nencoder to efficiently extract sparse features of the image. Unlike most of\nexisting deep image reconstruction networks, our network does not require\nknowledge of sensitivity maps, which can be difficult to estimate accurately,\nand have been a major bottleneck of image reconstruction in real-world pMRI\napplications. The experimental results demonstrate the promising performance of\nour method on a variety of pMRI imaging data sets.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 08:39:36 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 22:15:30 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2020 15:03:33 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Bian", "Wanyu", ""], ["Chen", "Yunmei", ""], ["Ye", "Xiaojing", ""]]}, {"id": "2008.01411", "submitter": "Xi Li", "authors": "Hanbin Zhao, Hui Wang, Yongjian Fu, Fei Wu, Xi Li", "title": "Memory Efficient Class-Incremental Learning for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the memory-resource-limited constraints, class-incremental learning\n(CIL) usually suffers from the \"catastrophic forgetting\" problem when updating\nthe joint classification model on the arrival of newly added classes. To cope\nwith the forgetting problem, many CIL methods transfer the knowledge of old\nclasses by preserving some exemplar samples into the size-constrained memory\nbuffer. To utilize the memory buffer more efficiently, we propose to keep more\nauxiliary low-fidelity exemplar samples rather than the original real\nhigh-fidelity exemplar samples. Such a memory-efficient exemplar preserving\nscheme makes the old-class knowledge transfer more effective. However, the\nlow-fidelity exemplar samples are often distributed in a different domain away\nfrom that of the original exemplar samples, that is, a domain shift. To\nalleviate this problem, we propose a duplet learning scheme that seeks to\nconstruct domain-compatible feature extractors and classifiers, which greatly\nnarrows down the above domain gap. As a result, these low-fidelity auxiliary\nexemplar samples have the ability to moderately replace the original exemplar\nsamples with a lower memory cost. In addition, we present a robust classifier\nadaptation scheme, which further refines the biased classifier (learned with\nthe samples containing distillation label knowledge about old classes) with the\nhelp of the samples of pure true class labels. Experimental results demonstrate\nthe effectiveness of this work against the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 08:39:40 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 13:32:08 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Zhao", "Hanbin", ""], ["Wang", "Hui", ""], ["Fu", "Yongjian", ""], ["Wu", "Fei", ""], ["Li", "Xi", ""]]}, {"id": "2008.01421", "submitter": "Yenan Jiang", "authors": "Yenan Jiang, Ying Li, Shanrong Zou, Haokui Zhang, Yunpeng Bai", "title": "Hyperspectral Image Classification with Spatial Consistence Using Fully\n  Convolutional Spatial Propagation Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep convolutional neural networks (CNNs) have shown\nimpressive ability to represent hyperspectral images (HSIs) and achieved\nencouraging results in HSI classification. However, the existing CNN-based\nmodels operate at the patch-level, in which pixel is separately classified into\nclasses using a patch of images around it. This patch-level classification will\nlead to a large number of repeated calculations, and it is difficult to\ndetermine the appropriate patch size that is beneficial to classification\naccuracy. In addition, the conventional CNN models operate convolutions with\nlocal receptive fields, which cause failures in modeling contextual spatial\ninformation. To overcome the aforementioned limitations, we propose a novel\nend-to-end, pixels-to-pixels fully convolutional spatial propagation network\n(FCSPN) for HSI classification. Our FCSPN consists of a 3D fully convolution\nnetwork (3D-FCN) and a convolutional spatial propagation network (CSPN).\nSpecifically, the 3D-FCN is firstly introduced for reliable preliminary\nclassification, in which a novel dual separable residual (DSR) unit is proposed\nto effectively capture spectral and spatial information simultaneously with\nfewer parameters. Moreover, the channel-wise attention mechanism is adapted in\nthe 3D-FCN to grasp the most informative channels from redundant channel\ninformation. Finally, the CSPN is introduced to capture the spatial\ncorrelations of HSI via learning a local linear spatial propagation, which\nallows maintaining the HSI spatial consistency and further refining the\nclassification results. Experimental results on three HSI benchmark datasets\ndemonstrate that the proposed FCSPN achieves state-of-the-art performance on\nHSI classification.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 09:05:52 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Jiang", "Yenan", ""], ["Li", "Ying", ""], ["Zou", "Shanrong", ""], ["Zhang", "Haokui", ""], ["Bai", "Yunpeng", ""]]}, {"id": "2008.01429", "submitter": "Paolo Giannitrapani", "authors": "Elio D. Di Claudio, Paolo Giannitrapani, Giovanni Jacovitti", "title": "Predicting the Blur Visual Discomfort for Natural Scenes by the Loss of\n  Positional Information", "comments": "12 pages, 8 figures, article submitted to Vision Research (Elsevier)\n  Journal in July 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The perception of the blur due to accommodation failures, insufficient\noptical correction or imperfect image reproduction is a common source of visual\ndiscomfort, usually attributed to an anomalous and annoying distribution of the\nimage spectrum in the spatial frequency domain. In the present paper, this\ndiscomfort is attributed to a loss of the localization accuracy of the observed\npatterns. It is assumed, as a starting perceptual principle, that the visual\nsystem is optimally adapted to pattern localization in a natural environment.\nThus, since the best possible accuracy of the image patterns localization is\nindicated by the positional Fisher Information, it is argued that the blur\ndiscomfort is strictly related to a loss of this information. Following this\nconcept, a receptive field functional model, tuned to common features of\nnatural scenes, is adopted to predict the visual discomfort. It is a\ncomplex-valued operator, orientation-selective both in the space domain and in\nthe spatial frequency domain. Starting from the case of Gaussian blur, the\nanalysis is extended to a generic type of blur by applying a positional Fisher\nInformation equivalence criterion. Out-of-focus blur and astigmatic blur are\npresented as significant examples. The validity of the proposed model is\nverified by comparing its predictions with subjective ratings. The model fits\nlinearly with the experiments reported in independent databases, based on\ndifferent protocols and settings.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 09:30:38 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 19:17:59 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Di Claudio", "Elio D.", ""], ["Giannitrapani", "Paolo", ""], ["Jacovitti", "Giovanni", ""]]}, {"id": "2008.01430", "submitter": "Enzo Tartaglione", "authors": "Enzo Tartaglione and Marco Grangetto", "title": "A non-discriminatory approach to ethical deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks perform state-of-the-art in an ever-growing number\nof tasks, nowadays they are used to solve an incredibly large variety of tasks.\nHowever, typical training strategies do not take into account lawful, ethical\nand discriminatory potential issues the trained ANN models could incur in. In\nthis work we propose NDR, a non-discriminatory regularization strategy to\nprevent the ANN model to solve the target task using some discriminatory\nfeatures like, for example, the ethnicity in an image classification task for\nhuman faces. In particular, a part of the ANN model is trained to hide the\ndiscriminatory information such that the rest of the network focuses in\nlearning the given learning task. Our experiments show that NDR can be\nexploited to achieve non-discriminatory models with both minimal computational\noverhead and performance loss.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 09:33:02 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Tartaglione", "Enzo", ""], ["Grangetto", "Marco", ""]]}, {"id": "2008.01432", "submitter": "Yueran Bai", "authors": "Yueran Bai, Yingying Wang, Yunhai Tong, Yang Yang, Qiyue Liu, Junhui\n  Liu", "title": "Boundary Content Graph Neural Network for Temporal Action Proposal\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal action proposal generation plays an important role in video action\nunderstanding, which requires localizing high-quality action content precisely.\nHowever, generating temporal proposals with both precise boundaries and\nhigh-quality action content is extremely challenging. To address this issue, we\npropose a novel Boundary Content Graph Neural Network (BC-GNN) to model the\ninsightful relations between the boundary and action content of temporal\nproposals by the graph neural networks. In BC-GNN, the boundaries and content\nof temporal proposals are taken as the nodes and edges of the graph neural\nnetwork, respectively, where they are spontaneously linked. Then a novel graph\ncomputation operation is proposed to update features of edges and nodes. After\nthat, one updated edge and two nodes it connects are used to predict boundary\nprobabilities and content confidence score, which will be combined to generate\na final high-quality proposal. Experiments are conducted on two mainstream\ndatasets: ActivityNet-1.3 and THUMOS14. Without the bells and whistles, BC-GNN\noutperforms previous state-of-the-art methods in both temporal action proposal\nand temporal action detection tasks.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 09:35:11 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Bai", "Yueran", ""], ["Wang", "Yingying", ""], ["Tong", "Yunhai", ""], ["Yang", "Yang", ""], ["Liu", "Qiyue", ""], ["Liu", "Junhui", ""]]}, {"id": "2008.01437", "submitter": "Dhruv Verma", "authors": "Dhruv Verma, Kshitij Gulati and Rajiv Ratn Shah", "title": "Addressing the Cold-Start Problem in Outfit Recommendation Using Visual\n  Preference Modelling", "comments": "Sixth IEEE International Conference on Multimedia Big Data (BigMM'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the global transformation of the fashion industry and a rise in the\ndemand for fashion items worldwide, the need for an effectual fashion\nrecommendation has never been more. Despite various cutting-edge solutions\nproposed in the past for personalising fashion recommendation, the technology\nis still limited by its poor performance on new entities, i.e. the cold-start\nproblem. In this paper, we attempt to address the cold-start problem for new\nusers, by leveraging a novel visual preference modelling approach on a small\nset of input images. We demonstrate the use of our approach with\nfeature-weighted clustering to personalise occasion-oriented outfit\nrecommendation. Quantitatively, our results show that the proposed visual\npreference modelling approach outperforms state of the art in terms of clothing\nattribute prediction. Qualitatively, through a pilot study, we demonstrate the\nefficacy of our system to provide diverse and personalised recommendations in\ncold-start scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 10:07:09 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Verma", "Dhruv", ""], ["Gulati", "Kshitij", ""], ["Shah", "Rajiv Ratn", ""]]}, {"id": "2008.01438", "submitter": "Andrey Ignatov", "authors": "Dmitry Ignatov and Andrey Ignatov", "title": "Controlling Information Capacity of Binary Neural Network", "comments": null, "journal-ref": null, "doi": "10.1016/j.patrec.2020.07.033", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the growing popularity of deep learning technologies, high memory\nrequirements and power consumption are essentially limiting their application\nin mobile and IoT areas. While binary convolutional networks can alleviate\nthese problems, the limited bitwidth of weights is often leading to significant\ndegradation of prediction accuracy. In this paper, we present a method for\ntraining binary networks that maintains a stable predefined level of their\ninformation capacity throughout the training process by applying Shannon\nentropy based penalty to convolutional filters. The results of experiments\nconducted on SVHN, CIFAR and ImageNet datasets demonstrate that the proposed\napproach can statistically significantly improve the accuracy of binary\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 10:08:28 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Ignatov", "Dmitry", ""], ["Ignatov", "Andrey", ""]]}, {"id": "2008.01449", "submitter": "Zhuotao Tian", "authors": "Zhuotao Tian, Hengshuang Zhao, Michelle Shu, Zhicheng Yang, Ruiyu Li,\n  Jiaya Jia", "title": "Prior Guided Feature Enrichment Network for Few-Shot Segmentation", "comments": "16 pages. To appear in TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art semantic segmentation methods require sufficient labeled\ndata to achieve good results and hardly work on unseen classes without\nfine-tuning. Few-shot segmentation is thus proposed to tackle this problem by\nlearning a model that quickly adapts to new classes with a few labeled support\nsamples. Theses frameworks still face the challenge of generalization ability\nreduction on unseen classes due to inappropriate use of high-level semantic\ninformation of training classes and spatial inconsistency between query and\nsupport targets. To alleviate these issues, we propose the Prior Guided Feature\nEnrichment Network (PFENet). It consists of novel designs of (1) a\ntraining-free prior mask generation method that not only retains generalization\npower but also improves model performance and (2) Feature Enrichment Module\n(FEM) that overcomes spatial inconsistency by adaptively enriching query\nfeatures with support features and prior masks. Extensive experiments on\nPASCAL-5$^i$ and COCO prove that the proposed prior generation method and FEM\nboth improve the baseline method significantly. Our PFENet also outperforms\nstate-of-the-art methods by a large margin without efficiency loss. It is\nsurprising that our model even generalizes to cases without labeled support\nsamples. Our code is available at https://github.com/Jia-Research-Lab/PFENet/.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 10:41:32 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Tian", "Zhuotao", ""], ["Zhao", "Hengshuang", ""], ["Shu", "Michelle", ""], ["Yang", "Zhicheng", ""], ["Li", "Ruiyu", ""], ["Jia", "Jiaya", ""]]}, {"id": "2008.01458", "submitter": "Yuchen Dai", "authors": "Youcai Zhang, Zhonghao Lan, Yuchen Dai, Fangao Zeng, Yan Bai, Jie\n  Chang, and Yichen Wei", "title": "Prime-Aware Adaptive Distillation", "comments": "Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation(KD) aims to improve the performance of a student\nnetwork by mimicing the knowledge from a powerful teacher network. Existing\nmethods focus on studying what knowledge should be transferred and treat all\nsamples equally during training. This paper introduces the adaptive sample\nweighting to KD. We discover that previous effective hard mining methods are\nnot appropriate for distillation. Furthermore, we propose Prime-Aware Adaptive\nDistillation (PAD) by the incorporation of uncertainty learning. PAD perceives\nthe prime samples in distillation and then emphasizes their effect adaptively.\nPAD is fundamentally different from and would refine existing methods with the\ninnovative view of unequal training. For this reason, PAD is versatile and has\nbeen applied in various tasks including classification, metric learning, and\nobject detection. With ten teacher-student combinations on six datasets, PAD\npromotes the performance of existing distillation methods and outperforms\nrecent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 10:53:12 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Zhang", "Youcai", ""], ["Lan", "Zhonghao", ""], ["Dai", "Yuchen", ""], ["Zeng", "Fangao", ""], ["Bai", "Yan", ""], ["Chang", "Jie", ""], ["Wei", "Yichen", ""]]}, {"id": "2008.01469", "submitter": "Yan Bai", "authors": "Yuke Zhu, Yan Bai, Yichen Wei", "title": "Spherical Feature Transform for Deep Metric Learning", "comments": "ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation in feature space is effective to increase data diversity.\nPrevious methods assume that different classes have the same covariance in\ntheir feature distributions. Thus, feature transform between different classes\nis performed via translation. However, this approach is no longer valid for\nrecent deep metric learning scenarios, where feature normalization is widely\nadopted and all features lie on a hypersphere.\n  This work proposes a novel spherical feature transform approach. It relaxes\nthe assumption of identical covariance between classes to an assumption of\nsimilar covariances of different classes on a hypersphere. Consequently, the\nfeature transform is performed by a rotation that respects the spherical data\ndistributions. We provide a simple and effective training method, and in depth\nanalysis on the relation between the two different transforms. Comprehensive\nexperiments on various deep metric learning benchmarks and different baselines\nverify that our method achieves consistent performance improvement and\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 11:32:23 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Zhu", "Yuke", ""], ["Bai", "Yan", ""], ["Wei", "Yichen", ""]]}, {"id": "2008.01475", "submitter": "Lingxi Xie", "authors": "Lingxi Xie, Xin Chen, Kaifeng Bi, Longhui Wei, Yuhui Xu, Zhengsu Chen,\n  Lanfei Wang, An Xiao, Jianlong Chang, Xiaopeng Zhang, Qi Tian", "title": "Weight-Sharing Neural Architecture Search: A Battle to Shrink the\n  Optimization Gap", "comments": "24 pages, 3 figures, 2 tables, meta data updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) has attracted increasing attentions in both\nacademia and industry. In the early age, researchers mostly applied individual\nsearch methods which sample and evaluate the candidate architectures separately\nand thus incur heavy computational overheads. To alleviate the burden,\nweight-sharing methods were proposed in which exponentially many architectures\nshare weights in the same super-network, and the costly training procedure is\nperformed only once. These methods, though being much faster, often suffer the\nissue of instability. This paper provides a literature review on NAS, in\nparticular the weight-sharing methods, and points out that the major challenge\ncomes from the optimization gap between the super-network and the\nsub-architectures. From this perspective, we summarize existing approaches into\nseveral categories according to their efforts in bridging the gap, and analyze\nboth advantages and disadvantages of these methodologies. Finally, we share our\nopinions on the future directions of NAS and AutoML. Due to the expertise of\nthe authors, this paper mainly focuses on the application of NAS to computer\nvision problems and may bias towards the work in our group.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 11:57:03 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 03:30:13 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Xie", "Lingxi", ""], ["Chen", "Xin", ""], ["Bi", "Kaifeng", ""], ["Wei", "Longhui", ""], ["Xu", "Yuhui", ""], ["Chen", "Zhengsu", ""], ["Wang", "Lanfei", ""], ["Xiao", "An", ""], ["Chang", "Jianlong", ""], ["Zhang", "Xiaopeng", ""], ["Tian", "Qi", ""]]}, {"id": "2008.01478", "submitter": "Mara Graziani Miss", "authors": "Mara Graziani and Sebastian Otalora and Henning Muller and Vincent\n  Andrearczyk", "title": "Guiding CNNs towards Relevant Concepts by Multi-task and Adversarial\n  Learning", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The opaqueness of deep learning limits its deployment in critical application\nscenarios such as cancer grading in medical images. In this paper, a framework\nfor guiding CNN training is built on top of successful existing techniques of\nhard parameter sharing, with the main goal of explicitly introducing expert\nknowledge in the training objectives. The learning process is guided by\nidentifying concepts that are relevant or misleading for the task. Relevant\nconcepts are encouraged to appear in the representation through multi-task\nlearning. Undesired and misleading concepts are discouraged by a gradient\nreversal operation. In this way, a shift in the deep representations can be\ncorrected to match the clinicians' assumptions. The application on breast lymph\nnodes histopathology data from the Camelyon challenge shows a significant\nincrease in the generalization performance on unseen patients (from 0.839 to\n0.864 average AUC, $\\text{p-value} = 0,0002$) when the internal representations\nare controlled by prior knowledge regarding the acquisition center and visual\nfeatures of the tissue. The code will be shared for reproducibility on our\nGitHub repository.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 12:10:35 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Graziani", "Mara", ""], ["Otalora", "Sebastian", ""], ["Muller", "Henning", ""], ["Andrearczyk", "Vincent", ""]]}, {"id": "2008.01483", "submitter": "Alan Smeaton", "authors": "Alan F. Smeaton and Swathikiran Srungavarapu and Cyril Messaraa and\n  Claire Tansey", "title": "Tracking Skin Colour and Wrinkle Changes During Cosmetic Product Trials\n  Using Smartphone Images", "comments": "17 pages, 12 figures, 3 tables. This is the submitted version, the\n  definitive published version is at\n  https://onlinelibrary.wiley.com/doi/abs/10.1111/srt.12928", "journal-ref": null, "doi": "10.1111/srt.12928", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: To explore how the efficacy of product trials for skin cosmetics\ncan be improved through the use of consumer-level images taken by volunteers\nusing a conventional smartphone.\n  Materials and Methods: 12 women aged 30 to 60 years participated in a product\ntrial and had close-up images of the cheek and temple regions of their faces\ntaken with a high-resolution Antera 3D CS camera at the start and end of a\n4-week period. Additionally, they each had ``selfies'' of the same regions of\ntheir faces taken regularly throughout the trial period. Automatic image\nanalysis to identify changes in skin colour used three kinds of colour\nnormalisation and analysis for wrinkle composition identified edges and\ncalculated their magnitude.\n  Results: Images taken at the start and end of the trial acted as baseline\nground truth for normalisation of smartphone images and showed large changes in\nboth colour and wrinkle magnitude during the trial for many volunteers.\nConclusions: Results demonstrate that regular use of selfie smartphone images\nwithin trial periods can add value to interpretation of the efficacy of the\ntrial.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 12:19:44 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Smeaton", "Alan F.", ""], ["Srungavarapu", "Swathikiran", ""], ["Messaraa", "Cyril", ""], ["Tansey", "Claire", ""]]}, {"id": "2008.01484", "submitter": "Michael Firman", "authors": "Jamie Watson, Oisin Mac Aodha, Daniyar Turmukhambetov, Gabriel J.\n  Brostow, Michael Firman", "title": "Learning Stereo from Single Images", "comments": "Accepted as an oral presentation at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised deep networks are among the best methods for finding\ncorrespondences in stereo image pairs. Like all supervised approaches, these\nnetworks require ground truth data during training. However, collecting large\nquantities of accurate dense correspondence data is very challenging. We\npropose that it is unnecessary to have such a high reliance on ground truth\ndepths or even corresponding stereo pairs. Inspired by recent progress in\nmonocular depth estimation, we generate plausible disparity maps from single\nimages. In turn, we use those flawed disparity maps in a carefully designed\npipeline to generate stereo training pairs. Training in this manner makes it\npossible to convert any collection of single RGB images into stereo training\ndata. This results in a significant reduction in human effort, with no need to\ncollect real depths or to hand-design synthetic data. We can consequently train\na stereo matching network from scratch on datasets like COCO, which were\npreviously hard to exploit for stereo. Through extensive experiments we show\nthat our approach outperforms stereo networks trained with standard synthetic\ndatasets, when evaluated on KITTI, ETH3D, and Middlebury.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 12:22:21 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 18:11:25 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Watson", "Jamie", ""], ["Mac Aodha", "Oisin", ""], ["Turmukhambetov", "Daniyar", ""], ["Brostow", "Gabriel J.", ""], ["Firman", "Michael", ""]]}, {"id": "2008.01503", "submitter": "Ming-Wei Li", "authors": "Ming-Wei Li, Qing-Yuan Jiang, Wu-Jun Li", "title": "Multiple Code Hashing for Efficient Image Retrieval", "comments": "12 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its low storage cost and fast query speed, hashing has been widely\nused in large-scale image retrieval tasks. Hash bucket search returns data\npoints within a given Hamming radius to each query, which can enable search at\na constant or sub-linear time cost. However, existing hashing methods cannot\nachieve satisfactory retrieval performance for hash bucket search in complex\nscenarios, since they learn only one hash code for each image. More\nspecifically, by using one hash code to represent one image, existing methods\nmight fail to put similar image pairs to the buckets with a small Hamming\ndistance to the query when the semantic information of images is complex. As a\nresult, a large number of hash buckets need to be visited for retrieving\nsimilar images, based on the learned codes. This will deteriorate the\nefficiency of hash bucket search. In this paper, we propose a novel hashing\nframework, called multiple code hashing (MCH), to improve the performance of\nhash bucket search. The main idea of MCH is to learn multiple hash codes for\neach image, with each code representing a different region of the image.\nFurthermore, we propose a deep reinforcement learning algorithm to learn the\nparameters in MCH. To the best of our knowledge, this is the first work that\nproposes to learn multiple hash codes for each image in image retrieval.\nExperiments demonstrate that MCH can achieve a significant improvement in hash\nbucket search, compared with existing methods that learn only one hash code for\neach image.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 13:18:19 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Li", "Ming-Wei", ""], ["Jiang", "Qing-Yuan", ""], ["Li", "Wu-Jun", ""]]}, {"id": "2008.01510", "submitter": "Enrico Fini", "authors": "Enrico Fini, St\\'ephane Lathuili\\`ere, Enver Sangineto, Moin Nabi,\n  Elisa Ricci", "title": "Online Continual Learning under Extreme Memory Constraints", "comments": "Accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual Learning (CL) aims to develop agents emulating the human ability to\nsequentially learn new tasks while being able to retain knowledge obtained from\npast experiences. In this paper, we introduce the novel problem of\nMemory-Constrained Online Continual Learning (MC-OCL) which imposes strict\nconstraints on the memory overhead that a possible algorithm can use to avoid\ncatastrophic forgetting. As most, if not all, previous CL methods violate these\nconstraints, we propose an algorithmic solution to MC-OCL: Batch-level\nDistillation (BLD), a regularization-based CL approach, which effectively\nbalances stability and plasticity in order to learn from data streams, while\npreserving the ability to solve old tasks through distillation. Our extensive\nexperimental evaluation, conducted on three publicly available benchmarks,\nempirically demonstrates that our approach successfully addresses the MC-OCL\nproblem and achieves comparable accuracy to prior distillation methods\nrequiring higher memory overhead.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 13:25:26 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 09:10:34 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Fini", "Enrico", ""], ["Lathuili\u00e8re", "St\u00e9phane", ""], ["Sangineto", "Enver", ""], ["Nabi", "Moin", ""], ["Ricci", "Elisa", ""]]}, {"id": "2008.01550", "submitter": "Xinge Zhu", "authors": "Hui Zhou, Xinge Zhu, Xiao Song, Yuexin Ma, Zhe Wang, Hongsheng Li,\n  Dahua Lin", "title": "Cylinder3D: An Effective 3D Framework for Driving-scene LiDAR Semantic\n  Segmentation", "comments": "Source code: https://github.com/xinge008/Cylinder3D", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  State-of-the-art methods for large-scale driving-scene LiDAR semantic\nsegmentation often project and process the point clouds in the 2D space. The\nprojection methods includes spherical projection, bird-eye view projection,\netc. Although this process makes the point cloud suitable for the 2D CNN-based\nnetworks, it inevitably alters and abandons the 3D topology and geometric\nrelations. A straightforward solution to tackle the issue of 3D-to-2D\nprojection is to keep the 3D representation and process the points in the 3D\nspace. In this work, we first perform an in-depth analysis for different\nrepresentations and backbones in 2D and 3D spaces, and reveal the effectiveness\nof 3D representations and networks on LiDAR segmentation. Then, we develop a 3D\ncylinder partition and a 3D cylinder convolution based framework, termed as\nCylinder3D, which exploits the 3D topology relations and structures of\ndriving-scene point clouds. Moreover, a dimension-decomposition based context\nmodeling module is introduced to explore the high-rank context information in\npoint clouds in a progressive manner. We evaluate the proposed model on a\nlarge-scale driving-scene dataset, i.e. SematicKITTI. Our method achieves\nstate-of-the-art performance and outperforms existing methods by 6% in terms of\nmIoU.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 13:56:19 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Zhou", "Hui", ""], ["Zhu", "Xinge", ""], ["Song", "Xiao", ""], ["Ma", "Yuexin", ""], ["Wang", "Zhe", ""], ["Li", "Hongsheng", ""], ["Lin", "Dahua", ""]]}, {"id": "2008.01567", "submitter": "Soumendu Majee", "authors": "Soumendu Majee, Thilo Balke, Craig A.J. Kemp, Gregery T. Buzzard,\n  Charles A. Bouman", "title": "Multi-Slice Fusion for Sparse-View and Limited-Angle 4D CT\n  Reconstruction", "comments": "arXiv admin note: substantial text overlap with arXiv:1906.06601", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse problems spanning four or more dimensions such as space, time and\nother independent parameters have become increasingly important.\nState-of-the-art 4D reconstruction methods use model based iterative\nreconstruction (MBIR), but depend critically on the quality of the prior\nmodeling. Recently, plug-and-play (PnP) methods have been shown to be an\neffective way to incorporate advanced prior models using state-of-the-art\ndenoising algorithms. However, state-of-the-art denoisers such as BM4D and deep\nconvolutional neural networks (CNNs) are primarily available for 2D or 3D\nimages and extending them to higher dimensions is difficult due to algorithmic\ncomplexity and the increased difficulty of effective training.\n  In this paper, we present multi-slice fusion, a novel algorithm for 4D\nreconstruction, based on the fusion of multiple low-dimensional denoisers. Our\napproach uses multi-agent consensus equilibrium (MACE), an extension of\nplug-and-play, as a framework for integrating the multiple lower-dimensional\nmodels. We apply our method to 4D cone-beam X-ray CT reconstruction for non\ndestructive evaluation (NDE) of samples that are dynamically moving during\nacquisition. We implement multi-slice fusion on distributed, heterogeneous\nclusters in order to reconstruct large 4D volumes in reasonable time and\ndemonstrate the inherent parallelizable nature of the algorithm. We present\nsimulated and real experimental results on sparse-view and limited-angle CT\ndata to demonstrate that multi-slice fusion can substantially improve the\nquality of reconstructions relative to traditional methods, while also being\npractical to implement and train.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 02:32:43 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 19:20:03 GMT"}, {"version": "v3", "created": "Sat, 20 Feb 2021 01:06:06 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Majee", "Soumendu", ""], ["Balke", "Thilo", ""], ["Kemp", "Craig A. J.", ""], ["Buzzard", "Gregery T.", ""], ["Bouman", "Charles A.", ""]]}, {"id": "2008.01574", "submitter": "Fei Wen", "authors": "Fei Wen, Hewen Wei, Yipeng Liu, and Peilin Liu", "title": "Simultaneous Consensus Maximization and Model Fitting", "comments": "16 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum consensus (MC) robust fitting is a fundamental problem in low-level\nvision to process raw-data. Typically, it firstly finds a consensus set of\ninliers and then fits a model on the consensus set. This work proposes a new\nformulation to achieve simultaneous maximum consensus and model estimation\n(MCME), which has two significant features compared with traditional MC robust\nfitting. First, it takes fitting residual into account in finding inliers,\nhence its lowest achievable residual in model fitting is lower than that of MC\nrobust fitting. Second, it has an unconstrained formulation involving binary\nvariables, which facilitates the use of the effective semidefinite relaxation\n(SDR) method to handle the underlying challenging combinatorial optimization\nproblem. Though still nonconvex after SDR, it becomes biconvex in some\napplications, for which we use an alternating minimization algorithm to solve.\nFurther, the sparsity of the problem is exploited in combination with low-rank\nfactorization to develop an efficient algorithm. Experiments show that MCME\nsignificantly outperforms RANSAC and deterministic approximate MC methods at\nhigh outlier ratios. Besides, in rotation and Euclidean registration, it also\ncompares favorably with state-of-the-art registration methods, especially in\nhigh noise and outliers. Code is available at\n\\textit{https://github.com/FWen/mcme.git}.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 14:10:41 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Wen", "Fei", ""], ["Wei", "Hewen", ""], ["Liu", "Yipeng", ""], ["Liu", "Peilin", ""]]}, {"id": "2008.01576", "submitter": "Xihui Liu", "authors": "Xihui Liu, Zhe Lin, Jianming Zhang, Handong Zhao, Quan Tran, Xiaogang\n  Wang, Hongsheng Li", "title": "Open-Edit: Open-Domain Image Manipulation with Open-Vocabulary\n  Instructions", "comments": "ECCV 2020. Introduction video at https://youtu.be/8E3bwvjCHYE and\n  code at https://github.com/xh-liu/Open-Edit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel algorithm, named Open-Edit, which is the first attempt on\nopen-domain image manipulation with open-vocabulary instructions. It is a\nchallenging task considering the large variation of image domains and the lack\nof training supervision. Our approach takes advantage of the unified\nvisual-semantic embedding space pretrained on a general image-caption dataset,\nand manipulates the embedded visual features by applying text-guided vector\narithmetic on the image feature maps. A structure-preserving image decoder then\ngenerates the manipulated images from the manipulated feature maps. We further\npropose an on-the-fly sample-specific optimization approach with\ncycle-consistency constraints to regularize the manipulated images and force\nthem to preserve details of the source images. Our approach shows promising\nresults in manipulating open-vocabulary color, texture, and high-level\nattributes for various scenarios of open-domain images.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 14:15:40 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 13:31:55 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Liu", "Xihui", ""], ["Lin", "Zhe", ""], ["Zhang", "Jianming", ""], ["Zhao", "Handong", ""], ["Tran", "Quan", ""], ["Wang", "Xiaogang", ""], ["Li", "Hongsheng", ""]]}, {"id": "2008.01584", "submitter": "David Cian", "authors": "David Cian, Jan van Gemert, Attila Lengyel", "title": "Evaluating the performance of the LIME and Grad-CAM explanation methods\n  on a LEGO multi-label image classification task", "comments": "Supervision by Jan van Gemert and Attila Lengyel", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we run two methods of explanation, namely LIME and Grad-CAM,\non a convolutional neural network trained to label images with the LEGO bricks\nthat are visible in them. We evaluate them on two criteria, the improvement of\nthe network's core performance and the trust they are able to generate for\nusers of the system. We find that in general, Grad-CAM seems to outperform LIME\non this specific task: it yields more detailed insight from the point of view\nof core performance and 80\\% of respondents asked to choose between them when\nit comes to the trust they inspire in the model choose Grad-CAM. However, we\nalso posit that it is more useful to employ these two methods together, as the\ninsights they yield are complementary.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 14:27:13 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Cian", "David", ""], ["van Gemert", "Jan", ""], ["Lengyel", "Attila", ""]]}, {"id": "2008.01589", "submitter": "Levi Osterno Vasconcelos", "authors": "Levi O. Vasconcelos, Massimiliano Mancini, Davide Boscaini, Samuel\n  Rota Bulo, Barbara Caputo, Elisa Ricci", "title": "Shape Consistent 2D Keypoint Estimation under Domain Shift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent unsupervised domain adaptation methods based on deep architectures\nhave shown remarkable performance not only in traditional classification tasks\nbut also in more complex problems involving structured predictions (e.g.\nsemantic segmentation, depth estimation). Following this trend, in this paper\nwe present a novel deep adaptation framework for estimating keypoints under\ndomain shift}, i.e. when the training (source) and the test (target) images\nsignificantly differ in terms of visual appearance. Our method seamlessly\ncombines three different components: feature alignment, adversarial training\nand self-supervision. Specifically, our deep architecture leverages from\ndomain-specific distribution alignment layers to perform target adaptation at\nthe feature level. Furthermore, a novel loss is proposed which combines an\nadversarial term for ensuring aligned predictions in the output space and a\ngeometric consistency term which guarantees coherent predictions between a\ntarget sample and its perturbed version. Our extensive experimental evaluation\nconducted on three publicly available benchmarks shows that our approach\noutperforms state-of-the-art domain adaptation methods in the 2D keypoint\nprediction task.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 14:32:06 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Vasconcelos", "Levi O.", ""], ["Mancini", "Massimiliano", ""], ["Boscaini", "Davide", ""], ["Bulo", "Samuel Rota", ""], ["Caputo", "Barbara", ""], ["Ricci", "Elisa", ""]]}, {"id": "2008.01635", "submitter": "Silvia Liberata Ullo", "authors": "R. Ganesh Babu, K. Uma Maheswari, C. Zarro, B. D. Parameshachari, and\n  S. L. Ullo", "title": "Land Use and Land Cover Classification using a Human Group based\n  Particle Swarm Optimization Algorithm with a LSTM classifier on\n  hybrid-pre-processing Remote Sensing Images", "comments": "21 pages, 11 figures, submitted to the Special Issue Multimedia\n  Vision and Machine Learning for Remote Sensing di MDPI Remote Sensing (in\n  review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Land use and land cover (LULC) classification using remote sensing imagery\nplays a vital role in many environment modeling and land use inventories. In\nthis study, a hybrid feature optimization algorithm along with a deep learning\nclassifier is proposed to improve the performance of LULC classification,\nhelping to predict wildlife habitat, deteriorating environmental quality,\nhaphazard, etc. LULC classification is assessed using Sat 4, Sat 6 and Eurosat\ndatasets. After the selection of remote sensing images, normalization and\nhistogram equalization methods are used to improve the quality of the images.\nThen, a hybrid optimization is accomplished by using the Local Gabor Binary\nPattern Histogram Sequence (LGBPHS), the Histogram of Oriented Gradient (HOG)\nand Haralick texture features, for the feature extraction from the selected\nimages. The benefits of this hybrid optimization are a high discriminative\npower and invariance to color and grayscale images. Next, a Human Group based\nParticle Swarm Optimization (PSO) algorithm is applied to select the optimal\nfeatures, whose benefits are fast convergence rate and easy to implement. After\nselecting the optimal feature values, a Long Short Term Memory (LSTM) network\nis utilized to classify the LULC classes. Experimental results showed that the\nHuman Group based PSO algorithm with a LSTM classifier effectively well\ndifferentiates the land use and land cover classes in terms of classification\naccuracy, recall and precision. An improvement of 2.56% in accuracy is achieved\ncompared to the existing models GoogleNet, VGG, AlexNet, ConvNet, when the\nproposed method is applied.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 15:30:10 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 18:33:09 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Babu", "R. Ganesh", ""], ["Maheswari", "K. Uma", ""], ["Zarro", "C.", ""], ["Parameshachari", "B. D.", ""], ["Ullo", "S. L.", ""]]}, {"id": "2008.01639", "submitter": "Edgar Tretschk", "authors": "Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollh\\\"ofer,\n  Carsten Stoll, Christian Theobalt", "title": "PatchNets: Patch-Based Generalizable Deep Implicit 3D Shape\n  Representations", "comments": "25 pages, including supplementary material. Code:\n  https://github.com/edgar-tr/patchnets Project page:\n  https://gvv.mpi-inf.mpg.de/projects/PatchNets/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit surface representations, such as signed-distance functions, combined\nwith deep learning have led to impressive models which can represent detailed\nshapes of objects with arbitrary topology. Since a continuous function is\nlearned, the reconstructions can also be extracted at any arbitrary resolution.\nHowever, large datasets such as ShapeNet are required to train such models. In\nthis paper, we present a new mid-level patch-based surface representation. At\nthe level of patches, objects across different categories share similarities,\nwhich leads to more generalizable models. We then introduce a novel method to\nlearn this patch-based representation in a canonical space, such that it is as\nobject-agnostic as possible. We show that our representation trained on one\ncategory of objects from ShapeNet can also well represent detailed shapes from\nany other category. In addition, it can be trained using much fewer shapes,\ncompared to existing approaches. We show several applications of our new\nrepresentation, including shape interpolation and partial point cloud\ncompletion. Due to explicit control over positions, orientations and scales of\npatches, our representation is also more controllable compared to object-level\nrepresentations, which enables us to deform encoded shapes non-rigidly.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 15:34:46 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 13:32:02 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Tretschk", "Edgar", ""], ["Tewari", "Ayush", ""], ["Golyanik", "Vladislav", ""], ["Zollh\u00f6fer", "Michael", ""], ["Stoll", "Carsten", ""], ["Theobalt", "Christian", ""]]}, {"id": "2008.01645", "submitter": "Takanori Fujiwara", "authors": "Takanori Fujiwara, Shilpika, Naohisa Sakamoto, Jorji Nonaka, Keiji\n  Yamamoto, and Kwan-Liu Ma", "title": "A Visual Analytics Framework for Reviewing Multivariate Time-Series Data\n  with Dimensionality Reduction", "comments": "To appear in IEEE Transactions on Visualization and Computer Graphics\n  and IEEE VIS 2020 (VAST)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven problem solving in many real-world applications involves analysis\nof time-dependent multivariate data, for which dimensionality reduction (DR)\nmethods are often used to uncover the intrinsic structure and features of the\ndata. However, DR is usually applied to a subset of data that is either\nsingle-time-point multivariate or univariate time-series, resulting in the need\nto manually examine and correlate the DR results out of different data subsets.\nWhen the number of dimensions is large either in terms of the number of time\npoints or attributes, this manual task becomes too tedious and infeasible. In\nthis paper, we present MulTiDR, a new DR framework that enables processing of\ntime-dependent multivariate data as a whole to provide a comprehensive overview\nof the data. With the framework, we employ DR in two steps. When treating the\ninstances, time points, and attributes of the data as a 3D array, the first DR\nstep reduces the three axes of the array to two, and the second DR step\nvisualizes the data in a lower-dimensional space. In addition, by coupling with\na contrastive learning method and interactive visualizations, our framework\nenhances analysts' ability to interpret DR results. We demonstrate the\neffectiveness of our framework with four case studies using real-world\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 04:22:43 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 00:37:25 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Fujiwara", "Takanori", ""], ["Shilpika", "", ""], ["Sakamoto", "Naohisa", ""], ["Nonaka", "Jorji", ""], ["Yamamoto", "Keiji", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "2008.01652", "submitter": "Yanhui Guo", "authors": "Yanhui Guo, Xi Zhang, Xiaolin Wu", "title": "Deep Multi-modality Soft-decoding of Very Low Bit-rate Face Videos", "comments": "Accepted by Proceedings of the 28th ACM International Conference on\n  Multimedia(ACM MM),2020", "journal-ref": "Proceedings of the 28th ACM International Conference on\n  Multimedia,2020", "doi": "10.1145/3394171.3413709", "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep multi-modality neural network for restoring very low\nbit rate videos of talking heads. Such video contents are very common in social\nmedia, teleconferencing, distance education, tele-medicine, etc., and often\nneed to be transmitted with limited bandwidth. The proposed CNN method exploits\nthe correlations among three modalities, video, audio and emotion state of the\nspeaker, to remove the video compression artifacts caused by spatial down\nsampling and quantization. The deep learning approach turns out to be ideally\nsuited for the video restoration task, as the complex non-linear cross-modality\ncorrelations are very difficult to model analytically and explicitly. The new\nmethod is a video post processor that can significantly boost the perceptual\nquality of aggressively compressed talking head videos, while being fully\ncompatible with all existing video compression standards.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 04:38:59 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Guo", "Yanhui", ""], ["Zhang", "Xi", ""], ["Wu", "Xiaolin", ""]]}, {"id": "2008.01677", "submitter": "Binhui Xie", "authors": "Shuang Li, Binhui Xie, Jiashu Wu, Ying Zhao, Chi Harold Liu, Zhengming\n  Ding", "title": "Simultaneous Semantic Alignment Network for Heterogeneous Domain\n  Adaptation", "comments": "Accepted at ACM MM 2020", "journal-ref": null, "doi": "10.1145/3394171.3413995", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous domain adaptation (HDA) transfers knowledge across source and\ntarget domains that present heterogeneities e.g., distinct domain distributions\nand difference in feature type or dimension. Most previous HDA methods tackle\nthis problem through learning a domain-invariant feature subspace to reduce the\ndiscrepancy between domains. However, the intrinsic semantic properties\ncontained in data are under-explored in such alignment strategy, which is also\nindispensable to achieve promising adaptability. In this paper, we propose a\nSimultaneous Semantic Alignment Network (SSAN) to simultaneously exploit\ncorrelations among categories and align the centroids for each category across\ndomains. In particular, we propose an implicit semantic correlation loss to\ntransfer the correlation knowledge of source categorical prediction\ndistributions to target domain. Meanwhile, by leveraging target pseudo-labels,\na robust triplet-centroid alignment mechanism is explicitly applied to align\nfeature representations for each category. Notably, a pseudo-label refinement\nprocedure with geometric similarity involved is introduced to enhance the\ntarget pseudo-label assignment accuracy. Comprehensive experiments on various\nHDA tasks across text-to-image, image-to-image and text-to-text successfully\nvalidate the superiority of our SSAN against state-of-the-art HDA methods. The\ncode is publicly available at https://github.com/BIT-DA/SSAN.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 16:20:37 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 03:04:20 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Li", "Shuang", ""], ["Xie", "Binhui", ""], ["Wu", "Jiashu", ""], ["Zhao", "Ying", ""], ["Liu", "Chi Harold", ""], ["Ding", "Zhengming", ""]]}, {"id": "2008.01679", "submitter": "Junqi Zhao", "authors": "Junqi Zhao and Esther Obonyo", "title": "Applying Incremental Deep Neural Networks-based Posture Recognition\n  Model for Injury Risk Assessment in Construction", "comments": "27 pages, journal manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring awkward postures is a proactive prevention for Musculoskeletal\nDisorders (MSDs)in construction. Machine Learning (ML) models have shown\npromising results for posture recognition from Wearable Sensors. However,\nfurther investigations are needed concerning: i) Incremental Learning (IL),\nwhere trained models adapt to learn new postures and control the forgetting of\nlearned postures; ii) MSDs assessment with recognized postures. This study\nproposed an incremental Convolutional Long Short-Term Memory (CLN) model,\ninvestigated effective IL strategies, and evaluated MSDs assessment using\nrecognized postures. Tests with nine workers showed the CLN model with shallow\nconvolutional layers achieved high recognition performance (F1 Score) under\npersonalized (0.87) and generalized (0.84) modeling. Generalized shallow CLN\nmodel under Many-to-One IL scheme can balance the adaptation (0.73) and\nforgetting of learnt subjects (0.74). MSDs assessment using postures recognized\nfrom incremental CLN model had minor difference with ground-truth, which\ndemonstrates the high potential for automated MSDs monitoring in construction.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 16:27:25 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Zhao", "Junqi", ""], ["Obonyo", "Esther", ""]]}, {"id": "2008.01681", "submitter": "Shihua Huang", "authors": "Shihua Huang, Cheng He, Ran Cheng", "title": "Multimodal Image-to-Image Translation via a Single Generative\n  Adversarial Network", "comments": "pages 13, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant advances in image-to-image (I2I) translation with\nGenerative Adversarial Networks (GANs) have been made, it remains challenging\nto effectively translate an image to a set of diverse images in multiple target\ndomains using a pair of generator and discriminator. Existing multimodal I2I\ntranslation methods adopt multiple domain-specific content encoders for\ndifferent domains, where each domain-specific content encoder is trained with\nimages from the same domain only. Nevertheless, we argue that the content\n(domain-invariant) features should be learned from images among all the\ndomains. Consequently, each domain-specific content encoder of existing schemes\nfails to extract the domain-invariant features efficiently. To address this\nissue, we present a flexible and general SoloGAN model for efficient multimodal\nI2I translation among multiple domains with unpaired data. In contrast to\nexisting methods, the SoloGAN algorithm uses a single projection discriminator\nwith an additional auxiliary classifier, and shares the encoder and generator\nfor all domains. As such, the SoloGAN model can be trained effectively with\nimages from all domains such that the domain-invariant content representation\ncan be efficiently extracted. Qualitative and quantitative results over a wide\nrange of datasets against several counterparts and variants of the SoloGAN\nmodel demonstrate the merits of the method, especially for the challenging I2I\ntranslation tasks, i.e., tasks that involve extreme shape variations or need to\nkeep the complex backgrounds unchanged after translations. Furthermore, we\ndemonstrate the contribution of each component using ablation studies.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 16:31:15 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Huang", "Shihua", ""], ["He", "Cheng", ""], ["Cheng", "Ran", ""]]}, {"id": "2008.01699", "submitter": "Murari Mandal", "authors": "Murari Mandal, Lav Kush Kumar, Santosh Kumar Vipparthi", "title": "MOR-UAV: A Benchmark Dataset and Baselines for Moving Object Recognition\n  in UAV Videos", "comments": null, "journal-ref": "ACM Multimedia (ACM MM-2020)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual data collected from Unmanned Aerial Vehicles (UAVs) has opened a new\nfrontier of computer vision that requires automated analysis of aerial\nimages/videos. However, the existing UAV datasets primarily focus on object\ndetection. An object detector does not differentiate between the moving and\nnon-moving objects. Given a real-time UAV video stream, how can we both\nlocalize and classify the moving objects, i.e. perform moving object\nrecognition (MOR)? The MOR is one of the essential tasks to support various UAV\nvision-based applications including aerial surveillance, search and rescue,\nevent recognition, urban and rural scene understanding.To the best of our\nknowledge, no labeled dataset is available for MOR evaluation in UAV videos.\nTherefore, in this paper, we introduce MOR-UAV, a large-scale video dataset for\nMOR in aerial videos. We achieve this by labeling axis-aligned bounding boxes\nfor moving objects which requires less computational resources than producing\npixel-level estimates. We annotate 89,783 moving object instances collected\nfrom 30 UAV videos, consisting of 10,948 frames in various scenarios such as\nweather conditions, occlusion, changing flying altitude and multiple camera\nviews. We assigned the labels for two categories of vehicles (car and heavy\nvehicle). Furthermore, we propose a deep unified framework MOR-UAVNet for MOR\nin UAV videos. Since, this is a first attempt for MOR in UAV videos, we present\n16 baseline results based on the proposed framework over the MOR-UAV dataset\nthrough quantitative and qualitative experiments. We also analyze the\nmotion-salient regions in the network through multiple layer visualizations.\nThe MOR-UAVNet works online at inference as it requires only few past frames.\nMoreover, it doesn't require predefined target initialization from user.\nExperiments also demonstrate that the MOR-UAV dataset is quite challenging.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 17:02:29 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 04:28:59 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Mandal", "Murari", ""], ["Kumar", "Lav Kush", ""], ["Vipparthi", "Santosh Kumar", ""]]}, {"id": "2008.01701", "submitter": "Aupendu Kar", "authors": "Aupendu Kar, Sobhan Kanti Dhara, Debashis Sen, Prabir Kumar Biswas", "title": "Transmission Map and Atmospheric Light Guided Iterative Updater Network\n  for Single Image Dehazing", "comments": "First two authors contributed equally. This work has been submitted\n  to the IEEE for possible publication. Copyright may be transferred without\n  notice, after which this version may no longer be accessible. Project\n  Website: https://aupendu.github.io/iterative-dehaze", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hazy images obscure content visibility and hinder several subsequent computer\nvision tasks. For dehazing in a wide variety of hazy conditions, an end-to-end\ndeep network jointly estimating the dehazed image along with suitable\ntransmission map and atmospheric light for guidance could prove effective. To\nthis end, we propose an Iterative Prior Updated Dehazing Network (IPUDN) based\non a novel iterative update framework. We present a novel convolutional\narchitecture to estimate channel-wise atmospheric light, which along with an\nestimated transmission map are used as priors for the dehazing network. Use of\nchannel-wise atmospheric light allows our network to handle color casts in hazy\nimages. In our IPUDN, the transmission map and atmospheric light estimates are\nupdated iteratively using corresponding novel updater networks. The iterative\nmechanism is leveraged to gradually modify the estimates toward those\nappropriately representing the hazy condition. These updates occur jointly with\nthe iterative estimation of the dehazed image using a convolutional neural\nnetwork with LSTM driven recurrence, which introduces inter-iteration\ndependencies. Our approach is qualitatively and quantitatively found effective\nfor synthetic and real-world hazy images depicting varied hazy conditions, and\nit outperforms the state-of-the-art. Thorough analyses of IPUDN through\nadditional experiments and detailed ablation studies are also presented.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 17:05:48 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Kar", "Aupendu", ""], ["Dhara", "Sobhan Kanti", ""], ["Sen", "Debashis", ""], ["Biswas", "Prabir Kumar", ""]]}, {"id": "2008.01774", "submitter": "Yiqiu Shen", "authors": "Farah E. Shamout, Yiqiu Shen, Nan Wu, Aakash Kaku, Jungkyu Park, Taro\n  Makino, Stanis{\\l}aw Jastrz\\k{e}bski, Jan Witowski, Duo Wang, Ben Zhang,\n  Siddhant Dogra, Meng Cao, Narges Razavian, David Kudlowitz, Lea Azour,\n  William Moore, Yvonne W. Lui, Yindalon Aphinyanaphongs, Carlos\n  Fernandez-Granda, Krzysztof J. Geras", "title": "An artificial intelligence system for predicting the deterioration of\n  COVID-19 patients in the emergency department", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the coronavirus disease 2019 (COVID-19) pandemic, rapid and accurate\ntriage of patients at the emergency department is critical to inform\ndecision-making. We propose a data-driven approach for automatic prediction of\ndeterioration risk using a deep neural network that learns from chest X-ray\nimages and a gradient boosting model that learns from routine clinical\nvariables. Our AI prognosis system, trained using data from 3,661 patients,\nachieves an area under the receiver operating characteristic curve (AUC) of\n0.786 (95% CI: 0.745-0.830) when predicting deterioration within 96 hours. The\ndeep neural network extracts informative areas of chest X-ray images to assist\nclinicians in interpreting the predictions and performs comparably to two\nradiologists in a reader study. In order to verify performance in a real\nclinical setting, we silently deployed a preliminary version of the deep neural\nnetwork at New York University Langone Health during the first wave of the\npandemic, which produced accurate predictions in real-time. In summary, our\nfindings demonstrate the potential of the proposed system for assisting\nfront-line physicians in the triage of COVID-19 patients.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 19:20:31 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 02:36:36 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Shamout", "Farah E.", ""], ["Shen", "Yiqiu", ""], ["Wu", "Nan", ""], ["Kaku", "Aakash", ""], ["Park", "Jungkyu", ""], ["Makino", "Taro", ""], ["Jastrz\u0119bski", "Stanis\u0142aw", ""], ["Witowski", "Jan", ""], ["Wang", "Duo", ""], ["Zhang", "Ben", ""], ["Dogra", "Siddhant", ""], ["Cao", "Meng", ""], ["Razavian", "Narges", ""], ["Kudlowitz", "David", ""], ["Azour", "Lea", ""], ["Moore", "William", ""], ["Lui", "Yvonne W.", ""], ["Aphinyanaphongs", "Yindalon", ""], ["Fernandez-Granda", "Carlos", ""], ["Geras", "Krzysztof J.", ""]]}, {"id": "2008.01777", "submitter": "Patrick Esser", "authors": "Robin Rombach, Patrick Esser, Bj\\\"orn Ommer", "title": "Making Sense of CNNs: Interpreting Deep Representations & Their\n  Invariances with INNs", "comments": "ECCV 2020. Project page and code at\n  https://compvis.github.io/invariances/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To tackle increasingly complex tasks, it has become an essential ability of\nneural networks to learn abstract representations. These task-specific\nrepresentations and, particularly, the invariances they capture turn neural\nnetworks into black box models that lack interpretability. To open such a black\nbox, it is, therefore, crucial to uncover the different semantic concepts a\nmodel has learned as well as those that it has learned to be invariant to. We\npresent an approach based on INNs that (i) recovers the task-specific, learned\ninvariances by disentangling the remaining factor of variation in the data and\nthat (ii) invertibly transforms these recovered invariances combined with the\nmodel representation into an equally expressive one with accessible semantic\nconcepts. As a consequence, neural network representations become\nunderstandable by providing the means to (i) expose their semantic meaning,\n(ii) semantically modify a representation, and (iii) visualize individual\nlearned semantic concepts and invariances. Our invertible approach\nsignificantly extends the abilities to understand black box models by enabling\npost-hoc interpretations of state-of-the-art networks without compromising\ntheir performance. Our implementation is available at\nhttps://compvis.github.io/invariances/ .\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 19:27:46 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Rombach", "Robin", ""], ["Esser", "Patrick", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "2008.01780", "submitter": "Sejal Bhalla", "authors": "Dikshant Sagar, Jatin Garg, Prarthana Kansal, Sejal Bhalla, Rajiv Ratn\n  Shah and Yi Yu", "title": "PAI-BPR: Personalized Outfit Recommendation Scheme with Attribute-wise\n  Interpretability", "comments": "10 pages, 5 figures, to be published in IEEE BigMM, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion is an important part of human experience. Events such as interviews,\nmeetings, marriages, etc. are often based on clothing styles. The rise in the\nfashion industry and its effect on social influencing have made outfit\ncompatibility a need. Thus, it necessitates an outfit compatibility model to\naid people in clothing recommendation. However, due to the highly subjective\nnature of compatibility, it is necessary to account for personalization. Our\npaper devises an attribute-wise interpretable compatibility scheme with\npersonal preference modelling which captures user-item interaction along with\ngeneral item-item interaction. Our work solves the problem of interpretability\nin clothing matching by locating the discordant and harmonious attributes\nbetween fashion items. Extensive experiment results on IQON3000, a publicly\navailable real-world dataset, verify the effectiveness of the proposed model.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 19:30:06 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Sagar", "Dikshant", ""], ["Garg", "Jatin", ""], ["Kansal", "Prarthana", ""], ["Bhalla", "Sejal", ""], ["Shah", "Rajiv Ratn", ""], ["Yu", "Yi", ""]]}, {"id": "2008.01786", "submitter": "Sabrina Narimene Benassou", "authors": "Sabrina Narimene Benassou, Wuzhen Shi, Feng Jiang", "title": "Entropy Guided Adversarial Model for Weakly Supervised Object\n  Localization", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2020.11.006", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly Supervised Object Localization is challenging because of the lack of\nbounding box annotations. Previous works tend to generate a class activation\nmap i.e CAM to localize the object. Unfortunately, the network activates only\nthe features that discriminate the object and does not activate the whole\nobject. Some methods tend to remove some parts of the object to force the CNN\nto detect other features, whereas, others change the network structure to\ngenerate multiple CAMs from different levels of the model. In this present\narticle, we propose to take advantage of the generalization ability of the\nnetwork and train the model using clean examples and adversarial examples to\nlocalize the whole object. Adversarial examples are typically used to train\nrobust models and are images where a perturbation is added. To get a good\nclassification accuracy, the CNN trained with adversarial examples is forced to\ndetect more features that discriminate the object. We futher propose to apply\nthe shannon entropy on the CAMs generated by the network to guide it during\ntraining. Our method does not erase any part of the image neither does it\nchange the network architecure and extensive experiments show that our Entropy\nGuided Adversarial model (EGA model) improved performance on state of the arts\nbenchmarks for both localization and classification accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 19:39:12 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Benassou", "Sabrina Narimene", ""], ["Shi", "Wuzhen", ""], ["Jiang", "Feng", ""]]}, {"id": "2008.01792", "submitter": "Elcin Huseyn", "authors": "Elcin Huseyn", "title": "Deep Learning Based Early Diagnostics of Parkinsons Disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.PL eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the world, about 7 to 10 million elderly people are suffering from\nParkinson's Disease (PD) disease. Parkinson's disease is a common neurological\ndegenerative disease, and its clinical characteristics are Tremors, rigidity,\nbradykinesia, and decreased autonomy. Its clinical manifestations are very\nsimilar to Multiple System Atrophy (MSA) disorders. Studies have shown that\npatients with Parkinson's disease often reach an irreparable situation when\ndiagnosed, so As Parkinson's disease can be distinguished from MSA disease and\nget an early diagnosis, people are constantly exploring new methods. With the\nadvent of the era of big data, deep learning has made major breakthroughs in\nimage recognition and classification. Therefore, this study proposes to use The\ndeep learning method to realize the diagnosis of Parkinson's disease, multiple\nsystem atrophy, and healthy people. This data source is from Istanbul\nUniversity Cerrahpasa Faculty of Medicine Hospital. The processing of the\noriginal magnetic resonance image (Magnetic Resonance Image, MRI) is guided by\nthe doctor of Istanbul University Cerrahpasa Faculty of Medicine Hospital. The\nfocus of this experiment is to improve the existing neural network so that it\ncan obtain good results in medical image recognition and diagnosis. An improved\nalgorithm was proposed based on the pathological characteristics of Parkinson's\ndisease, and good experimental results were obtained by comparing indicators\nsuch as model loss and accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 19:50:52 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Huseyn", "Elcin", ""]]}, {"id": "2008.01806", "submitter": "Shuai Huang", "authors": "Shuai Huang, James J. Lah, Jason W. Allen, Deqiang Qiu", "title": "Fast Nonconvex $T_2^*$ Mapping Using ADMM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance (MR)-$T_2^*$ mapping is widely used to study hemorrhage,\ncalcification and iron deposition in various clinical applications, it provides\na direct and precise mapping of desired contrast in the tissue. However, the\nlong acquisition time required by conventional 3D high-resolution $T_2^*$\nmapping method causes discomfort to patients and introduces motion artifacts to\nreconstructed images, which limits its wider applicability. In this paper we\naddress this issue by performing $T_2^*$ mapping from undersampled data using\ncompressive sensing (CS). We formulate the reconstruction as a nonconvex\nproblem that can be decomposed into two subproblems. They can be solved either\nseparately via the standard approach or jointly via the alternating direction\nmethod of multipliers (ADMM). Compared to previous CS-based approaches that\nonly apply sparse regularization on the spin density $\\boldsymbol X_0$ and the\nrelaxation rate $\\boldsymbol R_2^*$, our formulation enforces additional sparse\npriors on the $T_2^*$-weighted images at multiple echoes to improve the\nreconstruction performance. We performed convergence analysis of the proposed\nalgorithm, evaluated its performance on in vivo data, and studied the effects\nof different sampling schemes. Experimental results showed that the proposed\njoint-recovery approach generally outperforms the state-of-the-art method,\nespecially in the low-sampling rate regime, making it a preferred choice to\nperform fast 3D $T_2^*$ mapping in practice. The framework adopted in this work\ncan be easily extended to other problems arising from MR or other imaging\nmodalities with non-linearly coupled variables.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 20:08:43 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Huang", "Shuai", ""], ["Lah", "James J.", ""], ["Allen", "Jason W.", ""], ["Qiu", "Deqiang", ""]]}, {"id": "2008.01808", "submitter": "Nicolas Gonthier", "authors": "Nicolas Gonthier and Yann Gousseau and Sa\\\"id Ladjal", "title": "High resolution neural texture synthesis with long range constraints", "comments": "25 pages, 18 figures. LOW RESOLUTION PDF: Images may show compression\n  artifacts", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of texture synthesis has witnessed important progresses over the\nlast years, most notably through the use of Convolutional Neural Networks.\nHowever, neural synthesis methods still struggle to reproduce large scale\nstructures, especially with high resolution textures. To address this issue, we\nfirst introduce a simple multi-resolution framework that efficiently accounts\nfor long-range dependency. Then, we show that additional statistical\nconstraints further improve the reproduction of textures with strong\nregularity. This can be achieved by constraining both the Gram matrices of a\nneural network and the power spectrum of the image. Alternatively one may\nconstrain only the autocorrelation of the features of the network and drop the\nGram matrices constraints. In an experimental part, the proposed methods are\nthen extensively tested and compared to alternative approaches, both in an\nunsupervised way and through a user study. Experiments show the interest of the\nmulti-scale scheme for high resolution textures and the interest of combining\nit with additional constraints for regular textures.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 20:11:27 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Gonthier", "Nicolas", ""], ["Gousseau", "Yann", ""], ["Ladjal", "Sa\u00efd", ""]]}, {"id": "2008.01815", "submitter": "Kai-En Lin", "authors": "Kai-En Lin, Zexiang Xu, Ben Mildenhall, Pratul P. Srinivasan, Yannick\n  Hold-Geoffroy, Stephen DiVerdi, Qi Sun, Kalyan Sunkavalli, and Ravi\n  Ramamoorthi", "title": "Deep Multi Depth Panoramas for View Synthesis", "comments": "Published at the European Conference on Computer Vision, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a learning-based approach for novel view synthesis for\nmulti-camera 360$^{\\circ}$ panorama capture rigs. Previous work constructs RGBD\npanoramas from such data, allowing for view synthesis with small amounts of\ntranslation, but cannot handle the disocclusions and view-dependent effects\nthat are caused by large translations. To address this issue, we present a\nnovel scene representation - Multi Depth Panorama (MDP) - that consists of\nmultiple RGBD$\\alpha$ panoramas that represent both scene geometry and\nappearance. We demonstrate a deep neural network-based method to reconstruct\nMDPs from multi-camera 360$^{\\circ}$ images. MDPs are more compact than\nprevious 3D scene representations and enable high-quality, efficient new view\nrendering. We demonstrate this via experiments on both synthetic and real data\nand comparisons with previous state-of-the-art methods spanning both\nlearning-based approaches and classical RGBD-based methods.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 20:29:15 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Lin", "Kai-En", ""], ["Xu", "Zexiang", ""], ["Mildenhall", "Ben", ""], ["Srinivasan", "Pratul P.", ""], ["Hold-Geoffroy", "Yannick", ""], ["DiVerdi", "Stephen", ""], ["Sun", "Qi", ""], ["Sunkavalli", "Kalyan", ""], ["Ramamoorthi", "Ravi", ""]]}, {"id": "2008.01818", "submitter": "Xiuyuan Cheng", "authors": "Xiuyuan Cheng, Zichen Miao, Qiang Qiu", "title": "Graph Convolution with Low-rank Learnable Local Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric variations like rotation, scaling, and viewpoint changes pose a\nsignificant challenge to visual understanding. One common solution is to\ndirectly model certain intrinsic structures, e.g., using landmarks. However, it\nthen becomes non-trivial to build effective deep models, especially when the\nunderlying non-Euclidean grid is irregular and coarse. Recent deep models using\ngraph convolutions provide an appropriate framework to handle such\nnon-Euclidean data, but many of them, particularly those based on global graph\nLaplacians, lack expressiveness to capture local features required for\nrepresentation of signals lying on the non-Euclidean grid. The current paper\nintroduces a new type of graph convolution with learnable low-rank local\nfilters, which is provably more expressive than previous spectral graph\nconvolution methods. The model also provides a unified framework for both\nspectral and spatial graph convolutions. To improve model robustness,\nregularization by local graph Laplacians is introduced. The representation\nstability against input graph data perturbation is theoretically proved, making\nuse of the graph filter locality and the local graph regularization.\nExperiments on spherical mesh data, real-world facial expression\nrecognition/skeleton-based action recognition data, and data with simulated\ngraph noise show the empirical advantage of the proposed model.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 20:34:59 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 17:07:57 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Cheng", "Xiuyuan", ""], ["Miao", "Zichen", ""], ["Qiu", "Qiang", ""]]}, {"id": "2008.01846", "submitter": "Ge Wang Dr.", "authors": "Weiwen Wu, Dianlin Hu, Wenxiang Cong, Hongming Shan, Shaoyu Wang,\n  Chuang Niu, Pingkun Yan, Hengyong Yu, Varut Vardhanabhuti, Ge Wang", "title": "Stabilizing Deep Tomographic Reconstruction Networks", "comments": "68 pages, 26 figures, 140 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tomographic image reconstruction with deep learning (DL) is an emerging field\nof applied artificial intelligence, but a recent landmark study reveals that\nseveral deep reconstruction networks are unstable for computed tomography (CT)\nand magnetic resonance imaging (MRI). Since deep reconstruction is now a\nmainstream approach to achieve better tomographic image quality, stabilizing\ndeep networks is an urgent challenge. Here we propose an Analytic Compressive\nIterative Deep (ACID) framework to address this challenge. Instead of only\nusing DL or compressed sensing, ACID consists of four modules: deep learning,\ncompressed sensing-inspired sparsity promotion, analytic mapping, and iterative\nrefinement. This paper shows the convergence and stability of ACID under a\nbounded error norm condition (a special case of the Lipschitz continuity),\nimproves deep reconstruction quality by stabilizing an unstable deep\nreconstruction network in the ACID framework, and demonstrates the power of\nACID in both stabilizing an unstable network and being resilient against\nadversarial attacks to the whole ACID workflow. In our experiments, ACID\neliminated all three kinds of instabilities and significantly improved image\nquality in the context of the aforementioned study on the instabilities,\ndemonstrating that data-driven reconstruction can be stabilized to outperform\nreconstruction using sparsity-regularized reconstruction alone. The mechanism\nof ACID is to synergize a deep reconstruction network trained on big data,\ncompressed sensing-based improvement with kernel awareness, and iterative\nrefinement to eliminate any data residual inconsistent with real data. We\nanticipate that this integrative closed-loop data-driven approach helps advance\ndeep tomographic image reconstruction methods into clinical applications.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 21:35:32 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 12:59:48 GMT"}, {"version": "v3", "created": "Sat, 23 Jan 2021 17:08:44 GMT"}, {"version": "v4", "created": "Thu, 4 Mar 2021 04:24:47 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Wu", "Weiwen", ""], ["Hu", "Dianlin", ""], ["Cong", "Wenxiang", ""], ["Shan", "Hongming", ""], ["Wang", "Shaoyu", ""], ["Niu", "Chuang", ""], ["Yan", "Pingkun", ""], ["Yu", "Hengyong", ""], ["Vardhanabhuti", "Varut", ""], ["Wang", "Ge", ""]]}, {"id": "2008.01860", "submitter": "S. Alireza Golestaneh", "authors": "S. Alireza Golestaneh, Kris M. Kitani", "title": "Importance of Self-Consistency in Active Learning for Semantic\n  Segmentation", "comments": "Accepted in The British Machine Vision Conference (BMVC) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the task of active learning in the context of semantic\nsegmentation and show that self-consistency can be a powerful source of\nself-supervision to greatly improve the performance of a data-driven model with\naccess to only a small amount of labeled data. Self-consistency uses the simple\nobservation that the results of semantic segmentation for a specific image\nshould not change under transformations like horizontal flipping (i.e., the\nresults should only be flipped). In other words, the output of a model should\nbe consistent under equivariant transformations. The self-supervisory signal of\nself-consistency is particularly helpful during active learning since the model\nis prone to overfitting when there is only a small amount of labeled training\ndata. In our proposed active learning framework, we iteratively extract small\nimage patches that need to be labeled, by selecting image patches that have\nhigh uncertainty (high entropy) under equivariant transformations. We enforce\npixel-wise self-consistency between the outputs of segmentation network for\neach image and its transformation (horizontally flipped) to utilize the rich\nself-supervisory information and reduce the uncertainty of the network. In this\nway, we are able to find the image patches over which the current model\nstruggles the most to classify. By iteratively training over these difficult\nimage patches, our experiments show that our active learning approach reaches\n$\\sim96\\%$ of the top performance of a model trained on all data, by using only\n$12\\%$ of the total data on benchmark semantic segmentation datasets (e.g.,\nCamVid and Cityscapes).\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 22:18:35 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Golestaneh", "S. Alireza", ""], ["Kitani", "Kris M.", ""]]}, {"id": "2008.01864", "submitter": "Davide Moroni", "authors": "Mario D'Acunto, Massimo Martinelli, Davide Moroni", "title": "From Human Mesenchymal Stromal Cells to Osteosarcoma Cells\n  Classification by Deep Learning", "comments": "Submitted authors' version", "journal-ref": "Journal of Intelligent & Fuzzy Systems, vol. 37, no. 6, pp.\n  7199-7206, 2019", "doi": "10.3233/JIFS-179332", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early diagnosis of cancer often allows for a more vast choice of therapy\nopportunities. After a cancer diagnosis, staging provides essential information\nabout the extent of disease in the body and the expected response to a\nparticular treatment. The leading importance of classifying cancer patients at\nthe early stage into high or low-risk groups has led many research teams, both\nfrom the biomedical and bioinformatics field, to study the application of Deep\nLearning (DL) methods. The ability of DL to detect critical features from\ncomplex datasets is a significant achievement in early diagnosis and cell\ncancer progression. In this paper, we focus the attention on osteosarcoma.\nOsteosarcoma is one of the primary malignant bone tumors which usually afflicts\npeople in adolescence. Our contribution to the classification of osteosarcoma\ncells is made as follows: a DL approach is applied to discriminate human\nMesenchymal Stromal Cells (MSCs) from osteosarcoma cells and to classify the\ndifferent cell populations under investigation. Glass slides of differ-ent cell\npopulations were cultured including MSCs, differentiated in healthy bone cells\n(osteoblasts) and osteosarcoma cells, both single cell populations or mixed.\nImages of such samples of isolated cells (single-type of mixed) are recorded\nwith traditional optical microscopy. DL is then applied to identify and\nclassify single cells. Proper data augmentation techniques and cross-fold\nvalidation are used to appreciate the capabilities of a convolutional neural\nnetwork to address the cell detection and classification problem. Based on the\nresults obtained on individual cells, and to the versatility and scalability of\nour DL approach, the next step will be its application to discriminate and\nclassify healthy or cancer tissues to advance digital pathology.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 22:23:58 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["D'Acunto", "Mario", ""], ["Martinelli", "Massimo", ""], ["Moroni", "Davide", ""]]}, {"id": "2008.01874", "submitter": "Mohit Prabhushankar", "authors": "Yutong Sun, Mohit Prabhushankar and Ghassan AlRegib", "title": "Implicit Saliency in Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that existing recognition and localization deep\narchitectures, that have not been exposed to eye tracking data or any saliency\ndatasets, are capable of predicting the human visual saliency. We term this as\nimplicit saliency in deep neural networks. We calculate this implicit saliency\nusing expectancy-mismatch hypothesis in an unsupervised fashion. Our\nexperiments show that extracting saliency in this fashion provides comparable\nperformance when measured against the state-of-art supervised algorithms.\nAdditionally, the robustness outperforms those algorithms when we add large\nnoise to the input images. Also, we show that semantic features contribute more\nthan low-level features for human visual saliency detection.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 23:14:24 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Sun", "Yutong", ""], ["Prabhushankar", "Mohit", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "2008.01882", "submitter": "Giovanni Pasqualino", "authors": "Giovanni Pasqualino and Antonino Furnari and Giovanni Signorello and\n  Giovanni Maria Farinella", "title": "An Unsupervised Domain Adaptation Scheme for Single-Stage Artwork\n  Recognition in Cultural Sites", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing artworks in a cultural site using images acquired from the user's\npoint of view (First Person Vision) allows to build interesting applications\nfor both the visitors and the site managers. However, current object detection\nalgorithms working in fully supervised settings need to be trained with large\nquantities of labeled data, whose collection requires a lot of times and high\ncosts in order to achieve good performance. Using synthetic data generated from\nthe 3D model of the cultural site to train the algorithms can reduce these\ncosts. On the other hand, when these models are tested with real images, a\nsignificant drop in performance is observed due to the differences between real\nand synthetic images. In this study we consider the problem of Unsupervised\nDomain Adaptation for object detection in cultural sites. To address this\nproblem, we created a new dataset containing both synthetic and real images of\n16 different artworks. We hence investigated different domain adaptation\ntechniques based on one-stage and two-stage object detector, image-to-image\ntranslation and feature alignment. Based on the observation that single-stage\ndetectors are more robust to the domain shift in the considered settings, we\nproposed a new method which builds on RetinaNet and feature alignment that we\ncalled DA-RetinaNet. The proposed approach achieves better results than\ncompared methods on the proposed dataset and on Cityscapes. To support research\nin this field we release the dataset at the following link\nhttps://iplab.dmi.unict.it/EGO-CH-OBJ-UDA/ and the code of the proposed\narchitecture at https://github.com/fpv-iplab/DA-RetinaNet.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 23:51:06 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 12:54:38 GMT"}, {"version": "v3", "created": "Mon, 21 Dec 2020 20:37:19 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Pasqualino", "Giovanni", ""], ["Furnari", "Antonino", ""], ["Signorello", "Giovanni", ""], ["Farinella", "Giovanni Maria", ""]]}, {"id": "2008.01885", "submitter": "Zachary Baum", "authors": "Zachary M. C. Baum, Yipeng Hu, Dean C. Barratt", "title": "Multimodality Biomedical Image Registration using Free Point Transformer\n  Networks", "comments": "10 pages, 4 figures. Accepted for publication at International\n  Conference on Medical Image Computing and Computer Assisted Intervention\n  (MICCAI) workshop on Advances in Simplifying Medical UltraSound (ASMUS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a point-set registration algorithm based on a novel free point\ntransformer (FPT) network, designed for points extracted from multimodal\nbiomedical images for registration tasks, such as those frequently encountered\nin ultrasound-guided interventional procedures. FPT is constructed with a\nglobal feature extractor which accepts unordered source and target point-sets\nof variable size. The extracted features are conditioned by a shared multilayer\nperceptron point transformer module to predict a displacement vector for each\nsource point, transforming it into the target space. The point transformer\nmodule assumes no vicinity or smoothness in predicting spatial transformation\nand, together with the global feature extractor, is trained in a data-driven\nfashion with an unsupervised loss function. In a multimodal registration task\nusing prostate MR and sparsely acquired ultrasound images, FPT yields\ncomparable or improved results over other rigid and non-rigid registration\nmethods. This demonstrates the versatility of FPT to learn registration\ndirectly from real, clinical training data and to generalize to a challenging\ntask, such as the interventional application presented.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 00:13:04 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Baum", "Zachary M. C.", ""], ["Hu", "Yipeng", ""], ["Barratt", "Dean C.", ""]]}, {"id": "2008.01896", "submitter": "Shanshan Wang", "authors": "Weijian Huang, Hao Yang, Xinfeng Liu, Cheng Li, Ian Zhang, Rongpin\n  Wang, Hairong Zheng, Shanshan Wang", "title": "A coarse-to-fine framework for unsupervised multi-contrast MR image\n  deformable registration with dual consistency constraint", "comments": null, "journal-ref": "IEEE Transactions on Medical Imaging (2021)", "doi": "10.1109/TMI.2021.3059282", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-contrast magnetic resonance (MR) image registration is useful in the\nclinic to achieve fast and accurate imaging-based disease diagnosis and\ntreatment planning. Nevertheless, the efficiency and performance of the\nexisting registration algorithms can still be improved. In this paper, we\npropose a novel unsupervised learning-based framework to achieve accurate and\nefficient multi-contrast MR image registrations. Specifically, an end-to-end\ncoarse-to-fine network architecture consisting of affine and deformable\ntransformations is designed to improve the robustness and achieve end-to-end\nregistration. Furthermore, a dual consistency constraint and a new prior\nknowledge-based loss function are developed to enhance the registration\nperformances. The proposed method has been evaluated on a clinical dataset\ncontaining 555 cases, and encouraging performances have been achieved. Compared\nto the commonly utilized registration methods, including VoxelMorph, SyN, and\nLT-Net, the proposed method achieves better registration performance with a\nDice score of 0.8397 in identifying stroke lesions. With regards to the\nregistration speed, our method is about 10 times faster than the most\ncompetitive method of SyN (Affine) when testing on a CPU. Moreover, we prove\nthat our method can still perform well on more challenging tasks with lacking\nscanning information data, showing high robustness for the clinical\napplication.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 01:16:45 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 12:15:40 GMT"}, {"version": "v3", "created": "Tue, 16 Feb 2021 06:07:20 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Huang", "Weijian", ""], ["Yang", "Hao", ""], ["Liu", "Xinfeng", ""], ["Li", "Cheng", ""], ["Zhang", "Ian", ""], ["Wang", "Rongpin", ""], ["Zheng", "Hairong", ""], ["Wang", "Shanshan", ""]]}, {"id": "2008.01897", "submitter": "Hong-Gyu Jung", "authors": "Sin-Han Kang, Hong-Gyu Jung, Dong-Ok Won, Seong-Whan Lee", "title": "Counterfactual Explanation Based on Gradual Construction for Deep\n  Networks", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand the black-box characteristics of deep networks, counterfactual\nexplanation that deduces not only the important features of an input space but\nalso how those features should be modified to classify input as a target class\nhas gained an increasing interest. The patterns that deep networks have learned\nfrom a training dataset can be grasped by observing the feature variation among\nvarious classes. However, current approaches perform the feature modification\nto increase the classification probability for the target class irrespective of\nthe internal characteristics of deep networks. This often leads to unclear\nexplanations that deviate from real-world data distributions. To address this\nproblem, we propose a counterfactual explanation method that exploits the\nstatistics learned from a training dataset. Especially, we gradually construct\nan explanation by iterating over masking and composition steps. The masking\nstep aims to select an important feature from the input data to be classified\nas a target class. Meanwhile, the composition step aims to optimize the\npreviously selected feature by ensuring that its output score is close to the\nlogit space of the training data that are classified as the target class.\nExperimental results show that our method produces human-friendly\ninterpretations on various classification datasets and verify that such\ninterpretations can be achieved with fewer feature modification.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 01:18:31 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Kang", "Sin-Han", ""], ["Jung", "Hong-Gyu", ""], ["Won", "Dong-Ok", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2008.01910", "submitter": "Li Sun", "authors": "Li Sun, Junxiang Chen, Yanwu Xu, Mingming Gong, Ke Yu, Kayhan\n  Batmanghelich", "title": "Hierarchical Amortized Training for Memory-efficient High Resolution 3D\n  GAN", "comments": "12 pages, 9 figures. Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GAN) have many potential medical imaging\napplications, including data augmentation, domain adaptation, and model\nexplanation. Due to the limited embedded memory of Graphical Processing Units\n(GPUs), most current 3D GAN models are trained on low-resolution medical\nimages. In this work, we propose a novel end-to-end GAN architecture that can\ngenerate high-resolution 3D images. We achieve this goal by separating training\nand inference. During training, we adopt a hierarchical structure that\nsimultaneously generates a low-resolution version of the image and a randomly\nselected sub-volume of the high-resolution image. The hierarchical design has\ntwo advantages: First, the memory demand for training on high-resolution images\nis amortized among subvolumes. Furthermore, anchoring the high-resolution\nsubvolumes to a single low-resolution image ensures anatomical consistency\nbetween subvolumes. During inference, our model can directly generate full\nhigh-resolution images. We also incorporate an encoder with a similar\nhierarchical structure into the model to extract features from the images.\nExperiments on 3D thorax CT and brain MRI demonstrate that our approach\noutperforms state of the art in image generation and clinical-relevant feature\nextraction.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 02:33:04 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 16:40:37 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Sun", "Li", ""], ["Chen", "Junxiang", ""], ["Xu", "Yanwu", ""], ["Gong", "Mingming", ""], ["Yu", "Ke", ""], ["Batmanghelich", "Kayhan", ""]]}, {"id": "2008.01918", "submitter": "Jiahao Pang", "authors": "Wei Hu, Jiahao Pang, Xianming Liu, Dong Tian, Chia-Wen Lin, Anthony\n  Vetro", "title": "Graph Signal Processing for Geometric Data and Beyond: Theory and\n  Applications", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric data acquired from real-world scenes, e.g, 2D depth images, 3D\npoint clouds, and 4D dynamic point clouds, have found a wide range of\napplications including immersive telepresence, autonomous driving,\nsurveillance, etc. Due to irregular sampling patterns of most geometric data,\ntraditional image/video processing methodologies are limited, while Graph\nSignal Processing (GSP) -- a fast-developing field in the signal processing\ncommunity -- enables processing signals that reside on irregular domains and\nplays a critical role in numerous applications of geometric data from low-level\nprocessing to high-level analysis. To further advance the research in this\nfield, we provide the first timely and comprehensive overview of GSP\nmethodologies for geometric data in a unified manner by bridging the\nconnections between geometric data and graphs, among the various geometric data\nmodalities, and with spectral/nodal graph filtering techniques. We also discuss\nthe recently developed Graph Neural Networks (GNNs) and interpret the operation\nof these networks from the perspective of GSP. We conclude with a brief\ndiscussion of open problems and challenges.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 03:20:16 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 20:59:03 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Hu", "Wei", ""], ["Pang", "Jiahao", ""], ["Liu", "Xianming", ""], ["Tian", "Dong", ""], ["Lin", "Chia-Wen", ""], ["Vetro", "Anthony", ""]]}, {"id": "2008.01928", "submitter": "Pengxu Wei", "authors": "Pengxu Wei, Ziwei Xie, Hannan Lu, Zongyuan Zhan, Qixiang Ye, Wangmeng\n  Zuo, Liang Lin", "title": "Component Divide-and-Conquer for Real-World Image Super-Resolution", "comments": null, "journal-ref": "European Conference on Computer Vision (ECCV), 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a large-scale Diverse Real-world image\nSuper-Resolution dataset, i.e., DRealSR, as well as a divide-and-conquer\nSuper-Resolution (SR) network, exploring the utility of guiding SR model with\nlow-level image components. DRealSR establishes a new SR benchmark with diverse\nreal-world degradation processes, mitigating the limitations of conventional\nsimulated image degradation. In general, the targets of SR vary with image\nregions with different low-level image components, e.g., smoothness preserving\nfor flat regions, sharpening for edges, and detail enhancing for textures.\nLearning an SR model with conventional pixel-wise loss usually is easily\ndominated by flat regions and edges, and fails to infer realistic details of\ncomplex textures. We propose a Component Divide-and-Conquer (CDC) model and a\nGradient-Weighted (GW) loss for SR. Our CDC parses an image with three\ncomponents, employs three Component-Attentive Blocks (CABs) to learn attentive\nmasks and intermediate SR predictions with an intermediate supervision learning\nstrategy, and trains an SR model following a divide-and-conquer learning\nprinciple. Our GW loss also provides a feasible way to balance the difficulties\nof image components for SR. Extensive experiments validate the superior\nperformance of our CDC and the challenging aspects of our DRealSR dataset\nrelated to diverse real-world scenarios. Our dataset and codes are publicly\navailable at\nhttps://github.com/xiezw5/Component-Divide-and-Conquer-for-Real-World-Image-Super-Resolution\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 04:26:26 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Wei", "Pengxu", ""], ["Xie", "Ziwei", ""], ["Lu", "Hannan", ""], ["Zhan", "Zongyuan", ""], ["Ye", "Qixiang", ""], ["Zuo", "Wangmeng", ""], ["Lin", "Liang", ""]]}, {"id": "2008.01936", "submitter": "Kangxue Yin", "authors": "Kangxue Yin, Zhiqin Chen, Siddhartha Chaudhuri, Matthew Fisher,\n  Vladimir G. Kim, Hao Zhang", "title": "COALESCE: Component Assembly by Learning to Synthesize Connections", "comments": "20 pages: paper + supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce COALESCE, the first data-driven framework for component-based\nshape assembly which employs deep learning to synthesize part connections. To\nhandle geometric and topological mismatches between parts, we remove the\nmismatched portions via erosion, and rely on a joint synthesis step, which is\nlearned from data, to fill the gap and arrive at a natural and plausible part\njoint. Given a set of input parts extracted from different objects, COALESCE\nautomatically aligns them and synthesizes plausible joints to connect the parts\ninto a coherent 3D object represented by a mesh. The joint synthesis network,\ndesigned to focus on joint regions, reconstructs the surface between the parts\nby predicting an implicit shape representation that agrees with existing parts,\nwhile generating a smooth and topologically meaningful connection. We employ\ntest-time optimization to further ensure that the synthesized joint region\nclosely aligns with the input parts to create realistic component assemblies\nfrom diverse input parts. We demonstrate that our method significantly\noutperforms prior approaches including baseline deep models for 3D shape\nsynthesis, as well as state-of-the-art methods for shape completion.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 05:12:06 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2020 08:11:55 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Yin", "Kangxue", ""], ["Chen", "Zhiqin", ""], ["Chaudhuri", "Siddhartha", ""], ["Fisher", "Matthew", ""], ["Kim", "Vladimir G.", ""], ["Zhang", "Hao", ""]]}, {"id": "2008.01942", "submitter": "Ke Wang", "authors": "Ke Wang, Siyuan Zhang, Junlan Chen, Fan Ren, Lei Xiao", "title": "A feature-supervised generative adversarial network for environmental\n  monitoring during hazy days", "comments": null, "journal-ref": "Science of the Total Environment (2020),748, 141445", "doi": "10.1016/j.scitotenv.2020.141445", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adverse haze weather condition has brought considerable difficulties in\nvision-based environmental applications. While, until now, most of the existing\nenvironmental monitoring studies are under ordinary conditions, and the studies\nof complex haze weather conditions have been ignored. Thence, this paper\nproposes a feature-supervised learning network based on generative adversarial\nnetworks (GAN) for environmental monitoring during hazy days. Its main idea is\nto train the model under the supervision of feature maps from the ground truth.\nFour key technical contributions are made in the paper. First, pairs of hazy\nand clean images are used as inputs to supervise the encoding process and\nobtain high-quality feature maps. Second, the basic GAN formulation is modified\nby introducing perception loss, style loss, and feature regularization loss to\ngenerate better results. Third, multi-scale images are applied as the input to\nenhance the performance of discriminator. Finally, a hazy remote sensing\ndataset is created for testing our dehazing method and environmental detection.\nExtensive experimental results show that the proposed method has achieved\nbetter performance than current state-of-the-art methods on both synthetic\ndatasets and real-world remote sensing images.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 05:27:15 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Wang", "Ke", ""], ["Zhang", "Siyuan", ""], ["Chen", "Junlan", ""], ["Ren", "Fan", ""], ["Xiao", "Lei", ""]]}, {"id": "2008.01973", "submitter": "Ahmad El Sallab Dr", "authors": "Abdullah Tarek Farag, Ahmed Raafat Abd El-Wahab, Mahmoud Nada, Mohamed\n  Yasser Abd El-Hakeem, Omar Sayed Mahmoud, Reem Khaled Rashwan and Ahmad El\n  Sallab", "title": "MultiCheXNet: A Multi-Task Learning Deep Network For Pneumonia-like\n  Diseases Diagnosis From X-ray Scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MultiCheXNet, an end-to-end Multi-task learning model, that is\nable to take advantage of different X-rays data sets of Pneumonia-like diseases\nin one neural architecture, performing three tasks at the same time; diagnosis,\nsegmentation and localization. The common encoder in our architecture can\ncapture useful common features present in the different tasks. The common\nencoder has another advantage of efficient computations, which speeds up the\ninference time compared to separate models. The specialized decoders heads can\nthen capture the task-specific features. We employ teacher forcing to address\nthe issue of negative samples that hurt the segmentation and localization\nperformance. Finally,we employ transfer learning to fine tune the classifier on\nunseen pneumonia-like diseases. The MTL architecture can be trained on joint or\ndis-joint labeled data sets. The training of the architecture follows a\ncarefully designed protocol, that pre trains different sub-models on\nspecialized datasets, before being integrated in the joint MTL model. Our\nexperimental setup involves variety of data sets, where the baseline\nperformance of the 3 tasks is compared to the MTL architecture performance.\nMoreover, we evaluate the transfer learning mode to COVID-19 data set,both from\nindividual classifier model, and from MTL architecture classification head.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 07:45:24 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Farag", "Abdullah Tarek", ""], ["El-Wahab", "Ahmed Raafat Abd", ""], ["Nada", "Mahmoud", ""], ["El-Hakeem", "Mohamed Yasser Abd", ""], ["Mahmoud", "Omar Sayed", ""], ["Rashwan", "Reem Khaled", ""], ["Sallab", "Ahmad El", ""]]}, {"id": "2008.01980", "submitter": "Hendrik Heuer", "authors": "Hendrik Heuer, Andreas Breiter", "title": "More Than Accuracy: Towards Trustworthy Machine Learning Interfaces for\n  Object Recognition", "comments": "UMAP '20: Proceedings of the 28th ACM Conference on User Modeling,\n  Adaptation and Personalization", "journal-ref": "UMAP 2020: Proceedings of the 28th ACM Conference on User\n  Modeling, Adaptation and Personalization", "doi": "10.1145/3340631.3394873", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the user experience of visualizations of a machine\nlearning (ML) system that recognizes objects in images. This is important since\neven good systems can fail in unexpected ways as misclassifications on\nphoto-sharing websites showed. In our study, we exposed users with a background\nin ML to three visualizations of three systems with different levels of\naccuracy. In interviews, we explored how the visualization helped users assess\nthe accuracy of systems in use and how the visualization and the accuracy of\nthe system affected trust and reliance. We found that participants do not only\nfocus on accuracy when assessing ML systems. They also take the perceived\nplausibility and severity of misclassification into account and prefer seeing\nthe probability of predictions. Semantically plausible errors are judged as\nless severe than errors that are implausible, which means that system accuracy\ncould be communicated through the types of errors.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 07:56:37 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Heuer", "Hendrik", ""], ["Breiter", "Andreas", ""]]}, {"id": "2008.01993", "submitter": "Saheb Chhabra", "authors": "Puspita Majumdar, Saheb Chhabra, Richa Singh, Mayank Vatsa", "title": "Subclass Contrastive Loss for Injured Face Recognition", "comments": "Accepted in BTAS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deaths and injuries are common in road accidents, violence, and natural\ndisaster. In such cases, one of the main tasks of responders is to retrieve the\nidentity of the victims to reunite families and ensure proper identification of\ndeceased/ injured individuals. Apart from this, identification of unidentified\ndead bodies due to violence and accidents is crucial for the police\ninvestigation. In the absence of identification cards, current practices for\nthis task include DNA profiling and dental profiling. Face is one of the most\ncommonly used and widely accepted biometric modalities for recognition.\nHowever, face recognition is challenging in the presence of facial injuries\nsuch as swelling, bruises, blood clots, laceration, and avulsion which affect\nthe features used in recognition. In this paper, for the first time, we address\nthe problem of injured face recognition and propose a novel Subclass\nContrastive Loss (SCL) for this task. A novel database, termed as Injured Face\n(IF) database, is also created to instigate research in this direction.\nExperimental analysis shows that the proposed loss function surpasses existing\nalgorithm for injured face recognition.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 08:30:29 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Majumdar", "Puspita", ""], ["Chhabra", "Saheb", ""], ["Singh", "Richa", ""], ["Vatsa", "Mayank", ""]]}, {"id": "2008.01995", "submitter": "Vladimir Puzyrev", "authors": "Vladimir Puzyrev and Chris Elders", "title": "Unsupervised seismic facies classification using deep convolutional\n  autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increased size and complexity of seismic surveys, manual labeling of\nseismic facies has become a significant challenge. Application of automatic\nmethods for seismic facies interpretation could significantly reduce the manual\nlabor and subjectivity of a particular interpreter present in conventional\nmethods. A recently emerged group of methods is based on deep neural networks.\nThese approaches are data-driven and require large labeled datasets for network\ntraining. We apply a deep convolutional autoencoder for unsupervised seismic\nfacies classification, which does not require manually labeled examples. The\nfacies maps are generated by clustering the deep-feature vectors obtained from\nthe input data. Our method yields accurate results on real data and provides\nthem instantaneously. The proposed approach opens up possibilities to analyze\ngeological patterns in real time without human intervention.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 08:33:09 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Puzyrev", "Vladimir", ""], ["Elders", "Chris", ""]]}, {"id": "2008.01999", "submitter": "Yan Hong", "authors": "Yan Hong, Li Niu, Jianfu Zhang, Weijie Zhao, Chen Fu, Liqing Zhang", "title": "F2GAN: Fusing-and-Filling GAN for Few-shot Image Generation", "comments": "This paper is accepted by ACM Multimedia 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to generate images for a given category, existing deep generative\nmodels generally rely on abundant training images. However, extensive data\nacquisition is expensive and fast learning ability from limited data is\nnecessarily required in real-world applications. Also, these existing methods\nare not well-suited for fast adaptation to a new category.\n  Few-shot image generation, aiming to generate images from only a few images\nfor a new category, has attracted some research interest. In this paper, we\npropose a Fusing-and-Filling Generative Adversarial Network (F2GAN) to generate\nrealistic and diverse images for a new category with only a few images. In our\nF2GAN, a fusion generator is designed to fuse the high-level features of\nconditional images with random interpolation coefficients, and then fills in\nattended low-level details with non-local attention module to produce a new\nimage. Moreover, our discriminator can ensure the diversity of generated images\nby a mode seeking loss and an interpolation regression loss. Extensive\nexperiments on five datasets demonstrate the effectiveness of our proposed\nmethod for few-shot image generation.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 08:47:42 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 04:36:47 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Hong", "Yan", ""], ["Niu", "Li", ""], ["Zhang", "Jianfu", ""], ["Zhao", "Weijie", ""], ["Fu", "Chen", ""], ["Zhang", "Liqing", ""]]}, {"id": "2008.02001", "submitter": "Nils Gessert", "authors": "Nils Gessert, Julia Kr\\\"uger, Roland Opfer, Ann-Christin Ostwaldt,\n  Praveena Manogaran, Hagen H. Kitzler, Sven Schippling, Alexander Schlaefer", "title": "Multiple Sclerosis Lesion Activity Segmentation with Attention-Guided\n  Two-Path CNNs", "comments": "Accepted for publication in Computerized Medical Imaging and Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple sclerosis is an inflammatory autoimmune demyelinating disease that\nis characterized by lesions in the central nervous system. Typically, magnetic\nresonance imaging (MRI) is used for tracking disease progression. Automatic\nimage processing methods can be used to segment lesions and derive quantitative\nlesion parameters. So far, methods have focused on lesion segmentation for\nindividual MRI scans. However, for monitoring disease progression,\n\\textit{lesion activity} in terms of new and enlarging lesions between two time\npoints is a crucial biomarker. For this problem, several classic methods have\nbeen proposed, e.g., using difference volumes. Despite their success for\nsingle-volume lesion segmentation, deep learning approaches are still rare for\nlesion activity segmentation. In this work, convolutional neural networks\n(CNNs) are studied for lesion activity segmentation from two time points. For\nthis task, CNNs are designed and evaluated that combine the information from\ntwo points in different ways. In particular, two-path architectures with\nattention-guided interactions are proposed that enable effective information\nexchange between the two time point's processing paths. It is demonstrated that\ndeep learning-based methods outperform classic approaches and it is shown that\nattention-guided interactions significantly improve performance. Furthermore,\nthe attention modules produce plausible attention maps that have a masking\neffect that suppresses old, irrelevant lesions. A lesion-wise false positive\nrate of 26.4% is achieved at a true positive rate of 74.2%, which is not\nsignificantly different from the interrater performance.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 08:49:20 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Gessert", "Nils", ""], ["Kr\u00fcger", "Julia", ""], ["Opfer", "Roland", ""], ["Ostwaldt", "Ann-Christin", ""], ["Manogaran", "Praveena", ""], ["Kitzler", "Hagen H.", ""], ["Schippling", "Sven", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "2008.02002", "submitter": "Jianqiu Lu", "authors": "Xiaozheng Jian, Jianqiu Lu, Zexi Yuan, Ao Li", "title": "Fast top-K Cosine Similarity Search through XOR-Friendly Binary\n  Quantization on GPUs", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the use of GPU for accelerating large scale nearest neighbor\nsearch and we propose a fast vector-quantization-based exhaustive nearest\nneighbor search algorithm that can achieve high accuracy without any indexing\nconstruction specifically designed for cosine similarity. This algorithm uses a\nnovel XOR-friendly binary quantization method to encode floating-point numbers\nsuch that high-complexity multiplications can be optimized as low-complexity\nbitwise operations. Experiments show that, our quantization method takes short\npreprocessing time, and helps make the search speed of our exhaustive search\nmethod much more faster than that of popular approximate nearest neighbor\nalgorithms when high accuracy is needed.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 08:50:21 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Jian", "Xiaozheng", ""], ["Lu", "Jianqiu", ""], ["Yuan", "Zexi", ""], ["Li", "Ao", ""]]}, {"id": "2008.02004", "submitter": "Johanna Wald", "authors": "Johanna Wald, Torsten Sattler, Stuart Golodetz, Tommaso Cavallari,\n  Federico Tombari", "title": "Beyond Controlled Environments: 3D Camera Re-Localization in Changing\n  Indoor Scenes", "comments": "ECCV 2020, project website https://waldjohannau.github.io/RIO10", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-term camera re-localization is an important task with numerous computer\nvision and robotics applications. Whilst various outdoor benchmarks exist that\ntarget lighting, weather and seasonal changes, far less attention has been paid\nto appearance changes that occur indoors. This has led to a mismatch between\npopular indoor benchmarks, which focus on static scenes, and indoor\nenvironments that are of interest for many real-world applications. In this\npaper, we adapt 3RScan - a recently introduced indoor RGB-D dataset designed\nfor object instance re-localization - to create RIO10, a new long-term camera\nre-localization benchmark focused on indoor scenes. We propose new metrics for\nevaluating camera re-localization and explore how state-of-the-art camera\nre-localizers perform according to these metrics. We also examine in detail how\ndifferent types of scene change affect the performance of different methods,\nbased on novel ways of detecting such changes in a given RGB-D frame. Our\nresults clearly show that long-term indoor re-localization is an unsolved\nproblem. Our benchmark and tools are publicly available at\nwaldjohannau.github.io/RIO10\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 09:02:12 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Wald", "Johanna", ""], ["Sattler", "Torsten", ""], ["Golodetz", "Stuart", ""], ["Cavallari", "Tommaso", ""], ["Tombari", "Federico", ""]]}, {"id": "2008.02030", "submitter": "Sebastian Guendel", "authors": "Sebastian Guendel, Arnaud Arindra Adiyoso Setio, Sasa Grbic, Andreas\n  Maier, Dorin Comaniciu", "title": "Extracting and Leveraging Nodule Features with Lung Inpainting for Local\n  Feature Augmentation", "comments": "Accepted at MICCAI MLMI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest X-ray (CXR) is the most common examination for fast detection of\npulmonary abnormalities. Recently, automated algorithms have been developed to\nclassify multiple diseases and abnormalities in CXR scans. However, because of\nthe limited availability of scans containing nodules and the subtle properties\nof nodules in CXRs, state-of-the-art methods do not perform well on nodule\nclassification. To create additional data for the training process, standard\naugmentation techniques are applied. However, the variance introduced by these\nmethods are limited as the images are typically modified globally. In this\npaper, we propose a method for local feature augmentation by extracting local\nnodule features using a generative inpainting network. The network is applied\nto generate realistic, healthy tissue and structures in patches containing\nnodules. The nodules are entirely removed in the inpainted representation. The\nextraction of the nodule features is processed by subtraction of the inpainted\npatch from the nodule patch. With arbitrary displacement of the extracted\nnodules in the lung area across different CXR scans and further local\nmodifications during training, we significantly increase the nodule\nclassification performance and outperform state-of-the-art augmentation\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 10:13:41 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Guendel", "Sebastian", ""], ["Setio", "Arnaud Arindra Adiyoso", ""], ["Grbic", "Sasa", ""], ["Maier", "Andreas", ""], ["Comaniciu", "Dorin", ""]]}, {"id": "2008.02042", "submitter": "Zhijun Liang", "authors": "Zhijun Liang, Junfa Liu, Yisheng Guan, and Juan Rojas", "title": "Pose-based Modular Network for Human-Object Interaction Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-object interaction(HOI) detection is a critical task in scene\nunderstanding. The goal is to infer the triplet <subject, predicate, object> in\na scene. In this work, we note that the human pose itself as well as the\nrelative spatial information of the human pose with respect to the target\nobject can provide informative cues for HOI detection. We contribute a\nPose-based Modular Network (PMN) which explores the absolute pose features and\nrelative spatial pose features to improve HOI detection and is fully compatible\nwith existing networks. Our module consists of a branch that first processes\nthe relative spatial pose features of each joint independently. Another branch\nupdates the absolute pose features via fully connected graph structures. The\nprocessed pose features are then fed into an action classifier. To evaluate our\nproposed method, we combine the module with the state-of-the-art model named\nVS-GATs and obtain significant improvement on two public benchmarks: V-COCO and\nHICO-DET, which shows its efficacy and flexibility. Code is available at\n\\url{https://github.com/birlrobotics/PMN}.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 10:56:09 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Liang", "Zhijun", ""], ["Liu", "Junfa", ""], ["Guan", "Yisheng", ""], ["Rojas", "Juan", ""]]}, {"id": "2008.02043", "submitter": "Jonghwa Yim", "authors": "Jonghwa Yim, Sang Hwan Kim", "title": "Learning Boost by Exploiting the Auxiliary Task in Multi-task Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Learning two tasks in a single shared function has some benefits. Firstly by\nacquiring information from the second task, the shared function leverages\nuseful information that could have been neglected or underestimated in the\nfirst task. Secondly, it helps to generalize the function that can be learned\nusing generally applicable information for both tasks. To fully enjoy these\nbenefits, Multi-task Learning (MTL) has long been researched in various domains\nsuch as computer vision, language understanding, and speech synthesis. While\nMTL benefits from the positive transfer of information from multiple tasks, in\na real environment, tasks inevitably have a conflict between them during the\nlearning phase, called negative transfer. The negative transfer hampers\nfunction from achieving the optimality and degrades the performance. To solve\nthe problem of the task conflict, previous works only suggested partial\nsolutions that are not fundamental, but ad-hoc. A common approach is using a\nweighted sum of losses. The weights are adjusted to induce positive transfer.\nParadoxically, this kind of solution acknowledges the problem of negative\ntransfer and cannot remove it unless the weight of the task is set to zero.\nTherefore, these previous methods had limited success. In this paper, we\nintroduce a novel approach that can drive positive transfer and suppress\nnegative transfer by leveraging class-wise weights in the learning process. The\nweights act as an arbitrator of the fundamental unit of information to\ndetermine its positive or negative status to the main task.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 10:56:56 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Yim", "Jonghwa", ""], ["Kim", "Sang Hwan", ""]]}, {"id": "2008.02063", "submitter": "Amir Shirian", "authors": "A. Shirian, T. Guha", "title": "Compact Graph Architecture for Speech Emotion Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep graph approach to address the task of speech emotion\nrecognition. A compact, efficient and scalable way to represent data is in the\nform of graphs. Following the theory of graph signal processing, we propose to\nmodel speech signal as a cycle graph or a line graph. Such graph structure\nenables us to construct a Graph Convolution Network (GCN)-based architecture\nthat can perform an accurate graph convolution in contrast to the approximate\nconvolution used in standard GCNs. We evaluated the performance of our model\nfor speech emotion recognition on the popular IEMOCAP and MSP-IMPROV databases.\nOur model outperforms standard GCN and other relevant deep graph architectures\nindicating the effectiveness of our approach. When compared with existing\nspeech emotion recognition methods, our model achieves comparable performance\nto the state-of-the-art with significantly fewer learnable parameters (~30K)\nindicating its applicability in resource-constrained devices.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 12:09:09 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 12:42:52 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2021 14:08:11 GMT"}, {"version": "v4", "created": "Tue, 2 Feb 2021 10:34:47 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Shirian", "A.", ""], ["Guha", "T.", ""]]}, {"id": "2008.02086", "submitter": "Jinpeng Wang", "authors": "Jinpeng Wang, Yiqi Lin, Andy J.Ma", "title": "Self-supervised learning using consistency regularization of\n  spatio-temporal data augmentation for action recognition", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning has shown great potentials in improving the deep\nlearning model in an unsupervised manner by constructing surrogate supervision\nsignals directly from the unlabeled data. Different from existing works, we\npresent a novel way to obtain the surrogate supervision signal based on\nhigh-level feature maps under consistency regularization. In this paper, we\npropose a Spatio-Temporal Consistency Regularization between different output\nfeatures generated from a siamese network including a clean path fed with\noriginal video and a noise path fed with the corresponding augmented video.\nBased on the Spatio-Temporal characteristics of video, we develop two\nvideo-based data augmentation methods, i.e., Spatio-Temporal Transformation and\nIntra-Video Mixup. Consistency of the former one is proposed to model\ntransformation consistency of features, while the latter one aims at retaining\nspatial invariance to extract action-related features. Extensive experiments\ndemonstrate that our method achieves substantial improvements compared with\nstate-of-the-art self-supervised learning methods for action recognition. When\nusing our method as an additional regularization term and combine with current\nsurrogate supervision signals, we achieve 22% relative improvement over the\nprevious state-of-the-art on HMDB51 and 7% on UCF101.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 12:41:59 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Wang", "Jinpeng", ""], ["Lin", "Yiqi", ""], ["Ma", "Andy J.", ""]]}, {"id": "2008.02093", "submitter": "Duncan Tilley", "authors": "Duncan Tilley, Christopher W. Cleghorn, Kshitij Thorat, Roger Deane", "title": "Point Proposal Network: Accelerating Point Source Detection Through Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point source detection techniques are used to identify and localise point\nsources in radio astronomical surveys. With the development of the Square\nKilometre Array (SKA) telescope, survey images will see a massive increase in\nsize from Gigapixels to Terapixels. Point source detection has already proven\nto be a challenge in recent surveys performed by SKA pathfinder telescopes.\nThis paper proposes the Point Proposal Network (PPN): a point source detector\nthat utilises deep convolutional neural networks for fast source detection.\nResults measured on simulated MeerKAT images show that, although less precise\nwhen compared to leading alternative approaches, PPN performs source detection\nfaster and is able to scale to large images, unlike the alternative approaches.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 12:54:04 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 13:40:28 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Tilley", "Duncan", ""], ["Cleghorn", "Christopher W.", ""], ["Thorat", "Kshitij", ""], ["Deane", "Roger", ""]]}, {"id": "2008.02101", "submitter": "Dwarikanath Mahapatra", "authors": "Dwarikanath Mahapatra, Behzad Bozorgtabar, Jean-Philippe Thiran, Ling\n  Shao", "title": "Structure Preserving Stain Normalization of Histopathology Images Using\n  Self-Supervised Semantic Guidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although generative adversarial network (GAN) based style transfer is state\nof the art in histopathology color-stain normalization, they do not explicitly\nintegrate structural information of tissues. We propose a self-supervised\napproach to incorporate semantic guidance into a GAN based stain normalization\nframework and preserve detailed structural information. Our method does not\nrequire manual segmentation maps which is a significant advantage over existing\nmethods. We integrate semantic information at different layers between a\npre-trained semantic network and the stain color normalization network. The\nproposed scheme outperforms other color normalization methods leading to better\nclassification and segmentation performance.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 12:59:15 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 08:57:52 GMT"}, {"version": "v3", "created": "Thu, 3 Jun 2021 15:35:06 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Mahapatra", "Dwarikanath", ""], ["Bozorgtabar", "Behzad", ""], ["Thiran", "Jean-Philippe", ""], ["Shao", "Ling", ""]]}, {"id": "2008.02107", "submitter": "Gemma Roig", "authors": "Kshitij Dwivedi, Jiahui Huang, Radoslaw Martin Cichy, Gemma Roig", "title": "Duality Diagram Similarity: a generic framework for initialization\n  selection in task transfer learning", "comments": "accepted at ECCV 2020. Code available here:\n  https://github.com/cvai-repo/duality-diagram-similarity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle an open research question in transfer learning,\nwhich is selecting a model initialization to achieve high performance on a new\ntask, given several pre-trained models. We propose a new highly efficient and\naccurate approach based on duality diagram similarity (DDS) between deep neural\nnetworks (DNNs). DDS is a generic framework to represent and compare data of\ndifferent feature dimensions. We validate our approach on the Taskonomy dataset\nby measuring the correspondence between actual transfer learning performance\nrankings on 17 taskonomy tasks and predicted rankings. Computing DDS based\nranking for $17\\times17$ transfers requires less than 2 minutes and shows a\nhigh correlation ($0.86$) with actual transfer learning rankings, outperforming\nstate-of-the-art methods by a large margin ($10\\%$) on the Taskonomy benchmark.\nWe also demonstrate the robustness of our model selection approach to a new\ntask, namely Pascal VOC semantic segmentation. Additionally, we show that our\nmethod can be applied to select the best layer locations within a DNN for\ntransfer learning on 2D, 3D and semantic tasks on NYUv2 and Pascal VOC\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 13:00:34 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Dwivedi", "Kshitij", ""], ["Huang", "Jiahui", ""], ["Cichy", "Radoslaw Martin", ""], ["Roig", "Gemma", ""]]}, {"id": "2008.02129", "submitter": "Jinpeng Wang", "authors": "Jinpeng Wang, Yiqi Lin, Andy J. Ma, Pong C. Yuen", "title": "Self-supervised Temporal Discriminative Learning for Video\n  Representation Learning", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal cues in videos provide important information for recognizing actions\naccurately. However, temporal-discriminative features can hardly be extracted\nwithout using an annotated large-scale video action dataset for training. This\npaper proposes a novel Video-based Temporal-Discriminative Learning (VTDL)\nframework in self-supervised manner. Without labelled data for network\npretraining, temporal triplet is generated for each anchor video by using\nsegment of the same or different time interval so as to enhance the capacity\nfor temporal feature representation. Measuring temporal information by time\nderivative, Temporal Consistent Augmentation (TCA) is designed to ensure that\nthe time derivative (in any order) of the augmented positive is invariant\nexcept for a scaling constant. Finally, temporal-discriminative features are\nlearnt by minimizing the distance between each anchor and its augmented\npositive, while the distance between each anchor and its augmented negative as\nwell as other videos saved in the memory bank is maximized to enrich the\nrepresentation diversity. In the downstream action recognition task, the\nproposed method significantly outperforms existing related works. Surprisingly,\nthe proposed self-supervised approach is better than fully-supervised methods\non UCF101 and HMDB51 when a small-scale video dataset (with only thousands of\nvideos) is used for pre-training. The code has been made publicly available on\nhttps://github.com/FingerRec/Self-Supervised-Temporal-Discriminative-Representation-Learning-for-Video-Action-Recognition.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 13:36:59 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Wang", "Jinpeng", ""], ["Lin", "Yiqi", ""], ["Ma", "Andy J.", ""], ["Yuen", "Pong C.", ""]]}, {"id": "2008.02150", "submitter": "Rula Amer M.Sc", "authors": "Rula Amer, Maayan Frid-Adar, Ophir Gozes, Jannette Nassar, Hayit\n  Greenspan", "title": "COVID-19 in CXR: from Detection and Severity Scoring to Patient Disease\n  Monitoring", "comments": "paper was accepted to JBHI IEEE journal", "journal-ref": "IEEE J Biomed Health Inform. 2021 Mar 26;PP", "doi": "10.1109/JBHI.2021.3069169", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we estimate the severity of pneumonia in COVID-19 patients and\nconduct a longitudinal study of disease progression. To achieve this goal, we\ndeveloped a deep learning model for simultaneous detection and segmentation of\npneumonia in chest Xray (CXR) images and generalized to COVID-19 pneumonia. The\nsegmentations were utilized to calculate a \"Pneumonia Ratio\" which indicates\nthe disease severity. The measurement of disease severity enables to build a\ndisease extent profile over time for hospitalized patients. To validate the\nmodel relevance to the patient monitoring task, we developed a validation\nstrategy which involves a synthesis of Digital Reconstructed Radiographs (DRRs\n- synthetic Xray) from serial CT scans; we then compared the disease\nprogression profiles that were generated from the DRRs to those that were\ngenerated from CT volumes.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 13:50:35 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 09:37:30 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Amer", "Rula", ""], ["Frid-Adar", "Maayan", ""], ["Gozes", "Ophir", ""], ["Nassar", "Jannette", ""], ["Greenspan", "Hayit", ""]]}, {"id": "2008.02170", "submitter": "Ivan Khokhlov", "authors": "Ivan Khokhlov, Egor Davydenko, Ilya Osokin, Ilya Ryakin, Azer Babaev,\n  Vladimir Litvinenko, Roman Gorbachev", "title": "Tiny-YOLO object detection supplemented with geometrical data", "comments": "5 pages, 5 figures, published in 2020 IEEE 91st Vehicular Technology\n  Conference (VTC2020-Spring)", "journal-ref": null, "doi": "10.1109/VTC2020-Spring48590.2020.9128749", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method of improving detection precision (mAP) with the help of\nthe prior knowledge about the scene geometry: we assume the scene to be a plane\nwith objects placed on it. We focus our attention on autonomous robots, so\ngiven the robot's dimensions and the inclination angles of the camera, it is\npossible to predict the spatial scale for each pixel of the input frame. With\nslightly modified YOLOv3-tiny we demonstrate that the detection supplemented by\nthe scale channel, further referred as S, outperforms standard RGB-based\ndetection with small computational overhead.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 14:45:19 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 19:15:01 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Khokhlov", "Ivan", ""], ["Davydenko", "Egor", ""], ["Osokin", "Ilya", ""], ["Ryakin", "Ilya", ""], ["Babaev", "Azer", ""], ["Litvinenko", "Vladimir", ""], ["Gorbachev", "Roman", ""]]}, {"id": "2008.02191", "submitter": "Siddharth Ancha", "authors": "Siddharth Ancha, Yaadhav Raaj, Peiyun Hu, Srinivasa G. Narasimhan,\n  David Held", "title": "Active Perception using Light Curtains for Autonomous Driving", "comments": "Published at the European Conference on Computer Vision (ECCV), 2020", "journal-ref": null, "doi": "10.1007/978-3-030-58558-7_44", "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most real-world 3D sensors such as LiDARs perform fixed scans of the entire\nenvironment, while being decoupled from the recognition system that processes\nthe sensor data. In this work, we propose a method for 3D object recognition\nusing light curtains, a resource-efficient controllable sensor that measures\ndepth at user-specified locations in the environment. Crucially, we propose\nusing prediction uncertainty of a deep learning based 3D point cloud detector\nto guide active perception. Given a neural network's uncertainty, we derive an\noptimization objective to place light curtains using the principle of\nmaximizing information gain. Then, we develop a novel and efficient\noptimization algorithm to maximize this objective by encoding the physical\nconstraints of the device into a constraint graph and optimizing with dynamic\nprogramming. We show how a 3D detector can be trained to detect objects in a\nscene by sequentially placing uncertainty-guided light curtains to successively\nimprove detection accuracy. Code and details can be found on the project\nwebpage: http://siddancha.github.io/projects/active-perception-light-curtains.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 15:38:18 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Ancha", "Siddharth", ""], ["Raaj", "Yaadhav", ""], ["Hu", "Peiyun", ""], ["Narasimhan", "Srinivasa G.", ""], ["Held", "David", ""]]}, {"id": "2008.02198", "submitter": "Zhixiang Wang", "authors": "Hsin-Yu Chang, Zhixiang Wang, Yung-Yu Chuang", "title": "Domain-Specific Mappings for Generative Adversarial Style Transfer", "comments": "ECCV 2020, Project url: https://acht7111020.github.io/DSMAP-demo/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Style transfer generates an image whose content comes from one image and\nstyle from the other. Image-to-image translation approaches with disentangled\nrepresentations have been shown effective for style transfer between two image\ncategories. However, previous methods often assume a shared domain-invariant\ncontent space, which could compromise the content representation power. For\naddressing this issue, this paper leverages domain-specific mappings for\nremapping latent features in the shared content space to domain-specific\ncontent spaces. This way, images can be encoded more properly for style\ntransfer. Experiments show that the proposed method outperforms previous style\ntransfer methods, particularly on challenging scenarios that would require\nsemantic correspondences between images. Code and results are available at\nhttps://acht7111020.github.io/DSMAP-demo/.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 15:55:25 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Chang", "Hsin-Yu", ""], ["Wang", "Zhixiang", ""], ["Chuang", "Yung-Yu", ""]]}, {"id": "2008.02231", "submitter": "Roee Litman", "authors": "Amir Markovitz, Inbal Lavi, Or Perel, Shai Mazor and Roee Litman", "title": "Can You Read Me Now? Content Aware Rectification using Angle Supervision", "comments": "Presented in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquity of smartphone cameras has led to more and more documents being\ncaptured by cameras rather than scanned. Unlike flatbed scanners, photographed\ndocuments are often folded and crumpled, resulting in large local variance in\ntext structure. The problem of document rectification is fundamental to the\nOptical Character Recognition (OCR) process on documents, and its ability to\novercome geometric distortions significantly affects recognition accuracy.\nDespite the great progress in recent OCR systems, most still rely on a\npre-process that ensures the text lines are straight and axis aligned. Recent\nworks have tackled the problem of rectifying document images taken in-the-wild\nusing various supervision signals and alignment means. However, they focused on\nglobal features that can be extracted from the document's boundaries, ignoring\nvarious signals that could be obtained from the document's content.\n  We present CREASE: Content Aware Rectification using Angle Supervision, the\nfirst learned method for document rectification that relies on the document's\ncontent, the location of the words and specifically their orientation, as hints\nto assist in the rectification process. We utilize a novel pixel-wise angle\nregression approach and a curvature estimation side-task for optimizing our\nrectification model. Our method surpasses previous approaches in terms of OCR\naccuracy, geometric error and visual similarity.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 16:58:13 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Markovitz", "Amir", ""], ["Lavi", "Inbal", ""], ["Perel", "Or", ""], ["Mazor", "Shai", ""], ["Litman", "Roee", ""]]}, {"id": "2008.02251", "submitter": "Thomas K\\\"ustner", "authors": "Thomas K\\\"ustner, Tobias Hepp, Marc Fischer, Martin Schwartz, Andreas\n  Fritsche, Hans-Ulrich H\\\"aring, Konstantin Nikolaou, Fabian Bamberg, Bin\n  Yang, Fritz Schick, Sergios Gatidis, J\\\"urgen Machann", "title": "Fully Automated and Standardized Segmentation of Adipose Tissue\n  Compartments by Deep Learning in Three-dimensional Whole-body MRI of\n  Epidemiological Cohort Studies", "comments": "This manuscript has been accepted for publication in Radiology:\n  Artificial Intelligence (https://pubs.rsna.org/journal/ai), which is\n  published by the Radiological Society of North America (RSNA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To enable fast and reliable assessment of subcutaneous and visceral\nadipose tissue compartments derived from whole-body MRI. Methods:\nQuantification and localization of different adipose tissue compartments from\nwhole-body MR images is of high interest to examine metabolic conditions. For\ncorrect identification and phenotyping of individuals at increased risk for\nmetabolic diseases, a reliable automatic segmentation of adipose tissue into\nsubcutaneous and visceral adipose tissue is required. In this work we propose a\n3D convolutional neural network (DCNet) to provide a robust and objective\nsegmentation. In this retrospective study, we collected 1000 cases (66$\\pm$ 13\nyears; 523 women) from the Tuebingen Family Study and from the German Center\nfor Diabetes research (TUEF/DZD), as well as 300 cases (53$\\pm$ 11 years; 152\nwomen) from the German National Cohort (NAKO) database for model training,\nvalidation, and testing with a transfer learning between the cohorts. These\ndatasets had variable imaging sequences, imaging contrasts, receiver coil\narrangements, scanners and imaging field strengths. The proposed DCNet was\ncompared against a comparable 3D UNet segmentation in terms of sensitivity,\nspecificity, precision, accuracy, and Dice overlap. Results: Fast (5-7seconds)\nand reliable adipose tissue segmentation can be obtained with high Dice overlap\n(0.94), sensitivity (96.6%), specificity (95.1%), precision (92.1%) and\naccuracy (98.4%) from 3D whole-body MR datasets (field of view coverage\n450x450x2000mm${}^3$). Segmentation masks and adipose tissue profiles are\nautomatically reported back to the referring physician. Conclusion: Automatic\nadipose tissue segmentation is feasible in 3D whole-body MR data sets and is\ngeneralizable to different epidemiological cohort studies with the proposed\nDCNet.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 17:30:14 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["K\u00fcstner", "Thomas", ""], ["Hepp", "Tobias", ""], ["Fischer", "Marc", ""], ["Schwartz", "Martin", ""], ["Fritsche", "Andreas", ""], ["H\u00e4ring", "Hans-Ulrich", ""], ["Nikolaou", "Konstantin", ""], ["Bamberg", "Fabian", ""], ["Yang", "Bin", ""], ["Schick", "Fritz", ""], ["Gatidis", "Sergios", ""], ["Machann", "J\u00fcrgen", ""]]}, {"id": "2008.02254", "submitter": "Agostinho A. F. J\\'unior", "authors": "Janderson Ferreira (1), Agostinho A. F. J\\'unior (1), Yves M. Galv\\~ao\n  (1), Pablo Barros (2), Sergio Murilo Maciel Fernandes (1), Bruno J. T.\n  Fernandes (1) ((1) Universidade de Pernambuco - Escola Polit\\'ecnica de\n  Pernambuco, (2) Cognitive Architecture for Collaborative Technologies Unit -\n  Istituto Italiano di Tecnologia)", "title": "Performance Improvement of Path Planning algorithms with Deep Learning\n  Encoder Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, path planning algorithms are used in many daily tasks. They are\nrelevant to find the best route in traffic and make autonomous robots able to\nnavigate. The use of path planning presents some issues in large and dynamic\nenvironments. Large environments make these algorithms spend much time finding\nthe shortest path. On the other hand, dynamic environments request a new\nexecution of the algorithm each time a change occurs in the environment, and it\nincreases the execution time. The dimensionality reduction appears as a\nsolution to this problem, which in this context means removing useless paths\npresent in those environments. Most of the algorithms that reduce\ndimensionality are limited to the linear correlation of the input data.\nRecently, a Convolutional Neural Network (CNN) Encoder was used to overcome\nthis situation since it can use both linear and non-linear information to data\nreduction. This paper analyzes in-depth the performance to eliminate the\nuseless paths using this CNN Encoder model. To measure the mentioned model\nefficiency, we combined it with different path planning algorithms. Next, the\nfinal algorithms (combined and not combined) are checked in a database that is\ncomposed of five scenarios. Each scenario contains fixed and dynamic obstacles.\nTheir proposed model, the CNN Encoder, associated to other existent path\nplanning algorithms in the literature, was able to obtain a time decrease to\nfind the shortest path in comparison to all path planning algorithms analyzed.\nthe average decreased time was 54.43 %.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 17:34:31 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Ferreira", "Janderson", ""], ["J\u00fanior", "Agostinho A. F.", ""], ["Galv\u00e3o", "Yves M.", ""], ["Barros", "Pablo", ""], ["Fernandes", "Sergio Murilo Maciel", ""], ["Fernandes", "Bruno J. T.", ""]]}, {"id": "2008.02265", "submitter": "Haozhi Qi", "authors": "Haozhi Qi, Xiaolong Wang, Deepak Pathak, Yi Ma, Jitendra Malik", "title": "Learning Long-term Visual Dynamics with Region Proposal Interaction\n  Networks", "comments": "ICLR 2021; Code: https://github.com/HaozhiQi/RPIN Website:\n  https://haozhiqi.github.io/RPIN/ v5: update PHYRE results of each evaluation\n  fold", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning long-term dynamics models is the key to understanding physical\ncommon sense. Most existing approaches on learning dynamics from visual input\nsidestep long-term predictions by resorting to rapid re-planning with\nshort-term models. This not only requires such models to be super accurate but\nalso limits them only to tasks where an agent can continuously obtain feedback\nand take action at each step until completion. In this paper, we aim to\nleverage the ideas from success stories in visual recognition tasks to build\nobject representations that can capture inter-object and object-environment\ninteractions over a long-range. To this end, we propose Region Proposal\nInteraction Networks (RPIN), which reason about each object's trajectory in a\nlatent region-proposal feature space. Thanks to the simple yet effective object\nrepresentation, our approach outperforms prior methods by a significant margin\nboth in terms of prediction quality and their ability to plan for downstream\ntasks, and also generalize well to novel environments. Code, pre-trained\nmodels, and more visualization results are available at https://haozhi.io/RPIN.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 17:48:00 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 21:52:30 GMT"}, {"version": "v3", "created": "Thu, 18 Mar 2021 03:36:37 GMT"}, {"version": "v4", "created": "Thu, 1 Apr 2021 05:06:27 GMT"}, {"version": "v5", "created": "Fri, 2 Apr 2021 20:12:04 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Qi", "Haozhi", ""], ["Wang", "Xiaolong", ""], ["Pathak", "Deepak", ""], ["Ma", "Yi", ""], ["Malik", "Jitendra", ""]]}, {"id": "2008.02268", "submitter": "Daniel Duckworth", "authors": "Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T.\n  Barron, Alexey Dosovitskiy, Daniel Duckworth", "title": "NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo\n  Collections", "comments": "Project website: https://nerf-w.github.io. Ricardo Martin-Brualla,\n  Noha Radwan, and Mehdi S. M. Sajjadi contributed equally to this work.\n  Updated with results for three additional scenes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learning-based method for synthesizing novel views of complex\nscenes using only unstructured collections of in-the-wild photographs. We build\non Neural Radiance Fields (NeRF), which uses the weights of a multilayer\nperceptron to model the density and color of a scene as a function of 3D\ncoordinates. While NeRF works well on images of static subjects captured under\ncontrolled settings, it is incapable of modeling many ubiquitous, real-world\nphenomena in uncontrolled images, such as variable illumination or transient\noccluders. We introduce a series of extensions to NeRF to address these issues,\nthereby enabling accurate reconstructions from unstructured image collections\ntaken from the internet. We apply our system, dubbed NeRF-W, to internet photo\ncollections of famous landmarks, and demonstrate temporally consistent novel\nview renderings that are significantly closer to photorealism than the prior\nstate of the art.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 17:51:16 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 11:02:36 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 13:45:14 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Martin-Brualla", "Ricardo", ""], ["Radwan", "Noha", ""], ["Sajjadi", "Mehdi S. M.", ""], ["Barron", "Jonathan T.", ""], ["Dosovitskiy", "Alexey", ""], ["Duckworth", "Daniel", ""]]}, {"id": "2008.02312", "submitter": "Qingyong Hu", "authors": "Ruigang Fu, Qingyong Hu, Xiaohu Dong, Yulan Guo, Yinghui Gao, Biao Li", "title": "Axiom-based Grad-CAM: Towards Accurate Visualization and Explanation of\n  CNNs", "comments": "BMVC 2020 (Oral presentation). Code is avaliable at:\n  https://github.com/Fu0511/XGrad-CAM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  To have a better understanding and usage of Convolution Neural Networks\n(CNNs), the visualization and interpretation of CNNs has attracted increasing\nattention in recent years. In particular, several Class Activation Mapping\n(CAM) methods have been proposed to discover the connection between CNN's\ndecision and image regions. In spite of the reasonable visualization, lack of\nclear and sufficient theoretical support is the main limitation of these\nmethods. In this paper, we introduce two axioms -- Conservation and Sensitivity\n-- to the visualization paradigm of the CAM methods. Meanwhile, a dedicated\nAxiom-based Grad-CAM (XGrad-CAM) is proposed to satisfy these axioms as much as\npossible. Experiments demonstrate that XGrad-CAM is an enhanced version of\nGrad-CAM in terms of conservation and sensitivity. It is able to achieve better\nvisualization performance than Grad-CAM, while also be class-discriminative and\neasy-to-implement compared with Grad-CAM++ and Ablation-CAM. The code is\navailable at https://github.com/Fu0511/XGrad-CAM.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 18:42:33 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 10:47:39 GMT"}, {"version": "v3", "created": "Sat, 15 Aug 2020 06:38:53 GMT"}, {"version": "v4", "created": "Wed, 19 Aug 2020 06:04:28 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Fu", "Ruigang", ""], ["Hu", "Qingyong", ""], ["Dong", "Xiaohu", ""], ["Guo", "Yulan", ""], ["Gao", "Yinghui", ""], ["Li", "Biao", ""]]}, {"id": "2008.02321", "submitter": "Hongtao Wu", "authors": "Hongtao Wu, Gregory S. Chirikjian", "title": "Can I Pour into It? Robot Imagining Open Containability Affordance of\n  Previously Unseen Objects via Physical Simulations", "comments": "IEEE Robotics and Automation Letters. Video demos are available on\n  https://chirikjianlab.github.io/realcontainerimagination/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open containers, i.e., containers without covers, are an important and\nubiquitous class of objects in human life. In this letter, we propose a novel\nmethod for robots to \"imagine\" the open containability affordance of a\npreviously unseen object via physical simulations. We implement our imagination\nmethod on a UR5 manipulator. The robot autonomously scans the object with an\nRGB-D camera. The scanned 3D model is used for open containability imagination\nwhich quantifies the open containability affordance by physically simulating\ndropping particles onto the object and counting how many particles are retained\nin it. This quantification is used for open-container vs. non-open-container\nbinary classification (hereafter referred to as open container classification).\nIf the object is classified as an open container, the robot further imagines\npouring into the object, again using physical simulations, to obtain the\npouring position and orientation for real robot autonomous pouring. We evaluate\nour method on open container classification and autonomous pouring of granular\nmaterial on a dataset containing 130 previously unseen objects with 57 object\ncategories. Although our proposed method uses only 11 objects for simulation\ncalibration (training), its open container classification aligns well with\nhuman judgements. In addition, our method endows the robot with the capability\nto autonomously pour into the 55 containers in the dataset with a very high\nsuccess rate. We also compare to a deep learning method. Results show that our\nmethod achieves the same performance as the deep learning method on open\ncontainer classification and outperforms it on autonomous pouring. Moreover,\nour method is fully explainable.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 19:00:36 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 03:04:37 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Wu", "Hongtao", ""], ["Chirikjian", "Gregory S.", ""]]}, {"id": "2008.02340", "submitter": "Zhengyang Wang", "authors": "Zhengyang Wang, Yaochen Xie, Shuiwang Ji", "title": "Global Voxel Transformer Networks for Augmented Microscopy", "comments": "Supplementary Material:\n  https://documentcloud.adobe.com/link/track?uri=urn:aaid:scds:US:9fcf9e0d-6ea2-470b-8a89-ed09ac634ef8", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in deep learning have led to remarkable success in augmented\nmicroscopy, enabling us to obtain high-quality microscope images without using\nexpensive microscopy hardware and sample preparation techniques. However,\ncurrent deep learning models for augmented microscopy are mostly U-Net based\nneural networks, thus sharing certain drawbacks that limit the performance. In\nthis work, we introduce global voxel transformer networks (GVTNets), an\nadvanced deep learning tool for augmented microscopy that overcomes intrinsic\nlimitations of the current U-Net based models and achieves improved\nperformance. GVTNets are built on global voxel transformer operators (GVTOs),\nwhich are able to aggregate global information, as opposed to local operators\nlike convolutions. We apply the proposed methods on existing datasets for three\ndifferent augmented microscopy tasks under various settings. The performance is\nsignificantly and consistently better than previous U-Net based approaches.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 20:11:15 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 16:45:20 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Wang", "Zhengyang", ""], ["Xie", "Yaochen", ""], ["Ji", "Shuiwang", ""]]}, {"id": "2008.02344", "submitter": "Aryansh Omray", "authors": "Aryansh Omray and Samyak Jain and Utsav Krishnan and Pratik\n  Chattopadhyay", "title": "Exploiting Temporal Attention Features for Effective Denoising in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video Denoising is one of the fundamental tasks of any videoprocessing\npipeline. It is different from image denoising due to the tem-poral aspects of\nvideo frames, and any image denoising approach appliedto videos will result in\nflickering. The proposed method makes use oftemporal as well as spatial\ndimensions of video frames as part of a two-stage pipeline. Each stage in the\narchitecture named as Spatio-TemporalNetwork uses a channel-wise attention\nmechanism to forward the encodersignal to the decoder side. The Attention Block\nused in this paper usessoft attention to ranks the filters for better training.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 20:17:18 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 05:04:50 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Omray", "Aryansh", ""], ["Jain", "Samyak", ""], ["Krishnan", "Utsav", ""], ["Chattopadhyay", "Pratik", ""]]}, {"id": "2008.02356", "submitter": "Michael Kissner", "authors": "Michael Kissner", "title": "A Neural-Symbolic Framework for Mental Simulation", "comments": "Dissertation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural-symbolic framework for observing the environment and\ncontinuously learning visual semantics and intuitive physics to reproduce them\nin an interactive simulation. The framework consists of five parts, a\nneural-symbolic hybrid network based on capsules for inverse graphics, an\nepisodic memory to store observations, an interaction network for intuitive\nphysics, a meta-learning agent that continuously improves the framework and a\nquerying language that acts as the framework's interface for simulation. By\nmeans of lifelong meta-learning, the capsule network is expanded and trained\ncontinuously, in order to better adapt to its environment with each iteration.\nThis enables it to learn new semantics using a few-shot approach and with\nminimal input from an oracle over its lifetime. From what it learned through\nobservation, the part for intuitive physics infers all the required physical\nproperties of the objects in a scene, enabling predictions. Finally, a custom\nquery language ties all parts together, which allows to perform various mental\nsimulation tasks, such as navigation, sorting and simulation of a game\nenvironment, with which we illustrate the potential of our novel approach.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 20:43:55 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Kissner", "Michael", ""]]}, {"id": "2008.02366", "submitter": "Leszek Pecyna", "authors": "Leszek Pecyna, Angelo Cangelosi, Alessandro Di Nuovo", "title": "A robot that counts like a child: a developmental model of counting and\n  pointing", "comments": "28 pages, 13 figures. This is a pre-print of an article published in\n  Psychological Research. The final authenticated version is available online\n  at: https://doi.org/10.1007/s00426-020-01428-8", "journal-ref": "Psychological Research (2020)", "doi": "10.1007/s00426-020-01428-8", "report-no": null, "categories": "cs.LG cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, a novel neuro-robotics model capable of counting real items is\nintroduced. The model allows us to investigate the interaction between\nembodiment and numerical cognition. This is composed of a deep neural network\ncapable of image processing and sequential tasks performance, and a robotic\nplatform providing the embodiment - the iCub humanoid robot. The network is\ntrained using images from the robot's cameras and proprioceptive signals from\nits joints. The trained model is able to count a set of items and at the same\ntime points to them. We investigate the influence of pointing on the counting\nprocess and compare our results with those from studies with children. Several\ntraining approaches are presented in this paper all of them uses pre-training\nroutine allowing the network to gain the ability of pointing and number\nrecitation (from 1 to 10) prior to counting training. The impact of the counted\nset size and distance to the objects are investigated. The obtained results on\ncounting performance show similarities with those from human studies.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 21:06:27 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 23:48:30 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Pecyna", "Leszek", ""], ["Cangelosi", "Angelo", ""], ["Di Nuovo", "Alessandro", ""]]}, {"id": "2008.02382", "submitter": "Parichehr Behjati", "authors": "Parichehr Behjati, Pau Rodriguez, Armin Mehri, Isabelle Hupont, Jordi\n  Gonzalez, Carles Fernandez Tena", "title": "OverNet: Lightweight Multi-Scale Super-Resolution with Overscaling\n  Network", "comments": "10 pages, 4 figures, conference, accepted by WACV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution (SR) has achieved great success due to the development of\ndeep convolutional neural networks (CNNs). However, as the depth and width of\nthe networks increase, CNN-based SR methods have been faced with the challenge\nof computational complexity in practice. Moreover, most of them train a\ndedicated model for each target resolution, losing generality and increasing\nmemory requirements. To address these limitations we introduce OverNet, a deep\nbut lightweight convolutional network to solve SISR at arbitrary scale factors\nwith a single model. We make the following contributions: first, we introduce a\nlightweight recursive feature extractor that enforces efficient reuse of\ninformation through a novel recursive structure of skip and dense connections.\nSecond, to maximize the performance of the feature extractor we propose a\nreconstruction module that generates accurate high-resolution images from\noverscaled feature maps and can be independently used to improve existing\narchitectures. Third, we introduce a multi-scale loss function to achieve\ngeneralization across scales. Through extensive experiments, we demonstrate\nthat our network outperforms previous state-of-the-art results in standard\nbenchmarks while using fewer parameters than previous approaches.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 22:10:29 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 17:11:58 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Behjati", "Parichehr", ""], ["Rodriguez", "Pau", ""], ["Mehri", "Armin", ""], ["Hupont", "Isabelle", ""], ["Gonzalez", "Jordi", ""], ["Tena", "Carles Fernandez", ""]]}, {"id": "2008.02396", "submitter": "Chloe LeGendre", "authors": "Chloe LeGendre, Wan-Chun Ma, Rohit Pandey, Sean Fanello, Christoph\n  Rhemann, Jason Dourgarian, Jay Busch, Paul Debevec", "title": "Learning Illumination from Diverse Portraits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learning-based technique for estimating high dynamic range\n(HDR), omnidirectional illumination from a single low dynamic range (LDR)\nportrait image captured under arbitrary indoor or outdoor lighting conditions.\nWe train our model using portrait photos paired with their ground truth\nenvironmental illumination. We generate a rich set of such photos by using a\nlight stage to record the reflectance field and alpha matte of 70 diverse\nsubjects in various expressions. We then relight the subjects using image-based\nrelighting with a database of one million HDR lighting environments,\ncompositing the relit subjects onto paired high-resolution background imagery\nrecorded during the lighting acquisition. We train the lighting estimation\nmodel using rendering-based loss functions and add a multi-scale adversarial\nloss to estimate plausible high frequency lighting detail. We show that our\ntechnique outperforms the state-of-the-art technique for portrait-based\nlighting estimation, and we also show that our method reliably handles the\ninherent ambiguity between overall lighting strength and surface albedo,\nrecovering a similar scale of illumination for subjects with diverse skin\ntones. We demonstrate that our method allows virtual objects and digital\ncharacters to be added to a portrait photograph with consistent illumination.\nOur lighting inference runs in real-time on a smartphone, enabling realistic\nrendering and compositing of virtual objects into live video for augmented\nreality applications.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 23:41:23 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["LeGendre", "Chloe", ""], ["Ma", "Wan-Chun", ""], ["Pandey", "Rohit", ""], ["Fanello", "Sean", ""], ["Rhemann", "Christoph", ""], ["Dourgarian", "Jason", ""], ["Busch", "Jay", ""], ["Debevec", "Paul", ""]]}, {"id": "2008.02401", "submitter": "Rameen Abdal", "authors": "Rameen Abdal, Peihao Zhu, Niloy Mitra, Peter Wonka", "title": "StyleFlow: Attribute-conditioned Exploration of StyleGAN-Generated\n  Images using Conditional Continuous Normalizing Flows", "comments": "\"Project Page https://rameenabdal.github.io/StyleFlow Video:\n  https://youtu.be/LRAUJUn3EqQ \"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-quality, diverse, and photorealistic images can now be generated by\nunconditional GANs (e.g., StyleGAN). However, limited options exist to control\nthe generation process using (semantic) attributes, while still preserving the\nquality of the output. Further, due to the entangled nature of the GAN latent\nspace, performing edits along one attribute can easily result in unwanted\nchanges along other attributes. In this paper, in the context of conditional\nexploration of entangled latent spaces, we investigate the two sub-problems of\nattribute-conditioned sampling and attribute-controlled editing. We present\nStyleFlow as a simple, effective, and robust solution to both the sub-problems\nby formulating conditional exploration as an instance of conditional continuous\nnormalizing flows in the GAN latent space conditioned by attribute features. We\nevaluate our method using the face and the car latent space of StyleGAN, and\ndemonstrate fine-grained disentangled edits along various attributes on both\nreal photographs and StyleGAN generated images. For example, for faces, we vary\ncamera pose, illumination variation, expression, facial hair, gender, and age.\nFinally, via extensive qualitative and quantitative comparisons, we demonstrate\nthe superiority of StyleFlow to other concurrent works.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 00:10:03 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2020 15:39:46 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Abdal", "Rameen", ""], ["Zhu", "Peihao", ""], ["Mitra", "Niloy", ""], ["Wonka", "Peter", ""]]}, {"id": "2008.02421", "submitter": "Yok Yen Nguwi", "authors": "Ng Hui Xian Lynnette, Henry Ng Siong Hock, Nguwi Yok Yen", "title": "Cross-Model Image Annotation Platform with Active Learning", "comments": "8 pages.2 figures. 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We have seen significant leapfrog advancement in machine learning in recent\ndecades. The central idea of machine learnability lies on constructing learning\nalgorithms that learn from good data. The availability of more data being made\npublicly available also accelerates the growth of AI in recent years. In the\ndomain of computer vision, the quality of image data arises from the accuracy\nof image annotation. Labeling large volume of image data is a daunting and\ntedious task. This work presents an End-to-End pipeline tool for object\nannotation and recognition aims at enabling quick image labeling. We have\ndeveloped a modular image annotation platform which seamlessly incorporates\nassisted image annotation (annotation assistance), active learning and model\ntraining and evaluation. Our approach provides a number of advantages over\ncurrent image annotation tools. Firstly, the annotation assistance utilizes\nreference hierarchy and reference images to locate the objects in the images,\nthus reducing the need for annotating the whole object. Secondly, images can be\nannotated using polygon points allowing for objects of any shape to be\nannotated. Thirdly, it is also interoperable across several image models, and\nthe tool provides an interface for object model training and evaluation across\na series of pre-trained models. We have tested the model and embeds several\nbenchmarking deep learning models. The highest accuracy achieved is 74%.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 01:42:25 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Lynnette", "Ng Hui Xian", ""], ["Hock", "Henry Ng Siong", ""], ["Yen", "Nguwi Yok", ""]]}, {"id": "2008.02427", "submitter": "Yazhou Yao", "authors": "Zeren Sun, Xian-Sheng Hua, Yazhou Yao, Xiu-Shen Wei, Guosheng Hu, Jian\n  Zhang", "title": "Salvage Reusable Samples from Noisy Data for Robust Learning", "comments": "accepted by ACM MM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the existence of label noise in web images and the high memorization\ncapacity of deep neural networks, training deep fine-grained (FG) models\ndirectly through web images tends to have an inferior recognition ability. In\nthe literature, to alleviate this issue, loss correction methods try to\nestimate the noise transition matrix, but the inevitable false correction would\ncause severe accumulated errors. Sample selection methods identify clean\n(\"easy\") samples based on the fact that small losses can alleviate the\naccumulated errors. However, \"hard\" and mislabeled examples that can both boost\nthe robustness of FG models are also dropped. To this end, we propose a\ncertainty-based reusable sample selection and correction approach, termed as\nCRSSC, for coping with label noise in training deep FG models with web images.\nOur key idea is to additionally identify and correct reusable samples, and then\nleverage them together with clean examples to update the networks. We\ndemonstrate the superiority of the proposed approach from both theoretical and\nexperimental perspectives.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 02:07:21 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Sun", "Zeren", ""], ["Hua", "Xian-Sheng", ""], ["Yao", "Yazhou", ""], ["Wei", "Xiu-Shen", ""], ["Hu", "Guosheng", ""], ["Zhang", "Jian", ""]]}, {"id": "2008.02436", "submitter": "Ying Liu", "authors": "Ying Liu and Wenhong Cai and Xiaohui Yuan and Jinhai Xiang", "title": "GL-GAN: Adaptive Global and Local Bilevel Optimization model of Image\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Generative Adversarial Networks have shown remarkable performance in\nimage generation, there are some challenges in image realism and convergence\nspeed. The results of some models display the imbalances of quality within a\ngenerated image, in which some defective parts appear compared with other\nregions. Different from general single global optimization methods, we\nintroduce an adaptive global and local bilevel optimization model(GL-GAN). The\nmodel achieves the generation of high-resolution images in a complementary and\npromoting way, where global optimization is to optimize the whole images and\nlocal is only to optimize the low-quality areas. With a simple network\nstructure, GL-GAN is allowed to effectively avoid the nature of imbalance by\nlocal bilevel optimization, which is accomplished by first locating low-quality\nareas and then optimizing them. Moreover, by using feature map cues from\ndiscriminator output, we propose the adaptive local and global optimization\nmethod(Ada-OP) for specific implementation and find that it boosts the\nconvergence speed. Compared with the current GAN methods, our model has shown\nimpressive performance on CelebA, CelebA-HQ and LSUN datasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 03:00:09 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Liu", "Ying", ""], ["Cai", "Wenhong", ""], ["Yuan", "Xiaohui", ""], ["Xiang", "Jinhai", ""]]}, {"id": "2008.02438", "submitter": "Yazhou Yao", "authors": "Chuanyi Zhang, Yazhou Yao, Xiangbo Shu, Zechao Li, Zhenmin Tang, Qi Wu", "title": "Data-driven Meta-set Based Fine-Grained Visual Classification", "comments": "accepted by ACM MM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructing fine-grained image datasets typically requires domain-specific\nexpert knowledge, which is not always available for crowd-sourcing platform\nannotators. Accordingly, learning directly from web images becomes an\nalternative method for fine-grained visual recognition. However, label noise in\nthe web training set can severely degrade the model performance. To this end,\nwe propose a data-driven meta-set based approach to deal with noisy web images\nfor fine-grained recognition. Specifically, guided by a small amount of clean\nmeta-set, we train a selection net in a meta-learning manner to distinguish in-\nand out-of-distribution noisy images. To further boost the robustness of model,\nwe also learn a labeling net to correct the labels of in-distribution noisy\ndata. In this way, our proposed method can alleviate the harmful effects caused\nby out-of-distribution noise and properly exploit the in-distribution noisy\nsamples for training. Extensive experiments on three commonly used fine-grained\ndatasets demonstrate that our approach is much superior to state-of-the-art\nnoise-robust methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 03:04:16 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Zhang", "Chuanyi", ""], ["Yao", "Yazhou", ""], ["Shu", "Xiangbo", ""], ["Li", "Zechao", ""], ["Tang", "Zhenmin", ""], ["Wu", "Qi", ""]]}, {"id": "2008.02441", "submitter": "Junwen Chen", "authors": "Junwen Chen, Wentao Bao, Yu Kong", "title": "Group Activity Prediction with Sequential Relational Anticipation Model", "comments": "This paper is accepted to ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach to predict group activities given\nthe beginning frames with incomplete activity executions. Existing action\nprediction approaches learn to enhance the representation power of the partial\nobservation. However, for group activity prediction, the relation evolution of\npeople's activity and their positions over time is an important cue for\npredicting group activity. To this end, we propose a sequential relational\nanticipation model (SRAM) that summarizes the relational dynamics in the\npartial observation and progressively anticipates the group representations\nwith rich discriminative information. Our model explicitly anticipates both\nactivity features and positions by two graph auto-encoders, aiming to learn a\ndiscriminative group representation for group activity prediction. Experimental\nresults on two popularly used datasets demonstrate that our approach\nsignificantly outperforms the state-of-the-art activity prediction methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 03:17:14 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Chen", "Junwen", ""], ["Bao", "Wentao", ""], ["Kong", "Yu", ""]]}, {"id": "2008.02448", "submitter": "Yu Cheng", "authors": "Xiaoye Qu, Pengwei Tang, Zhikang Zhou, Yu Cheng, Jianfeng Dong, Pan\n  Zhou", "title": "Fine-grained Iterative Attention Network for TemporalLanguage\n  Localization in Videos", "comments": "ACM MM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal language localization in videos aims to ground one video segment in\nan untrimmed video based on a given sentence query. To tackle this task,\ndesigning an effective model to extract ground-ing information from both visual\nand textual modalities is crucial. However, most previous attempts in this\nfield only focus on unidirectional interactions from video to query, which\nemphasizes which words to listen and attends to sentence information via\nvanilla soft attention, but clues from query-by-video interactions implying\nwhere to look are not taken into consideration. In this paper, we propose a\nFine-grained Iterative Attention Network (FIAN) that consists of an iterative\nattention module for bilateral query-video in-formation extraction.\nSpecifically, in the iterative attention module, each word in the query is\nfirst enhanced by attending to each frame in the video through fine-grained\nattention, then video iteratively attends to the integrated query. Finally,\nboth video and query information is utilized to provide robust cross-modal\nrepresentation for further moment localization. In addition, to better predict\nthe target segment, we propose a content-oriented localization strategy instead\nof applying recent anchor-based localization. We evaluate the proposed method\non three challenging public benchmarks: Ac-tivityNet Captions, TACoS, and\nCharades-STA. FIAN significantly outperforms the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 04:09:03 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Qu", "Xiaoye", ""], ["Tang", "Pengwei", ""], ["Zhou", "Zhikang", ""], ["Cheng", "Yu", ""], ["Dong", "Jianfeng", ""], ["Zhou", "Pan", ""]]}, {"id": "2008.02454", "submitter": "Yash Bhalgat", "authors": "Yash Bhalgat, Yizhe Zhang, Jamie Lin, Fatih Porikli", "title": "Structured Convolutions for Efficient Neural Network Design", "comments": "Camera-ready for NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we tackle model efficiency by exploiting redundancy in the\n\\textit{implicit structure} of the building blocks of convolutional neural\nnetworks. We start our analysis by introducing a general definition of\nComposite Kernel structures that enable the execution of convolution operations\nin the form of efficient, scaled, sum-pooling components. As its special case,\nwe propose \\textit{Structured Convolutions} and show that these allow\ndecomposition of the convolution operation into a sum-pooling operation\nfollowed by a convolution with significantly lower complexity and fewer\nweights. We show how this decomposition can be applied to 2D and 3D kernels as\nwell as the fully-connected layers. Furthermore, we present a Structural\nRegularization loss that promotes neural network layers to leverage on this\ndesired structure in a way that, after training, they can be decomposed with\nnegligible performance loss. By applying our method to a wide range of CNN\narchitectures, we demonstrate \"structured\" versions of the ResNets that are up\nto 2$\\times$ smaller and a new Structured-MobileNetV2 that is more efficient\nwhile staying within an accuracy loss of 1% on ImageNet and CIFAR-10 datasets.\nWe also show similar structured versions of EfficientNet on ImageNet and HRNet\narchitecture for semantic segmentation on the Cityscapes dataset. Our method\nperforms equally well or superior in terms of the complexity reduction in\ncomparison to the existing tensor decomposition and channel pruning methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 04:38:38 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 04:41:55 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Bhalgat", "Yash", ""], ["Zhang", "Yizhe", ""], ["Lin", "Jamie", ""], ["Porikli", "Fatih", ""]]}, {"id": "2008.02457", "submitter": "Danfeng Hong", "authors": "Danfeng Hong, Lianru Gao, Jing Yao, Bing Zhang, Antonio Plaza, Jocelyn\n  Chanussot", "title": "Graph Convolutional Networks for Hyperspectral Image Classification", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2020.3015157", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To read the final version please go to IEEE TGRS on IEEE Xplore.\nConvolutional neural networks (CNNs) have been attracting increasing attention\nin hyperspectral (HS) image classification, owing to their ability to capture\nspatial-spectral feature representations. Nevertheless, their ability in\nmodeling relations between samples remains limited. Beyond the limitations of\ngrid sampling, graph convolutional networks (GCNs) have been recently proposed\nand successfully applied in irregular (or non-grid) data representation and\nanalysis. In this paper, we thoroughly investigate CNNs and GCNs (qualitatively\nand quantitatively) in terms of HS image classification. Due to the\nconstruction of the adjacency matrix on all the data, traditional GCNs usually\nsuffer from a huge computational cost, particularly in large-scale remote\nsensing (RS) problems. To this end, we develop a new mini-batch GCN (called\nminiGCN hereinafter) which allows to train large-scale GCNs in a mini-batch\nfashion. More significantly, our miniGCN is capable of inferring out-of-sample\ndata without re-training networks and improving classification performance.\nFurthermore, as CNNs and GCNs can extract different types of HS features, an\nintuitive solution to break the performance bottleneck of a single model is to\nfuse them. Since miniGCNs can perform batch-wise network training (enabling the\ncombination of CNNs and GCNs) we explore three fusion strategies: additive\nfusion, element-wise multiplicative fusion, and concatenation fusion to measure\nthe obtained performance gain. Extensive experiments, conducted on three HS\ndatasets, demonstrate the advantages of miniGCNs over GCNs and the superiority\nof the tested fusion strategies with regards to the single CNN or GCN models.\nThe codes of this work will be available at\nhttps://github.com/danfenghong/IEEE_TGRS_GCN for the sake of reproducibility.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 05:01:25 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 00:06:09 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Hong", "Danfeng", ""], ["Gao", "Lianru", ""], ["Yao", "Jing", ""], ["Zhang", "Bing", ""], ["Plaza", "Antonio", ""], ["Chanussot", "Jocelyn", ""]]}, {"id": "2008.02465", "submitter": "Zihang Jiang", "authors": "Zihang Jiang, Bingyi Kang, Kuangqi Zhou, Jiashi Feng", "title": "Few-shot Classification via Adaptive Attention", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a neural network model that can quickly adapt to a new task is\nhighly desirable yet challenging for few-shot learning problems. Recent\nfew-shot learning methods mostly concentrate on developing various\nmeta-learning strategies from two aspects, namely optimizing an initial model\nor learning a distance metric. In this work, we propose a novel few-shot\nlearning method via optimizing and fast adapting the query sample\nrepresentation based on very few reference samples. To be specific, we devise a\nsimple and efficient meta-reweighting strategy to adapt the sample\nrepresentations and generate soft attention to refine the representation such\nthat the relevant features from the query and support samples can be extracted\nfor a better few-shot classification. Such an adaptive attention model is also\nable to explain what the classification model is looking for as the evidence\nfor classification to some extent. As demonstrated experimentally, the proposed\nmodel achieves state-of-the-art classification results on various benchmark\nfew-shot classification and fine-grained recognition datasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 05:52:59 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 15:15:20 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Jiang", "Zihang", ""], ["Kang", "Bingyi", ""], ["Zhou", "Kuangqi", ""], ["Feng", "Jiashi", ""]]}, {"id": "2008.02492", "submitter": "Meng-Jiun Chiou", "authors": "Meng-Jiun Chiou, Zhenguang Liu, Yifang Yin, Anan Liu, Roger Zimmermann", "title": "Zero-Shot Multi-View Indoor Localization via Graph Location Networks", "comments": "Accepted at ACM MM 2020. 10 pages, 7 figures. Code and datasets\n  available at\n  https://github.com/coldmanck/zero-shot-indoor-localization-release", "journal-ref": "Proceedings of the 28th ACM International Conference on\n  Multimedia, 2020", "doi": "10.1145/3394171.3413856", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indoor localization is a fundamental problem in location-based applications.\nCurrent approaches to this problem typically rely on Radio Frequency\ntechnology, which requires not only supporting infrastructures but human\nefforts to measure and calibrate the signal. Moreover, data collection for all\nlocations is indispensable in existing methods, which in turn hinders their\nlarge-scale deployment. In this paper, we propose a novel neural network based\narchitecture Graph Location Networks (GLN) to perform infrastructure-free,\nmulti-view image based indoor localization. GLN makes location predictions\nbased on robust location representations extracted from images through\nmessage-passing networks. Furthermore, we introduce a novel zero-shot indoor\nlocalization setting and tackle it by extending the proposed GLN to a dedicated\nzero-shot version, which exploits a novel mechanism Map2Vec to train\nlocation-aware embeddings and make predictions on novel unseen locations. Our\nextensive experiments show that the proposed approach outperforms\nstate-of-the-art methods in the standard setting, and achieves promising\naccuracy even in the zero-shot setting where data for half of the locations are\nnot available. The source code and datasets are publicly available at\nhttps://github.com/coldmanck/zero-shot-indoor-localization-release.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 07:36:55 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Chiou", "Meng-Jiun", ""], ["Liu", "Zhenguang", ""], ["Yin", "Yifang", ""], ["Liu", "Anan", ""], ["Zimmermann", "Roger", ""]]}, {"id": "2008.02500", "submitter": "Wojciech Michal Matkowski", "authors": "Wojciech Michal Matkowski, Adams Wai Kin Kong", "title": "Gender and Ethnicity Classification based on Palmprint and Palmar Hand\n  Images from Uncontrolled Environment", "comments": "Accepted in the International Joint Conference on Biometrics (IJCB\n  2020), scheduled for Sep 28-Oct 1, 2020", "journal-ref": null, "doi": "10.1109/IJCB48548.2020.9304907", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Soft biometric attributes such as gender, ethnicity or age may provide useful\ninformation for biometrics and forensics applications. Researchers used, e.g.,\nface, gait, iris, and hand, etc. to classify such attributes. Even though hand\nhas been widely studied for biometric recognition, relatively less attention\nhas been given to soft biometrics from hand. Previous studies of soft\nbiometrics based on hand images focused on gender and well-controlled imaging\nenvironment. In this paper, the gender and ethnicity classification in\nuncontrolled environment are considered. Gender and ethnicity labels are\ncollected and provided for subjects in a publicly available database, which\ncontains hand images from the Internet. Five deep learning models are\nfine-tuned and evaluated in gender and ethnicity classification scenarios based\non palmar 1) full hand, 2) segmented hand and 3) palmprint images. The\nexperimental results indicate that for gender and ethnicity classification in\nuncontrolled environment, full and segmented hand images are more suitable than\npalmprint images.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 07:50:06 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Matkowski", "Wojciech Michal", ""], ["Kong", "Adams Wai Kin", ""]]}, {"id": "2008.02514", "submitter": "Yue Dong", "authors": "Xin Wei, Guojun Chen, Yue Dong, Stephen Lin and Xin Tong", "title": "Object-based Illumination Estimation with Rendering-aware Neural\n  Networks", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scheme for fast environment light estimation from the RGBD\nappearance of individual objects and their local image areas. Conventional\ninverse rendering is too computationally demanding for real-time applications,\nand the performance of purely learning-based techniques may be limited by the\nmeager input data available from individual objects. To address these issues,\nwe propose an approach that takes advantage of physical principles from inverse\nrendering to constrain the solution, while also utilizing neural networks to\nexpedite the more computationally expensive portions of its processing, to\nincrease robustness to noisy input data as well as to improve temporal and\nspatial stability. This results in a rendering-aware system that estimates the\nlocal illumination distribution at an object with high accuracy and in real\ntime. With the estimated lighting, virtual objects can be rendered in AR\nscenarios with shading that is consistent to the real scene, leading to\nimproved realism.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 08:23:19 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Wei", "Xin", ""], ["Chen", "Guojun", ""], ["Dong", "Yue", ""], ["Lin", "Stephen", ""], ["Tong", "Xin", ""]]}, {"id": "2008.02516", "submitter": "Jinglin Liu", "authors": "Jinglin Liu, Yi Ren, Zhou Zhao, Chen Zhang, Baoxing Huai, Nicholas\n  Jing Yuan", "title": "FastLR: Non-Autoregressive Lipreading Model with Integrate-and-Fire", "comments": "Accepted by ACM MM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.CV cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lipreading is an impressive technique and there has been a definite\nimprovement of accuracy in recent years. However, existing methods for\nlipreading mainly build on autoregressive (AR) model, which generate target\ntokens one by one and suffer from high inference latency. To breakthrough this\nconstraint, we propose FastLR, a non-autoregressive (NAR) lipreading model\nwhich generates all target tokens simultaneously. NAR lipreading is a\nchallenging task that has many difficulties: 1) the discrepancy of sequence\nlengths between source and target makes it difficult to estimate the length of\nthe output sequence; 2) the conditionally independent behavior of NAR\ngeneration lacks the correlation across time which leads to a poor\napproximation of target distribution; 3) the feature representation ability of\nencoder can be weak due to lack of effective alignment mechanism; and 4) the\nremoval of AR language model exacerbates the inherent ambiguity problem of\nlipreading. Thus, in this paper, we introduce three methods to reduce the gap\nbetween FastLR and AR model: 1) to address challenges 1 and 2, we leverage\nintegrate-and-fire (I\\&F) module to model the correspondence between source\nvideo frames and output text sequence. 2) To tackle challenge 3, we add an\nauxiliary connectionist temporal classification (CTC) decoder to the top of the\nencoder and optimize it with extra CTC loss. We also add an auxiliary\nautoregressive decoder to help the feature extraction of encoder. 3) To\novercome challenge 4, we propose a novel Noisy Parallel Decoding (NPD) for I\\&F\nand bring Byte-Pair Encoding (BPE) into lipreading. Our experiments exhibit\nthat FastLR achieves the speedup up to 10.97$\\times$ comparing with\nstate-of-the-art lipreading model with slight WER absolute increase of 1.5\\%\nand 5.5\\% on GRID and LRS2 lipreading datasets respectively, which demonstrates\nthe effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 08:28:56 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 12:17:54 GMT"}, {"version": "v3", "created": "Mon, 18 Jan 2021 05:04:54 GMT"}, {"version": "v4", "created": "Mon, 15 Mar 2021 07:23:19 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Liu", "Jinglin", ""], ["Ren", "Yi", ""], ["Zhao", "Zhou", ""], ["Zhang", "Chen", ""], ["Huai", "Baoxing", ""], ["Yuan", "Nicholas Jing", ""]]}, {"id": "2008.02520", "submitter": "Nan Pu", "authors": "Nan Pu, Wei Chen, Yu Liu, Erwin M. Bakker, Michael S. Lew", "title": "Dual Gaussian-based Variational Subspace Disentanglement for\n  Visible-Infrared Person Re-Identification", "comments": "Accepted by ACM MM 2020 poster. 12 pages, 10 appendixes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visible-infrared person re-identification (VI-ReID) is a challenging and\nessential task in night-time intelligent surveillance systems. Except for the\nintra-modality variance that RGB-RGB person re-identification mainly overcomes,\nVI-ReID suffers from additional inter-modality variance caused by the inherent\nheterogeneous gap. To solve the problem, we present a carefully designed dual\nGaussian-based variational auto-encoder (DG-VAE), which disentangles an\nidentity-discriminable and an identity-ambiguous cross-modality feature\nsubspace, following a mixture-of-Gaussians (MoG) prior and a standard Gaussian\ndistribution prior, respectively. Disentangling cross-modality\nidentity-discriminable features leads to more robust retrieval for VI-ReID. To\nachieve efficient optimization like conventional VAE, we theoretically derive\ntwo variational inference terms for the MoG prior under the supervised setting,\nwhich not only restricts the identity-discriminable subspace so that the model\nexplicitly handles the cross-modality intra-identity variance, but also enables\nthe MoG distribution to avoid posterior collapse. Furthermore, we propose a\ntriplet swap reconstruction (TSR) strategy to promote the above disentangling\nprocess. Extensive experiments demonstrate that our method outperforms\nstate-of-the-art methods on two VI-ReID datasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 08:43:35 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Pu", "Nan", ""], ["Chen", "Wei", ""], ["Liu", "Yu", ""], ["Bakker", "Erwin M.", ""], ["Lew", "Michael S.", ""]]}, {"id": "2008.02531", "submitter": "Li Tao", "authors": "Li Tao, Xueting Wang, Toshihiko Yamasaki", "title": "Self-supervised Video Representation Learning Using Inter-intra\n  Contrastive Framework", "comments": "Accepted by ACMMM 2020. Our project page is at\n  https://bestjuly.github.io/Inter-intra-video-contrastive-learning/", "journal-ref": null, "doi": "10.1145/3394171.3413694", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a self-supervised method to learn feature representations from\nvideos. A standard approach in traditional self-supervised methods uses\npositive-negative data pairs to train with contrastive learning strategy. In\nsuch a case, different modalities of the same video are treated as positives\nand video clips from a different video are treated as negatives. Because the\nspatio-temporal information is important for video representation, we extend\nthe negative samples by introducing intra-negative samples, which are\ntransformed from the same anchor video by breaking temporal relations in video\nclips. With the proposed Inter-Intra Contrastive (IIC) framework, we can train\nspatio-temporal convolutional networks to learn video representations. There\nare many flexible options in our IIC framework and we conduct experiments by\nusing several different configurations. Evaluations are conducted on video\nretrieval and video recognition tasks using the learned video representation.\nOur proposed IIC outperforms current state-of-the-art results by a large\nmargin, such as 16.7% and 9.5% points improvements in top-1 accuracy on UCF101\nand HMDB51 datasets for video retrieval, respectively. For video recognition,\nimprovements can also be obtained on these two benchmark datasets. Code is\navailable at\nhttps://github.com/BestJuly/Inter-intra-video-contrastive-learning.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 09:08:14 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 07:28:38 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Tao", "Li", ""], ["Wang", "Xueting", ""], ["Yamasaki", "Toshihiko", ""]]}, {"id": "2008.02543", "submitter": "Leevi Raivio", "authors": "Leevi Raivio, Han He, Johanna Virkki, Heikki Huttunen", "title": "Handwritten Character Recognition from Wearable Passive RFID", "comments": "Submitted to ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the recognition of handwritten characters from data\ncaptured by a novel wearable electro-textile sensor panel. The data is\ncollected sequentially, such that we record both the stroke order and the\nresulting bitmap. We propose a preprocessing pipeline that fuses the sequence\nand bitmap representations together. The data is collected from ten subjects\ncontaining altogether 7500 characters. We also propose a convolutional neural\nnetwork architecture, whose novel upsampling structure enables successful use\nof conventional ImageNet pretrained networks, despite the small input size of\nonly 10x10 pixels. The proposed model reaches 72\\% accuracy in experimental\ntests, which can be considered good accuracy for this challenging dataset. Both\nthe data and the model are released to the public.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 09:45:29 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Raivio", "Leevi", ""], ["He", "Han", ""], ["Virkki", "Johanna", ""], ["Huttunen", "Heikki", ""]]}, {"id": "2008.02565", "submitter": "Nandan Kumar Jha", "authors": "Nandan Kumar Jha and Sparsh Mittal", "title": "Modeling Data Reuse in Deep Neural Networks by Taking Data-Types into\n  Cognizance", "comments": "Accepted at IEEE Transactions on Computers (Special Issue on\n  Machine-Learning Architectures and Accelerators) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, researchers have focused on reducing the model size and\nnumber of computations (measured as \"multiply-accumulate\" or MAC operations) of\nDNNs. The energy consumption of a DNN depends on both the number of MAC\noperations and the energy efficiency of each MAC operation. The former can be\nestimated at design time; however, the latter depends on the intricate data\nreuse patterns and underlying hardware architecture. Hence, estimating it at\ndesign time is challenging. This work shows that the conventional approach to\nestimate the data reuse, viz. arithmetic intensity, does not always correctly\nestimate the degree of data reuse in DNNs since it gives equal importance to\nall the data types. We propose a novel model, termed \"data type aware weighted\narithmetic intensity\" ($DI$), which accounts for the unequal importance of\ndifferent data types in DNNs. We evaluate our model on 25 state-of-the-art DNNs\non two GPUs. We show that our model accurately models data-reuse for all\npossible data reuse patterns for different types of convolution and different\ntypes of layers. We show that our model is a better indicator of the energy\nefficiency of DNNs. We also show its generality using the central limit\ntheorem.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 10:38:07 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Jha", "Nandan Kumar", ""], ["Mittal", "Sparsh", ""]]}, {"id": "2008.02566", "submitter": "Konstantin Bulatov", "authors": "Konstantin Bulatov, Nadezhda Fedotova, Vladimir V. Arlazarov", "title": "Fast Approximate Modelling of the Next Combination Result for Stopping\n  the Text Recognition in a Video", "comments": "8 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a task of stopping the video stream recognition\nprocess of a text field, in which each frame is recognized independently and\nthe individual results are combined together. The video stream recognition\nstopping problem is an under-researched topic with regards to computer vision,\nbut its relevance for building high-performance video recognition systems is\nclear.\n  Firstly, we describe an existing method of optimally stopping such a process\nbased on a modelling of the next combined result. Then, we describe\napproximations and assumptions which allowed us to build an optimized\ncomputation scheme and thus obtain a method with reduced computational\ncomplexity.\n  The methods were evaluated for the tasks of document text field recognition\nand arbitrary text recognition in a video. The experimental comparison shows\nthat the introduced approximations do not diminish the quality of the stopping\nmethod in terms of the achieved combined result precision, while dramatically\nreducing the time required to make the stopping decision. The results were\nconsistent for both text recognition tasks.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 10:42:30 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Bulatov", "Konstantin", ""], ["Fedotova", "Nadezhda", ""], ["Arlazarov", "Vladimir V.", ""]]}, {"id": "2008.02569", "submitter": "Ajoy Mondal Dr.", "authors": "Ajoy Mondal, Peter Lipps, and C. V. Jawahar", "title": "IIIT-AR-13K: A New Dataset for Graphical Object Detection in Documents", "comments": "15 pages, DAS 2020", "journal-ref": "Published in DAS 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new dataset for graphical object detection in business\ndocuments, more specifically annual reports. This dataset, IIIT-AR-13k, is\ncreated by manually annotating the bounding boxes of graphical or page objects\nin publicly available annual reports. This dataset contains a total of 13k\nannotated page images with objects in five different popular categories -\ntable, figure, natural image, logo, and signature. It is the largest manually\nannotated dataset for graphical object detection. Annual reports created in\nmultiple languages for several years from various companies bring high\ndiversity into this dataset. We benchmark IIIT-AR-13K dataset with two state of\nthe art graphical object detection techniques using Faster R-CNN [20] and Mask\nR-CNN [11] and establish high baselines for further research. Our dataset is\nhighly effective as training data for developing practical solutions for\ngraphical object detection in both business documents and technical articles.\nBy training with IIIT-AR-13K, we demonstrate the feasibility of a single\nsolution that can report superior performance compared to the equivalent ones\ntrained with a much larger amount of data, for table detection. We hope that\nour dataset helps in advancing the research for detecting various types of\ngraphical objects in business documents.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 10:59:20 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Mondal", "Ajoy", ""], ["Lipps", "Peter", ""], ["Jawahar", "C. V.", ""]]}, {"id": "2008.02580", "submitter": "Theo Ladune", "authors": "Th\\'eo Ladune (IETR), Pierrick Philippe, Wassim Hamidouche (IETR), Lu\n  Zhang (IETR), Olivier D\\'eforges (IETR)", "title": "Optical Flow and Mode Selection for Learning-based Video Coding", "comments": "MMSP 2020, IEEE 22nd International Workshop on Multimedia Signal\n  Processing, Sep 2020, Tampere, Finland", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new method for inter-frame coding based on two\ncomplementary autoencoders: MOFNet and CodecNet. MOFNet aims at computing and\nconveying the Optical Flow and a pixel-wise coding Mode selection. The optical\nflow is used to perform a prediction of the frame to code. The coding mode\nselection enables competition between direct copy of the prediction or\ntransmission through CodecNet. The proposed coding scheme is assessed under the\nChallenge on Learned Image Compression 2020 (CLIC20) P-frame coding conditions,\nwhere it is shown to perform on par with the state-of-the-art video codec\nITU/MPEG HEVC. Moreover, the possibility of copying the prediction enables to\nlearn the optical flow in an end-to-end fashion i.e. without relying on\npre-training and/or a dedicated loss term.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 11:21:22 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Ladune", "Th\u00e9o", "", "IETR"], ["Philippe", "Pierrick", "", "IETR"], ["Hamidouche", "Wassim", "", "IETR"], ["Zhang", "Lu", "", "IETR"], ["D\u00e9forges", "Olivier", "", "IETR"]]}, {"id": "2008.02593", "submitter": "Thanh Nguyen", "authors": "Thanh Nguyen-Duc, He Zhao, Jianfei Cai and Dinh Phung", "title": "MED-TEX: Transferring and Explaining Knowledge with Less Data from\n  Pretrained Medical Imaging Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network based image classification methods usually require a\nlarge amount of training data and lack interpretability, which are critical in\nthe medical imaging domain. In this paper, we develop a novel knowledge\ndistillation and model interpretation framework for medical image\nclassification that jointly solves the above two issues. Specifically, to\naddress the data-hungry issue, we propose to learn a small student model with\nless data by distilling knowledge only from a cumbersome pretrained teacher\nmodel. To interpret the teacher model as well as assisting the learning of the\nstudent, an explainer module is introduced to highlight the regions of an input\nmedical image that are important for the predictions of the teacher model.\nFurthermore, the joint framework is trained by a principled way derived from\nthe information-theoretic perspective. Our framework performance is\ndemonstrated by the comprehensive experiments on the knowledge distillation and\nmodel interpretation tasks compared to state-of-the-art methods on a fundus\ndisease dataset.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 11:50:32 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Nguyen-Duc", "Thanh", ""], ["Zhao", "He", ""], ["Cai", "Jianfei", ""], ["Phung", "Dinh", ""]]}, {"id": "2008.02595", "submitter": "Peter Harrison", "authors": "Peter M. C. Harrison, Raja Marjieh, Federico Adolfi, Pol van Rijn,\n  Manuel Anglada-Tort, Ofer Tchernichovski, Pauline Larrouy-Maestri, Nori\n  Jacoby", "title": "Gibbs Sampling with People", "comments": "Accepted for oral presentation at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A core problem in cognitive science and machine learning is to understand how\nhumans derive semantic representations from perceptual objects, such as color\nfrom an apple, pleasantness from a musical chord, or seriousness from a face.\nMarkov Chain Monte Carlo with People (MCMCP) is a prominent method for studying\nsuch representations, in which participants are presented with binary choice\ntrials constructed such that the decisions follow a Markov Chain Monte Carlo\nacceptance rule. However, while MCMCP has strong asymptotic properties, its\nbinary choice paradigm generates relatively little information per trial, and\nits local proposal function makes it slow to explore the parameter space and\nfind the modes of the distribution. Here we therefore generalize MCMCP to a\ncontinuous-sampling paradigm, where in each iteration the participant uses a\nslider to continuously manipulate a single stimulus dimension to optimize a\ngiven criterion such as 'pleasantness'. We formulate both methods from a\nutility-theory perspective, and show that the new method can be interpreted as\n'Gibbs Sampling with People' (GSP). Further, we introduce an aggregation\nparameter to the transition step, and show that this parameter can be\nmanipulated to flexibly shift between Gibbs sampling and deterministic\noptimization. In an initial study, we show GSP clearly outperforming MCMCP; we\nthen show that GSP provides novel and interpretable results in three other\ndomains, namely musical chords, vocal emotions, and faces. We validate these\nresults through large-scale perceptual rating experiments. The final\nexperiments use GSP to navigate the latent space of a state-of-the-art image\nsynthesis network (StyleGAN), a promising approach for applying GSP to\nhigh-dimensional perceptual spaces. We conclude by discussing future cognitive\napplications and ethical implications.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 11:57:07 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 16:55:40 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Harrison", "Peter M. C.", ""], ["Marjieh", "Raja", ""], ["Adolfi", "Federico", ""], ["van Rijn", "Pol", ""], ["Anglada-Tort", "Manuel", ""], ["Tchernichovski", "Ofer", ""], ["Larrouy-Maestri", "Pauline", ""], ["Jacoby", "Nori", ""]]}, {"id": "2008.02596", "submitter": "Andriy Sarabakha", "authors": "Theo Morales, Andriy Sarabakha, Erdal Kayacan", "title": "Image Generation for Efficient Neural Network Training in Autonomous\n  Drone Racing", "comments": "2020 International Joint Conference on Neural Networks (IJCNN 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drone racing is a recreational sport in which the goal is to pass through a\nsequence of gates in a minimum amount of time while avoiding collisions. In\nautonomous drone racing, one must accomplish this task by flying fully\nautonomously in an unknown environment by relying only on computer vision\nmethods for detecting the target gates. Due to the challenges such as\nbackground objects and varying lighting conditions, traditional object\ndetection algorithms based on colour or geometry tend to fail. Convolutional\nneural networks offer impressive advances in computer vision but require an\nimmense amount of data to learn. Collecting this data is a tedious process\nbecause the drone has to be flown manually, and the data collected can suffer\nfrom sensor failures. In this work, a semi-synthetic dataset generation method\nis proposed, using a combination of real background images and randomised 3D\nrenders of the gates, to provide a limitless amount of training samples that do\nnot suffer from those drawbacks. Using the detection results, a line-of-sight\nguidance algorithm is used to cross the gates. In several experimental\nreal-time tests, the proposed framework successfully demonstrates fast and\nreliable detection and navigation.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 12:07:36 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Morales", "Theo", ""], ["Sarabakha", "Andriy", ""], ["Kayacan", "Erdal", ""]]}, {"id": "2008.02604", "submitter": "Qianru Zhang", "authors": "Qianru Zhang, Meng Zhang, Chinthaka Gamanayake, Chau Yuen, Zehao Geng,\n  Hirunima Jayasekara, Xuewen Zhang, Chia-wei Woo, Jenny Low, Xiang Liu", "title": "Deep Learning Based Defect Detection for Solder Joints on Industrial\n  X-Ray Circuit Board Images", "comments": "Accepted by conference INDIN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality control is of vital importance during electronics production. As the\nmethods of producing electronic circuits improve, there is an increasing chance\nof solder defects during assembling the printed circuit board (PCB). Many\ntechnologies have been incorporated for inspecting failed soldering, such as\nX-ray imaging, optical imaging, and thermal imaging. With some advanced\nalgorithms, the new technologies are expected to control the production quality\nbased on the digital images. However, current algorithms sometimes are not\naccurate enough to meet the quality control. Specialists are needed to do a\nfollow-up checking. For automated X-ray inspection, joint of interest on the\nX-ray image is located by region of interest (ROI) and inspected by some\nalgorithms. Some incorrect ROIs deteriorate the inspection algorithm. The high\ndimension of X-ray images and the varying sizes of image dimensions also\nchallenge the inspection algorithms. On the other hand, recent advances on deep\nlearning shed light on image-based tasks and are competitive to human levels.\nIn this paper, deep learning is incorporated in X-ray imaging based quality\ncontrol during PCB quality inspection. Two artificial intelligence (AI) based\nmodels are proposed and compared for joint defect detection. The noised ROI\nproblem and the varying sizes of imaging dimension problem are addressed. The\nefficacy of the proposed methods are verified through experimenting on a\nreal-world 3D X-ray dataset. By incorporating the proposed methods, specialist\ninspection workload is largely saved.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 12:25:18 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 05:28:18 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Zhang", "Qianru", ""], ["Zhang", "Meng", ""], ["Gamanayake", "Chinthaka", ""], ["Yuen", "Chau", ""], ["Geng", "Zehao", ""], ["Jayasekara", "Hirunima", ""], ["Zhang", "Xuewen", ""], ["Woo", "Chia-wei", ""], ["Low", "Jenny", ""], ["Liu", "Xiang", ""]]}, {"id": "2008.02615", "submitter": "Daniil Tropin", "authors": "Daniil V. Tropin, Sergey A. Ilyuhin, Dmitry P. Nikolaev and Vladimir\n  V. Arlazarov", "title": "Approach for Document Detection by Contours and Contrasts", "comments": "This paper has been accepted to the ICPR 2020 conference in Milan\n  which will be held on the 10-15 January 2021. Therefore this work has not yet\n  been presented", "journal-ref": "2020 25th International Conference on Pattern Recognition (ICPR),\n  (2021) 9689-9695", "doi": "10.1109/ICPR48806.2021.9413271", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers arbitrary document detection performed on a mobile\ndevice. The classical contour-based approach often fails in cases featuring\nocclusion, complex background, or blur. The region-based approach, which relies\non the contrast between object and background, does not have application\nlimitations, however, its known implementations are highly resource-consuming.\nWe propose a modification of the contour-based method, in which the competing\ncontour location hypotheses are ranked according to the contrast between the\nareas inside and outside the border. In the experiments, such modification\nallows for the decrease of alternatives ordering errors by 40% and the decrease\nof the overall detection errors by 10%. The proposed method provides unmatched\nstate-of-the-art performance on the open MIDV-500 dataset, and it demonstrates\nresults comparable with state-of-the-art performance on the SmartDoc dataset.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 12:44:40 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 14:03:44 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Tropin", "Daniil V.", ""], ["Ilyuhin", "Sergey A.", ""], ["Nikolaev", "Dmitry P.", ""], ["Arlazarov", "Vladimir V.", ""]]}, {"id": "2008.02655", "submitter": "Vikas Kumar", "authors": "Vikas Kumar, Shivansh Rao, Li Yu", "title": "Noisy Student Training using Body Language Dataset Improves Facial\n  Expression Recognition", "comments": "Accepted in ECCV workshop BEEU (Bodily Expressed Emotion\n  Understanding) 2020", "journal-ref": null, "doi": "10.1007/978-3-030-66415-2_53", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression recognition from videos in the wild is a challenging task\ndue to the lack of abundant labelled training data. Large DNN (deep neural\nnetwork) architectures and ensemble methods have resulted in better\nperformance, but soon reach saturation at some point due to data inadequacy. In\nthis paper, we use a self-training method that utilizes a combination of a\nlabelled dataset and an unlabelled dataset (Body Language Dataset - BoLD).\nExperimental analysis shows that training a noisy student network iteratively\nhelps in achieving significantly better results. Additionally, our model\nisolates different regions of the face and processes them independently using a\nmulti-level attention mechanism which further boosts the performance. Our\nresults show that the proposed method achieves state-of-the-art performance on\nbenchmark datasets CK+ and AFEW 8.0 when compared to other single models.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 13:45:52 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 19:11:20 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Kumar", "Vikas", ""], ["Rao", "Shivansh", ""], ["Yu", "Li", ""]]}, {"id": "2008.02661", "submitter": "Amir Shirian", "authors": "A. Shirian, S. Tripathi, T. Guha", "title": "Dynamic Emotion Modeling with Learnable Graphs and Graph Inception\n  Network", "comments": null, "journal-ref": "10.1109/TMM.2021.3059169", "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human emotion is expressed, perceived and captured using a variety of dynamic\ndata modalities, such as speech (verbal), videos (facial expressions) and\nmotion sensors (body gestures). We propose a generalized approach to emotion\nrecognition that can adapt across modalities by modeling dynamic data as\nstructured graphs. The motivation behind the graph approach is to build compact\nmodels without compromising on performance. To alleviate the problem of optimal\ngraph construction, we cast this as a joint graph learning and classification\ntask. To this end, we present the Learnable Graph Inception Network (L-GrIN)\nthat jointly learns to recognize emotion and to identify the underlying graph\nstructure in the dynamic data. Our architecture comprises multiple novel\ncomponents: a new graph convolution operation, a graph inception layer,\nlearnable adjacency, and a learnable pooling function that yields a graph-level\nembedding. We evaluate the proposed architecture on five benchmark emotion\nrecognition databases spanning three different modalities (video, audio, motion\ncapture), where each database captures one of the following emotional cues:\nfacial expressions, speech and body gestures. We achieve state-of-the-art\nperformance on all five databases outperforming several competitive baselines\nand relevant existing methods. Our graph architecture shows superior\nperformance with significantly fewer parameters (compared to convolutional or\nrecurrent neural networks) promising its applicability to resource-constrained\ndevices.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 13:51:31 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 12:21:00 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Shirian", "A.", ""], ["Tripathi", "S.", ""], ["Guha", "T.", ""]]}, {"id": "2008.02693", "submitter": "Xuewen Yang", "authors": "Xuewen Yang, Heming Zhang, Di Jin, Yingru Liu, Chi-Hao Wu, Jianchao\n  Tan, Dongliang Xie, Jue Wang, Xin Wang", "title": "Fashion Captioning: Towards Generating Accurate Descriptions with\n  Semantic Rewards", "comments": "In proceedings of ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating accurate descriptions for online fashion items is important not\nonly for enhancing customers' shopping experiences, but also for the increase\nof online sales. Besides the need of correctly presenting the attributes of\nitems, the expressions in an enchanting style could better attract customer\ninterests. The goal of this work is to develop a novel learning framework for\naccurate and expressive fashion captioning. Different from popular work on\nimage captioning, it is hard to identify and describe the rich attributes of\nfashion items. We seed the description of an item by first identifying its\nattributes, and introduce attribute-level semantic (ALS) reward and\nsentence-level semantic (SLS) reward as metrics to improve the quality of text\ndescriptions. We further integrate the training of our model with maximum\nlikelihood estimation (MLE), attribute embedding, and Reinforcement Learning\n(RL). To facilitate the learning, we build a new FAshion CAptioning Dataset\n(FACAD), which contains 993K images and 130K corresponding enchanting and\ndiverse descriptions. Experiments on FACAD demonstrate the effectiveness of our\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 14:52:13 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Yang", "Xuewen", ""], ["Zhang", "Heming", ""], ["Jin", "Di", ""], ["Liu", "Yingru", ""], ["Wu", "Chi-Hao", ""], ["Tan", "Jianchao", ""], ["Xie", "Dongliang", ""], ["Wang", "Jue", ""], ["Wang", "Xin", ""]]}, {"id": "2008.02699", "submitter": "Yutong Xie", "authors": "Yutong Xie, Jianpeng Zhang, Zhibin Liao, Chunhua Shen, Johan Verjans,\n  Yong Xia", "title": "Pairwise Relation Learning for Semi-supervised Gland Segmentation", "comments": "Accepted by MICCAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Accurate and automated gland segmentation on histology tissue images is an\nessential but challenging task in the computer-aided diagnosis of\nadenocarcinoma. Despite their prevalence, deep learning models always require a\nmyriad number of densely annotated training images, which are difficult to\nobtain due to extensive labor and associated expert costs related to histology\nimage annotations. In this paper, we propose the pairwise relation-based\nsemi-supervised (PRS^2) model for gland segmentation on histology images. This\nmodel consists of a segmentation network (S-Net) and a pairwise relation\nnetwork (PR-Net). The S-Net is trained on labeled data for segmentation, and\nPR-Net is trained on both labeled and unlabeled data in an unsupervised way to\nenhance its image representation ability via exploiting the semantic\nconsistency between each pair of images in the feature space. Since both\nnetworks share their encoders, the image representation ability learned by\nPR-Net can be transferred to S-Net to improve its segmentation performance. We\nalso design the object-level Dice loss to address the issues caused by touching\nglands and combine it with other two loss functions for S-Net. We evaluated our\nmodel against five recent methods on the GlaS dataset and three recent methods\non the CRAG dataset. Our results not only demonstrate the effectiveness of the\nproposed PR-Net and object-level Dice loss, but also indicate that our PRS^2\nmodel achieves the state-of-the-art gland segmentation performance on both\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 15:02:38 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Xie", "Yutong", ""], ["Zhang", "Jianpeng", ""], ["Liao", "Zhibin", ""], ["Shen", "Chunhua", ""], ["Verjans", "Johan", ""], ["Xia", "Yong", ""]]}, {"id": "2008.02711", "submitter": "Dezhao Luo", "authors": "Dezhao Luo, Bo Fang, Yu Zhou, Yucan Zhou, Dayan Wu, Weiping Wang", "title": "Exploring Relations in Untrimmed Videos for Self-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing video self-supervised learning methods mainly rely on trimmed videos\nfor model training. However, trimmed datasets are manually annotated from\nuntrimmed videos. In this sense, these methods are not really self-supervised.\nIn this paper, we propose a novel self-supervised method, referred to as\nExploring Relations in Untrimmed Videos (ERUV), which can be straightforwardly\napplied to untrimmed videos (real unlabeled) to learn spatio-temporal features.\nERUV first generates single-shot videos by shot change detection. Then a\ndesigned sampling strategy is used to model relations for video clips. The\nstrategy is saved as our self-supervision signals. Finally, the network learns\nrepresentations by predicting the category of relations between the video\nclips. ERUV is able to compare the differences and similarities of videos,\nwhich is also an essential procedure for action and video related tasks. We\nvalidate our learned models with action recognition and video retrieval tasks\nwith three kinds of 3D CNNs. Experimental results show that ERUV is able to\nlearn richer representations and it outperforms state-of-the-art\nself-supervised methods with significant margins.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 15:29:25 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Luo", "Dezhao", ""], ["Fang", "Bo", ""], ["Zhou", "Yu", ""], ["Zhou", "Yucan", ""], ["Wu", "Dayan", ""], ["Wang", "Weiping", ""]]}, {"id": "2008.02725", "submitter": "Anthony Ngo", "authors": "Anthony Ngo, Max Paul Bauer, Michael Resch", "title": "A Sensitivity Analysis Approach for Evaluating a Radar Simulation for\n  Virtual Testing of Autonomous Driving Functions", "comments": "IEEE 2020 Asia-Pacific Conference on Intelligent Robot Systems (ACIRS\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation-based testing is a promising approach to significantly reduce the\nvalidation effort of automated driving functions. Realistic models of\nenvironment perception sensors such as camera, radar and lidar play a key role\nin this testing strategy. A generally accepted method to validate these sensor\nmodels does not yet exist. Particularly radar has traditionally been one of the\nmost difficult sensors to model. Although promising as an alternative to real\ntest drives, virtual tests are time-consuming due to the fact that they\nsimulate the entire radar system in detail, using computation-intensive\nsimulation techniques to approximate the propagation of electromagnetic waves.\nIn this paper, we introduce a sensitivity analysis approach for developing and\nevaluating a radar simulation, with the objective to identify the parameters\nwith the greatest impact regarding the system under test. A modular radar\nsystem simulation is presented and parameterized to conduct a sensitivity\nanalysis in order to evaluate a spatial clustering algorithm as the system\nunder test, while comparing the output from the radar model to real driving\nmeasurements to ensure a realistic model behavior. The presented approach is\nevaluated and it is demonstrated that with this approach results from different\nsituations can be traced back to the contribution of the individual sub-modules\nof the radar simulation.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 15:51:52 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 14:14:17 GMT"}, {"version": "v3", "created": "Thu, 13 Aug 2020 07:05:13 GMT"}, {"version": "v4", "created": "Mon, 12 Oct 2020 09:02:46 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Ngo", "Anthony", ""], ["Bauer", "Max Paul", ""], ["Resch", "Michael", ""]]}, {"id": "2008.02737", "submitter": "David Rosen", "authors": "Frank Dellaert, David M. Rosen, Jing Wu, Robert Mahony, and Luca\n  Carlone", "title": "Shonan Rotation Averaging: Global Optimality by Surfing $SO(p)^n$", "comments": "30 pages (paper + supplementary material). To appear at the European\n  Conference on Computer Vision (ECCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shonan Rotation Averaging is a fast, simple, and elegant rotation averaging\nalgorithm that is guaranteed to recover globally optimal solutions under mild\nassumptions on the measurement noise. Our method employs semidefinite\nrelaxation in order to recover provably globally optimal solutions of the\nrotation averaging problem. In contrast to prior work, we show how to solve\nlarge-scale instances of these relaxations using manifold minimization on (only\nslightly) higher-dimensional rotation manifolds, re-using existing\nhigh-performance (but local) structure-from-motion pipelines. Our method thus\npreserves the speed and scalability of current SFM methods, while recovering\nglobally optimal solutions.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 16:08:23 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Dellaert", "Frank", ""], ["Rosen", "David M.", ""], ["Wu", "Jing", ""], ["Mahony", "Robert", ""], ["Carlone", "Luca", ""]]}, {"id": "2008.02745", "submitter": "Zhipeng Zhang", "authors": "Zhipeng Zhang, Bing Li, Weiming Hu, Houwen Peng", "title": "Towards Accurate Pixel-wise Object Tracking by Attention Retrieval", "comments": "Some technical errors. We would provide new versions later", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The encoding of the target in object tracking moves from the coarse\nbounding-box to fine-grained segmentation map recently. Revisiting de facto\nreal-time approaches that are capable of predicting mask during tracking, we\nobserved that they usually fork a light branch from the backbone network for\nsegmentation. Although efficient, directly fusing backbone features without\nconsidering the negative influence of background clutter tends to introduce\nfalse-negative predictions, lagging the segmentation accuracy. To mitigate this\nproblem, we propose an attention retrieval network (ARN) to perform soft\nspatial constraints on backbone features. We first build a look-up-table (LUT)\nwith the ground-truth mask in the starting frame, and then retrieves the LUT to\nobtain an attention map for spatial constraints. Moreover, we introduce a\nmulti-resolution multi-stage segmentation network (MMS) to further weaken the\ninfluence of background clutter by reusing the predicted mask to filter\nbackbone features. Our approach set a new state-of-the-art on recent pixel-wise\nobject tracking benchmark VOT2020 while running at 40 fps. Notably, the\nproposed model surpasses SiamMask by 11.7/4.2/5.5 points on VOT2020, DAVIS2016,\nand DAVIS2017, respectively. We will release our code at\nhttps://github.com/researchmm/TracKit.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 16:25:23 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 14:55:51 GMT"}, {"version": "v3", "created": "Tue, 8 Sep 2020 02:06:33 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Zhang", "Zhipeng", ""], ["Li", "Bing", ""], ["Hu", "Weiming", ""], ["Peng", "Houwen", ""]]}, {"id": "2008.02749", "submitter": "Lucia Vadicamo", "authors": "Giuseppe Amato, Paolo Bolettieri, Fabio Carrara, Franca Debole,\n  Fabrizio Falchi, Claudio Gennaro, Lucia Vadicamo, Claudio Vairo", "title": "The VISIONE Video Search System: Exploiting Off-the-Shelf Text Search\n  Engines for Large-Scale Video Retrieval", "comments": "22 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe in details VISIONE, a video search system that\nallows users to search for videos using textual keywords, occurrence of objects\nand their spatial relationships, occurrence of colors and their spatial\nrelationships, and image similarity. These modalities can be combined together\nto express complex queries and satisfy user needs. The peculiarity of our\napproach is that we encode all the information extracted from the keyframes,\nsuch as visual deep features, tags, color and object locations, using a\nconvenient textual encoding indexed in a single text retrieval engine. This\noffers great flexibility when results corresponding to various parts of the\nquery (visual, text and locations) have to be merged. In addition, we report an\nextensive analysis of the system retrieval performance, using the query logs\ngenerated during the Video Browser Showdown (VBS) 2019 competition. This\nallowed us to fine-tune the system by choosing the optimal parameters and\nstrategies among the ones that we tested.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 16:32:17 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 14:37:27 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Amato", "Giuseppe", ""], ["Bolettieri", "Paolo", ""], ["Carrara", "Fabio", ""], ["Debole", "Franca", ""], ["Falchi", "Fabrizio", ""], ["Gennaro", "Claudio", ""], ["Vadicamo", "Lucia", ""], ["Vairo", "Claudio", ""]]}, {"id": "2008.02757", "submitter": "Michelle Kuchera", "authors": "Robert Solli, Daniel Bazin, Michelle P. Kuchera, Ryan R. Strauss,\n  Morten Hjorth-Jensen", "title": "Unsupervised Learning for Identifying Events in Active Target\n  Experiments", "comments": null, "journal-ref": null, "doi": "10.1016/j.nima.2021.165461", "report-no": null, "categories": "cs.CV nucl-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents novel applications of unsupervised machine learning\nmethods to the problem of event separation in an active target detector, the\nActive-Target Time Projection Chamber (AT-TPC). The overarching goal is to\ngroup similar events in the early stages of the data analysis, thereby\nimproving efficiency by limiting the computationally expensive processing of\nunnecessary events. The application of unsupervised clustering algorithms to\nthe analysis of two-dimensional projections of particle tracks from a resonant\nproton scattering experiment on $^{46}$Ar is introduced. We explore the\nperformance of autoencoder neural networks and a pre-trained VGG16\nconvolutional neural network. We study clustering performance on both data from\na simulated $^{46}$Ar experiment, and real events from the AT-TPC detector. We\nfind that a $k$-means algorithm applied to simulated data in the VGG16 latent\nspace forms almost perfect clusters. Additionally, the VGG16+$k$-means approach\nfinds high purity clusters of proton events for real experimental data. We also\nexplore the application of clustering the latent space of autoencoder neural\nnetworks for event separation. While these networks show strong performance,\nthey suffer from high variability in their results.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 16:49:39 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 00:48:31 GMT"}, {"version": "v3", "created": "Sat, 13 Mar 2021 17:48:41 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Solli", "Robert", ""], ["Bazin", "Daniel", ""], ["Kuchera", "Michelle P.", ""], ["Strauss", "Ryan R.", ""], ["Hjorth-Jensen", "Morten", ""]]}, {"id": "2008.02760", "submitter": "Sadegh Rabiee", "authors": "Sadegh Rabiee and Joydeep Biswas", "title": "IV-SLAM: Introspective Vision for Simultaneous Localization and Mapping", "comments": "To be published in CoRL 2020 (Conference on Robot Learning)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing solutions to visual simultaneous localization and mapping (V-SLAM)\nassume that errors in feature extraction and matching are independent and\nidentically distributed (i.i.d), but this assumption is known to not be true --\nfeatures extracted from low-contrast regions of images exhibit wider error\ndistributions than features from sharp corners. Furthermore, V-SLAM algorithms\nare prone to catastrophic tracking failures when sensed images include\nchallenging conditions such as specular reflections, lens flare, or shadows of\ndynamic objects. To address such failures, previous work has focused on\nbuilding more robust visual frontends, to filter out challenging features. In\nthis paper, we present introspective vision for SLAM (IV-SLAM), a fundamentally\ndifferent approach for addressing these challenges. IV-SLAM explicitly models\nthe noise process of reprojection errors from visual features to be\ncontext-dependent, and hence non-i.i.d. We introduce an autonomously supervised\napproach for IV-SLAM to collect training data to learn such a context-aware\nnoise model. Using this learned noise model, IV-SLAM guides feature extraction\nto select more features from parts of the image that are likely to result in\nlower noise, and further incorporate the learned noise model into the joint\nmaximum likelihood estimation, thus making it robust to the aforementioned\ntypes of errors. We present empirical results to demonstrate that IV-SLAM 1) is\nable to accurately predict sources of error in input images, 2) reduces\ntracking error compared to V-SLAM, and 3) increases the mean distance between\ntracking failures by more than 70% on challenging real robot data compared to\nV-SLAM.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 17:01:39 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 23:19:31 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Rabiee", "Sadegh", ""], ["Biswas", "Joydeep", ""]]}, {"id": "2008.02763", "submitter": "Cong Wang", "authors": "Cong Wang, Yutong Wu, Zhixun Su, Junyang Chen", "title": "Joint Self-Attention and Scale-Aggregation for Self-Calibrated Deraining\n  Network", "comments": "Accepted to ACM International Conference on Multimedia (MM'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of multimedia, single image deraining is a basic pre-processing\nwork, which can greatly improve the visual effect of subsequent high-level\ntasks in rainy conditions. In this paper, we propose an effective algorithm,\ncalled JDNet, to solve the single image deraining problem and conduct the\nsegmentation and detection task for applications. Specifically, considering the\nimportant information on multi-scale features, we propose a Scale-Aggregation\nmodule to learn the features with different scales. Simultaneously,\nSelf-Attention module is introduced to match or outperform their convolutional\ncounterparts, which allows the feature aggregation to adapt to each channel.\nFurthermore, to improve the basic convolutional feature transformation process\nof Convolutional Neural Networks (CNNs), Self-Calibrated convolution is applied\nto build long-range spatial and inter-channel dependencies around each spatial\nlocation that explicitly expand fields-of-view of each convolutional layer\nthrough internal communications and hence enriches the output features. By\ndesigning the Scale-Aggregation and Self-Attention modules with Self-Calibrated\nconvolution skillfully, the proposed model has better deraining results both on\nreal-world and synthetic datasets. Extensive experiments are conducted to\ndemonstrate the superiority of our method compared with state-of-the-art\nmethods. The source code will be available at\n\\url{https://supercong94.wixsite.com/supercong94}.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 17:04:34 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Wang", "Cong", ""], ["Wu", "Yutong", ""], ["Su", "Zhixun", ""], ["Chen", "Junyang", ""]]}, {"id": "2008.02766", "submitter": "Praveer Singh", "authors": "Nishanth Arun, Nathan Gaw, Praveer Singh, Ken Chang, Mehak Aggarwal,\n  Bryan Chen, Katharina Hoebel, Sharut Gupta, Jay Patel, Mishka Gidwani, Julius\n  Adebayo, Matthew D. Li, Jayashree Kalpathy-Cramer", "title": "Assessing the (Un)Trustworthiness of Saliency Maps for Localizing\n  Abnormalities in Medical Imaging", "comments": "Submitted to Radiology AI journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency maps have become a widely used method to make deep learning models\nmore interpretable by providing post-hoc explanations of classifiers through\nidentification of the most pertinent areas of the input medical image. They are\nincreasingly being used in medical imaging to provide clinically plausible\nexplanations for the decisions the neural network makes. However, the utility\nand robustness of these visualization maps has not yet been rigorously examined\nin the context of medical imaging. We posit that trustworthiness in this\ncontext requires 1) localization utility, 2) sensitivity to model weight\nrandomization, 3) repeatability, and 4) reproducibility. Using the localization\ninformation available in two large public radiology datasets, we quantify the\nperformance of eight commonly used saliency map approaches for the above\ncriteria using area under the precision-recall curves (AUPRC) and structural\nsimilarity index (SSIM), comparing their performance to various baseline\nmeasures. Using our framework to quantify the trustworthiness of saliency maps,\nwe show that all eight saliency map techniques fail at least one of the\ncriteria and are, in most cases, less trustworthy when compared to the\nbaselines. We suggest that their usage in the high-risk domain of medical\nimaging warrants additional scrutiny and recommend that detection or\nsegmentation models be used if localization is the desired output of the\nnetwork. Additionally, to promote reproducibility of our findings, we provide\nthe code we used for all tests performed in this work at this link:\nhttps://github.com/QTIM-Lab/Assessing-Saliency-Maps.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 17:11:19 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 02:39:22 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Arun", "Nishanth", ""], ["Gaw", "Nathan", ""], ["Singh", "Praveer", ""], ["Chang", "Ken", ""], ["Aggarwal", "Mehak", ""], ["Chen", "Bryan", ""], ["Hoebel", "Katharina", ""], ["Gupta", "Sharut", ""], ["Patel", "Jay", ""], ["Gidwani", "Mishka", ""], ["Adebayo", "Julius", ""], ["Li", "Matthew D.", ""], ["Kalpathy-Cramer", "Jayashree", ""]]}, {"id": "2008.02777", "submitter": "Bernhard Liebl", "authors": "Bernhard Liebl, Manuel Burghardt", "title": "On the Accuracy of CRNNs for Line-Based OCR: A Multi-Parameter\n  Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate how to train a high quality optical character recognition\n(OCR) model for difficult historical typefaces on degraded paper. Through\nextensive grid searches, we obtain a neural network architecture and a set of\noptimal data augmentation settings. We discuss the influence of factors such as\nbinarization, input line height, network width, network depth, and other\nnetwork training parameters such as dropout. Implementing these findings into a\npractical model, we are able to obtain a 0.44% character error rate (CER) model\nfrom only 10,000 lines of training data, outperforming currently available\npretrained models that were trained on more than 20 times the amount of data.\nWe show ablations for all components of our training pipeline, which relies on\nthe open source framework Calamari.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 17:20:56 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Liebl", "Bernhard", ""], ["Burghardt", "Manuel", ""]]}, {"id": "2008.02787", "submitter": "Ye Yuan", "authors": "Mariko Isogawa, Dorian Chan, Ye Yuan, Kris Kitani, Matthew O'Toole", "title": "Efficient Non-Line-of-Sight Imaging from Transient Sinograms", "comments": "ECCV 2020. Project page:\n  https://marikoisogawa.github.io/project/c2nlos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV eess.SP physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-line-of-sight (NLOS) imaging techniques use light that diffusely reflects\noff of visible surfaces (e.g., walls) to see around corners. One approach\ninvolves using pulsed lasers and ultrafast sensors to measure the travel time\nof multiply scattered light. Unlike existing NLOS techniques that generally\nrequire densely raster scanning points across the entirety of a relay wall, we\nexplore a more efficient form of NLOS scanning that reduces both acquisition\ntimes and computational requirements. We propose a circular and confocal\nnon-line-of-sight (C2NLOS) scan that involves illuminating and imaging a common\npoint, and scanning this point in a circular path along a wall. We observe that\n(1) these C2NLOS measurements consist of a superposition of sinusoids, which we\nrefer to as a transient sinogram, (2) there exists computationally efficient\nreconstruction procedures that transform these sinusoidal measurements into 3D\npositions of hidden scatterers or NLOS images of hidden objects, and (3)\ndespite operating on an order of magnitude fewer measurements than previous\napproaches, these C2NLOS scans provide sufficient information about the hidden\nscene to solve these different NLOS imaging tasks. We show results from both\nsimulated and real C2NLOS scans.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 17:50:50 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Isogawa", "Mariko", ""], ["Chan", "Dorian", ""], ["Yuan", "Ye", ""], ["Kitani", "Kris", ""], ["O'Toole", "Matthew", ""]]}, {"id": "2008.02792", "submitter": "Davis Rempe", "authors": "Davis Rempe, Tolga Birdal, Yongheng Zhao, Zan Gojcic, Srinath Sridhar,\n  Leonidas J. Guibas", "title": "CaSPR: Learning Canonical Spatiotemporal Point Cloud Representations", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose CaSPR, a method to learn object-centric Canonical Spatiotemporal\nPoint Cloud Representations of dynamically moving or evolving objects. Our goal\nis to enable information aggregation over time and the interrogation of object\nstate at any spatiotemporal neighborhood in the past, observed or not.\nDifferent from previous work, CaSPR learns representations that support\nspacetime continuity, are robust to variable and irregularly spacetime-sampled\npoint clouds, and generalize to unseen object instances. Our approach divides\nthe problem into two subtasks. First, we explicitly encode time by mapping an\ninput point cloud sequence to a spatiotemporally-canonicalized object space. We\nthen leverage this canonicalization to learn a spatiotemporal latent\nrepresentation using neural ordinary differential equations and a generative\nmodel of dynamically evolving shapes using continuous normalizing flows. We\ndemonstrate the effectiveness of our method on several applications including\nshape reconstruction, camera pose estimation, continuous spatiotemporal\nsequence reconstruction, and correspondence estimation from irregularly or\nintermittently sampled observations.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 17:58:48 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 19:00:31 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Rempe", "Davis", ""], ["Birdal", "Tolga", ""], ["Zhao", "Yongheng", ""], ["Gojcic", "Zan", ""], ["Sridhar", "Srinath", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "2008.02793", "submitter": "Arun Mallya", "authors": "Ming-Yu Liu, Xun Huang, Jiahui Yu, Ting-Chun Wang, Arun Mallya", "title": "Generative Adversarial Networks for Image and Video Synthesis:\n  Algorithms and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generative adversarial network (GAN) framework has emerged as a powerful\ntool for various image and video synthesis tasks, allowing the synthesis of\nvisual content in an unconditional or input-conditional manner. It has enabled\nthe generation of high-resolution photorealistic images and videos, a task that\nwas challenging or impossible with prior methods. It has also led to the\ncreation of many new applications in content creation. In this paper, we\nprovide an overview of GANs with a special focus on algorithms and applications\nfor visual synthesis. We cover several important techniques to stabilize GAN\ntraining, which has a reputation for being notoriously difficult. We also\ndiscuss its applications to image translation, image processing, video\nsynthesis, and neural rendering.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 17:59:04 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 18:18:55 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Liu", "Ming-Yu", ""], ["Huang", "Xun", ""], ["Yu", "Jiahui", ""], ["Wang", "Ting-Chun", ""], ["Mallya", "Arun", ""]]}, {"id": "2008.02796", "submitter": "Andrew Liu", "authors": "Andrew Liu, Shiry Ginosar, Tinghui Zhou, Alexei A. Efros, Noah Snavely", "title": "Learning to Factorize and Relight a City", "comments": "ECCV 2020 (Spotlight). Supplemental Material attached", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a learning-based framework for disentangling outdoor scenes into\ntemporally-varying illumination and permanent scene factors. Inspired by the\nclassic intrinsic image decomposition, our learning signal builds upon two\ninsights: 1) combining the disentangled factors should reconstruct the original\nimage, and 2) the permanent factors should stay constant across multiple\ntemporal samples of the same scene. To facilitate training, we assemble a\ncity-scale dataset of outdoor timelapse imagery from Google Street View, where\nthe same locations are captured repeatedly through time. This data represents\nan unprecedented scale of spatio-temporal outdoor imagery. We show that our\nlearned disentangled factors can be used to manipulate novel images in\nrealistic ways, such as changing lighting effects and scene geometry. Please\nvisit factorize-a-city.github.io for animated results.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 17:59:54 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Liu", "Andrew", ""], ["Ginosar", "Shiry", ""], ["Zhou", "Tinghui", ""], ["Efros", "Alexei A.", ""], ["Snavely", "Noah", ""]]}, {"id": "2008.02797", "submitter": "Sanjay Sahay", "authors": "Shriya TP Gupta and Sanjay K Sahay", "title": "A Novel Spatial-Spectral Framework for the Classification of\n  Hyperspectral Satellite Imagery", "comments": "13 Pages, 15 Figures, EANN-2020", "journal-ref": "Springer, INNS, Vol. 2, pp 227-239, 2020", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyper-spectral satellite imagery is now widely being used for accurate\ndisaster prediction and terrain feature classification. However, in such\nclassification tasks, most of the present approaches use only the spectral\ninformation contained in the images. Therefore, in this paper, we present a\nnovel framework that takes into account both the spectral and spatial\ninformation contained in the data for land cover classification. For this\npurpose, we use the Gaussian Maximum Likelihood (GML) and Convolutional Neural\nNetwork methods for the pixel-wise spectral classification and then, using\nsegmentation maps generated by the Watershed algorithm, we incorporate the\nspatial contextual information into our model with a modified majority vote\ntechnique. The experimental analyses on two benchmark datasets demonstrate that\nour proposed methodology performs better than the earlier approaches by\nachieving an accuracy of 99.52% and 98.31% on the Pavia University and the\nIndian Pines datasets respectively. Additionally, our GML based approach, a\nnon-deep learning algorithm, shows comparable performance to the\nstate-of-the-art deep learning techniques, which indicates the importance of\nthe proposed approach for performing a computationally efficient classification\nof hyper-spectral imagery.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 16:12:08 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Gupta", "Shriya TP", ""], ["Sahay", "Sanjay K", ""]]}, {"id": "2008.02834", "submitter": "Stephane Vujasinovic", "authors": "St\\'ephane Vujasinovi\\'c, Stefan Becker, Timo Breuer, Sebastian\n  Bullinger, Norbert Scherer-Negenborn, Michael Arens", "title": "Integration of the 3D Environment for UAV Onboard Visual Object Tracking", "comments": "Accepted in MDPI Journal of Applied Sciences", "journal-ref": "Vujasinovi\\'c, S.; Becker, S.; Breuer, T.; Bullinger, S.;\n  Scherer-Negenborn, N.; Arens, M. Integration of the 3D Environment for UAV\n  Onboard Visual Object Tracking. Appl. Sci. 2020, 10, 7622", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single visual object tracking from an unmanned aerial vehicle (UAV) poses\nfundamental challenges such as object occlusion, small-scale objects,\nbackground clutter, and abrupt camera motion. To tackle these difficulties, we\npropose to integrate the 3D structure of the observed scene into a\ndetection-by-tracking algorithm. We introduce a pipeline that combines a\nmodel-free visual object tracker, a sparse 3D reconstruction, and a state\nestimator. The 3D reconstruction of the scene is computed with an image-based\nStructure-from-Motion (SfM) component that enables us to leverage a state\nestimator in the corresponding 3D scene during tracking. By representing the\nposition of the target in 3D space rather than in image space, we stabilize the\ntracking during ego-motion and improve the handling of occlusions, background\nclutter, and small-scale objects. We evaluated our approach on prototypical\nimage sequences, captured from a UAV with low-altitude oblique views. For this\npurpose, we adapted an existing dataset for visual object tracking and\nreconstructed the observed scene in 3D. The experimental results demonstrate\nthat the proposed approach outperforms methods using plain visual cues as well\nas approaches leveraging image-space-based state estimations. We believe that\nour approach can be beneficial for traffic monitoring, video surveillance, and\nnavigation.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 18:37:29 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 07:53:13 GMT"}, {"version": "v3", "created": "Thu, 29 Oct 2020 10:20:21 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Vujasinovi\u0107", "St\u00e9phane", ""], ["Becker", "Stefan", ""], ["Breuer", "Timo", ""], ["Bullinger", "Sebastian", ""], ["Scherer-Negenborn", "Norbert", ""], ["Arens", "Michael", ""]]}, {"id": "2008.02859", "submitter": "Pengfei Guo", "authors": "Pengfei Guo, Puyang Wang, Rajeev Yasarla, Jinyuan Zhou, Vishal M.\n  Patel, and Shanshan Jiang", "title": "Confidence-guided Lesion Mask-based Simultaneous Synthesis of Anatomic\n  and Molecular MR Images in Patients with Post-treatment Malignant Gliomas", "comments": "Submit to IEEE TMI. arXiv admin note: text overlap with\n  arXiv:2006.14761", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven automatic approaches have demonstrated their great potential in\nresolving various clinical diagnostic dilemmas in neuro-oncology, especially\nwith the help of standard anatomic and advanced molecular MR images. However,\ndata quantity and quality remain a key determinant of, and a significant limit\non, the potential of such applications. In our previous work, we explored\nsynthesis of anatomic and molecular MR image network (SAMR) in patients with\npost-treatment malignant glioms. Now, we extend it and propose Confidence\nGuided SAMR (CG-SAMR) that synthesizes data from lesion information to\nmulti-modal anatomic sequences, including T1-weighted (T1w), gadolinium\nenhanced T1w (Gd-T1w), T2-weighted (T2w), and fluid-attenuated inversion\nrecovery (FLAIR), and the molecular amide proton transfer-weighted (APTw)\nsequence. We introduce a module which guides the synthesis based on confidence\nmeasure about the intermediate results. Furthermore, we extend the proposed\narchitecture for unsupervised synthesis so that unpaired data can be used for\ntraining the network. Extensive experiments on real clinical data demonstrate\nthat the proposed model can perform better than the state-of-theart synthesis\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 20:20:22 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Guo", "Pengfei", ""], ["Wang", "Puyang", ""], ["Yasarla", "Rajeev", ""], ["Zhou", "Jinyuan", ""], ["Patel", "Vishal M.", ""], ["Jiang", "Shanshan", ""]]}, {"id": "2008.02866", "submitter": "Faraz Hussain", "authors": "Edward Verenich, Alvaro Velasquez, Nazar Khan and Faraz Hussain", "title": "Improving Explainability of Image Classification in Scenarios with Class\n  Overlap: Application to COVID-19 and Pneumonia", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trust in predictions made by machine learning models is increased if the\nmodel generalizes well on previously unseen samples and when inference is\naccompanied by cogent explanations of the reasoning behind predictions. In the\nimage classification domain, generalization can be assessed through accuracy,\nsensitivity, and specificity. Explainability can be assessed by how well the\nmodel localizes the object of interest within an image. However, both\ngeneralization and explainability through localization are degraded in\nscenarios with significant overlap between classes. We propose a method based\non binary expert networks that enhances the explainability of image\nclassifications through better localization by mitigating the model uncertainty\ninduced by class overlap. Our technique performs discriminative localization on\nimages that contain features with significant class overlap, without explicitly\ntraining for localization. Our method is particularly promising in real-world\nclass overlap scenarios, such as COVID-19 and pneumonia, where expertly labeled\ndata for localization is not readily available. This can be useful for early,\nrapid, and trustworthy screening for COVID-19.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 20:47:36 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 21:20:02 GMT"}, {"version": "v3", "created": "Sun, 16 Aug 2020 01:33:25 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Verenich", "Edward", ""], ["Velasquez", "Alvaro", ""], ["Khan", "Nazar", ""], ["Hussain", "Faraz", ""]]}, {"id": "2008.02871", "submitter": "Yang Bai", "authors": "Yang Bai, Yu Guan, Wan-Fai Ng", "title": "Fatigue Assessment using ECG and Actigraphy Sensors", "comments": "accepted by ISWC 2020", "journal-ref": null, "doi": "10.1145/3410531.3414308", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fatigue is one of the key factors in the loss of work efficiency and\nhealth-related quality of life, and most fatigue assessment methods were based\non self-reporting, which may suffer from many factors such as recall bias. To\naddress this issue, we developed an automated system using wearable sensing and\nmachine learning techniques for objective fatigue assessment. ECG/Actigraphy\ndata were collected from subjects in free-living environments. Preprocessing\nand feature engineering methods were applied, before interpretable solution and\ndeep learning solution were introduced. Specifically, for interpretable\nsolution, we proposed a feature selection approach which can select less\ncorrelated and high informative features for better understanding system's\ndecision-making process. For deep learning solution, we used state-of-the-art\nself-attention model, based on which we further proposed a consistency\nself-attention (CSA) mechanism for fatigue assessment. Extensive experiments\nwere conducted, and very promising results were achieved.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 21:04:33 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 15:15:56 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Bai", "Yang", ""], ["Guan", "Yu", ""], ["Ng", "Wan-Fai", ""]]}, {"id": "2008.02880", "submitter": "Yannick Le Cacheux", "authors": "Yannick Le Cacheux, Adrian Popescu, Herv\\'e Le Borgne", "title": "Webly Supervised Semantic Embeddings for Large Scale Zero-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) makes object recognition in images possible in\nabsence of visual training data for a part of the classes from a dataset. When\nthe number of classes is large, classes are usually represented by semantic\nclass prototypes learned automatically from unannotated text collections. This\ntypically leads to much lower performances than with manually designed semantic\nprototypes such as attributes. While most ZSL works focus on the visual aspect\nand reuse standard semantic prototypes learned from generic text collections,\nwe focus on the problem of semantic class prototype design for large scale ZSL.\nMore specifically, we investigate the use of noisy textual metadata associated\nto photos as text collections, as we hypothesize they are likely to provide\nmore plausible semantic embeddings for visual classes if exploited\nappropriately. We thus make use of a source-based voting strategy to improve\nthe robustness of semantic prototypes. Evaluation on the large scale ImageNet\ndataset shows a significant improvement in ZSL performances over two strong\nbaselines, and over usual semantic embeddings used in previous works. We show\nthat this improvement is obtained for several embedding methods, leading to\nstate of the art results when one uses automatically created visual and text\nfeatures.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 21:33:44 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Cacheux", "Yannick Le", ""], ["Popescu", "Adrian", ""], ["Borgne", "Herv\u00e9 Le", ""]]}, {"id": "2008.02881", "submitter": "Jana Pavlasek", "authors": "Jana Pavlasek, Stanley Lewis, Karthik Desingh, Odest Chadwicke Jenkins", "title": "Parts-Based Articulated Object Localization in Clutter Using Belief\n  Propagation", "comments": "Accepted to the 2020 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS). Contact: Jana Pavlasek, pavlasek@umich.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots working in human environments must be able to perceive and act on\nchallenging objects with articulations, such as a pile of tools. Articulated\nobjects increase the dimensionality of the pose estimation problem, and partial\nobservations under clutter create additional challenges. To address this\nproblem, we present a generative-discriminative parts-based recognition and\nlocalization method for articulated objects in clutter. We formulate the\nproblem of articulated object pose estimation as a Markov Random Field (MRF).\nHidden nodes in this MRF express the pose of the object parts, and edges\nexpress the articulation constraints between parts. Localization is performed\nwithin the MRF using an efficient belief propagation method. The method is\ninformed by both part segmentation heatmaps over the observation, generated by\na neural network, and the articulation constraints between object parts. Our\ngenerative-discriminative approach allows the proposed method to function in\ncluttered environments by inferring the pose of occluded parts using hypotheses\nfrom the visible parts. We demonstrate the efficacy of our methods in a\ntabletop environment for recognizing and localizing hand tools in uncluttered\nand cluttered configurations.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 21:34:52 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Pavlasek", "Jana", ""], ["Lewis", "Stanley", ""], ["Desingh", "Karthik", ""], ["Jenkins", "Odest Chadwicke", ""]]}, {"id": "2008.02890", "submitter": "Mohammad-Parsa Hosseini", "authors": "Madison Beary, Alex Hadsell, Ryan Messersmith, Mohammad-Parsa Hosseini", "title": "Diagnosis of Autism in Children using Facial Analysis and Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a deep learning model to classify children as\neither healthy or potentially autistic with 94.6% accuracy using Deep Learning.\nAutistic patients struggle with social skills, repetitive behaviors, and\ncommunication, both verbal and nonverbal. Although the disease is considered to\nbe genetic, the highest rates of accurate diagnosis occur when the child is\ntested on behavioral characteristics and facial features. Patients have a\ncommon pattern of distinct facial deformities, allowing researchers to analyze\nonly an image of the child to determine if the child has the disease. While\nthere are other techniques and models used for facial analysis and autism\nclassification on their own, our proposal bridges these two ideas allowing\nclassification in a cheaper, more efficient method. Our deep learning model\nuses MobileNet and two dense layers in order to perform feature extraction and\nimage classification. The model is trained and tested using 3,014 images,\nevenly split between children with autism and children without it. 90% of the\ndata is used for training, and 10% is used for testing. Based on our accuracy,\nwe propose that the diagnosis of autism can be done effectively using only a\npicture. Additionally, there may be other diseases that are similarly\ndiagnosable.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 22:15:20 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Beary", "Madison", ""], ["Hadsell", "Alex", ""], ["Messersmith", "Ryan", ""], ["Hosseini", "Mohammad-Parsa", ""]]}, {"id": "2008.02912", "submitter": "Aaron Hertzmann", "authors": "Camilo Fosco, Vincent Casser, Amish Kumar Bedi, Peter O'Donovan, Aaron\n  Hertzmann, Zoya Bylinskii", "title": "Predicting Visual Importance Across Graphic Design Types", "comments": null, "journal-ref": "Proceedings of UIST 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a Unified Model of Saliency and Importance (UMSI),\nwhich learns to predict visual importance in input graphic designs, and\nsaliency in natural images, along with a new dataset and applications. Previous\nmethods for predicting saliency or visual importance are trained individually\non specialized datasets, making them limited in application and leading to poor\ngeneralization on novel image classes, while requiring a user to know which\nmodel to apply to which input. UMSI is a deep learning-based model\nsimultaneously trained on images from different design classes, including\nposters, infographics, mobile UIs, as well as natural images, and includes an\nautomatic classification module to classify the input. This allows the model to\nwork more effectively without requiring a user to label the input. We also\nintroduce Imp1k, a new dataset of designs annotated with importance\ninformation. We demonstrate two new design interfaces that use importance\nprediction, including a tool for adjusting the relative importance of design\nelements, and a tool for reflowing designs to new aspect ratios while\npreserving visual importance. The model, code, and importance dataset are\navailable at https://predimportance.mit.edu .\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 00:12:18 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Fosco", "Camilo", ""], ["Casser", "Vincent", ""], ["Bedi", "Amish Kumar", ""], ["O'Donovan", "Peter", ""], ["Hertzmann", "Aaron", ""], ["Bylinskii", "Zoya", ""]]}, {"id": "2008.02916", "submitter": "Bart Iver van Blokland", "authors": "Bart Iver van Blokland and Theoharis Theoharis", "title": "An Indexing Scheme and Descriptor for 3D Object Retrieval Based on Local\n  Shape Querying", "comments": "13 pages, 13 figures, to be published in a Special Issue in Computers\n  & Graphics", "journal-ref": "Computers & Graphics Volume 92, November 2020, Pages 55-66", "doi": "10.1016/j.cag.2020.09.001", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A binary descriptor indexing scheme based on Hamming distance called the\nHamming tree for local shape queries is presented. A new binary clutter\nresistant descriptor named Quick Intersection Count Change Image (QUICCI) is\nalso introduced. This local shape descriptor is extremely small and fast to\ncompare. Additionally, a novel distance function called Weighted Hamming\napplicable to QUICCI images is proposed for retrieval applications. The\neffectiveness of the indexing scheme and QUICCI is demonstrated on 828 million\nQUICCI images derived from the SHREC2017 dataset, while the clutter resistance\nof QUICCI is shown using the clutterbox experiment.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 00:46:58 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["van Blokland", "Bart Iver", ""], ["Theoharis", "Theoharis", ""]]}, {"id": "2008.02918", "submitter": "Changxing Ding", "authors": "Xubin Zhong, Changxing Ding, Xian Qu, Dacheng Tao", "title": "Polysemy Deciphering Network for Robust Human-Object Interaction\n  Detection", "comments": "The IJCV version extended significantly from our ECCV2020 conference\n  paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-Object Interaction (HOI) detection is important to human-centric scene\nunderstanding tasks. Existing works tend to assume that the same verb has\nsimilar visual characteristics in different HOI categories, an approach that\nignores the diverse semantic meanings of the verb. To address this issue, in\nthis paper, we propose a novel Polysemy Deciphering Network (PD-Net) that\ndecodes the visual polysemy of verbs for HOI detection in three distinct ways.\nFirst, we refine features for HOI detection to be polysemyaware through the use\nof two novel modules: namely, Language Prior-guided Channel Attention (LPCA)\nand Language Prior-based Feature Augmentation (LPFA). LPCA highlights important\nelements in human and object appearance features for each HOI category to be\nidentified; moreover, LPFA augments human pose and spatial features for HOI\ndetection using language priors, enabling the verb classifiers to receive\nlanguage hints that reduce intra-class variation for the same verb. Second, we\nintroduce a novel Polysemy-Aware Modal Fusion module (PAMF), which guides\nPD-Net to make decisions based on feature types deemed more important according\nto the language priors. Third, we propose to relieve the verb polysemy problem\nthrough sharing verb classifiers for semantically similar HOI categories.\nFurthermore, to expedite research on the verb polysemy problem, we build a new\nbenchmark dataset named HOI-VerbPolysemy (HOIVP), which includes common verbs\n(predicates) that have diverse semantic meanings in the real world. Finally,\nthrough deciphering the visual polysemy of verbs, our approach is demonstrated\nto outperform state-of-the-art methods by significant margins on the HICO-DET,\nV-COCO, and HOI-VP databases. Code and data in this paper are available at\nhttps://github.com/MuchHair/PD-Net.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 00:49:27 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 11:56:07 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 01:13:06 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Zhong", "Xubin", ""], ["Ding", "Changxing", ""], ["Qu", "Xian", ""], ["Tao", "Dacheng", ""]]}, {"id": "2008.02938", "submitter": "Chenglizhao Chen", "authors": "Zhenyu Wu, Shuai Li, Chenglizhao Chen, Aimin Hao, Hong Qin", "title": "A Deeper Look at Salient Object Detection: Bi-stream Network with a\n  Small Training Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with the conventional hand-crafted approaches, the deep learning\nbased methods have achieved tremendous performance improvements by training\nexquisitely crafted fancy networks over large-scale training sets. However, do\nwe really need large-scale training set for salient object detection (SOD)? In\nthis paper, we provide a deeper insight into the interrelationship between the\nSOD performances and the training sets. To alleviate the conventional demands\nfor large-scale training data, we provide a feasible way to construct a novel\nsmall-scale training set, which only contains 4K images. Moreover, we propose a\nnovel bi-stream network to take full advantage of our proposed small training\nset, which is consisted of two feature backbones with different structures,\nachieving complementary semantical saliency fusion via the proposed gate\ncontrol unit. To our best knowledge, this is the first attempt to use a\nsmall-scale training set to outperform state-of-the-art models which are\ntrained on large-scale training sets; nevertheless, our method can still\nachieve the leading state-of-the-art performance on five benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 01:24:33 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Wu", "Zhenyu", ""], ["Li", "Shuai", ""], ["Chen", "Chenglizhao", ""], ["Hao", "Aimin", ""], ["Qin", "Hong", ""]]}, {"id": "2008.02952", "submitter": "Sohini Roychowdhury", "authors": "Sohini Roychowdhury", "title": "Few Shot Learning Framework to Reduce Inter-observer Variability in\n  Medical Images", "comments": "8 pages, 8 figures, 4 tables", "journal-ref": "ICPR 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most computer aided pathology detection systems rely on large volumes of\nquality annotated data to aid diagnostics and follow up procedures. However,\nquality assuring large volumes of annotated medical image data can be\nsubjective and expensive. In this work we present a novel standardization\nframework that implements three few-shot learning (FSL) models that can be\niteratively trained by atmost 5 images per 3D stack to generate multiple\nregional proposals (RPs) per test image. These FSL models include a novel\nparallel echo state network (ParESN) framework and an augmented U-net model.\nAdditionally, we propose a novel target label selection algorithm (TLSA) that\nmeasures relative agreeability between RPs and the manually annotated target\nlabels to detect the \"best\" quality annotation per image. Using the FSL models,\nour system achieves 0.28-0.64 Dice coefficient across vendor image stacks for\nintra-retinal cyst segmentation. Additionally, the TLSA is capable of\nautomatically classifying high quality target labels from their noisy\ncounterparts for 60-97% of the images while ensuring manual supervision on\nremaining images. Also, the proposed framework with ParESN model minimizes\nmanual annotation checking to 12-28% of the total number of images. The TLSA\nmetrics further provide confidence scores for the automated annotation quality\nassurance. Thus, the proposed framework is flexible to extensions for quality\nimage annotation curation of other image stacks as well.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 02:05:51 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Roychowdhury", "Sohini", ""]]}, {"id": "2008.02966", "submitter": "Chenglizhao Chen", "authors": "Chenglizhao Chen, Jia Song, Chong Peng, Guodong Wang, Yuming Fang", "title": "A Novel Video Salient Object Detection Method via Semi-supervised Motion\n  Quality Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous video salient object detection (VSOD) approaches have mainly focused\non designing fancy networks to achieve their performance improvements. However,\nwith the slow-down in development of deep learning techniques recently, it may\nbecome more and more difficult to anticipate another breakthrough via fancy\nnetworks solely. To this end, this paper proposes a universal learning scheme\nto get a further 3\\% performance improvement for all state-of-the-art (SOTA)\nmethods. The major highlight of our method is that we resort the \"motion\nquality\"---a brand new concept, to select a sub-group of video frames from the\noriginal testing set to construct a new training set. The selected frames in\nthis new training set should all contain high-quality motions, in which the\nsalient objects will have large probability to be successfully detected by the\n\"target SOTA method\"---the one we want to improve. Consequently, we can achieve\na significant performance improvement by using this new training set to start a\nnew round of network training. During this new round training, the VSOD results\nof the target SOTA method will be applied as the pseudo training objectives.\nOur novel learning scheme is simple yet effective, and its semi-supervised\nmethodology may have large potential to inspire the VSOD community in the\nfuture.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 02:58:51 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Chen", "Chenglizhao", ""], ["Song", "Jia", ""], ["Peng", "Chong", ""], ["Wang", "Guodong", ""], ["Fang", "Yuming", ""]]}, {"id": "2008.02973", "submitter": "Chenglizhao Chen", "authors": "Chenglizhao Chen, Guotao Wang, Chong Peng, Dingwen Zhang, Yuming Fang,\n  and Hong Qin", "title": "Exploring Rich and Efficient Spatial Temporal Interactions for Real Time\n  Video Salient Object Detection", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2021.3068644", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current main stream methods formulate their video saliency mainly from\ntwo independent venues, i.e., the spatial and temporal branches. As a\ncomplementary component, the main task for the temporal branch is to\nintermittently focus the spatial branch on those regions with salient\nmovements. In this way, even though the overall video saliency quality is\nheavily dependent on its spatial branch, however, the performance of the\ntemporal branch still matter. Thus, the key factor to improve the overall video\nsaliency is how to further boost the performance of these branches efficiently.\nIn this paper, we propose a novel spatiotemporal network to achieve such\nimprovement in a full interactive fashion. We integrate a lightweight temporal\nmodel into the spatial branch to coarsely locate those spatially salient\nregions which are correlated with trustworthy salient movements. Meanwhile, the\nspatial branch itself is able to recurrently refine the temporal model in a\nmulti-scale manner. In this way, both the spatial and temporal branches are\nable to interact with each other, achieving the mutual performance improvement.\nOur method is easy to implement yet effective, achieving high quality video\nsaliency detection in real-time speed with 50 FPS.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 03:24:04 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Chen", "Chenglizhao", ""], ["Wang", "Guotao", ""], ["Peng", "Chong", ""], ["Zhang", "Dingwen", ""], ["Fang", "Yuming", ""], ["Qin", "Hong", ""]]}, {"id": "2008.02980", "submitter": "Ajoy Mondal Dr.", "authors": "Ajoy Mondal and C. V. Jawahar", "title": "Textual Description for Mathematical Equations", "comments": "8", "journal-ref": "Published in ICDAR 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reading of mathematical expression or equation in the document images is very\nchallenging due to the large variability of mathematical symbols and\nexpressions. In this paper, we pose reading of mathematical equation as a task\nof generation of the textual description which interprets the internal meaning\nof this equation. Inspired by the natural image captioning problem in computer\nvision, we present a mathematical equation description (MED) model, a novel\nend-to-end trainable deep neural network based approach that learns to generate\na textual description for reading mathematical equation images. Our MED model\nconsists of a convolution neural network as an encoder that extracts features\nof input mathematical equation images and a recurrent neural network with\nattention mechanism which generates description related to the input\nmathematical equation images. Due to the unavailability of mathematical\nequation image data sets with their textual descriptions, we generate two data\nsets for experimental purpose. To validate the effectiveness of our MED model,\nwe conduct a real-world experiment to see whether the students are able to\nwrite equations by only reading or listening their textual descriptions or not.\nExperiments conclude that the students are able to write most of the equations\ncorrectly by reading their textual descriptions only.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 03:46:32 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Mondal", "Ajoy", ""], ["Jawahar", "C. V.", ""]]}, {"id": "2008.02984", "submitter": "Chongyi Li", "authors": "Chongyi Li, Huazhu Fu, Runmin Cong, Zechao Li, Qianqian Xu", "title": "NuI-Go: Recursive Non-Local Encoder-Decoder Network for Retinal Image\n  Non-Uniform Illumination Removal", "comments": "ACM MM2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinal images have been widely used by clinicians for early diagnosis of\nocular diseases. However, the quality of retinal images is often clinically\nunsatisfactory due to eye lesions and imperfect imaging process. One of the\nmost challenging quality degradation issues in retinal images is non-uniform\nwhich hinders the pathological information and further impairs the diagnosis of\nophthalmologists and computer-aided analysis.To address this issue, we propose\na non-uniform illumination removal network for retinal image, called NuI-Go,\nwhich consists of three Recursive Non-local Encoder-Decoder Residual Blocks\n(NEDRBs) for enhancing the degraded retinal images in a progressive manner.\nEach NEDRB contains a feature encoder module that captures the hierarchical\nfeature representations, a non-local context module that models the context\ninformation, and a feature decoder module that recovers the details and spatial\ndimension. Additionally, the symmetric skip-connections between the encoder\nmodule and the decoder module provide long-range information compensation and\nreuse. Extensive experiments demonstrate that the proposed method can\neffectively remove the non-uniform illumination on retinal images while well\npreserving the image details and color. We further demonstrate the advantages\nof the proposed method for improving the accuracy of retinal vessel\nsegmentation.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 04:31:33 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Li", "Chongyi", ""], ["Fu", "Huazhu", ""], ["Cong", "Runmin", ""], ["Li", "Zechao", ""], ["Xu", "Qianqian", ""]]}, {"id": "2008.02986", "submitter": "Binh-Son Hua", "authors": "Zhiyuan Zhang, Binh-Son Hua, Wei Chen, Yibin Tian, Sai-Kit Yeung", "title": "Global Context Aware Convolutions for 3D Point Cloud Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning for 3D point clouds have shown great\npromises in scene understanding tasks thanks to the introduction of convolution\noperators to consume 3D point clouds directly in a neural network. Point cloud\ndata, however, could have arbitrary rotations, especially those acquired from\n3D scanning. Recent works show that it is possible to design point cloud\nconvolutions with rotation invariance property, but such methods generally do\nnot perform as well as translation-invariant only convolution. We found that a\nkey reason is that compared to point coordinates, rotation-invariant features\nconsumed by point cloud convolution are not as distinctive. To address this\nproblem, we propose a novel convolution operator that enhances feature\ndistinction by integrating global context information from the input point\ncloud to the convolution. To this end, a globally weighted local reference\nframe is constructed in each point neighborhood in which the local point set is\ndecomposed into bins. Anchor points are generated in each bin to represent\nglobal shape features. A convolution can then be performed to transform the\npoints and anchor features into final rotation-invariant features. We conduct\nseveral experiments on point cloud classification, part segmentation, shape\nretrieval, and normals estimation to evaluate our convolution, which achieves\nstate-of-the-art accuracy under challenging rotations.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 04:33:27 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Zhang", "Zhiyuan", ""], ["Hua", "Binh-Son", ""], ["Chen", "Wei", ""], ["Tian", "Yibin", ""], ["Yeung", "Sai-Kit", ""]]}, {"id": "2008.02992", "submitter": "Zhongang Cai", "authors": "Zhongang Cai, Cunjun Yu, Junzhe Zhang, Jiawei Ren, Haiyu Zhao", "title": "Leveraging Localization for Multi-camera Association", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present McAssoc, a deep learning approach to the as-sociation of detection\nbounding boxes in different views ofa multi-camera system. The vast majority of\nthe academiahas been developing single-camera computer vision algo-rithms,\nhowever, little research attention has been directedto incorporating them into\na multi-camera system. In thispaper, we designed a 3-branch architecture that\nleveragesdirect association and additional cross localization infor-mation. A\nnew metric, image-pair association accuracy(IPAA) is designed specifically for\nperformance evaluationof cross-camera detection association. We show in the\nex-periments that localization information is critical to suc-cessful\ncross-camera association, especially when similar-looking objects are present.\nThis paper is an experimentalwork prior to MessyTable, which is a large-scale\nbench-mark for instance association in mutliple cameras.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 05:16:27 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Cai", "Zhongang", ""], ["Yu", "Cunjun", ""], ["Zhang", "Junzhe", ""], ["Ren", "Jiawei", ""], ["Zhao", "Haiyu", ""]]}, {"id": "2008.02999", "submitter": "Philipp V. Rouast", "authors": "Philipp V. Rouast and Marc T. P. Adam", "title": "Single-stage intake gesture detection using CTC loss and extended prefix\n  beam search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate detection of individual intake gestures is a key step towards\nautomatic dietary monitoring. Both inertial sensor data of wrist movements and\nvideo data depicting the upper body have been used for this purpose. The most\nadvanced approaches to date use a two-stage approach, in which (i) frame-level\nintake probabilities are learned from the sensor data using a deep neural\nnetwork, and then (ii) sparse intake events are detected by finding the maxima\nof the frame-level probabilities. In this study, we propose a single-stage\napproach which directly decodes the probabilities learned from sensor data into\nsparse intake detections. This is achieved by weakly supervised training using\nConnectionist Temporal Classification (CTC) loss, and decoding using a novel\nextended prefix beam search decoding algorithm. Benefits of this approach\ninclude (i) end-to-end training for detections, (ii) simplified timing\nrequirements for intake gesture labels, and (iii) improved detection\nperformance compared to existing approaches. Across two separate datasets, we\nachieve relative $F_1$ score improvements between 1.9% and 6.2% over the\ntwo-stage approach for intake detection and eating/drinking detection tasks,\nfor both video and inertial sensors.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 06:04:25 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 01:05:45 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Rouast", "Philipp V.", ""], ["Adam", "Marc T. P.", ""]]}, {"id": "2008.03008", "submitter": "Bayu Adhi Nugroho", "authors": "Bayu A. Nugroho", "title": "An Aggregate Method for Thorax Diseases Classification", "comments": "code available: https://github.com/bayu-ladom-ipok/weOpen", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common problem found in real-word medical image classification is the\ninherent imbalance of the positive and negative patterns in the dataset where\npositive patterns are usually rare. Moreover, in the classification of multiple\nclasses with neural network, a training pattern is treated as a positive\npattern in one output node and negative in all the remaining output nodes. In\nthis paper, the weights of a training pattern in the loss function are designed\nbased not only on the number of the training patterns in the class but also on\nthe different nodes where one of them treats this training pattern as positive\nand the others treat it as negative. We propose a combined approach of weights\ncalculation algorithm for deep network training and the training optimization\nfrom the state-of-the-art deep network architecture for thorax diseases\nclassification problem. Experimental results on the Chest X-Ray image dataset\ndemonstrate that this new weighting scheme improves classification\nperformances, also the training optimization from the EfficientNet improves the\nperformance furthermore. We compare the aggregate method with several\nperformances from the previous study of thorax diseases classifications to\nprovide the fair comparisons against the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 06:36:07 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 03:12:57 GMT"}, {"version": "v3", "created": "Wed, 12 Aug 2020 03:15:53 GMT"}, {"version": "v4", "created": "Thu, 12 Nov 2020 07:15:33 GMT"}, {"version": "v5", "created": "Thu, 24 Dec 2020 12:35:14 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Nugroho", "Bayu A.", ""]]}, {"id": "2008.03014", "submitter": "Behnoosh Parsa", "authors": "Behnoosh Parsa, Ashis G. Banerjee", "title": "A Multi-Task Learning Approach for Human Activity Segmentation and\n  Ergonomics Risk Assessment", "comments": "To appear at the 2021 Winter Conference on Applications of Computer\n  Vision (WACV'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to Human Activity Evaluation (HAE) in long videos\nusing graph-based multi-task modeling. Previous works in activity evaluation\neither directly compute a metric using a detected skeleton or use the scene\ninformation to regress the activity score. These approaches are insufficient\nfor accurate activity assessment since they only compute an average score over\na clip, and do not consider the correlation between the joints and body\ndynamics. Moreover, they are highly scene-dependent which makes the\ngeneralizability of these methods questionable. We propose a novel multi-task\nframework for HAE that utilizes a Graph Convolutional Network backbone to embed\nthe interconnections between human joints in the features. In this framework,\nwe solve the Human Activity Segmentation (HAS) problem as an auxiliary task to\nimprove activity assessment. The HAS head is powered by an Encoder-Decoder\nTemporal Convolutional Network to semantically segment long videos into\ndistinct activity classes, whereas, HAE uses a Long-Short-Term-Memory-based\narchitecture. We evaluate our method on the UW-IOM and TUM Kitchen datasets and\ndiscuss the success and failure cases in these two datasets.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 06:53:56 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 00:03:53 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Parsa", "Behnoosh", ""], ["Banerjee", "Ashis G.", ""]]}, {"id": "2008.03030", "submitter": "Huasong Zhong", "authors": "Huasong Zhong, Chong Chen, Zhongming Jin, Xian-Sheng Hua", "title": "Deep Robust Clustering by Contrastive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many unsupervised deep learning methods have been proposed to learn\nclustering with unlabelled data. By introducing data augmentation, most of the\nlatest methods look into deep clustering from the perspective that the original\nimage and its transformation should share similar semantic clustering\nassignment. However, the representation features could be quite different even\nthey are assigned to the same cluster since softmax function is only sensitive\nto the maximum value. This may result in high intra-class diversities in the\nrepresentation feature space, which will lead to unstable local optimal and\nthus harm the clustering performance. To address this drawback, we proposed\nDeep Robust Clustering (DRC). Different from existing methods, DRC looks into\ndeep clustering from two perspectives of both semantic clustering assignment\nand representation feature, which can increase inter-class diversities and\ndecrease intra-class diversities simultaneously. Furthermore, we summarized a\ngeneral framework that can turn any maximizing mutual information into\nminimizing contrastive loss by investigating the internal relationship between\nmutual information and contrastive learning. And we successfully applied it in\nDRC to learn invariant features and robust clusters. Extensive experiments on\nsix widely-adopted deep clustering benchmarks demonstrate the superiority of\nDRC in both stability and accuracy. e.g., attaining 71.6% mean accuracy on\nCIFAR-10, which is 7.1% higher than state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 08:05:53 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 05:42:48 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Zhong", "Huasong", ""], ["Chen", "Chong", ""], ["Jin", "Zhongming", ""], ["Hua", "Xian-Sheng", ""]]}, {"id": "2008.03043", "submitter": "Kailai Zhou", "authors": "Kailai Zhou, Linsen Chen, Xun Cao", "title": "Improving Multispectral Pedestrian Detection by Addressing Modality\n  Imbalance Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multispectral pedestrian detection is capable of adapting to insufficient\nillumination conditions by leveraging color-thermal modalities. On the other\nhand, it is still lacking of in-depth insights on how to fuse the two\nmodalities effectively. Compared with traditional pedestrian detection, we find\nmultispectral pedestrian detection suffers from modality imbalance problems\nwhich will hinder the optimization process of dual-modality network and depress\nthe performance of detector. Inspired by this observation, we propose Modality\nBalance Network (MBNet) which facilitates the optimization process in a much\nmore flexible and balanced manner. Firstly, we design a novel Differential\nModality Aware Fusion (DMAF) module to make the two modalities complement each\nother. Secondly, an illumination aware feature alignment module selects\ncomplementary features according to the illumination conditions and aligns the\ntwo modality features adaptively. Extensive experimental results demonstrate\nMBNet outperforms the state-of-the-arts on both the challenging KAIST and\nCVC-14 multispectral pedestrian datasets in terms of the accuracy and the\ncomputational efficiency. Code is available at\nhttps://github.com/CalayZhou/MBNet.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 08:58:46 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 02:21:34 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Zhou", "Kailai", ""], ["Chen", "Linsen", ""], ["Cao", "Xun", ""]]}, {"id": "2008.03064", "submitter": "Xuefei Ning", "authors": "Xuefei Ning, Changcheng Tang, Wenshuo Li, Zixuan Zhou, Shuang Liang,\n  Huazhong Yang, Yu Wang", "title": "Evaluating Efficient Performance Estimators of Neural Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conducting efficient performance estimations of neural architectures is a\nmajor challenge in neural architecture search (NAS). To reduce the architecture\ntraining costs in NAS, one-shot estimators (OSEs) amortize the architecture\ntraining costs by sharing the parameters of one supernet between all\narchitectures. Recently, zero-shot estimators (ZSEs) that involve no training\nare proposed to further reduce the architecture evaluation cost. Despite the\nhigh efficiency of these estimators, the quality of such estimations has not\nbeen thoroughly studied. In this paper, we conduct an extensive and organized\nassessment of OSEs and ZSEs on three NAS benchmarks: NAS-Bench-101/201/301.\nSpecifically, we employ a set of NAS-oriented criteria to study the behavior of\nOSEs and ZSEs and reveal that they have certain biases and variances. After\nanalyzing how and why the OSE estimations are unsatisfying, we explore how to\nmitigate the correlation gap of OSEs from several perspectives. For ZSEs, we\nfind that current ZSEs are not satisfying enough in these benchmark search\nspaces, and analyze their biases. Through our analysis, we give out suggestions\nfor future application and development of efficient architecture performance\nestimators. Furthermore, the analysis framework proposed in our work could be\nutilized in future research to give a more comprehensive understanding of newly\ndesigned architecture performance estimators. All codes and analysis scripts\nare available at https://github.com/walkerning/aw_nas.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 09:56:54 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 13:50:29 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 05:32:13 GMT"}, {"version": "v4", "created": "Wed, 28 Jul 2021 08:38:48 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Ning", "Xuefei", ""], ["Tang", "Changcheng", ""], ["Li", "Wenshuo", ""], ["Zhou", "Zixuan", ""], ["Liang", "Shuang", ""], ["Yang", "Huazhong", ""], ["Wang", "Yu", ""]]}, {"id": "2008.03071", "submitter": "Pourya Shamsolmoali", "authors": "Masoumeh Zareapoor, Pourya Shamsolmoali, Jie Yang", "title": "Oversampling Adversarial Network for Class-Imbalanced Fault Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The collected data from industrial machines are often imbalanced, which poses\na negative effect on learning algorithms. However, this problem becomes more\nchallenging for a mixed type of data or while there is overlapping between\nclasses. Class-imbalance problem requires a robust learning system which can\ntimely predict and classify the data. We propose a new adversarial network for\nsimultaneous classification and fault detection. In particular, we restore the\nbalance in the imbalanced dataset by generating faulty samples from the\nproposed mixture of data distribution. We designed the discriminator of our\nmodel to handle the generated faulty samples to prevent outlier and\noverfitting. We empirically demonstrate that; (i) the discriminator trained\nwith a generator to generates samples from a mixture of normal and faulty data\ndistribution which can be considered as a fault detector; (ii), the quality of\nthe generated faulty samples outperforms the other synthetic resampling\ntechniques. Experimental results show that the proposed model performs well\nwhen comparing to other fault diagnosis methods across several evaluation\nmetrics; in particular, coalescing of generative adversarial network (GAN) and\nfeature matching function is effective at recognizing faulty samples.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 10:12:07 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Zareapoor", "Masoumeh", ""], ["Shamsolmoali", "Pourya", ""], ["Yang", "Jie", ""]]}, {"id": "2008.03077", "submitter": "Hongming Shan", "authors": "Haiping Zhu, Hongming Shan, Yuheng Zhang, Lingfu Che, Xiaoyang Xu,\n  Junping Zhang, Jianbo Shi, Fei-Yue Wang", "title": "Convolutional Ordinal Regression Forest for Image Ordinal Estimation", "comments": "Accepted by IEEE TNNLS", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, 2021", "doi": "10.1109/TNNLS.2021.3055816", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image ordinal estimation is to predict the ordinal label of a given image,\nwhich can be categorized as an ordinal regression problem. Recent methods\nformulate an ordinal regression problem as a series of binary classification\nproblems. Such methods cannot ensure that the global ordinal relationship is\npreserved since the relationships among different binary classifiers are\nneglected. We propose a novel ordinal regression approach, termed Convolutional\nOrdinal Regression Forest or CORF, for image ordinal estimation, which can\nintegrate ordinal regression and differentiable decision trees with a\nconvolutional neural network for obtaining precise and stable global ordinal\nrelationships. The advantages of the proposed CORF are twofold. First, instead\nof learning a series of binary classifiers \\emph{independently}, the proposed\nmethod aims at learning an ordinal distribution for ordinal regression by\noptimizing those binary classifiers \\emph{simultaneously}. Second, the\ndifferentiable decision trees in the proposed CORF can be trained together with\nthe ordinal distribution in an end-to-end manner. The effectiveness of the\nproposed CORF is verified on two image ordinal estimation tasks, i.e. facial\nage estimation and image aesthetic assessment, showing significant improvements\nand better stability over the state-of-the-art ordinal regression methods.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 10:41:17 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 05:20:02 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Zhu", "Haiping", ""], ["Shan", "Hongming", ""], ["Zhang", "Yuheng", ""], ["Che", "Lingfu", ""], ["Xu", "Xiaoyang", ""], ["Zhang", "Junping", ""], ["Shi", "Jianbo", ""], ["Wang", "Fei-Yue", ""]]}, {"id": "2008.03085", "submitter": "Aritra Banerjee", "authors": "Aritra Banerjee", "title": "SimPatch: A Nearest Neighbor Similarity Match between Image Patches", "comments": "12 pages, 13 figures, Submitted to International Journal of\n  Artificial Intelligence and Soft Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the similarity between patches in images is a fundamental building\nblock in various tasks. Naturally, the patch-size has a major impact on the\nmatching quality, and on the consequent application performance. We try to use\nlarge patches instead of relatively small patches so that each patch contains\nmore information. We use different feature extraction mechanisms to extract the\nfeatures of each individual image patches which forms a feature matrix and find\nout the nearest neighbor patches in the image. The nearest patches are\ncalculated using two different nearest neighbor algorithms in this paper for a\nquery patch for a given image and the results have been demonstrated in this\npaper.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 10:51:10 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Banerjee", "Aritra", ""]]}, {"id": "2008.03087", "submitter": "Ao Luo", "authors": "Ao Luo, Xin Li, Fan Yang, Zhicheng Jiao, Hong Cheng and Siwei Lyu", "title": "Cascade Graph Neural Networks for RGB-D Salient Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of salient object detection (SOD) for\nRGB-D images using both color and depth information.A major technical challenge\nin performing salient object detection fromRGB-D images is how to fully\nleverage the two complementary data sources. Current works either simply\ndistill prior knowledge from the corresponding depth map for handling the\nRGB-image or blindly fuse color and geometric information to generate the\ncoarse depth-aware representations, hindering the performance of RGB-D saliency\ndetectors.In this work, we introduceCascade Graph Neural Networks(Cas-Gnn),a\nunified framework which is capable of comprehensively distilling and reasoning\nthe mutual benefits between these two data sources through a set of cascade\ngraphs, to learn powerful representations for RGB-D salient object detection.\nCas-Gnn processes the two data sources individually and employs a novelCascade\nGraph Reasoning(CGR) module to learn powerful dense feature embeddings, from\nwhich the saliency map can be easily inferred. Contrast to the previous\napproaches, the explicitly modeling and reasoning of high-level relations\nbetween complementary data sources allows us to better overcome challenges such\nas occlusions and ambiguities. Extensive experiments demonstrate that Cas-Gnn\nachieves significantly better performance than all existing RGB-DSOD approaches\non several widely-used benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 10:59:04 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Luo", "Ao", ""], ["Li", "Xin", ""], ["Yang", "Fan", ""], ["Jiao", "Zhicheng", ""], ["Cheng", "Hong", ""], ["Lyu", "Siwei", ""]]}, {"id": "2008.03111", "submitter": "Sungeun Hong", "authors": "Youngeun Kim, Sungeun Hong, Seunghan Yang, Sungil Kang, Yunho Jeon,\n  Jiwon Kim", "title": "Associative Partial Domain Adaptation", "comments": "8 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial Adaptation (PDA) addresses a practical scenario in which the target\ndomain contains only a subset of classes in the source domain. While PDA should\ntake into account both class-level and sample-level to mitigate negative\ntransfer, current approaches mostly rely on only one of them. In this paper, we\npropose a novel approach to fully exploit multi-level associations that can\narise in PDA. Our Associative Partial Domain Adaptation (APDA) utilizes\nintra-domain association to actively select out non-trivial anomaly samples in\neach source-private class that sample-level weighting cannot handle.\nAdditionally, our method considers inter-domain association to encourage\npositive transfer by mapping between nearby target samples and source samples\nwith high label-commonness. For this, we exploit feature propagation in a\nproposed label space consisting of source ground-truth labels and target\nprobabilistic labels. We further propose a geometric guidance loss based on the\nlabel commonness of each source class to encourage positive transfer. Our APDA\nconsistently achieves state-of-the-art performance across public datasets.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 12:15:38 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Kim", "Youngeun", ""], ["Hong", "Sungeun", ""], ["Yang", "Seunghan", ""], ["Kang", "Sungil", ""], ["Jeon", "Yunho", ""], ["Kim", "Jiwon", ""]]}, {"id": "2008.03128", "submitter": "Yixiong Zou", "authors": "Yixiong Zou, Shanghang Zhang, JianPeng Yu, Yonghong Tian, Jos\\'e M. F.\n  Moura", "title": "Revisiting Mid-Level Patterns for Cross-Domain Few-Shot Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing few-shot learning (FSL) methods usually assume base classes and\nnovel classes are from the same domain (in-domain setting). However, in\npractice, it may be infeasible to collect sufficient training samples for some\nspecial domains to construct base classes. To solve this problem, cross-domain\nFSL (CDFSL) is proposed very recently to transfer knowledge from general-domain\nbase classes to special-domain novel classes. Existing CDFSL works mostly focus\non transferring between near domains, while rarely consider transferring\nbetween distant domains, which is in practical need as any novel classes could\nappear in real-world applications, and is even more challenging. In this paper,\nwe study a challenging subset of CDFSL where the novel classes are in distant\ndomains from base classes, by revisiting the mid-level features, which are more\ntransferable yet under-explored in main stream FSL work. To boost the\ndiscriminability of mid-level features, we propose a residual-prediction task\nto encourage mid-level features to learn discriminative information of each\nsample. Notably, such mechanism also benefits the in-domain FSL and CDFSL in\nnear domains. Therefore, we provide two types of features for both cross- and\nin-domain FSL respectively, under the same training framework. Experiments\nunder both settings on six public datasets, including two challenging medical\ndatasets, validate the our rationale and demonstrate state-of-the-art\nperformance. Code will be released.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 12:45:39 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 23:54:01 GMT"}, {"version": "v3", "created": "Tue, 20 Apr 2021 09:03:52 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Zou", "Yixiong", ""], ["Zhang", "Shanghang", ""], ["Yu", "JianPeng", ""], ["Tian", "Yonghong", ""], ["Moura", "Jos\u00e9 M. F.", ""]]}, {"id": "2008.03143", "submitter": "Hiroki Ito", "authors": "Hiroki Ito, Yuma Kinoshita, Hitoshi Kiya", "title": "Image Transformation Network for Privacy-Preserving Deep Neural Networks\n  and Its Security Evaluation", "comments": "To appear in 2020 IEEE 9th Global Conference on Consumer Electronics\n  (GCCE 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a transformation network for generating visually-protected images\nfor privacy-preserving DNNs. The proposed transformation network is trained by\nusing a plain image dataset so that plain images are transformed into visually\nprotected ones. Conventional perceptual encryption methods have a weak\nvisual-protection performance and some accuracy degradation in image\nclassification. In contrast, the proposed network enables us not only to\nstrongly protect visual information but also to maintain the image\nclassification accuracy that using plain images achieves. In an image\nclassification experiment, the proposed network is demonstrated to strongly\nprotect visual information on plain images without any performance degradation\nunder the use of CIFAR datasets. In addition, it is shown that the visually\nprotected images are robust against a DNN-based attack, called inverse\ntransformation network attack (ITN-Attack) in an experiment.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 12:58:45 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Ito", "Hiroki", ""], ["Kinoshita", "Yuma", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "2008.03154", "submitter": "Ho Law Mr.", "authors": "Ho Law, Lok Ming Lui, Chun Yin Siu", "title": "Decomposition of Longitudinal Deformations via Beltrami Descriptors", "comments": "arXiv admin note: text overlap with arXiv:1402.6908 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a mathematical model to decompose a longitudinal deformation into\nnormal and abnormal components. The goal is to detect and extract subtle\nquivers from periodic motions in a video sequence. It has important\napplications in medical image analysis. To achieve this goal, we consider a\nrepresentation of the longitudinal deformation, called the Beltrami descriptor,\nbased on quasiconformal theories. The Beltrami descriptor is a complex-valued\nmatrix. Each longitudinal deformation is associated to a Beltrami descriptor\nand vice versa. To decompose the longitudinal deformation, we propose to carry\nout the low rank and sparse decomposition of the Beltrami descriptor. The low\nrank component corresponds to the periodic motions, whereas the sparse part\ncorresponds to the abnormal motions of a longitudinal deformation. Experiments\nhave been carried out on both synthetic and real video sequences. Results\ndemonstrate the efficacy of our proposed model to decompose a longitudinal\ndeformation into regular and irregular components.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 05:47:36 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 03:54:43 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Law", "Ho", ""], ["Lui", "Lok Ming", ""], ["Siu", "Chun Yin", ""]]}, {"id": "2008.03195", "submitter": "Ailbhe Gill", "authors": "Ailbhe Gill, Emin Zerman, Cagri Ozcinar, Aljosa Smolic", "title": "A Study on Visual Perception of Light Field Content", "comments": "To appear in Irish Machine Vision and Image Processing (IMVIP) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effective design of visual computing systems depends heavily on the\nanticipation of visual attention, or saliency. While visual attention is well\ninvestigated for conventional 2D images and video, it is nevertheless a very\nactive research area for emerging immersive media. In particular, visual\nattention of light fields (light rays of a scene captured by a grid of cameras\nor micro lenses) has only recently become a focus of research. As they may be\nrendered and consumed in various ways, a primary challenge that arises is the\ndefinition of what visual perception of light field content should be. In this\nwork, we present a visual attention study on light field content. We conducted\nperception experiments displaying them to users in various ways and collected\ncorresponding visual attention data. Our analysis highlights characteristics of\nuser behaviour in light field imaging applications. The light field data set\nand attention data are provided with this paper.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 14:23:27 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Gill", "Ailbhe", ""], ["Zerman", "Emin", ""], ["Ozcinar", "Cagri", ""], ["Smolic", "Aljosa", ""]]}, {"id": "2008.03201", "submitter": "Tobias Fechter", "authors": "Dejan Kostyszyn, Tobias Fechter, Nico Bartl, Anca L. Grosu, Christian\n  Gratzke, August Sigle, Michael Mix, Juri Ruf, Thomas F. Fassbender, Selina\n  Kiefer, Alisa S. Bettermann, Nils H. Nicolay, Simon Spohn, Maria U. Kramer,\n  Peter Bronsert, Hongqian Guo, Xuefeng Qiu, Feng Wang, Christoph Henkenberens,\n  Rudolf A. Werner, Dimos Baltas, Philipp T. Meyer, Thorsten Derlin, Mengxia\n  Chen, Constantinos Zamboglou", "title": "Convolutional neural network based deep-learning architecture for\n  intraprostatic tumour contouring on PSMA PET images in patients with primary\n  prostate cancer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate delineation of the intraprostatic gross tumour volume (GTV) is a\nprerequisite for treatment approaches in patients with primary prostate cancer\n(PCa). Prostate-specific membrane antigen positron emission tomography\n(PSMA-PET) may outperform MRI in GTV detection. However, visual GTV delineation\nunderlies interobserver heterogeneity and is time consuming. The aim of this\nstudy was to develop a convolutional neural network (CNN) for automated\nsegmentation of intraprostatic tumour (GTV-CNN) in PSMA-PET.\n  Methods: The CNN (3D U-Net) was trained on [68Ga]PSMA-PET images of 152\npatients from two different institutions and the training labels were generated\nmanually using a validated technique. The CNN was tested on two independent\ninternal (cohort 1: [68Ga]PSMA-PET, n=18 and cohort 2: [18F]PSMA-PET, n=19) and\none external (cohort 3: [68Ga]PSMA-PET, n=20) test-datasets. Accordance between\nmanual contours and GTV-CNN was assessed with Dice-S{\\o}rensen coefficient\n(DSC). Sensitivity and specificity were calculated for the two internal\ntest-datasets by using whole-mount histology.\n  Results: Median DSCs for cohorts 1-3 were 0.84 (range: 0.32-0.95), 0.81\n(range: 0.28-0.93) and 0.83 (range: 0.32-0.93), respectively. Sensitivities and\nspecificities for GTV-CNN were comparable with manual expert contours: 0.98 and\n0.76 (cohort 1) and 1 and 0.57 (cohort 2), respectively. Computation time was\naround 6 seconds for a standard dataset.\n  Conclusion: The application of a CNN for automated contouring of\nintraprostatic GTV in [68Ga]PSMA- and [18F]PSMA-PET images resulted in a high\nconcordance with expert contours and in high sensitivities and specificities in\ncomparison with histology reference. This robust, accurate and fast technique\nmay be implemented for treatment concepts in primary PCa. The trained model and\nthe study's source code are available in an open source repository.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 14:32:14 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Kostyszyn", "Dejan", ""], ["Fechter", "Tobias", ""], ["Bartl", "Nico", ""], ["Grosu", "Anca L.", ""], ["Gratzke", "Christian", ""], ["Sigle", "August", ""], ["Mix", "Michael", ""], ["Ruf", "Juri", ""], ["Fassbender", "Thomas F.", ""], ["Kiefer", "Selina", ""], ["Bettermann", "Alisa S.", ""], ["Nicolay", "Nils H.", ""], ["Spohn", "Simon", ""], ["Kramer", "Maria U.", ""], ["Bronsert", "Peter", ""], ["Guo", "Hongqian", ""], ["Qiu", "Xuefeng", ""], ["Wang", "Feng", ""], ["Henkenberens", "Christoph", ""], ["Werner", "Rudolf A.", ""], ["Baltas", "Dimos", ""], ["Meyer", "Philipp T.", ""], ["Derlin", "Thorsten", ""], ["Chen", "Mengxia", ""], ["Zamboglou", "Constantinos", ""]]}, {"id": "2008.03205", "submitter": "Aakarsh Malhotra", "authors": "Aakarsh Malhotra, Surbhi Mittal, Puspita Majumdar, Saheb Chhabra,\n  Kartik Thakral, Mayank Vatsa, Richa Singh, Santanu Chaudhury, Ashwin Pudrod,\n  Anjali Agrawal", "title": "Multi-Task Driven Explainable Diagnosis of COVID-19 using Chest X-ray\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing number of COVID-19 cases globally, all the countries are\nramping up the testing numbers. While the RT-PCR kits are available in\nsufficient quantity in several countries, others are facing challenges with\nlimited availability of testing kits and processing centers in remote areas.\nThis has motivated researchers to find alternate methods of testing which are\nreliable, easily accessible and faster. Chest X-Ray is one of the modalities\nthat is gaining acceptance as a screening modality. Towards this direction, the\npaper has two primary contributions. Firstly, we present the COVID-19\nMulti-Task Network which is an automated end-to-end network for COVID-19\nscreening. The proposed network not only predicts whether the CXR has COVID-19\nfeatures present or not, it also performs semantic segmentation of the regions\nof interest to make the model explainable. Secondly, with the help of medical\nprofessionals, we manually annotate the lung regions of 9000 frontal chest\nradiographs taken from ChestXray-14, CheXpert and a consolidated COVID-19\ndataset. Further, 200 chest radiographs pertaining to COVID-19 patients are\nalso annotated for semantic segmentation. This database will be released to the\nresearch community.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 12:52:23 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Malhotra", "Aakarsh", ""], ["Mittal", "Surbhi", ""], ["Majumdar", "Puspita", ""], ["Chhabra", "Saheb", ""], ["Thakral", "Kartik", ""], ["Vatsa", "Mayank", ""], ["Singh", "Richa", ""], ["Chaudhury", "Santanu", ""], ["Pudrod", "Ashwin", ""], ["Agrawal", "Anjali", ""]]}, {"id": "2008.03206", "submitter": "Francesco Guarnera", "authors": "Sebastiano Battiato (1), Oliver Giudice (1), Francesco Guarnera (1),\n  Giovanni Puglisi (2) ((1) University of Catania, (2) University of Cagliari)", "title": "In-Depth DCT Coefficient Distribution Analysis for First Quantization\n  Estimation", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-68780-9_45", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exploitation of traces in JPEG double compressed images is of utter\nimportance for investigations. Properly exploiting such insights, First\nQuantization Estimation (FQE) could be performed in order to obtain source\ncamera model identification (CMI) and therefore reconstruct the history of a\ndigital image. In this paper, a method able to estimate the first quantization\nfactors for JPEG double compressed images is presented, employing a mixed\nstatistical and Machine Learning approach. The presented solution is\ndemonstrated to work without any a-priori assumptions about the quantization\nmatrices. Experimental results and comparisons with the state-of-the-art show\nthe goodness of the proposed technique.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 14:46:10 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Battiato", "Sebastiano", "", "University of Catania"], ["Giudice", "Oliver", "", "University of Catania"], ["Guarnera", "Francesco", "", "University of Catania"], ["Puglisi", "Giovanni", "", "University of Cagliari"]]}, {"id": "2008.03230", "submitter": "Shohreh Deldari", "authors": "Shohreh Deldari, Daniel V. Smith, Amin Sadri, Flora D. Salim", "title": "ESPRESSO: Entropy and ShaPe awaRe timE-Series SegmentatiOn for\n  processing heterogeneous sensor data", "comments": "23 pages, 11 figures, accepted at IMWUT Volume(4) issue(3)", "journal-ref": null, "doi": "10.1145/3411832", "report-no": null, "categories": "cs.LG cs.CV cs.DB cs.IT eess.SP math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting informative and meaningful temporal segments from high-dimensional\nwearable sensor data, smart devices, or IoT data is a vital preprocessing step\nin applications such as Human Activity Recognition (HAR), trajectory\nprediction, gesture recognition, and lifelogging. In this paper, we propose\nESPRESSO (Entropy and ShaPe awaRe timE-Series SegmentatiOn), a hybrid\nsegmentation model for multi-dimensional time-series that is formulated to\nexploit the entropy and temporal shape properties of time-series. ESPRESSO\ndiffers from existing methods that focus upon particular statistical or\ntemporal properties of time-series exclusively. As part of model development, a\nnovel temporal representation of time-series $WCAC$ was introduced along with a\ngreedy search approach that estimate segments based upon the entropy metric.\nESPRESSO was shown to offer superior performance to four state-of-the-art\nmethods across seven public datasets of wearable and wear-free sensing. In\naddition, we undertake a deeper investigation of these datasets to understand\nhow ESPRESSO and its constituent methods perform with respect to different\ndataset characteristics. Finally, we provide two interesting case-studies to\nshow how applying ESPRESSO can assist in inferring daily activity routines and\nthe emotional state of humans.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 10:41:20 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Deldari", "Shohreh", ""], ["Smith", "Daniel V.", ""], ["Sadri", "Amin", ""], ["Salim", "Flora D.", ""]]}, {"id": "2008.03247", "submitter": "Vishwas M Shetty", "authors": "Vishwas M. Shetty, Metilda Sagaya Mary N J, S. Umesh", "title": "Investigation of Speaker-adaptation methods in Transformer based ASR", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end models are fast replacing conventional hybrid models in automatic\nspeech recognition. A transformer is a sequence-to-sequence framework solely\nbased on attention, that was initially applied to machine translation task.\nThis end-to-end framework has been shown to give promising results when used\nfor automatic speech recognition as well. In this paper, we explore different\nways of incorporating speaker information while training a transformer-based\nmodel to improve its performance. We present speaker information in the form of\nspeaker embeddings for each of the speakers. Two broad categories of speaker\nembeddings are used: (i)fixed embeddings, and (ii)learned embeddings. We\nexperiment using speaker embeddings learned along with the model training, as\nwell as one-hot vectors and x-vectors. Using these different speaker\nembeddings, we obtain an average relative improvement of 1% to 3% in the token\nerror rate. We report results on the NPTEL lecture database. NPTEL is an\nopen-source e-learning portal providing content from top Indian universities.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 16:09:03 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Shetty", "Vishwas M.", ""], ["J", "Metilda Sagaya Mary N", ""], ["Umesh", "S.", ""]]}, {"id": "2008.03270", "submitter": "Xiang Wang", "authors": "Xiang Wang, Changxin Gao, Shiwei Zhang, and Nong Sang", "title": "Multi-Level Temporal Pyramid Network for Action Detection", "comments": "Accepted by PRCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, one-stage frameworks have been widely applied for temporal action\ndetection, but they still suffer from the challenge that the action instances\nspan a wide range of time. The reason is that these one-stage detectors, e.g.,\nSingle Shot Multi-Box Detector (SSD), extract temporal features only applying a\nsingle-level layer for each head, which is not discriminative enough to perform\nclassification and regression. In this paper, we propose a Multi-Level Temporal\nPyramid Network (MLTPN) to improve the discrimination of the features.\nSpecially, we first fuse the features from multiple layers with different\ntemporal resolutions, to encode multi-layer temporal information. We then apply\na multi-level feature pyramid architecture on the features to enhance their\ndiscriminative abilities. Finally, we design a simple yet effective feature\nfusion module to fuse the multi-level multi-scale features. By this means, the\nproposed MLTPN can learn rich and discriminative features for different action\ninstances with different durations. We evaluate MLTPN on two challenging\ndatasets: THUMOS'14 and Activitynet v1.3, and the experimental results show\nthat MLTPN obtains competitive performance on Activitynet v1.3 and outperforms\nthe state-of-the-art approaches on THUMOS'14 significantly.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 17:08:24 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Wang", "Xiang", ""], ["Gao", "Changxin", ""], ["Zhang", "Shiwei", ""], ["Sang", "Nong", ""]]}, {"id": "2008.03285", "submitter": "Guillermo Garcia-Hernando", "authors": "Guillermo Garcia-Hernando and Edward Johns and Tae-Kyun Kim", "title": "Physics-Based Dexterous Manipulations with Estimated Hand Poses and\n  Residual Reinforcement Learning", "comments": "To appear in IROS2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dexterous manipulation of objects in virtual environments with our bare\nhands, by using only a depth sensor and a state-of-the-art 3D hand pose\nestimator (HPE), is challenging. While virtual environments are ruled by\nphysics, e.g. object weights and surface frictions, the absence of force\nfeedback makes the task challenging, as even slight inaccuracies on finger tips\nor contact points from HPE may make the interactions fail. Prior arts simply\ngenerate contact forces in the direction of the fingers' closures, when finger\njoints penetrate virtual objects. Although useful for simple grasping\nscenarios, they cannot be applied to dexterous manipulations such as in-hand\nmanipulation. Existing reinforcement learning (RL) and imitation learning (IL)\napproaches train agents that learn skills by using task-specific rewards,\nwithout considering any online user input. In this work, we propose to learn a\nmodel that maps noisy input hand poses to target virtual poses, which\nintroduces the needed contacts to accomplish the tasks on a physics simulator.\nThe agent is trained in a residual setting by using a model-free hybrid RL+IL\napproach. A 3D hand pose estimation reward is introduced leading to an\nimprovement on HPE accuracy when the physics-guided corrected target poses are\nremapped to the input space. As the model corrects HPE errors by applying minor\nbut crucial joint displacements for contacts, this helps to keep the generated\nmotion visually close to the user input. Since HPE sequences performing\nsuccessful virtual interactions do not exist, a data generation scheme to train\nand evaluate the system is proposed. We test our framework in two applications\nthat use hand pose estimates for dexterous manipulations: hand-object\ninteractions in VR and hand-object motion reconstruction in-the-wild.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 17:34:28 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Garcia-Hernando", "Guillermo", ""], ["Johns", "Edward", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "2008.03286", "submitter": "Yichao Zhou", "authors": "Yichao Zhou, Jingwei Huang, Xili Dai, Shichen Liu, Linjie Luo, Zhili\n  Chen, Yi Ma", "title": "HoliCity: A City-Scale Data Platform for Learning Holistic 3D Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present HoliCity, a city-scale 3D dataset with rich structural\ninformation. Currently, this dataset has 6,300 real-world panoramas of\nresolution $13312 \\times 6656$ that are accurately aligned with the CAD model\nof downtown London with an area of more than 20 km$^2$, in which the median\nreprojection error of the alignment of an average image is less than half a\ndegree. This dataset aims to be an all-in-one data platform for research of\nlearning abstracted high-level holistic 3D structures that can be derived from\ncity CAD models, e.g., corners, lines, wireframes, planes, and cuboids, with\nthe ultimate goal of supporting real-world applications including city-scale\nreconstruction, localization, mapping, and augmented reality. The accurate\nalignment of the 3D CAD models and panoramas also benefits low-level 3D vision\ntasks such as surface normal estimation, as the surface normal extracted from\nprevious LiDAR-based datasets is often noisy. We conduct experiments to\ndemonstrate the applications of HoliCity, such as predicting surface\nsegmentation, normal maps, depth maps, and vanishing points, as well as test\nthe generalizability of methods trained on HoliCity and other related datasets.\nHoliCity is available at https://holicity.io.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 17:34:47 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 05:06:19 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Zhou", "Yichao", ""], ["Huang", "Jingwei", ""], ["Dai", "Xili", ""], ["Liu", "Shichen", ""], ["Luo", "Linjie", ""], ["Chen", "Zhili", ""], ["Ma", "Yi", ""]]}, {"id": "2008.03346", "submitter": "Thomas Truong", "authors": "Thomas Truong and Svetlana Yanushkevich", "title": "Generative Adversarial Network for Radar Signal Generation", "comments": null, "journal-ref": null, "doi": "10.1109/IJCNN.2019.8851887", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major obstacle in radar based methods for concealed object detection on\nhumans and seamless integration into security and access control system is the\ndifficulty in collecting high quality radar signal data. Generative adversarial\nnetworks (GAN) have shown promise in data generation application in the fields\nof image and audio processing. As such, this paper proposes the design of a GAN\nfor application in radar signal generation. Data collected using the\nFinite-Difference Time-Domain (FDTD) method on three concealed object classes\n(no object, large object, and small object) were used as training data to train\na GAN to generate radar signal samples for each class. The proposed GAN\ngenerated radar signal data which was indistinguishable from the training data\nby qualitative human observers.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 19:31:06 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Truong", "Thomas", ""], ["Yanushkevich", "Svetlana", ""]]}, {"id": "2008.03352", "submitter": "Bowen Li", "authors": "Bowen Li, Ke Yan, Dar-In Tai, Yuankai Huo, Le Lu, Jing Xiao, Adam P.\n  Harrison", "title": "Reliable Liver Fibrosis Assessment from Ultrasound using Global\n  Hetero-Image Fusion and View-Specific Parameterization", "comments": "10 pages, MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound (US) is a critical modality for diagnosing liver fibrosis.\nUnfortunately, assessment is very subjective, motivating automated approaches.\nWe introduce a principled deep convolutional neural network (CNN) workflow that\nincorporates several innovations. First, to avoid overfitting on non-relevant\nimage features, we force the network to focus on a clinical region of interest\n(ROI), encompassing the liver parenchyma and upper border. Second, we introduce\nglobal heteroimage fusion (GHIF), which allows the CNN to fuse features from\nany arbitrary number of images in a study, increasing its versatility and\nflexibility. Finally, we use 'style'-based view-specific parameterization (VSP)\nto tailor the CNN processing for different viewpoints of the liver, while\nkeeping the majority of parameters the same across views. Experiments on a\ndataset of 610 patient studies (6979 images) demonstrate that our pipeline can\ncontribute roughly 7% and 22% improvements in partial area under the curve and\nrecall at 90% precision, respectively, over conventional classifiers,\nvalidating our approach to this crucial problem.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 19:50:15 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Li", "Bowen", ""], ["Yan", "Ke", ""], ["Tai", "Dar-In", ""], ["Huo", "Yuankai", ""], ["Lu", "Le", ""], ["Xiao", "Jing", ""], ["Harrison", "Adam P.", ""]]}, {"id": "2008.03353", "submitter": "Thomas Truong", "authors": "Thomas Truong, Jonathan Graf, Svetlana Yanushkevich", "title": "Hybrid Score- and Rank-level Fusion for Person Identification using Face\n  and ECG Data", "comments": null, "journal-ref": null, "doi": "10.1109/EST.2019.8806206", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uni-modal identification systems are vulnerable to errors in sensor data\ncollection and are therefore more likely to misidentify subjects. For instance,\nrelying on data solely from an RGB face camera can cause problems in poorly lit\nenvironments or if subjects do not face the camera. Other identification\nmethods such as electrocardiograms (ECG) have issues with improper lead\nconnections to the skin. Errors in identification are minimized through the\nfusion of information gathered from both of these models. This paper proposes a\nmethodology for combining the identification results of face and ECG data using\nPart A of the BioVid Heat Pain Database containing synchronized RGB-video and\nECG data on 87 subjects. Using 10-fold cross-validation, face identification\nwas 98.8% accurate, while the ECG identification was 96.1% accurate. By using a\nfusion approach the identification accuracy improved to 99.8%. Our proposed\nmethodology allows for identification accuracies to be significantly improved\nby using disparate face and ECG models that have non-overlapping modalities.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 19:54:59 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Truong", "Thomas", ""], ["Graf", "Jonathan", ""], ["Yanushkevich", "Svetlana", ""]]}, {"id": "2008.03356", "submitter": "Anna Solovyova", "authors": "A.Solovyova, I.Solovyov", "title": "X-Ray bone abnormalities detection using MURA dataset", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the deep network trained on the MURA dataset from the Stanford\nUniversity released in 2017. Our system is able to detect bone abnormalities on\nthe radiographs and visualise such zones. We found that our solution has the\naccuracy comparable to the best results that have been achieved by other\ndevelopment teams that used MURA dataset, in particular the overall Kappa score\nthat was achieved by our team is about 0.942 on the wrist, 0.862 on the hand\nand o.735 on the shoulder (compared to the best available results to this\nmoment on the official web-site 0.931, 0.851 and 0.729 accordingly). However,\ndespite the good results there are a lot of directions for the future\nenhancement of the proposed technology. We see a big potential in the further\ndevelopment computer aided systems (CAD) for the radiographs as the one that\nwill help practical specialists diagnose bone fractures as well as bone\noncology cases faster and with the higher accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 19:58:56 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Solovyova", "A.", ""], ["Solovyov", "I.", ""]]}, {"id": "2008.03364", "submitter": "Xuanqing Liu", "authors": "Jiachen Zhong, Xuanqing Liu, Cho-Jui Hsieh", "title": "Improving the Speed and Quality of GAN by Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GAN) have shown remarkable results in image\ngeneration tasks. High fidelity class-conditional GAN methods often rely on\nstabilization techniques by constraining the global Lipschitz continuity. Such\nregularization leads to less expressive models and slower convergence speed;\nother techniques, such as the large batch training, require unconventional\ncomputing power and are not widely accessible. In this paper, we develop an\nefficient algorithm, namely FastGAN (Free AdverSarial Training), to improve the\nspeed and quality of GAN training based on the adversarial training technique.\nWe benchmark our method on CIFAR10, a subset of ImageNet, and the full ImageNet\ndatasets. We choose strong baselines such as SNGAN and SAGAN; the results\ndemonstrate that our training algorithm can achieve better generation quality\n(in terms of the Inception score and Frechet Inception distance) with less\noverall training time. Most notably, our training algorithm brings ImageNet\ntraining to the broader public by requiring 2-4 GPUs.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 20:21:31 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Zhong", "Jiachen", ""], ["Liu", "Xuanqing", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "2008.03375", "submitter": "Viktor Nikitin", "authors": "Viktor Nikitin, Vincent De Andrade, Azat Slyamov, Benjamin J. Gould,\n  Yuepeng Zhang, Vandana Sampathkumar, Narayanan Kasthuri, Doga Gursoy,\n  Francesco De Carlo", "title": "Distributed optimization for nonrigid nano-tomography", "comments": null, "journal-ref": null, "doi": "10.1109/TCI.2021.3060915", "report-no": null, "categories": "cs.CE cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resolution level and reconstruction quality in nano-computed tomography\n(nano-CT) are in part limited by the stability of microscopes, because the\nmagnitude of mechanical vibrations during scanning becomes comparable to the\nimaging resolution, and the ability of the samples to resist beam damage during\ndata acquisition. In such cases, there is no incentive in recovering the sample\nstate at different time steps like in time-resolved reconstruction methods, but\ninstead the goal is to retrieve a single reconstruction at the highest possible\nspatial resolution and without any imaging artifacts. Here we propose a joint\nsolver for imaging samples at the nanoscale with projection alignment,\nunwarping and regularization. Projection data consistency is regulated by dense\noptical flow estimated by Farneback's algorithm, leading to sharp sample\nreconstructions with less artifacts. Synthetic data tests show robustness of\nthe method to Poisson and low-frequency background noise. Applicability of the\nmethod is demonstrated on two large-scale nano-imaging experimental data sets.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 19:22:43 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 17:14:31 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Nikitin", "Viktor", ""], ["De Andrade", "Vincent", ""], ["Slyamov", "Azat", ""], ["Gould", "Benjamin J.", ""], ["Zhang", "Yuepeng", ""], ["Sampathkumar", "Vandana", ""], ["Kasthuri", "Narayanan", ""], ["Gursoy", "Doga", ""], ["De Carlo", "Francesco", ""]]}, {"id": "2008.03404", "submitter": "Yan Xia", "authors": "Yan Xia, Yusheng Xu, Cheng Wang, Uwe Stilla", "title": "VPC-Net: Completion of 3D Vehicles from MLS Point Clouds", "comments": "accepted by ISPRS Journal of Photogrammetry and Remote Sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a dynamic and essential component in the road environment of urban\nscenarios, vehicles are the most popular investigation targets. To monitor\ntheir behavior and extract their geometric characteristics, an accurate and\ninstant measurement of vehicles plays a vital role in traffic and\ntransportation fields. Point clouds acquired from the mobile laser scanning\n(MLS) system deliver 3D information of road scenes with unprecedented detail.\nThey have proven to be an adequate data source in the fields of intelligent\ntransportation and autonomous driving, especially for extracting vehicles.\nHowever, acquired 3D point clouds of vehicles from MLS systems are inevitably\nincomplete due to object occlusion or self-occlusion. To tackle this problem,\nwe proposed a neural network to synthesize complete, dense, and uniform point\nclouds for vehicles from MLS data, named Vehicle Points Completion-Net\n(VPC-Net). In this network, we introduce a new encoder module to extract global\nfeatures from the input instance, consisting of a spatial transformer network\nand point feature enhancement layer. Moreover, a new refiner module is also\npresented to preserve the vehicle details from inputs and refine the complete\noutputs with fine-grained information. Given sparse and partial point clouds as\ninputs, the network can generate complete and realistic vehicle structures and\nkeep the fine-grained details from the partial inputs. We evaluated the\nproposed VPC-Net in different experiments using synthetic and real-scan\ndatasets and applied the results to 3D vehicle monitoring tasks. Quantitative\nand qualitative experiments demonstrate the promising performance of the\nproposed VPC-Net and show state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 00:22:43 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 16:32:40 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Xia", "Yan", ""], ["Xu", "Yusheng", ""], ["Wang", "Cheng", ""], ["Stilla", "Uwe", ""]]}, {"id": "2008.03411", "submitter": "Wei Wang", "authors": "Wei Wang, Lin Cheng, Yanjie Zhu, Dong Liang", "title": "Exploring the parameter reusability of CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent times, using small data to train networks has become a hot topic in\nthe field of deep learning. Reusing pre-trained parameters is one of the most\nimportant strategies to address the issue of semi-supervised and transfer\nlearning. However, the fundamental reason for the success of these methods is\nstill unclear. In this paper, we propose a solution that can not only judge\nwhether a given network is reusable or not based on the performance of reusing\nconvolution kernels but also judge which layers' parameters of the given\nnetwork can be reused, based on the performance of reusing corresponding\nparameters and, ultimately, judge whether those parameters are reusable or not\nin a target task based on the root mean square error (RMSE) of the\ncorresponding convolution kernels. Specifically, we define that the success of\na CNN's parameter reuse depends upon two conditions: first, the network is a\nreusable network; and second, the RMSE between the convolution kernels from the\nsource domain and target domain is small enough. The experimental results\ndemonstrate that the performance of reused parameters applied to target tasks,\nwhen these conditions are met, is significantly improved.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 01:23:22 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 04:23:25 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Wang", "Wei", ""], ["Cheng", "Lin", ""], ["Zhu", "Yanjie", ""], ["Liang", "Dong", ""]]}, {"id": "2008.03412", "submitter": "Iacopo Masi", "authors": "Iacopo Masi, Aditya Killekar, Royston Marian Mascarenhas, Shenoy\n  Pratik Gurudatt, Wael AbdAlmageed", "title": "Two-branch Recurrent Network for Isolating Deepfakes in Videos", "comments": "To appear in the 16th European Conference on Computer Vision ECCV\n  2020 (added link to our demo and to the video presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current spike of hyper-realistic faces artificially generated using\ndeepfakes calls for media forensics solutions that are tailored to video\nstreams and work reliably with a low false alarm rate at the video level. We\npresent a method for deepfake detection based on a two-branch network structure\nthat isolates digitally manipulated faces by learning to amplify artifacts\nwhile suppressing the high-level face content. Unlike current methods that\nextract spatial frequencies as a preprocessing step, we propose a two-branch\nstructure: one branch propagates the original information, while the other\nbranch suppresses the face content yet amplifies multi-band frequencies using a\nLaplacian of Gaussian (LoG) as a bottleneck layer. To better isolate\nmanipulated faces, we derive a novel cost function that, unlike regular\nclassification, compresses the variability of natural faces and pushes away the\nunrealistic facial samples in the feature space. Our two novel components show\npromising results on the FaceForensics++, Celeb-DF, and Facebook's DFDC preview\nbenchmarks, when compared to prior work. We then offer a full, detailed\nablation study of our network architecture and cost function. Finally, although\nthe bar is still high to get very remarkable figures at a very low false alarm\nrate, our study shows that we can achieve good video-level performance when\ncross-testing in terms of video-level AUC.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 01:38:56 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 02:35:55 GMT"}, {"version": "v3", "created": "Fri, 4 Sep 2020 01:03:55 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Masi", "Iacopo", ""], ["Killekar", "Aditya", ""], ["Mascarenhas", "Royston Marian", ""], ["Gurudatt", "Shenoy Pratik", ""], ["AbdAlmageed", "Wael", ""]]}, {"id": "2008.03414", "submitter": "Wei Wang", "authors": "Wei Wang", "title": "Using UNet and PSPNet to explore the reusability principle of CNN\n  parameters", "comments": "arXiv admin note: substantial text overlap with arXiv:2008.03411", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to reduce the requirement on training dataset size is a hot topic in deep\nlearning community. One straightforward way is to reuse some pre-trained\nparameters. Some previous work like Deep transfer learning reuse the model\nparameters trained for the first task as the starting point for the second\ntask, and semi-supervised learning is trained upon a combination of labeled and\nunlabeled data. However, the fundamental reason of the success of these methods\nis unclear. In this paper, the reusability of parameters in each layer of a\ndeep convolutional neural network is experimentally quantified by using a\nnetwork to do segmentation and auto-encoder task. This paper proves that\nnetwork parameters can be reused for two reasons: first, the network features\nare general; Second, there is little difference between the pre-trained\nparameters and the ideal network parameters. Through the use of parameter\nreplacement and comparison, we demonstrate that reusability is different in\nBN(Batch Normalization)[7] layer and Convolution layer and some observations:\n(1)Running mean and running variance plays an important role than Weight and\nBias in BN layer.(2)The weight and bias can be reused in BN layers.( 3) The\nnetwork is very sensitive to the weight of convolutional layer.(4) The bias in\nConvolution layers are not sensitive, and it can be reused directly.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 01:51:08 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Wang", "Wei", ""]]}, {"id": "2008.03426", "submitter": "Renwei Dian", "authors": "Renwei Dian, Shutao Li, Bin Sun, and Anjing Guo", "title": "Recent Advances and New Guidelines on Hyperspectral and Multispectral\n  Image Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral image (HSI) with high spectral resolution often suffers from\nlow spatial resolution owing to the limitations of imaging sensors. Image\nfusion is an effective and economical way to enhance the spatial resolution of\nHSI, which combines HSI with higher spatial resolution multispectral image\n(MSI) of the same scenario. In the past years, many HSI and MSI fusion\nalgorithms are introduced to obtain high-resolution HSI. However, it lacks a\nfull-scale review for the newly proposed HSI and MSI fusion approaches. To\ntackle this problem,this work gives a comprehensive review and new guidelines\nfor HSI-MSI fusion. According to the characteristics of HSI-MSI fusion methods,\nthey are categorized as four categories, including pan-sharpening based\napproaches, matrix factorization based approaches, tensor representation based\napproaches, and deep convolution neural network based approaches. We make a\ndetailed introduction, discussions, and comparison for the fusion methods in\neach category. Additionally, the existing challenges and possible future\ndirections for the HSI-MSI fusion are presented.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 03:05:46 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Dian", "Renwei", ""], ["Li", "Shutao", ""], ["Sun", "Bin", ""], ["Guo", "Anjing", ""]]}, {"id": "2008.03428", "submitter": "Renzhen Wang", "authors": "Renzhen Wang, Kaiqin Hu, Yanwen Zhu, Jun Shu, Qian Zhao, Deyu Meng", "title": "Meta Feature Modulator for Long-tailed Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks often degrade significantly when training data suffer\nfrom class imbalance problems. Existing approaches, e.g., re-sampling and\nre-weighting, commonly address this issue by rearranging the label distribution\nof training data to train the networks fitting well to the implicit balanced\nlabel distribution. However, most of them hinder the representative ability of\nlearned features due to insufficient use of intra/inter-sample information of\ntraining data. To address this issue, we propose meta feature modulator (MFM),\na meta-learning framework to model the difference between the long-tailed\ntraining data and the balanced meta data from the perspective of representation\nlearning. Concretely, we employ learnable hyper-parameters (dubbed modulation\nparameters) to adaptively scale and shift the intermediate features of\nclassification networks, and the modulation parameters are optimized together\nwith the classification network parameters guided by a small amount of balanced\nmeta data. We further design a modulator network to guide the generation of the\nmodulation parameters, and such a meta-learner can be readily adapted to train\nthe classification network on other long-tailed datasets. Extensive experiments\non benchmark vision datasets substantiate the superiority of our approach on\nlong-tailed recognition tasks beyond other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 03:19:03 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Wang", "Renzhen", ""], ["Hu", "Kaiqin", ""], ["Zhu", "Yanwen", ""], ["Shu", "Jun", ""], ["Zhao", "Qian", ""], ["Meng", "Deyu", ""]]}, {"id": "2008.03435", "submitter": "Yuhao Huang", "authors": "Wang Jian, Miao Juzheng, Yang Xin, Li Rui, Zhou Guangquan, Huang\n  Yuhao, Lin Zehui, Xue Wufeng, Jia Xiaohong, Zhou Jianqiao, Huang Ruobing, Ni\n  Dong", "title": "Auto-weighting for Breast Cancer Classification in Multimodal Ultrasound", "comments": "Early Accepted by MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is the most common invasive cancer in women. Besides the\nprimary B-mode ultrasound screening, sonographers have explored the inclusion\nof Doppler, strain and shear-wave elasticity imaging to advance the diagnosis.\nHowever, recognizing useful patterns in all types of images and weighing up the\nsignificance of each modality can elude less-experienced clinicians. In this\npaper, we explore, for the first time, an automatic way to combine the four\ntypes of ultrasonography to discriminate between benign and malignant breast\nnodules. A novel multimodal network is proposed, along with promising\nlearnability and simplicity to improve classification accuracy. The key is\nusing a weight-sharing strategy to encourage interactions between modalities\nand adopting an additional cross-modalities objective to integrate global\ninformation. In contrast to hardcoding the weights of each modality in the\nmodel, we embed it in a Reinforcement Learning framework to learn this\nweighting in an end-to-end manner. Thus the model is trained to seek the\noptimal multimodal combination without handcrafted heuristics. The proposed\nframework is evaluated on a dataset contains 1616 set of multimodal images.\nResults showed that the model scored a high classification accuracy of 95.4%,\nwhich indicates the efficiency of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 03:42:00 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Jian", "Wang", ""], ["Juzheng", "Miao", ""], ["Xin", "Yang", ""], ["Rui", "Li", ""], ["Guangquan", "Zhou", ""], ["Yuhao", "Huang", ""], ["Zehui", "Lin", ""], ["Wufeng", "Xue", ""], ["Xiaohong", "Jia", ""], ["Jianqiao", "Zhou", ""], ["Ruobing", "Huang", ""], ["Dong", "Ni", ""]]}, {"id": "2008.03440", "submitter": "Bowen Jiang", "authors": "Bowen Jiang, Maohao Shen", "title": "Dimensionality Reduction via Diffusion Map Improved with Supervised\n  Linear Projection", "comments": "This paper is accepted to be published in the 27th IEEE International\n  Conference on Image Processing (ICIP 2020). Two authors contribute equally to\n  the work", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When performing classification tasks, raw high dimensional features often\ncontain redundant information, and lead to increased computational complexity\nand overfitting. In this paper, we assume the data samples lie on a single\nunderlying smooth manifold, and define intra-class and inter-class similarities\nusing pairwise local kernel distances. We aim to find a linear projection to\nmaximize the intra-class similarities and minimize the inter-class similarities\nsimultaneously, so that the projected low dimensional data has optimized\npairwise distances based on the label information, which is more suitable for a\nDiffusion Map to do further dimensionality reduction. Numerical experiments on\nseveral benchmark datasets show that our proposed approaches are able to\nextract low dimensional discriminate features that could help us achieve higher\nclassification accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 04:26:07 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Jiang", "Bowen", ""], ["Shen", "Maohao", ""]]}, {"id": "2008.03455", "submitter": "Zhang Yunlong", "authors": "Yunlong Zhang, Changxing Jing, Huangxing Lin, Chaoqi Chen, Yue Huang,\n  Xinghao Ding, Yang Zou", "title": "Hard Class Rectification for Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation (DA) aims to transfer knowledge from a label-rich and\nrelated domain (source domain) to a label-scare domain (target domain).\nPseudo-labeling has recently been widely explored and used in DA. However, this\nline of research is still confined to the inaccuracy of pseudo-labels. In this\npaper, we reveal an interesting observation that the target samples belonging\nto the classes with larger domain shift are easier to be misclassified compared\nwith the other classes. These classes are called hard class, which deteriorates\nthe performance of DA and restricts the applications of DA. We propose a novel\nframework, called Hard Class Rectification Pseudo-labeling (HCRPL), to\nalleviate the hard class problem from two aspects. First, as is difficult to\nidentify the target samples as hard class, we propose a simple yet effective\nscheme, named Adaptive Prediction Calibration (APC), to calibrate the\npredictions of the target samples according to the difficulty degree for each\nclass. Second, we further consider that the predictions of target samples\nbelonging to the hard class are vulnerable to perturbations. To prevent these\nsamples to be misclassified easily, we introduce Temporal-Ensembling (TE) and\nSelf-Ensembling (SE) to obtain consistent predictions. The proposed method is\nevaluated in both unsupervised domain adaptation (UDA) and semi-supervised\ndomain adaptation (SSDA). The experimental results on several real-world\ncross-domain benchmarks, including ImageCLEF, Office-31 and Office-Home,\nsubstantiates the superiority of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 06:21:58 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 00:03:55 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Zhang", "Yunlong", ""], ["Jing", "Changxing", ""], ["Lin", "Huangxing", ""], ["Chen", "Chaoqi", ""], ["Huang", "Yue", ""], ["Ding", "Xinghao", ""], ["Zou", "Yang", ""]]}, {"id": "2008.03462", "submitter": "Can Zhang", "authors": "Can Zhang, Yuexian Zou, Guang Chen, Lei Gan", "title": "PAN: Towards Fast Action Recognition via Learning Persistence of\n  Appearance", "comments": "submitted to TIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficiently modeling dynamic motion information in videos is crucial for\naction recognition task. Most state-of-the-art methods heavily rely on dense\noptical flow as motion representation. Although combining optical flow with RGB\nframes as input can achieve excellent recognition performance, the optical flow\nextraction is very time-consuming. This undoubtably will count against\nreal-time action recognition. In this paper, we shed light on fast action\nrecognition by lifting the reliance on optical flow. Our motivation lies in the\nobservation that small displacements of motion boundaries are the most critical\ningredients for distinguishing actions, so we design a novel motion cue called\nPersistence of Appearance (PA). In contrast to optical flow, our PA focuses\nmore on distilling the motion information at boundaries. Also, it is more\nefficient by only accumulating pixel-wise differences in feature space, instead\nof using exhaustive patch-wise search of all the possible motion vectors. Our\nPA is over 1000x faster (8196fps vs. 8fps) than conventional optical flow in\nterms of motion modeling speed. To further aggregate the short-term dynamics in\nPA to long-term dynamics, we also devise a global temporal fusion strategy\ncalled Various-timescale Aggregation Pooling (VAP) that can adaptively model\nlong-range temporal relationships across various timescales. We finally\nincorporate the proposed PA and VAP to form a unified framework called\nPersistent Appearance Network (PAN) with strong temporal modeling ability.\nExtensive experiments on six challenging action recognition benchmarks verify\nthat our PAN outperforms recent state-of-the-art methods at low FLOPs. Codes\nand models are available at: https://github.com/zhang-can/PAN-PyTorch.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 07:09:54 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Zhang", "Can", ""], ["Zou", "Yuexian", ""], ["Chen", "Guang", ""], ["Gan", "Lei", ""]]}, {"id": "2008.03465", "submitter": "Hongwei Li", "authors": "Hongwei Li, Aurore Menegaux, Felix JB B\u007f\\\"auerlein, Suprosanna Shit,\n  Benita Schmitz-Koep, Christian Sorg, Bjoern Menze and Dennis Hedderich", "title": "Complex Grey Matter Structure Segmentation in Brains via Deep Learning:\n  Example of the Claustrum", "comments": "submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentationand parcellation of the brain has been widely performed on brain\nMRI using atlas-based methods. However, segmentation of the claustrum, a thin\nand sheet-like structure between insular cortex and putamen has not been\namenable to automatized segmentation, thus limiting its investigation in larger\nimaging cohorts. Recently, deep-learning based approaches have been introduced\nfor automated segmentation of brain structures, yielding great potential to\novercome preexisting limitations. In the following, we present a multi-view\ndeep-learning based approach to segment the claustrum in T1-weighted MRI scans.\nWe trained and evaluated the proposed method on 181 manual bilateral claustrum\nannotations by an expert neuroradiologist serving as reference standard.\nCross-validation experiments yielded median volumetric similarity, robust\nHausdor? distance and Dice score of 93.3%, 1.41mm and 71.8% respectively which\nrepresents equal or superior segmentation performance compared to human\nintra-rater reliability. Leave-one-scanner-out evaluation showed good\ntransfer-ability of the algorithm to images from unseen scanners, however at\nslightly inferior performance. Furthermore, we found that AI-based claustrum\nsegmentation benefits from multi-view information and requires sample sizes of\naround 75 MRI scans in the training set. In conclusion, the developed algorithm\nhas large potential in independent study cohorts and to facilitate MRI-based\nresearch of the human claustrum through automated segmentation. The software\nand models of our method are made publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 07:25:48 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Li", "Hongwei", ""], ["Menegaux", "Aurore", ""], ["B\u007f\u00e4uerlein", "Felix JB", ""], ["Shit", "Suprosanna", ""], ["Schmitz-Koep", "Benita", ""], ["Sorg", "Christian", ""], ["Menze", "Bjoern", ""], ["Hedderich", "Dennis", ""]]}, {"id": "2008.03467", "submitter": "Haitao Zhang", "authors": "Ziang Ma, Linyuan Wang, Haitao Zhang, Wei Lu and Jun Yin", "title": "RPT: Learning Point Set Representation for Siamese Visual Tracking", "comments": "Accepted to ECCV2020 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While remarkable progress has been made in robust visual tracking, accurate\ntarget state estimation still remains a highly challenging problem. In this\npaper, we argue that this issue is closely related to the prevalent bounding\nbox representation, which provides only a coarse spatial extent of object. Thus\nan effcient visual tracking framework is proposed to accurately estimate the\ntarget state with a finer representation as a set of representative points. The\npoint set is trained to indicate the semantically and geometrically significant\npositions of target region, enabling more fine-grained localization and\nmodeling of object appearance. We further propose a multi-level aggregation\nstrategy to obtain detailed structure information by fusing hierarchical\nconvolution layers. Extensive experiments on several challenging benchmarks\nincluding OTB2015, VOT2018, VOT2019 and GOT-10k demonstrate that our method\nachieves new state-of-the-art performance while running at over 20 FPS.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 07:42:58 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 01:27:02 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Ma", "Ziang", ""], ["Wang", "Linyuan", ""], ["Zhang", "Haitao", ""], ["Lu", "Wei", ""], ["Yin", "Jun", ""]]}, {"id": "2008.03470", "submitter": "Yulia Sandamirskaya", "authors": "Sandro Baumgartner, Alpha Renner, Raphaela Kreiser, Dongchen Liang,\n  Giacomo Indiveri, Yulia Sandamirskaya", "title": "Visual Pattern Recognition with on On-chip Learning: towards a Fully\n  Neuromorphic Approach", "comments": "5 pages. Accepted to ISCAS 2020 conference", "journal-ref": null, "doi": "10.1109/ISCAS45731.2020.9180628", "report-no": null, "categories": "cs.NE cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a spiking neural network (SNN) for visual pattern recognition with\non-chip learning on neuromorphichardware. We show how this network can learn\nsimple visual patterns composed of horizontal and vertical bars sensed by a\nDynamic Vision Sensor, using a local spike-based plasticity rule. During\nrecognition, the network classifies the pattern's identity while at the same\ntime estimating its location and scale. We build on previous work that used\nlearning with neuromorphic hardware in the loop and demonstrate that the\nproposed network can properly operate with on-chip learning, demonstrating a\ncomplete neuromorphic pattern learning and recognition setup. Our results show\nthat the network is robust against noise on the input (no accuracy drop when\nadding 130% noise) and against up to 20% noise in the neuron parameters.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 08:07:36 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Baumgartner", "Sandro", ""], ["Renner", "Alpha", ""], ["Kreiser", "Raphaela", ""], ["Liang", "Dongchen", ""], ["Indiveri", "Giacomo", ""], ["Sandamirskaya", "Yulia", ""]]}, {"id": "2008.03473", "submitter": "Perla Mayo", "authors": "Perla Mayo, Oktay Karaku\\c{s}, Robin Holmes and Alin Achim", "title": "Representation Learning via Cauchy Convolutional Sparse Coding", "comments": "19 pages, 9 figures, journal draft", "journal-ref": null, "doi": "10.1109/ACCESS.2021.3096643", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In representation learning, Convolutional Sparse Coding (CSC) enables\nunsupervised learning of features by jointly optimising both an \\(\\ell_2\\)-norm\nfidelity term and a sparsity enforcing penalty. This work investigates using a\nregularisation term derived from an assumed Cauchy prior for the coefficients\nof the feature maps of a CSC generative model. The sparsity penalty term\nresulting from this prior is solved via its proximal operator, which is then\napplied iteratively, element-wise, on the coefficients of the feature maps to\noptimise the CSC cost function. The performance of the proposed Iterative\nCauchy Thresholding (ICT) algorithm in reconstructing natural images is\ncompared against the common choice of \\(\\ell_1\\)-norm optimised via soft and\nhard thresholding. ICT outperforms IHT and IST in most of these reconstruction\nexperiments across various datasets, with an average PSNR of up to 11.30 and\n7.04 above ISTA and IHT respectively.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 08:21:44 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Mayo", "Perla", ""], ["Karaku\u015f", "Oktay", ""], ["Holmes", "Robin", ""], ["Achim", "Alin", ""]]}, {"id": "2008.03509", "submitter": "Zhipu Liu", "authors": "Zhipu Liu, Lei Zhang, Yang Yang", "title": "Hierarchical Bi-Directional Feature Perception Network for Person\n  Re-Identification", "comments": "Accepted by ACM MM2020", "journal-ref": null, "doi": "10.1145/3394171.3413689", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous Person Re-Identification (Re-ID) models aim to focus on the most\ndiscriminative region of an image, while its performance may be compromised\nwhen that region is missing caused by camera viewpoint changes or occlusion. To\nsolve this issue, we propose a novel model named Hierarchical Bi-directional\nFeature Perception Network (HBFP-Net) to correlate multi-level information and\nreinforce each other. First, the correlation maps of cross-level feature-pairs\nare modeled via low-rank bilinear pooling. Then, based on the correlation maps,\nBi-directional Feature Perception (BFP) module is employed to enrich the\nattention regions of high-level feature, and to learn abstract and specific\ninformation in low-level feature. And then, we propose a novel end-to-end\nhierarchical network which integrates multi-level augmented features and inputs\nthe augmented low- and middle-level features to following layers to retrain a\nnew powerful network. What's more, we propose a novel trainable generalized\npooling, which can dynamically select any value of all locations in feature\nmaps to be activated. Extensive experiments implemented on the mainstream\nevaluation datasets including Market-1501, CUHK03 and DukeMTMC-ReID show that\nour method outperforms the recent SOTA Re-ID models.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 12:33:32 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Liu", "Zhipu", ""], ["Zhang", "Lei", ""], ["Yang", "Yang", ""]]}, {"id": "2008.03511", "submitter": "KeYang Wang", "authors": "Keyang Wang and Lei Zhang", "title": "Single-Shot Two-Pronged Detector with Rectified IoU Loss", "comments": "Accepted by ACM MM 2020", "journal-ref": null, "doi": "10.1145/3394171.3413691", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the CNN based object detectors, feature pyramids are widely exploited to\nalleviate the problem of scale variation across object instances. These object\ndetectors, which strengthen features via a top-down pathway and lateral\nconnections, are mainly to enrich the semantic information of low-level\nfeatures, but ignore the enhancement of high-level features. This can lead to\nan imbalance between different levels of features, in particular a serious lack\nof detailed information in the high-level features, which makes it difficult to\nget accurate bounding boxes. In this paper, we introduce a novel two-pronged\ntransductive idea to explore the relationship among different layers in both\nbackward and forward directions, which can enrich the semantic information of\nlow-level features and detailed information of high-level features at the same\ntime. Under the guidance of the two-pronged idea, we propose a Two-Pronged\nNetwork (TPNet) to achieve bidirectional transfer between high-level features\nand low-level features, which is useful for accurately detecting object at\ndifferent scales. Furthermore, due to the distribution imbalance between the\nhard and easy samples in single-stage detectors, the gradient of localization\nloss is always dominated by the hard examples that have poor localization\naccuracy. This will enable the model to be biased toward the hard samples. So\nin our TPNet, an adaptive IoU based localization loss, named Rectified IoU\n(RIoU) loss, is proposed to rectify the gradients of each kind of samples. The\nRectified IoU loss increases the gradients of examples with high IoU while\nsuppressing the gradients of examples with low IoU, which can improve the\noverall localization accuracy of model. Extensive experiments demonstrate the\nsuperiority of our TPNet and RIoU loss.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 12:36:55 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Wang", "Keyang", ""], ["Zhang", "Lei", ""]]}, {"id": "2008.03512", "submitter": "Zhongzhou Zhang", "authors": "Zhongzhou Zhang, Lei Zhang", "title": "Hard Negative Samples Emphasis Tracker without Anchors", "comments": "accepted by ACM Mutlimedia Conference, 2020", "journal-ref": null, "doi": "10.1145/3394171.3413692", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trackers based on Siamese network have shown tremendous success, because of\ntheir balance between accuracy and speed. Nevertheless, with tracking scenarios\nbecoming more and more sophisticated, most existing Siamese-based approaches\nignore the addressing of the problem that distinguishes the tracking target\nfrom hard negative samples in the tracking phase. The features learned by these\nnetworks lack of discrimination, which significantly weakens the robustness of\nSiamese-based trackers and leads to suboptimal performance. To address this\nissue, we propose a simple yet efficient hard negative samples emphasis method,\nwhich constrains Siamese network to learn features that are aware of hard\nnegative samples and enhance the discrimination of embedding features. Through\na distance constraint, we force to shorten the distance between exemplar vector\nand positive vectors, meanwhile, enlarge the distance between exemplar vector\nand hard negative vectors. Furthermore, we explore a novel anchor-free tracking\nframework in a per-pixel prediction fashion, which can significantly reduce the\nnumber of hyper-parameters and simplify the tracking process by taking full\nadvantage of the representation of convolutional neural network. Extensive\nexperiments on six standard benchmark datasets demonstrate that the proposed\nmethod can perform favorable results against state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 12:38:38 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Zhang", "Zhongzhou", ""], ["Zhang", "Lei", ""]]}, {"id": "2008.03515", "submitter": "Baozhou Zhu", "authors": "Baozhou Zhu, Zaid Al-Ars, Peter Hofstee", "title": "NASB: Neural Architecture Search for Binary Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary Convolutional Neural Networks (CNNs) have significantly reduced the\nnumber of arithmetic operations and the size of memory storage needed for CNNs,\nwhich makes their deployment on mobile and embedded systems more feasible.\nHowever, the CNN architecture after binarizing requires to be redesigned and\nrefined significantly due to two reasons: 1. the large accumulation error of\nbinarization in the forward propagation, and 2. the severe gradient mismatch\nproblem of binarization in the backward propagation. Even though the\nsubstantial effort has been invested in designing architectures for single and\nmultiple binary CNNs, it is still difficult to find an optimal architecture for\nbinary CNNs. In this paper, we propose a strategy, named NASB, which adopts\nNeural Architecture Search (NAS) to find an optimal architecture for the\nbinarization of CNNs. Due to the flexibility of this automated strategy, the\nobtained architecture is not only suitable for binarization but also has low\noverhead, achieving a better trade-off between the accuracy and computational\ncomplexity of hand-optimized binary CNNs. The implementation of NASB strategy\nis evaluated on the ImageNet dataset and demonstrated as a better solution\ncompared to existing quantized CNNs. With the insignificant overhead increase,\nNASB outperforms existing single and multiple binary CNNs by up to 4.0% and\n1.0% Top-1 accuracy respectively, bringing them closer to the precision of\ntheir full precision counterpart. The code and pretrained models will be\npublicly available.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 13:06:11 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Zhu", "Baozhou", ""], ["Al-Ars", "Zaid", ""], ["Hofstee", "Peter", ""]]}, {"id": "2008.03520", "submitter": "Baozhou Zhu", "authors": "Baozhou Zhu, Zaid Al-Ars, Wei Pan", "title": "Towards Lossless Binary Convolutional Neural Networks Using Piecewise\n  Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary Convolutional Neural Networks (CNNs) can significantly reduce the\nnumber of arithmetic operations and the size of memory storage, which makes the\ndeployment of CNNs on mobile or embedded systems more promising. However, the\naccuracy degradation of single and multiple binary CNNs is unacceptable for\nmodern architectures and large scale datasets like ImageNet. In this paper, we\nproposed a Piecewise Approximation (PA) scheme for multiple binary CNNs which\nlessens accuracy loss by approximating full precision weights and activations\nefficiently and maintains parallelism of bitwise operations to guarantee\nefficiency. Unlike previous approaches, the proposed PA scheme segments\npiece-wisely the full precision weights and activations, and approximates each\npiece with a scaling coefficient. Our implementation on ResNet with different\ndepths on ImageNet can reduce both Top-1 and Top-5 classification accuracy gap\ncompared with full precision to approximately 1.0%. Benefited from the\nbinarization of the downsampling layer, our proposed PA-ResNet50 requires less\nmemory usage and two times Flops than single binary CNNs with 4 weights and 5\nactivations bases. The PA scheme can also generalize to other architectures\nlike DenseNet and MobileNet with similar approximation power as ResNet which is\npromising for other tasks using binary convolutions. The code and pretrained\nmodels will be publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 13:32:33 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2020 19:06:19 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Zhu", "Baozhou", ""], ["Al-Ars", "Zaid", ""], ["Pan", "Wei", ""]]}, {"id": "2008.03522", "submitter": "Rohit Keshari", "authors": "Rohit Keshari, Soumyadeep Ghosh, Saheb Chhabra, Mayank Vatsa, Richa\n  Singh", "title": "Unravelling Small Sample Size Problems in the Deep Learning World", "comments": "3 figures, 2 tables, accepted in BigMM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growth and success of deep learning approaches can be attributed to two\nmajor factors: availability of hardware resources and availability of large\nnumber of training samples. For problems with large training databases, deep\nlearning models have achieved superlative performances. However, there are a\nlot of \\textit{small sample size or $S^3$} problems for which it is not\nfeasible to collect large training databases. It has been observed that deep\nlearning models do not generalize well on $S^3$ problems and specialized\nsolutions are required. In this paper, we first present a review of deep\nlearning algorithms for small sample size problems in which the algorithms are\nsegregated according to the space in which they operate, i.e. input space,\nmodel space, and feature space. Secondly, we present Dynamic Attention Pooling\napproach which focuses on extracting global information from the most\ndiscriminative sub-part of the feature map. The performance of the proposed\ndynamic attention pooling is analyzed with state-of-the-art ResNet model on\nrelatively small publicly available datasets such as SVHN, C10, C100, and\nTinyImageNet.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 13:35:49 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Keshari", "Rohit", ""], ["Ghosh", "Soumyadeep", ""], ["Chhabra", "Saheb", ""], ["Vatsa", "Mayank", ""], ["Singh", "Richa", ""]]}, {"id": "2008.03529", "submitter": "Zhiwen Zuo", "authors": "Zhiwen Zuo, Lei Zhao, Zhizhong Wang, Haibo Chen, Ailin Li, Qijiang Xu,\n  Wei Xing, Dongming Lu", "title": "Multimodal Image-to-Image Translation via Mutual Information Estimation\n  and Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal image-to-image translation (I2IT) aims to learn a conditional\ndistribution that explores multiple possible images in the target domain given\nan input image in the source domain. Conditional generative adversarial\nnetworks (cGANs) are often adopted for modeling such a conditional\ndistribution. However, cGANs are prone to ignore the latent code and learn a\nunimodal distribution in conditional image synthesis, which is also known as\nthe mode collapse issue of GANs. To solve the problem, we propose a simple yet\neffective method that explicitly estimates and maximizes the mutual information\nbetween the latent code and the output image in cGANs by using a deep mutual\ninformation neural estimator in this paper. Maximizing the mutual information\nstrengthens the statistical dependency between the latent code and the output\nimage, which prevents the generator from ignoring the latent code and\nencourages cGANs to fully utilize the latent code for synthesizing diverse\nresults. Our method not only provides a new perspective from information theory\nto improve diversity for I2IT but also achieves disentanglement between the\nsource domain content and the target domain style for free.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 14:09:23 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 15:34:46 GMT"}, {"version": "v3", "created": "Mon, 24 Aug 2020 08:29:29 GMT"}, {"version": "v4", "created": "Mon, 31 Aug 2020 04:08:50 GMT"}, {"version": "v5", "created": "Tue, 1 Sep 2020 10:49:43 GMT"}, {"version": "v6", "created": "Sun, 6 Sep 2020 04:17:36 GMT"}, {"version": "v7", "created": "Sat, 8 May 2021 14:15:56 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zuo", "Zhiwen", ""], ["Zhao", "Lei", ""], ["Wang", "Zhizhong", ""], ["Chen", "Haibo", ""], ["Li", "Ailin", ""], ["Xu", "Qijiang", ""], ["Xing", "Wei", ""], ["Lu", "Dongming", ""]]}, {"id": "2008.03533", "submitter": "Tran Thien Dat Nguyen", "authors": "Hamid Rezatofighi, Tran Thien Dat Nguyen, Ba-Ngu Vo, Ba-Tuong Vo,\n  Silvio Savarese, Ian Reid", "title": "How Trustworthy are Performance Evaluations for Basic Vision Tasks?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines performance evaluation criteria for basic vision tasks\ninvolving sets of objects namely, object detection,instance-level segmentation\nand multi-object tracking. The rankings of algorithms by an existing criterion\ncan fluctuate with different choices of parameters,e.g.Intersection over Union\n(IoU) threshold, making their evaluations unreliable. More importantly, there\nis no means to verify whether we can trust the evaluations of a criterion. This\nwork suggests a notion of trustworthiness for performance criteria, which\nrequires (i) robustness to parameters for reliability, (ii) contextual\nmeaningfulness in sanity tests, and (iii) consistency with mathematical\nrequirements such as the metric properties. We observe that these requirements\nwere overlooked by many widely-used criteria, and explore alternative criteria\nusing metrics for sets of shapes. We also assess all these criteria based on\nthe suggested requirements for trustworthiness.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 14:21:15 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 14:30:38 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 11:45:13 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Rezatofighi", "Hamid", ""], ["Nguyen", "Tran Thien Dat", ""], ["Vo", "Ba-Ngu", ""], ["Vo", "Ba-Tuong", ""], ["Savarese", "Silvio", ""], ["Reid", "Ian", ""]]}, {"id": "2008.03539", "submitter": "Ioannis Kansizoglou", "authors": "Ioannis Kansizoglou, Nicholas Santavas, Loukas Bampis and Antonios\n  Gasteratos", "title": "HASeparator: Hyperplane-Assisted Softmax", "comments": "Submitted to IEEE ICMLA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Efficient feature learning with Convolutional Neural Networks (CNNs)\nconstitutes an increasingly imperative property since several challenging tasks\nof computer vision tend to require cascade schemes and modalities fusion.\nFeature learning aims at CNN models capable of extracting embeddings,\nexhibiting high discrimination among the different classes, as well as\nintra-class compactness. In this paper, a novel approach is introduced that has\nseparator, which focuses on an effective hyperplane-based segregation of the\nclasses instead of the common class centers separation scheme. Accordingly, an\ninnovatory separator, namely the Hyperplane-Assisted Softmax separator\n(HASeparator), is proposed that demonstrates superior discrimination\ncapabilities, as evaluated on popular image classification benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 15:24:56 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Kansizoglou", "Ioannis", ""], ["Santavas", "Nicholas", ""], ["Bampis", "Loukas", ""], ["Gasteratos", "Antonios", ""]]}, {"id": "2008.03546", "submitter": "Anyi Rao", "authors": "Jiangyue Xia, Anyi Rao, Qingqiu Huang, Linning Xu, Jiangtao Wen, Dahua\n  Lin", "title": "Online Multi-modal Person Search in Videos", "comments": "ECCV2020. Project page:\n  http://movienet.site/projects/eccv20onlineperson.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of searching certain people in videos has seen increasing potential\nin real-world applications, such as video organization and editing. Most\nexisting approaches are devised to work in an offline manner, where identities\ncan only be inferred after an entire video is examined. This working manner\nprecludes such methods from being applied to online services or those\napplications that require real-time responses. In this paper, we propose an\nonline person search framework, which can recognize people in a video on the\nfly. This framework maintains a multimodal memory bank at its heart as the\nbasis for person recognition, and updates it dynamically with a policy obtained\nby reinforcement learning. Our experiments on a large movie dataset show that\nthe proposed method is effective, not only achieving remarkable improvements\nover online schemes but also outperforming offline methods.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 15:48:32 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Xia", "Jiangyue", ""], ["Rao", "Anyi", ""], ["Huang", "Qingqiu", ""], ["Xu", "Linning", ""], ["Wen", "Jiangtao", ""], ["Lin", "Dahua", ""]]}, {"id": "2008.03548", "submitter": "Anyi Rao", "authors": "Anyi Rao, Jiaze Wang, Linning Xu, Xuekun Jiang, Qingqiu Huang, Bolei\n  Zhou, Dahua Lin", "title": "A Unified Framework for Shot Type Classification Based on Subject\n  Centric Lens", "comments": "ECCV2020. Project page: https://anyirao.com/projects/ShotType.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shots are key narrative elements of various videos, e.g. movies, TV series,\nand user-generated videos that are thriving over the Internet. The types of\nshots greatly influence how the underlying ideas, emotions, and messages are\nexpressed. The technique to analyze shot types is important to the\nunderstanding of videos, which has seen increasing demand in real-world\napplications in this era. Classifying shot type is challenging due to the\nadditional information required beyond the video content, such as the spatial\ncomposition of a frame and camera movement. To address these issues, we propose\na learning framework Subject Guidance Network (SGNet) for shot type\nrecognition. SGNet separates the subject and background of a shot into two\nstreams, serving as separate guidance maps for scale and movement type\nclassification respectively. To facilitate shot type analysis and model\nevaluations, we build a large-scale dataset MovieShots, which contains 46K\nshots from 7K movie trailers with annotations of their scale and movement\ntypes. Experiments show that our framework is able to recognize these two\nattributes of shot accurately, outperforming all the previous methods.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 15:49:40 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Rao", "Anyi", ""], ["Wang", "Jiaze", ""], ["Xu", "Linning", ""], ["Jiang", "Xuekun", ""], ["Huang", "Qingqiu", ""], ["Zhou", "Bolei", ""], ["Lin", "Dahua", ""]]}, {"id": "2008.03549", "submitter": "Italos de Souza", "authors": "Italos Estilon de Souza and Alexandre Xavier Falc\\~ao", "title": "Learning CNN filters from user-drawn image markers for coconut-tree\n  image classification", "comments": null, "journal-ref": null, "doi": "10.1109/LGRS.2020.3020098", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying species of trees in aerial images is essential for land-use\nclassification, plantation monitoring, and impact assessment of natural\ndisasters. The manual identification of trees in aerial images is tedious,\ncostly, and error-prone, so automatic classification methods are necessary.\nConvolutional Neural Network (CNN) models have well succeeded in image\nclassification applications from different domains. However, CNN models usually\nrequire intensive manual annotation to create large training sets. One may\nconceptually divide a CNN into convolutional layers for feature extraction and\nfully connected layers for feature space reduction and classification. We\npresent a method that needs a minimal set of user-selected images to train the\nCNN's feature extractor, reducing the number of required images to train the\nfully connected layers. The method learns the filters of each convolutional\nlayer from user-drawn markers in image regions that discriminate classes,\nallowing better user control and understanding of the training process. It does\nnot rely on optimization based on backpropagation, and we demonstrate its\nadvantages on the binary classification of coconut-tree aerial images against\none of the most popular CNN models.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 15:50:23 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 23:02:43 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["de Souza", "Italos Estilon", ""], ["Falc\u00e3o", "Alexandre Xavier", ""]]}, {"id": "2008.03553", "submitter": "Morteza Babaie", "authors": "Aditya Sriram, Shivam Kalra, Morteza Babaie, Brady Kieffer, Waddah Al\n  Drobi, Shahryar Rahnamayan, Hany Kashani, Hamid R. Tizhoosh", "title": "Forming Local Intersections of Projections for Classifying and Searching\n  Histopathology Images", "comments": "To appear in International Conference on AI in Medicine (AIME 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel image descriptor called Forming Local\nIntersections of Projections (FLIP) and its multi-resolution version (mFLIP)\nfor representing histopathology images. The descriptor is based on the Radon\ntransform wherein we apply parallel projections in small local neighborhoods of\ngray-level images. Using equidistant projection directions in each window, we\nextract unique and invariant characteristics of the neighborhood by taking the\nintersection of adjacent projections. Thereafter, we construct a histogram for\neach image, which we call the FLIP histogram. Various resolutions provide\ndifferent FLIP histograms which are then concatenated to form the mFLIP\ndescriptor. Our experiments included training common networks from scratch and\nfine-tuning pre-trained networks to benchmark our proposed descriptor.\nExperiments are conducted on the publicly available dataset KIMIA Path24 and\nKIMIA Path960. For both of these datasets, FLIP and mFLIP descriptors show\npromising results in all experiments.Using KIMIA Path24 data, FLIP outperformed\nnon-fine-tuned Inception-v3 and fine-tuned VGG16 and mFLIP outperformed\nfine-tuned Inception-v3 in feature extracting.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 16:32:04 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Sriram", "Aditya", ""], ["Kalra", "Shivam", ""], ["Babaie", "Morteza", ""], ["Kieffer", "Brady", ""], ["Drobi", "Waddah Al", ""], ["Rahnamayan", "Shahryar", ""], ["Kashani", "Hany", ""], ["Tizhoosh", "Hamid R.", ""]]}, {"id": "2008.03555", "submitter": "Sandeep Inuganti", "authors": "Sandeep Inuganti, Vineeth N Balasubramanian", "title": "Assisting Scene Graph Generation with Self-Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in scene graph generation has quickly gained traction in the past\nfew years because of its potential to help in downstream tasks like visual\nquestion answering, image captioning, etc. Many interesting approaches have\nbeen proposed to tackle this problem. Most of these works have a pre-trained\nobject detection model as a preliminary feature extractor. Therefore, getting\nobject bounding box proposals from the object detection model is relatively\ncheaper. We take advantage of this ready availability of bounding box\nannotations produced by the pre-trained detector. We propose a set of three\nnovel yet simple self-supervision tasks and train them as auxiliary multi-tasks\nto the main model. While comparing, we train the base-model from scratch with\nthese self-supervision tasks, we achieve state-of-the-art results in all the\nmetrics and recall settings. We also resolve some of the confusion between two\ntypes of relationships: geometric and possessive, by training the model with\nthe proposed self-supervision losses. We use the benchmark dataset, Visual\nGenome to conduct our experiments and show our results.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 16:38:03 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Inuganti", "Sandeep", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "2008.03560", "submitter": "Cihan \\\"Ong\\\"un", "authors": "Cihan \\\"Ong\\\"un, Alptekin Temizel", "title": "LPMNet: Latent Part Modification and Generation for 3D Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on latent modification and generation of 3D point\ncloud object models with respect to their semantic parts. Different to the\nexisting methods which use separate networks for part generation and assembly,\nwe propose a single end-to-end Autoencoder model that can handle generation and\nmodification of both semantic parts, and global shapes. The proposed method\nsupports part exchange between 3D point cloud models and composition by\ndifferent parts to form new models by directly editing latent representations.\nThis holistic approach does not need part-based training to learn part\nrepresentations and does not introduce any extra loss besides the standard\nreconstruction loss. The experiments demonstrate the robustness of the proposed\nmethod with different object categories and varying number of points. The\nmethod can generate new models by integration of generative models such as GANs\nand VAEs and can work with unannotated point clouds by integration of a\nsegmentation module.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 17:24:37 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 12:41:37 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 15:44:08 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["\u00d6ng\u00fcn", "Cihan", ""], ["Temizel", "Alptekin", ""]]}, {"id": "2008.03561", "submitter": "Elahe Vahdani", "authors": "Longlong Jing and Elahe Vahdani and Jiaxing Tan and Yingli Tian", "title": "Cross-modal Center Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal retrieval aims to learn discriminative and modal-invariant\nfeatures for data from different modalities. Unlike the existing methods which\nusually learn from the features extracted by offline networks, in this paper,\nwe propose an approach to jointly train the components of cross-modal retrieval\nframework with metadata, and enable the network to find optimal features. The\nproposed end-to-end framework is updated with three loss functions: 1) a novel\ncross-modal center loss to eliminate cross-modal discrepancy, 2) cross-entropy\nloss to maximize inter-class variations, and 3) mean-square-error loss to\nreduce modality variations. In particular, our proposed cross-modal center loss\nminimizes the distances of features from objects belonging to the same class\nacross all modalities. Extensive experiments have been conducted on the\nretrieval tasks across multi-modalities, including 2D image, 3D point cloud,\nand mesh data. The proposed framework significantly outperforms the\nstate-of-the-art methods on the ModelNet40 dataset.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 17:26:35 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Jing", "Longlong", ""], ["Vahdani", "Elahe", ""], ["Tan", "Jiaxing", ""], ["Tian", "Yingli", ""]]}, {"id": "2008.03580", "submitter": "Hong Wang", "authors": "Hong Wang, Zongsheng Yue, Qi Xie, Qian Zhao, Yefeng Zheng, Deyu Meng", "title": "From Rain Generation to Rain Removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the single image rain removal (SIRR) task, the performance of deep\nlearning (DL)-based methods is mainly affected by the designed deraining models\nand training datasets. Most of current state-of-the-art focus on constructing\npowerful deep models to obtain better deraining results. In this paper, to\nfurther improve the deraining performance, we novelly attempt to handle the\nSIRR task from the perspective of training datasets by exploring a more\nefficient way to synthesize rainy images. Specifically, we build a full\nBayesian generative model for rainy image where the rain layer is parameterized\nas a generator with the input as some latent variables representing the\nphysical structural rain factors, e.g., direction, scale, and thickness. To\nsolve this model, we employ the variational inference framework to approximate\nthe expected statistical distribution of rainy image in a data-driven manner.\nWith the learned generator, we can automatically and sufficiently generate\ndiverse and non-repetitive training pairs so as to efficiently enrich and\naugment the existing benchmark datasets. User study qualitatively and\nquantitatively evaluates the realism of generated rainy images. Comprehensive\nexperiments substantiate that the proposed model can faithfully extract the\ncomplex rain distribution that not only helps significantly improve the\nderaining performance of current deep single image derainers, but also largely\nloosens the requirement of large training sample pre-collection for the SIRR\ntask.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 18:56:51 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 06:57:25 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Wang", "Hong", ""], ["Yue", "Zongsheng", ""], ["Xie", "Qi", ""], ["Zhao", "Qian", ""], ["Zheng", "Yefeng", ""], ["Meng", "Deyu", ""]]}, {"id": "2008.03592", "submitter": "Sefik Emre Eskimez", "authors": "Sefik Emre Eskimez, You Zhang, Zhiyao Duan", "title": "Speech Driven Talking Face Generation from a Single Image and an Emotion\n  Condition", "comments": "Accepted to IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual emotion expression plays an important role in audiovisual speech\ncommunication. In this work, we propose a novel approach to rendering visual\nemotion expression in speech-driven talking face generation. Specifically, we\ndesign an end-to-end talking face generation system that takes a speech\nutterance, a single face image, and a categorical emotion label as input to\nrender a talking face video synchronized with the speech and expressing the\nconditioned emotion. Objective evaluation on image quality, audiovisual\nsynchronization, and visual emotion expression shows that the proposed system\noutperforms a state-of-the-art baseline system. Subjective evaluation of visual\nemotion expression and video realness also demonstrates the superiority of the\nproposed system. Furthermore, we conduct a human emotion recognition pilot\nstudy using generated videos with mismatched emotions among the audio and\nvisual modalities. Results show that humans respond to the visual modality more\nsignificantly than the audio modality on this task.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 20:46:31 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 22:45:01 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Eskimez", "Sefik Emre", ""], ["Zhang", "You", ""], ["Duan", "Zhiyao", ""]]}, {"id": "2008.03609", "submitter": "Linhai Ma", "authors": "Linhai Ma, Liang Liang", "title": "Enhance CNN Robustness Against Noises for Classification of 12-Lead ECG\n  with Variable Length", "comments": "This paper is accepted by 19TH IEEE International Conference on\n  Machine Learning and Applications (ICMLA2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electrocardiogram (ECG) is the most widely used diagnostic tool to monitor\nthe condition of the cardiovascular system. Deep neural networks (DNNs), have\nbeen developed in many research labs for automatic interpretation of ECG\nsignals to identify potential abnormalities in patient hearts. Studies have\nshown that given a sufficiently large amount of data, the classification\naccuracy of DNNs could reach human-expert cardiologist level. However, despite\nof the excellent performance in classification accuracy, it has been shown that\nDNNs are highly vulnerable to adversarial noises which are subtle changes in\ninput of a DNN and lead to a wrong class-label prediction with a high\nconfidence. Thus, it is challenging and essential to improve robustness of DNNs\nagainst adversarial noises for ECG signal classification, a life-critical\napplication. In this work, we designed a CNN for classification of 12-lead ECG\nsignals with variable length, and we applied three defense methods to improve\nrobustness of this CNN for this classification task. The ECG data in this study\nis very challenging because the sample size is limited, and the length of each\nECG recording varies in a large range. The evaluation results show that our\ncustomized CNN reached satisfying F1 score and average accuracy, comparable to\nthe top-6 entries in the CPSC2018 ECG classification challenge, and the defense\nmethods enhanced robustness of our CNN against adversarial noises and white\nnoises, with a minimal reduction in accuracy on clean data.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 22:21:24 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 21:04:46 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2020 20:04:38 GMT"}, {"version": "v4", "created": "Mon, 30 Nov 2020 20:09:42 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Ma", "Linhai", ""], ["Liang", "Liang", ""]]}, {"id": "2008.03614", "submitter": "Muhammad Siraj", "authors": "Constantinou Miti, Demetriou Zatte, Siraj Sajid Gondal", "title": "Tracking in Crowd is Challenging: Analyzing Crowd based on Physical\n  Characteristics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, the safety of people has become a very important problem in\ndifferent places including subway station, universities, colleges, airport,\nshopping mall and square, city squares. Therefore, considering intelligence\nevent detection systems is more and urgently required. The event detection\nmethod is developed to identify abnormal behavior intelligently, so public can\ntake action as soon as possible to prevent unwanted activities. The problem is\nvery challenging due to high crowd density in different areas. One of these\nissues is occlusion due to which individual tracking and analysis becomes\nimpossible as shown in Fig. 1. Secondly, more challenging is the proper\nrepresentation of individual behavior in the crowd. We consider a novel method\nto deal with these challenges. Considering the challenge of tracking, we\npartition complete frame into smaller patches, and extract motion pattern to\ndemonstrate the motion in each individual patch. For this purpose, our work\ntakes into account KLT corners as consolidated features to describe moving\nregions and track these features by considering optical flow method. To embed\nmotion patterns, we develop and consider the distribution of all motion\ninformation in a patch as Gaussian distribution, and formulate parameters of\nGaussian model as our motion pattern descriptor.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 22:42:25 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Miti", "Constantinou", ""], ["Zatte", "Demetriou", ""], ["Gondal", "Siraj Sajid", ""]]}, {"id": "2008.03628", "submitter": "Lijun Wang", "authors": "Lijun Wang, Yanting Zhu, Jue Shi, Xiaodan Fan", "title": "Appearance-free Tripartite Matching for Multiple Object Tracking", "comments": "30 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Object Tracking (MOT) detects the trajectories of multiple objects\ngiven an input video, and it has become more and more popular in various\nresearch and industry areas, such as cell tracking for biomedical research and\nhuman tracking in video surveillance. We target at the general MOT problem\nregardless of the object appearance. The appearance-free tripartite matching is\nproposed to avoid the irregular velocity problem of traditional bipartite\nmatching. The tripartite matching is formulated as maximizing the likelihood of\nthe state vectors constituted of the position and velocity of objects, and a\ndynamic programming algorithm is employed to solve such maximum likelihood\nestimate (MLE). To overcome the high computational cost induced by the vast\nsearch space of dynamic programming, we decompose the space by the number of\ndisappearing objects and propose a reduced-space approach by truncating the\ndecomposition. Extensive simulations have shown the superiority and efficiency\nof our proposed method. We also applied our method to track the motion of\nnatural killer cells around tumor cells in a cancer research.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 02:16:44 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Wang", "Lijun", ""], ["Zhu", "Yanting", ""], ["Shi", "Jue", ""], ["Fan", "Xiaodan", ""]]}, {"id": "2008.03632", "submitter": "Kang Zhou", "authors": "Kang Zhou, Yuting Xiao, Jianlong Yang, Jun Cheng, Wen Liu, Weixin Luo,\n  Zaiwang Gu, Jiang Liu, Shenghua Gao", "title": "Encoding Structure-Texture Relation with P-Net for Anomaly Detection in\n  Retinal Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection in retinal image refers to the identification of\nabnormality caused by various retinal diseases/lesions, by only leveraging\nnormal images in training phase. Normal images from healthy subjects often have\nregular structures (e.g., the structured blood vessels in the fundus image, or\nstructured anatomy in optical coherence tomography image). On the contrary, the\ndiseases and lesions often destroy these structures. Motivated by this, we\npropose to leverage the relation between the image texture and structure to\ndesign a deep neural network for anomaly detection. Specifically, we first\nextract the structure of the retinal images, then we combine both the structure\nfeatures and the last layer features extracted from original health image to\nreconstruct the original input healthy image. The image feature provides the\ntexture information and guarantees the uniqueness of the image recovered from\nthe structure. In the end, we further utilize the reconstructed image to\nextract the structure and measure the difference between structure extracted\nfrom original and the reconstructed image. On the one hand, minimizing the\nreconstruction difference behaves like a regularizer to guarantee that the\nimage is corrected reconstructed. On the other hand, such structure difference\ncan also be used as a metric for normality measurement. The whole network is\ntermed as P-Net because it has a ``P'' shape. Extensive experiments on RESC\ndataset and iSee dataset validate the effectiveness of our approach for anomaly\ndetection in retinal images. Further, our method also generalizes well to novel\nclass discovery in retinal images and anomaly detection in real-world images.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 02:59:33 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Zhou", "Kang", ""], ["Xiao", "Yuting", ""], ["Yang", "Jianlong", ""], ["Cheng", "Jun", ""], ["Liu", "Wen", ""], ["Luo", "Weixin", ""], ["Gu", "Zaiwang", ""], ["Liu", "Jiang", ""], ["Gao", "Shenghua", ""]]}, {"id": "2008.03633", "submitter": "Juan Luis Gonzalez Bello", "authors": "Juan Luis Gonzalez and Munchurl Kim", "title": "Forget About the LiDAR: Self-Supervised Depth Estimators with MED\n  Probability Volumes", "comments": "Accepted to NeurIPS2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised depth estimators have recently shown results comparable to\nthe supervised methods on the challenging single image depth estimation (SIDE)\ntask, by exploiting the geometrical relations between target and reference\nviews in the training data. However, previous methods usually learn forward or\nbackward image synthesis, but not depth estimation, as they cannot effectively\nneglect occlusions between the target and the reference images. Previous works\nrely on rigid photometric assumptions or the SIDE network to infer depth and\nocclusions, resulting in limited performance. On the other hand, we propose a\nmethod to \"Forget About the LiDAR\" (FAL), for the training of depth estimators,\nwith Mirrored Exponential Disparity (MED) probability volumes, from which we\nobtain geometrically inspired occlusion maps with our novel Mirrored Occlusion\nModule (MOM). Our MOM does not impose a burden on our FAL-net. Contrary to the\nprevious methods that learn SIDE from stereo pairs by regressing disparity in\nthe linear space, our FAL-net regresses disparity by binning it into the\nexponential space, which allows for better detection of distant and nearby\nobjects. We define a two-step training strategy for our FAL-net: It is first\ntrained for view synthesis and then fine-tuned for depth estimation with our\nMOM. Our FAL-net is remarkably light-weight and outperforms the previous\nstate-of-the-art methods with 8x fewer parameters and 3x faster inference\nspeeds on the challenging KITTI dataset. We present extensive experimental\nresults on the KITTI, CityScapes, and Make3D datasets to verify our method's\neffectiveness. To the authors' best knowledge, the presented method performs\nthe best among all the previous self-supervised methods until now.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 03:03:00 GMT"}, {"version": "v2", "created": "Sat, 26 Sep 2020 15:07:29 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Gonzalez", "Juan Luis", ""], ["Kim", "Munchurl", ""]]}, {"id": "2008.03646", "submitter": "Daniel De Marchi", "authors": "Daniel de Marchi, Amarjit Budhiraja", "title": "Augmenting Molecular Images with Vector Representations as a\n  Featurization Technique for Drug Classification", "comments": null, "journal-ref": "ICASSP 2020 - 2020 IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP)", "doi": "10.1109/ICASSP40776.2020.9053425", "report-no": null, "categories": "cs.CV q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key steps in building deep learning systems for drug\nclassification and generation is the choice of featurization for the molecules.\nPrevious featurization methods have included molecular images, binary strings,\ngraphs, and SMILES strings. This paper proposes the creation of molecular\nimages captioned with binary vectors that encode information not contained in\nor easily understood from a molecular image alone. Specifically, we use Morgan\nfingerprints, which encode higher level structural information, and MACCS keys,\nwhich encode yes or no questions about a molecules properties and structure. We\ntested our method on the HIV dataset published by the Pande lab, which consists\nof 41,127 molecules labeled by if they inhibit the HIV virus. Our final model\nachieved a state of the art AUC ROC on the HIV dataset, outperforming all other\nmethods. Moreover, the model converged significantly faster than most other\nmethods, requiring dramatically less computational power than unaugmented\nimages.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 04:26:16 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["de Marchi", "Daniel", ""], ["Budhiraja", "Amarjit", ""]]}, {"id": "2008.03669", "submitter": "Pierre-Alain Fayolle", "authors": "Markus Friedrich and Sebastian Feld and Thomy Phan and Pierre-Alain\n  Fayolle", "title": "Accelerating Evolutionary Construction Tree Extraction via Graph\n  Partitioning", "comments": "The 26th International Conference in Central Europe on Computer\n  Graphics, Visualization and Computer Vision 2016 in co-operation with\n  EUROGRAPHICS: University of West Bohemia, Plzen, Czech Republic May 28 - June\n  1 2018, p. 29-37", "journal-ref": null, "doi": "10.24132/CSRN.2018.2802.5", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting a Construction Tree from potentially noisy point clouds is an\nimportant aspect of Reverse Engineering tasks in Computer Aided Design.\nSolutions based on algorithmic geometry impose constraints on usable model\nrepresentations (e.g. quadric surfaces only) and noise robustness.\nRe-formulating the problem as a combinatorial optimization problem and solving\nit with an Evolutionary Algorithm can mitigate some of these constraints at the\ncost of increased computational complexity. This paper proposes a graph-based\nsearch space partitioning scheme that is able to accelerate Evolutionary\nConstruction Tree extraction while exploiting parallelization capabilities of\nmodern CPUs. The evaluation indicates a speed-up up to a factor of $46.6$\ncompared to the baseline approach while resulting tree sizes increased by\n$25.2\\%$ to $88.6\\%$.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 06:11:33 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Friedrich", "Markus", ""], ["Feld", "Sebastian", ""], ["Phan", "Thomy", ""], ["Fayolle", "Pierre-Alain", ""]]}, {"id": "2008.03673", "submitter": "Peng Chu", "authors": "Peng Chu and Xiao Bian and Shaopeng Liu and Haibin Ling", "title": "Feature Space Augmentation for Long-Tailed Data", "comments": "To be appeared in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world data often follow a long-tailed distribution as the frequency of\neach class is typically different. For example, a dataset can have a large\nnumber of under-represented classes and a few classes with more than sufficient\ndata. However, a model to represent the dataset is usually expected to have\nreasonably homogeneous performances across classes. Introducing class-balanced\nloss and advanced methods on data re-sampling and augmentation are among the\nbest practices to alleviate the data imbalance problem. However, the other part\nof the problem about the under-represented classes will have to rely on\nadditional knowledge to recover the missing information.\n  In this work, we present a novel approach to address the long-tailed problem\nby augmenting the under-represented classes in the feature space with the\nfeatures learned from the classes with ample samples. In particular, we\ndecompose the features of each class into a class-generic component and a\nclass-specific component using class activation maps. Novel samples of\nunder-represented classes are then generated on the fly during training stages\nby fusing the class-specific features from the under-represented classes with\nthe class-generic features from confusing classes. Our results on different\ndatasets such as iNaturalist, ImageNet-LT, Places-LT and a long-tailed version\nof CIFAR have shown the state of the art performances.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 06:38:00 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Chu", "Peng", ""], ["Bian", "Xiao", ""], ["Liu", "Shaopeng", ""], ["Ling", "Haibin", ""]]}, {"id": "2008.03685", "submitter": "Slimane Larabi", "authors": "Chayma Zatout, Slimane Larabi", "title": "Semantic scene synthesis: Application to assistive systems", "comments": "paper Not published", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this work is to provide a semantic scene synthesis from a single\ndepth image. This is used in assistive aid systems for visually impaired and\nblind people that allow them to understand their surroundings by the touch\nsense. The fact that blind people use touch to recognize objects and rely on\nlistening to replace sight, motivated us to propose this work. First, the\nacquired depth image is segmented and each segment is classified in the context\nof assistive systems using a deep learning network. Second, inspired by the\nBraille system and the Japanese writing system Kanji, the obtained classes are\ncoded with semantic labels. The scene is then synthesized using these labels\nand the extracted geometric features. Our system is able to predict more than\n17 classes only by understanding the provided illustrative labels. For the\nremaining objects, their geometric features are transmitted. The labels and the\ngeometric features are mapped on a synthesis area to be sensed by the touch\nsense. Experiments are conducted on noisy and incomplete data including\nacquired depth images of indoor scenes and public datasets. The obtained\nresults are reported and discussed.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 08:13:22 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 21:14:03 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Zatout", "Chayma", ""], ["Larabi", "Slimane", ""]]}, {"id": "2008.03694", "submitter": "Jiang Yue Dr.", "authors": "Jiang Yue, Weisong Wen, Jing Han, and Li-Ta Hsu", "title": "LiDAR Data Enrichment Using Deep Learning Based on High-Resolution\n  Image: An Approach to Achieve High-Performance LiDAR SLAM Using Low-cost\n  LiDAR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LiDAR-based SLAM algorithms are extensively studied to providing robust and\naccurate positioning for autonomous driving vehicles (ADV) in the past decades.\nSatisfactory performance can be obtained using high-grade 3D LiDAR with 64\nchannels, which can provide dense point clouds. Unfortunately, the high price\nsignificantly prevents its extensive commercialization in ADV. The\ncost-effective 3D LiDAR with 16 channels is a promising replacement. However,\nonly limited and sparse point clouds can be provided by the 16 channels LiDAR,\nwhich cannot guarantee sufficient positioning accuracy for ADV in challenging\ndynamic environments. The high-resolution image from the low-cost camera can\nprovide ample information about the surroundings. However, the explicit depth\ninformation is not available from the image. Inspired by the complementariness\nof 3D LiDAR and camera, this paper proposes to make use of the high-resolution\nimages from a camera to enrich the raw 3D point clouds from the low-cost 16\nchannels LiDAR based on a state-of-the-art deep learning algorithm. An ERFNet\nis firstly employed to segment the image with the aid of the raw sparse 3D\npoint clouds. Meanwhile, the sparse convolutional neural network is employed to\npredict the dense point clouds based on raw sparse 3D point clouds. Then, the\npredicted dense point clouds are fused with the segmentation outputs from\nERFnet using a novel multi-layer convolutional neural network to refine the\npredicted 3D point clouds. Finally, the enriched point clouds are employed to\nperform LiDAR SLAM based on the state-of-the-art normal distribution transform\n(NDT). We tested our approach on the re-edited KITTI datasets: (1)the sparse 3D\npoint clouds are significantly enriched with a mean square error of 1.1m MSE.\n(2)the map generated from the LiDAR SLAM is denser which includes more details\nwithout significant accuracy loss.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 09:20:47 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Yue", "Jiang", ""], ["Wen", "Weisong", ""], ["Han", "Jing", ""], ["Hsu", "Li-Ta", ""]]}, {"id": "2008.03696", "submitter": "Christopher Diehl", "authors": "Christopher Diehl, Eduard Feicho, Alexander Schwambach, Thomas\n  Dammeier, Eric Mares, Torsten Bertram", "title": "Radar-based Dynamic Occupancy Grid Mapping and Object Detection", "comments": "Accepted to be published as part of the 23rd IEEE International\n  Conference on Intelligent Transportation Systems (ITSC), Rhodes, Greece,\n  September 20-23, 2020", "journal-ref": null, "doi": "10.1109/ITSC45102.2020.9294626", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environment modeling utilizing sensor data fusion and object tracking is\ncrucial for safe automated driving. In recent years, the classical occupancy\ngrid map approach, which assumes a static environment, has been extended to\ndynamic occupancy grid maps, which maintain the possibility of a low-level data\nfusion while also estimating the position and velocity distribution of the\ndynamic local environment. This paper presents the further development of a\nprevious approach. To the best of the author's knowledge, there is no\npublication about dynamic occupancy grid mapping with subsequent analysis based\nonly on radar data. Therefore in this work, the data of multiple radar sensors\nare fused, and a grid-based object tracking and mapping method is applied.\nSubsequently, the clustering of dynamic areas provides high-level object\ninformation. For comparison, also a lidar-based method is developed. The\napproach is evaluated qualitatively and quantitatively with real-world data\nfrom a moving vehicle in urban environments. The evaluation illustrates the\nadvantages of the radar-based dynamic occupancy grid map, considering different\ncomparison metrics.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 09:26:30 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Diehl", "Christopher", ""], ["Feicho", "Eduard", ""], ["Schwambach", "Alexander", ""], ["Dammeier", "Thomas", ""], ["Mares", "Eric", ""], ["Bertram", "Torsten", ""]]}, {"id": "2008.03697", "submitter": "Meida Chen", "authors": "Meida Chen, Andrew Feng, Kyle McCullough, Pratusha Bhuvana Prasad,\n  Ryan McAlinden, Lucio Soibelman, Mike Enloe", "title": "Fully Automated Photogrammetric Data Segmentation and Object Information\n  Extraction Approach for Creating Simulation Terrain", "comments": null, "journal-ref": "Interservice/Industry Training, Simulation, and Education\n  Conference (I/ITSEC) 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our previous works have demonstrated that visually realistic 3D meshes can be\nautomatically reconstructed with low-cost, off-the-shelf unmanned aerial\nsystems (UAS) equipped with capable cameras, and efficient photogrammetric\nsoftware techniques. However, such generated data do not contain semantic\ninformation/features of objects (i.e., man-made objects, vegetation, ground,\nobject materials, etc.) and cannot allow the sophisticated user-level and\nsystem-level interaction. Considering the use case of the data in creating\nrealistic virtual environments for training and simulations (i.e., mission\nplanning, rehearsal, threat detection, etc.), segmenting the data and\nextracting object information are essential tasks. Thus, the objective of this\nresearch is to design and develop a fully automated photogrammetric data\nsegmentation and object information extraction framework. To validate the\nproposed framework, the segmented data and extracted features were used to\ncreate virtual environments in the authors previously designed simulation tool\ni.e., Aerial Terrain Line of Sight Analysis System (ATLAS). The results showed\nthat 3D mesh trees could be replaced with geo-typical 3D tree models using the\nextracted individual tree locations. The extracted tree features (i.e., color,\nwidth, height) are valuable for selecting the appropriate tree species and\nenhance visual quality. Furthermore, the identified ground material information\ncan be taken into consideration for pathfinding. The shortest path can be\ncomputed not only considering the physical distance, but also considering the\noff-road vehicle performance capabilities on different ground surface\nmaterials.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 09:32:09 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Chen", "Meida", ""], ["Feng", "Andrew", ""], ["McCullough", "Kyle", ""], ["Prasad", "Pratusha Bhuvana", ""], ["McAlinden", "Ryan", ""], ["Soibelman", "Lucio", ""], ["Enloe", "Mike", ""]]}, {"id": "2008.03704", "submitter": "Xiaoxiao Yang", "authors": "Changhong Fu, Xiaoxiao Yang, Fan Li, Juntao Xu, Changjing Liu, and\n  Peng Lu", "title": "Learning Consistency Pursued Correlation Filters for Real-Time UAV\n  Tracking", "comments": "IROS 2020 accepted, 8 pages, 7 figures, and 2 tables", "journal-ref": "IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS 2020), Las Vegas, USA", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation filter (CF)-based methods have demonstrated exceptional\nperformance in visual object tracking for unmanned aerial vehicle (UAV)\napplications, but suffer from the undesirable boundary effect. To solve this\nissue, spatially regularized correlation filters (SRDCF) proposes the spatial\nregularization to penalize filter coefficients, thereby significantly improving\nthe tracking performance. However, the temporal information hidden in the\nresponse maps is not considered in SRDCF, which limits the discriminative power\nand the robustness for accurate tracking. This work proposes a novel approach\nwith dynamic consistency pursued correlation filters, i.e., the CPCF tracker.\nSpecifically, through a correlation operation between adjacent response maps, a\npractical consistency map is generated to represent the consistency level\nacross frames. By minimizing the difference between the practical and the\nscheduled ideal consistency map, the consistency level is constrained to\nmaintain temporal smoothness, and rich temporal information contained in\nresponse maps is introduced. Besides, a dynamic constraint strategy is proposed\nto further improve the adaptability of the proposed tracker in complex\nsituations. Comprehensive experiments are conducted on three challenging UAV\nbenchmarks, i.e., UAV123@10FPS, UAVDT, and DTB70. Based on the experimental\nresults, the proposed tracker favorably surpasses the other 25 state-of-the-art\ntrackers with real-time running speed ($\\sim$43FPS) on a single CPU.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 10:22:52 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Fu", "Changhong", ""], ["Yang", "Xiaoxiao", ""], ["Li", "Fan", ""], ["Xu", "Juntao", ""], ["Liu", "Changjing", ""], ["Lu", "Peng", ""]]}, {"id": "2008.03706", "submitter": "Zhe Chen", "authors": "Weifeng Ma, Zhe Chen, Caoting Ji", "title": "Block Shuffle: A Method for High-resolution Fast Style Transfer with\n  Limited Memory", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": "10.1109/ACCESS.2020.3020053", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast Style Transfer is a series of Neural Style Transfer algorithms that use\nfeed-forward neural networks to render input images. Because of the high\ndimension of the output layer, these networks require much memory for\ncomputation. Therefore, for high-resolution images, most mobile devices and\npersonal computers cannot stylize them, which greatly limits the application\nscenarios of Fast Style Transfer. At present, the two existing solutions are\npurchasing more memory and using the feathering-based method, but the former\nrequires additional cost, and the latter has poor image quality. To solve this\nproblem, we propose a novel image synthesis method named \\emph{block shuffle},\nwhich converts a single task with high memory consumption to multiple subtasks\nwith low memory consumption. This method can act as a plug-in for Fast Style\nTransfer without any modification to the network architecture. We use the most\npopular Fast Style Transfer repository on GitHub as the baseline. Experiments\nshow that the quality of high-resolution images generated by our method is\nbetter than that of the feathering-based method. Although our method is an\norder of magnitude slower than the baseline, it can stylize high-resolution\nimages with limited memory, which is impossible with the baseline. The code and\nmodels will be made available on \\url{https://github.com/czczup/block-shuffle}.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 10:33:21 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Ma", "Weifeng", ""], ["Chen", "Zhe", ""], ["Ji", "Caoting", ""]]}, {"id": "2008.03713", "submitter": "Gyeongsik Moon", "authors": "Gyeongsik Moon and Kyoung Mu Lee", "title": "I2L-MeshNet: Image-to-Lixel Prediction Network for Accurate 3D Human\n  Pose and Mesh Estimation from a Single RGB Image", "comments": "Published at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the previous image-based 3D human pose and mesh estimation methods\nestimate parameters of the human mesh model from an input image. However,\ndirectly regressing the parameters from the input image is a highly non-linear\nmapping because it breaks the spatial relationship between pixels in the input\nimage. In addition, it cannot model the prediction uncertainty, which can make\ntraining harder. To resolve the above issues, we propose I2L-MeshNet, an\nimage-to-lixel (line+pixel) prediction network. The proposed I2L-MeshNet\npredicts the per-lixel likelihood on 1D heatmaps for each mesh vertex\ncoordinate instead of directly regressing the parameters. Our lixel-based 1D\nheatmap preserves the spatial relationship in the input image and models the\nprediction uncertainty. We demonstrate the benefit of the image-to-lixel\nprediction and show that the proposed I2L-MeshNet outperforms previous methods.\nThe code is publicly available https://github.com/mks0601/I2L-MeshNet_RELEASE.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 12:13:31 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2020 11:39:58 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Moon", "Gyeongsik", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "2008.03718", "submitter": "Jeong-Kyun Lee", "authors": "Jeong-Kyun Lee and Young-Ki Baik and Hankyu Cho and Kang Kim and Duck\n  Hoon Kim", "title": "1-Point RANSAC-Based Method for Ground Object Pose Estimation", "comments": "Accepted in the workshop on Autonomous Driving: Perception,\n  Prediction and Planning in conjunction with CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving Perspective-n-Point (PnP) problems is a traditional way of estimating\nobject poses. Given outlier-contaminated data, a pose of an object is\ncalculated with PnP algorithms of n = {3, 4} in the RANSAC-based scheme.\nHowever, the computational complexity considerably increases along with n and\nthe high complexity imposes a severe strain on devices which should estimate\nmultiple object poses in real time. In this paper, we propose an efficient\nmethod based on 1-point RANSAC for estimating a pose of an object on the\nground. In the proposed method, a pose is calculated with 1-DoF\nparameterization by using a ground object assumption and a 2D object bounding\nbox as an additional observation, thereby achieving the fastest performance\namong the RANSAC-based methods. In addition, since the method suffers from the\nerrors of the additional information, we propose a hierarchical robust\nestimation method for polishing a rough pose estimate and discovering more\ninliers in a coarse-to-fine manner. The experiments in synthetic and real-world\ndatasets demonstrate the superiority of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 12:58:58 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 07:22:42 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Lee", "Jeong-Kyun", ""], ["Baik", "Young-Ki", ""], ["Cho", "Hankyu", ""], ["Kim", "Kang", ""], ["Kim", "Duck Hoon", ""]]}, {"id": "2008.03722", "submitter": "Jeong-Kyun Lee", "authors": "Jeong-Kyun Lee and Young-Ki Baik and Hankyu Cho and Seungwoo Yoo", "title": "Online Extrinsic Camera Calibration for Temporally Consistent IPM Using\n  Lane Boundary Observations with a Lane Width Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method for online extrinsic camera calibration,\ni.e., estimating pitch, yaw, roll angles and camera height from road surface in\nsequential driving scene images. The proposed method estimates the extrinsic\ncamera parameters in two steps: 1) pitch and yaw angles are estimated\nsimultaneously using a vanishing point computed from a set of lane boundary\nobservations, and then 2) roll angle and camera height are computed by\nminimizing difference between lane width observations and a lane width prior.\nThe extrinsic camera parameters are sequentially updated using extended Kalman\nfiltering (EKF) and are finally used to generate a temporally consistent\nbird-eye-view (BEV) image by inverse perspective mapping (IPM). We demonstrate\nthe superiority of the proposed method in synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 13:11:17 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Lee", "Jeong-Kyun", ""], ["Baik", "Young-Ki", ""], ["Cho", "Hankyu", ""], ["Yoo", "Seungwoo", ""]]}, {"id": "2008.03723", "submitter": "Haoran Duan", "authors": "Haoran Duan, Shidong Wang, Yu Guan", "title": "SOFA-Net: Second-Order and First-order Attention Network for Crowd\n  Counting", "comments": "Accepted by BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated crowd counting from images/videos has attracted more attention in\nrecent years because of its wide application in smart cities. But modelling the\ndense crowd heads is challenging and most of the existing works become less\nreliable. To obtain the appropriate crowd representation, in this work we\nproposed SOFA-Net(Second-Order and First-order Attention Network): second-order\nstatistics were extracted to retain selectivity of the channel-wise spatial\ninformation for dense heads while first-order statistics, which can enhance the\nfeature discrimination for the heads' areas, were used as complementary\ninformation. Via a multi-stream architecture, the proposed second/first-order\nstatistics were learned and transformed into attention for robust\nrepresentation refinement. We evaluated our method on four public datasets and\nthe performance reached state-of-the-art on most of them. Extensive experiments\nwere also conducted to study the components in the proposed SOFA-Net, and the\nresults suggested the high-capability of second/first-order statistics on\nmodelling crowd in challenging scenarios. To the best of our knowledge, we are\nthe first work to explore the second/first-order statistics for crowd counting.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 13:13:04 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Duan", "Haoran", ""], ["Wang", "Shidong", ""], ["Guan", "Yu", ""]]}, {"id": "2008.03727", "submitter": "Iskander A. Taimanov", "authors": "M.V. Andreeva, A.V. Kalyuzhnyuk, V.V. Krutko, N.E. Russkikh, I.A.\n  Taimanov", "title": "Representative elementary volume via averaged scalar Minkowski\n  functionals", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representative Elementary Volume (REV) at which the material properties do\nnot vary with change in volume is an important quantity for making measurements\nor simulations which represent the whole. We discuss the geometrical method to\nevaluation of REV based on the quantities coming in the Steiner formula from\nconvex geometry. For bodies in the three-space this formula gives us four\nscalar functionals known as scalar Minkowski functionals. We demonstrate on\ncertain samples that the values of such averaged functionals almost stabilize\nfor cells for which the length of edges are greater than certain threshold\nvalue R. Therefore, from this point of view, it is reasonable to consider cubes\nof volume R^3 as representative elementary volumes.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 13:46:32 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Andreeva", "M. V.", ""], ["Kalyuzhnyuk", "A. V.", ""], ["Krutko", "V. V.", ""], ["Russkikh", "N. E.", ""], ["Taimanov", "I. A.", ""]]}, {"id": "2008.03737", "submitter": "Jingyuan Li", "authors": "Jingyuan Li, Ning Wang, Lefei Zhang, Bo Du, Dacheng Tao", "title": "Recurrent Feature Reasoning for Image Inpainting", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing inpainting methods have achieved promising performance for\nrecovering regular or small image defects. However, filling in large continuous\nholes remains difficult due to the lack of constraints for the hole center. In\nthis paper, we devise a Recurrent Feature Reasoning (RFR) network which is\nmainly constructed by a plug-and-play Recurrent Feature Reasoning module and a\nKnowledge Consistent Attention (KCA) module. Analogous to how humans solve\npuzzles (i.e., first solve the easier parts and then use the results as\nadditional information to solve difficult parts), the RFR module recurrently\ninfers the hole boundaries of the convolutional feature maps and then uses them\nas clues for further inference. The module progressively strengthens the\nconstraints for the hole center and the results become explicit. To capture\ninformation from distant places in the feature map for RFR, we further develop\nKCA and incorporate it in RFR. Empirically, we first compare the proposed\nRFR-Net with existing backbones, demonstrating that RFR-Net is more efficient\n(e.g., a 4\\% SSIM improvement for the same model size). We then place the\nnetwork in the context of the current state-of-the-art, where it exhibits\nimproved performance. The corresponding source code is available at:\nhttps://github.com/jingyuanli001/RFR-Inpainting\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 14:40:04 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Li", "Jingyuan", ""], ["Wang", "Ning", ""], ["Zhang", "Lefei", ""], ["Du", "Bo", ""], ["Tao", "Dacheng", ""]]}, {"id": "2008.03741", "submitter": "Yutao Liu", "authors": "Chenggang Yan, Zhisheng Li, Yongbing Zhang, Yutao Liu, Xiangyang Ji,\n  Yongdong Zhang", "title": "Depth image denoising using nuclear norm and learning graph model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The depth images denoising are increasingly becoming the hot research topic\nnowadays because they reflect the three-dimensional (3D) scene and can be\napplied in various fields of computer vision. But the depth images obtained\nfrom depth camera usually contain stains such as noise, which greatly impairs\nthe performance of depth related applications. In this paper, considering that\ngroup-based image restoration methods are more effective in gathering the\nsimilarity among patches, a group based nuclear norm and learning graph (GNNLG)\nmodel was proposed. For each patch, we find and group the most similar patches\nwithin a searching window. The intrinsic low-rank property of the grouped\npatches is exploited in our model. In addition, we studied the manifold\nlearning method and devised an effective optimized learning strategy to obtain\nthe graph Laplacian matrix, which reflects the topological structure of image,\nto further impose the smoothing priors to the denoised depth image. To achieve\nfast speed and high convergence, the alternating direction method of\nmultipliers (ADMM) is proposed to solve our GNNLG. The experimental results\nshow that the proposed method is superior to other current state-of-the-art\ndenoising methods in both subjective and objective criterion.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 15:12:16 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Yan", "Chenggang", ""], ["Li", "Zhisheng", ""], ["Zhang", "Yongbing", ""], ["Liu", "Yutao", ""], ["Ji", "Xiangyang", ""], ["Zhang", "Yongdong", ""]]}, {"id": "2008.03750", "submitter": "Deepak Anand", "authors": "Deepak Anand, Gaurav Patel, Yaman Dang, Amit Sethi", "title": "Switching Loss for Generalized Nucleus Detection in Histopathology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The accuracy of deep learning methods for two foundational tasks in medical\nimage analysis -- detection and segmentation -- can suffer from class\nimbalance. We propose a `switching loss' function that adaptively shifts the\nemphasis between foreground and background classes. While the existing loss\nfunctions to address this problem were motivated by the classification task,\nthe switching loss is based on Dice loss, which is better suited for\nsegmentation and detection. Furthermore, to get the most out the training\nsamples, we adapt the loss with each mini-batch, unlike previous proposals that\nadapt once for the entire training set. A nucleus detector trained using the\nproposed loss function on a source dataset outperformed those trained using\ncross-entropy, Dice, or focal losses. Remarkably, without retraining on target\ndatasets, our pre-trained nucleus detector also outperformed existing nucleus\ndetectors that were trained on at least some of the images from the target\ndatasets. To establish a broad utility of the proposed loss, we also confirmed\nthat it led to more accurate ventricle segmentation in MRI as compared to the\nother loss functions. Our GPU-enabled pre-trained nucleus detection software is\nalso ready to process whole slide images right out-of-the-box and is usably\nfast.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 15:42:50 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Anand", "Deepak", ""], ["Patel", "Gaurav", ""], ["Dang", "Yaman", ""], ["Sethi", "Amit", ""]]}, {"id": "2008.03763", "submitter": "Jose Escalona", "authors": "Jos\\'e L. Escalona", "title": "A methodology for the measurement of track geometry based on computer\n  vision and inertial sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document describes the theory used for the calculation of track\ngeometric irregularities on a Track Geometry Measuring System (TGMS) to be\ninstalled in railway vehicles. The TGMS includes a computer for data\nacquisition and process, a set of sensors including an inertial measuring unit\n(IMU, 3D gyroscope and 3D accelerometer), two video cameras and an encoder. The\nmain features of the proposed system are: 1. It is capable to measure track\nalignment, vertical profile, cross-level, gauge, twist and rail-head profile\nusing non-contact technology. 2. It can be installed in line railway vehicles.\nIt is compact and low cost. Provided that the equipment sees the rail heads\nwhen the vehicle is moving, it can be installed in any body of the vehicle: at\nthe wheelsets level, above primary suspension (bogie frame) or above the\nsecondary suspension (car body).\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 16:57:51 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Escalona", "Jos\u00e9 L.", ""]]}, {"id": "2008.03765", "submitter": "Wen Liu", "authors": "Yu Guo, Yuxu Lu, Ryan Wen Liu, Meifang Yang, Kwok Tai Chui", "title": "Low-Light Maritime Image Enhancement with Regularized Illumination\n  Optimization and Deep Noise Suppression", "comments": "17 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maritime images captured under low-light imaging condition easily suffer from\nlow visibility and unexpected noise, leading to negative effects on maritime\ntraffic supervision and management. To promote imaging performance, it is\nnecessary to restore the important visual information from degraded low-light\nimages. In this paper, we propose to enhance the low-light images through\nregularized illumination optimization and deep noise suppression. In\nparticular, a hybrid regularized variational model, which combines L0-norm\ngradient sparsity prior with structure-aware regularization, is presented to\nrefine the coarse illumination map originally estimated using Max-RGB. The\nadaptive gamma correction method is then introduced to adjust the refined\nillumination map. Based on the assumption of Retinex theory, a guided\nfilter-based detail boosting method is introduced to optimize the reflection\nmap. The adjusted illumination and optimized reflection maps are finally\ncombined to generate the enhanced maritime images. To suppress the effect of\nunwanted noise on imaging performance, a deep learning-based blind denoising\nframework is further introduced to promote the visual quality of enhanced\nimage. In particular, this framework is composed of two sub-networks, i.e.,\nE-Net and D-Net adopted for noise level estimation and non-blind noise\nreduction, respectively. The main benefit of our image enhancement method is\nthat it takes full advantage of the regularized illumination optimization and\ndeep blind denoising. Comprehensive experiments have been conducted on both\nsynthetic and realistic maritime images to compare our proposed method with\nseveral state-of-the-art imaging methods. Experimental results have illustrated\nits superior performance in terms of both quantitative and qualitative\nevaluations.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 17:05:23 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Guo", "Yu", ""], ["Lu", "Yuxu", ""], ["Liu", "Ryan Wen", ""], ["Yang", "Meifang", ""], ["Chui", "Kwok Tai", ""]]}, {"id": "2008.03781", "submitter": "Chhavi Sharma", "authors": "Chhavi Sharma and Deepesh Bhageria and William Scott and Srinivas PYKL\n  and Amitava Das and Tanmoy Chakraborty and Viswanath Pulabaigari and Bjorn\n  Gamback", "title": "SemEval-2020 Task 8: Memotion Analysis -- The Visuo-Lingual Metaphor!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Information on social media comprises of various modalities such as textual,\nvisual and audio. NLP and Computer Vision communities often leverage only one\nprominent modality in isolation to study social media. However, the\ncomputational processing of Internet memes needs a hybrid approach. The growing\nubiquity of Internet memes on social media platforms such as Facebook,\nInstagram, and Twiter further suggests that we can not ignore such multimodal\ncontent anymore. To the best of our knowledge, there is not much attention\ntowards meme emotion analysis. The objective of this proposal is to bring the\nattention of the research community towards the automatic processing of\nInternet memes. The task Memotion analysis released approx 10K annotated memes,\nwith human-annotated labels namely sentiment (positive, negative, neutral),\ntype of emotion (sarcastic, funny, offensive, motivation) and their\ncorresponding intensity. The challenge consisted of three subtasks: sentiment\n(positive, negative, and neutral) analysis of memes, overall emotion (humour,\nsarcasm, offensive, and motivational) classification of memes, and classifying\nintensity of meme emotion. The best performances achieved were F1 (macro\naverage) scores of 0.35, 0.51 and 0.32, respectively for each of the three\nsubtasks.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 18:17:33 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Sharma", "Chhavi", ""], ["Bhageria", "Deepesh", ""], ["Scott", "William", ""], ["PYKL", "Srinivas", ""], ["Das", "Amitava", ""], ["Chakraborty", "Tanmoy", ""], ["Pulabaigari", "Viswanath", ""], ["Gamback", "Bjorn", ""]]}, {"id": "2008.03788", "submitter": "Madhu Kiran", "authors": "Madhu Kiran, Amran Bhuiyan, Louis-Antoine Blais-Morin, Mehrsan Javan,\n  Ismail Ben Ayed, Eric Granger", "title": "A Flow-Guided Mutual Attention Network for Video-Based Person\n  Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person Re-Identification (ReID) is a challenging problem in many video\nanalytics and surveillance applications, where a person's identity must be\nassociated across a distributed non-overlapping network of cameras. Video-based\nperson ReID has recently gained much interest because it allows capturing\ndiscriminant spatio-temporal information from video clips that is unavailable\nfor image-based ReID. Despite recent advances, deep learning (DL) models for\nvideo ReID often fail to leverage this information to improve the robustness of\nfeature representations. In this paper, the motion pattern of a person is\nexplored as an additional cue for ReID. In particular, a flow-guided Mutual\nAttention network is proposed for fusion of image and optical flow sequences\nusing any 2D-CNN backbone, allowing to encode temporal information along with\nspatial appearance information. Our Mutual Attention network relies on the\njoint spatial attention between image and optical flow features maps to\nactivate a common set of salient features across them. In addition to\nflow-guided attention, we introduce a method to aggregate features from longer\ninput streams for better video sequence-level representation. Our extensive\nexperiments on three challenging video ReID datasets indicate that using the\nproposed Mutual Attention network allows to improve recognition accuracy\nconsiderably with respect to conventional gated-attention networks, and\nstate-of-the-art methods for video-based person ReID.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 18:58:11 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 23:56:23 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Kiran", "Madhu", ""], ["Bhuiyan", "Amran", ""], ["Blais-Morin", "Louis-Antoine", ""], ["Javan", "Mehrsan", ""], ["Ayed", "Ismail Ben", ""], ["Granger", "Eric", ""]]}, {"id": "2008.03789", "submitter": "Zhengyi Luo", "authors": "Zhengyi Luo, S. Alireza Golestaneh, Kris M. Kitani", "title": "3D Human Motion Estimation via Motion Compression and Refinement", "comments": "Accepted by ACCV 2020 (Oral). Project page:\n  https://zhengyiluo.github.io/projects/meva/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a technique for generating smooth and accurate 3D human pose and\nmotion estimates from RGB video sequences. Our method, which we call Motion\nEstimation via Variational Autoencoder (MEVA), decomposes a temporal sequence\nof human motion into a smooth motion representation using auto-encoder-based\nmotion compression and a residual representation learned through motion\nrefinement. This two-step encoding of human motion captures human motion in two\nstages: a general human motion estimation step that captures the coarse overall\nmotion, and a residual estimation that adds back person-specific motion\ndetails. Experiments show that our method produces both smooth and accurate 3D\nhuman pose and motion estimates.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 19:02:29 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 20:24:59 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Luo", "Zhengyi", ""], ["Golestaneh", "S. Alireza", ""], ["Kitani", "Kris M.", ""]]}, {"id": "2008.03791", "submitter": "Yi-Fan Song", "authors": "Yi-Fan Song, Zhang Zhang, Caifeng Shan, Liang Wang", "title": "Richly Activated Graph Convolutional Network for Robust Skeleton-based\n  Action Recognition", "comments": "Accepted by IEEE T-CSVT, 11 pages, 6 figures, 10 tables", "journal-ref": null, "doi": "10.1109/TCSVT.2020.3015051", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current methods for skeleton-based human action recognition usually work with\ncomplete skeletons. However, in real scenarios, it is inevitable to capture\nincomplete or noisy skeletons, which could significantly deteriorate the\nperformance of current methods when some informative joints are occluded or\ndisturbed. To improve the robustness of action recognition models, a\nmulti-stream graph convolutional network (GCN) is proposed to explore\nsufficient discriminative features spreading over all skeleton joints, so that\nthe distributed redundant representation reduces the sensitivity of the action\nmodels to non-standard skeletons. Concretely, the backbone GCN is extended by a\nseries of ordered streams which is responsible for learning discriminative\nfeatures from the joints less activated by preceding streams. Here, the\nactivation degrees of skeleton joints of each GCN stream are measured by the\nclass activation maps (CAM), and only the information from the unactivated\njoints will be passed to the next stream, by which rich features over all\nactive joints are obtained. Thus, the proposed method is termed richly\nactivated GCN (RA-GCN). Compared to the state-of-the-art (SOTA) methods, the\nRA-GCN achieves comparable performance on the standard NTU RGB+D 60 and 120\ndatasets. More crucially, on the synthetic occlusion and jittering datasets,\nthe performance deterioration due to the occluded and disturbed joints can be\nsignificantly alleviated by utilizing the proposed RA-GCN.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 19:06:29 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 02:07:30 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Song", "Yi-Fan", ""], ["Zhang", "Zhang", ""], ["Shan", "Caifeng", ""], ["Wang", "Liang", ""]]}, {"id": "2008.03800", "submitter": "Rui Qian", "authors": "Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang,\n  Serge Belongie, Yin Cui", "title": "Spatiotemporal Contrastive Video Representation Learning", "comments": "CVPR2021 Camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a self-supervised Contrastive Video Representation Learning (CVRL)\nmethod to learn spatiotemporal visual representations from unlabeled videos.\nOur representations are learned using a contrastive loss, where two augmented\nclips from the same short video are pulled together in the embedding space,\nwhile clips from different videos are pushed away. We study what makes for good\ndata augmentations for video self-supervised learning and find that both\nspatial and temporal information are crucial. We carefully design data\naugmentations involving spatial and temporal cues. Concretely, we propose a\ntemporally consistent spatial augmentation method to impose strong spatial\naugmentations on each frame of the video while maintaining the temporal\nconsistency across frames. We also propose a sampling-based temporal\naugmentation method to avoid overly enforcing invariance on clips that are\ndistant in time. On Kinetics-600, a linear classifier trained on the\nrepresentations learned by CVRL achieves 70.4% top-1 accuracy with a\n3D-ResNet-50 (R3D-50) backbone, outperforming ImageNet supervised pre-training\nby 15.7% and SimCLR unsupervised pre-training by 18.8% using the same inflated\nR3D-50. The performance of CVRL can be further improved to 72.9% with a larger\nR3D-152 (2x filters) backbone, significantly closing the gap between\nunsupervised and supervised video representation learning. Our code and models\nwill be available at\nhttps://github.com/tensorflow/models/tree/master/official/.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 19:58:45 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 23:27:25 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2020 18:07:27 GMT"}, {"version": "v4", "created": "Mon, 5 Apr 2021 19:51:20 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Qian", "Rui", ""], ["Meng", "Tianjian", ""], ["Gong", "Boqing", ""], ["Yang", "Ming-Hsuan", ""], ["Wang", "Huisheng", ""], ["Belongie", "Serge", ""], ["Cui", "Yin", ""]]}, {"id": "2008.03806", "submitter": "Xiuming Zhang", "authors": "Xiuming Zhang, Sean Fanello, Yun-Ta Tsai, Tiancheng Sun, Tianfan Xue,\n  Rohit Pandey, Sergio Orts-Escolano, Philip Davidson, Christoph Rhemann, Paul\n  Debevec, Jonathan T. Barron, Ravi Ramamoorthi, William T. Freeman", "title": "Neural Light Transport for Relighting and View Synthesis", "comments": "Camera-ready version for TOG 2021. Project Page:\n  http://nlt.csail.mit.edu/", "journal-ref": null, "doi": "10.1145/3446328", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The light transport (LT) of a scene describes how it appears under different\nlighting and viewing directions, and complete knowledge of a scene's LT enables\nthe synthesis of novel views under arbitrary lighting. In this paper, we focus\non image-based LT acquisition, primarily for human bodies within a light stage\nsetup. We propose a semi-parametric approach to learn a neural representation\nof LT that is embedded in the space of a texture atlas of known geometric\nproperties, and model all non-diffuse and global LT as residuals added to a\nphysically-accurate diffuse base rendering. In particular, we show how to fuse\npreviously seen observations of illuminants and views to synthesize a new image\nof the same scene under a desired lighting condition from a chosen viewpoint.\nThis strategy allows the network to learn complex material effects (such as\nsubsurface scattering) and global illumination, while guaranteeing the physical\ncorrectness of the diffuse LT (such as hard shadows). With this learned LT, one\ncan relight the scene photorealistically with a directional light or an HDRI\nmap, synthesize novel views with view-dependent effects, or do both\nsimultaneously, all in a unified framework using a set of sparse, previously\nseen observations. Qualitative and quantitative experiments demonstrate that\nour neural LT (NLT) outperforms state-of-the-art solutions for relighting and\nview synthesis, without separate treatment for both problems that prior work\nrequires.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 20:13:15 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 16:32:01 GMT"}, {"version": "v3", "created": "Wed, 20 Jan 2021 15:45:52 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Zhang", "Xiuming", ""], ["Fanello", "Sean", ""], ["Tsai", "Yun-Ta", ""], ["Sun", "Tiancheng", ""], ["Xue", "Tianfan", ""], ["Pandey", "Rohit", ""], ["Orts-Escolano", "Sergio", ""], ["Davidson", "Philip", ""], ["Rhemann", "Christoph", ""], ["Debevec", "Paul", ""], ["Barron", "Jonathan T.", ""], ["Ramamoorthi", "Ravi", ""], ["Freeman", "William T.", ""]]}, {"id": "2008.03813", "submitter": "Xudong Wang", "authors": "Xudong Wang, Ziwei Liu, Stella X. Yu", "title": "Unsupervised Feature Learning by Cross-Level Instance-Group\n  Discrimination", "comments": "Accepted at CVPR 2021; Project page:\n  http://people.eecs.berkeley.edu/~xdwang/projects/CLD/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised feature learning has made great strides with contrastive\nlearning based on instance discrimination and invariant mapping, as benchmarked\non curated class-balanced datasets. However, natural data could be highly\ncorrelated and long-tail distributed. Natural between-instance similarity\nconflicts with the presumed instance distinction, causing unstable training and\npoor performance.\n  Our idea is to discover and integrate between-instance similarity into\ncontrastive learning, not directly by instance grouping, but by cross-level\ndiscrimination (CLD) between instances and local instance groups. While\ninvariant mapping of each instance is imposed by attraction within its\naugmented views, between-instance similarity could emerge from common repulsion\nagainst instance groups.\n  Our batch-wise and cross-view comparisons also greatly improve the\npositive/negative sample ratio of contrastive learning and achieve better\ninvariant mapping. To effect both grouping and discrimination objectives, we\nimpose them on features separately derived from a shared representation. In\naddition, we propose normalized projection heads and unsupervised\nhyper-parameter tuning for the first time.\n  Our extensive experimentation demonstrates that CLD is a lean and powerful\nadd-on to existing methods such as NPID, MoCo, InfoMin, and BYOL on highly\ncorrelated, long-tail, or balanced datasets. It not only achieves new\nstate-of-the-art on self-supervision, semi-supervision, and transfer learning\nbenchmarks, but also beats MoCo v2 and SimCLR on every reported performance\nattained with a much larger compute. CLD effectively brings unsupervised\nlearning closer to natural data and real-world applications. Our code is\npublicly available at: https://github.com/frank-xwang/CLD-UnsupervisedLearning.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 21:13:13 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 00:00:23 GMT"}, {"version": "v3", "created": "Mon, 5 Oct 2020 08:16:51 GMT"}, {"version": "v4", "created": "Thu, 3 Dec 2020 06:43:35 GMT"}, {"version": "v5", "created": "Sun, 16 May 2021 03:11:23 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Wang", "Xudong", ""], ["Liu", "Ziwei", ""], ["Yu", "Stella X.", ""]]}, {"id": "2008.03824", "submitter": "Sai Bi", "authors": "Sai Bi, Zexiang Xu, Pratul Srinivasan, Ben Mildenhall, Kalyan\n  Sunkavalli, Milo\\v{s} Ha\\v{s}an, Yannick Hold-Geoffroy, David Kriegman, Ravi\n  Ramamoorthi", "title": "Neural Reflectance Fields for Appearance Acquisition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Neural Reflectance Fields, a novel deep scene representation that\nencodes volume density, normal and reflectance properties at any 3D point in a\nscene using a fully-connected neural network. We combine this representation\nwith a physically-based differentiable ray marching framework that can render\nimages from a neural reflectance field under any viewpoint and light. We\ndemonstrate that neural reflectance fields can be estimated from images\ncaptured with a simple collocated camera-light setup, and accurately model the\nappearance of real-world scenes with complex geometry and reflectance. Once\nestimated, they can be used to render photo-realistic images under novel\nviewpoint and (non-collocated) lighting conditions and accurately reproduce\nchallenging effects like specularities, shadows and occlusions. This allows us\nto perform high-quality view synthesis and relighting that is significantly\nbetter than previous methods. We also demonstrate that we can compose the\nestimated neural reflectance field of a real scene with traditional scene\nmodels and render them using standard Monte Carlo rendering engines. Our work\nthus enables a complete pipeline from high-quality and practical appearance\nacquisition to 3D scene composition and rendering.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 22:04:36 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2020 08:39:07 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Bi", "Sai", ""], ["Xu", "Zexiang", ""], ["Srinivasan", "Pratul", ""], ["Mildenhall", "Ben", ""], ["Sunkavalli", "Kalyan", ""], ["Ha\u0161an", "Milo\u0161", ""], ["Hold-Geoffroy", "Yannick", ""], ["Kriegman", "David", ""], ["Ramamoorthi", "Ravi", ""]]}, {"id": "2008.03834", "submitter": "Jichao Zhang", "authors": "Jichao Zhang, Jingjing Chen, Hao Tang, Wei Wang, Yan Yan, Enver\n  Sangineto, Nicu Sebe", "title": "Dual In-painting Model for Unsupervised Gaze Correction and Animation in\n  the Wild", "comments": "Accepted By ACMMM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of unsupervised gaze correction in the\nwild, presenting a solution that works without the need for precise annotations\nof the gaze angle and the head pose. We have created a new dataset called\nCelebAGaze, which consists of two domains X, Y, where the eyes are either\nstaring at the camera or somewhere else. Our method consists of three novel\nmodules: the Gaze Correction module (GCM), the Gaze Animation module (GAM), and\nthe Pretrained Autoencoder module (PAM). Specifically, GCM and GAM separately\ntrain a dual in-painting network using data from the domain $X$ for gaze\ncorrection and data from the domain $Y$ for gaze animation. Additionally, a\nSynthesis-As-Training method is proposed when training GAM to encourage the\nfeatures encoded from the eye region to be correlated with the angle\ninformation, resulting in a gaze animation which can be achieved by\ninterpolation in the latent space. To further preserve the identity\ninformation~(e.g., eye shape, iris color), we propose the PAM with an\nAutoencoder, which is based on Self-Supervised mirror learning where the\nbottleneck features are angle-invariant and which works as an extra input to\nthe dual in-painting models. Extensive experiments validate the effectiveness\nof the proposed method for gaze correction and gaze animation in the wild and\ndemonstrate the superiority of our approach in producing more compelling\nresults than state-of-the-art baselines. Our code, the pretrained models and\nthe supplementary material are available at:\nhttps://github.com/zhangqianhui/GazeAnimation.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 23:14:16 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Zhang", "Jichao", ""], ["Chen", "Jingjing", ""], ["Tang", "Hao", ""], ["Wang", "Wei", ""], ["Yan", "Yan", ""], ["Sangineto", "Enver", ""], ["Sebe", "Nicu", ""]]}, {"id": "2008.03848", "submitter": "Yingguo Xu", "authors": "Yingguo Xu, Lei Zhang, Qingyan Duan", "title": "Domain Private and Agnostic Feature for Modality Adaptive Face\n  Recognition", "comments": "Accepted by IJCB2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous face recognition is a challenging task due to the large\nmodality discrepancy and insufficient cross-modal samples. Most existing works\nfocus on discriminative feature transformation, metric learning and cross-modal\nface synthesis. However, the fact that cross-modal faces are always coupled by\ndomain (modality) and identity information has received little attention.\nTherefore, how to learn and utilize the domain-private feature and\ndomain-agnostic feature for modality adaptive face recognition is the focus of\nthis work. Specifically, this paper proposes a Feature Aggregation Network\n(FAN), which includes disentangled representation module (DRM), feature fusion\nmodule (FFM) and adaptive penalty metric (APM) learning session. First, in DRM,\ntwo subnetworks, i.e. domain-private network and domain-agnostic network are\nspecially designed for learning modality features and identity features,\nrespectively. Second, in FFM, the identity features are fused with domain\nfeatures to achieve cross-modal bi-directional identity feature transformation,\nwhich, to a large extent, further disentangles the modality information and\nidentity information. Third, considering that the distribution imbalance\nbetween easy and hard pairs exists in cross-modal datasets, which increases the\nrisk of model bias, the identity preserving guided metric learning with\nadaptive hard pairs penalization is proposed in our FAN. The proposed APM also\nguarantees the cross-modality intra-class compactness and inter-class\nseparation. Extensive experiments on benchmark cross-modal face datasets show\nthat our FAN outperforms SOTA methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 00:59:42 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Xu", "Yingguo", ""], ["Zhang", "Lei", ""], ["Duan", "Qingyan", ""]]}, {"id": "2008.03864", "submitter": "Jing Zhang", "authors": "Jing Zhang and Yang Cao and Zheng-Jun Zha and Dacheng Tao", "title": "Nighttime Dehazing with a Synthetic Benchmark", "comments": "ACM MM 2020. Both the dataset and source code will be available at\n  \\url{https://github.com/chaimi2013/3R}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing the visibility of nighttime hazy images is challenging because of\nuneven illumination from active artificial light sources and haze\nabsorbing/scattering. The absence of large-scale benchmark datasets hampers\nprogress in this area. To address this issue, we propose a novel synthetic\nmethod called 3R to simulate nighttime hazy images from daytime clear images,\nwhich first reconstructs the scene geometry, then simulates the light rays and\nobject reflectance, and finally renders the haze effects. Based on it, we\ngenerate realistic nighttime hazy images by sampling real-world light colors\nfrom a prior empirical distribution. Experiments on the synthetic benchmark\nshow that the degrading factors jointly reduce the image quality. To address\nthis issue, we propose an optimal-scale maximum reflectance prior to\ndisentangle the color correction from haze removal and address them\nsequentially. Besides, we also devise a simple but effective learning-based\nbaseline which has an encoder-decoder structure based on the MobileNet-v2\nbackbone. Experiment results demonstrate their superiority over\nstate-of-the-art methods in terms of both image quality and runtime. Both the\ndataset and source code will be available at https://github.com/chaimi2013/3R.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 02:16:46 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 05:45:11 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 00:41:38 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Zhang", "Jing", ""], ["Cao", "Yang", ""], ["Zha", "Zheng-Jun", ""], ["Tao", "Dacheng", ""]]}, {"id": "2008.03875", "submitter": "Juncheng Liu Dr", "authors": "Juncheng Liu, Steven Mills, Brendan McCane", "title": "RocNet: Recursive Octree Network for Efficient 3D Deep Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a deep recursive octree network for the compression of 3D voxel\ndata. Our network compresses a voxel grid of any size down to a very small\nlatent space in an autoencoder-like network. We show results for compressing\n32, 64 and 128 grids down to just 80 floats in the latent space. We demonstrate\nthe effectiveness and efficiency of our proposed method on several publicly\navailable datasets with three experiments: 3D shape classification, 3D shape\nreconstruction, and shape generation. Experimental results show that our\nalgorithm maintains accuracy while consuming less memory with shorter training\ntimes compared to existing methods, especially in 3D reconstruction tasks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 03:02:10 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Liu", "Juncheng", ""], ["Mills", "Steven", ""], ["McCane", "Brendan", ""]]}, {"id": "2008.03889", "submitter": "Dingquan Li", "authors": "Dingquan Li, Tingting Jiang and Ming Jiang", "title": "Norm-in-Norm Loss with Faster Convergence and Better Performance for\n  Image Quality Assessment", "comments": "Accepted by ACM MM 2020, + supplemental materials", "journal-ref": null, "doi": "10.1145/3394171.3413804", "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, most image quality assessment (IQA) models are supervised by the\nMAE or MSE loss with empirically slow convergence. It is well-known that\nnormalization can facilitate fast convergence. Therefore, we explore\nnormalization in the design of loss functions for IQA. Specifically, we first\nnormalize the predicted quality scores and the corresponding subjective quality\nscores. Then, the loss is defined based on the norm of the differences between\nthese normalized values. The resulting \"Norm-in-Norm'' loss encourages the IQA\nmodel to make linear predictions with respect to subjective quality scores.\nAfter training, the least squares regression is applied to determine the linear\nmapping from the predicted quality to the subjective quality. It is shown that\nthe new loss is closely connected with two common IQA performance criteria\n(PLCC and RMSE). Through theoretical analysis, it is proved that the embedded\nnormalization makes the gradients of the loss function more stable and more\npredictable, which is conducive to the faster convergence of the IQA model.\nFurthermore, to experimentally verify the effectiveness of the proposed loss,\nit is applied to solve a challenging problem: quality assessment of in-the-wild\nimages. Experiments on two relevant datasets (KonIQ-10k and CLIVE) show that,\ncompared to MAE or MSE loss, the new loss enables the IQA model to converge\nabout 10 times faster and the final model achieves better performance. The\nproposed model also achieves state-of-the-art prediction performance on this\nchallenging problem. For reproducible scientific research, our code is publicly\navailable at https://github.com/lidq92/LinearityIQA.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 04:01:21 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Li", "Dingquan", ""], ["Jiang", "Tingting", ""], ["Jiang", "Ming", ""]]}, {"id": "2008.03897", "submitter": "Po-Heng Chen", "authors": "Po-Heng Chen, Zhao-Xu Luo, Zu-Kuan Huang, Chun Yang, Kuan-Wen Chen", "title": "IF-Net: An Illumination-invariant Feature Network", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature descriptor matching is a critical step is many computer vision\napplications such as image stitching, image retrieval and visual localization.\nHowever, it is often affected by many practical factors which will degrade its\nperformance. Among these factors, illumination variations are the most\ninfluential one, and especially no previous descriptor learning works focus on\ndealing with this problem. In this paper, we propose IF-Net, aimed to generate\na robust and generic descriptor under crucial illumination changes conditions.\nWe find out not only the kind of training data important but also the order it\nis presented. To this end, we investigate several dataset scheduling methods\nand propose a separation training scheme to improve the matching accuracy.\nFurther, we propose a ROI loss and hard-positive mining strategy along with the\ntraining scheme, which can strengthen the ability of generated descriptor\ndealing with large illumination change conditions. We evaluate our approach on\npublic patch matching benchmark and achieve the best results compared with\nseveral state-of-the-arts methods. To show the practicality, we further\nevaluate IF-Net on the task of visual localization under large illumination\nchanges scenes, and achieves the best localization accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 04:32:32 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Chen", "Po-Heng", ""], ["Luo", "Zhao-Xu", ""], ["Huang", "Zu-Kuan", ""], ["Yang", "Chun", ""], ["Chen", "Kuan-Wen", ""]]}, {"id": "2008.03898", "submitter": "Boyu Yang", "authors": "Boyu Yang, Chang Liu, Bohao Li, Jianbin Jiao, and Qixiang Ye", "title": "Prototype Mixture Models for Few-shot Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot segmentation is challenging because objects within the support and\nquery images could significantly differ in appearance and pose. Using a single\nprototype acquired directly from the support image to segment the query image\ncauses semantic ambiguity. In this paper, we propose prototype mixture models\n(PMMs), which correlate diverse image regions with multiple prototypes to\nenforce the prototype-based semantic representation. Estimated by an\nExpectation-Maximization algorithm, PMMs incorporate rich channel-wised and\nspatial semantics from limited support images. Utilized as representations as\nwell as classifiers, PMMs fully leverage the semantics to activate objects in\nthe query image while depressing background regions in a duplex manner.\nExtensive experiments on Pascal VOC and MS-COCO datasets show that PMMs\nsignificantly improve upon state-of-the-arts. Particularly, PMMs improve 5-shot\nsegmentation performance on MS-COCO by up to 5.82\\% with only a moderate cost\nfor model size and inference speed.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 04:33:17 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 11:23:17 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Yang", "Boyu", ""], ["Liu", "Chang", ""], ["Li", "Bohao", ""], ["Jiao", "Jianbin", ""], ["Ye", "Qixiang", ""]]}, {"id": "2008.03901", "submitter": "Fanghui Xue", "authors": "Fanghui Xue, Yingyong Qi, Jack Xin", "title": "RARTS: a Relaxed Architecture Search Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentiable architecture search (DARTS) is an effective method for\ndata-driven neural network design based on solving a bilevel optimization\nproblem. In this paper, we formulate a single level alternative and a relaxed\narchitecture search (RARTS) method that utilizes training and validation\ndatasets in architecture learning without involving mixed second derivatives of\nthe corresponding loss functions. Through weight/architecture variable\nsplitting and Gauss-Seidel iterations, the core algorithm outperforms DARTS\nsignificantly in accuracy and search efficiency, as shown in both a solvable\nmodel and CIFAR-10 based architecture search. Our model continues to\nout-perform DARTS upon transfer to ImageNet and is on par with recent variants\nof DARTS even though our innovation is purely on the training algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 04:55:51 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Xue", "Fanghui", ""], ["Qi", "Yingyong", ""], ["Xin", "Jack", ""]]}, {"id": "2008.03912", "submitter": "Fangqiang Ding", "authors": "Changhong Fu, Fangqiang Ding, Yiming Li, Jin Jin and Chen Feng", "title": "DR^2Track: Towards Real-Time Visual Tracking for UAV via Distractor\n  Repressed Dynamic Regression", "comments": "8pages, 7 figures, accepted by 2020 IEEE/RJS International Conference\n  on Intelligent Robots and Systems(IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual tracking has yielded promising applications with unmanned aerial\nvehicle (UAV). In literature, the advanced discriminative correlation filter\n(DCF) type trackers generally distinguish the foreground from the background\nwith a learned regressor which regresses the implicit circulated samples into a\nfixed target label. However, the predefined and unchanged regression target\nresults in low robustness and adaptivity to uncertain aerial tracking\nscenarios. In this work, we exploit the local maximum points of the response\nmap generated in the detection phase to automatically locate current\ndistractors. By repressing the response of distractors in the regressor\nlearning, we can dynamically and adaptively alter our regression target to\nleverage the tracking robustness as well as adaptivity. Substantial experiments\nconducted on three challenging UAV benchmarks demonstrate both excellent\nperformance and extraordinary speed (~50fps on a cheap CPU) of our tracker.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 06:08:31 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Fu", "Changhong", ""], ["Ding", "Fangqiang", ""], ["Li", "Yiming", ""], ["Jin", "Jin", ""], ["Feng", "Chen", ""]]}, {"id": "2008.03915", "submitter": "Fangqiang Ding", "authors": "Fangqiang Ding, Changhong Fu, Yiming Li, Jin Jin and Chen Feng", "title": "Automatic Failure Recovery and Re-Initialization for Online UAV Tracking\n  with Joint Scale and Aspect Ratio Optimization", "comments": "8pages, 8 figures, accepted by 2020 IEEE/RSJ International Conference\n  on Intelligent Robots and Systems(IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current unmanned aerial vehicle (UAV) visual tracking algorithms are\nprimarily limited with respect to: (i) the kind of size variation they can deal\nwith, (ii) the implementation speed which hardly meets the real-time\nrequirement. In this work, a real-time UAV tracking algorithm with powerful\nsize estimation ability is proposed. Specifically, the overall tracking task is\nallocated to two 2D filters: (i) translation filter for location prediction in\nthe space domain, (ii) size filter for scale and aspect ratio optimization in\nthe size domain. Besides, an efficient two-stage re-detection strategy is\nintroduced for long-term UAV tracking tasks. Large-scale experiments on four\nUAV benchmarks demonstrate the superiority of the presented method which has\ncomputation feasibility on a low-cost CPU.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 06:31:30 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Ding", "Fangqiang", ""], ["Fu", "Changhong", ""], ["Li", "Yiming", ""], ["Jin", "Jin", ""], ["Feng", "Chen", ""]]}, {"id": "2008.03922", "submitter": "Tao Deng", "authors": "Jiyong Zhang, Tao Deng, Fei Yan and Wenbo Liu", "title": "Lane Detection Model Based on Spatio-Temporal Network With Double\n  Convolutional Gated Recurrent Units", "comments": "13 pages, 9 figures, 8 tables, Accepted by IEEE Transactions on\n  Intelligent Transportation Systems", "journal-ref": null, "doi": "10.1109/TITS.2021.3060258", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lane detection is one of the indispensable and key elements of self-driving\nenvironmental perception. Many lane detection models have been proposed,\nsolving lane detection under challenging conditions, including intersection\nmerging and splitting, curves, boundaries, occlusions and combinations of scene\ntypes. Nevertheless, lane detection will remain an open problem for some time\nto come. The ability to cope well with those challenging scenes impacts greatly\nthe applications of lane detection on advanced driver assistance systems\n(ADASs). In this paper, a spatio-temporal network with double Convolutional\nGated Recurrent Units (ConvGRUs) is proposed to address lane detection in\nchallenging scenes. Both of ConvGRUs have the same structures, but different\nlocations and functions in our network. One is used to extract the information\nof the most likely low-level features of lane markings. The extracted features\nare input into the next layer of the end-to-end network after concatenating\nthem with the outputs of some blocks. The other one takes some continuous\nframes as its input to process the spatio-temporal driving information.\nExtensive experiments on the large-scale TuSimple lane marking challenge\ndataset and Unsupervised LLAMAS dataset demonstrate that the proposed model can\neffectively detect lanes in the challenging driving scenes. Our model can\noutperform the state-of-the-art lane detection models.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 06:50:48 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 11:59:41 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Zhang", "Jiyong", ""], ["Deng", "Tao", ""], ["Yan", "Fei", ""], ["Liu", "Wenbo", ""]]}, {"id": "2008.03927", "submitter": "Hans Jacob Teglbj{\\ae}rg Stephensen", "authors": "Hans JT Stephensen, Anne Marie Svane, Carlos Benitez, Steven A.\n  Goldman, Jon Sporring", "title": "Measuring shape relations using r-parallel sets", "comments": "10 pages 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometrical measurements of biological objects form the basis of many\nquantitative analyses. Hausdorff measures such as the volume and the area of\nobjects are simple and popular descriptors of individual objects, however, for\nmost biological processes, the interaction between objects cannot be ignored,\nand the shape and function of neighboring objects are mutually influential.\n  In this paper, we present a theory on the geometrical interaction between\nobjects based on the theory of spatial point processes. Our theory is based on\nthe relation between two objects: a reference and an observed object. We\ngenerate the $r$-parallel sets of the reference object, we calculate the\nintersection between the $r$-parallel sets and the observed object, and we\ndefine measures on these intersections. Our measures are simple like the volume\nand area of an object, but describe further details about the shape of\nindividual objects and their pairwise geometrical relation. Finally, we propose\na summary statistics for collections of shapes and their interaction.\n  We evaluate these measures on a publicly available FIB-SEM 3D data set of an\nadult rodent.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 07:30:51 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Stephensen", "Hans JT", ""], ["Svane", "Anne Marie", ""], ["Benitez", "Carlos", ""], ["Goldman", "Steven A.", ""], ["Sporring", "Jon", ""]]}, {"id": "2008.03928", "submitter": "Shi-Jie Li", "authors": "Shijie Li, Yun Liu, Juergen Gall", "title": "Projected-point-based Segmentation: A New Paradigm for LiDAR Point Cloud\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most point-based semantic segmentation methods are designed for indoor\nscenarios, but many applications such as autonomous driving vehicles require\naccurate segmentation for outdoor scenarios. For this goal, light detection and\nranging (LiDAR) sensors are often used to collect outdoor environmental data.\nThe problem is that directly applying previous point-based segmentation methods\nto LiDAR point clouds usually leads to unsatisfactory results due to the domain\ngap between indoor and outdoor scenarios. To address such a domain gap, we\npropose a new paradigm, namely projected-point-based methods, to transform\npoint-based methods to a suitable form for LiDAR point cloud segmentation by\nutilizing the characteristics of LiDAR point clouds. Specifically, we utilize\nthe inherent ordered information of LiDAR points for point sampling and\ngrouping, thus reducing unnecessary computation. All computations are carried\nout on the projected image, and there are only pointwise convolutions and\nmatrix multiplication in projected-point-based methods. We compare\nprojected-point-based methods with point-based methods on the challenging\nSemanticKITTI dataset, and experimental results demonstrate that\nprojected-point-based methods achieve better accuracy than all baselines more\nefficiently. Even with a simple baseline architecture, projected-point-based\nmethods perform favorably against previous state-of-the-art methods. The code\nwill be released upon paper acceptance.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 07:30:53 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Li", "Shijie", ""], ["Liu", "Yun", ""], ["Gall", "Juergen", ""]]}, {"id": "2008.03949", "submitter": "Samah Khawaled", "authors": "Samah Khawaled and Moti Freiman", "title": "Unsupervised Deep-Learning Based Deformable Image Registration: A\n  Bayesian Framework", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised deep-learning (DL) models were recently proposed for deformable\nimage registration tasks. In such models, a neural-network is trained to\npredict the best deformation field by minimizing some dissimilarity function\nbetween the moving and the target images. After training on a dataset without\nreference deformation fields available, such a model can be used to rapidly\npredict the deformation field between newly seen moving and target images.\nCurrently, the training process effectively provides a point-estimate of the\nnetwork weights rather than characterizing their entire posterior distribution.\nThis may result in a potential over-fitting which may yield sub-optimal results\nat inference phase, especially for small-size datasets, frequently present in\nthe medical imaging domain. We introduce a fully Bayesian framework for\nunsupervised DL-based deformable image registration. Our method provides a\nprincipled way to characterize the true posterior distribution, thus, avoiding\npotential over-fitting. We used stochastic gradient Langevin dynamics (SGLD) to\nconduct the posterior sampling, which is both theoretically well-founded and\ncomputationally efficient. We demonstrated the added-value of our Basyesian\nunsupervised DL-based registration framework on the MNIST and brain MRI (MGH10)\ndatasets in comparison to the VoxelMorph unsupervised DL-based image\nregistration framework. Our experiments show that our approach provided better\nestimates of the deformation field by means of improved mean-squared-error\n($0.0063$ vs. $0.0065$) and Dice coefficient ($0.73$ vs. $0.71$) for the MNIST\nand the MGH10 datasets respectively. Further, our approach provides an estimate\nof the uncertainty in the deformation-field by characterizing the true\nposterior distribution.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 08:15:49 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Khawaled", "Samah", ""], ["Freiman", "Moti", ""]]}, {"id": "2008.03964", "submitter": "Swaroop Mishra", "authors": "Swaroop Mishra, Anjana Arunkumar, Bhavdeep Sachdeva, Chris Bryan and\n  Chitta Baral", "title": "DQI: A Guide to Benchmark Evaluation", "comments": "ICML UDL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A `state of the art' model A surpasses humans in a benchmark B, but fails on\nsimilar benchmarks C, D, and E. What does B have that the other benchmarks do\nnot? Recent research provides the answer: spurious bias. However, developing A\nto solve benchmarks B through E does not guarantee that it will solve future\nbenchmarks. To progress towards a model that `truly learns' an underlying task,\nwe need to quantify the differences between successive benchmarks, as opposed\nto existing binary and black-box approaches. We propose a novel approach to\nsolve this underexplored task of quantifying benchmark quality by debuting a\ndata quality metric: DQI.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 08:38:55 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Mishra", "Swaroop", ""], ["Arunkumar", "Anjana", ""], ["Sachdeva", "Bhavdeep", ""], ["Bryan", "Chris", ""], ["Baral", "Chitta", ""]]}, {"id": "2008.03973", "submitter": "Zhenzhen Wang", "authors": "Zhenzhen Wang, Weixiang Hong and Junsong Yuan", "title": "Deep Reinforcement Learning with Label Embedding Reward for Supervised\n  Image Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep hashing has shown promising results in image retrieval and recognition.\nDespite its success, most existing deep hashing approaches are rather similar:\neither multi-layer perceptron or CNN is applied to extract image feature,\nfollowed by different binarization activation functions such as sigmoid, tanh\nor autoencoder to generate binary code. In this work, we introduce a novel\ndecision-making approach for deep supervised hashing. We formulate the hashing\nproblem as travelling across the vertices in the binary code space, and learn a\ndeep Q-network with a novel label embedding reward defined by\nBose-Chaudhuri-Hocquenghem (BCH) codes to explore the best path. Extensive\nexperiments and analysis on the CIFAR-10 and NUS-WIDE dataset show that our\napproach outperforms state-of-the-art supervised hashing methods under various\ncode lengths.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 09:17:20 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Wang", "Zhenzhen", ""], ["Hong", "Weixiang", ""], ["Yuan", "Junsong", ""]]}, {"id": "2008.03988", "submitter": "Wei Wang", "authors": "Wei Wang, Xiang-Gen Xia, Chuanjiang He, Zemin Ren, Jian Lu, Tianfu\n  Wang and Baiying Lei", "title": "A model-guided deep network for limited-angle computed tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we first propose a variational model for the limited-angle\ncomputed tomography (CT) image reconstruction and then convert the model into\nan end-to-end deep network.We use the penalty method to solve the model and\ndivide it into three iterative subproblems, where the first subproblem\ncompletes the sinograms by utilizing the prior information of sinograms in the\nfrequency domain and the second refines the CT images by using the prior\ninformation of CT images in the spatial domain, and the last merges the outputs\nof the first two subproblems. In each iteration, we use the convolutional\nneural networks (CNNs) to approxiamte the solutions of the first two\nsubproblems and, thus, obtain an end-to-end deep network for the limited-angle\nCT image reconstruction. Our network tackles both the sinograms and the CT\nimages, and can simultaneously suppress the artifacts caused by the incomplete\ndata and recover fine structural information in the CT images. Experimental\nresults show that our method outperforms the existing algorithms for the\nlimited-angle CT image reconstruction.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 09:42:32 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Wang", "Wei", ""], ["Xia", "Xiang-Gen", ""], ["He", "Chuanjiang", ""], ["Ren", "Zemin", ""], ["Lu", "Jian", ""], ["Wang", "Tianfu", ""], ["Lei", "Baiying", ""]]}, {"id": "2008.03996", "submitter": "Haoyu Chen", "authors": "Haoyu Chen, Zitong Yu, Xin Liu, Wei Peng, Yoon Lee, and Guoying Zhao", "title": "2nd Place Scheme on Action Recognition Track of ECCV 2020 VIPriors\n  Challenges: An Efficient Optical Flow Stream Guided Framework", "comments": "Technical report for ECCV 2020 VIPrior Challenge, Action Recognition\n  Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address the problem of training on small datasets for action recognition\ntasks, most prior works are either based on a large number of training samples\nor require pre-trained models transferred from other large datasets to tackle\noverfitting problems. However, it limits the research within organizations that\nhave strong computational abilities. In this work, we try to propose a\ndata-efficient framework that can train the model from scratch on small\ndatasets while achieving promising results. Specifically, by introducing a 3D\ncentral difference convolution operation, we proposed a novel C3D neural\nnetwork-based two-stream (Rank Pooling RGB and Optical Flow) framework for the\ntask. The method is validated on the action recognition track of the ECCV 2020\nVIPriors challenges and got the 2nd place (88.31%). It is proved that our\nmethod can achieve a promising result even without a pre-trained model on large\nscale datasets. The code will be released soon.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 09:50:28 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Chen", "Haoyu", ""], ["Yu", "Zitong", ""], ["Liu", "Xin", ""], ["Peng", "Wei", ""], ["Lee", "Yoon", ""], ["Zhao", "Guoying", ""]]}, {"id": "2008.03997", "submitter": "Stylianos Venieris", "authors": "Stefanos Laskaridis, Stylianos I. Venieris, Hyeji Kim and Nicholas D.\n  Lane", "title": "HAPI: Hardware-Aware Progressive Inference", "comments": "Accepted at the 39th International Conference on Computer-Aided\n  Design (ICCAD), 2020", "journal-ref": null, "doi": "10.1145/3400302.3415698", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have recently become the\nstate-of-the-art in a diversity of AI tasks. Despite their popularity, CNN\ninference still comes at a high computational cost. A growing body of work aims\nto alleviate this by exploiting the difference in the classification difficulty\namong samples and early-exiting at different stages of the network.\nNevertheless, existing studies on early exiting have primarily focused on the\ntraining scheme, without considering the use-case requirements or the\ndeployment platform. This work presents HAPI, a novel methodology for\ngenerating high-performance early-exit networks by co-optimising the placement\nof intermediate exits together with the early-exit strategy at inference time.\nFurthermore, we propose an efficient design space exploration algorithm which\nenables the faster traversal of a large number of alternative architectures and\ngenerates the highest-performing design, tailored to the use-case requirements\nand target hardware. Quantitative evaluation shows that our system consistently\noutperforms alternative search mechanisms and state-of-the-art early-exit\nschemes across various latency budgets. Moreover, it pushes further the\nperformance of highly optimised hand-crafted early-exit CNNs, delivering up to\n5.11x speedup over lightweight models on imposed latency-driven SLAs for\nembedded devices.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 09:55:18 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Laskaridis", "Stefanos", ""], ["Venieris", "Stylianos I.", ""], ["Kim", "Hyeji", ""], ["Lane", "Nicholas D.", ""]]}, {"id": "2008.04010", "submitter": "Hongchen Tan", "authors": "Hongchen Tan, Yuhao Bian, Huasheng Wang, Xiuping Liu, and Baocai Yin", "title": "Incomplete Descriptor Mining with Elastic Loss for Person\n  Re-Identification", "comments": "Acceped by IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT)", "journal-ref": null, "doi": "10.1109/TCSVT.2021.3061412", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel person Re-ID model, Consecutive Batch\nDropBlock Network (CBDB-Net), to capture the attentive and robust person\ndescriptor for the person Re-ID task. The CBDB-Net contains two novel designs:\nthe Consecutive Batch DropBlock Module (CBDBM) and the Elastic Loss (EL). In\nthe Consecutive Batch DropBlock Module (CBDBM), we firstly conduct uniform\npartition on the feature maps. And then, we independently and continuously drop\neach patch from top to bottom on the feature maps, which can output multiple\nincomplete feature maps. In the training stage, these multiple incomplete\nfeatures can better encourage the Re-ID model to capture the robust person\ndescriptor for the Re-ID task. In the Elastic Loss (EL), we design a novel\nweight control item to help the Re-ID model adaptively balance hard sample\npairs and easy sample pairs in the whole training process. Through an extensive\nset of ablation studies, we verify that the Consecutive Batch DropBlock Module\n(CBDBM) and the Elastic Loss (EL) each contribute to the performance boosts of\nCBDB-Net. We demonstrate that our CBDB-Net can achieve the competitive\nperformance on the three standard person Re-ID datasets (the Market-1501, the\nDukeMTMC-Re-ID, and the CUHK03 dataset), three occluded Person Re-ID datasets\n(the Occluded DukeMTMC, the Partial-REID, and the Partial iLIDS dataset), and a\ngeneral image retrieval dataset (In-Shop Clothes Retrieval dataset).\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 10:29:15 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 08:42:14 GMT"}, {"version": "v3", "created": "Mon, 17 Aug 2020 05:52:45 GMT"}, {"version": "v4", "created": "Thu, 4 Mar 2021 05:59:53 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Tan", "Hongchen", ""], ["Bian", "Yuhao", ""], ["Wang", "Huasheng", ""], ["Liu", "Xiuping", ""], ["Yin", "Baocai", ""]]}, {"id": "2008.04015", "submitter": "Hongchen Tan", "authors": "Hongchen Tan, Xiuping Liu, Shengjing Tian, Baocai Yin and Xin Li", "title": "MHSA-Net: Multi-Head Self-Attention Network for Occluded Person\n  Re-Identification", "comments": "Submitted to IEEE Transactions on Image Processing (TIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel person re-identification model, named Multi-Head\nSelf-Attention Network (MHSA-Net), to prune unimportant information and capture\nkey local information from person images. MHSA-Net contains two main novel\ncomponents: Multi-Head Self-Attention Branch (MHSAB) and Attention Competition\nMechanism (ACM). The MHSAM adaptively captures key local person information,\nand then produces effective diversity embeddings of an image for the person\nmatching. The ACM further helps filter out attention noise and non-key\ninformation. Through extensive ablation studies, we verified that the\nStructured Self-Attention Branch and Attention Competition Mechanism both\ncontribute to the performance improvement of the MHSA-Net. Our MHSA-Net\nachieves state-of-the-art performance especially on images with occlusions. We\nhave released our models (and will release the source codes after the paper is\naccepted) on https://github.com/hongchenphd/MHSA-Net.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 10:42:23 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 02:00:43 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2020 08:37:31 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Tan", "Hongchen", ""], ["Liu", "Xiuping", ""], ["Tian", "Shengjing", ""], ["Yin", "Baocai", ""], ["Li", "Xin", ""]]}, {"id": "2008.04017", "submitter": "Senthil Yogamani", "authors": "Varun Ravi Kumar, Marvin Klingner, Senthil Yogamani, Stefan Milz, Tim\n  Fingscheidt and Patrick Maeder", "title": "SynDistNet: Self-Supervised Monocular Fisheye Camera Distance Estimation\n  Synergized with Semantic Segmentation for Autonomous Driving", "comments": "Camera ready version + supplementary. Accepted for presentation at\n  Winter Conference on Applications of Computer Vision 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art self-supervised learning approaches for monocular depth\nestimation usually suffer from scale ambiguity. They do not generalize well\nwhen applied on distance estimation for complex projection models such as in\nfisheye and omnidirectional cameras. This paper introduces a novel multi-task\nlearning strategy to improve self-supervised monocular distance estimation on\nfisheye and pinhole camera images. Our contribution to this work is threefold:\nFirstly, we introduce a novel distance estimation network architecture using a\nself-attention based encoder coupled with robust semantic feature guidance to\nthe decoder that can be trained in a one-stage fashion. Secondly, we integrate\na generalized robust loss function, which improves performance significantly\nwhile removing the need for hyperparameter tuning with the reprojection loss.\nFinally, we reduce the artifacts caused by dynamic objects violating static\nworld assumptions using a semantic masking strategy. We significantly improve\nupon the RMSE of previous work on fisheye by 25% reduction in RMSE. As there is\nlittle work on fisheye cameras, we evaluated the proposed method on KITTI using\na pinhole model. We achieved state-of-the-art performance among self-supervised\nmethods without requiring an external scale estimation.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 10:52:47 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 21:58:32 GMT"}, {"version": "v3", "created": "Sat, 14 Nov 2020 21:03:21 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Kumar", "Varun Ravi", ""], ["Klingner", "Marvin", ""], ["Yogamani", "Senthil", ""], ["Milz", "Stefan", ""], ["Fingscheidt", "Tim", ""], ["Maeder", "Patrick", ""]]}, {"id": "2008.04021", "submitter": "Pourya Shamsolmoali", "authors": "Pourya Shamsolmoali, Masoumeh Zareapoor, Huiyu Zhou, Ruili Wang, and\n  Jie Yang", "title": "Road Segmentation for Remote Sensing Images using Adversarial Spatial\n  Pyramid Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2020.3016086", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Road extraction in remote sensing images is of great importance for a wide\nrange of applications. Because of the complex background, and high density,\nmost of the existing methods fail to accurately extract a road network that\nappears correct and complete. Moreover, they suffer from either insufficient\ntraining data or high costs of manual annotation. To address these problems, we\nintroduce a new model to apply structured domain adaption for synthetic image\ngeneration and road segmentation. We incorporate a feature pyramid network into\ngenerative adversarial networks to minimize the difference between the source\nand target domains. A generator is learned to produce quality synthetic images,\nand the discriminator attempts to distinguish them. We also propose a feature\npyramid network that improves the performance of the proposed model by\nextracting effective features from all the layers of the network for describing\ndifferent scales objects. Indeed, a novel scale-wise architecture is introduced\nto learn from the multi-level feature maps and improve the semantics of the\nfeatures. For optimization, the model is trained by a joint reconstruction loss\nfunction, which minimizes the difference between the fake images and the real\nones. A wide range of experiments on three datasets prove the superior\nperformance of the proposed approach in terms of accuracy and efficiency. In\nparticular, our model achieves state-of-the-art 78.86 IOU on the Massachusetts\ndataset with 14.89M parameters and 86.78B FLOPs, with 4x fewer FLOPs but higher\naccuracy (+3.47% IOU) than the top performer among state-of-the-art approaches\nused in the evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 11:00:19 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Shamsolmoali", "Pourya", ""], ["Zareapoor", "Masoumeh", ""], ["Zhou", "Huiyu", ""], ["Wang", "Ruili", ""], ["Yang", "Jie", ""]]}, {"id": "2008.04024", "submitter": "Xin Zhang", "authors": "Xin Zhang, Liangxiu Han, Wenyong Zhu, Liang Sun, Daoqiang Zhang", "title": "An Explainable 3D Residual Self-Attention Deep Neural Network FOR Joint\n  Atrophy Localization and Alzheimer's Disease Diagnosis using Structural MRI", "comments": "IEEE Journal of Biomedical and Health Informatics (2021)", "journal-ref": null, "doi": "10.1109/JBHI.2021.3066832", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-aided early diagnosis of Alzheimer's disease (AD) and its prodromal\nform mild cognitive impairment (MCI) based on structure Magnetic Resonance\nImaging (sMRI) has provided a cost-effective and objective way for early\nprevention and treatment of disease progression, leading to improved patient\ncare. In this work, we have proposed a novel computer-aided approach for early\ndiagnosis of AD by introducing an explainable 3D Residual Attention Deep Neural\nNetwork (3D ResAttNet) for end-to-end learning from sMRI scans. Different from\nthe existing approaches, the novelty of our approach is three-fold: 1) A\nResidual Self-Attention Deep Neural Network has been proposed to capture local,\nglobal and spatial information of MR images to improve diagnostic performance;\n2) An explanation method using Gradient-based Localization Class Activation\nmapping (Grad-CAM) has been introduced to improve the explainable of the\nproposed method; 3) This work has provided a full end-to-end learning solution\nfor automated disease diagnosis. Our proposed 3D ResAttNet method has been\nevaluated on a large cohort of subjects from real datasets for two changeling\nclassification tasks (i.e., Alzheimer's disease (AD) vs. Normal cohort (NC) and\nprogressive MCI (pMCI) vs. stable MCI (sMCI)). The experimental results show\nthat the proposed approach has a competitive advantage over the\nstate-of-the-art models in terms of accuracy performance and generalizability.\nThe explainable mechanism in our approach is able to identify and highlight the\ncontribution of the important brain parts (e.g., hippocampus, lateral ventricle\nand most parts of the cortex) for transparent decisions.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 11:08:55 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 23:44:53 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Zhang", "Xin", ""], ["Han", "Liangxiu", ""], ["Zhu", "Wenyong", ""], ["Sun", "Liang", ""], ["Zhang", "Daoqiang", ""]]}, {"id": "2008.04030", "submitter": "Zhe Chen", "authors": "Zhe Chen, Shohei Nobuhara, and Ko Nishino", "title": "Invertible Neural BRDF for Object Inverse Rendering", "comments": "accepted to ECCV 2020 as spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel neural network-based BRDF model and a Bayesian framework\nfor object inverse rendering, i.e., joint estimation of reflectance and natural\nillumination from a single image of an object of known geometry. The BRDF is\nexpressed with an invertible neural network, namely, normalizing flow, which\nprovides the expressive power of a high-dimensional representation,\ncomputational simplicity of a compact analytical model, and physical\nplausibility of a real-world BRDF. We extract the latent space of real-world\nreflectance by conditioning this model, which directly results in a strong\nreflectance prior. We refer to this model as the invertible neural BRDF model\n(iBRDF). We also devise a deep illumination prior by leveraging the structural\nbias of deep neural networks. By integrating this novel BRDF model and\nreflectance and illumination priors in a MAP estimation formulation, we show\nthat this joint estimation can be computed efficiently with stochastic gradient\ndescent. We experimentally validate the accuracy of the invertible neural BRDF\nmodel on a large number of measured data and demonstrate its use in object\ninverse rendering on a number of synthetic and real images. The results show\nnew ways in which deep neural networks can help solve challenging radiometric\ninverse problems.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 11:27:01 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 04:15:00 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Chen", "Zhe", ""], ["Nobuhara", "Shohei", ""], ["Nishino", "Ko", ""]]}, {"id": "2008.04031", "submitter": "Yifan Zhao", "authors": "Zeyuan Wang, Yifan Zhao, Jia Li, Yonghong Tian", "title": "Cooperative Bi-path Metric for Few-shot Learning", "comments": "9 pages, 4 figures. Accepted by ACM MultiMedia 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given base classes with sufficient labeled samples, the target of few-shot\nclassification is to recognize unlabeled samples of novel classes with only a\nfew labeled samples. Most existing methods only pay attention to the\nrelationship between labeled and unlabeled samples of novel classes, which do\nnot make full use of information within base classes. In this paper, we make\ntwo contributions to investigate the few-shot classification problem. First, we\nreport a simple and effective baseline trained on base classes in the way of\ntraditional supervised learning, which can achieve comparable results to the\nstate of the art. Second, based on the baseline, we propose a cooperative\nbi-path metric for classification, which leverages the correlations between\nbase classes and novel classes to further improve the accuracy. Experiments on\ntwo widely used benchmarks show that our method is a simple and effective\nframework, and a new state of the art is established in the few-shot\nclassification field.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 11:28:52 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Wang", "Zeyuan", ""], ["Zhao", "Yifan", ""], ["Li", "Jia", ""], ["Tian", "Yonghong", ""]]}, {"id": "2008.04047", "submitter": "Abdelhak Loukkal", "authors": "Abdelhak Loukkal (UTC), Yves Grandvalet (Heudiasyc), Tom Drummond, You\n  Li (NRCIEA)", "title": "Driving among Flatmobiles: Bird-Eye-View occupancy grids from a\n  monocular camera for holistic trajectory planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera-based end-to-end driving neural networks bring the promise of a\nlow-cost system that maps camera images to driving control commands. These\nnetworks are appealing because they replace laborious hand engineered building\nblocks but their black-box nature makes them difficult to delve in case of\nfailure. Recent works have shown the importance of using an explicit\nintermediate representation that has the benefits of increasing both the\ninterpretability and the accuracy of networks' decisions. Nonetheless, these\ncamera-based networks reason in camera view where scale is not homogeneous and\nhence not directly suitable for motion forecasting. In this paper, we introduce\na novel monocular camera-only holistic end-to-end trajectory planning network\nwith a Bird-Eye-View (BEV) intermediate representation that comes in the form\nof binary Occupancy Grid Maps (OGMs). To ease the prediction of OGMs in BEV\nfrom camera images, we introduce a novel scheme where the OGMs are first\npredicted as semantic masks in camera view and then warped in BEV using the\nhomography between the two planes. The key element allowing this transformation\nto be applied to 3D objects such as vehicles, consists in predicting solely\ntheir footprint in camera-view, hence respecting the flat world hypothesis\nimplied by the homography.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 12:16:44 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Loukkal", "Abdelhak", "", "UTC"], ["Grandvalet", "Yves", "", "Heudiasyc"], ["Drummond", "Tom", "", "NRCIEA"], ["Li", "You", "", "NRCIEA"]]}, {"id": "2008.04094", "submitter": "Alex Serban", "authors": "Alex Serban, Erik Poll, Joost Visser", "title": "Adversarial Examples on Object Recognition: A Comprehensive Survey", "comments": "Published in ACM CSUR. arXiv admin note: text overlap with\n  arXiv:1810.01185", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks are at the forefront of machine learning research.\nHowever, despite achieving impressive performance on complex tasks, they can be\nvery sensitive: Small perturbations of inputs can be sufficient to induce\nincorrect behavior. Such perturbations, called adversarial examples, are\nintentionally designed to test the network's sensitivity to distribution\ndrifts. Given their surprisingly small size, a wide body of literature\nconjectures on their existence and how this phenomenon can be mitigated. In\nthis article we discuss the impact of adversarial examples on security, safety,\nand robustness of neural networks. We start by introducing the hypotheses\nbehind their existence, the methods used to construct or protect against them,\nand the capacity to transfer adversarial examples between different machine\nlearning models. Altogether, the goal is to provide a comprehensive and\nself-contained survey of this growing field of research.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 08:51:21 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 09:53:48 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Serban", "Alex", ""], ["Poll", "Erik", ""], ["Visser", "Joost", ""]]}, {"id": "2008.04095", "submitter": "Luca Guarnera", "authors": "Luca Guarnera (1 and 2), Oliver Giudice (1), Sebastiano Battiato (1\n  and 2) ((1) University of Catania, (2) iCTLab s.r.l. - Spin-off of University\n  of Catania)", "title": "Fighting Deepfake by Exposing the Convolutional Traces on Images", "comments": "arXiv admin note: text overlap with arXiv:2004.10448", "journal-ref": "IEEE Access 2020", "doi": "10.1109/ACCESS.2020.3023037", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in Artificial Intelligence and Image Processing are changing the way\npeople interacts with digital images and video. Widespread mobile apps like\nFACEAPP make use of the most advanced Generative Adversarial Networks (GAN) to\nproduce extreme transformations on human face photos such gender swap, aging,\netc. The results are utterly realistic and extremely easy to be exploited even\nfor non-experienced users. This kind of media object took the name of Deepfake\nand raised a new challenge in the multimedia forensics field: the Deepfake\ndetection challenge. Indeed, discriminating a Deepfake from a real image could\nbe a difficult task even for human eyes but recent works are trying to apply\nthe same technology used for generating images for discriminating them with\npreliminary good results but with many limitations: employed Convolutional\nNeural Networks are not so robust, demonstrate to be specific to the context\nand tend to extract semantics from images. In this paper, a new approach aimed\nto extract a Deepfake fingerprint from images is proposed. The method is based\non the Expectation-Maximization algorithm trained to detect and extract a\nfingerprint that represents the Convolutional Traces (CT) left by GANs during\nimage generation. The CT demonstrates to have high discriminative power\nachieving better results than state-of-the-art in the Deepfake detection task\nalso proving to be robust to different attacks. Achieving an overall\nclassification accuracy of over 98%, considering Deepfakes from 10 different\nGAN architectures not only involved in images of faces, the CT demonstrates to\nbe reliable and without any dependence on image semantic. Finally, tests\ncarried out on Deepfakes generated by FACEAPP achieving 93% of accuracy in the\nfake detection task, demonstrated the effectiveness of the proposed technique\non a real-case scenario.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 08:49:23 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Guarnera", "Luca", "", "1 and 2"], ["Giudice", "Oliver", "", "1\n  and 2"], ["Battiato", "Sebastiano", "", "1\n  and 2"]]}, {"id": "2008.04114", "submitter": "Teena Sharma Ms.", "authors": "Vikas Singh, Pooja Agrawal, Teena Sharma, and Nishchal K. Verma", "title": "Improved Adaptive Type-2 Fuzzy Filter with Exclusively Two Fuzzy\n  Membership Function for Filtering Salt and Pepper Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image denoising is one of the preliminary steps in image processing methods\nin which the presence of noise can deteriorate the image quality. To overcome\nthis limitation, in this paper a improved two-stage fuzzy filter is proposed\nfor filtering salt and pepper noise from the images. In the first-stage, the\npixels in the image are categorized as good or noisy based on adaptive\nthresholding using type-2 fuzzy logic with exclusively two different membership\nfunctions in the filter window. In the second-stage, the noisy pixels are\ndenoised using modified ordinary fuzzy logic in the respective filter window.\nThe proposed filter is validated on standard images with various noise levels.\nThe proposed filter removes the noise and preserves useful image\ncharacteristics, i.e., edges and corners at higher noise level. The performance\nof the proposed filter is compared with the various state-of-the-art methods in\nterms of peak signal-to-noise ratio and computation time. To show the\neffectiveness of filter statistical tests, i.e., Friedman test and\nBonferroni-Dunn (BD) test are also carried out which clearly ascertain that the\nproposed filter outperforms in comparison of various filtering approaches.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 13:18:42 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Singh", "Vikas", ""], ["Agrawal", "Pooja", ""], ["Sharma", "Teena", ""], ["Verma", "Nishchal K.", ""]]}, {"id": "2008.04115", "submitter": "Hyeonseong Jeon", "authors": "Hyeonseong Jeon, Youngoh Bang, Junyaup Kim, and Simon S. Woo", "title": "T-GD: Transferable GAN-generated Images Detection Framework", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in Generative Adversarial Networks (GANs) enable the\ngeneration of highly realistic images, raising concerns about their misuse for\nmalicious purposes. Detecting these GAN-generated images (GAN-images) becomes\nincreasingly challenging due to the significant reduction of underlying\nartifacts and specific patterns. The absence of such traces can hinder\ndetection algorithms from identifying GAN-images and transferring knowledge to\nidentify other types of GAN-images as well. In this work, we present the\nTransferable GAN-images Detection framework T-GD, a robust transferable\nframework for an effective detection of GAN-images. T-GD is composed of a\nteacher and a student model that can iteratively teach and evaluate each other\nto improve the detection performance. First, we train the teacher model on the\nsource dataset and use it as a starting point for learning the target dataset.\nTo train the student model, we inject noise by mixing up the source and target\ndatasets, while constraining the weight variation to preserve the starting\npoint. Our approach is a self-training method, but distinguishes itself from\nprior approaches by focusing on improving the transferability of GAN-image\ndetection. T-GD achieves high performance on the source dataset by overcoming\ncatastrophic forgetting and effectively detecting state-of-the-art GAN-images\nwith only a small volume of data without any metadata information.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 13:20:19 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Jeon", "Hyeonseong", ""], ["Bang", "Youngoh", ""], ["Kim", "Junyaup", ""], ["Woo", "Simon S.", ""]]}, {"id": "2008.04139", "submitter": "Fabian Balsiger", "authors": "Fabian Balsiger, Alain Jungo, Olivier Scheidegger, Benjamin Marty,\n  Mauricio Reyes", "title": "Learning Bloch Simulations for MR Fingerprinting by Invertible Neural\n  Networks", "comments": "Accepted at MICCAI MLMIR 2020", "journal-ref": "Machine Learning for Medical Image Reconstruction. MLMIR 2020.\n  Lecture Notes in Computer Science, vol 12450. Springer, Cham", "doi": "10.1007/978-3-030-61598-7_6", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance fingerprinting (MRF) enables fast and multiparametric MR\nimaging. Despite fast acquisition, the state-of-the-art reconstruction of MRF\nbased on dictionary matching is slow and lacks scalability. To overcome these\nlimitations, neural network (NN) approaches estimating MR parameters from\nfingerprints have been proposed recently. Here, we revisit NN-based MRF\nreconstruction to jointly learn the forward process from MR parameters to\nfingerprints and the backward process from fingerprints to MR parameters by\nleveraging invertible neural networks (INNs). As a proof-of-concept, we perform\nvarious experiments showing the benefit of learning the forward process, i.e.,\nthe Bloch simulations, for improved MR parameter estimation. The benefit\nespecially accentuates when MR parameter estimation is difficult due to MR\nphysical restrictions. Therefore, INNs might be a feasible alternative to the\ncurrent solely backward-based NNs for MRF reconstruction.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 14:09:03 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 12:32:49 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Balsiger", "Fabian", ""], ["Jungo", "Alain", ""], ["Scheidegger", "Olivier", ""], ["Marty", "Benjamin", ""], ["Reyes", "Mauricio", ""]]}, {"id": "2008.04146", "submitter": "Yiheng Liu", "authors": "Yiheng Liu, Wengang Zhou, Mao Xi, Sanjing Shen, Houqiang Li", "title": "Vision Meets Wireless Positioning: Effective Person Re-identification\n  with Recurrent Context Propagation", "comments": "Accepted by ACM MM 2020 as Oral paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing person re-identification methods rely on the visual sensor to\ncapture the pedestrians. The image or video data from visual sensor inevitably\nsuffers the occlusion and dramatic variations of pedestrian postures, which\ndegrades the re-identification performance and further limits its application\nto the open environment. On the other hand, for most people, one of the most\nimportant carry-on items is the mobile phone, which can be sensed by WiFi and\ncellular networks in the form of a wireless positioning signal. Such signal is\nrobust to the pedestrian occlusion and visual appearance change, but suffers\nsome positioning error. In this work, we approach person re-identification with\nthe sensing data from both vision and wireless positioning. To take advantage\nof such cross-modality cues, we propose a novel recurrent context propagation\nmodule that enables information to propagate between visual data and wireless\npositioning data and finally improves the matching accuracy. To evaluate our\napproach, we contribute a new Wireless Positioning Person Re-identification\n(WP-ReID) dataset. Extensive experiments are conducted and demonstrate the\neffectiveness of the proposed algorithm. Code will be released at\nhttps://github.com/yolomax/WP-ReID.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 14:19:15 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 09:10:28 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Liu", "Yiheng", ""], ["Zhou", "Wengang", ""], ["Xi", "Mao", ""], ["Shen", "Sanjing", ""], ["Li", "Houqiang", ""]]}, {"id": "2008.04149", "submitter": "Xiaoyu Li", "authors": "Xiaoyu Li, Bo Zhang, Jing Liao and Pedro V. Sander", "title": "Deep Sketch-guided Cartoon Video Inbetweening", "comments": "15 pages, 16 figures", "journal-ref": null, "doi": "10.1109/TVCG.2021.3049419", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework to produce cartoon videos by fetching the color\ninformation from two input keyframes while following the animated motion guided\nby a user sketch. The key idea of the proposed approach is to estimate the\ndense cross-domain correspondence between the sketch and cartoon video frames,\nand employ a blending module with occlusion estimation to synthesize the middle\nframe guided by the sketch. After that, the input frames and the synthetic\nframe equipped with established correspondence are fed into an arbitrary-time\nframe interpolation pipeline to generate and refine additional inbetween\nframes. Finally, a module to preserve temporal consistency is employed.\nCompared to common frame interpolation methods, our approach can address frames\nwith relatively large motion and also has the flexibility to enable users to\ncontrol the generated video sequences by editing the sketch guidance. By\nexplicitly considering the correspondence between frames and the sketch, we can\nachieve higher quality results than other image synthesis methods. Our results\nshow that our system generalizes well to different movie frames, achieving\nbetter results than existing solutions.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 14:22:04 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 17:15:39 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Li", "Xiaoyu", ""], ["Zhang", "Bo", ""], ["Liao", "Jing", ""], ["Sander", "Pedro V.", ""]]}, {"id": "2008.04152", "submitter": "Mehdi Moradi", "authors": "Sandesh Ghimire, Satyananda Kashyap, Joy T. Wu, Alexandros Karargyris,\n  Mehdi Moradi", "title": "Learning Invariant Feature Representation to Improve Generalization\n  across Chest X-ray Datasets", "comments": "Accepted to Machine Learning in Medical Imaging (MLMI 2020), in\n  conjunction with MICCAI 2020, Oct. 4, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest radiography is the most common medical image examination for screening\nand diagnosis in hospitals. Automatic interpretation of chest X-rays at the\nlevel of an entry-level radiologist can greatly benefit work prioritization and\nassist in analyzing a larger population. Subsequently, several datasets and\ndeep learning-based solutions have been proposed to identify diseases based on\nchest X-ray images. However, these methods are shown to be vulnerable to shift\nin the source of data: a deep learning model performing well when tested on the\nsame dataset as training data, starts to perform poorly when it is tested on a\ndataset from a different source. In this work, we address this challenge of\ngeneralization to a new source by forcing the network to learn a\nsource-invariant representation. By employing an adversarial training strategy,\nwe show that a network can be forced to learn a source-invariant\nrepresentation. Through pneumonia-classification experiments on multi-source\nchest X-ray datasets, we show that this algorithm helps in improving\nclassification accuracy on a new source of X-ray dataset.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 07:41:15 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Ghimire", "Sandesh", ""], ["Kashyap", "Satyananda", ""], ["Wu", "Joy T.", ""], ["Karargyris", "Alexandros", ""], ["Moradi", "Mehdi", ""]]}, {"id": "2008.04157", "submitter": "Chenglizhao Chen", "authors": "Xuehao Wang, Shuai Li, Chenglizhao Chen, Aimin Hao, Hong Qin", "title": "Knowing Depth Quality In Advance: A Depth Quality Assessment Method For\n  RGB-D Salient Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous RGB-D salient object detection (SOD) methods have widely adopted\ndeep learning tools to automatically strike a trade-off between RGB and D\n(depth), whose key rationale is to take full advantage of their complementary\nnature, aiming for a much-improved SOD performance than that of using either of\nthem solely. However, such fully automatic fusions may not always be helpful\nfor the SOD task because the D quality itself usually varies from scene to\nscene. It may easily lead to a suboptimal fusion result if the D quality is not\nconsidered beforehand. Moreover, as an objective factor, the D quality has long\nbeen overlooked by previous work. As a result, it is becoming a clear\nperformance bottleneck. Thus, we propose a simple yet effective scheme to\nmeasure D quality in advance, the key idea of which is to devise a series of\nfeatures in accordance with the common attributes of high-quality D regions. To\nbe more concrete, we conduct D quality assessments for each image region,\nfollowing a multi-scale methodology that includes low-level edge consistency,\nmid-level regional uncertainty and high-level model variance. All these\ncomponents will be computed independently and then be assembled with RGB and D\nfeatures, applied as implicit indicators, to guide the selective fusion.\nCompared with the state-of-the-art fusion schemes, our method can achieve a\nmore reasonable fusion status between RGB and D. Specifically, the proposed D\nquality measurement method achieves steady performance improvements for almost\n2.0\\% in general.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 10:52:52 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Wang", "Xuehao", ""], ["Li", "Shuai", ""], ["Chen", "Chenglizhao", ""], ["Hao", "Aimin", ""], ["Qin", "Hong", ""]]}, {"id": "2008.04158", "submitter": "Chenglizhao Chen", "authors": "Zhenyu Wu, Shuai Li, Chenglizhao Chen, Aimin Hao, Hong Qin", "title": "Recursive Multi-model Complementary Deep Fusion forRobust Salient Object\n  Detection via Parallel Sub Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully convolutional networks have shown outstanding performance in the\nsalient object detection (SOD) field. The state-of-the-art (SOTA) methods have\na tendency to become deeper and more complex, which easily homogenize their\nlearned deep features, resulting in a clear performance bottleneck. In sharp\ncontrast to the conventional ``deeper'' schemes, this paper proposes a\n``wider'' network architecture which consists of parallel sub networks with\ntotally different network architectures. In this way, those deep features\nobtained via these two sub networks will exhibit large diversity, which will\nhave large potential to be able to complement with each other. However, a large\ndiversity may easily lead to the feature conflictions, thus we use the dense\nshort-connections to enable a recursively interaction between the parallel sub\nnetworks, pursuing an optimal complementary status between multi-model deep\nfeatures. Finally, all these complementary multi-model deep features will be\nselectively fused to make high-performance salient object detections. Extensive\nexperiments on several famous benchmarks clearly demonstrate the superior\nperformance, good generalization, and powerful learning ability of the proposed\nwider framework.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 10:39:11 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Wu", "Zhenyu", ""], ["Li", "Shuai", ""], ["Chen", "Chenglizhao", ""], ["Hao", "Aimin", ""], ["Qin", "Hong", ""]]}, {"id": "2008.04159", "submitter": "Chenglizhao Chen", "authors": "Chenglizhao Chen, Jipeng Wei, Chong Peng, Hong Qin", "title": "Depth Quality Aware Salient Object Detection", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2021.3052069", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing fusion based RGB-D salient object detection methods usually\nadopt the bi-stream structure to strike the fusion trade-off between RGB and\ndepth (D). The D quality usually varies from scene to scene, while the SOTA\nbi-stream approaches are depth quality unaware, which easily result in\nsubstantial difficulties in achieving complementary fusion status between RGB\nand D, leading to poor fusion results in facing of low-quality D. Thus, this\npaper attempts to integrate a novel depth quality aware subnet into the classic\nbi-stream structure, aiming to assess the depth quality before conducting the\nselective RGB-D fusion. Compared with the SOTA bi-stream methods, the major\nhighlight of our method is its ability to lessen the importance of those\nlow-quality, no-contribution, or even negative-contribution D regions during\nthe RGB-D fusion, achieving a much improved complementary status between RGB\nand D.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 09:54:39 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Chen", "Chenglizhao", ""], ["Wei", "Jipeng", ""], ["Peng", "Chong", ""], ["Qin", "Hong", ""]]}, {"id": "2008.04168", "submitter": "Fabian Timm", "authors": "Di Feng and Lars Rosenbaum and Fabian Timm and Klaus Dietmayer", "title": "Labels Are Not Perfect: Improving Probabilistic Object Detection via\n  Label Uncertainty", "comments": "A shorter version of this work is to appear at the Workshop on\n  Perception for Autonomous Driving, 16th European Conference on Computer\n  Vision (ECCV) 2020. Video to this work https://youtu.be/m05HnYietSk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable uncertainty estimation is crucial for robust object detection in\nautonomous driving. However, previous works on probabilistic object detection\neither learn predictive probability for bounding box regression in an\nun-supervised manner, or use simple heuristics to do uncertainty\nregularization. This leads to unstable training or suboptimal detection\nperformance. In this work, we leverage our previously proposed method for\nestimating uncertainty inherent in ground truth bounding box parameters (which\nwe call label uncertainty) to improve the detection accuracy of a probabilistic\nLiDAR-based object detector. Experimental results on the KITTI dataset show\nthat our method surpasses both the baseline model and the models based on\nsimple heuristics by up to 3.6% in terms of Average Precision.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 14:49:49 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Feng", "Di", ""], ["Rosenbaum", "Lars", ""], ["Timm", "Fabian", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "2008.04197", "submitter": "Timo Hinzmann", "authors": "Timo Hinzmann, Tobias Stegemann, Cesar Cadena, Roland Siegwart", "title": "Deep Learning-based Human Detection for UAVs with Optical and Infrared\n  Cameras: System and Experiments", "comments": "Initial submission; 21 pages, 16 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present our deep learning-based human detection system that\nuses optical (RGB) and long-wave infrared (LWIR) cameras to detect, track,\nlocalize, and re-identify humans from UAVs flying at high altitude. In each\nspectrum, a customized RetinaNet network with ResNet backbone provides human\ndetections which are subsequently fused to minimize the overall false detection\nrate. We show that by optimizing the bounding box anchors and augmenting the\nimage resolution the number of missed detections from high altitudes can be\ndecreased by over 20 percent. Our proposed network is compared to different\nRetinaNet and YOLO variants, and to a classical optical-infrared human\ndetection framework that uses hand-crafted features. Furthermore, along with\nthe publication of this paper, we release a collection of annotated\noptical-infrared datasets recorded with different UAVs during search-and-rescue\nfield tests and the source code of the implemented annotation tool.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 15:30:42 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Hinzmann", "Timo", ""], ["Stegemann", "Tobias", ""], ["Cadena", "Cesar", ""], ["Siegwart", "Roland", ""]]}, {"id": "2008.04200", "submitter": "Marco De Nadai", "authors": "Yahui Liu, Marco De Nadai, Deng Cai, Huayang Li, Xavier\n  Alameda-Pineda, Nicu Sebe and Bruno Lepri", "title": "Describe What to Change: A Text-guided Unsupervised Image-to-Image\n  Translation Approach", "comments": "Submitted to ACM MM '20, October 12-16, 2020, Seattle, WA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manipulating visual attributes of images through human-written text is a very\nchallenging task. On the one hand, models have to learn the manipulation\nwithout the ground truth of the desired output. On the other hand, models have\nto deal with the inherent ambiguity of natural language. Previous research\nusually requires either the user to describe all the characteristics of the\ndesired image or to use richly-annotated image captioning datasets. In this\nwork, we propose a novel unsupervised approach, based on image-to-image\ntranslation, that alters the attributes of a given image through a command-like\nsentence such as \"change the hair color to black\". Contrarily to\nstate-of-the-art approaches, our model does not require a human-annotated\ndataset nor a textual description of all the attributes of the desired image,\nbut only those that have to be modified. Our proposed model disentangles the\nimage content from the visual attributes, and it learns to modify the latter\nusing the textual description, before generating a new image from the content\nand the modified attribute representation. Because text might be inherently\nambiguous (blond hair may refer to different shadows of blond, e.g. golden,\nicy, sandy), our method generates multiple stochastic versions of the same\ntranslation. Experiments show that the proposed model achieves promising\nperformances on two large-scale public datasets: CelebA and CUB. We believe our\napproach will pave the way to new avenues of research combining textual and\nspeech commands with visual attributes.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 15:40:05 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Liu", "Yahui", ""], ["De Nadai", "Marco", ""], ["Cai", "Deng", ""], ["Li", "Huayang", ""], ["Alameda-Pineda", "Xavier", ""], ["Sebe", "Nicu", ""], ["Lepri", "Bruno", ""]]}, {"id": "2008.04221", "submitter": "Hengrong Lan", "authors": "Changchun Yang, Hengrong Lan, Feng Gao, and Fei Gao", "title": "Deep learning for photoacoustic imaging: a survey", "comments": "A review of deep learning for photoacoustic imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has been developed dramatically and witnessed a lot of\napplications in various fields over the past few years. This boom originated in\n2009, when a new model emerged, that is, the deep artificial neural network,\nwhich began to surpass other established mature models on some important\nbenchmarks. Later, it was widely used in academia and industry. Ranging from\nimage analysis to natural language processing, it fully exerted its magic and\nnow become the state-of-the-art machine learning models. Deep neural networks\nhave great potential in medical imaging technology, medical data analysis,\nmedical diagnosis and other healthcare issues, and is promoted in both\npre-clinical and even clinical stages. In this review, we performed an overview\nof some new developments and challenges in the application of machine learning\nto medical image analysis, with a special focus on deep learning in\nphotoacoustic imaging. The aim of this review is threefold: (i) introducing\ndeep learning with some important basics, (ii) reviewing recent works that\napply deep learning in the entire ecological chain of photoacoustic imaging,\nfrom image reconstruction to disease diagnosis, (iii) providing some open\nsource materials and other resources for researchers interested in applying\ndeep learning to photoacoustic imaging.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 15:53:30 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 01:24:07 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 05:00:38 GMT"}, {"version": "v4", "created": "Wed, 2 Dec 2020 02:02:26 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Yang", "Changchun", ""], ["Lan", "Hengrong", ""], ["Gao", "Feng", ""], ["Gao", "Fei", ""]]}, {"id": "2008.04237", "submitter": "Joon Son Chung", "authors": "Triantafyllos Afouras, Andrew Owens, Joon Son Chung, Andrew Zisserman", "title": "Self-Supervised Learning of Audio-Visual Objects from Video", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our objective is to transform a video into a set of discrete audio-visual\nobjects using self-supervised learning. To this end, we introduce a model that\nuses attention to localize and group sound sources, and optical flow to\naggregate information over time. We demonstrate the effectiveness of the\naudio-visual object embeddings that our model learns by using them for four\ndownstream speech-oriented tasks: (a) multi-speaker sound source separation,\n(b) localizing and tracking speakers, (c) correcting misaligned audio-visual\ndata, and (d) active speaker detection. Using our representation, these tasks\ncan be solved entirely by training on unlabeled video, without the aid of\nobject detectors. We also demonstrate the generality of our method by applying\nit to non-human speakers, including cartoons and puppets.Our model\nsignificantly outperforms other self-supervised approaches, and obtains\nperformance competitive with methods that use supervised face detection.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 16:18:01 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Afouras", "Triantafyllos", ""], ["Owens", "Andrew", ""], ["Chung", "Joon Son", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2008.04254", "submitter": "Baifeng Shi", "authors": "Baifeng Shi, Dinghuai Zhang, Qi Dai, Zhanxing Zhu, Yadong Mu, Jingdong\n  Wang", "title": "Informative Dropout for Robust Representation Learning: A Shape-bias\n  Perspective", "comments": "Accepted to ICML2020. Code is available at\n  https://github.com/bfshi/InfoDrop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are known to rely more on local texture\nrather than global shape when making decisions. Recent work also indicates a\nclose relationship between CNN's texture-bias and its robustness against\ndistribution shift, adversarial perturbation, random corruption, etc. In this\nwork, we attempt at improving various kinds of robustness universally by\nalleviating CNN's texture bias. With inspiration from the human visual system,\nwe propose a light-weight model-agnostic method, namely Informative Dropout\n(InfoDrop), to improve interpretability and reduce texture bias. Specifically,\nwe discriminate texture from shape based on local self-information in an image,\nand adopt a Dropout-like algorithm to decorrelate the model output from the\nlocal texture. Through extensive experiments, we observe enhanced robustness\nunder various scenarios (domain generalization, few-shot classification, image\ncorruption, and adversarial perturbation). To the best of our knowledge, this\nwork is one of the earliest attempts to improve different kinds of robustness\nin a unified model, shedding new light on the relationship between shape-bias\nand robustness, also on new approaches to trustworthy machine learning\nalgorithms. Code is available at https://github.com/bfshi/InfoDrop.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 16:52:24 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Shi", "Baifeng", ""], ["Zhang", "Dinghuai", ""], ["Dai", "Qi", ""], ["Zhu", "Zhanxing", ""], ["Mu", "Yadong", ""], ["Wang", "Jingdong", ""]]}, {"id": "2008.04370", "submitter": "Naama Hammel", "authors": "Ashish Bora, Siva Balasubramanian, Boris Babenko, Sunny Virmani,\n  Subhashini Venugopalan, Akinori Mitani, Guilherme de Oliveira Marinho, Jorge\n  Cuadros, Paisan Ruamviboonsuk, Greg S Corrado, Lily Peng, Dale R Webster,\n  Avinash V Varadarajan, Naama Hammel, Yun Liu, Pinal Bavishi", "title": "Predicting Risk of Developing Diabetic Retinopathy using Deep Learning", "comments": null, "journal-ref": null, "doi": "10.1016/S2589-7500(20)30250-8", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic retinopathy (DR) screening is instrumental in preventing blindness,\nbut faces a scaling challenge as the number of diabetic patients rises. Risk\nstratification for the development of DR may help optimize screening intervals\nto reduce costs while improving vision-related outcomes. We created and\nvalidated two versions of a deep learning system (DLS) to predict the\ndevelopment of mild-or-worse (\"Mild+\") DR in diabetic patients undergoing DR\nscreening. The two versions used either three-fields or a single field of color\nfundus photographs (CFPs) as input. The training set was derived from 575,431\neyes, of which 28,899 had known 2-year outcome, and the remaining were used to\naugment the training process via multi-task learning. Validation was performed\non both an internal validation set (set A; 7,976 eyes; 3,678 with known\noutcome) and an external validation set (set B; 4,762 eyes; 2,345 with known\noutcome). For predicting 2-year development of DR, the 3-field DLS had an area\nunder the receiver operating characteristic curve (AUC) of 0.79 (95%CI,\n0.78-0.81) on validation set A. On validation set B (which contained only a\nsingle field), the 1-field DLS's AUC was 0.70 (95%CI, 0.67-0.74). The DLS was\nprognostic even after adjusting for available risk factors (p<0.001). When\nadded to the risk factors, the 3-field DLS improved the AUC from 0.72 (95%CI,\n0.68-0.76) to 0.81 (95%CI, 0.77-0.84) in validation set A, and the 1-field DLS\nimproved the AUC from 0.62 (95%CI, 0.58-0.66) to 0.71 (95%CI, 0.68-0.75) in\nvalidation set B. The DLSs in this study identified prognostic information for\nDR development from CFPs. This information is independent of and more\ninformative than the available risk factors.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 19:07:22 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Bora", "Ashish", ""], ["Balasubramanian", "Siva", ""], ["Babenko", "Boris", ""], ["Virmani", "Sunny", ""], ["Venugopalan", "Subhashini", ""], ["Mitani", "Akinori", ""], ["Marinho", "Guilherme de Oliveira", ""], ["Cuadros", "Jorge", ""], ["Ruamviboonsuk", "Paisan", ""], ["Corrado", "Greg S", ""], ["Peng", "Lily", ""], ["Webster", "Dale R", ""], ["Varadarajan", "Avinash V", ""], ["Hammel", "Naama", ""], ["Liu", "Yun", ""], ["Bavishi", "Pinal", ""]]}, {"id": "2008.04378", "submitter": "Yang Li", "authors": "Yang Li, Shichao Kan, and Zhihai He", "title": "Unsupervised Deep Metric Learning with Transformed Attention Consistency\n  and Contrastive Clustering Loss", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches for unsupervised metric learning focus on exploring\nself-supervision information within the input image itself. We observe that,\nwhen analyzing images, human eyes often compare images against each other\ninstead of examining images individually. In addition, they often pay attention\nto certain keypoints, image regions, or objects which are discriminative\nbetween image classes but highly consistent within classes. Even if the image\nis being transformed, the attention pattern will be consistent. Motivated by\nthis observation, we develop a new approach to unsupervised deep metric\nlearning where the network is learned based on self-supervision information\nacross images instead of within one single image. To characterize the\nconsistent pattern of human attention during image comparisons, we introduce\nthe idea of transformed attention consistency. It assumes that visually similar\nimages, even undergoing different image transforms, should share the same\nconsistent visual attention map. This consistency leads to a pairwise\nself-supervision loss, allowing us to learn a Siamese deep neural network to\nencode and compare images against their transformed or matched pairs. To\nfurther enhance the inter-class discriminative power of the feature generated\nby this network, we adapt the concept of triplet loss from supervised metric\nlearning to our unsupervised case and introduce the contrastive clustering\nloss. Our extensive experimental results on benchmark datasets demonstrate that\nour proposed method outperforms current state-of-the-art methods for\nunsupervised metric learning by a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 19:33:47 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Li", "Yang", ""], ["Kan", "Shichao", ""], ["He", "Zhihai", ""]]}, {"id": "2008.04381", "submitter": "Hao Tang", "authors": "Hao Tang, Song Bai, Philip H.S. Torr, Nicu Sebe", "title": "Bipartite Graph Reasoning GANs for Person Image Generation", "comments": "13 pages, 6 figures, accepted to BMVC 2020 as an oral paper, fix\n  typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel Bipartite Graph Reasoning GAN (BiGraphGAN) for the\nchallenging person image generation task. The proposed graph generator mainly\nconsists of two novel blocks that aim to model the pose-to-pose and\npose-to-image relations, respectively. Specifically, the proposed Bipartite\nGraph Reasoning (BGR) block aims to reason the crossing long-range relations\nbetween the source pose and the target pose in a bipartite graph, which\nmitigates some challenges caused by pose deformation. Moreover, we propose a\nnew Interaction-and-Aggregation (IA) block to effectively update and enhance\nthe feature representation capability of both person's shape and appearance in\nan interactive way. Experiments on two challenging and public datasets, i.e.,\nMarket-1501 and DeepFashion, show the effectiveness of the proposed BiGraphGAN\nin terms of objective quantitative scores and subjective visual realness. The\nsource code and trained models are available at\nhttps://github.com/Ha0Tang/BiGraphGAN.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 19:37:10 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 22:01:35 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Tang", "Hao", ""], ["Bai", "Song", ""], ["Torr", "Philip H. S.", ""], ["Sebe", "Nicu", ""]]}, {"id": "2008.04393", "submitter": "Hoo Chang Shin", "authors": "Hoo-Chang Shin, Alvin Ihsani, Swetha Mandava, Sharath Turuvekere\n  Sreenivas, Christopher Forster, Jiook Cha and Alzheimer's Disease\n  Neuroimaging Initiative", "title": "GANBERT: Generative Adversarial Networks with Bidirectional Encoder\n  Representations from Transformers for MRI to PET synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing medical images, such as PET, is a challenging task due to the\nfact that the intensity range is much wider and denser than those in\nphotographs and digital renderings and are often heavily biased toward zero.\nAbove all, intensity values in PET have absolute significance, and are used to\ncompute parameters that are reproducible across the population. Yet, usually\nmuch manual adjustment has to be made in pre-/post- processing when\nsynthesizing PET images, because its intensity ranges can vary a lot, e.g.,\nbetween -100 to 1000 in floating point values. To overcome these challenges, we\nadopt the Bidirectional Encoder Representations from Transformers (BERT)\nalgorithm that has had great success in natural language processing (NLP),\nwhere wide-range floating point intensity values are represented as integers\nranging between 0 to 10000 that resemble a dictionary of natural language\nvocabularies. BERT is then trained to predict a proportion of masked values\nimages, where its \"next sentence prediction (NSP)\" acts as GAN discriminator.\nOur proposed approach, is able to generate PET images from MRI images in wide\nintensity range, with no manual adjustments in pre-/post- processing. It is a\nmethod that can scale and ready to deploy.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 20:07:33 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Shin", "Hoo-Chang", ""], ["Ihsani", "Alvin", ""], ["Mandava", "Swetha", ""], ["Sreenivas", "Sharath Turuvekere", ""], ["Forster", "Christopher", ""], ["Cha", "Jiook", ""], ["Initiative", "Alzheimer's Disease Neuroimaging", ""]]}, {"id": "2008.04396", "submitter": "Hoo Chang Shin", "authors": "Hoo-Chang Shin, Alvin Ihsani, Ziyue Xu, Swetha Mandava, Sharath\n  Turuvekere Sreenivas, Christopher Forster, Jiook Cha, and Alzheimer's Disease\n  Neuroimaging Initiative", "title": "GANDALF: Generative Adversarial Networks with Discriminator-Adaptive\n  Loss Fine-tuning for Alzheimer's Disease Diagnosis from MRI", "comments": "Accepted for publication at the MICCAI 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positron Emission Tomography (PET) is now regarded as the gold standard for\nthe diagnosis of Alzheimer's Disease (AD). However, PET imaging can be\nprohibitive in terms of cost and planning, and is also among the imaging\ntechniques with the highest dosage of radiation. Magnetic Resonance Imaging\n(MRI), in contrast, is more widely available and provides more flexibility when\nsetting the desired image resolution. Unfortunately, the diagnosis of AD using\nMRI is difficult due to the very subtle physiological differences between\nhealthy and AD subjects visible on MRI. As a result, many attempts have been\nmade to synthesize PET images from MR images using generative adversarial\nnetworks (GANs) in the interest of enabling the diagnosis of AD from MR.\nExisting work on PET synthesis from MRI has largely focused on Conditional\nGANs, where MR images are used to generate PET images and subsequently used for\nAD diagnosis. There is no end-to-end training goal. This paper proposes an\nalternative approach to the aforementioned, where AD diagnosis is incorporated\nin the GAN training objective to achieve the best AD classification\nperformance. Different GAN lossesare fine-tuned based on the discriminator\nperformance, and the overall training is stabilized. The proposed network\narchitecture and training regime show state-of-the-art performance for three-\nand four- class AD classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 20:09:35 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Shin", "Hoo-Chang", ""], ["Ihsani", "Alvin", ""], ["Xu", "Ziyue", ""], ["Mandava", "Swetha", ""], ["Sreenivas", "Sharath Turuvekere", ""], ["Forster", "Christopher", ""], ["Cha", "Jiook", ""], ["Initiative", "Alzheimer's Disease Neuroimaging", ""]]}, {"id": "2008.04428", "submitter": "Logan Gilmour", "authors": "Logan Gilmour, Nilanjan Ray", "title": "Locating Cephalometric X-Ray Landmarks with Foveated Pyramid Attention", "comments": "Presented at MIDL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CNNs, initially inspired by human vision, differ in a key way: they sample\nuniformly, rather than with highest density in a focal point. For very large\nimages, this makes training untenable, as the memory and computation required\nfor activation maps scales quadratically with the side length of an image. We\npropose an image pyramid based approach that extracts narrow glimpses of the of\nthe input image and iteratively refines them to accomplish regression tasks. To\nassist with high-accuracy regression, we introduce a novel intermediate\nrepresentation we call 'spatialized features'. Our approach scales\nlogarithmically with the side length, so it works with very large images. We\napply our method to Cephalometric X-ray Landmark Detection and get\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 21:44:45 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Gilmour", "Logan", ""], ["Ray", "Nilanjan", ""]]}, {"id": "2008.04431", "submitter": "Ameet Rahane", "authors": "Ameet Annasaheb Rahane and Anbumani Subramanian", "title": "Measures of Complexity for Large Scale Image Datasets", "comments": "6 pages, 3 tables, 4 figures", "journal-ref": null, "doi": "10.1109/ICAIIC48513.2020.9065274", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Large scale image datasets are a growing trend in the field of machine\nlearning. However, it is hard to quantitatively understand or specify how\nvarious datasets compare to each other - i.e., if one dataset is more complex\nor harder to ``learn'' with respect to a deep-learning based network. In this\nwork, we build a series of relatively computationally simple methods to measure\nthe complexity of a dataset. Furthermore, we present an approach to demonstrate\nvisualizations of high dimensional data, in order to assist with visual\ncomparison of datasets. We present our analysis using four datasets from the\nautonomous driving research community - Cityscapes, IDD, BDD and Vistas. Using\nentropy based metrics, we present a rank-order complexity of these datasets,\nwhich we compare with an established rank-order with respect to deep learning.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 21:54:23 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Rahane", "Ameet Annasaheb", ""], ["Subramanian", "Anbumani", ""]]}, {"id": "2008.04437", "submitter": "Shuyue Lan", "authors": "Shuyue Lan, Zhilu Wang, Amit K. Roy-Chowdhury, Ermin Wei, Qi Zhu", "title": "Distributed Multi-agent Video Fast-forwarding", "comments": "To appear at ACM Multimedia 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many intelligent systems, a network of agents collaboratively perceives\nthe environment for better and more efficient situation awareness. As these\nagents often have limited resources, it could be greatly beneficial to identify\nthe content overlapping among camera views from different agents and leverage\nit for reducing the processing, transmission and storage of\nredundant/unimportant video frames. This paper presents a consensus-based\ndistributed multi-agent video fast-forwarding framework, named DMVF, that\nfast-forwards multi-view video streams collaboratively and adaptively. In our\nframework, each camera view is addressed by a reinforcement learning based\nfast-forwarding agent, which periodically chooses from multiple strategies to\nselectively process video frames and transmits the selected frames at\nadjustable paces. During every adaptation period, each agent communicates with\na number of neighboring agents, evaluates the importance of the selected frames\nfrom itself and those from its neighbors, refines such evaluation together with\nother agents via a system-wide consensus algorithm, and uses such evaluation to\ndecide their strategy for the next period. Compared with approaches in the\nliterature on a real-world surveillance video dataset VideoWeb, our method\nsignificantly improves the coverage of important frames and also reduces the\nnumber of frames processed in the system.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 22:08:49 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Lan", "Shuyue", ""], ["Wang", "Zhilu", ""], ["Roy-Chowdhury", "Amit K.", ""], ["Wei", "Ermin", ""], ["Zhu", "Qi", ""]]}, {"id": "2008.04442", "submitter": "Guanqun Cao", "authors": "Guanqun Cao, Yi Zhou, Danushka Bollegala and Shan Luo", "title": "Spatio-temporal Attention Model for Tactile Texture Recognition", "comments": "7 pages, accepted by International Conference on Intelligent Robots\n  and Systems 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, tactile sensing has attracted great interest in robotics,\nespecially for facilitating exploration of unstructured environments and\neffective manipulation. A detailed understanding of the surface textures via\ntactile sensing is essential for many of these tasks. Previous works on texture\nrecognition using camera based tactile sensors have been limited to treating\nall regions in one tactile image or all samples in one tactile sequence\nequally, which includes much irrelevant or redundant information. In this\npaper, we propose a novel Spatio-Temporal Attention Model (STAM) for tactile\ntexture recognition, which is the very first of its kind to our best knowledge.\nThe proposed STAM pays attention to both spatial focus of each single tactile\ntexture and the temporal correlation of a tactile sequence. In the experiments\nto discriminate 100 different fabric textures, the spatially and temporally\nselective attention has resulted in a significant improvement of the\nrecognition accuracy, by up to 18.8%, compared to the non-attention based\nmodels. Specifically, after introducing noisy data that is collected before the\ncontact happens, our proposed STAM can learn the salient features efficiently\nand the accuracy can increase by 15.23% on average compared with the CNN based\nbaseline approach. The improved tactile texture perception can be applied to\nfacilitate robot tasks like grasping and manipulation.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 22:32:34 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Cao", "Guanqun", ""], ["Zhou", "Yi", ""], ["Bollegala", "Danushka", ""], ["Luo", "Shan", ""]]}, {"id": "2008.04451", "submitter": "Korrawe Karunratanakul", "authors": "Korrawe Karunratanakul, Jinlong Yang, Yan Zhang, Michael Black,\n  Krikamol Muandet, Siyu Tang", "title": "Grasping Field: Learning Implicit Representations for Human Grasps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic grasping of house-hold objects has made remarkable progress in recent\nyears. Yet, human grasps are still difficult to synthesize realistically. There\nare several key reasons: (1) the human hand has many degrees of freedom (more\nthan robotic manipulators); (2) the synthesized hand should conform to the\nsurface of the object; and (3) it should interact with the object in a\nsemantically and physically plausible manner. To make progress in this\ndirection, we draw inspiration from the recent progress on learning-based\nimplicit representations for 3D object reconstruction. Specifically, we propose\nan expressive representation for human grasp modelling that is efficient and\neasy to integrate with deep neural networks. Our insight is that every point in\na three-dimensional space can be characterized by the signed distances to the\nsurface of the hand and the object, respectively. Consequently, the hand, the\nobject, and the contact area can be represented by implicit surfaces in a\ncommon space, in which the proximity between the hand and the object can be\nmodelled explicitly. We name this 3D to 2D mapping as Grasping Field,\nparameterize it with a deep neural network, and learn it from data. We\ndemonstrate that the proposed grasping field is an effective and expressive\nrepresentation for human grasp generation. Specifically, our generative model\nis able to synthesize high-quality human grasps, given only on a 3D object\npoint cloud. The extensive experiments demonstrate that our generative model\ncompares favorably with a strong baseline and approaches the level of natural\nhuman grasps. Our method improves the physical plausibility of the hand-object\ncontact reconstruction and achieves comparable performance for 3D hand\nreconstruction compared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 23:08:26 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 21:24:21 GMT"}, {"version": "v3", "created": "Thu, 26 Nov 2020 16:07:13 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Karunratanakul", "Korrawe", ""], ["Yang", "Jinlong", ""], ["Zhang", "Yan", ""], ["Black", "Michael", ""], ["Muandet", "Krikamol", ""], ["Tang", "Siyu", ""]]}, {"id": "2008.04469", "submitter": "Jeffrey Byrne", "authors": "Jeffrey Byrne and Brian DeCann and Scott Bloom", "title": "Key-Nets: Optical Transformation Convolutional Networks for Privacy\n  Preserving Vision Sensors", "comments": "BMVC'20 (Best Paper - Runner up)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern cameras are not designed with computer vision or machine learning as\nthe target application. There is a need for a new class of vision sensors that\nare privacy preserving by design, that do not leak private information and\ncollect only the information necessary for a target machine learning task. In\nthis paper, we introduce key-nets, which are convolutional networks paired with\na custom vision sensor which applies an optical/analog transform such that the\nkey-net can perform exact encrypted inference on this transformed image, but\nthe image is not interpretable by a human or any other key-net. We provide five\nsufficient conditions for an optical transformation suitable for a key-net, and\nshow that generalized stochastic matrices (e.g. scale, bias and fractional\npixel shuffling) satisfy these conditions. We motivate the key-net by showing\nthat without it there is a utility/privacy tradeoff for a network fine-tuned\ndirectly on optically transformed images for face identification and object\ndetection. Finally, we show that a key-net is equivalent to homomorphic\nencryption using a Hill cipher, with an upper bound on memory and runtime that\nscales quadratically with a user specified privacy parameter. Therefore, the\nkey-net is the first practical, efficient and privacy preserving vision sensor\nbased on optical homomorphic encryption.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 01:21:29 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 13:50:29 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Byrne", "Jeffrey", ""], ["DeCann", "Brian", ""], ["Bloom", "Scott", ""]]}, {"id": "2008.04488", "submitter": "Zhuangzhuang Zhang", "authors": "Zhuangzhuang Zhang, Tianyu Zhao, Hiram Gay, Weixiong Zhang, Baozhou\n  Sun", "title": "ARPM-net: A novel CNN-based adversarial method with Markov Random Field\n  enhancement for prostate and organs at risk segmentation in pelvic CT images", "comments": "21 pages, 8 figures; accepted as a journal article at Medical\n  Physics; abstract presented at AAPM 2020", "journal-ref": null, "doi": "10.1002/mp.14580", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: The research is to develop a novel CNN-based adversarial deep\nlearning method to improve and expedite the multi-organ semantic segmentation\nof CT images, and to generate accurate contours on pelvic CT images. Methods:\nPlanning CT and structure datasets for 120 patients with intact prostate cancer\nwere retrospectively selected and divided for 10-fold cross-validation. The\nproposed adversarial multi-residual multi-scale pooling Markov Random Field\n(MRF) enhanced network (ARPM-net) implements an adversarial training scheme. A\nsegmentation network and a discriminator network were trained jointly, and only\nthe segmentation network was used for prediction. The segmentation network\nintegrates a newly designed MRF block into a variation of multi-residual U-net.\nThe discriminator takes the product of the original CT and the\nprediction/ground-truth as input and classifies the input into fake/real. The\nsegmentation network and discriminator network can be trained jointly as a\nwhole, or the discriminator can be used for fine-tuning after the segmentation\nnetwork is coarsely trained. Multi-scale pooling layers were introduced to\npreserve spatial resolution during pooling using less memory compared to atrous\nconvolution layers. An adaptive loss function was proposed to enhance the\ntraining on small or low contrast organs. The accuracy of modeled contours was\nmeasured with the Dice similarity coefficient (DSC), Average Hausdorff Distance\n(AHD), Average Surface Hausdorff Distance (ASHD), and relative Volume\nDifference (VD) using clinical contours as references to the ground-truth. The\nproposed ARPM-net method was compared to several stateof-the-art deep learning\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 02:40:53 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 23:12:59 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2020 00:42:32 GMT"}, {"version": "v4", "created": "Thu, 17 Sep 2020 21:28:26 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Zhang", "Zhuangzhuang", ""], ["Zhao", "Tianyu", ""], ["Gay", "Hiram", ""], ["Zhang", "Weixiong", ""], ["Sun", "Baozhou", ""]]}, {"id": "2008.04502", "submitter": "Ruoxi Shi", "authors": "Ruoxi Shi, Zhengrong Xue, Xinyang Li", "title": "Keypoint Autoencoders: Learning Interest Points of Semantics", "comments": "4 Pages. Conference: IMVIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Understanding point clouds is of great importance. Many previous methods\nfocus on detecting salient keypoints to identity structures of point clouds.\nHowever, existing methods neglect the semantics of points selected, leading to\npoor performance on downstream tasks. In this paper, we propose Keypoint\nAutoencoder, an unsupervised learning method for detecting keypoints. We\nencourage selecting sparse semantic keypoints by enforcing the reconstruction\nfrom keypoints to the original point cloud. To make sparse keypoint selection\ndifferentiable, Soft Keypoint Proposal is adopted by calculating weighted\naverages among input points. A downstream task of classifying shape with sparse\nkeypoints is conducted to demonstrate the distinctiveness of our selected\nkeypoints. Semantic Accuracy and Semantic Richness are proposed and our method\ngives competitive or even better performance than state of the arts on these\ntwo metrics.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 03:43:18 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Shi", "Ruoxi", ""], ["Xue", "Zhengrong", ""], ["Li", "Xinyang", ""]]}, {"id": "2008.04504", "submitter": "Li Jiacheng", "authors": "Jiacheng Li, Siliang Tang, Juncheng Li, Jun Xiao, Fei Wu, Shiliang Pu,\n  Yueting Zhuang", "title": "Topic Adaptation and Prototype Encoding for Few-Shot Visual Storytelling", "comments": "ACM Multimedia 2020", "journal-ref": null, "doi": "10.1145/3394171.3413886", "report-no": null, "categories": "cs.CL cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Storytelling~(VIST) is a task to tell a narrative story about a\ncertain topic according to the given photo stream. The existing studies focus\non designing complex models, which rely on a huge amount of human-annotated\ndata. However, the annotation of VIST is extremely costly and many topics\ncannot be covered in the training dataset due to the long-tail topic\ndistribution. In this paper, we focus on enhancing the generalization ability\nof the VIST model by considering the few-shot setting. Inspired by the way\nhumans tell a story, we propose a topic adaptive storyteller to model the\nability of inter-topic generalization. In practice, we apply the gradient-based\nmeta-learning algorithm on multi-modal seq2seq models to endow the model the\nability to adapt quickly from topic to topic. Besides, We further propose a\nprototype encoding structure to model the ability of intra-topic derivation.\nSpecifically, we encode and restore the few training story text to serve as a\nreference to guide the generation at inference time. Experimental results show\nthat topic adaptation and prototype encoding structure mutually bring benefit\nto the few-shot model on BLEU and METEOR metric. The further case study shows\nthat the stories generated after few-shot adaptation are more relative and\nexpressive.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 03:55:11 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Li", "Jiacheng", ""], ["Tang", "Siliang", ""], ["Li", "Juncheng", ""], ["Xiao", "Jun", ""], ["Wu", "Fei", ""], ["Pu", "Shiliang", ""], ["Zhuang", "Yueting", ""]]}, {"id": "2008.04509", "submitter": "Nguyen-Dong Ho", "authors": "Nguyen-Dong Ho, Ik-Joon Chang", "title": "TCL: an ANN-to-SNN Conversion with Trainable Clipping Layers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Spiking-neural-networks (SNNs) are promising at edge devices since the\nevent-driven operations of SNNs provides significantly lower power compared to\nanalog-neural-networks (ANNs). Although it is difficult to efficiently train\nSNNs, many techniques to convert trained ANNs to SNNs have been developed.\nHowever, after the conversion, a trade-off relation between accuracy and\nlatency exists in SNNs, causing considerable latency in large size datasets\nsuch as ImageNet. We present a technique, named as TCL, to alleviate the\ntrade-off problem, enabling the accuracy of 73.87% (VGG-16) and 70.37%\n(ResNet-34) for ImageNet with the moderate latency of 250 cycles in SNNs.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 04:20:27 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 06:46:20 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Ho", "Nguyen-Dong", ""], ["Chang", "Ik-Joon", ""]]}, {"id": "2008.04526", "submitter": "Srijay Deshpande", "authors": "Srijay Deshpande, Fayyaz Minhas, Simon Graham, Nasir Rajpoot", "title": "SAFRON: Stitching Across the Frontier for Generating Colorectal Cancer\n  Histology Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic images can be used for the development and evaluation of deep\nlearning algorithms in the context of limited availability of data. In the\nfield of computational pathology, where histology images are large in size and\nvisual context is crucial, synthesis of large high resolution images via\ngenerative modeling is a challenging task. This is due to memory and\ncomputational constraints hindering the generation of large images. To address\nthis challenge, we propose a novel SAFRON (Stitching Across the FRONtiers)\nframework to construct realistic, large high resolution tissue image tiles from\nground truth annotations while preserving morphological features and with\nminimal boundary artifacts. We show that the proposed method can generate\nrealistic image tiles of arbitrarily large size after training it on relatively\nsmall image patches. We demonstrate that our model can generate high quality\nimages, both visually and in terms of the Frechet Inception Distance. Compared\nto other existing approaches, our framework is efficient in terms of the memory\nrequirements for training and also in terms of the number of computations to\nconstruct a large high-resolution image. We also show that training on\nsynthetic data generated by SAFRON can significantly boost the performance of a\nstate-of-the-art algorithm for gland segmentation in colorectal cancer\nhistology images. Sample high resolution images generated using SAFRON are\navailable at the URL: https://warwick.ac.uk/TIALab/SAFRON\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 05:47:00 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 16:08:45 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Deshpande", "Srijay", ""], ["Minhas", "Fayyaz", ""], ["Graham", "Simon", ""], ["Rajpoot", "Nasir", ""]]}, {"id": "2008.04529", "submitter": "Chenxi Duan", "authors": "Chenxi Duan, Jun Pan, Rui Li", "title": "Thick Cloud Removal of Remote Sensing Images Using Temporal Smoothness\n  and Sparsity-Regularized Tensor Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In remote sensing images, the presence of thick cloud accompanying cloud\nshadow is a high probability event, which can affect the quality of subsequent\nprocessing and limit the scenarios of application. Hence, removing the thick\ncloud and cloud shadow as well as recovering the cloud-contaminated pixels is\nindispensable to make good use of remote sensing images. In this paper, a novel\nthick cloud removal method for remote sensing images based on temporal\nsmoothness and sparsity-regularized tensor optimization (TSSTO) is proposed.\nThe basic idea of TSSTO is that the thick cloud and cloud shadow are not only\nsparse but also smooth along the horizontal and vertical direction in images\nwhile the clean images are smooth along the temporal direction between images.\nTherefore, the sparsity norm is used to boost the sparsity of the cloud and\ncloud shadow, and unidirectional total variation (UTV) regularizers are applied\nto ensure the unidirectional smoothness. This paper utilizes alternation\ndirection method of multipliers to solve the presented model and generate the\ncloud and cloud shadow element as well as the clean element. The cloud and\ncloud shadow element is purified to get the cloud area and cloud shadow area.\nThen, the clean area of the original cloud-contaminated images is replaced to\nthe corresponding area of the clean element. Finally, the reference image is\nselected to reconstruct details of the cloud area and cloud shadow area using\nthe information cloning method. A series of experiments are conducted both on\nsimulated and real cloud-contaminated images from different sensors and with\ndifferent resolutions, and the results demonstrate the potential of the\nproposed TSSTO method for removing cloud and cloud shadow from both qualitative\nand quantitative viewpoints.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 05:59:20 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 04:28:23 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Duan", "Chenxi", ""], ["Pan", "Jun", ""], ["Li", "Rui", ""]]}, {"id": "2008.04556", "submitter": "Tianhao Zhang", "authors": "Tianhao Zhang, Hung-Yu Tseng, Lu Jiang, Honglak Lee, Irfan Essa,\n  Weilong Yang", "title": "Text as Neural Operator: Image Manipulation by Text Instruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, text-guided image manipulation has gained increasing\nattention in the image generation research field. Recent works have proposed to\ndeal with a simplified setting where the input image only has a single object\nand the text modification is acquired by swapping image captions or labels. In\nthis paper, we study a setting that allows users to edit an image with multiple\nobjects using complex text instructions. In this image generation task, the\ninputs are a reference image and an instruction in natural language that\ndescribes desired modifications to the input image. We propose a GAN-based\nmethod to tackle this problem. The key idea is to treat text as neural\noperators to locally modify the image feature. We show that the proposed model\nperforms favorably against recent baselines on three public datasets.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 07:07:10 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 02:35:11 GMT"}, {"version": "v3", "created": "Sun, 18 Oct 2020 08:39:15 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Zhang", "Tianhao", ""], ["Tseng", "Hung-Yu", ""], ["Jiang", "Lu", ""], ["Lee", "Honglak", ""], ["Essa", "Irfan", ""], ["Yang", "Weilong", ""]]}, {"id": "2008.04558", "submitter": "Hiroyuki Kobayashi", "authors": "Hiroyuki Kobayashi and Hitoshi Kiya", "title": "Extension of JPEG XS for Two-Layer Lossless Coding", "comments": "to appear in 2020 IEEE 9th Global Conference on Consumer Electronics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A two-layer lossless image coding method compatible with JPEG XS is proposed.\nJPEG XS is a new international standard for still image coding that has the\ncharacteristics of very low latency and very low complexity. However, it does\nnot support lossless coding, although it can achieve visual lossless coding.\nThe proposed method has a two-layer structure similar to JPEG XT, which\nconsists of JPEG XS coding and a lossless coding method. As a result, it\nenables us to losslessly restore original images, while maintaining\ncompatibility with JPEG XS.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 07:14:17 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Kobayashi", "Hiroyuki", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "2008.04575", "submitter": "Peter Meltzer", "authors": "Peter Meltzer, Marcelo Daniel Gutierrez Mallea and Peter J. Bentley", "title": "PiNet: Attention Pooling for Graph Classification", "comments": "4 pages, 3 figures 1 table", "journal-ref": "Neural Information Processing Systems (NIPS): Graph Representation\n  Learning Workshop 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose PiNet, a generalised differentiable attention-based pooling\nmechanism for utilising graph convolution operations for graph level\nclassification. We demonstrate high sample efficiency and superior performance\nover other graph neural networks in distinguishing isomorphic graph classes, as\nwell as competitive results with state of the art methods on standard\nchemo-informatics datasets.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 08:17:14 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Meltzer", "Peter", ""], ["Mallea", "Marcelo Daniel Gutierrez", ""], ["Bentley", "Peter J.", ""]]}, {"id": "2008.04582", "submitter": "Xinzhu Ma", "authors": "Xinzhu Ma, Shinan Liu, Zhiyi Xia, Hongwen Zhang, Xingyu Zeng and Wanli\n  Ouyang", "title": "Rethinking Pseudo-LiDAR Representation", "comments": "ECCV2020. Supplemental Material attached", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed pseudo-LiDAR based 3D detectors greatly improve the\nbenchmark of monocular/stereo 3D detection task. However, the underlying\nmechanism remains obscure to the research community. In this paper, we perform\nan in-depth investigation and observe that the efficacy of pseudo-LiDAR\nrepresentation comes from the coordinate transformation, instead of data\nrepresentation itself. Based on this observation, we design an image based CNN\ndetector named Patch-Net, which is more generalized and can be instantiated as\npseudo-LiDAR based 3D detectors. Moreover, the pseudo-LiDAR data in our\nPatchNet is organized as the image representation, which means existing 2D CNN\ndesigns can be easily utilized for extracting deep features from input data and\nboosting 3D detection performance. We conduct extensive experiments on the\nchallenging KITTI dataset, where the proposed PatchNet outperforms all existing\npseudo-LiDAR based counterparts. Code has been made available at:\nhttps://github.com/xinzhuma/patchnet.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 08:44:18 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Ma", "Xinzhu", ""], ["Liu", "Shinan", ""], ["Xia", "Zhiyi", ""], ["Zhang", "Hongwen", ""], ["Zeng", "Xingyu", ""], ["Ouyang", "Wanli", ""]]}, {"id": "2008.04585", "submitter": "YueFeng Chen", "authors": "Xiaodan Li, Yining Lang, Yuefeng Chen, Xiaofeng Mao, Yuan He, Shuhui\n  Wang, Hui Xue, Quan Lu", "title": "Sharp Multiple Instance Learning for DeepFake Video Detection", "comments": "Accepted at ACM MM 2020. 11 pages, 8 figures, with appendix", "journal-ref": "Proceedings of the 28th ACM International Conference on\n  Multimedia, 2020", "doi": "10.1145/3394171.3414034", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of facial manipulation techniques, face forgery\nhas received considerable attention in multimedia and computer vision community\ndue to security concerns. Existing methods are mostly designed for single-frame\ndetection trained with precise image-level labels or for video-level prediction\nby only modeling the inter-frame inconsistency, leaving potential high risks\nfor DeepFake attackers. In this paper, we introduce a new problem of partial\nface attack in DeepFake video, where only video-level labels are provided but\nnot all the faces in the fake videos are manipulated. We address this problem\nby multiple instance learning framework, treating faces and input video as\ninstances and bag respectively. A sharp MIL (S-MIL) is proposed which builds\ndirect mapping from instance embeddings to bag prediction, rather than from\ninstance embeddings to instance prediction and then to bag prediction in\ntraditional MIL. Theoretical analysis proves that the gradient vanishing in\ntraditional MIL is relieved in S-MIL. To generate instances that can accurately\nincorporate the partially manipulated faces, spatial-temporal encoded instance\nis designed to fully model the intra-frame and inter-frame inconsistency, which\nfurther helps to promote the detection performance. We also construct a new\ndataset FFPMS for partially attacked DeepFake video detection, which can\nbenefit the evaluation of different methods at both frame and video levels.\nExperiments on FFPMS and the widely used DFDC dataset verify that S-MIL is\nsuperior to other counterparts for partially attacked DeepFake video detection.\nIn addition, S-MIL can also be adapted to traditional DeepFake image detection\ntasks and achieve state-of-the-art performance on single-frame datasets.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 08:52:17 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Li", "Xiaodan", ""], ["Lang", "Yining", ""], ["Chen", "Yuefeng", ""], ["Mao", "Xiaofeng", ""], ["He", "Yuan", ""], ["Wang", "Shuhui", ""], ["Xue", "Hui", ""], ["Lu", "Quan", ""]]}, {"id": "2008.04590", "submitter": "Steffen Illium", "authors": "Steffen Illium, Robert M\\\"uller, Andreas Sedlmeier and Claudia\n  Linnhoff-Popien", "title": "Surgical Mask Detection with Convolutional Neural Networks and Data\n  Augmentations on Spectrograms", "comments": "5 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many fields of research, labeled datasets are hard to acquire. This is\nwhere data augmentation promises to overcome the lack of training data in the\ncontext of neural network engineering and classification tasks. The idea here\nis to reduce model over-fitting to the feature distribution of a small\nunder-descriptive training dataset. We try to evaluate such data augmentation\ntechniques to gather insights in the performance boost they provide for several\nconvolutional neural networks on mel-spectrogram representations of audio data.\nWe show the impact of data augmentation on the binary classification task of\nsurgical mask detection in samples of human voice (ComParE Challenge 2020).\nAlso we consider four varying architectures to account for augmentation\nrobustness. Results show that most of the baselines given by ComParE are\noutperformed.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 09:02:47 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Illium", "Steffen", ""], ["M\u00fcller", "Robert", ""], ["Sedlmeier", "Andreas", ""], ["Linnhoff-Popien", "Claudia", ""]]}, {"id": "2008.04594", "submitter": "Jonathan Zopes", "authors": "Jonathan Zopes, Moritz Platscher, Silvio Paganucci, Christian Federau", "title": "Multi-modal segmentation of 3D brain scans using neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To implement a brain segmentation pipeline based on convolutional\nneural networks, which rapidly segments 3D volumes into 27 anatomical\nstructures. To provide an extensive, comparative study of segmentation\nperformance on various contrasts of magnetic resonance imaging (MRI) and\ncomputed tomography (CT) scans. Methods: Deep convolutional neural networks are\ntrained to segment 3D MRI (MPRAGE, DWI, FLAIR) and CT scans. A large database\nof in total 851 MRI/CT scans is used for neural network training. Training\nlabels are obtained on the MPRAGE contrast and coregistered to the other\nimaging modalities. The segmentation quality is quantified using the Dice\nmetric for a total of 27 anatomical structures. Dropout sampling is implemented\nto identify corrupted input scans or low-quality segmentations. Full\nsegmentation of 3D volumes with more than 2 million voxels is obtained in less\nthan 1s of processing time on a graphical processing unit. Results: The best\naverage Dice score is found on $T_1$-weighted MPRAGE ($85.3\\pm4.6\\,\\%$).\nHowever, for FLAIR ($80.0\\pm7.1\\,\\%$), DWI ($78.2\\pm7.9\\,\\%$) and CT ($79.1\\pm\n7.9\\,\\%$), good-quality segmentation is feasible for most anatomical\nstructures. Corrupted input volumes or low-quality segmentations can be\ndetected using dropout sampling. Conclusion: The flexibility and performance of\ndeep convolutional neural networks enables the direct, real-time segmentation\nof FLAIR, DWI and CT scans without requiring $T_1$-weighted scans.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 09:13:54 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Zopes", "Jonathan", ""], ["Platscher", "Moritz", ""], ["Paganucci", "Silvio", ""], ["Federau", "Christian", ""]]}, {"id": "2008.04615", "submitter": "Aysen Degerli", "authors": "Serkan Kiranyaz, Aysen Degerli, Tahir Hamid, Rashid Mazhar, Rayyan\n  Ahmed, Rayaan Abouhasera, Morteza Zabihi, Junaid Malik, Ridha Hamila, and\n  Moncef Gabbouj", "title": "Left Ventricular Wall Motion Estimation by Active Polynomials for Acute\n  Myocardial Infarction Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Echocardiogram (echo) is the earliest and the primary tool for identifying\nregional wall motion abnormalities (RWMA) in order to diagnose myocardial\ninfarction (MI) or commonly known as heart attack. This paper proposes a novel\napproach, Active Polynomials, which can accurately and robustly estimate the\nglobal motion of the Left Ventricular (LV) wall from any echo in a robust and\naccurate way. The proposed algorithm quantifies the true wall motion occurring\nin LV wall segments so as to assist cardiologists diagnose early signs of an\nacute MI. It further enables medical experts to gain an enhanced visualization\ncapability of echo images through color-coded segments along with their\n\"maximum motion displacement\" plots helping them to better assess wall motion\nand LV Ejection-Fraction (LVEF). The outputs of the method can further help\necho-technicians to assess and improve the quality of the echocardiogram\nrecording. A major contribution of this study is the first public echo database\ncollection composed by physicians at the Hamad Medical Corporation Hospital in\nQatar. The so-called HMC-QU database will serve as the benchmark for the\nforthcoming relevant studies. The results over the HMC-QU dataset show that the\nproposed approach can achieve high accuracy, sensitivity and precision in MI\ndetection even though the echo quality is quite poor, and the temporal\nresolution is low.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 10:29:22 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Kiranyaz", "Serkan", ""], ["Degerli", "Aysen", ""], ["Hamid", "Tahir", ""], ["Mazhar", "Rashid", ""], ["Ahmed", "Rayyan", ""], ["Abouhasera", "Rayaan", ""], ["Zabihi", "Morteza", ""], ["Malik", "Junaid", ""], ["Hamila", "Ridha", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "2008.04619", "submitter": "Timo Hinzmann", "authors": "Timo Hinzmann, Roland Siegwart", "title": "Deep UAV Localization with Reference View Rendering", "comments": "Initial submission; 15 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a framework for the localization of Unmanned Aerial\nVehicles (UAVs) in unstructured environments with the help of deep learning. A\nreal-time rendering engine is introduced that generates optical and depth\nimages given a six Degrees-of-Freedom (DoF) camera pose, camera model,\ngeo-referenced orthoimage, and elevation map. The rendering engine is embedded\ninto a learning-based six-DoF Inverse Compositional Lucas-Kanade (ICLK)\nalgorithm that is able to robustly align the rendered and real-world image\ntaken by the UAV. To learn the alignment under environmental changes, the\narchitecture is trained using maps spanning multiple years at high resolution.\nThe evaluation shows that the deep 6DoF-ICLK algorithm outperforms its\nnon-trainable counterparts by a large margin. To further support the research\nin this field, the real-time rendering engine and accompanying datasets are\nreleased along with this publication.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 10:54:36 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Hinzmann", "Timo", ""], ["Siegwart", "Roland", ""]]}, {"id": "2008.04620", "submitter": "Laura D\\\"orr", "authors": "Laura D\\\"orr, Felix Brandt, Martin Pouls, Alexander Naumann", "title": "Fully-Automated Packaging Structure Recognition in Logistics\n  Environments", "comments": "Accepted for IEEE International Conference on Emerging Technologies\n  and Factory Automation ETFA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Within a logistics supply chain, a large variety of transported goods need to\nbe handled, recognized and checked at many different network points. Often,\nhuge manual effort is involved in recognizing or verifying packet identity or\npackaging structure, for instance to check the delivery for completeness. We\npropose a method for complete automation of packaging structure recognition:\nBased on a single image, one or multiple transport units are localized and, for\neach of these transport units, the characteristics, the total number and the\narrangement of its packaging units is recognized. Our algorithm is based on\ndeep learning models, more precisely convolutional neural networks for instance\nsegmentation in images, as well as computer vision methods and heuristic\ncomponents. We use a custom data set of realistic logistics images for training\nand evaluation of our method. We show that the solution is capable of correctly\nrecognizing the packaging structure in approximately 85% of our test cases, and\neven more (91%) when focusing on most common package types.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 10:57:23 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["D\u00f6rr", "Laura", ""], ["Brandt", "Felix", ""], ["Pouls", "Martin", ""], ["Naumann", "Alexander", ""]]}, {"id": "2008.04621", "submitter": "Jireh Jam", "authors": "Jireh Jam and Connah Kendrick and Vincent Drouard and Kevin Walker and\n  Gee-Sern Hsu and Moi Hoon Yap", "title": "R-MNet: A Perceptual Adversarial Network for Image Inpainting", "comments": "10 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial image inpainting is a problem that is widely studied, and in recent\nyears the introduction of Generative Adversarial Networks, has led to\nimprovements in the field. Unfortunately some issues persists, in particular\nwhen blending the missing pixels with the visible ones. We address the problem\nby proposing a Wasserstein GAN combined with a new reverse mask operator,\nnamely Reverse Masking Network (R-MNet), a perceptual adversarial network for\nimage inpainting. The reverse mask operator transfers the reverse masked image\nto the end of the encoder-decoder network leaving only valid pixels to be\ninpainted. Additionally, we propose a new loss function computed in feature\nspace to target only valid pixels combined with adversarial training. These\nthen capture data distributions and generate images similar to those in the\ntraining data with achieved realism (realistic and coherent) on the output\nimages. We evaluate our method on publicly available dataset, and compare with\nstate-of-the-art methods. We show that our method is able to generalize to\nhigh-resolution inpainting task, and further show more realistic outputs that\nare plausible to the human visual system when compared with the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 10:58:10 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 13:11:10 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 17:08:51 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Jam", "Jireh", ""], ["Kendrick", "Connah", ""], ["Drouard", "Vincent", ""], ["Walker", "Kevin", ""], ["Hsu", "Gee-Sern", ""], ["Yap", "Moi Hoon", ""]]}, {"id": "2008.04637", "submitter": "Amit Moryossef", "authors": "Amit Moryossef, Ioannis Tsochantaridis, Roee Aharoni, Sarah Ebling,\n  and Srini Narayanan", "title": "Real-Time Sign Language Detection using Human Pose Estimation", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a lightweight real-time sign language detection model, as we\nidentify the need for such a case in videoconferencing. We extract optical flow\nfeatures based on human pose estimation and, using a linear classifier, show\nthese features are meaningful with an accuracy of 80%, evaluated on the DGS\nCorpus. Using a recurrent model directly on the input, we see improvements of\nup to 91% accuracy, while still working under 4ms. We describe a demo\napplication to sign language detection in the browser in order to demonstrate\nits usage possibility in videoconferencing applications.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 11:42:03 GMT"}, {"version": "v2", "created": "Sun, 13 Sep 2020 11:40:20 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Moryossef", "Amit", ""], ["Tsochantaridis", "Ioannis", ""], ["Aharoni", "Roee", ""], ["Ebling", "Sarah", ""], ["Narayanan", "Srini", ""]]}, {"id": "2008.04646", "submitter": "Willi Menapace", "authors": "Willi Menapace, St\\'ephane Lathuili\\`ere and Elisa Ricci", "title": "Learning to Cluster under Domain Shift", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While unsupervised domain adaptation methods based on deep architectures have\nachieved remarkable success in many computer vision tasks, they rely on a\nstrong assumption, i.e. labeled source data must be available. In this work we\novercome this assumption and we address the problem of transferring knowledge\nfrom a source to a target domain when both source and target data have no\nannotations. Inspired by recent works on deep clustering, our approach\nleverages information from data gathered from multiple source domains to build\na domain-agnostic clustering model which is then refined at inference time when\ntarget data become available. Specifically, at training time we propose to\noptimize a novel information-theoretic loss which, coupled with\ndomain-alignment layers, ensures that our model learns to correctly discover\nsemantic labels while discarding domain-specific features. Importantly, our\narchitecture design ensures that at inference time the resulting source model\ncan be effectively adapted to the target domain without having access to source\ndata, thanks to feature alignment and self-supervision. We evaluate the\nproposed approach in a variety of settings, considering several domain\nadaptation benchmarks and we show that our method is able to automatically\ndiscover relevant semantic information even in presence of few target samples\nand yields state-of-the-art results on multiple domain adaptation benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 12:03:01 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Menapace", "Willi", ""], ["Lathuili\u00e8re", "St\u00e9phane", ""], ["Ricci", "Elisa", ""]]}, {"id": "2008.04673", "submitter": "Yang Chen", "authors": "Yang Chen, Martin Alain, Aljosa Smolic", "title": "Fast and Accurate Optical Flow based Depth Map Estimation from Light\n  Fields", "comments": "Accepted at IMVIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth map estimation is a crucial task in computer vision, and new approaches\nhave recently emerged taking advantage of light fields, as this new imaging\nmodality captures much more information about the angular direction of light\nrays compared to common approaches based on stereoscopic images or multi-view.\nIn this paper, we propose a novel depth estimation method from light fields\nbased on existing optical flow estimation methods. The optical flow estimator\nis applied on a sequence of images taken along an angular dimension of the\nlight field, which produces several disparity map estimates. Considering both\naccuracy and efficiency, we choose the feature flow method as our optical flow\nestimator. Thanks to its spatio-temporal edge-aware filtering properties, the\ndifferent disparity map estimates that we obtain are very consistent, which\nallows a fast and simple aggregation step to create a single disparity map,\nwhich can then converted into a depth map. Since the disparity map estimates\nare consistent, we can also create a depth map from each disparity estimate,\nand then aggregate the different depth maps in the 3D space to create a single\ndense depth map.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 12:53:31 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Chen", "Yang", ""], ["Alain", "Martin", ""], ["Smolic", "Aljosa", ""]]}, {"id": "2008.04679", "submitter": "Brian Groenke", "authors": "Brian Groenke, Luke Madaus, Claire Monteleoni", "title": "ClimAlign: Unsupervised statistical downscaling of climate variables via\n  normalizing flows", "comments": "8 pages, submitted as journal paper to the 10th International\n  Conference on Climate Informatics (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Downscaling is a landmark task in climate science and meteorology in which\nthe goal is to use coarse scale, spatio-temporal data to infer values at finer\nscales. Statistical downscaling aims to approximate this task using statistical\npatterns gleaned from an existing dataset of downscaled values, often obtained\nfrom observations or physical models. In this work, we investigate the\napplication of deep latent variable learning to the task of statistical\ndownscaling. We present ClimAlign, a novel method for unsupervised, generative\ndownscaling using adaptations of recent work in normalizing flows for\nvariational inference. We evaluate the viability of our method using several\ndifferent metrics on two datasets consisting of daily temperature and\nprecipitation values gridded at low (1 degree latitude/longitude) and high (1/4\nand 1/8 degree) resolutions. We show that our method achieves comparable\npredictive performance to existing supervised statistical downscaling methods\nwhile simultaneously allowing for both conditional and unconditional sampling\nfrom the joint distribution over high and low resolution spatial fields. We\nprovide publicly accessible implementations of our method, as well as the\nbaselines used for comparison, on GitHub.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 13:01:53 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Groenke", "Brian", ""], ["Madaus", "Luke", ""], ["Monteleoni", "Claire", ""]]}, {"id": "2008.04690", "submitter": "D\\'ario Oliveira", "authors": "Dario Augusto Borges Oliveira", "title": "Implanting Synthetic Lesions for Improving Liver Lesion Segmentation in\n  CT Exams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of supervised lesion segmentation algorithms using Computed\nTomography (CT) exams depends significantly on the quantity and variability of\nsamples available for training. While annotating such data constitutes a\nchallenge itself, the variability of lesions in the dataset also depends on the\nprevalence of different types of lesions. This phenomenon adds an inherent bias\nto lesion segmentation algorithms that can be diminished, among different\npossibilities, using aggressive data augmentation methods. In this paper, we\npresent a method for implanting realistic lesions in CT slices to provide a\nrich and controllable set of training samples and ultimately improving semantic\nsegmentation network performances for delineating lesions in CT exams. Our\nresults show that implanting synthetic lesions not only improves (up to around\n12\\%) the segmentation performance considering different architectures but also\nthat this improvement is consistent among different image synthesis networks.\nWe conclude that increasing the variability of lesions synthetically in terms\nof size, density, shape, and position seems to improve the performance of\nsegmentation models for liver lesion segmentation in CT slices.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 13:23:04 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Oliveira", "Dario Augusto Borges", ""]]}, {"id": "2008.04693", "submitter": "Eunhyeok Park", "authors": "Eunhyeok Park and Sungjoo Yoo", "title": "PROFIT: A Novel Training Method for sub-4-bit MobileNet Models", "comments": "Published at ECCV2020, spotlight paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  4-bit and lower precision mobile models are required due to the\never-increasing demand for better energy efficiency in mobile devices. In this\nwork, we report that the activation instability induced by weight quantization\n(AIWQ) is the key obstacle to sub-4-bit quantization of mobile networks. To\nalleviate the AIWQ problem, we propose a novel training method called\nPROgressive-Freezing Iterative Training (PROFIT), which attempts to freeze\nlayers whose weights are affected by the instability problem stronger than the\nother layers. We also propose a differentiable and unified quantization method\n(DuQ) and a negative padding idea to support asymmetric activation functions\nsuch as h-swish. We evaluate the proposed methods by quantizing MobileNet-v1,\nv2, and v3 on ImageNet and report that 4-bit quantization offers comparable\n(within 1.48 % top-1 accuracy) accuracy to full precision baseline. In the\nablation study of the 3-bit quantization of MobileNet-v3, our proposed method\noutperforms the state-of-the-art method by a large margin, 12.86 % of top-1\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 13:29:50 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Park", "Eunhyeok", ""], ["Yoo", "Sungjoo", ""]]}, {"id": "2008.04694", "submitter": "Yang Chen", "authors": "Yang Chen, Martin Alain, Aljosa Smolic", "title": "A Study of Efficient Light Field Subsampling and Reconstruction\n  Strategies", "comments": "Accepted at IMVIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limited angular resolution is one of the main obstacles for practical\napplications of light fields. Although numerous approaches have been proposed\nto enhance angular resolution, view selection strategies have not been well\nexplored in this area. In this paper, we study subsampling and reconstruction\nstrategies for light fields. First, different subsampling strategies are\nstudied with a fixed sampling ratio, such as row-wise sampling, column-wise\nsampling, or their combinations. Second, several strategies are explored to\nreconstruct intermediate views from four regularly sampled input views. The\ninfluence of the angular density of the input is also evaluated. We evaluate\nthese strategies on both real-world and synthetic datasets, and optimal\nselection strategies are devised from our results. These can be applied in\nfuture light field research such as compression, angular super-resolution, and\ndesign of camera systems.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 13:32:11 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Chen", "Yang", ""], ["Alain", "Martin", ""], ["Smolic", "Aljosa", ""]]}, {"id": "2008.04722", "submitter": "Seokeon Choi", "authors": "Seokeon Choi, Junhyun Lee, Yunsung Lee, Alexander Hauptmann", "title": "Robust Long-Term Object Tracking via Improved Discriminative Model\n  Prediction", "comments": "Accepted to ECCV 2020 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an improved discriminative model prediction method for robust\nlong-term tracking based on a pre-trained short-term tracker. The baseline\npre-trained short-term tracker is SuperDiMP which combines the bounding-box\nregressor of PrDiMP with the standard DiMP classifier. Our tracker RLT-DiMP\nimproves SuperDiMP in the following three aspects: (1) Uncertainty reduction\nusing random erasing: To make our model robust, we exploit an agreement from\nmultiple images after erasing random small rectangular areas as a certainty.\nAnd then, we correct the tracking state of our model accordingly. (2) Random\nsearch with spatio-temporal constraints: we propose a robust random search\nmethod with a score penalty applied to prevent the problem of sudden detection\nat a distance. (3) Background augmentation for more discriminative feature\nlearning: We augment various backgrounds that are not included in the search\narea to train a more robust model in the background clutter. In experiments on\nthe VOT-LT2020 benchmark dataset, the proposed method achieves comparable\nperformance to the state-of-the-art long-term trackers. The source code is\navailable at: https://github.com/bismex/RLT-DIMP.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 14:31:11 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 15:37:50 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Choi", "Seokeon", ""], ["Lee", "Junhyun", ""], ["Lee", "Yunsung", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "2008.04724", "submitter": "Malin Stanescu", "authors": "Malin Stanescu, Ovidiu Vaduvescu", "title": "The Umbrella software suite for automated asteroid detection", "comments": "Manuscript as accepted to Astronomy and Computing 15 Feb 2020", "journal-ref": null, "doi": "10.1016/j.ascom.2021.100453", "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present the Umbrella software suite for asteroid detection, validation,\nidentification and reporting. The current core of Umbrella is an open-source\nmodular library, called Umbrella2, that includes algorithms and interfaces for\nall steps of the processing pipeline, including a novel detection algorithm for\nfaint trails. Building on the library, we have also implemented a detection\npipeline accessible both as a desktop program (ViaNearby) and via a web server\n(Webrella), which we have successfully used in near real-time data reduction of\na few asteroid surveys on the Wide Field Camera of the Isaac Newton Telescope.\nIn this paper we describe the library, focusing on the interfaces and\nalgorithms available, and we present the results obtained with the desktop\nversion on a set of well-curated fields used by the EURONEAR project as an\nasteroid detection benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 14:37:58 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 10:48:56 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Stanescu", "Malin", ""], ["Vaduvescu", "Ovidiu", ""]]}, {"id": "2008.04729", "submitter": "Lei Li", "authors": "Lei Li and Veronika A. Zimmer and Julia A. Schnabel and Xiahai Zhuang", "title": "AtrialJSQnet: A New Framework for Joint Segmentation and Quantification\n  of Left Atrium and Scars Incorporating Spatial and Shape Information", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Left atrial (LA) and atrial scar segmentation from late gadolinium enhanced\nmagnetic resonance imaging (LGE MRI) is an important task in clinical practice.\n%, to guide ablation therapy and predict treatment results for atrial\nfibrillation (AF) patients. The automatic segmentation is however still\nchallenging, due to the poor image quality, the various LA shapes, the thin\nwall, and the surrounding enhanced regions. Previous methods normally solved\nthe two tasks independently and ignored the intrinsic spatial relationship\nbetween LA and scars. In this work, we develop a new framework, namely\nAtrialJSQnet, where LA segmentation, scar projection onto the LA surface, and\nscar quantification are performed simultaneously in an end-to-end style. We\npropose a mechanism of shape attention (SA) via an explicit surface projection,\nto utilize the inherent correlation between LA and LA scars. In specific, the\nSA scheme is embedded into a multi-task architecture to perform joint LA\nsegmentation and scar quantification. Besides, a spatial encoding (SE) loss is\nintroduced to incorporate continuous spatial information of the target, in\norder to reduce noisy patches in the predicted segmentation. We evaluated the\nproposed framework on 60 LGE MRIs from the MICCAI2018 LA challenge. Extensive\nexperiments on a public dataset demonstrated the effect of the proposed\nAtrialJSQnet, which achieved competitive performance over the state-of-the-art.\nThe relatedness between LA segmentation and scar quantification was explicitly\nexplored and has shown significant performance improvements for both tasks. The\ncode and results will be released publicly once the manuscript is accepted for\npublication via https://zmiclab.github.io/projects.html.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 14:44:19 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Li", "Lei", ""], ["Zimmer", "Veronika A.", ""], ["Schnabel", "Julia A.", ""], ["Zhuang", "Xiahai", ""]]}, {"id": "2008.04738", "submitter": "Andrey De Aguiar Salvi", "authors": "Andrey Salvi and Nathan Gavenski and Eduardo Pooch and Felipe\n  Tasoniero and Rodrigo Barros", "title": "Attention-based 3D Object Reconstruction from a Single Image", "comments": "8 pages, 4 figures, 3 tables", "journal-ref": "International Joint Conference on Neural Networks (IJCNN) - 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, learning-based approaches for 3D reconstruction from 2D images have\ngained popularity due to its modern applications, e.g., 3D printers, autonomous\nrobots, self-driving cars, virtual reality, and augmented reality. The computer\nvision community has applied a great effort in developing functions to\nreconstruct the full 3D geometry of objects and scenes. However, to extract\nimage features, they rely on convolutional neural networks, which are\nineffective in capturing long-range dependencies. In this paper, we propose to\nsubstantially improve Occupancy Networks, a state-of-the-art method for 3D\nobject reconstruction. For such we apply the concept of self-attention within\nthe network's encoder in order to leverage complementary input features rather\nthan those based on local regions, helping the encoder to extract global\ninformation. With our approach, we were capable of improving the original work\nin 5.05% of mesh IoU, 0.83% of Normal Consistency, and more than 10X the\nChamfer-L1 distance. We also perform a qualitative study that shows that our\napproach was able to generate much more consistent meshes, confirming its\nincreased generalization power over the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 14:51:18 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Salvi", "Andrey", ""], ["Gavenski", "Nathan", ""], ["Pooch", "Eduardo", ""], ["Tasoniero", "Felipe", ""], ["Barros", "Rodrigo", ""]]}, {"id": "2008.04751", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, Yimeng Zhang, Xiongchang Liu, Song Bai, Site Li, Jane\n  You", "title": "Reinforced Wasserstein Training for Severity-Aware Semantic Segmentation\n  in Autonomous Driving", "comments": "Accepted to IEEE Transactions on Intelligent Transportation Systems\n  (T-ITS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.PF cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is important for many real-world systems, e.g.,\nautonomous vehicles, which predict the class of each pixel. Recently, deep\nnetworks achieved significant progress w.r.t. the mean Intersection-over Union\n(mIoU) with the cross-entropy loss. However, the cross-entropy loss can\nessentially ignore the difference of severity for an autonomous car with\ndifferent wrong prediction mistakes. For example, predicting the car to the\nroad is much more servery than recognize it as the bus. Targeting for this\ndifficulty, we develop a Wasserstein training framework to explore the\ninter-class correlation by defining its ground metric as misclassification\nseverity. The ground metric of Wasserstein distance can be pre-defined\nfollowing the experience on a specific task. From the optimization perspective,\nwe further propose to set the ground metric as an increasing function of the\npre-defined ground metric. Furthermore, an adaptively learning scheme of the\nground matrix is proposed to utilize the high-fidelity CARLA simulator.\nSpecifically, we follow a reinforcement alternative learning scheme. The\nexperiments on both CamVid and Cityscapes datasets evidenced the effectiveness\nof our Wasserstein loss. The SegNet, ENet, FCN and Deeplab networks can be\nadapted following a plug-in manner. We achieve significant improvements on the\npredefined important classes, and much longer continuous playtime in our\nsimulator.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 15:00:41 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Zhang", "Yimeng", ""], ["Liu", "Xiongchang", ""], ["Bai", "Song", ""], ["Li", "Site", ""], ["You", "Jane", ""]]}, {"id": "2008.04753", "submitter": "Raja Muhammad Saad Bashir", "authors": "R.M. Saad Bashir, Talha Qaiser, Shan E Ahmed Raza, Nasir M. Rajpoot", "title": "HydraMix-Net: A Deep Multi-task Semi-supervised Learning Approach for\n  Cell Detection and Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised techniques have removed the barriers of large scale labelled\nset by exploiting unlabelled data to improve the performance of a model. In\nthis paper, we propose a semi-supervised deep multi-task classification and\nlocalization approach HydraMix-Net in the field of medical imagining where\nlabelling is time consuming and costly. Firstly, the pseudo labels are\ngenerated using the model's prediction on the augmented set of unlabelled image\nwith averaging. The high entropy predictions are further sharpened to reduced\nthe entropy and are then mixed with the labelled set for training. The model is\ntrained in multi-task learning manner with noise tolerant joint loss for\nclassification localization and achieves better performance when given limited\ndata in contrast to a simple deep model. On DLBCL data it achieves 80\\%\naccuracy in contrast to simple CNN achieving 70\\% accuracy when given only 100\nlabelled examples.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 15:00:59 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Bashir", "R. M. Saad", ""], ["Qaiser", "Talha", ""], ["Raza", "Shan E Ahmed", ""], ["Rajpoot", "Nasir M.", ""]]}, {"id": "2008.04757", "submitter": "Alexander Hudson", "authors": "Alexander Hudson and Shaogang Gong", "title": "Transfer Learning for Protein Structure Classification at Low Resolution", "comments": "9 pages excluding references and appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structure determination is key to understanding protein function at a\nmolecular level. Whilst significant advances have been made in predicting\nstructure and function from amino acid sequence, researchers must still rely on\nexpensive, time-consuming analytical methods to visualise detailed protein\nconformation. In this study, we demonstrate that it is possible to make\naccurate ($\\geq$80%) predictions of protein class and architecture from\nstructures determined at low ($>$3A) resolution, using a deep convolutional\nneural network trained on high-resolution ($\\leq$3A) structures represented as\n2D matrices. Thus, we provide proof of concept for high-speed, low-cost protein\nstructure classification at low resolution, and a basis for extension to\nprediction of function. We investigate the impact of the input representation\non classification performance, showing that side-chain information may not be\nnecessary for fine-grained structure predictions. Finally, we confirm that\nhigh-resolution, low-resolution and NMR-determined structures inhabit a common\nfeature space, and thus provide a theoretical foundation for boosting with\nsingle-image super-resolution.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 15:01:32 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2020 16:51:55 GMT"}, {"version": "v3", "created": "Thu, 27 Aug 2020 07:21:30 GMT"}, {"version": "v4", "created": "Mon, 31 Aug 2020 17:02:33 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Hudson", "Alexander", ""], ["Gong", "Shaogang", ""]]}, {"id": "2008.04776", "submitter": "Jiangning Zhang", "authors": "Jiangning Zhang, Chao Xu, Liang Liu, Mengmeng Wang, Xia Wu, Yong Liu,\n  Yunliang Jiang", "title": "DTVNet: Dynamic Time-lapse Video Generation via Single Still Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel end-to-end dynamic time-lapse video generation\nframework, named DTVNet, to generate diversified time-lapse videos from a\nsingle landscape image, which are conditioned on normalized motion vectors. The\nproposed DTVNet consists of two submodules: \\emph{Optical Flow Encoder} (OFE)\nand \\emph{Dynamic Video Generator} (DVG). The OFE maps a sequence of optical\nflow maps to a \\emph{normalized motion vector} that encodes the motion\ninformation inside the generated video. The DVG contains motion and content\nstreams that learn from the motion vector and the single image respectively, as\nwell as an encoder and a decoder to learn shared content features and construct\nvideo frames with corresponding motion respectively. Specifically, the\n\\emph{motion stream} introduces multiple \\emph{adaptive instance normalization}\n(AdaIN) layers to integrate multi-level motion information that are processed\nby linear layers. In the testing stage, videos with the same content but\nvarious motion information can be generated by different \\emph{normalized\nmotion vectors} based on only one input image. We further conduct experiments\non Sky Time-lapse dataset, and the results demonstrate the superiority of our\napproach over the state-of-the-art methods for generating high-quality and\ndynamic videos, as well as the variety for generating videos with various\nmotion information.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 15:26:10 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Zhang", "Jiangning", ""], ["Xu", "Chao", ""], ["Liu", "Liang", ""], ["Wang", "Mengmeng", ""], ["Wu", "Xia", ""], ["Liu", "Yong", ""], ["Jiang", "Yunliang", ""]]}, {"id": "2008.04800", "submitter": "Jingyang Zhang", "authors": "Jingyang Zhang, Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian\n  Fang, Long Quan", "title": "Learning Stereo Matchability in Disparity Regression Networks", "comments": "Accepted to ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based stereo matching has recently achieved promising results, yet\nstill suffers difficulties in establishing reliable matches in weakly matchable\nregions that are textureless, non-Lambertian, or occluded. In this paper, we\naddress this challenge by proposing a stereo matching network that considers\npixel-wise matchability. Specifically, the network jointly regresses disparity\nand matchability maps from 3D probability volume through expectation and\nentropy operations. Next, a learned attenuation is applied as the robust loss\nfunction to alleviate the influence of weakly matchable pixels in the training.\nFinally, a matchability-aware disparity refinement is introduced to improve the\ndepth inference in weakly matchable regions. The proposed deep stereo\nmatchability (DSM) framework can improve the matching result or accelerate the\ncomputation while still guaranteeing the quality. Moreover, the DSM framework\nis portable to many recent stereo networks. Extensive experiments are conducted\non Scene Flow and KITTI stereo datasets to demonstrate the effectiveness of the\nproposed framework over the state-of-the-art learning-based stereo methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 15:55:49 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Zhang", "Jingyang", ""], ["Yao", "Yao", ""], ["Luo", "Zixin", ""], ["Li", "Shiwei", ""], ["Shen", "Tianwei", ""], ["Fang", "Tian", ""], ["Quan", "Long", ""]]}, {"id": "2008.04802", "submitter": "Vikash Gupta", "authors": "Richard D. White, Barbaros S. Erdal, Mutlu Demirer, Vikash Gupta,\n  Matthew T. Bigelow, Engin Dikici, Sema Candemir, Mauricio S. Galizia, Jessica\n  L. Carpenter, Thomas P. O Donnell, Abdul H. Halabi, Luciano M. Prevedello", "title": "Artificial Intelligence to Assist in Exclusion of Coronary\n  Atherosclerosis during CCTA Evaluation of Chest-Pain in the Emergency\n  Department: Preparing an Application for Real-World Use", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronary Computed Tomography Angiography (CCTA) evaluation of chest-pain\npatients in an Emergency Department (ED) is considered appropriate. While a\nnegative CCTA interpretation supports direct patient discharge from an ED,\nlabor-intensive analyses are required, with accuracy in jeopardy from\ndistractions. We describe the development of an Artificial Intelligence (AI)\nalgorithm and workflow for assisting interpreting physicians in CCTA screening\nfor the absence of coronary atherosclerosis. The two-phase approach consisted\nof (1) Phase 1 - focused on the development and preliminary testing of an\nalgorithm for vessel-centerline extraction classification in a balanced study\npopulation (n = 500 with 50% disease prevalence) derived by retrospective\nrandom case selection; and (2) Phase 2 - concerned with simulated-clinical\nTrialing of the developed algorithm on a per-case basis in a more real-world\nstudy population (n = 100 with 28% disease prevalence) from an ED chest-pain\nseries. This allowed pre-deployment evaluation of the AI-based CCTA screening\napplication which provides a vessel-by-vessel graphic display of algorithm\ninference results integrated into a clinically capable viewer. Algorithm\nperformance evaluation used Area Under the Receiver-Operating-Characteristic\nCurve (AUC-ROC); confusion matrices reflected ground-truth vs AI\ndeterminations. The vessel-based algorithm demonstrated strong performance with\nAUC-ROC = 0.96. In both Phase 1 and Phase 2, independent of disease prevalence\ndifferences, negative predictive values at the case level were very high at\n95%. The rate of completion of the algorithm workflow process (96% with\ninference results in 55-80 seconds) in Phase 2 depended on adequate image\nquality. There is potential for this AI application to assist in CCTA\ninterpretation to help extricate atherosclerosis from chest-pain presentations.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 16:07:04 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["White", "Richard D.", ""], ["Erdal", "Barbaros S.", ""], ["Demirer", "Mutlu", ""], ["Gupta", "Vikash", ""], ["Bigelow", "Matthew T.", ""], ["Dikici", "Engin", ""], ["Candemir", "Sema", ""], ["Galizia", "Mauricio S.", ""], ["Carpenter", "Jessica L.", ""], ["Donnell", "Thomas P. O", ""], ["Halabi", "Abdul H.", ""], ["Prevedello", "Luciano M.", ""]]}, {"id": "2008.04808", "submitter": "Jonathan Alush-Aben", "authors": "Jonathan Alush-Aben, Linor Ackerman-Schraier, Tomer Weiss, Sanketh\n  Vedula, Ortal Senouf and Alex Bronstein", "title": "3D FLAT: Feasible Learned Acquisition Trajectories for Accelerated MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance Imaging (MRI) has long been considered to be among the\ngold standards of today's diagnostic imaging. The most significant drawback of\nMRI is long acquisition times, prohibiting its use in standard practice for\nsome applications. Compressed sensing (CS) proposes to subsample the k-space\n(the Fourier domain dual to the physical space of spatial coordinates) leading\nto significantly accelerated acquisition. However, the benefit of compressed\nsensing has not been fully exploited; most of the sampling densities obtained\nthrough CS do not produce a trajectory that obeys the stringent constraints of\nthe MRI machine imposed in practice. Inspired by recent success of deep\nlearning based approaches for image reconstruction and ideas from computational\nimaging on learning-based design of imaging systems, we introduce 3D FLAT, a\nnovel protocol for data-driven design of 3D non-Cartesian accelerated\ntrajectories in MRI. Our proposal leverages the entire 3D k-space to\nsimultaneously learn a physically feasible acquisition trajectory with a\nreconstruction method. Experimental results, performed as a proof-of-concept,\nsuggest that 3D FLAT achieves higher image quality for a given readout time\ncompared to standard trajectories such as radial, stack-of-stars, or 2D learned\ntrajectories (trajectories that evolve only in the 2D plane while fully\nsampling along the third dimension). Furthermore, we demonstrate evidence\nsupporting the significant benefit of performing MRI acquisitions using\nnon-Cartesian 3D trajectories over 2D non-Cartesian trajectories acquired\nslice-wise.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 16:03:51 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Alush-Aben", "Jonathan", ""], ["Ackerman-Schraier", "Linor", ""], ["Weiss", "Tomer", ""], ["Vedula", "Sanketh", ""], ["Senouf", "Ortal", ""], ["Bronstein", "Alex", ""]]}, {"id": "2008.04815", "submitter": "Md. Milon Islam Islam", "authors": "Md. Milon Islam, Fakhri Karray, Reda Alhajj, Jia Zeng", "title": "A Review on Deep Learning Techniques for the Diagnosis of Novel\n  Coronavirus (COVID-19)", "comments": "18 pages, 2 figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Novel coronavirus (COVID-19) outbreak, has raised a calamitous situation all\nover the world and has become one of the most acute and severe ailments in the\npast hundred years. The prevalence rate of COVID-19 is rapidly rising every day\nthroughout the globe. Although no vaccines for this pandemic have been\ndiscovered yet, deep learning techniques proved themselves to be a powerful\ntool in the arsenal used by clinicians for the automatic diagnosis of COVID-19.\nThis paper aims to overview the recently developed systems based on deep\nlearning techniques using different medical imaging modalities like Computer\nTomography (CT) and X-ray. This review specifically discusses the systems\ndeveloped for COVID-19 diagnosis using deep learning techniques and provides\ninsights on well-known data sets used to train these networks. It also\nhighlights the data partitioning techniques and various performance measures\ndeveloped by researchers in this field. A taxonomy is drawn to categorize the\nrecent works for proper insight. Finally, we conclude by addressing the\nchallenges associated with the use of deep learning methods for COVID-19\ndetection and probable future trends in this research area. This paper is\nintended to provide experts (medical or otherwise) and technicians with new\ninsights into the ways deep learning techniques are used in this regard and how\nthey potentially further works in combatting the outbreak of COVID-19.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 02:37:50 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Islam", "Md. Milon", ""], ["Karray", "Fakhri", ""], ["Alhajj", "Reda", ""], ["Zeng", "Jia", ""]]}, {"id": "2008.04821", "submitter": "Chien-Yi Wang", "authors": "Chien-Yi Wang, Ya-Liang Chang, Shang-Ta Yang, Dong Chen, Shang-Hong\n  Lai", "title": "Unified Representation Learning for Cross Model Compatibility", "comments": "To appear in British Machine Vision Conference (BMVC) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a unified representation learning framework to address the Cross\nModel Compatibility (CMC) problem in the context of visual search applications.\nCross compatibility between different embedding models enables the visual\nsearch systems to correctly recognize and retrieve identities without\nre-encoding user images, which are usually not available due to privacy\nconcerns. While there are existing approaches to address CMC in face\nidentification, they fail to work in a more challenging setting where the\ndistributions of embedding models shift drastically. The proposed solution\nimproves CMC performance by introducing a light-weight Residual Bottleneck\nTransformation (RBT) module and a new training scheme to optimize the embedding\nspaces. Extensive experiments demonstrate that our proposed solution\noutperforms previous approaches by a large margin for various challenging\nvisual search scenarios of face recognition and person re-identification.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 16:14:53 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Wang", "Chien-Yi", ""], ["Chang", "Ya-Liang", ""], ["Yang", "Shang-Ta", ""], ["Chen", "Dong", ""], ["Lai", "Shang-Hong", ""]]}, {"id": "2008.04829", "submitter": "Ephrem Admasu Yekun", "authors": "Ephrem Admasu Yekun, Petros Reda Samsom", "title": "Detecting Urban Dynamics Using Deep Siamese Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection is a fast-growing discipline in the areas of computer vision\nand remote sensing. In this work, we designed and developed a variant of\nconvolutional neural network (CNN), known as Siamese CNN to extract features\nfrom pairs of Sentinel-2 temporal images of Mekelle city captured at different\ntimes and detect changes due to urbanization: buildings and roads. The\ndetection capability of the proposed was measured in terms of overall accuracy\n(95.8), Kappa measure (72.5), recall (76.5), precision (77.7), F1 measure\n(77.1). The model has achieved a good performance in terms of most of these\nmeasures and can be used to detect changes in Mekelle and other cities at\ndifferent time horizons undergoing urbanization.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 16:20:11 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Yekun", "Ephrem Admasu", ""], ["Samsom", "Petros Reda", ""]]}, {"id": "2008.04838", "submitter": "Tom\\'a\\v{s} Sou\\v{c}ek", "authors": "Tom\\'a\\v{s} Sou\\v{c}ek and Jakub Loko\\v{c}", "title": "TransNet V2: An effective deep network architecture for fast shot\n  transition detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although automatic shot transition detection approaches are already\ninvestigated for more than two decades, an effective universal human-level\nmodel was not proposed yet. Even for common shot transitions like hard cuts or\nsimple gradual changes, the potential diversity of analyzed video contents may\nstill lead to both false hits and false dismissals. Recently, deep\nlearning-based approaches significantly improved the accuracy of shot\ntransition detection using 3D convolutional architectures and artificially\ncreated training data. Nevertheless, one hundred percent accuracy is still an\nunreachable ideal. In this paper, we share the current version of our deep\nnetwork TransNet V2 that reaches state-of-the-art performance on respected\nbenchmarks. A trained instance of the model is provided so it can be instantly\nutilized by the community for a highly efficient analysis of large video\narchives. Furthermore, the network architecture, as well as our experience with\nthe training process, are detailed, including simple code snippets for\nconvenient usage of the proposed model and visualization of results.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 16:37:59 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Sou\u010dek", "Tom\u00e1\u0161", ""], ["Loko\u010d", "Jakub", ""]]}, {"id": "2008.04848", "submitter": "Gengxing Wang", "authors": "Gengxing Wang, Jiahuan Zhou, Ying Wu", "title": "Exposing Deep-faked Videos by Anomalous Co-motion Pattern Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep learning based video synthesis approaches, in particular with\napplications that can forge identities such as \"DeepFake\", have raised great\nsecurity concerns. Therefore, corresponding deep forensic methods are proposed\nto tackle this problem. However, existing methods are either based on\nunexplainable deep networks which greatly degrades the principal\ninterpretability factor to media forensic, or rely on fragile image statistics\nsuch as noise pattern, which in real-world scenarios can be easily deteriorated\nby data compression. In this paper, we propose an fully-interpretable video\nforensic method that is designed specifically to expose deep-faked videos. To\nenhance generalizability on videos with various content, we model the temporal\nmotion of multiple specific spatial locations in the videos to extract a robust\nand reliable representation, called Co-Motion Pattern. Such kind of conjoint\npattern is mined across local motion features which is independent of the video\ncontents so that the instance-wise variation can also be largely alleviated.\nMore importantly, our proposed co-motion pattern possesses both superior\ninterpretability and sufficient robustness against data compression for\ndeep-faked videos. We conduct extensive experiments to empirically demonstrate\nthe superiority and effectiveness of our approach under both classification and\nanomaly detection evaluation settings against the state-of-the-art deep\nforensic methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 16:47:02 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Wang", "Gengxing", ""], ["Zhou", "Jiahuan", ""], ["Wu", "Ying", ""]]}, {"id": "2008.04851", "submitter": "Xi Li", "authors": "Fangfang Wang, Yifeng Chen, Fei Wu, and Xi Li", "title": "TextRay: Contour-based Geometric Modeling for Arbitrary-shaped Scene\n  Text Detection", "comments": "Accepted to ACM MM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arbitrary-shaped text detection is a challenging task due to the complex\ngeometric layouts of texts such as large aspect ratios, various scales, random\nrotations and curve shapes. Most state-of-the-art methods solve this problem\nfrom bottom-up perspectives, seeking to model a text instance of complex\ngeometric layouts with simple local units (e.g., local boxes or pixels) and\ngenerate detections with heuristic post-processings. In this work, we propose\nan arbitrary-shaped text detection method, namely TextRay, which conducts\ntop-down contour-based geometric modeling and geometric parameter learning\nwithin a single-shot anchor-free framework. The geometric modeling is carried\nout under polar system with a bidirectional mapping scheme between shape space\nand parameter space, encoding complex geometric layouts into unified\nrepresentations. For effective learning of the representations, we design a\ncentral-weighted training strategy and a content loss which builds propagation\npaths between geometric encodings and visual content. TextRay outputs simple\npolygon detections at one pass with only one NMS post-processing. Experiments\non several benchmark datasets demonstrate the effectiveness of the proposed\napproach. The code is available at https://github.com/LianaWang/TextRay.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 16:52:10 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 07:29:25 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Wang", "Fangfang", ""], ["Chen", "Yifeng", ""], ["Wu", "Fei", ""], ["Li", "Xi", ""]]}, {"id": "2008.04852", "submitter": "Ricardo Martin Brualla", "authors": "Ricardo Martin-Brualla, Rohit Pandey, Sofien Bouaziz, Matthew Brown,\n  Dan B Goldman", "title": "GeLaTO: Generative Latent Textured Objects", "comments": "ECCV 2020 Spotlight. Project website: https://gelato-paper.github.io", "journal-ref": "European Conference on Computer Vision 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate modeling of 3D objects exhibiting transparency, reflections and thin\nstructures is an extremely challenging problem. Inspired by billboards and\ngeometric proxies used in computer graphics, this paper proposes Generative\nLatent Textured Objects (GeLaTO), a compact representation that combines a set\nof coarse shape proxies defining low frequency geometry with learned neural\ntextures, to encode both medium and fine scale geometry as well as\nview-dependent appearance. To generate the proxies' textures, we learn a joint\nlatent space allowing category-level appearance and geometry interpolation. The\nproxies are independently rasterized with their corresponding neural texture\nand composited using a U-Net, which generates an output photorealistic image\nincluding an alpha map. We demonstrate the effectiveness of our approach by\nreconstructing complex objects from a sparse set of views. We show results on a\ndataset of real images of eyeglasses frames, which are particularly challenging\nto reconstruct using classical methods. We also demonstrate that these coarse\nproxies can be handcrafted when the underlying object geometry is easy to\nmodel, like eyeglasses, or generated using a neural network for more complex\ncategories, such as cars.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 16:55:26 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Martin-Brualla", "Ricardo", ""], ["Pandey", "Rohit", ""], ["Bouaziz", "Sofien", ""], ["Brown", "Matthew", ""], ["Goldman", "Dan B", ""]]}, {"id": "2008.04858", "submitter": "Xiaoze Jiang", "authors": "Xiaoze Jiang, Siyi Du, Zengchang Qin, Yajing Sun, Jing Yu", "title": "KBGN: Knowledge-Bridge Graph Network for Adaptive Vision-Text Reasoning\n  in Visual Dialogue", "comments": "Accepted by the 28th ACM International Conference on Multimedia (ACM\n  MM 2020), Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual dialogue is a challenging task that needs to extract implicit\ninformation from both visual (image) and textual (dialogue history) contexts.\nClassical approaches pay more attention to the integration of the current\nquestion, vision knowledge and text knowledge, despising the heterogeneous\nsemantic gaps between the cross-modal information. In the meantime, the\nconcatenation operation has become de-facto standard to the cross-modal\ninformation fusion, which has a limited ability in information retrieval. In\nthis paper, we propose a novel Knowledge-Bridge Graph Network (KBGN) model by\nusing graph to bridge the cross-modal semantic relations between vision and\ntext knowledge in fine granularity, as well as retrieving required knowledge\nvia an adaptive information selection mode. Moreover, the reasoning clues for\nvisual dialogue can be clearly drawn from intra-modal entities and inter-modal\nbridges. Experimental results on VisDial v1.0 and VisDial-Q datasets\ndemonstrate that our model outperforms existing models with state-of-the-art\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 17:03:06 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 07:34:49 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Jiang", "Xiaoze", ""], ["Du", "Siyi", ""], ["Qin", "Zengchang", ""], ["Sun", "Yajing", ""], ["Yu", "Jing", ""]]}, {"id": "2008.04859", "submitter": "Dimitris Tsipras", "authors": "Shibani Santurkar, Dimitris Tsipras, Aleksander Madry", "title": "BREEDS: Benchmarks for Subpopulation Shift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a methodology for assessing the robustness of models to\nsubpopulation shift---specifically, their ability to generalize to novel data\nsubpopulations that were not observed during training. Our approach leverages\nthe class structure underlying existing datasets to control the data\nsubpopulations that comprise the training and test distributions. This enables\nus to synthesize realistic distribution shifts whose sources can be precisely\ncontrolled and characterized, within existing large-scale datasets. Applying\nthis methodology to the ImageNet dataset, we create a suite of subpopulation\nshift benchmarks of varying granularity. We then validate that the\ncorresponding shifts are tractable by obtaining human baselines for them.\nFinally, we utilize these benchmarks to measure the sensitivity of standard\nmodel architectures as well as the effectiveness of off-the-shelf train-time\nrobustness interventions. Code and data available at\nhttps://github.com/MadryLab/BREEDS-Benchmarks .\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 17:04:47 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Santurkar", "Shibani", ""], ["Tsipras", "Dimitris", ""], ["Madry", "Aleksander", ""]]}, {"id": "2008.04872", "submitter": "Xingyu Chen", "authors": "Xingyu Chen, Xuguang Lan, Fuchun Sun, Nanning Zheng", "title": "A Boundary Based Out-of-Distribution Classifier for Generalized\n  Zero-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized Zero-Shot Learning (GZSL) is a challenging topic that has\npromising prospects in many realistic scenarios. Using a gating mechanism that\ndiscriminates the unseen samples from the seen samples can decompose the GZSL\nproblem to a conventional Zero-Shot Learning (ZSL) problem and a supervised\nclassification problem. However, training the gate is usually challenging due\nto the lack of data in the unseen domain. To resolve this problem, in this\npaper, we propose a boundary based Out-of-Distribution (OOD) classifier which\nclassifies the unseen and seen domains by only using seen samples for training.\nFirst, we learn a shared latent space on a unit hyper-sphere where the latent\ndistributions of visual features and semantic attributes are aligned\nclass-wisely. Then we find the boundary and the center of the manifold for each\nclass. By leveraging the class centers and boundaries, the unseen samples can\nbe separated from the seen samples. After that, we use two experts to classify\nthe seen and unseen samples separately. We extensively validate our approach on\nfive popular benchmark datasets including AWA1, AWA2, CUB, FLO and SUN. The\nexperimental results show that our approach surpasses state-of-the-art\napproaches by a significant margin.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 11:27:19 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Chen", "Xingyu", ""], ["Lan", "Xuguang", ""], ["Sun", "Fuchun", ""], ["Zheng", "Nanning", ""]]}, {"id": "2008.04878", "submitter": "Zhijian Liu", "authors": "Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, Song Han", "title": "Hardware-Centric AutoML for Mixed-Precision Quantization", "comments": "Journal preprint of arXiv:1811.08886 (IJCV, 2020). The first three\n  authors contributed equally to this work. Project page:\n  https://hanlab.mit.edu/projects/haq/", "journal-ref": "International Journal of Computer Vision (IJCV), Volume 128, Issue\n  8-9, pp 2035-2048, 2020", "doi": "10.1007/s11263-020-01339-6", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model quantization is a widely used technique to compress and accelerate deep\nneural network (DNN) inference. Emergent DNN hardware accelerators begin to\nsupport mixed precision (1-8 bits) to further improve the computation\nefficiency, which raises a great challenge to find the optimal bitwidth for\neach layer: it requires domain experts to explore the vast design space trading\noff among accuracy, latency, energy, and model size, which is both\ntime-consuming and sub-optimal. Conventional quantization algorithm ignores the\ndifferent hardware architectures and quantizes all the layers in a uniform way.\nIn this paper, we introduce the Hardware-Aware Automated Quantization (HAQ)\nframework which leverages the reinforcement learning to automatically determine\nthe quantization policy, and we take the hardware accelerator's feedback in the\ndesign loop. Rather than relying on proxy signals such as FLOPs and model size,\nwe employ a hardware simulator to generate direct feedback signals (latency and\nenergy) to the RL agent. Compared with conventional methods, our framework is\nfully automated and can specialize the quantization policy for different neural\nnetwork architectures and hardware architectures. Our framework effectively\nreduced the latency by 1.4-1.95x and the energy consumption by 1.9x with\nnegligible loss of accuracy compared with the fixed bitwidth (8 bits)\nquantization. Our framework reveals that the optimal policies on different\nhardware architectures (i.e., edge and cloud architectures) under different\nresource constraints (i.e., latency, energy, and model size) are drastically\ndifferent. We interpreted the implication of different quantization policies,\nwhich offer insights for both neural network architecture design and hardware\narchitecture design.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 17:30:22 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Wang", "Kuan", ""], ["Liu", "Zhijian", ""], ["Lin", "Yujun", ""], ["Lin", "Ji", ""], ["Han", "Song", ""]]}, {"id": "2008.04888", "submitter": "Aj Piergiovanni", "authors": "AJ Piergiovanni, Anelia Angelova, Alexander Toshev, Michael S. Ryoo", "title": "Adversarial Generative Grammars for Human Activity Prediction", "comments": "ECCV 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an adversarial generative grammar model for future\nprediction. The objective is to learn a model that explicitly captures temporal\ndependencies, providing a capability to forecast multiple, distinct future\nactivities. Our adversarial grammar is designed so that it can learn stochastic\nproduction rules from the data distribution, jointly with its latent\nnon-terminal representations. Being able to select multiple production rules\nduring inference leads to different predicted outcomes, thus efficiently\nmodeling many plausible futures. The adversarial generative grammar is\nevaluated on the Charades, MultiTHUMOS, Human3.6M, and 50 Salads datasets and\non two activity prediction tasks: future 3D human pose prediction and future\nactivity prediction. The proposed adversarial grammar outperforms the\nstate-of-the-art approaches, being able to predict much more accurately and\nfurther in the future, than prior work.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 17:47:53 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 15:16:46 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Piergiovanni", "AJ", ""], ["Angelova", "Anelia", ""], ["Toshev", "Alexander", ""], ["Ryoo", "Michael S.", ""]]}, {"id": "2008.04899", "submitter": "Sarah Young", "authors": "Sarah Young, Dhiraj Gandhi, Shubham Tulsiani, Abhinav Gupta, Pieter\n  Abbeel, Lerrel Pinto", "title": "Visual Imitation Made Easy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual imitation learning provides a framework for learning complex\nmanipulation behaviors by leveraging human demonstrations. However, current\ninterfaces for imitation such as kinesthetic teaching or teleoperation\nprohibitively restrict our ability to efficiently collect large-scale data in\nthe wild. Obtaining such diverse demonstration data is paramount for the\ngeneralization of learned skills to novel scenarios. In this work, we present\nan alternate interface for imitation that simplifies the data collection\nprocess while allowing for easy transfer to robots. We use commercially\navailable reacher-grabber assistive tools both as a data collection device and\nas the robot's end-effector. To extract action information from these visual\ndemonstrations, we use off-the-shelf Structure from Motion (SfM) techniques in\naddition to training a finger detection network. We experimentally evaluate on\ntwo challenging tasks: non-prehensile pushing and prehensile stacking, with\n1000 diverse demonstrations for each task. For both tasks, we use standard\nbehavior cloning to learn executable policies from the previously collected\noffline demonstrations. To improve learning performance, we employ a variety of\ndata augmentations and provide an extensive analysis of its effects. Finally,\nwe demonstrate the utility of our interface by evaluating on real robotic\nscenarios with previously unseen objects and achieve a 87% success rate on\npushing and a 62% success rate on stacking. Robot videos are available at\nhttps://dhiraj100892.github.io/Visual-Imitation-Made-Easy.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 17:58:50 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Young", "Sarah", ""], ["Gandhi", "Dhiraj", ""], ["Tulsiani", "Shubham", ""], ["Gupta", "Abhinav", ""], ["Abbeel", "Pieter", ""], ["Pinto", "Lerrel", ""]]}, {"id": "2008.04902", "submitter": "Yu-Lun Liu", "authors": "Yu-Lun Liu, Wei-Sheng Lai, Ming-Hsuan Yang, Yung-Yu Chuang, Jia-Bin\n  Huang", "title": "Learning to See Through Obstructions with Layered Decomposition", "comments": "Project page: https://alex04072000.github.io/SOLD/ Code:\n  https://github.com/alex04072000/SOLD Extension of the CVPR 2020 paper:\n  arXiv:2004.01180", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learning-based approach for removing unwanted obstructions, such\nas window reflections, fence occlusions, or adherent raindrops, from a short\nsequence of images captured by a moving camera. Our method leverages motion\ndifferences between the background and obstructing elements to recover both\nlayers. Specifically, we alternate between estimating dense optical flow fields\nof the two layers and reconstructing each layer from the flow-warped images via\na deep convolutional neural network. This learning-based layer reconstruction\nmodule facilitates accommodating potential errors in the flow estimation and\nbrittle assumptions, such as brightness consistency. We show that the proposed\napproach learned from synthetically generated data performs well to real\nimages. Experimental results on numerous challenging scenarios of reflection\nand fence removal demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 17:59:31 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 03:38:28 GMT"}, {"version": "v3", "created": "Sun, 25 Jul 2021 06:47:11 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Liu", "Yu-Lun", ""], ["Lai", "Wei-Sheng", ""], ["Yang", "Ming-Hsuan", ""], ["Chuang", "Yung-Yu", ""], ["Huang", "Jia-Bin", ""]]}, {"id": "2008.04903", "submitter": "Jiaxiang Wang", "authors": "Jiaxiang Wang and Kunyong Chen", "title": "Automatic assembly of aero engine low pressure turbine shaft based on 3D\n  vision measurement", "comments": "5pages,12figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to solve the problem of low automation of Aero-engine Turbine shaft\nassembly and the difficulty of non-contact high-precision measurement, a\nstructured light binocular measurement technology for key components of\naero-engine is proposed in this paper. Combined with three-dimensional point\ncloud data processing and assembly position matching algorithm, the\nhigh-precision measurement of shaft hole assembly posture in the process of\nturbine shaft docking is realized. Firstly, the screw thread curve on the bolt\nsurface is segmented based on PCA projection and edge point cloud clustering,\nand Hough transform is used to model fit the three-dimensional thread curve.\nThen the preprocessed two-dimensional convex hull is constructed to segment the\nkey hole location features, and the mounting surface and hole location obtained\nby segmentation are fitted based on RANSAC method. Finally, the geometric\nfeature matching is used the evaluation index of turbine shaft assembly is\nestablished to optimize the pose. The final measurement accuracy of mounting\nsurface matching is less than 0.05mm, and the measurement accuracy of mounting\nhole matching based on minimum ance optimization is less than 0.1 degree. The\nmeasurement algorithm is implemented on the automatic assembly test-bed of a\ncertain type of aero-engine low-pressure turbine rotor. In the narrow\ninstallation space, the assembly process of the turbine shaft assembly, such as\nthe automatic alignment and docking of the shaft hole, the automatic heating\nand temperature measurement of the installation seam, and the automatic\ntightening of the two guns, are realized in the narrow installation space\nGuidance, real-time inspection and assembly result evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 17:06:39 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Wang", "Jiaxiang", ""], ["Chen", "Kunyong", ""]]}, {"id": "2008.04907", "submitter": "Sanskriti Singh", "authors": "Sanskriti Singh", "title": "PneumoXttention: A CNN compensating for Human Fallibility when Detecting\n  Pneumonia through CXR images with Attention", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic Chest Radiograph X-ray (CXR) interpretation by machines is an\nimportant research topic of Artificial Intelligence. As part of my journey\nthrough the California Science Fair, I have developed an algorithm that can\ndetect pneumonia from a CXR image to compensate for human fallibility. My\nalgorithm, PneumoXttention, is an ensemble of two 13 layer convolutional neural\nnetwork trained on the RSNA dataset, a dataset provided by the Radiological\nSociety of North America, containing 26,684 frontal X-ray images split into the\ncategories of pneumonia and no pneumonia. The dataset was annotated by many\nprofessional radiologists in North America. It achieved an impressive F1 score,\n0.82, on the test set (20% random split of RSNA dataset) and completely\ncompensated Human Radiologists on a random set of 25 test images drawn from\nRSNA and NIH. I don't have a direct comparison but Stanford's Chexnet has a F1\nscore of 0.435 on the NIH dataset for category Pneumonia.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 05:11:16 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Singh", "Sanskriti", ""]]}, {"id": "2008.04933", "submitter": "Fotios Logothetis Dr", "authors": "Fotios Logothetis, Ignas Budvytis, Roberto Mecca, Roberto Cipolla", "title": "PX-NET: Simple and Efficient Pixel-Wise Training of Photometric Stereo\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrieving accurate 3D reconstructions of objects from the way they reflect\nlight is a very challenging task in computer vision. Despite more than four\ndecades since the definition of the Photometric Stereo problem, most of the\nliterature has had limited success when global illumination effects such as\ncast shadows, self-reflections and ambient light come into play, especially for\nspecular surfaces.\n  Recent approaches have leveraged the power of deep learning in conjunction\nwith computer graphics in order to cope with the need of a vast number of\ntraining data in order to invert the image irradiance equation and retrieve the\ngeometry of the object. However, rendering global illumination effects is a\nslow process which can limit the amount of training data that can be generated.\n  In this work we propose a novel pixel-wise training procedure for normal\nprediction by replacing the training data (observation maps) of globally\nrendered images with independent per-pixel generated data. We show that global\nphysical effects can be approximated on the observation map domain and this\nsimplifies and speeds up the data creation procedure.\n  Our network, PX-NET, achieves the state-of-the-art performance compared to\nother pixelwise methods on synthetic datasets, as well as the Diligent real\ndataset on both dense and sparse light settings.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 18:03:13 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 16:41:20 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Logothetis", "Fotios", ""], ["Budvytis", "Ignas", ""], ["Mecca", "Roberto", ""], ["Cipolla", "Roberto", ""]]}, {"id": "2008.04946", "submitter": "Girik Malik", "authors": "Girik Malik and Ish K. Gulati", "title": "Little Motion, Big Results: Using Motion Magnification to Reveal Subtle\n  Tremors in Infants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting tremors is challenging for both humans and machines. Infants\nexposed to opioids during pregnancy often show signs and symptoms of withdrawal\nafter birth, which are easy to miss with the human eye. The constellation of\nclinical features, termed as Neonatal Abstinence Syndrome (NAS), include\ntremors, seizures, irritability, etc. The current standard of care uses\nFinnegan Neonatal Abstinence Syndrome Scoring System (FNASS), based on\nsubjective evaluations. Monitoring with FNASS requires highly skilled nursing\nstaff, making continuous monitoring difficult. In this paper we propose an\nautomated tremor detection system using amplified motion signals. We\ndemonstrate its applicability on bedside video of infant exhibiting signs of\nNAS. Further, we test different modes of deep convolutional network based\nmotion magnification, and identify that dynamic mode works best in the clinical\nsetting, being invariant to common orientational changes. We propose a strategy\nfor discharge and follow up for NAS patients, using motion magnification to\nsupplement the existing protocols. Overall our study suggests methods for\nbridging the gap in current practices, training and resource utilization.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 15:35:55 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Malik", "Girik", ""], ["Gulati", "Ish K.", ""]]}, {"id": "2008.04965", "submitter": "Mark Sandler", "authors": "Mark Sandler, Andrey Zhmoginov, Liangcheng Luo, Alexander Mordvintsev,\n  Ettore Randazzo, Blaise Ag\\'uera y Arcas", "title": "Image segmentation via Cellular Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new approach for building cellular automata to\nsolve real-world segmentation problems. We design and train a cellular\nautomaton that can successfully segment high-resolution images. We consider a\ncolony that densely inhabits the pixel grid, and all cells are governed by a\nrandomized update that uses the current state, the color, and the state of the\n$3\\times 3$ neighborhood. The space of possible rules is defined by a small\nneural network. The update rule is applied repeatedly in parallel to a large\nrandom subset of cells and after convergence is used to produce segmentation\nmasks that are then back-propagated to learn the optimal update rules using\nstandard gradient descent methods. We demonstrate that such models can be\nlearned efficiently with only limited trajectory length and that they show\nremarkable ability to organize the information to produce a globally consistent\nsegmentation result, using only local information exchange. From a practical\nperspective, our approach allows us to build very efficient models -- our\nsmallest automaton uses less than 10,000 parameters to solve complex\nsegmentation tasks.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 19:04:09 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 00:37:47 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Sandler", "Mark", ""], ["Zhmoginov", "Andrey", ""], ["Luo", "Liangcheng", ""], ["Mordvintsev", "Alexander", ""], ["Randazzo", "Ettore", ""], ["Arcas", "Blaise Ag\u00faera y", ""]]}, {"id": "2008.04968", "submitter": "Chongshou Li Dr", "authors": "Xinke Li, Chongshou Li, Zekun Tong, Andrew Lim, Junsong Yuan, Yuwei\n  Wu, Jing Tang, Raymond Huang", "title": "Campus3D: A Photogrammetry Point Cloud Benchmark for Hierarchical\n  Understanding of Outdoor Scene", "comments": "Accepted by the 28th ACM International Conference on Multimedia (ACM\n  MM 2020)", "journal-ref": "Proceedings of the 28th ACM International Conference on Multimedia\n  2020", "doi": "10.1145/3394171.3413661", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning on 3D scene-based point cloud has received extensive attention as\nits promising application in many fields, and well-annotated and multisource\ndatasets can catalyze the development of those data-driven approaches. To\nfacilitate the research of this area, we present a richly-annotated 3D point\ncloud dataset for multiple outdoor scene understanding tasks and also an\neffective learning framework for its hierarchical segmentation task. The\ndataset was generated via the photogrammetric processing on unmanned aerial\nvehicle (UAV) images of the National University of Singapore (NUS) campus, and\nhas been point-wisely annotated with both hierarchical and instance-based\nlabels. Based on it, we formulate a hierarchical learning problem for 3D point\ncloud segmentation and propose a measurement evaluating consistency across\nvarious hierarchies. To solve this problem, a two-stage method including\nmulti-task (MT) learning and hierarchical ensemble (HE) with consistency\nconsideration is proposed. Experimental results demonstrate the superiority of\nthe proposed method and potential advantages of our hierarchical annotations.\nIn addition, we benchmark results of semantic and instance segmentation, which\nis accessible online at https://3d.dataset.site with the dataset and all source\ncodes.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 19:10:32 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Li", "Xinke", ""], ["Li", "Chongshou", ""], ["Tong", "Zekun", ""], ["Lim", "Andrew", ""], ["Yuan", "Junsong", ""], ["Wu", "Yuwei", ""], ["Tang", "Jing", ""], ["Huang", "Raymond", ""]]}, {"id": "2008.04991", "submitter": "Marco De Nadai", "authors": "Raul Gomez, Yahui Liu, Marco De Nadai, Dimosthenis Karatzas, Bruno\n  Lepri and Nicu Sebe", "title": "Retrieval Guided Unsupervised Multi-domain Image-to-Image Translation", "comments": "Submitted to ACM MM '20, October 12-16, 2020, Seattle, WA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image to image translation aims to learn a mapping that transforms an image\nfrom one visual domain to another. Recent works assume that images descriptors\ncan be disentangled into a domain-invariant content representation and a\ndomain-specific style representation. Thus, translation models seek to preserve\nthe content of source images while changing the style to a target visual\ndomain. However, synthesizing new images is extremely challenging especially in\nmulti-domain translations, as the network has to compose content and style to\ngenerate reliable and diverse images in multiple domains. In this paper we\npropose the use of an image retrieval system to assist the image-to-image\ntranslation task. First, we train an image-to-image translation model to map\nimages to multiple domains. Then, we train an image retrieval model using real\nand generated images to find images similar to a query one in content but in a\ndifferent domain. Finally, we exploit the image retrieval system to fine-tune\nthe image-to-image translation model and generate higher quality images. Our\nexperiments show the effectiveness of the proposed solution and highlight the\ncontribution of the retrieval network, which can benefit from additional\nunlabeled data and help image-to-image translation models in the presence of\nscarce data.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 20:11:53 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Gomez", "Raul", ""], ["Liu", "Yahui", ""], ["De Nadai", "Marco", ""], ["Karatzas", "Dimosthenis", ""], ["Lepri", "Bruno", ""], ["Sebe", "Nicu", ""]]}, {"id": "2008.04999", "submitter": "Faegheh Sardari", "authors": "Faegheh Sardari, Adeline Paiement, Sion Hannuna, and Majid Mirmehdi", "title": "VI-Net: View-Invariant Quality of Human Movement Assessment", "comments": "13 pages, 6 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a view-invariant method towards the assessment of the quality of\nhuman movements which does not rely on skeleton data. Our end-to-end\nconvolutional neural network consists of two stages, where at first a\nview-invariant trajectory descriptor for each body joint is generated from RGB\nimages, and then the collection of trajectories for all joints are processed by\nan adapted, pre-trained 2D CNN (e.g. VGG-19 or ResNeXt-50) to learn the\nrelationship amongst the different body parts and deliver a score for the\nmovement quality. We release the only publicly-available, multi-view,\nnon-skeleton, non-mocap, rehabilitation movement dataset (QMAR), and provide\nresults for both cross-subject and cross-view scenarios on this dataset. We\nshow that VI-Net achieves average rank correlation of 0.66 on cross-subject and\n0.65 on unseen views when trained on only two views. We also evaluate the\nproposed method on the single-view rehabilitation dataset KIMORE and obtain\n0.66 rank correlation against a baseline of 0.62.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 20:51:42 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Sardari", "Faegheh", ""], ["Paiement", "Adeline", ""], ["Hannuna", "Sion", ""], ["Mirmehdi", "Majid", ""]]}, {"id": "2008.05023", "submitter": "Alexander Richard", "authors": "Alexander Richard, Colin Lea, Shugao Ma, Juergen Gall, Fernando de la\n  Torre, Yaser Sheikh", "title": "Audio- and Gaze-driven Facial Animation of Codec Avatars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Codec Avatars are a recent class of learned, photorealistic face models that\naccurately represent the geometry and texture of a person in 3D (i.e., for\nvirtual reality), and are almost indistinguishable from video. In this paper we\ndescribe the first approach to animate these parametric models in real-time\nwhich could be deployed on commodity virtual reality hardware using audio\nand/or eye tracking. Our goal is to display expressive conversations between\nindividuals that exhibit important social signals such as laughter and\nexcitement solely from latent cues in our lossy input signals. To this end we\ncollected over 5 hours of high frame rate 3D face scans across three\nparticipants including traditional neutral speech as well as expressive and\nconversational speech. We investigate a multimodal fusion approach that\ndynamically identifies which sensor encoding should animate which parts of the\nface at any time. See the supplemental video which demonstrates our ability to\ngenerate full face motion far beyond the typically neutral lip articulations\nseen in competing work:\nhttps://research.fb.com/videos/audio-and-gaze-driven-facial-animation-of-codec-avatars/\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 22:28:48 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Richard", "Alexander", ""], ["Lea", "Colin", ""], ["Ma", "Shugao", ""], ["Gall", "Juergen", ""], ["de la Torre", "Fernando", ""], ["Sheikh", "Yaser", ""]]}, {"id": "2008.05024", "submitter": "Kuo-Wei Lai", "authors": "Kuo-Wei Lai, Manisha Aggarwal, Peter van Zijl, Xu Li, Jeremias Sulam", "title": "Learned Proximal Networks for Quantitative Susceptibility Mapping", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative Susceptibility Mapping (QSM) estimates tissue magnetic\nsusceptibility distributions from Magnetic Resonance (MR) phase measurements by\nsolving an ill-posed dipole inversion problem. Conventional single orientation\nQSM methods usually employ regularization strategies to stabilize such\ninversion, but may suffer from streaking artifacts or over-smoothing. Multiple\norientation QSM such as calculation of susceptibility through multiple\norientation sampling (COSMOS) can give well-conditioned inversion and an\nartifact free solution but has expensive acquisition costs. On the other hand,\nConvolutional Neural Networks (CNN) show great potential for medical image\nreconstruction, albeit often with limited interpretability. Here, we present a\nLearned Proximal Convolutional Neural Network (LP-CNN) for solving the\nill-posed QSM dipole inversion problem in an iterative proximal gradient\ndescent fashion. This approach combines the strengths of data-driven\nrestoration priors and the clear interpretability of iterative solvers that can\ntake into account the physical model of dipole convolution. During training,\nour LP-CNN learns an implicit regularizer via its proximal, enabling the\ndecoupling between the forward operator and the data-driven parameters in the\nreconstruction algorithm. More importantly, this framework is believed to be\nthe first deep learning QSM approach that can naturally handle an arbitrary\nnumber of phase input measurements without the need for any ad-hoc rotation or\nre-training. We demonstrate that the LP-CNN provides state-of-the-art\nreconstruction results compared to both traditional and deep learning methods\nwhile allowing for more flexibility in the reconstruction process.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 22:35:24 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Lai", "Kuo-Wei", ""], ["Aggarwal", "Manisha", ""], ["van Zijl", "Peter", ""], ["Li", "Xu", ""], ["Sulam", "Jeremias", ""]]}, {"id": "2008.05028", "submitter": "Akin Yilmaz", "authors": "M. Akin Yilmaz and A. Murat Tekalp", "title": "End-to-End Rate-Distortion Optimization for Bi-Directional Learned Video\n  Compression", "comments": "This work is accepted for publication in IEEE ICIP 2020", "journal-ref": null, "doi": "10.1109/ICIP40778.2020.9190881", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional video compression methods employ a linear transform and block\nmotion model, and the steps of motion estimation, mode and quantization\nparameter selection, and entropy coding are optimized individually due to\ncombinatorial nature of the end-to-end optimization problem. Learned video\ncompression allows end-to-end rate-distortion optimized training of all\nnonlinear modules, quantization parameter and entropy model simultaneously.\nWhile previous work on learned video compression considered training a\nsequential video codec based on end-to-end optimization of cost averaged over\npairs of successive frames, it is well-known in conventional video compression\nthat hierarchical, bi-directional coding outperforms sequential compression. In\nthis paper, we propose for the first time end-to-end optimization of a\nhierarchical, bi-directional motion compensated learned codec by accumulating\ncost function over fixed-size groups of pictures (GOP). Experimental results\nshow that the rate-distortion performance of our proposed learned\nbi-directional {\\it GOP coder} outperforms the state-of-the-art end-to-end\noptimized learned sequential compression as expected.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 22:50:06 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 19:12:26 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Yilmaz", "M. Akin", ""], ["Tekalp", "A. Murat", ""]]}, {"id": "2008.05058", "submitter": "Abhinav Valada", "authors": "Borna Be\\v{s}i\\'c and Abhinav Valada", "title": "Dynamic Object Removal and Spatio-Temporal RGB-D Inpainting via\n  Geometry-Aware Adversarial Learning", "comments": "Dataset, code and models are available at\n  http://rl.uni-freiburg.de/research/rgbd-inpainting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic objects have a significant impact on the robot's perception of the\nenvironment which degrades the performance of essential tasks such as\nlocalization and mapping. In this work, we address this problem by synthesizing\nplausible color, texture and geometry in regions occluded by dynamic objects.\nWe propose the novel geometry-aware DynaFill architecture that follows a\ncoarse-to-fine topology and incorporates our gated recurrent feedback mechanism\nto adaptively fuse information from previous timesteps. We optimize our\narchitecture using adversarial training to synthesize fine realistic textures\nwhich enables it to hallucinate color and depth structure in occluded regions\nonline in a spatially and temporally coherent manner, without relying on future\nframe information. Casting our inpainting problem as an image-to-image\ntranslation task, our model also corrects regions correlated with the presence\nof dynamic objects in the scene, such as shadows or reflections. We introduce a\nlarge-scale hyperrealistic dataset with RGB-D images, semantic segmentation\nlabels, camera poses as well as groundtruth RGB-D information of occluded\nregions. Extensive quantitative and qualitative evaluations show that our\napproach achieves state-of-the-art performance, even in challenging weather\nconditions. Furthermore, we present results for retrieval-based visual\nlocalization with the synthesized images that demonstrate the utility of our\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 01:23:21 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 16:33:29 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 01:40:17 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Be\u0161i\u0107", "Borna", ""], ["Valada", "Abhinav", ""]]}, {"id": "2008.05060", "submitter": "Mona Jalal", "authors": "Won Hwa Kim, Mona Jalal, Seongjae Hwang, Sterling C. Johnson, Vikas\n  Singh", "title": "Online Graph Completion: Multivariate Signal Recovery in Computer Vision", "comments": "9 pages, 7 figures, CVPR 2017 Conference", "journal-ref": null, "doi": "10.1109/CVPR.2017.533", "report-no": null, "categories": "cs.CV cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adoption of \"human-in-the-loop\" paradigms in computer vision and machine\nlearning is leading to various applications where the actual data acquisition\n(e.g., human supervision) and the underlying inference algorithms are closely\ninterwined. While classical work in active learning provides effective\nsolutions when the learning module involves classification and regression\ntasks, many practical issues such as partially observed measurements, financial\nconstraints and even additional distributional or structural aspects of the\ndata typically fall outside the scope of this treatment. For instance, with\nsequential acquisition of partial measurements of data that manifest as a\nmatrix (or tensor), novel strategies for completion (or collaborative\nfiltering) of the remaining entries have only been studied recently. Motivated\nby vision problems where we seek to annotate a large dataset of images via a\ncrowdsourced platform or alternatively, complement results from a\nstate-of-the-art object detector using human feedback, we study the\n\"completion\" problem defined on graphs, where requests for additional\nmeasurements must be made sequentially. We design the optimization model in the\nFourier domain of the graph describing how ideas based on adaptive\nsubmodularity provide algorithms that work well in practice. On a large set of\nimages collected from Imgur, we see promising results on images that are\notherwise difficult to categorize. We also show applications to an experimental\ndesign problem in neuroimaging.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 01:34:21 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Kim", "Won Hwa", ""], ["Jalal", "Mona", ""], ["Hwang", "Seongjae", ""], ["Johnson", "Sterling C.", ""], ["Singh", "Vikas", ""]]}, {"id": "2008.05065", "submitter": "Hang Yang", "authors": "Hang Yang and Xiaotian Wu and Xinglong Sun", "title": "Select Good Regions for Deblurring based on Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of blind image deblurring is to recover sharp image from one input\nblurred image with an unknown blur kernel. Most of image deblurring approaches\nfocus on developing image priors, however, there is not enough attention to the\ninfluence of image details and structures on the blur kernel estimation. What\nis the useful image structure and how to choose a good deblurring region? In\nthis work, we propose a deep neural network model method for selecting good\nregions to estimate blur kernel. First we construct image patches with labels\nand train a deep neural networks, then the learned model is applied to\ndetermine which region of the image is most suitable to deblur. Experimental\nresults illustrate that the proposed approach is effective, and could be able\nto select good regions for image deblurring.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 01:58:35 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Yang", "Hang", ""], ["Wu", "Xiaotian", ""], ["Sun", "Xinglong", ""]]}, {"id": "2008.05079", "submitter": "Lixin Yang", "authors": "Lixin Yang, Jiasen Li, Wenqiang Xu, Yiqun Diao, Cewu Lu", "title": "BiHand: Recovering Hand Mesh with Multi-stage Bisected Hourglass\n  Networks", "comments": "To appear on BMVC2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D hand estimation has been a long-standing research topic in computer\nvision. A recent trend aims not only to estimate the 3D hand joint locations\nbut also to recover the mesh model. However, achieving those goals from a\nsingle RGB image remains challenging. In this paper, we introduce an end-to-end\nlearnable model, BiHand, which consists of three cascaded stages, namely 2D\nseeding stage, 3D lifting stage, and mesh generation stage. At the output of\nBiHand, the full hand mesh will be recovered using the joint rotations and\nshape parameters predicted from the network. Inside each stage, BiHand adopts a\nnovel bisecting design which allows the networks to encapsulate two closely\nrelated information (e.g. 2D keypoints and silhouette in 2D seeding stage, 3D\njoints, and depth map in 3D lifting stage, joint rotations and shape parameters\nin the mesh generation stage) in a single forward pass. As the information\nrepresents different geometry or structure details, bisecting the data flow can\nfacilitate optimization and increase robustness. For quantitative evaluation,\nwe conduct experiments on two public benchmarks, namely the Rendered Hand\nDataset (RHD) and the Stereo Hand Pose Tracking Benchmark (STB). Extensive\nexperiments show that our model can achieve superior accuracy in comparison\nwith state-of-the-art methods, and can produce appealing 3D hand meshes in\nseveral severe conditions.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 03:13:17 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Yang", "Lixin", ""], ["Li", "Jiasen", ""], ["Xu", "Wenqiang", ""], ["Diao", "Yiqun", ""], ["Lu", "Cewu", ""]]}, {"id": "2008.05084", "submitter": "Yang Chen", "authors": "Yang Chen, Martin Alain, Aljosa Smolic", "title": "Self-supervised Light Field View Synthesis Using Cycle Consistency", "comments": "Accepted at MMSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High angular resolution is advantageous for practical applications of light\nfields. In order to enhance the angular resolution of light fields, view\nsynthesis methods can be utilized to generate dense intermediate views from\nsparse light field input. Most successful view synthesis methods are\nlearning-based approaches which require a large amount of training data paired\nwith ground truth. However, collecting such large datasets for light fields is\nchallenging compared to natural images or videos. To tackle this problem, we\npropose a self-supervised light field view synthesis framework with cycle\nconsistency. The proposed method aims to transfer prior knowledge learned from\nhigh quality natural video datasets to the light field view synthesis task,\nwhich reduces the need for labeled light field data. A cycle consistency\nconstraint is used to build bidirectional mapping enforcing the generated views\nto be consistent with the input views. Derived from this key concept, two loss\nfunctions, cycle loss and reconstruction loss, are used to fine-tune the\npre-trained model of a state-of-the-art video interpolation method. The\nproposed method is evaluated on various datasets to validate its robustness,\nand results show it not only achieves competitive performance compared to\nsupervised fine-tuning, but also outperforms state-of-the-art light field view\nsynthesis methods, especially when generating multiple intermediate views.\nBesides, our generic light field view synthesis framework can be adopted to any\npre-trained model for advanced video interpolation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 03:20:19 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Chen", "Yang", ""], ["Alain", "Martin", ""], ["Smolic", "Aljosa", ""]]}, {"id": "2008.05090", "submitter": "Wenqing Chu", "authors": "Wenqing Chu, Wei-Chih Hung, Yi-Hsuan Tsai, Yu-Ting Chang, Yijun Li,\n  Deng Cai, Ming-Hsuan Yang", "title": "Learning to Caricature via Semantic Shape Transform", "comments": "Submitted to IJCV, code and model are available at\n  https://github.com/wenqingchu/Semantic-CariGANs/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caricature is an artistic drawing created to abstract or exaggerate facial\nfeatures of a person. Rendering visually pleasing caricatures is a difficult\ntask that requires professional skills, and thus it is of great interest to\ndesign a method to automatically generate such drawings. To deal with large\nshape changes, we propose an algorithm based on a semantic shape transform to\nproduce diverse and plausible shape exaggerations. Specifically, we predict\npixel-wise semantic correspondences and perform image warping on the input\nphoto to achieve dense shape transformation. We show that the proposed\nframework is able to render visually pleasing shape exaggerations while\nmaintaining their facial structures. In addition, our model allows users to\nmanipulate the shape via the semantic map. We demonstrate the effectiveness of\nour approach on a large photograph-caricature benchmark dataset with\ncomparisons to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 03:41:49 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 06:58:02 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Chu", "Wenqing", ""], ["Hung", "Wei-Chih", ""], ["Tsai", "Yi-Hsuan", ""], ["Chang", "Yu-Ting", ""], ["Li", "Yijun", ""], ["Cai", "Deng", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2008.05096", "submitter": "Xiaolin Zhang", "authors": "Xiaolin Zhang, Yunchao Wei, Yi Yang", "title": "Inter-Image Communication for Weakly Supervised Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised localization aims at finding target object regions using\nonly image-level supervision. However, localization maps extracted from\nclassification networks are often not accurate due to the lack of fine\npixel-level supervision. In this paper, we propose to leverage pixel-level\nsimilarities across different objects for learning more accurate object\nlocations in a complementary way. Particularly, two kinds of constraints are\nproposed to prompt the consistency of object features within the same\ncategories. The first constraint is to learn the stochastic feature consistency\namong discriminative pixels that are randomly sampled from different images\nwithin a batch. The discriminative information embedded in one image can be\nleveraged to benefit its counterpart with inter-image communication. The second\nconstraint is to learn the global consistency of object features throughout the\nentire dataset. We learn a feature center for each category and realize the\nglobal feature consistency by forcing the object features to approach\nclass-specific centers. The global centers are actively updated with the\ntraining process. The two constraints can benefit each other to learn\nconsistent pixel-level features within the same categories, and finally improve\nthe quality of localization maps. We conduct extensive experiments on two\npopular benchmarks, i.e., ILSVRC and CUB-200-2011. Our method achieves the\nTop-1 localization error rate of 45.17% on the ILSVRC validation set,\nsurpassing the current state-of-the-art method by a large margin. The code is\navailable at https://github.com/xiaomengyc/I2C.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 04:14:11 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Zhang", "Xiaolin", ""], ["Wei", "Yunchao", ""], ["Yang", "Yi", ""]]}, {"id": "2008.05101", "submitter": "Bohan Zhuang", "authors": "Peng Chen, Bohan Zhuang, Chunhua Shen", "title": "FATNN: Fast and Accurate Ternary Neural Networks", "comments": "Accepted to Proc. Int. Conf. Computer Vision, ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ternary Neural Networks (TNNs) have received much attention due to being\npotentially orders of magnitude faster in inference, as well as more power\nefficient, than full-precision counterparts. However, 2 bits are required to\nencode the ternary representation with only 3 quantization levels leveraged. As\na result, conventional TNNs have similar memory consumption and speed compared\nwith the standard 2-bit models, but have worse representational capability.\nMoreover, there is still a significant gap in accuracy between TNNs and\nfull-precision networks, hampering their deployment to real applications. To\ntackle these two challenges, in this work, we first show that, under some mild\nconstraints, computational complexity of the ternary inner product can be\nreduced by a factor of 2. Second, to mitigate the performance gap, we\nelaborately design an implementation-dependent ternary quantization algorithm.\nThe proposed framework is termed Fast and Accurate Ternary Neural Networks\n(FATNN). Experiments on image classification demonstrate that our FATNN\nsurpasses the state-of-the-arts by a significant margin in accuracy. More\nimportantly, speedup evaluation compared with various precisions is analyzed on\nseveral platforms, which serves as a strong benchmark for further research.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 04:26:18 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 09:21:34 GMT"}, {"version": "v3", "created": "Fri, 19 Mar 2021 03:57:13 GMT"}, {"version": "v4", "created": "Thu, 29 Jul 2021 11:50:10 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Chen", "Peng", ""], ["Zhuang", "Bohan", ""], ["Shen", "Chunhua", ""]]}, {"id": "2008.05105", "submitter": "Zhipeng Ding", "authors": "Zhipeng Ding, Xu Han, Peirong Liu, Marc Niethammer", "title": "Local Temperature Scaling for Probability Calibration", "comments": "Accepted by ICCV-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For semantic segmentation, label probabilities are often uncalibrated as they\nare typically only the by-product of a segmentation task. Intersection over\nUnion (IoU) and Dice score are often used as criteria for segmentation success,\nwhile metrics related to label probabilities are not often explored. However,\nprobability calibration approaches have been studied, which match probability\noutputs with experimentally observed errors. These approaches mainly focus on\nclassification tasks, but not on semantic segmentation. Thus, we propose a\nlearning-based calibration method that focuses on multi-label semantic\nsegmentation. Specifically, we adopt a convolutional neural network to predict\nlocal temperature values for probability calibration. One advantage of our\napproach is that it does not change prediction accuracy, hence allowing for\ncalibration as a post-processing step. Experiments on the COCO, CamVid, and\nLPBA40 datasets demonstrate improved calibration performance for a range of\ndifferent metrics. We also demonstrate the good performance of our method for\nmulti-atlas brain segmentation from magnetic resonance images.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 04:39:32 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 10:30:12 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Ding", "Zhipeng", ""], ["Han", "Xu", ""], ["Liu", "Peirong", ""], ["Niethammer", "Marc", ""]]}, {"id": "2008.05110", "submitter": "Keyu Chen", "authors": "Juyong Zhang, Keyu Chen, Jianmin Zheng", "title": "Facial Expression Retargeting from Human to Avatar Made Easy", "comments": "IEEE Transactions on Visualization and Computer Graphics (TVCG), to\n  appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression retargeting from humans to virtual characters is a useful\ntechnique in computer graphics and animation. Traditional methods use markers\nor blendshapes to construct a mapping between the human and avatar faces.\nHowever, these approaches require a tedious 3D modeling process, and the\nperformance relies on the modelers' experience. In this paper, we propose a\nbrand-new solution to this cross-domain expression transfer problem via\nnonlinear expression embedding and expression domain translation. We first\nbuild low-dimensional latent spaces for the human and avatar facial expressions\nwith variational autoencoder. Then we construct correspondences between the two\nlatent spaces guided by geometric and perceptual constraints. Specifically, we\ndesign geometric correspondences to reflect geometric matching and utilize a\ntriplet data structure to express users' perceptual preference of avatar\nexpressions. A user-friendly method is proposed to automatically generate\ntriplets for a system allowing users to easily and efficiently annotate the\ncorrespondences. Using both geometric and perceptual correspondences, we\ntrained a network for expression domain translation from human to avatar.\nExtensive experimental results and user studies demonstrate that even\nnonprofessional users can apply our method to generate high-quality facial\nexpression retargeting results with less time and effort.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 04:55:54 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Zhang", "Juyong", ""], ["Chen", "Keyu", ""], ["Zheng", "Jianmin", ""]]}, {"id": "2008.05117", "submitter": "Stefano Cerri", "authors": "Stefano Cerri, Andrew Hoopes, Douglas N. Greve, Mark M\\\"uhlau, Koen\n  Van Leemput", "title": "A Longitudinal Method for Simultaneous Whole-Brain and Lesion\n  Segmentation in Multiple Sclerosis", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-66843-3_12", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel method for the segmentation of longitudinal\nbrain MRI scans of patients suffering from Multiple Sclerosis. The method\nbuilds upon an existing cross-sectional method for simultaneous whole-brain and\nlesion segmentation, introducing subject-specific latent variables to encourage\ntemporal consistency between longitudinal scans. It is very generally\napplicable, as it does not make any prior assumptions on the scanner, the MRI\nprotocol, or the number and timing of longitudinal follow-up scans. Preliminary\nexperiments on three longitudinal datasets indicate that the proposed method\nproduces more reliable segmentations and detects disease effects better than\nthe cross-sectional method it is based upon.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 05:43:59 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 13:03:48 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Cerri", "Stefano", ""], ["Hoopes", "Andrew", ""], ["Greve", "Douglas N.", ""], ["M\u00fchlau", "Mark", ""], ["Van Leemput", "Koen", ""]]}, {"id": "2008.05129", "submitter": "Xin Sun", "authors": "Xin Sun, Chi Zhang, Guosheng Lin and Keck-Voon Ling", "title": "Open Set Recognition with Conditional Probabilistic Generative Models", "comments": "Extended version of CGDL arXiv:2003.08823 in CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have made breakthroughs in a wide range of visual\nunderstanding tasks. A typical challenge that hinders their real-world\napplications is that unknown samples may be fed into the system during the\ntesting phase, but traditional deep neural networks will wrongly recognize\nthese unknown samples as one of the known classes. Open set recognition (OSR)\nis a potential solution to overcome this problem, where the open set classifier\nshould have the flexibility to reject unknown samples and meanwhile maintain\nhigh classification accuracy in known classes. Probabilistic generative models,\nsuch as Variational Autoencoders (VAE) and Adversarial Autoencoders (AAE), are\npopular methods to detect unknowns, but they cannot provide discriminative\nrepresentations for known classification. In this paper, we propose a novel\nframework, called Conditional Probabilistic Generative Models (CPGM), for open\nset recognition. The core insight of our work is to add discriminative\ninformation into the probabilistic generative models, such that the proposed\nmodels can not only detect unknown samples but also classify known classes by\nforcing different latent features to approximate conditional Gaussian\ndistributions. We discuss many model variants and provide comprehensive\nexperiments to study their characteristics. Experiment results on multiple\nbenchmark datasets reveal that the proposed method significantly outperforms\nthe baselines and achieves new state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 06:23:49 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 10:18:43 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Sun", "Xin", ""], ["Zhang", "Chi", ""], ["Lin", "Guosheng", ""], ["Ling", "Keck-Voon", ""]]}, {"id": "2008.05132", "submitter": "Jieshan Chen", "authors": "Jieshan Chen, Mulong Xie, Zhenchang Xing, Chunyang Chen, Xiwei Xu,\n  Liming Zhu and Guoqiang Li", "title": "Object Detection for Graphical User Interface: Old Fashioned or Deep\n  Learning or a Combination?", "comments": "13 pages, accepted to ESEC/FSE '20", "journal-ref": null, "doi": "10.1145/3368089.3409691", "report-no": null, "categories": "cs.CV cs.HC cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting Graphical User Interface (GUI) elements in GUI images is a\ndomain-specific object detection task. It supports many software engineering\ntasks, such as GUI animation and testing, GUI search and code generation.\nExisting studies for GUI element detection directly borrow the mature methods\nfrom computer vision (CV) domain, including old fashioned ones that rely on\ntraditional image processing features (e.g., canny edge, contours), and deep\nlearning models that learn to detect from large-scale GUI data. Unfortunately,\nthese CV methods are not originally designed with the awareness of the unique\ncharacteristics of GUIs and GUI elements and the high localization accuracy of\nthe GUI element detection task. We conduct the first large-scale empirical\nstudy of seven representative GUI element detection methods on over 50k GUI\nimages to understand the capabilities, limitations and effective designs of\nthese methods. This study not only sheds the light on the technical challenges\nto be addressed but also informs the design of new GUI element detection\nmethods. We accordingly design a new GUI-specific old-fashioned method for\nnon-text GUI element detection which adopts a novel top-down coarse-to-fine\nstrategy, and incorporate it with the mature deep learning model for GUI text\ndetection.Our evaluation on 25,000 GUI images shows that our method\nsignificantly advances the start-of-the-art performance in GUI element\ndetection.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 06:36:33 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 12:57:33 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Chen", "Jieshan", ""], ["Xie", "Mulong", ""], ["Xing", "Zhenchang", ""], ["Chen", "Chunyang", ""], ["Xu", "Xiwei", ""], ["Zhu", "Liming", ""], ["Li", "Guoqiang", ""]]}, {"id": "2008.05133", "submitter": "Jiajun Cai", "authors": "Jiajun Cai and Bo Huang", "title": "An Inter- and Intra-Band Loss for Pansharpening Convolutional Neural\n  Networks", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pansharpening aims to fuse panchromatic and multispectral images from the\nsatellite to generate images with both high spatial and spectral resolution.\nWith the successful applications of deep learning in the computer vision field,\na lot of scholars have proposed many convolutional neural networks (CNNs) to\nsolve the pansharpening task. These pansharpening networks focused on various\ndistinctive structures of CNNs, and most of them are trained by L2 loss between\nfused images and simulated desired multispectral images. However, L2 loss is\ndesigned to directly minimize the difference of spectral information of each\nband, which does not consider the inter-band relations in the training process.\nIn this letter, we propose a novel inter- and intra-band (IIB) loss to overcome\nthe drawback of original L2 loss. Our proposed IIB loss can effectively\npreserve both inter- and intra-band relations and can be directly applied to\ndifferent pansharpening CNNs.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 06:38:15 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Cai", "Jiajun", ""], ["Huang", "Bo", ""]]}, {"id": "2008.05149", "submitter": "Hanwen Cao", "authors": "Hanwen Cao, Yongyi Lu, Cewu Lu, Bo Pang, Gongshen Liu, Alan Yuille", "title": "ASAP-Net: Attention and Structure Aware Point Cloud Sequence\n  Segmentation", "comments": "The British Machine Vision Conference (BMVC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works of point clouds show that mulit-frame spatio-temporal modeling\noutperforms single-frame versions by utilizing cross-frame information. In this\npaper, we further improve spatio-temporal point cloud feature learning with a\nflexible module called ASAP considering both attention and structure\ninformation across frames, which we find as two important factors for\nsuccessful segmentation in dynamic point clouds. Firstly, our ASAP module\ncontains a novel attentive temporal embedding layer to fuse the relatively\ninformative local features across frames in a recurrent fashion. Secondly, an\nefficient spatio-temporal correlation method is proposed to exploit more local\nstructure for embedding, meanwhile enforcing temporal consistency and reducing\ncomputation complexity. Finally, we show the generalization ability of the\nproposed ASAP module with different backbone networks for point cloud sequence\nsegmentation. Our ASAP-Net (backbone plus ASAP module) outperforms baselines\nand previous methods on both Synthia and SemanticKITTI datasets (+3.4 to +15.2\nmIoU points with different backbones). Code is availabe at\nhttps://github.com/intrepidChw/ASAP-Net\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 07:37:16 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Cao", "Hanwen", ""], ["Lu", "Yongyi", ""], ["Lu", "Cewu", ""], ["Pang", "Bo", ""], ["Liu", "Gongshen", ""], ["Yuille", "Alan", ""]]}, {"id": "2008.05156", "submitter": "Xiaoyu Yue", "authors": "Meng Wei, Chun Yuan, Xiaoyu Yue, Kuo Zhong", "title": "HOSE-Net: Higher Order Structure Embedded Network for Scene Graph\n  Generation", "comments": "Accepted to ACM MM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene graph generation aims to produce structured representations for images,\nwhich requires to understand the relations between objects. Due to the\ncontinuous nature of deep neural networks, the prediction of scene graphs is\ndivided into object detection and relation classification. However, the\nindependent relation classes cannot separate the visual features well. Although\nsome methods organize the visual features into graph structures and use message\npassing to learn contextual information, they still suffer from drastic\nintra-class variations and unbalanced data distributions. One important factor\nis that they learn an unstructured output space that ignores the inherent\nstructures of scene graphs. Accordingly, in this paper, we propose a Higher\nOrder Structure Embedded Network (HOSE-Net) to mitigate this issue. First, we\npropose a novel structure-aware embedding-to-classifier(SEC) module to\nincorporate both local and global structural information of relationships into\nthe output space. Specifically, a set of context embeddings are learned via\nlocal graph based message passing and then mapped to a global structure based\nclassification space. Second, since learning too many context-specific\nclassification subspaces can suffer from data sparsity issues, we propose a\nhierarchical semantic aggregation(HSA) module to reduces the number of\nsubspaces by introducing higher order structural information. HSA is also a\nfast and flexible tool to automatically search a semantic object hierarchy\nbased on relational knowledge graphs. Extensive experiments show that the\nproposed HOSE-Net achieves the state-of-the-art performance on two popular\nbenchmarks of Visual Genome and VRD.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 07:58:13 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Wei", "Meng", ""], ["Yuan", "Chun", ""], ["Yue", "Xiaoyu", ""], ["Zhong", "Kuo", ""]]}, {"id": "2008.05157", "submitter": "Di Qiu", "authors": "Di Qiu, Jin Zeng, Zhanghan Ke, Wenxiu Sun, Chengxi Yang", "title": "Towards Geometry Guided Neural Relighting with Flash Photography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous image based relighting methods require capturing multiple images to\nacquire high frequency lighting effect under different lighting conditions,\nwhich needs nontrivial effort and may be unrealistic in certain practical use\nscenarios. While such approaches rely entirely on cleverly sampling the color\nimages under different lighting conditions, little has been done to utilize\ngeometric information that crucially influences the high-frequency features in\nthe images, such as glossy highlight and cast shadow. We therefore propose a\nframework for image relighting from a single flash photograph with its\ncorresponding depth map using deep learning. By incorporating the depth map,\nour approach is able to extrapolate realistic high-frequency effects under\nnovel lighting via geometry guided image decomposition from the flashlight\nimage, and predict the cast shadow map from the shadow-encoding transformed\ndepth map. Moreover, the single-image based setup greatly simplifies the data\ncapture process. We experimentally validate the advantage of our geometry\nguided approach over state-of-the-art image-based approaches in intrinsic image\ndecomposition and image relighting, and also demonstrate our performance on\nreal mobile phone photo examples.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 08:03:28 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Qiu", "Di", ""], ["Zeng", "Jin", ""], ["Ke", "Zhanghan", ""], ["Sun", "Wenxiu", ""], ["Yang", "Chengxi", ""]]}, {"id": "2008.05158", "submitter": "Sungho Yoon", "authors": "Sungho Yoon and Ayoung Kim", "title": "Balanced Depth Completion between Dense Depth Inference and Sparse Range\n  Measurements via KISS-GP", "comments": "accepted to IROS 2020. 8 pages, 9 figures, 2 tables. Video at this\n  https://www.youtube.com/watch?v=x8n0lvjvorg&t=33s", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating a dense and accurate depth map is the key requirement for\nautonomous driving and robotics. Recent advances in deep learning have allowed\ndepth estimation in full resolution from a single image. Despite this\nimpressive result, many deep-learning-based monocular depth estimation (MDE)\nalgorithms have failed to keep their accuracy yielding a meter-level estimation\nerror. In many robotics applications, accurate but sparse measurements are\nreadily available from Light Detection and Ranging (LiDAR). Although they are\nhighly accurate, the sparsity limits full resolution depth map reconstruction.\nTargeting the problem of dense and accurate depth map recovery, this paper\nintroduces the fusion of these two modalities as a depth completion (DC)\nproblem by dividing the role of depth inference and depth regression. Utilizing\nthe state-of-the-art MDE and our Gaussian process (GP) based depth-regression\nmethod, we propose a general solution that can flexibly work with various MDE\nmodules by enhancing its depth with sparse range measurements. To overcome the\nmajor limitation of GP, we adopt Kernel Interpolation for Scalable Structured\n(KISS)-GP and mitigate the computational complexity from O(N^3) to O(N). Our\nexperiments demonstrate that the accuracy and robustness of our method\noutperform state-of-the-art unsupervised methods for sparse and biased\nmeasurements.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 08:07:55 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Yoon", "Sungho", ""], ["Kim", "Ayoung", ""]]}, {"id": "2008.05196", "submitter": "Shan Li", "authors": "Wenjing Yan, Shan Li, Chengtao Que, JiQuan Pei, Weihong Deng", "title": "RAF-AU Database: In-the-Wild Facial Expressions with Subjective Emotion\n  Judgement and Objective AU Annotations", "comments": "ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Much of the work on automatic facial expression recognition relies on\ndatabases containing a certain number of emotion classes and their exaggerated\nfacial configurations (generally six prototypical facial expressions), based on\nEkman's Basic Emotion Theory. However, recent studies have revealed that facial\nexpressions in our human life can be blended with multiple basic emotions. And\nthe emotion labels for these in-the-wild facial expressions cannot easily be\nannotated solely on pre-defined AU patterns. How to analyze the action units\nfor such complex expressions is still an open question. To address this issue,\nwe develop a RAF-AU database that employs a sign-based (i.e., AUs) and\njudgement-based (i.e., perceived emotion) approach to annotating blended facial\nexpressions in the wild. We first reviewed the annotation methods in existing\ndatabases and identified crowdsourcing as a promising strategy for labeling\nin-the-wild facial expressions. Then, RAF-AU was finely annotated by\nexperienced coders, on which we also conducted a preliminary investigation of\nwhich key AUs contribute most to a perceived emotion, and the relationship\nbetween AUs and facial expressions. Finally, we provided a baseline for AU\nrecognition in RAF-AU using popular features and multi-label learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 09:29:16 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 03:29:15 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2020 07:20:14 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Yan", "Wenjing", ""], ["Li", "Shan", ""], ["Que", "Chengtao", ""], ["Pei", "JiQuan", ""], ["Deng", "Weihong", ""]]}, {"id": "2008.05202", "submitter": "Changqian Yu", "authors": "Changqian Yu, Yifan Liu, Changxin Gao, Chunhua Shen, Nong Sang", "title": "Representative Graph Neural Network", "comments": "Accepted to ECCV 2020. Code is available at https://git.io/RepGraph", "journal-ref": "European Conference on Computer Vision 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-local operation is widely explored to model the long-range dependencies.\nHowever, the redundant computation in this operation leads to a prohibitive\ncomplexity. In this paper, we present a Representative Graph (RepGraph) layer\nto dynamically sample a few representative features, which dramatically reduces\nredundancy. Instead of propagating the messages from all positions, our\nRepGraph layer computes the response of one node merely with a few\nrepresentative nodes. The locations of representative nodes come from a learned\nspatial offset matrix. The RepGraph layer is flexible to integrate into many\nvisual architectures and combine with other operations. With the application of\nsemantic segmentation, without any bells and whistles, our RepGraph network can\ncompete or perform favourably against the state-of-the-art methods on three\nchallenging benchmarks: ADE20K, Cityscapes, and PASCAL-Context datasets. In the\ntask of object detection, our RepGraph layer can also improve the performance\non the COCO dataset compared to the non-local operation. Code is available at\nhttps://git.io/RepGraph.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 09:46:52 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Yu", "Changqian", ""], ["Liu", "Yifan", ""], ["Gao", "Changxin", ""], ["Shen", "Chunhua", ""], ["Sang", "Nong", ""]]}, {"id": "2008.05204", "submitter": "Iason Katsamenis", "authors": "Iason Katsamenis, Eftychios Protopapadakis, Anastasios Doulamis,\n  Nikolaos Doulamis, Athanasios Voulodimos", "title": "Pixel-level Corrosion Detection on Metal Constructions by Fusion of Deep\n  Learning Semantic and Contour Segmentation", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Corrosion detection on metal constructions is a major challenge in civil\nengineering for quick, safe and effective inspection. Existing image analysis\napproaches tend to place bounding boxes around the defected region which is not\nadequate both for structural analysis and pre-fabrication, an innovative\nconstruction concept which reduces maintenance cost, time and improves safety.\nIn this paper, we apply three semantic segmentation-oriented deep learning\nmodels (FCN, U-Net and Mask R-CNN) for corrosion detection, which perform\nbetter in terms of accuracy and time and require a smaller number of annotated\nsamples compared to other deep models, e.g. CNN. However, the final images\nderived are still not sufficiently accurate for structural analysis and\npre-fabrication. Thus, we adopt a novel data projection scheme that fuses the\nresults of color segmentation, yielding accurate but over-segmented contours of\na region, with a processed area of the deep masks, resulting in high-confidence\ncorroded pixels.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 09:54:17 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Katsamenis", "Iason", ""], ["Protopapadakis", "Eftychios", ""], ["Doulamis", "Anastasios", ""], ["Doulamis", "Nikolaos", ""], ["Voulodimos", "Athanasios", ""]]}, {"id": "2008.05217", "submitter": "Nicolas Basty", "authors": "Julie Fitzpatrick, Nicolas Basty, Madeleine Cule, Yi Liu, Jimmy D.\n  Bell, E. Louise Thomas, Brandon Whitcher", "title": "Large-Scale Analysis of Iliopsoas Muscle Volumes in the UK Biobank", "comments": "Julie Fitzpatrick and Nicolas Basty are joint first authors", "journal-ref": "Scientific Reports 10 (1), 1-10, 2020", "doi": "10.1038/s41598-020-77351-0", "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Psoas muscle measurements are frequently used as markers of sarcopenia and\npredictors of health. Manually measured cross-sectional areas are most commonly\nused, but there is a lack of consistency regarding the position of the\nmeasurementand manual annotations are not practical for large population\nstudies. We have developed a fully automated method to measure iliopsoas muscle\nvolume (comprised of the psoas and iliacus muscles) using a convolutional\nneural network. Magnetic resonance images were obtained from the UK Biobank for\n5,000 male and female participants, balanced for age, gender and BMI. Ninety\nmanual annotations were available for model training and validation. The model\nshowed excellent performance against out-of-sample data (dice score coefficient\nof 0.912 +/- 0.018). Iliopsoas muscle volumes were successfully measured in all\n5,000 participants. Iliopsoas volume was greater in male compared with female\nsubjects. There was a small but significant asymmetry between left and right\niliopsoas muscle volumes. We also found that iliopsoas volume was significantly\nrelated to height, BMI and age, and that there was an acceleration in muscle\nvolume decrease in men with age. Our method provides a robust technique for\nmeasuring iliopsoas muscle volume that can be applied to large cohorts.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 10:28:39 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 11:16:07 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Fitzpatrick", "Julie", ""], ["Basty", "Nicolas", ""], ["Cule", "Madeleine", ""], ["Liu", "Yi", ""], ["Bell", "Jimmy D.", ""], ["Thomas", "E. Louise", ""], ["Whitcher", "Brandon", ""]]}, {"id": "2008.05221", "submitter": "Manish Gupta", "authors": "Manish Gupta, Puneet Agrawal", "title": "Compression of Deep Learning Models for Text: A Survey", "comments": "Accepted at TKDD for publication. 53 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the fields of natural language processing (NLP) and\ninformation retrieval (IR) have made tremendous progress thanksto deep learning\nmodels like Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs) and\nLong Short-Term Memory (LSTMs)networks, and Transformer [120] based models like\nBidirectional Encoder Representations from Transformers (BERT) [24],\nGenerativePre-training Transformer (GPT-2) [94], Multi-task Deep Neural Network\n(MT-DNN) [73], Extra-Long Network (XLNet) [134], Text-to-text transfer\ntransformer (T5) [95], T-NLG [98] and GShard [63]. But these models are\nhumongous in size. On the other hand,real world applications demand small model\nsize, low response times and low computational power wattage. In this survey,\nwediscuss six different types of methods (Pruning, Quantization, Knowledge\nDistillation, Parameter Sharing, Tensor Decomposition, andSub-quadratic\nTransformer based methods) for compression of such models to enable their\ndeployment in real industry NLP projects.Given the critical need of building\napplications with efficient and small models, and the large amount of recently\npublished work inthis area, we believe that this survey organizes the plethora\nof work done by the 'deep learning for NLP' community in the past fewyears and\npresents it as a coherent story.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 10:42:14 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 10:41:02 GMT"}, {"version": "v3", "created": "Sun, 11 Apr 2021 12:25:10 GMT"}, {"version": "v4", "created": "Sun, 13 Jun 2021 17:47:28 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Gupta", "Manish", ""], ["Agrawal", "Puneet", ""]]}, {"id": "2008.05225", "submitter": "Ushasi Chaudhuri", "authors": "Ushasi Chaudhuri, Biplab Banerjee, Avik Bhattacharya, Mihai Datcu", "title": "A Zero-Shot Sketch-based Inter-Modal Object Retrieval Scheme for Remote\n  Sensing Images", "comments": null, "journal-ref": null, "doi": "10.1109/LGRS.2021.3056392", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional existing retrieval methods in remote sensing (RS) are often\nbased on a uni-modal data retrieval framework. In this work, we propose a novel\ninter-modal triplet-based zero-shot retrieval scheme utilizing a sketch-based\nrepresentation of RS data. The proposed scheme performs efficiently even when\nthe sketch representations are marginally prototypical of the image. We\nconducted experiments on a new bi-modal image-sketch dataset called Earth on\nCanvas (EoC) conceived during this study. We perform a thorough bench-marking\nof this dataset and demonstrate that the proposed network outperforms other\nstate-of-the-art methods for zero-shot sketch-based retrieval framework in\nremote sensing.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 10:51:24 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Chaudhuri", "Ushasi", ""], ["Banerjee", "Biplab", ""], ["Bhattacharya", "Avik", ""], ["Datcu", "Mihai", ""]]}, {"id": "2008.05230", "submitter": "Miaojing Shi", "authors": "Wenqing Liu, Miaojing Shi, Teddy Furon, Li Li", "title": "Defending Adversarial Examples via DNN Bottleneck Reinforcement", "comments": "ACM MM 2020 - Full Paper", "journal-ref": null, "doi": "10.1145/3394171.3413604", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a DNN bottleneck reinforcement scheme to alleviate the\nvulnerability of Deep Neural Networks (DNN) against adversarial attacks.\nTypical DNN classifiers encode the input image into a compressed latent\nrepresentation more suitable for inference. This information bottleneck makes a\ntrade-off between the image-specific structure and class-specific information\nin an image. By reinforcing the former while maintaining the latter, any\nredundant information, be it adversarial or not, should be removed from the\nlatent representation. Hence, this paper proposes to jointly train an\nauto-encoder (AE) sharing the same encoding weights with the visual classifier.\nIn order to reinforce the information bottleneck, we introduce the multi-scale\nlow-pass objective and multi-scale high-frequency communication for better\nfrequency steering in the network. Unlike existing approaches, our scheme is\nthe first reforming defense per se which keeps the classifier structure\nuntouched without appending any pre-processing head and is trained with clean\nimages only. Extensive experiments on MNIST, CIFAR-10 and ImageNet demonstrate\nthe strong defense of our method against various adversarial attacks.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 11:02:01 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Liu", "Wenqing", ""], ["Shi", "Miaojing", ""], ["Furon", "Teddy", ""], ["Li", "Li", ""]]}, {"id": "2008.05231", "submitter": "Nicola Messina", "authors": "Nicola Messina, Giuseppe Amato, Andrea Esuli, Fabrizio Falchi, Claudio\n  Gennaro, St\\'ephane Marchand-Maillet", "title": "Fine-grained Visual Textual Alignment for Cross-Modal Retrieval using\n  Transformer Encoders", "comments": "Accepted in ACM Transactions on Multimedia Computing, Communications,\n  and Applications (TOMM). arXiv admin note: text overlap with arXiv:2004.09144", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the evolution of deep-learning-based visual-textual processing\nsystems, precise multi-modal matching remains a challenging task. In this work,\nwe tackle the task of cross-modal retrieval through image-sentence matching\nbased on word-region alignments, using supervision only at the global\nimage-sentence level. Specifically, we present a novel approach called\nTransformer Encoder Reasoning and Alignment Network (TERAN). TERAN enforces a\nfine-grained match between the underlying components of images and sentences,\ni.e., image regions and words, respectively, in order to preserve the\ninformative richness of both modalities. TERAN obtains state-of-the-art results\non the image retrieval task on both MS-COCO and Flickr30k datasets. Moreover,\non MS-COCO, it also outperforms current approaches on the sentence retrieval\ntask.\n  Focusing on scalable cross-modal information retrieval, TERAN is designed to\nkeep the visual and textual data pipelines well separated. Cross-attention\nlinks invalidate any chance to separately extract visual and textual features\nneeded for the online search and the offline indexing steps in large-scale\nretrieval systems. In this respect, TERAN merges the information from the two\ndomains only during the final alignment phase, immediately before the loss\ncomputation. We argue that the fine-grained alignments produced by TERAN pave\nthe way towards the research for effective and efficient methods for\nlarge-scale cross-modal information retrieval. We compare the effectiveness of\nour approach against relevant state-of-the-art methods. On the MS-COCO 1K test\nset, we obtain an improvement of 5.7% and 3.5% respectively on the image and\nthe sentence retrieval tasks on the Recall@1 metric. The code used for the\nexperiments is publicly available on GitHub at\nhttps://github.com/mesnico/TERAN.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 11:02:40 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 16:12:52 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Messina", "Nicola", ""], ["Amato", "Giuseppe", ""], ["Esuli", "Andrea", ""], ["Falchi", "Fabrizio", ""], ["Gennaro", "Claudio", ""], ["Marchand-Maillet", "St\u00e9phane", ""]]}, {"id": "2008.05242", "submitter": "Dong Hwan Kim", "authors": "Myoungha Song, Jeongho Lee, Donghwan Kim", "title": "PAM:Point-wise Attention Module for 6D Object Pose Estimation", "comments": "11 pages, 5figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  6D pose estimation refers to object recognition and estimation of 3D rotation\nand 3D translation. The key technology for estimating 6D pose is to estimate\npose by extracting enough features to find pose in any environment. Previous\nmethods utilized depth information in the refinement process or were designed\nas a heterogeneous architecture for each data space to extract feature.\nHowever, these methods are limited in that they cannot extract sufficient\nfeature. Therefore, this paper proposes a Point Attention Module that can\nefficiently extract powerful feature from RGB-D. In our Module, attention map\nis formed through a Geometric Attention Path(GAP) and Channel Attention\nPath(CAP). In GAP, it is designed to pay attention to important information in\ngeometric information, and CAP is designed to pay attention to important\ninformation in Channel information. We show that the attention module\nefficiently creates feature representations without significantly increasing\ncomputational complexity. Experimental results show that the proposed method\noutperforms the existing methods in benchmarks, YCB Video and LineMod. In\naddition, the attention module was applied to the classification task, and it\nwas confirmed that the performance significantly improved compared to the\nexisting model.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 11:29:48 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Song", "Myoungha", ""], ["Lee", "Jeongho", ""], ["Kim", "Donghwan", ""]]}, {"id": "2008.05247", "submitter": "Alex Serban", "authors": "Alex Serban, Erik Poll, Joost Visser", "title": "Learning to Learn from Mistakes: Robust Optimization for Adversarial\n  Noise", "comments": "Published at ICANN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sensitivity to adversarial noise hinders deployment of machine learning\nalgorithms in security-critical applications. Although many adversarial\ndefenses have been proposed, robustness to adversarial noise remains an open\nproblem. The most compelling defense, adversarial training, requires a\nsubstantial increase in processing time and it has been shown to overfit on the\ntraining data. In this paper, we aim to overcome these limitations by training\nrobust models in low data regimes and transfer adversarial knowledge between\ndifferent models. We train a meta-optimizer which learns to robustly optimize a\nmodel using adversarial examples and is able to transfer the knowledge learned\nto new models, without the need to generate new adversarial examples.\nExperimental results show the meta-optimizer is consistent across different\narchitectures and data sets, suggesting it is possible to automatically patch\nadversarial vulnerabilities.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 11:44:01 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Serban", "Alex", ""], ["Poll", "Erik", ""], ["Visser", "Joost", ""]]}, {"id": "2008.05255", "submitter": "Jiangkai Wu", "authors": "Zichuan Xu, Jiangkai Wu, Qiufen Xia, Pan Zhou, Jiankang Ren, Huizhi\n  Liang", "title": "Identity-Aware Attribute Recognition via Real-Time Distributed Inference\n  in Mobile Edge Clouds", "comments": "9 pages, 8 figures, Proceedings of the 28th ACM International\n  Conference on Multimedia (ACM MM'20), Seattle, WA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of deep learning technologies, attribute recognition and\nperson re-identification (re-ID) have attracted extensive attention and\nachieved continuous improvement via executing computing-intensive deep neural\nnetworks in cloud datacenters. However, the datacenter deployment cannot meet\nthe real-time requirement of attribute recognition and person re-ID, due to the\nprohibitive delay of backhaul networks and large data transmissions from\ncameras to datacenters. A feasible solution thus is to employ mobile edge\nclouds (MEC) within the proximity of cameras and enable distributed inference.\nIn this paper, we design novel models for pedestrian attribute recognition with\nre-ID in an MEC-enabled camera monitoring system. We also investigate the\nproblem of distributed inference in the MEC-enabled camera network. To this\nend, we first propose a novel inference framework with a set of distributed\nmodules, by jointly considering the attribute recognition and person re-ID. We\nthen devise a learning-based algorithm for the distributions of the modules of\nthe proposed distributed inference framework, considering the dynamic\nMEC-enabled camera network with uncertainties. We finally evaluate the\nperformance of the proposed algorithm by both simulations with real datasets\nand system implementation in a real testbed. Evaluation results show that the\nperformance of the proposed algorithm with distributed inference framework is\npromising, by reaching the accuracies of attribute recognition and person\nidentification up to 92.9% and 96.6% respectively, and significantly reducing\nthe inference delay by at least 40.6% compared with existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 12:03:27 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Xu", "Zichuan", ""], ["Wu", "Jiangkai", ""], ["Xia", "Qiufen", ""], ["Zhou", "Pan", ""], ["Ren", "Jiankang", ""], ["Liang", "Huizhi", ""]]}, {"id": "2008.05258", "submitter": "Zhanghan Ke", "authors": "Zhanghan Ke, Di Qiu, Kaican Li, Qiong Yan, Rynson W.H. Lau", "title": "Guided Collaborative Training for Pixel-wise Semi-Supervised Learning", "comments": "16th European Conference on Computer Vision (ECCV 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the generalization of semi-supervised learning (SSL) to\ndiverse pixel-wise tasks. Although SSL methods have achieved impressive results\nin image classification, the performances of applying them to pixel-wise tasks\nare unsatisfactory due to their need for dense outputs. In addition, existing\npixel-wise SSL approaches are only suitable for certain tasks as they usually\nrequire to use task-specific properties. In this paper, we present a new SSL\nframework, named Guided Collaborative Training (GCT), for pixel-wise tasks,\nwith two main technical contributions. First, GCT addresses the issues caused\nby the dense outputs through a novel flaw detector. Second, the modules in GCT\nlearn from unlabeled data collaboratively through two newly proposed\nconstraints that are independent of task-specific properties. As a result, GCT\ncan be applied to a wide range of pixel-wise tasks without structural\nadaptation. Our extensive experiments on four challenging vision tasks,\nincluding semantic segmentation, real image denoising, portrait image matting,\nand night image enhancement, show that GCT outperforms state-of-the-art SSL\nmethods by a large margin. Our code available at:\nhttps://github.com/ZHKKKe/PixelSSL.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 12:08:25 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Ke", "Zhanghan", ""], ["Qiu", "Di", ""], ["Li", "Kaican", ""], ["Yan", "Qiong", ""], ["Lau", "Rynson W. H.", ""]]}, {"id": "2008.05309", "submitter": "Johannes P\\\"oschmann", "authors": "Johannes P\\\"oschmann, Tim Pfeifer and Peter Protzel", "title": "Factor Graph based 3D Multi-Object Tracking in Point Clouds", "comments": "8 pages, 4 figures, accepted by IEEE Intl. Conf. on Intelligent\n  Robots and Systems (IROS) 2020, visualization of the results of our offline\n  tracker available at https://www.youtube.com/watch?v=mvZmli4jrZQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and reliable tracking of multiple moving objects in 3D space is an\nessential component of urban scene understanding. This is a challenging task\nbecause it requires the assignment of detections in the current frame to the\npredicted objects from the previous one. Existing filter-based approaches tend\nto struggle if this initial assignment is not correct, which can happen easily.\nWe propose a novel optimization-based approach that does not rely on explicit\nand fixed assignments. Instead, we represent the result of an off-the-shelf 3D\nobject detector as Gaussian mixture model, which is incorporated in a factor\ngraph framework. This gives us the flexibility to assign all detections to all\nobjects simultaneously. As a result, the assignment problem is solved\nimplicitly and jointly with the 3D spatial multi-object state estimation using\nnon-linear least squares optimization. Despite its simplicity, the proposed\nalgorithm achieves robust and reliable tracking results and can be applied for\noffline as well as online tracking. We demonstrate its performance on the real\nworld KITTI tracking dataset and achieve better results than many\nstate-of-the-art algorithms. Especially the consistency of the estimated tracks\nis superior offline as well as online.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 13:34:46 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["P\u00f6schmann", "Johannes", ""], ["Pfeifer", "Tim", ""], ["Protzel", "Peter", ""]]}, {"id": "2008.05314", "submitter": "Yibo Hu", "authors": "Yibo Hu, Xiang Wu, Ran He", "title": "TF-NAS: Rethinking Three Search Freedoms of Latency-Constrained\n  Differentiable Neural Architecture Search", "comments": "Accepted by ECCV2020. Code is available at\n  https://github.com/AberHu/TF-NAS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the flourish of differentiable neural architecture search (NAS),\nautomatically searching latency-constrained architectures gives a new\nperspective to reduce human labor and expertise. However, the searched\narchitectures are usually suboptimal in accuracy and may have large jitters\naround the target latency. In this paper, we rethink three freedoms of\ndifferentiable NAS, i.e. operation-level, depth-level and width-level, and\npropose a novel method, named Three-Freedom NAS (TF-NAS), to achieve both good\nclassification accuracy and precise latency constraint. For the\noperation-level, we present a bi-sampling search algorithm to moderate the\noperation collapse. For the depth-level, we introduce a sink-connecting search\nspace to ensure the mutual exclusion between skip and other candidate\noperations, as well as eliminate the architecture redundancy. For the\nwidth-level, we propose an elasticity-scaling strategy that achieves precise\nlatency constraint in a progressively fine-grained manner. Experiments on\nImageNet demonstrate the effectiveness of TF-NAS. Particularly, our searched\nTF-NAS-A obtains 76.9% top-1 accuracy, achieving state-of-the-art results with\nless latency. The total search time is only 1.8 days on 1 Titan RTX GPU. Code\nis available at https://github.com/AberHu/TF-NAS.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 13:44:20 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Hu", "Yibo", ""], ["Wu", "Xiang", ""], ["He", "Ran", ""]]}, {"id": "2008.05332", "submitter": "Pargorn Puttapirat", "authors": "Zeyu Gao, Pargorn Puttapirat, Jiangbo Shi, Chen Li", "title": "Renal Cell Carcinoma Detection and Subtyping with Minimal Point-Based\n  Annotation in Whole-Slide Images", "comments": "10 pages, 5 figure, 3 tables, accepted at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Obtaining a large amount of labeled data in medical imaging is laborious and\ntime-consuming, especially for histopathology. However, it is much easier and\ncheaper to get unlabeled data from whole-slide images (WSIs). Semi-supervised\nlearning (SSL) is an effective way to utilize unlabeled data and alleviate the\nneed for labeled data. For this reason, we proposed a framework that employs an\nSSL method to accurately detect cancerous regions with a novel annotation\nmethod called Minimal Point-Based annotation, and then utilize the predicted\nresults with an innovative hybrid loss to train a classification model for\nsubtyping. The annotator only needs to mark a few points and label them are\ncancer or not in each WSI. Experiments on three significant subtypes of renal\ncell carcinoma (RCC) proved that the performance of the classifier trained with\nthe Min-Point annotated dataset is comparable to a classifier trained with the\nsegmentation annotated dataset for cancer region detection. And the subtyping\nmodel outperforms a model trained with only diagnostic labels by 12% in terms\nof f1-score for testing WSIs.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 14:12:07 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Gao", "Zeyu", ""], ["Puttapirat", "Pargorn", ""], ["Shi", "Jiangbo", ""], ["Li", "Chen", ""]]}, {"id": "2008.05336", "submitter": "Paul Rosin", "authors": "Paul L. Rosin and Yu-Kun Lai", "title": "Image-based Portrait Engraving", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a simple image-based method that applies engraving\nstylisation to portraits using ordered dithering. Face detection is used to\nestimate a rough proxy geometry of the head consisting of a cylinder, which is\nused to warp the dither matrix, causing the engraving lines to curve around the\nface for better stylisation. Finally, an application of the approach to colour\nengraving is demonstrated.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 14:13:53 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Rosin", "Paul L.", ""], ["Lai", "Yu-Kun", ""]]}, {"id": "2008.05359", "submitter": "Weiqing Min", "authors": "Jing Wang, Weiqing Min, Sujuan Hou, Shengnan Ma, Yuanjie Zheng,\n  Shuqiang Jiang", "title": "LogoDet-3K: A Large-Scale Image Dataset for Logo Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Logo detection has been gaining considerable attention because of its wide\nrange of applications in the multimedia field, such as copyright infringement\ndetection, brand visibility monitoring, and product brand management on social\nmedia. In this paper, we introduce LogoDet-3K, the largest logo detection\ndataset with full annotation, which has 3,000 logo categories, about 200,000\nmanually annotated logo objects and 158,652 images. LogoDet-3K creates a more\nchallenging benchmark for logo detection, for its higher comprehensive coverage\nand wider variety in both logo categories and annotated objects compared with\nexisting datasets. We describe the collection and annotation process of our\ndataset, analyze its scale and diversity in comparison to other datasets for\nlogo detection. We further propose a strong baseline method Logo-Yolo, which\nincorporates Focal loss and CIoU loss into the state-of-the-art YOLOv3\nframework for large-scale logo detection. Logo-Yolo can solve the problems of\nmulti-scale objects, logo sample imbalance and inconsistent bounding-box\nregression. It obtains about 4% improvement on the average performance compared\nwith YOLOv3, and greater improvements compared with reported several deep\ndetection models on LogoDet-3K. The evaluations on other three existing\ndatasets further verify the effectiveness of our method, and demonstrate better\ngeneralization ability of LogoDet-3K on logo detection and retrieval tasks. The\nLogoDet-3K dataset is used to promote large-scale logo-related research and it\ncan be found at https://github.com/Wangjing1551/LogoDet-3K-Dataset.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 14:57:53 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Wang", "Jing", ""], ["Min", "Weiqing", ""], ["Hou", "Sujuan", ""], ["Ma", "Shengnan", ""], ["Zheng", "Yuanjie", ""], ["Jiang", "Shuqiang", ""]]}, {"id": "2008.05369", "submitter": "David Dehaene", "authors": "David Dehaene, Pierre Eline", "title": "Anomaly localization by modeling perceptual features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although unsupervised generative modeling of an image dataset using a\nVariational AutoEncoder (VAE) has been used to detect anomalous images, or\nanomalous regions in images, recent works have shown that this method often\nidentifies images or regions that do not concur with human perception, even\nquestioning the usability of generative models for robust anomaly detection.\nHere, we argue that those issues can emerge from having a simplistic model of\nthe anomaly distribution and we propose a new VAE-based model expressing a more\ncomplex anomaly model that is also closer to human perception. This\nFeature-Augmented VAE is trained by not only reconstructing the input image in\npixel space, but also in several different feature spaces, which are computed\nby a convolutional neural network trained beforehand on a large image dataset.\nIt achieves clear improvement over state-of-the-art methods on the MVTec\nanomaly detection and localization datasets.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 15:09:13 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Dehaene", "David", ""], ["Eline", "Pierre", ""]]}, {"id": "2008.05373", "submitter": "Abdelrahman Abdallah", "authors": "Abdelrahman Abdallah, Mohamed Hamada and Daniyar Nurseitov", "title": "Attention-based Fully Gated CNN-BGRU for Russian Handwritten Text", "comments": null, "journal-ref": "J. Imaging 2020, 6, 141", "doi": "10.3390/jimaging6120141", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research approaches the task of handwritten text with attention\nencoder-decoder networks that are trained on Kazakh and Russian language. We\ndeveloped a novel deep neural network model based on Fully Gated CNN, supported\nby Multiple bidirectional GRU and Attention mechanisms to manipulate\nsophisticated features that achieve 0.045 Character Error Rate (CER), 0.192\nWord Error Rate (WER) and 0.253 Sequence Error Rate (SER) for the first test\ndataset and 0.064 CER, 0.24 WER and 0.361 SER for the second test dataset.\nAlso, we propose fully gated layers by taking the advantage of multiple the\noutput feature from Tahn and input feature, this proposed work achieves better\nresults and We experimented with our model on the Handwritten Kazakh & Russian\nDatabase (HKR). Our research is the first work on the HKR dataset and\ndemonstrates state-of-the-art results to most of the other existing models.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 15:14:47 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 09:54:20 GMT"}, {"version": "v3", "created": "Sat, 15 Aug 2020 18:54:49 GMT"}, {"version": "v4", "created": "Tue, 18 Aug 2020 10:10:31 GMT"}, {"version": "v5", "created": "Thu, 20 Aug 2020 13:59:37 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Abdallah", "Abdelrahman", ""], ["Hamada", "Mohamed", ""], ["Nurseitov", "Daniyar", ""]]}, {"id": "2008.05381", "submitter": "Jeff Druce", "authors": "Shashank Manjunath, Aitzaz Nathaniel, Jeff Druce, Stan German", "title": "Improving the Performance of Fine-Grain Image Classifiers via Generative\n  Data Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in machine learning (ML) and computer vision tools have\nenabled applications in a wide variety of arenas such as financial analytics,\nmedical diagnostics, and even within the Department of Defense. However, their\nwidespread implementation in real-world use cases poses several challenges: (1)\nmany applications are highly specialized, and hence operate in a \\emph{sparse\ndata} domain; (2) ML tools are sensitive to their training sets and typically\nrequire cumbersome, labor-intensive data collection and data labelling\nprocesses; and (3) ML tools can be extremely \"black box,\" offering users little\nto no insight into the decision-making process or how new data might affect\nprediction performance. To address these challenges, we have designed and\ndeveloped Data Augmentation from Proficient Pre-Training of Robust Generative\nAdversarial Networks (DAPPER GAN), an ML analytics support tool that\nautomatically generates novel views of training images in order to improve\ndownstream classifier performance. DAPPER GAN leverages high-fidelity\nembeddings generated by a StyleGAN2 model (trained on the LSUN cars dataset) to\ncreate novel imagery for previously unseen classes. We experimentally evaluate\nthis technique on the Stanford Cars dataset, demonstrating improved vehicle\nmake and model classification accuracy and reduced requirements for real data\nusing our GAN based data augmentation framework. The method's validity was\nsupported through an analysis of classifier performance on both augmented and\nnon-augmented datasets, achieving comparable or better accuracy with up to 30\\%\nless real data across visually similar classes. To support this method, we\ndeveloped a novel augmentation method that can manipulate semantically\nmeaningful dimensions (e.g., orientation) of the target object in the embedding\nspace.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 15:29:11 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Manjunath", "Shashank", ""], ["Nathaniel", "Aitzaz", ""], ["Druce", "Jeff", ""], ["German", "Stan", ""]]}, {"id": "2008.05383", "submitter": "Yuting Liu", "authors": "Yuting Liu, Zheng Wang, Miaojing Shi, Shin'ichi Satoh, Qijun Zhao,\n  Hongyu Yang", "title": "Towards Unsupervised Crowd Counting via Regression-Detection\n  Bi-knowledge Transfer", "comments": "This paper has been accepted by ACM MM 2020(Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised crowd counting is a challenging yet not largely explored task.\nIn this paper, we explore it in a transfer learning setting where we learn to\ndetect and count persons in an unlabeled target set by transferring\nbi-knowledge learnt from regression- and detection-based models in a labeled\nsource set. The dual source knowledge of the two models is heterogeneous and\ncomplementary as they capture different modalities of the crowd distribution.\nWe formulate the mutual transformations between the outputs of regression- and\ndetection-based models as two scene-agnostic transformers which enable\nknowledge distillation between the two models. Given the regression- and\ndetection-based models and their mutual transformers learnt in the source, we\nintroduce an iterative self-supervised learning scheme with\nregression-detection bi-knowledge transfer in the target. Extensive experiments\non standard crowd counting benchmarks, ShanghaiTech, UCF\\_CC\\_50, and UCF\\_QNRF\ndemonstrate a substantial improvement of our method over other\nstate-of-the-arts in the transfer learning setting.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 15:29:30 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 14:36:33 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Liu", "Yuting", ""], ["Wang", "Zheng", ""], ["Shi", "Miaojing", ""], ["Satoh", "Shin'ichi", ""], ["Zhao", "Qijun", ""], ["Yang", "Hongyu", ""]]}, {"id": "2008.05396", "submitter": "Chenglizhao Chen", "authors": "Chenglizhao Chen, Hongmeng Zhao, Huan Yang, Chong Peng, Teng Yu", "title": "Full Reference Screen Content Image Quality Assessment by Fusing\n  Multi-level Structure Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The screen content images (SCIs) usually comprise various content types with\nsharp edges, in which the artifacts or distortions can be well sensed by the\nvanilla structure similarity measurement in a full reference manner.\nNonetheless, almost all of the current SOTA structure similarity metrics are\n\"locally\" formulated in a single-level manner, while the true human visual\nsystem (HVS) follows the multi-level manner, and such mismatch could eventually\nprevent these metrics from achieving trustworthy quality assessment. To\nameliorate, this paper advocates a novel solution to measure structure\nsimilarity \"globally\" from the perspective of sparse representation. To perform\nmulti-level quality assessment in accordance with the real HVS, the\nabove-mentioned global metric will be integrated with the conventional local\nones by resorting to the newly devised selective deep fusion network. To\nvalidate its efficacy and effectiveness, we have compared our method with 12\nSOTA methods over two widely-used large-scale public SCI datasets, and the\nquantitative results indicate that our method yields significantly higher\nconsistency with subjective quality score than the currently leading works.\nBoth the source code and data are also publicly available to gain widespread\nacceptance and facilitate new advancement and its validation.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 10:20:25 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Chen", "Chenglizhao", ""], ["Zhao", "Hongmeng", ""], ["Yang", "Huan", ""], ["Peng", "Chong", ""], ["Yu", "Teng", ""]]}, {"id": "2008.05397", "submitter": "Chenglizhao Chen", "authors": "Zhenyu Wu, Shuai Li, Chenglizhao Chen, Aimin Hao, Hong Qin", "title": "Rethinking of the Image Salient Object Detection: Object-level Semantic\n  Saliency Re-ranking First, Pixel-wise Saliency Refinement Latter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The real human attention is an interactive activity between our visual system\nand our brain, using both low-level visual stimulus and high-level semantic\ninformation. Previous image salient object detection (SOD) works conduct their\nsaliency predictions in a multi-task manner, i.e., performing pixel-wise\nsaliency regression and segmentation-like saliency refinement at the same time,\nwhich degenerates their feature backbones in revealing semantic information.\nHowever, given an image, we tend to pay more attention to those regions which\nare semantically salient even in the case that these regions are perceptually\nnot the most salient ones at first glance. In this paper, we divide the SOD\nproblem into two sequential tasks: 1) we propose a lightweight, weakly\nsupervised deep network to coarsely locate those semantically salient regions\nfirst; 2) then, as a post-processing procedure, we selectively fuse multiple\noff-the-shelf deep models on these semantically salient regions as the\npixel-wise saliency refinement. In sharp contrast to the state-of-the-art\n(SOTA) methods that focus on learning pixel-wise saliency in \"single image\"\nusing perceptual clues mainly, our method has investigated the \"object-level\nsemantic ranks between multiple images\", of which the methodology is more\nconsistent with the real human attention mechanism. Our method is simple yet\neffective, which is the first attempt to consider the salient object detection\nmainly as an object-level semantic re-ranking problem.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 07:12:43 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Wu", "Zhenyu", ""], ["Li", "Shuai", ""], ["Chen", "Chenglizhao", ""], ["Hao", "Aimin", ""], ["Qin", "Hong", ""]]}, {"id": "2008.05402", "submitter": "Mourad A. Kenk", "authors": "Mourad A. Kenk, Mahmoud Hassaballah", "title": "DAWN: Vehicle Detection in Adverse Weather Nature Dataset", "comments": "Available at https://data.mendeley.com/datasets/766ygrbt8y/3 ,IEEE\n  Transactions on Intelligent Transportation Systems", "journal-ref": null, "doi": "10.17632/766ygrbt8y.3", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, self-driving vehicles have been introduced with several automated\nfeatures including lane-keep assistance, queuing assistance in traffic-jam,\nparking assistance and crash avoidance. These self-driving vehicles and\nintelligent visual traffic surveillance systems mainly depend on cameras and\nsensors fusion systems. Adverse weather conditions such as heavy fog, rain,\nsnow, and sandstorms are considered dangerous restrictions of the functionality\nof cameras impacting seriously the performance of adopted computer vision\nalgorithms for scene understanding (i.e., vehicle detection, tracking, and\nrecognition in traffic scenes). For example, reflection coming from rain flow\nand ice over roads could cause massive detection errors which will affect the\nperformance of intelligent visual traffic systems. Additionally, scene\nunderstanding and vehicle detection algorithms are mostly evaluated using\ndatasets contain certain types of synthetic images plus a few real-world\nimages. Thus, it is uncertain how these algorithms would perform on unclear\nimages acquired in the wild and how the progress of these algorithms is\nstandardized in the field. To this end, we present a new dataset (benchmark)\nconsisting of real-world images collected under various adverse weather\nconditions called DAWN. This dataset emphasizes a diverse traffic environment\n(urban, highway and freeway) as well as a rich variety of traffic flow. The\nDAWN dataset comprises a collection of 1000 images from real-traffic\nenvironments, which are divided into four sets of weather conditions: fog,\nsnow, rain and sandstorms. The dataset is annotated with object bounding boxes\nfor autonomous driving and video surveillance scenarios. This data helps\ninterpreting effects caused by the adverse weather conditions on the\nperformance of vehicle detection systems.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 15:48:49 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Kenk", "Mourad A.", ""], ["Hassaballah", "Mahmoud", ""]]}, {"id": "2008.05413", "submitter": "Youssef Alami Mejjati", "authors": "Youssef Alami Mejjati and Celso F. Gomez and Kwang In Kim and Eli\n  Shechtman and Zoya Bylinskii", "title": "Look here! A parametric learning based approach to redirect visual\n  attention", "comments": "To appear in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Across photography, marketing, and website design, being able to direct the\nviewer's attention is a powerful tool. Motivated by professional workflows, we\nintroduce an automatic method to make an image region more attention-capturing\nvia subtle image edits that maintain realism and fidelity to the original. From\nan input image and a user-provided mask, our GazeShiftNet model predicts a\ndistinct set of global parametric transformations to be applied to the\nforeground and background image regions separately. We present the results of\nquantitative and qualitative experiments that demonstrate improvements over\nprior state-of-the-art. In contrast to existing attention shifting algorithms,\nour global parametric approach better preserves image semantics and avoids\ntypical generative artifacts. Our edits enable inference at interactive rates\non any image size, and easily generalize to videos. Extensions of our model\nallow for multi-style edits and the ability to both increase and attenuate\nattention in an image region. Furthermore, users can customize the edited\nimages by dialing the edits up or down via interpolations in parameter space.\nThis paper presents a practical tool that can simplify future image editing\npipelines.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 16:08:36 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Mejjati", "Youssef Alami", ""], ["Gomez", "Celso F.", ""], ["Kim", "Kwang In", ""], ["Shechtman", "Eli", ""], ["Bylinskii", "Zoya", ""]]}, {"id": "2008.05416", "submitter": "Xuesong Shi", "authors": "Dongjiang Li, Xuesong Shi, Qiwei Long, Shenghui Liu, Wei Yang, Fangshi\n  Wang, Qi Wei, Fei Qiao", "title": "DXSLAM: A Robust and Efficient Visual SLAM System with Deep Features", "comments": "8 pages, 5 figures, to be published in IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robust and efficient Simultaneous Localization and Mapping (SLAM) system is\nessential for robot autonomy. For visual SLAM algorithms, though the\ntheoretical framework has been well established for most aspects, feature\nextraction and association is still empirically designed in most cases, and can\nbe vulnerable in complex environments. This paper shows that feature extraction\nwith deep convolutional neural networks (CNNs) can be seamlessly incorporated\ninto a modern SLAM framework. The proposed SLAM system utilizes a\nstate-of-the-art CNN to detect keypoints in each image frame, and to give not\nonly keypoint descriptors, but also a global descriptor of the whole image.\nThese local and global features are then used by different SLAM modules,\nresulting in much more robustness against environmental changes and viewpoint\nchanges compared with using hand-crafted features. We also train a visual\nvocabulary of local features with a Bag of Words (BoW) method. Based on the\nlocal features, global features, and the vocabulary, a highly reliable loop\nclosure detection method is built. Experimental results show that all the\nproposed modules significantly outperforms the baseline, and the full system\nachieves much lower trajectory errors and much higher correct rates on all\nevaluated data. Furthermore, by optimizing the CNN with Intel OpenVINO toolkit\nand utilizing the Fast BoW library, the system benefits greatly from the SIMD\n(single-instruction-multiple-data) techniques in modern CPUs. The full system\ncan run in real-time without any GPU or other accelerators. The code is public\nat https://github.com/ivipsourcecode/dxslam.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 16:14:46 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Li", "Dongjiang", ""], ["Shi", "Xuesong", ""], ["Long", "Qiwei", ""], ["Liu", "Shenghui", ""], ["Yang", "Wei", ""], ["Wang", "Fangshi", ""], ["Wei", "Qi", ""], ["Qiao", "Fei", ""]]}, {"id": "2008.05440", "submitter": "Jie Yang", "authors": "Jie Yang, Kaichun Mo, Yu-Kun Lai, Leonidas J. Guibas, Lin Gao", "title": "DSG-Net: Learning Disentangled Structure and Geometry for 3D Shape\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  D shape generation is a fundamental operation in computer graphics. While\nsignificant progress has been made, especially with recent deep generative\nmodels, it remains a challenge to synthesize high-quality shapes with rich\ngeometric details and complex structure, in a controllable manner. To tackle\nthis, we introduce DSG-Net, a deep neural network that learns a disentangled\nstructured and geometric mesh representation for 3D shapes, where two key\naspects of shapes, geometry, and structure, are encoded in a synergistic manner\nto ensure plausibility of the generated shapes, while also being disentangled\nas much as possible. This supports a range of novel shape generation\napplications with disentangled control, such as interpolation of structure\n(geometry) while keeping geometry (structure) unchanged. To achieve this, we\nsimultaneously learn structure and geometry through variational autoencoders\n(VAEs) in a hierarchical manner for both, with bijective mappings at each\nlevel. In this manner, we effectively encode geometry and structure in separate\nlatent spaces, while ensuring their compatibility: the structure is used to\nguide the geometry and vice versa. At the leaf level, the part geometry is\nrepresented using a conditional part VAE, to encode high-quality geometric\ndetails, guided by the structure context as the condition. Our method not only\nsupports controllable generation applications but also produces high-quality\nsynthesized shapes, outperforming state-of-the-art methods. The code has been\nreleased at https://github.com/IGLICT/DSG-Net.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 17:06:51 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 02:38:45 GMT"}, {"version": "v3", "created": "Mon, 24 May 2021 14:45:26 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Yang", "Jie", ""], ["Mo", "Kaichun", ""], ["Lai", "Yu-Kun", ""], ["Guibas", "Leonidas J.", ""], ["Gao", "Lin", ""]]}, {"id": "2008.05441", "submitter": "Konstantin Sobolev", "authors": "Anh-Huy Phan, Konstantin Sobolev, Konstantin Sozykin, Dmitry Ermilov,\n  Julia Gusak, Petr Tichavsky, Valeriy Glukhov, Ivan Oseledets, and Andrzej\n  Cichocki", "title": "Stable Low-rank Tensor Decomposition for Compression of Convolutional\n  Neural Network", "comments": "This paper is accepted to ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state of the art deep neural networks are overparameterized and exhibit\na high computational cost. A straightforward approach to this problem is to\nreplace convolutional kernels with its low-rank tensor approximations, whereas\nthe Canonical Polyadic tensor Decomposition is one of the most suited models.\nHowever, fitting the convolutional tensors by numerical optimization algorithms\noften encounters diverging components, i.e., extremely large rank-one tensors\nbut canceling each other. Such degeneracy often causes the non-interpretable\nresult and numerical instability for the neural network fine-tuning. This paper\nis the first study on degeneracy in the tensor decomposition of convolutional\nkernels. We present a novel method, which can stabilize the low-rank\napproximation of convolutional kernels and ensure efficient compression while\npreserving the high-quality performance of the neural networks. We evaluate our\napproach on popular CNN architectures for image classification and show that\nour method results in much lower accuracy degradation and provides consistent\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 17:10:12 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Phan", "Anh-Huy", ""], ["Sobolev", "Konstantin", ""], ["Sozykin", "Konstantin", ""], ["Ermilov", "Dmitry", ""], ["Gusak", "Julia", ""], ["Tichavsky", "Petr", ""], ["Glukhov", "Valeriy", ""], ["Oseledets", "Ivan", ""], ["Cichocki", "Andrzej", ""]]}, {"id": "2008.05457", "submitter": "Danfeng Hong", "authors": "Danfeng Hong and Lianru Gao and Naoto Yokoya and Jing Yao and Jocelyn\n  Chanussot and Qian Du and Bing Zhang", "title": "More Diverse Means Better: Multimodal Deep Learning Meets Remote Sensing\n  Imagery Classification", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, 2020", "doi": "10.1109/TGRS.2020.3016820", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification and identification of the materials lying over or beneath the\nEarth's surface have long been a fundamental but challenging research topic in\ngeoscience and remote sensing (RS) and have garnered a growing concern owing to\nthe recent advancements of deep learning techniques. Although deep networks\nhave been successfully applied in single-modality-dominated classification\ntasks, yet their performance inevitably meets the bottleneck in complex scenes\nthat need to be finely classified, due to the limitation of information\ndiversity. In this work, we provide a baseline solution to the aforementioned\ndifficulty by developing a general multimodal deep learning (MDL) framework. In\nparticular, we also investigate a special case of multi-modality learning (MML)\n-- cross-modality learning (CML) that exists widely in RS image classification\napplications. By focusing on \"what\", \"where\", and \"how\" to fuse, we show\ndifferent fusion strategies as well as how to train deep networks and build the\nnetwork architecture. Specifically, five fusion architectures are introduced\nand developed, further being unified in our MDL framework. More significantly,\nour framework is not only limited to pixel-wise classification tasks but also\napplicable to spatial information modeling with convolutional neural networks\n(CNNs). To validate the effectiveness and superiority of the MDL framework,\nextensive experiments related to the settings of MML and CML are conducted on\ntwo different multimodal RS datasets. Furthermore, the codes and datasets will\nbe available at https://github.com/danfenghong/IEEE_TGRS_MDL-RS, contributing\nto the RS community.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 17:45:25 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Hong", "Danfeng", ""], ["Gao", "Lianru", ""], ["Yokoya", "Naoto", ""], ["Yao", "Jing", ""], ["Chanussot", "Jocelyn", ""], ["Du", "Qian", ""], ["Zhang", "Bing", ""]]}, {"id": "2008.05503", "submitter": "Zeeshan Ahmad", "authors": "Zeeshan Ahmad and Naimul Khan", "title": "Multi-level Stress Assessment Using Multi-domain Fusion of ECG Signal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stress analysis and assessment of affective states of mind using ECG as a\nphysiological signal is a burning research topic in biomedical signal\nprocessing. However, existing literature provides only binary assessment of\nstress, while multiple levels of assessment may be more beneficial for\nhealthcare applications. Furthermore, in present research, ECG signal for\nstress analysis is examined independently in spatial domain or in transform\ndomains but the advantage of fusing these domains has not been fully utilized.\nTo get the maximum advantage of fusing diferent domains, we introduce a dataset\nwith multiple stress levels and then classify these levels using a novel deep\nlearning approach by converting ECG signal into signal images based on R-R\npeaks without any feature extraction. Moreover, We made signal images\nmultimodal and multidomain by converting them into time-frequency and frequency\ndomain using Gabor wavelet transform (GWT) and Discrete Fourier Transform (DFT)\nrespectively. Convolutional Neural networks (CNNs) are used to extract features\nfrom different modalities and then decision level fusion is performed for\nimproving the classification accuracy. The experimental results on an in-house\ndataset collected with 15 users show that with proposed fusion framework and\nusing ECG signal to image conversion, we reach an average accuracy of 85.45%.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 18:08:35 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Ahmad", "Zeeshan", ""], ["Khan", "Naimul", ""]]}, {"id": "2008.05511", "submitter": "Gernot Riegler", "authors": "Gernot Riegler, Vladlen Koltun", "title": "Free View Synthesis", "comments": "published at ECCV 2020, https://youtu.be/JDJPn3ZtfZs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for novel view synthesis from input images that are\nfreely distributed around a scene. Our method does not rely on a regular\narrangement of input views, can synthesize images for free camera movement\nthrough the scene, and works for general scenes with unconstrained geometric\nlayouts. We calibrate the input images via SfM and erect a coarse geometric\nscaffold via MVS. This scaffold is used to create a proxy depth map for a novel\nview of the scene. Based on this depth map, a recurrent encoder-decoder network\nprocesses reprojected features from nearby views and synthesizes the new view.\nOur network does not need to be optimized for a given scene. After training on\na dataset, it works in previously unseen environments with no fine-tuning or\nper-scene optimization. We evaluate the presented approach on challenging\nreal-world datasets, including Tanks and Temples, where we demonstrate\nsuccessful view synthesis for the first time and substantially outperform prior\nand concurrent work.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 18:16:08 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Riegler", "Gernot", ""], ["Koltun", "Vladlen", ""]]}, {"id": "2008.05524", "submitter": "Abir Das", "authors": "Aadarsh Sahoo, Ankit Singh, Rameswar Panda, Rogerio Feris, Abir Das", "title": "Mitigating Dataset Imbalance via Joint Generation and Classification", "comments": "Accepted in ECCV2020 Workshop on Imbalance Problems in Computer\n  Vision (IPCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised deep learning methods are enjoying enormous success in many\npractical applications of computer vision and have the potential to\nrevolutionize robotics. However, the marked performance degradation to biases\nand imbalanced data questions the reliability of these methods. In this work we\naddress these questions from the perspective of dataset imbalance resulting out\nof severe under-representation of annotated training data for certain classes\nand its effect on both deep classification and generation methods. We introduce\na joint dataset repairment strategy by combining a neural network classifier\nwith Generative Adversarial Networks (GAN) that makes up for the deficit of\ntraining examples from the under-representated class by producing additional\ntraining examples. We show that the combined training helps to improve the\nrobustness of both the classifier and the GAN against severe class imbalance.\nWe show the effectiveness of our proposed approach on three very different\ndatasets with different degrees of imbalance in them. The code is available at\nhttps://github.com/AadSah/ImbalanceCycleGAN .\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 18:40:38 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Sahoo", "Aadarsh", ""], ["Singh", "Ankit", ""], ["Panda", "Rameswar", ""], ["Feris", "Rogerio", ""], ["Das", "Abir", ""]]}, {"id": "2008.05534", "submitter": "Gabriel Villalonga", "authors": "Gabriel Villalonga and Antonio M. Lopez", "title": "Co-training for On-board Deep Object Detection", "comments": null, "journal-ref": "IEEE Access 8 (2020), 194441-194456", "doi": "10.1109/ACCESS.2020.3032024", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing ground truth supervision to train visual models has been a\nbottleneck over the years, exacerbated by domain shifts which degenerate the\nperformance of such models. This was the case when visual tasks relied on\nhandcrafted features and shallow machine learning and, despite its\nunprecedented performance gains, the problem remains open within the deep\nlearning paradigm due to its data-hungry nature. Best performing deep\nvision-based object detectors are trained in a supervised manner by relying on\nhuman-labeled bounding boxes which localize class instances (i.e.objects)\nwithin the training images.Thus, object detection is one of such tasks for\nwhich human labeling is a major bottleneck. In this paper, we assess\nco-training as a semi-supervised learning method for self-labeling objects in\nunlabeled images, so reducing the human-labeling effort for developing deep\nobject detectors. Our study pays special attention to a scenario involving\ndomain shift; in particular, when we have automatically generated virtual-world\nimages with object bounding boxes and we have real-world images which are\nunlabeled. Moreover, we are particularly interested in using co-training for\ndeep object detection in the context of driver assistance systems and/or\nself-driving vehicles. Thus, using well-established datasets and protocols for\nobject detection in these application contexts, we will show how co-training is\na paradigm worth to pursue for alleviating object labeling, working both alone\nand together with task-agnostic domain adaptation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 19:08:59 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Villalonga", "Gabriel", ""], ["Lopez", "Antonio M.", ""]]}, {"id": "2008.05557", "submitter": "Abdelrahman Elskhawy", "authors": "Abdelrahman Elskhawy, Aneta Lisowska, Matthias Keicher, Josep Henry,\n  Paul Thomson, Nassir Navab", "title": "Continual Class Incremental Learning for CT Thoracic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning organ segmentation approaches require large amounts of\nannotated training data, which is limited in supply due to reasons of\nconfidentiality and the time required for expert manual annotation. Therefore,\nbeing able to train models incrementally without having access to previously\nused data is desirable. A common form of sequential training is fine tuning\n(FT). In this setting, a model learns a new task effectively, but loses\nperformance on previously learned tasks. The Learning without Forgetting (LwF)\napproach addresses this issue via replaying its own prediction for past tasks\nduring model training. In this work, we evaluate FT and LwF for class\nincremental learning in multi-organ segmentation using the publicly available\nAAPM dataset. We show that LwF can successfully retain knowledge on previous\nsegmentations, however, its ability to learn a new class decreases with the\naddition of each class. To address this problem we propose an adversarial\ncontinual learning segmentation approach (ACLSeg), which disentangles feature\nspace into task-specific and task-invariant features. This enables preservation\nof performance on past tasks and effective acquisition of new knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 20:08:39 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Elskhawy", "Abdelrahman", ""], ["Lisowska", "Aneta", ""], ["Keicher", "Matthias", ""], ["Henry", "Josep", ""], ["Thomson", "Paul", ""], ["Navab", "Nassir", ""]]}, {"id": "2008.05563", "submitter": "Naimul Mefraz Khan", "authors": "Bita Houshmand, Naimul Khan", "title": "Facial Expression Recognition Under Partial Occlusion from Virtual\n  Reality Headsets based on Transfer Learning", "comments": "To be presented at the IEEE BigMM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expressions of emotion are a major channel in our daily\ncommunications, and it has been subject of intense research in recent years. To\nautomatically infer facial expressions, convolutional neural network based\napproaches has become widely adopted due to their proven applicability to\nFacial Expression Recognition (FER) task.On the other hand Virtual Reality (VR)\nhas gained popularity as an immersive multimedia platform, where FER can\nprovide enriched media experiences. However, recognizing facial expression\nwhile wearing a head-mounted VR headset is a challenging task due to the upper\nhalf of the face being completely occluded. In this paper we attempt to\novercome these issues and focus on facial expression recognition in presence of\na severe occlusion where the user is wearing a head-mounted display in a VR\nsetting. We propose a geometric model to simulate occlusion resulting from a\nSamsung Gear VR headset that can be applied to existing FER datasets. Then, we\nadopt a transfer learning approach, starting from two pretrained networks,\nnamely VGG and ResNet. We further fine-tune the networks on FER+ and RAF-DB\ndatasets. Experimental results show that our approach achieves comparable\nresults to existing methods while training on three modified benchmark datasets\nthat adhere to realistic occlusion resulting from wearing a commodity VR\nheadset. Code for this paper is available at:\nhttps://github.com/bita-github/MRP-FER\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 20:25:07 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Houshmand", "Bita", ""], ["Khan", "Naimul", ""]]}, {"id": "2008.05567", "submitter": "Soren Pirk", "authors": "Till Niese, S\\\"oren Pirk, Matthias Albrecht, Bedrich Benes, Oliver\n  Deussen", "title": "Procedural Urban Forestry", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The placement of vegetation plays a central role in the realism of virtual\nscenes. We introduce procedural placement models (PPMs) for vegetation in urban\nlayouts. PPMs are environmentally sensitive to city geometry and allow\nidentifying plausible plant positions based on structural and functional zones\nin an urban layout. PPMs can either be directly used by defining their\nparameters or can be learned from satellite images and land register data.\nTogether with approaches for generating buildings and trees, this allows us to\npopulate urban landscapes with complex 3D vegetation. The effectiveness of our\nframework is shown through examples of large-scale city scenes and close-ups of\nindividually grown tree models; we also validate it by a perceptual user study.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 20:44:56 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 00:35:44 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Niese", "Till", ""], ["Pirk", "S\u00f6ren", ""], ["Albrecht", "Matthias", ""], ["Benes", "Bedrich", ""], ["Deussen", "Oliver", ""]]}, {"id": "2008.05570", "submitter": "Siwei Zhang", "authors": "Siwei Zhang, Yan Zhang, Qianli Ma, Michael J. Black, Siyu Tang", "title": "PLACE: Proximity Learning of Articulation and Contact in 3D Environments", "comments": "Accepted by 3DV 2020, camera ready version with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High fidelity digital 3D environments have been proposed in recent years,\nhowever, it remains extremely challenging to automatically equip such\nenvironment with realistic human bodies. Existing work utilizes images, depth\nor semantic maps to represent the scene, and parametric human models to\nrepresent 3D bodies. While being straightforward, their generated human-scene\ninteractions are often lack of naturalness and physical plausibility. Our key\nobservation is that humans interact with the world through body-scene contact.\nTo synthesize realistic human-scene interactions, it is essential to\neffectively represent the physical contact and proximity between the body and\nthe world. To that end, we propose a novel interaction generation method, named\nPLACE (Proximity Learning of Articulation and Contact in 3D Environments),\nwhich explicitly models the proximity between the human body and the 3D scene\naround it. Specifically, given a set of basis points on a scene mesh, we\nleverage a conditional variational autoencoder to synthesize the minimum\ndistances from the basis points to the human body surface. The generated\nproximal relationship exhibits which region of the scene is in contact with the\nperson. Furthermore, based on such synthesized proximity, we are able to\neffectively obtain expressive 3D human bodies that interact with the 3D scene\nnaturally. Our perceptual study shows that PLACE significantly improves the\nstate-of-the-art method, approaching the realism of real human-scene\ninteraction. We believe our method makes an important step towards the fully\nautomatic synthesis of realistic 3D human bodies in 3D scenes. The code and\nmodel are available for research at\nhttps://sanweiliti.github.io/PLACE/PLACE.html.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 21:00:10 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 15:19:34 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 13:16:31 GMT"}, {"version": "v4", "created": "Thu, 12 Nov 2020 16:21:17 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Zhang", "Siwei", ""], ["Zhang", "Yan", ""], ["Ma", "Qianli", ""], ["Black", "Michael J.", ""], ["Tang", "Siyu", ""]]}, {"id": "2008.05571", "submitter": "Navid Alemi Koohbanani", "authors": "Navid Alemi Koohbanani, Balagopal Unnikrishnan, Syed Ali Khurram,\n  Pavitra Krishnaswamy, Nasir Rajpoot", "title": "Self-Path: Self-supervision for Classification of Pathology Images with\n  Limited Annotations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While high-resolution pathology images lend themselves well to `data hungry'\ndeep learning algorithms, obtaining exhaustive annotations on these images is a\nmajor challenge. In this paper, we propose a self-supervised CNN approach to\nleverage unlabeled data for learning generalizable and domain invariant\nrepresentations in pathology images. The proposed approach, which we term as\nSelf-Path, is a multi-task learning approach where the main task is tissue\nclassification and pretext tasks are a variety of self-supervised tasks with\nlabels inherent to the input data. We introduce novel domain specific\nself-supervision tasks that leverage contextual, multi-resolution and semantic\nfeatures in pathology images for semi-supervised learning and domain\nadaptation. We investigate the effectiveness of Self-Path on 3 different\npathology datasets. Our results show that Self-Path with the domain-specific\npretext tasks achieves state-of-the-art performance for semi-supervised\nlearning when small amounts of labeled data are available. Further, we show\nthat Self-Path improves domain adaptation for classification of histology image\npatches when there is no labeled data available for the target domain. This\napproach can potentially be employed for other applications in computational\npathology, where annotation budget is often limited or large amount of\nunlabeled image data is available.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 21:02:32 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Koohbanani", "Navid Alemi", ""], ["Unnikrishnan", "Balagopal", ""], ["Khurram", "Syed Ali", ""], ["Krishnaswamy", "Pavitra", ""], ["Rajpoot", "Nasir", ""]]}, {"id": "2008.05596", "submitter": "Allen Lee", "authors": "Alex Andonian, Camilo Fosco, Mathew Monfort, Allen Lee, Rogerio Feris,\n  Carl Vondrick, and Aude Oliva", "title": "We Have So Much In Common: Modeling Semantic Relational Set Abstractions\n  in Videos", "comments": "European Conference on Computer Vision (ECCV) 2020, accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying common patterns among events is a key ability in human and\nmachine perception, as it underlies intelligent decision making. We propose an\napproach for learning semantic relational set abstractions on videos, inspired\nby human learning. We combine visual features with natural language supervision\nto generate high-level representations of similarities across a set of videos.\nThis allows our model to perform cognitive tasks such as set abstraction (which\ngeneral concept is in common among a set of videos?), set completion (which new\nvideo goes well with the set?), and odd one out detection (which video does not\nbelong to the set?). Experiments on two video benchmarks, Kinetics and\nMulti-Moments in Time, show that robust and versatile representations emerge\nwhen learning to recognize commonalities among sets. We compare our model to\nseveral baseline algorithms and show that significant improvements result from\nexplicitly learning relational abstractions with semantic supervision.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 22:57:44 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Andonian", "Alex", ""], ["Fosco", "Camilo", ""], ["Monfort", "Mathew", ""], ["Lee", "Allen", ""], ["Feris", "Rogerio", ""], ["Vondrick", "Carl", ""], ["Oliva", "Aude", ""]]}, {"id": "2008.05642", "submitter": "Rongqun Lin", "authors": "Rongqun Lin, Linwei Zhu, Shiqi Wang and Sam Kwong", "title": "Towards Modality Transferable Visual Information Representation with\n  Optimal Model Compression", "comments": "Accepted in ACM Multimedia 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compactly representing the visual signals is of fundamental importance in\nvarious image/video-centered applications. Although numerous approaches were\ndeveloped for improving the image and video coding performance by removing the\nredundancies within visual signals, much less work has been dedicated to the\ntransformation of the visual signals to another well-established modality for\nbetter representation capability. In this paper, we propose a new scheme for\nvisual signal representation that leverages the philosophy of transferable\nmodality. In particular, the deep learning model, which characterizes and\nabsorbs the statistics of the input scene with online training, could be\nefficiently represented in the sense of rate-utility optimization to serve as\nthe enhancement layer in the bitstream. As such, the overall performance can be\nfurther guaranteed by optimizing the new modality incorporated. The proposed\nframework is implemented on the state-of-the-art video coding standard (i.e.,\nversatile video coding), and significantly better representation capability has\nbeen observed based on extensive evaluations.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 01:52:40 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Lin", "Rongqun", ""], ["Zhu", "Linwei", ""], ["Wang", "Shiqi", ""], ["Kwong", "Sam", ""]]}, {"id": "2008.05654", "submitter": "Homagni Saha", "authors": "Homagni Saha, Sin Yong Tan, Ali Saffari, Mohamad Katanbaf, Joshua R.\n  Smith, Soumik Sarkar", "title": "Few shot clustering for indoor occupancy detection with extremely\n  low-quality images from battery free cameras", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable detection of human occupancy in indoor environments is critical for\nvarious energy efficiency, security, and safety applications. We consider this\nchallenge of occupancy detection using extremely low-quality,\nprivacy-preserving images from low power image sensors. We propose a combined\nfew shot learning and clustering algorithm to address this challenge that has\nvery low commissioning and maintenance cost. While the few shot learning\nconcept enables us to commission our system with a few labeled examples, the\nclustering step serves the purpose of online adaptation to changing imaging\nenvironment over time. Apart from validating and comparing our algorithm on\nbenchmark datasets, we also demonstrate performance of our algorithm on\nstreaming images collected from real homes using our novel battery free camera\nhardware.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 02:47:01 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Saha", "Homagni", ""], ["Tan", "Sin Yong", ""], ["Saffari", "Ali", ""], ["Katanbaf", "Mohamad", ""], ["Smith", "Joshua R.", ""], ["Sarkar", "Soumik", ""]]}, {"id": "2008.05655", "submitter": "Weiqing Min", "authors": "Weiqing Min, Linhu Liu, Zhiling Wang, Zhengdong Luo, Xiaoming Wei,\n  Xiaolin Wei, Shuqiang Jiang", "title": "ISIA Food-500: A Dataset for Large-Scale Food Recognition via Stacked\n  Global-Local Attention Network", "comments": "Accepted by ACM Multimedia 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Food recognition has received more and more attention in the multimedia\ncommunity for its various real-world applications, such as diet management and\nself-service restaurants. A large-scale ontology of food images is urgently\nneeded for developing advanced large-scale food recognition algorithms, as well\nas for providing the benchmark dataset for such algorithms. To encourage\nfurther progress in food recognition, we introduce the dataset ISIA Food- 500\nwith 500 categories from the list in the Wikipedia and 399,726 images, a more\ncomprehensive food dataset that surpasses existing popular benchmark datasets\nby category coverage and data volume. Furthermore, we propose a stacked\nglobal-local attention network, which consists of two sub-networks for food\nrecognition. One subnetwork first utilizes hybrid spatial-channel attention to\nextract more discriminative features, and then aggregates these multi-scale\ndiscriminative features from multiple layers into global-level representation\n(e.g., texture and shape information about food). The other one generates\nattentional regions (e.g., ingredient relevant regions) from different regions\nvia cascaded spatial transformers, and further aggregates these multi-scale\nregional features from different layers into local-level representation. These\ntwo types of features are finally fused as comprehensive representation for\nfood recognition. Extensive experiments on ISIA Food-500 and other two popular\nbenchmark datasets demonstrate the effectiveness of our proposed method, and\nthus can be considered as one strong baseline. The dataset, code and models can\nbe found at http://123.57.42.89/FoodComputing-Dataset/ISIA-Food500.html.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 02:48:27 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Min", "Weiqing", ""], ["Liu", "Linhu", ""], ["Wang", "Zhiling", ""], ["Luo", "Zhengdong", ""], ["Wei", "Xiaoming", ""], ["Wei", "Xiaolin", ""], ["Jiang", "Shuqiang", ""]]}, {"id": "2008.05657", "submitter": "Jie Song", "authors": "Jie Song, Liang Xiao, Mohsen Molaei, and Zhichao Lian", "title": "Sparse Coding Driven Deep Decision Tree Ensembles for Nuclear\n  Segmentation in Digital Pathology Images", "comments": "Submitted to IEEE Transactions on Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an easily trained yet powerful representation\nlearning approach with performance highly competitive to deep neural networks\nin a digital pathology image segmentation task. The method, called sparse\ncoding driven deep decision tree ensembles that we abbreviate as ScD2TE,\nprovides a new perspective on representation learning. We explore the\npossibility of stacking several layers based on non-differentiable pairwise\nmodules and generate a densely concatenated architecture holding the\ncharacteristics of feature map reuse and end-to-end dense learning. Under this\narchitecture, fast convolutional sparse coding is used to extract multi-level\nfeatures from the output of each layer. In this way, rich image appearance\nmodels together with more contextual information are integrated by learning a\nseries of decision tree ensembles. The appearance and the high-level context\nfeatures of all the previous layers are seamlessly combined by concatenating\nthem to feed-forward as input, which in turn makes the outputs of subsequent\nlayers more accurate and the whole model efficient to train. Compared with deep\nneural networks, our proposed ScD2TE does not require back-propagation\ncomputation and depends on less hyper-parameters. ScD2TE is able to achieve a\nfast end-to-end pixel-wise training in a layer-wise manner. We demonstrated the\nsuperiority of our segmentation technique by evaluating it on the multi-disease\nstate and multi-organ dataset where consistently higher performances were\nobtained for comparison against several state-of-the-art deep learning methods\nsuch as convolutional neural networks (CNN), fully convolutional networks\n(FCN), etc.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 02:59:31 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Song", "Jie", ""], ["Xiao", "Liang", ""], ["Molaei", "Mohsen", ""], ["Lian", "Zhichao", ""]]}, {"id": "2008.05659", "submitter": "Tete Xiao", "authors": "Tete Xiao, Xiaolong Wang, Alexei A. Efros, Trevor Darrell", "title": "What Should Not Be Contrastive in Contrastive Learning", "comments": "Published as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent self-supervised contrastive methods have been able to produce\nimpressive transferable visual representations by learning to be invariant to\ndifferent data augmentations. However, these methods implicitly assume a\nparticular set of representational invariances (e.g., invariance to color), and\ncan perform poorly when a downstream task violates this assumption (e.g.,\ndistinguishing red vs. yellow cars). We introduce a contrastive learning\nframework which does not require prior knowledge of specific, task-dependent\ninvariances. Our model learns to capture varying and invariant factors for\nvisual representations by constructing separate embedding spaces, each of which\nis invariant to all but one augmentation. We use a multi-head network with a\nshared backbone which captures information across each augmentation and alone\noutperforms all baselines on downstream tasks. We further find that the\nconcatenation of the invariant and varying spaces performs best across all\ntasks we investigate, including coarse-grained, fine-grained, and few-shot\ndownstream classification tasks, and various data corruptions.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 03:02:32 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 21:08:52 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Xiao", "Tete", ""], ["Wang", "Xiaolong", ""], ["Efros", "Alexei A.", ""], ["Darrell", "Trevor", ""]]}, {"id": "2008.05667", "submitter": "Md Amirul Islam", "authors": "Md Amirul Islam, Matthew Kowal, Konstantinos G. Derpanis, Neil D. B.\n  Bruce", "title": "Feature Binding with Category-Dependant MixUp for Semantic Segmentation\n  and Adversarial Robustness", "comments": "Accepted to BMVC 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a strategy for training convolutional neural\nnetworks to effectively resolve interference arising from competing hypotheses\nrelating to inter-categorical information throughout the network. The premise\nis based on the notion of feature binding, which is defined as the process by\nwhich activation's spread across space and layers in the network are\nsuccessfully integrated to arrive at a correct inference decision. In our work,\nthis is accomplished for the task of dense image labelling by blending images\nbased on their class labels, and then training a feature binding network, which\nsimultaneously segments and separates the blended images. Subsequent feature\ndenoising to suppress noisy activations reveals additional desirable properties\nand high degrees of successful predictions. Through this process, we reveal a\ngeneral mechanism, distinct from any prior methods, for boosting the\nperformance of the base segmentation network while simultaneously increasing\nrobustness to adversarial attacks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 03:20:01 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Islam", "Md Amirul", ""], ["Kowal", "Matthew", ""], ["Derpanis", "Konstantinos G.", ""], ["Bruce", "Neil D. B.", ""]]}, {"id": "2008.05676", "submitter": "Jialian Wu", "authors": "Jialian Wu, Liangchen Song, Tiancai Wang, Qian Zhang, Junsong Yuan", "title": "Forest R-CNN: Large-Vocabulary Long-Tailed Object Detection and Instance\n  Segmentation", "comments": "Accepted to ACM MM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the previous success of object analysis, detecting and segmenting a\nlarge number of object categories with a long-tailed data distribution remains\na challenging problem and is less investigated. For a large-vocabulary\nclassifier, the chance of obtaining noisy logits is much higher, which can\neasily lead to a wrong recognition. In this paper, we exploit prior knowledge\nof the relations among object categories to cluster fine-grained classes into\ncoarser parent classes, and construct a classification tree that is responsible\nfor parsing an object instance into a fine-grained category via its parent\nclass. In the classification tree, as the number of parent class nodes are\nsignificantly less, their logits are less noisy and can be utilized to suppress\nthe wrong/noisy logits existed in the fine-grained class nodes. As the way to\nconstruct the parent class is not unique, we further build multiple trees to\nform a classification forest where each tree contributes its vote to the\nfine-grained classification. To alleviate the imbalanced learning caused by the\nlong-tail phenomena, we propose a simple yet effective resampling method, NMS\nResampling, to re-balance the data distribution. Our method, termed as Forest\nR-CNN, can serve as a plug-and-play module being applied to most object\nrecognition models for recognizing more than 1000 categories. Extensive\nexperiments are performed on the large vocabulary dataset LVIS. Compared with\nthe Mask R-CNN baseline, the Forest R-CNN significantly boosts the performance\nwith 11.5% and 3.9% AP improvements on the rare categories and overall\ncategories, respectively. Moreover, we achieve state-of-the-art results on the\nLVIS dataset. Code is available at https://github.com/JialianW/Forest_RCNN.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 03:52:37 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 04:51:39 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Wu", "Jialian", ""], ["Song", "Liangchen", ""], ["Wang", "Tiancai", ""], ["Zhang", "Qian", ""], ["Yuan", "Junsong", ""]]}, {"id": "2008.05678", "submitter": "Yiming Cui", "authors": "Dongfang Liu, Yiming Cui, Xiaolei Guo, Wei Ding, Baijian Yang, and\n  Yingjie Chen", "title": "Visual Localization for Autonomous Driving: Mapping the Accurate\n  Location in the City Maze", "comments": "Accepted at ICPR 2020, 8 pages, 10 figures. Code will be released\n  soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate localization is a foundational capacity, required for autonomous\nvehicles to accomplish other tasks such as navigation or path planning. It is a\ncommon practice for vehicles to use GPS to acquire location information.\nHowever, the application of GPS can result in severe challenges when vehicles\nrun within the inner city where different kinds of structures may shadow the\nGPS signal and lead to inaccurate location results. To address the localization\nchallenges of urban settings, we propose a novel feature voting technique for\nvisual localization. Different from the conventional front-view-based method,\nour approach employs views from three directions (front, left, and right) and\nthus significantly improves the robustness of location prediction. In our work,\nwe craft the proposed feature voting method into three state-of-the-art visual\nlocalization networks and modify their architectures properly so that they can\nbe applied for vehicular operation. Extensive field test results indicate that\nour approach can predict location robustly even in challenging inner-city\nsettings. Our research sheds light on using the visual localization approach to\nhelp autonomous vehicles to find accurate location information in a city maze,\nwithin a desirable time constraint.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 03:59:34 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 15:44:35 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2020 01:19:44 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Liu", "Dongfang", ""], ["Cui", "Yiming", ""], ["Guo", "Xiaolei", ""], ["Ding", "Wei", ""], ["Yang", "Baijian", ""], ["Chen", "Yingjie", ""]]}, {"id": "2008.05700", "submitter": "Rui Wang", "authors": "Rui Wang, Dhruv Mahajan, Vignesh Ramanathan", "title": "What leads to generalization of object proposals?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object proposal generation is often the first step in many detection models.\nIt is lucrative to train a good proposal model, that generalizes to unseen\nclasses. This could help scaling detection models to larger number of classes\nwith fewer annotations. Motivated by this, we study how a detection model\ntrained on a small set of source classes can provide proposals that generalize\nto unseen classes. We systematically study the properties of the dataset -\nvisual diversity and label space granularity - required for good\ngeneralization. We show the trade-off between using fine-grained labels and\ncoarse labels. We introduce the idea of prototypical classes: a set of\nsufficient and necessary classes required to train a detection model to obtain\ngeneralized proposals in a more data-efficient way. On the Open Images V4\ndataset, we show that only 25% of the classes can be selected to form such a\nprototypical set. The resulting proposals from a model trained with these\nclasses is only 4.3% worse than using all the classes, in terms of average\nrecall (AR). We also demonstrate that Faster R-CNN model leads to better\ngeneralization of proposals compared to a single-stage network like RetinaNet.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 05:51:35 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Wang", "Rui", ""], ["Mahajan", "Dhruv", ""], ["Ramanathan", "Vignesh", ""]]}, {"id": "2008.05706", "submitter": "Xingchao Peng", "authors": "Yichen Li, Xingchao Peng", "title": "Network Architecture Search for Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks have been used to learn transferable representations for domain\nadaptation. Existing deep domain adaptation methods systematically employ\npopular hand-crafted networks designed specifically for image-classification\ntasks, leading to sub-optimal domain adaptation performance. In this paper, we\npresent Neural Architecture Search for Domain Adaptation (NASDA), a principle\nframework that leverages differentiable neural architecture search to derive\nthe optimal network architecture for domain adaptation task. NASDA is designed\nwith two novel training strategies: neural architecture search with\nmulti-kernel Maximum Mean Discrepancy to derive the optimal architecture, and\nadversarial training between a feature generator and a batch of classifiers to\nconsolidate the feature generator. We demonstrate experimentally that NASDA\nleads to state-of-the-art performance on several domain adaptation benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 06:15:57 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Li", "Yichen", ""], ["Peng", "Xingchao", ""]]}, {"id": "2008.05708", "submitter": "Yi Fang", "authors": "Hao Huang, Jianchun Chen, Xiang Li, Lingjing Wang, Yi Fang", "title": "Robust Image Matching By Dynamic Feature Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating dense correspondences between images is a long-standing image\nunder-standing task. Recent works introduce convolutional neural networks\n(CNNs) to extract high-level feature maps and find correspondences through\nfeature matching. However,high-level feature maps are in low spatial resolution\nand therefore insufficient to provide accurate and fine-grained features to\ndistinguish intra-class variations for correspondence matching. To address this\nproblem, we generate robust features by dynamically selecting features at\ndifferent scales. To resolve two critical issues in feature selection,i.e.,how\nmany and which scales of features to be selected, we frame the feature\nselection process as a sequential Markov decision-making process (MDP) and\nintroduce an optimal selection strategy using reinforcement learning (RL). We\ndefine an RL environment for image matching in which each individual action\neither requires new features or terminates the selection episode by referring a\nmatching score. Deep neural networks are incorporated into our method and\ntrained for decision making. Experimental results show that our method achieves\ncomparable/superior performance with state-of-the-art methods on three\nbenchmarks, demonstrating the effectiveness of our feature selection strategy.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 06:21:33 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Huang", "Hao", ""], ["Chen", "Jianchun", ""], ["Li", "Xiang", ""], ["Wang", "Lingjing", ""], ["Fang", "Yi", ""]]}, {"id": "2008.05711", "submitter": "Jonah Philion", "authors": "Jonah Philion, Sanja Fidler", "title": "Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by\n  Implicitly Unprojecting to 3D", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of perception for autonomous vehicles is to extract semantic\nrepresentations from multiple sensors and fuse these representations into a\nsingle \"bird's-eye-view\" coordinate frame for consumption by motion planning.\nWe propose a new end-to-end architecture that directly extracts a\nbird's-eye-view representation of a scene given image data from an arbitrary\nnumber of cameras. The core idea behind our approach is to \"lift\" each image\nindividually into a frustum of features for each camera, then \"splat\" all\nfrustums into a rasterized bird's-eye-view grid. By training on the entire\ncamera rig, we provide evidence that our model is able to learn not only how to\nrepresent images but how to fuse predictions from all cameras into a single\ncohesive representation of the scene while being robust to calibration error.\nOn standard bird's-eye-view tasks such as object segmentation and map\nsegmentation, our model outperforms all baselines and prior work. In pursuit of\nthe goal of learning dense representations for motion planning, we show that\nthe representations inferred by our model enable interpretable end-to-end\nmotion planning by \"shooting\" template trajectories into a bird's-eye-view cost\nmap output by our network. We benchmark our approach against models that use\noracle depth from lidar. Project page with code:\nhttps://nv-tlabs.github.io/lift-splat-shoot .\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 06:29:01 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Philion", "Jonah", ""], ["Fidler", "Sanja", ""]]}, {"id": "2008.05714", "submitter": "Keyu Chen", "authors": "Keyu Chen, Jianmin Zheng, Jianfei Cai, Juyong Zhang", "title": "Modeling Caricature Expressions by 3D Blendshape and Dynamic Texture", "comments": "Accepted by the 28th ACM International Conference on Multimedia (ACM\n  MM 2020)", "journal-ref": null, "doi": "10.1145/3394171.3413643", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of deforming an artist-drawn caricature according to a given\nnormal face expression is of interest in applications such as social media,\nanimation and entertainment. This paper presents a solution to the problem,\nwith an emphasis on enhancing the ability to create desired expressions and\nmeanwhile preserve the identity exaggeration style of the caricature, which\nimposes challenges due to the complicated nature of caricatures. The key of our\nsolution is a novel method to model caricature expression, which extends\ntraditional 3DMM representation to caricature domain. The method consists of\nshape modelling and texture generation for caricatures. Geometric optimization\nis developed to create identity-preserving blendshapes for reconstructing\naccurate and stable geometric shape, and a conditional generative adversarial\nnetwork (cGAN) is designed for generating dynamic textures under target\nexpressions. The combination of both shape and texture components makes the\nnon-trivial expressions of a caricature be effectively defined by the extension\nof the popular 3DMM representation and a caricature can thus be flexibly\ndeformed into arbitrary expressions with good results visually in both shape\nand color spaces. The experiments demonstrate the effectiveness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 06:31:01 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Chen", "Keyu", ""], ["Zheng", "Jianmin", ""], ["Cai", "Jianfei", ""], ["Zhang", "Juyong", ""]]}, {"id": "2008.05717", "submitter": "Xixia Xu", "authors": "Xixia Xu, Qi Zou, Xue Lin", "title": "Alleviating Human-level Shift : A Robust Domain Adaptation Method for\n  Multi-person Pose Estimation", "comments": "Accepted By ACM MM'2020", "journal-ref": null, "doi": "10.1145/3394171.3414040", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose estimation has been widely studied with much focus on supervised\nlearning requiring sufficient annotations. However, in real applications, a\npretrained pose estimation model usually need be adapted to a novel domain with\nno labels or sparse labels. Such domain adaptation for 2D pose estimation\nhasn't been explored. The main reason is that a pose, by nature, has typical\ntopological structure and needs fine-grained features in local keypoints. While\nexisting adaptation methods do not consider topological structure of\nobject-of-interest and they align the whole images coarsely. Therefore, we\npropose a novel domain adaptation method for multi-person pose estimation to\nconduct the human-level topological structure alignment and fine-grained\nfeature alignment. Our method consists of three modules: Cross-Attentive\nFeature Alignment (CAFA), Intra-domain Structure Adaptation (ISA) and\nInter-domain Human-Topology Alignment (IHTA) module. The CAFA adopts a\nbidirectional spatial attention module (BSAM)that focuses on fine-grained local\nfeature correlation between two humans to adaptively aggregate consistent\nfeatures for adaptation. We adopt ISA only in semi-supervised domain adaptation\n(SSDA) to exploit the corresponding keypoint semantic relationship for reducing\nthe intra-domain bias. Most importantly, we propose an IHTA to learn more\ndomain-invariant human topological representation for reducing the inter-domain\ndiscrepancy. We model the human topological structure via the graph convolution\nnetwork (GCN), by passing messages on which, high-order relations can be\nconsidered. This structure preserving alignment based on GCN is beneficial to\nthe occluded or extreme pose inference. Extensive experiments are conducted on\ntwo popular benchmarks and results demonstrate the competency of our method\ncompared with existing supervised approaches.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 06:41:49 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Xu", "Xixia", ""], ["Zou", "Qi", ""], ["Lin", "Xue", ""]]}, {"id": "2008.05721", "submitter": "TaeOh Kim", "authors": "Taeoh Kim, Hyeongmin Lee, MyeongAh Cho, Ho Seong Lee, Dong Heon Cho,\n  Sangyoun Lee", "title": "Learning Temporally Invariant and Localizable Features via Data\n  Augmentation for Video Recognition", "comments": "European Conference on Computer Vision (ECCV) 2020, 1st Visual\n  Inductive Priors for Data-Efficient Deep Learning Workshop (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-Learning-based video recognition has shown promising improvements along\nwith the development of large-scale datasets and spatiotemporal network\narchitectures. In image recognition, learning spatially invariant features is a\nkey factor in improving recognition performance and robustness. Data\naugmentation based on visual inductive priors, such as cropping, flipping,\nrotating, or photometric jittering, is a representative approach to achieve\nthese features. Recent state-of-the-art recognition solutions have relied on\nmodern data augmentation strategies that exploit a mixture of augmentation\noperations. In this study, we extend these strategies to the temporal dimension\nfor videos to learn temporally invariant or temporally localizable features to\ncover temporal perturbations or complex actions in videos. Based on our novel\ntemporal data augmentation algorithms, video recognition performances are\nimproved using only a limited amount of training data compared to the\nspatial-only data augmentation algorithms, including the 1st Visual Inductive\nPriors (VIPriors) for data-efficient action recognition challenge. Furthermore,\nlearned features are temporally localizable that cannot be achieved using\nspatial augmentation algorithms. Our source code is available at\nhttps://github.com/taeoh-kim/temporal_data_augmentation.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 06:56:52 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Kim", "Taeoh", ""], ["Lee", "Hyeongmin", ""], ["Cho", "MyeongAh", ""], ["Lee", "Ho Seong", ""], ["Cho", "Dong Heon", ""], ["Lee", "Sangyoun", ""]]}, {"id": "2008.05723", "submitter": "Saket Anand", "authors": "Sharat Agarwal and Himanshu Arora and Saket Anand and Chetan Arora", "title": "Contextual Diversity for Active Learning", "comments": "A variant of this report is accepted in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Requirement of large annotated datasets restrict the use of deep\nconvolutional neural networks (CNNs) for many practical applications. The\nproblem can be mitigated by using active learning (AL) techniques which, under\na given annotation budget, allow to select a subset of data that yields maximum\naccuracy upon fine tuning. State of the art AL approaches typically rely on\nmeasures of visual diversity or prediction uncertainty, which are unable to\neffectively capture the variations in spatial context. On the other hand,\nmodern CNN architectures make heavy use of spatial context for achieving highly\naccurate predictions. Since the context is difficult to evaluate in the absence\nof ground-truth labels, we introduce the notion of contextual diversity that\ncaptures the confusion associated with spatially co-occurring classes.\nContextual Diversity (CD) hinges on a crucial observation that the probability\nvector predicted by a CNN for a region of interest typically contains\ninformation from a larger receptive field. Exploiting this observation, we use\nthe proposed CD measure within two AL frameworks: (1) a core-set based strategy\nand (2) a reinforcement learning based policy, for active frame selection. Our\nextensive empirical evaluation establish state of the art results for active\nlearning on benchmark datasets of Semantic Segmentation, Object Detection and\nImage Classification. Our ablation studies show clear advantages of using\ncontextual diversity for active learning. The source code and additional\nresults are available at https://github.com/sharat29ag/CDAL.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 07:04:15 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Agarwal", "Sharat", ""], ["Arora", "Himanshu", ""], ["Anand", "Saket", ""], ["Arora", "Chetan", ""]]}, {"id": "2008.05731", "submitter": "Gwenole Quellec", "authors": "Gwenol\\'e Quellec, Hassan Al Hajj, Mathieu Lamard, Pierre-Henri Conze,\n  Pascale Massin, B\\'eatrice Cochener", "title": "ExplAIn: Explanatory Artificial Intelligence for Diabetic Retinopathy\n  Diagnosis", "comments": null, "journal-ref": "Medical Image Analysis, Volume 72, August 2021, 102118", "doi": "10.1016/j.media.2021.102118", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In recent years, Artificial Intelligence (AI) has proven its relevance for\nmedical decision support. However, the \"black-box\" nature of successful AI\nalgorithms still holds back their wide-spread deployment. In this paper, we\ndescribe an eXplanatory Artificial Intelligence (XAI) that reaches the same\nlevel of performance as black-box AI, for the task of classifying Diabetic\nRetinopathy (DR) severity using Color Fundus Photography (CFP). This algorithm,\ncalled ExplAIn, learns to segment and categorize lesions in images; the final\nimage-level classification directly derives from these multivariate lesion\nsegmentations. The novelty of this explanatory framework is that it is trained\nfrom end to end, with image supervision only, just like black-box AI\nalgorithms: the concepts of lesions and lesion categories emerge by themselves.\nFor improved lesion localization, foreground/background separation is trained\nthrough self-supervision, in such a way that occluding foreground pixels\ntransforms the input image into a healthy-looking image. The advantage of such\nan architecture is that automatic diagnoses can be explained simply by an image\nand/or a few sentences. ExplAIn is evaluated at the image level and at the\npixel level on various CFP image datasets. We expect this new framework, which\njointly offers high classification performance and explainability, to\nfacilitate AI deployment.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 07:34:05 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 15:22:55 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 12:16:08 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Quellec", "Gwenol\u00e9", ""], ["Hajj", "Hassan Al", ""], ["Lamard", "Mathieu", ""], ["Conze", "Pierre-Henri", ""], ["Massin", "Pascale", ""], ["Cochener", "B\u00e9atrice", ""]]}, {"id": "2008.05732", "submitter": "Kenneth Lai", "authors": "Kenneth Lai and Svetlana Yanushkevich", "title": "An Ensemble of Knowledge Sharing Models for Dynamic Hand Gesture\n  Recognition", "comments": "Accepted at International Joint Conference on Neural Network", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of this paper is dynamic gesture recognition in the context of the\ninteraction between humans and machines. We propose a model consisting of two\nsub-networks, a transformer and an ordered-neuron long-short-term-memory\n(ON-LSTM) based recurrent neural network (RNN). Each sub-network is trained to\nperform the task of gesture recognition using only skeleton joints. Since each\nsub-network extracts different types of features due to the difference in\narchitecture, the knowledge can be shared between the sub-networks. Through\nknowledge distillation, the features and predictions from each sub-network are\nfused together into a new fusion classifier. In addition, a cyclical learning\nrate can be used to generate a series of models that are combined in an\nensemble, in order to yield a more generalizable prediction. The proposed\nensemble of knowledge-sharing models exhibits an overall accuracy of 86.11%\nusing only skeleton information, as tested using the Dynamic Hand Gesture-14/28\ndataset\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 07:37:27 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Lai", "Kenneth", ""], ["Yanushkevich", "Svetlana", ""]]}, {"id": "2008.05735", "submitter": "Kenneth Lai", "authors": "Kenneth Lai, Svetlana N. Yanushkevich, and Vlad Shmerko", "title": "Reliability of Decision Support in Cross-spectral Biometric-enabled\n  Systems", "comments": "submitted to IEEE International Conference on Systems, Man, and\n  Cybernetics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the evaluation of the performance of the decision\nsupport system that utilizes face and facial expression biometrics. The\nevaluation criteria include risk of error and related reliability of decision,\nas well as their contribution to the changes in the perceived operator's trust\nin the decision. The relevant applications include human behavior monitoring\nand stress detection in individuals and teams, and in situational awareness\nsystem. Using an available database of cross-spectral videos of faces and\nfacial expressions, we conducted a series of experiments that demonstrate the\nphenomenon of biases in biometrics that affect the evaluated measures of the\nperformance in human-machine systems.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 07:43:14 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Lai", "Kenneth", ""], ["Yanushkevich", "Svetlana N.", ""], ["Shmerko", "Vlad", ""]]}, {"id": "2008.05742", "submitter": "Jiapeng Tang", "authors": "Jiapeng Tang, Xiaoguang Han, Mingkui Tan, Xin Tong, Kui Jia", "title": "SkeletonNet: A Topology-Preserving Solution for Learning Mesh\n  Reconstruction of Object Surfaces from RGB Images", "comments": "17 pages, 13 figures; TPAMI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the challenging task of learning 3D object surface\nreconstructions from RGB images. Existingmethods achieve varying degrees of\nsuccess by using different surface representations. However, they all have\ntheir own drawbacks,and cannot properly reconstruct the surface shapes of\ncomplex topologies, arguably due to a lack of constraints on the\ntopologicalstructures in their learning frameworks. To this end, we propose to\nlearn and use the topology-preserved, skeletal shape representationto assist\nthe downstream task of object surface reconstruction from RGB images.\nTechnically, we propose the novelSkeletonNetdesign that learns a volumetric\nrepresentation of a skeleton via a bridged learning of a skeletal point set,\nwhere we use paralleldecoders each responsible for the learning of points on 1D\nskeletal curves and 2D skeletal sheets, as well as an efficient module\nofglobally guided subvolume synthesis for a refined, high-resolution skeletal\nvolume; we present a differentiablePoint2Voxellayer tomake SkeletonNet\nend-to-end and trainable. With the learned skeletal volumes, we propose two\nmodels, the Skeleton-Based GraphConvolutional Neural Network (SkeGCNN) and the\nSkeleton-Regularized Deep Implicit Surface Network (SkeDISN), which\nrespectivelybuild upon and improve over the existing frameworks of explicit\nmesh deformation and implicit field learning for the downstream\nsurfacereconstruction task. We conduct thorough experiments that verify the\nefficacy of our proposed SkeletonNet. SkeGCNN and SkeDISNoutperform existing\nmethods as well, and they have their own merits when measured by different\nmetrics. Additional results ingeneralized task settings further demonstrate the\nusefulness of our proposed methods. We have made both our implementation\ncodeand the ShapeNet-Skeleton dataset publicly available at ble at\nhttps://github.com/tangjiapeng/SkeletonNet.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 07:59:25 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 14:20:56 GMT"}, {"version": "v3", "created": "Thu, 10 Jun 2021 03:26:02 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Tang", "Jiapeng", ""], ["Han", "Xiaoguang", ""], ["Tan", "Mingkui", ""], ["Tong", "Xin", ""], ["Jia", "Kui", ""]]}, {"id": "2008.05743", "submitter": "Daniel Barath", "authors": "Istan Gergo Gal, Daniel Barath, Levente Hajder", "title": "Pose Estimation for Vehicle-mounted Cameras via Horizontal and Vertical\n  Planes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two novel solvers for estimating the egomotion of a calibrated\ncamera mounted to a moving vehicle from a single affine correspondence via\nrecovering special homographies. For the first class of solvers, the sought\nplane is expected to be perpendicular to one of the camera axes. For the second\nclass, the plane is orthogonal to the ground with unknown normal, e.g., it is a\nbuilding facade. Both methods are solved via a linear system with a small\ncoefficient matrix, thus, being extremely efficient. Both the minimal and\nover-determined cases can be solved by the proposed methods. They are tested on\nsynthetic data and on publicly available real-world datasets. The novel methods\nare more accurate or comparable to the traditional algorithms and are faster\nwhen included in state of the art robust estimators.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 08:01:48 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Gal", "Istan Gergo", ""], ["Barath", "Daniel", ""], ["Hajder", "Levente", ""]]}, {"id": "2008.05746", "submitter": "Akash Gupta", "authors": "Akash Gupta, Rameswar Panda, Sujoy Paul, Jianming Zhang, Amit K.\n  Roy-Chowdhury", "title": "Adversarial Knowledge Transfer from Unlabeled Data", "comments": "Accepted to ACM Multimedia 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While machine learning approaches to visual recognition offer great promise,\nmost of the existing methods rely heavily on the availability of large\nquantities of labeled training data. However, in the vast majority of\nreal-world settings, manually collecting such large labeled datasets is\ninfeasible due to the cost of labeling data or the paucity of data in a given\ndomain. In this paper, we present a novel Adversarial Knowledge Transfer (AKT)\nframework for transferring knowledge from internet-scale unlabeled data to\nimprove the performance of a classifier on a given visual recognition task. The\nproposed adversarial learning framework aligns the feature space of the\nunlabeled source data with the labeled target data such that the target\nclassifier can be used to predict pseudo labels on the source data. An\nimportant novel aspect of our method is that the unlabeled source data can be\nof different classes from those of the labeled target data, and there is no\nneed to define a separate pretext task, unlike some existing approaches.\nExtensive experiments well demonstrate that models learned using our approach\nhold a lot of promise across a variety of visual recognition tasks on multiple\nstandard datasets.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 08:04:27 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Gupta", "Akash", ""], ["Panda", "Rameswar", ""], ["Paul", "Sujoy", ""], ["Zhang", "Jianming", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "2008.05753", "submitter": "Jong Chul Ye", "authors": "Jawook Gu, Jong Chul Ye", "title": "AdaIN-Switchable CycleGAN for Efficient Unsupervised Low-Dose CT\n  Denoising", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Recently, deep learning approaches have been extensively studied for low-dose\nCT denoising thanks to its superior performance despite the fast computational\ntime. In particular, cycleGAN has been demonstrated as a powerful unsupervised\nlearning scheme to improve the low-dose CT image quality without requiring\nmatched high-dose reference data. Unfortunately, one of the main limitations of\nthe cycleGAN approach is that it requires two deep neural network generators at\nthe training phase, although only one of them is used at the inference phase.\nThe secondary auxiliary generator is needed to enforce the cycle-consistency,\nbut the additional memory requirement and increases of the learnable parameters\nare the main huddles for cycleGAN training. To address this issue, here we\npropose a novel cycleGAN architecture using a single switchable generator. In\nparticular, a single generator is implemented using adaptive instance\nnormalization (AdaIN) layers so that the baseline generator converting a\nlow-dose CT image to a routine-dose CT image can be switched to a generator\nconverting high-dose to low-dose by simply changing the AdaIN code. Thanks to\nthe shared baseline network, the additional memory requirement and weight\nincreases are minimized, and the training can be done more stably even with\nsmall training data. Experimental results show that the proposed method\noutperforms the previous cycleGAN approaches while using only about half the\nparameters.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 08:30:23 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Gu", "Jawook", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2008.05763", "submitter": "Hugo Touvron", "authors": "Hugo Touvron, Matthijs Douze, Matthieu Cord, Herv\\'e J\\'egou", "title": "Powers of layers for image-to-image translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple architecture to address unpaired image-to-image\ntranslation tasks: style or class transfer, denoising, deblurring, deblocking,\netc. We start from an image autoencoder architecture with fixed weights. For\neach task we learn a residual block operating in the latent space, which is\niteratively called until the target domain is reached. A specific training\nschedule is required to alleviate the exponentiation effect of the iterations.\nAt test time, it offers several advantages: the number of weight parameters is\nlimited and the compositional design allows one to modulate the strength of the\ntransformation with the number of iterations. This is useful, for instance,\nwhen the type or amount of noise to suppress is not known in advance.\nExperimentally, we provide proofs of concepts showing the interest of our\nmethod for many transformations. The performance of our model is comparable or\nbetter than CycleGAN with significantly fewer parameters.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 09:02:17 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Touvron", "Hugo", ""], ["Douze", "Matthijs", ""], ["Cord", "Matthieu", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "2008.05765", "submitter": "Takashi Isobe", "authors": "Takashi Isobe, Fang Zhu, Xu Jia and Shengjin Wang", "title": "Revisiting Temporal Modeling for Video Super-resolution", "comments": "BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video super-resolution plays an important role in surveillance video analysis\nand ultra-high-definition video display, which has drawn much attention in both\nthe research and industrial communities. Although many deep learning-based VSR\nmethods have been proposed, it is hard to directly compare these methods since\nthe different loss functions and training datasets have a significant impact on\nthe super-resolution results. In this work, we carefully study and compare\nthree temporal modeling methods (2D CNN with early fusion, 3D CNN with slow\nfusion and Recurrent Neural Network) for video super-resolution. We also\npropose a novel Recurrent Residual Network (RRN) for efficient video\nsuper-resolution, where residual learning is utilized to stabilize the training\nof RNN and meanwhile to boost the super-resolution performance. Extensive\nexperiments show that the proposed RRN is highly computational efficiency and\nproduces temporal consistent VSR results with finer details than other temporal\nmodeling methods. Besides, the proposed method achieves state-of-the-art\nresults on several widely used benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 09:09:37 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 02:00:20 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Isobe", "Takashi", ""], ["Zhu", "Fang", ""], ["Jia", "Xu", ""], ["Wang", "Shengjin", ""]]}, {"id": "2008.05767", "submitter": "Jihun Oh", "authors": "Jihun Oh, SangJeong Lee, Meejeong Park, Pooni Walagaurav and Kiseok\n  Kwon", "title": "Weight Equalizing Shift Scaler-Coupled Post-training Quantization", "comments": "9 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Post-training, layer-wise quantization is preferable because it is free from\nretraining and is hardware-friendly. Nevertheless, accuracy degradation has\noccurred when a neural network model has a big difference of per-out-channel\nweight ranges. In particular, the MobileNet family has a tragedy drop in top-1\naccuracy from 70.60% ~ 71.87% to 0.1% on the ImageNet dataset after 8-bit\nweight quantization. To mitigate this significant accuracy reduction, we\npropose a new weight equalizing shift scaler, i.e. rescaling the weight range\nper channel by a 4-bit binary shift, prior to a layer-wise quantization. To\nrecover the original output range, inverse binary shifting is efficiently fused\nto the existing per-layer scale compounding in the fixed-computing\nconvolutional operator of the custom neural processing unit. The binary shift\nis a key feature of our algorithm, which significantly improved the accuracy\nperformance without impeding the memory footprint. As a result, our proposed\nmethod achieved a top-1 accuracy of 69.78% ~ 70.96% in MobileNets and showed\nrobust performance in varying network models and tasks, which is competitive to\nchannel-wise quantization results.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 09:19:57 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Oh", "Jihun", ""], ["Lee", "SangJeong", ""], ["Park", "Meejeong", ""], ["Walagaurav", "Pooni", ""], ["Kwon", "Kiseok", ""]]}, {"id": "2008.05770", "submitter": "Chen Li", "authors": "Chen Li and Gim Hee Lee", "title": "Weakly Supervised Generative Network for Multiple 3D Human Pose\n  Hypotheses", "comments": "Accepted to BMVC2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D human pose estimation from a single image is an inverse problem due to the\ninherent ambiguity of the missing depth. Several previous works addressed the\ninverse problem by generating multiple hypotheses. However, these works are\nstrongly supervised and require ground truth 2D-to-3D correspondences which can\nbe difficult to obtain. In this paper, we propose a weakly supervised deep\ngenerative network to address the inverse problem and circumvent the need for\nground truth 2D-to-3D correspondences. To this end, we design our network to\nmodel a proposal distribution which we use to approximate the unknown\nmulti-modal target posterior distribution. We achieve the approximation by\nminimizing the KL divergence between the proposal and target distributions, and\nthis leads to a 2D reprojection error and a prior loss term that can be weakly\nsupervised. Furthermore, we determine the most probable solution as the\nconditional mode of the samples using the mean-shift algorithm. We evaluate our\nmethod on three benchmark datasets -- Human3.6M, MPII and MPI-INF-3DHP.\nExperimental results show that our approach is capable of generating multiple\nfeasible hypotheses and achieves state-of-the-art results compared to existing\nweakly supervised approaches. Our source code is available at the project\nwebsite.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 09:26:01 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Li", "Chen", ""], ["Lee", "Gim Hee", ""]]}, {"id": "2008.05772", "submitter": "Jong Chul Ye", "authors": "Boah Kim, Dong Hwan Kim, Seong Ho Park, Jieun Kim, June-Goo Lee, Jong\n  Chul Ye", "title": "CycleMorph: Cycle Consistent Unsupervised Deformable Image Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image registration is a fundamental task in medical image analysis. Recently,\ndeep learning based image registration methods have been extensively\ninvestigated due to their excellent performance despite the ultra-fast\ncomputational time. However, the existing deep learning methods still have\nlimitation in the preservation of original topology during the deformation with\nregistration vector fields. To address this issues, here we present a\ncycle-consistent deformable image registration. The cycle consistency enhances\nimage registration performance by providing an implicit regularization to\npreserve topology during the deformation. The proposed method is so flexible\nthat can be applied for both 2D and 3D registration problems for various\napplications, and can be easily extended to multi-scale implementation to deal\nwith the memory issues in large volume registration. Experimental results on\nvarious datasets from medical and non-medical applications demonstrate that the\nproposed method provides effective and accurate registration on diverse image\npairs within a few seconds. Qualitative and quantitative evaluations on\ndeformation fields also verify the effectiveness of the cycle consistency of\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 09:30:12 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Kim", "Boah", ""], ["Kim", "Dong Hwan", ""], ["Park", "Seong Ho", ""], ["Kim", "Jieun", ""], ["Lee", "June-Goo", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2008.05780", "submitter": "Zhen Zhang", "authors": "Zhen Zhang, Chenyu Liu, Wangbin Ding, Sihan Wang, Chenhao Pei,\n  Mingjing Yang, Liqin Huang", "title": "Multi-Modality Pathology Segmentation Framework: Application to Cardiac\n  Magnetic Resonance Images", "comments": "12 pages,MyoPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-sequence of cardiac magnetic resonance (CMR) images can provide\ncomplementary information for myocardial pathology (scar and edema). However,\nit is still challenging to fuse these underlying information for pathology\nsegmentation effectively. This work presents an automatic cascade pathology\nsegmentation framework based on multi-modality CMR images. It mainly consists\nof two neural networks: an anatomical structure segmentation network (ASSN) and\na pathological region segmentation network (PRSN). Specifically, the ASSN aims\nto segment the anatomical structure where the pathology may exist, and it can\nprovide a spatial prior for the pathological region segmentation. In addition,\nwe integrate a denoising auto-encoder (DAE) into the ASSN to generate\nsegmentation results with plausible shapes. The PRSN is designed to segment\npathological region based on the result of ASSN, in which a fusion block based\non channel attention is proposed to better aggregate multi-modality information\nfrom multi-modality CMR images. Experiments from the MyoPS2020 challenge\ndataset show that our framework can achieve promising performance for\nmyocardial scar and edema segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 09:57:04 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Zhang", "Zhen", ""], ["Liu", "Chenyu", ""], ["Ding", "Wangbin", ""], ["Wang", "Sihan", ""], ["Pei", "Chenhao", ""], ["Yang", "Mingjing", ""], ["Huang", "Liqin", ""]]}, {"id": "2008.05785", "submitter": "Daniyar Turmukhambetov", "authors": "Anita Rau, Guillermo Garcia-Hernando, Danail Stoyanov, Gabriel J.\n  Brostow, Daniyar Turmukhambetov", "title": "Predicting Visual Overlap of Images Through Interpretable Non-Metric Box\n  Embeddings", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To what extent are two images picturing the same 3D surfaces? Even when this\nis a known scene, the answer typically requires an expensive search across\nscale space, with matching and geometric verification of large sets of local\nfeatures. This expense is further multiplied when a query image is evaluated\nagainst a gallery, e.g. in visual relocalization. While we don't obviate the\nneed for geometric verification, we propose an interpretable image-embedding\nthat cuts the search in scale space to essentially a lookup.\n  Our approach measures the asymmetric relation between two images. The model\nthen learns a scene-specific measure of similarity, from training examples with\nknown 3D visible-surface overlaps. The result is that we can quickly identify,\nfor example, which test image is a close-up version of another, and by what\nscale factor. Subsequently, local features need only be detected at that scale.\nWe validate our scene-specific model by showing how this embedding yields\ncompetitive image-matching results, while being simpler, faster, and also\ninterpretable by humans.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 10:01:07 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Rau", "Anita", ""], ["Garcia-Hernando", "Guillermo", ""], ["Stoyanov", "Danail", ""], ["Brostow", "Gabriel J.", ""], ["Turmukhambetov", "Daniyar", ""]]}, {"id": "2008.05787", "submitter": "Marco Manfredi", "authors": "Marco Manfredi and Yu Wang", "title": "Shift Equivariance in Object Detection", "comments": "Accepted at ECCV 2020 Workshop: Beyond mAP: Reassessing the\n  Evaluation of Object Detectors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robustness to small image translations is a highly desirable property for\nobject detectors. However, recent works have shown that CNN-based classifiers\nare not shift invariant. It is unclear to what extent this could impact object\ndetection, mainly because of the architectural differences between the two and\nthe dimensionality of the prediction space of modern detectors. To assess shift\nequivariance of object detection models end-to-end, in this paper we propose an\nevaluation metric, built upon a greedy search of the lower and upper bounds of\nthe mean average precision on a shifted image set. Our new metric shows that\nmodern object detection architectures, no matter if one-stage or two-stage,\nanchor-based or anchor-free, are sensitive to even one pixel shift to the input\nimages. Furthermore, we investigate several possible solutions to this problem,\nboth taken from the literature and newly proposed, quantifying the\neffectiveness of each one with the suggested metric. Our results indicate that\nnone of these methods can provide full shift equivariance. Measuring and\nanalyzing the extent of shift variance of different models and the\ncontributions of possible factors, is a first step towards being able to devise\nmethods that mitigate or even leverage such variabilities.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 10:02:02 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Manfredi", "Marco", ""], ["Wang", "Yu", ""]]}, {"id": "2008.05789", "submitter": "Ying Cheng", "authors": "Ying Cheng, Ruize Wang, Zhihao Pan, Rui Feng, Yuejie Zhang", "title": "Look, Listen, and Attend: Co-Attention Network for Self-Supervised\n  Audio-Visual Representation Learning", "comments": "Accepted by the 28th ACM International Conference on Multimedia (ACM\n  MM 2020)", "journal-ref": null, "doi": "10.1145/3394171.3413869", "report-no": null, "categories": "cs.MM cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When watching videos, the occurrence of a visual event is often accompanied\nby an audio event, e.g., the voice of lip motion, the music of playing\ninstruments. There is an underlying correlation between audio and visual\nevents, which can be utilized as free supervised information to train a neural\nnetwork by solving the pretext task of audio-visual synchronization. In this\npaper, we propose a novel self-supervised framework with co-attention mechanism\nto learn generic cross-modal representations from unlabelled videos in the\nwild, and further benefit downstream tasks. Specifically, we explore three\ndifferent co-attention modules to focus on discriminative visual regions\ncorrelated to the sounds and introduce the interactions between them.\nExperiments show that our model achieves state-of-the-art performance on the\npretext task while having fewer parameters compared with existing methods. To\nfurther evaluate the generalizability and transferability of our approach, we\napply the pre-trained model on two downstream tasks, i.e., sound source\nlocalization and action recognition. Extensive experiments demonstrate that our\nmodel provides competitive results with other self-supervised methods, and also\nindicate that our approach can tackle the challenging scenes which contain\nmultiple sound sources.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 10:08:12 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Cheng", "Ying", ""], ["Wang", "Ruize", ""], ["Pan", "Zhihao", ""], ["Feng", "Rui", ""], ["Zhang", "Yuejie", ""]]}, {"id": "2008.05826", "submitter": "Tao Hu", "authors": "Pengwan Yang, Vincent Tao Hu, Pascal Mettes, Cees G. M. Snoek", "title": "Localizing the Common Action Among a Few Videos", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper strives to localize the temporal extent of an action in a long\nuntrimmed video. Where existing work leverages many examples with their start,\ntheir ending, and/or the class of the action during training time, we propose\nfew-shot common action localization. The start and end of an action in a long\nuntrimmed video is determined based on just a hand-full of trimmed video\nexamples containing the same action, without knowing their common class label.\nTo address this task, we introduce a new 3D convolutional network architecture\nable to align representations from the support videos with the relevant query\nvideo segments. The network contains: (\\textit{i}) a mutual enhancement module\nto simultaneously complement the representation of the few trimmed support\nvideos and the untrimmed query video; (\\textit{ii}) a progressive alignment\nmodule that iteratively fuses the support videos into the query branch; and\n(\\textit{iii}) a pairwise matching module to weigh the importance of different\nsupport videos. Evaluation of few-shot common action localization in untrimmed\nvideos containing a single or multiple action instances demonstrates the\neffectiveness and general applicability of our proposal.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 11:31:23 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 12:30:01 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Yang", "Pengwan", ""], ["Hu", "Vincent Tao", ""], ["Mettes", "Pascal", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "2008.05856", "submitter": "Hongyuan Yu", "authors": "Hongyuan Yu, Yan Huang, Lihong Pi, Liang Wang", "title": "Recurrent Deconvolutional Generative Adversarial Networks with\n  Application to Text Guided Video Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel model for video generation and especially makes\nthe attempt to deal with the problem of video generation from text\ndescriptions, i.e., synthesizing realistic videos conditioned on given texts.\nExisting video generation methods cannot be easily adapted to handle this task\nwell, due to the frame discontinuity issue and their text-free generation\nschemes. To address these problems, we propose a recurrent deconvolutional\ngenerative adversarial network (RD-GAN), which includes a recurrent\ndeconvolutional network (RDN) as the generator and a 3D convolutional neural\nnetwork (3D-CNN) as the discriminator. The RDN is a deconvolutional version of\nconventional recurrent neural network, which can well model the long-range\ntemporal dependency of generated video frames and make good use of conditional\ninformation. The proposed model can be jointly trained by pushing the RDN to\ngenerate realistic videos so that the 3D-CNN cannot distinguish them from real\nones. We apply the proposed RD-GAN to a series of tasks including conventional\nvideo generation, conditional video generation, video prediction and video\nclassification, and demonstrate its effectiveness by achieving well\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 12:22:27 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Yu", "Hongyuan", ""], ["Huang", "Yan", ""], ["Pi", "Lihong", ""], ["Wang", "Liang", ""]]}, {"id": "2008.05861", "submitter": "Jiangliu Wang", "authors": "Jiangliu Wang, Jianbo Jiao, and Yun-Hui Liu", "title": "Self-supervised Video Representation Learning by Pace Prediction", "comments": "Correct some typos;Update some cocurent works accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of self-supervised video representation\nlearning from a new perspective -- by video pace prediction. It stems from the\nobservation that human visual system is sensitive to video pace, e.g., slow\nmotion, a widely used technique in film making. Specifically, given a video\nplayed in natural pace, we randomly sample training clips in different paces\nand ask a neural network to identify the pace for each video clip. The\nassumption here is that the network can only succeed in such a pace reasoning\ntask when it understands the underlying video content and learns representative\nspatio-temporal features. In addition, we further introduce contrastive\nlearning to push the model towards discriminating different paces by maximizing\nthe agreement on similar video content. To validate the effectiveness of the\nproposed method, we conduct extensive experiments on action recognition and\nvideo retrieval tasks with several alternative network architectures.\nExperimental evaluations show that our approach achieves state-of-the-art\nperformance for self-supervised video representation learning across different\nnetwork architectures and different benchmarks. The code and pre-trained models\nare available at https://github.com/laura-wang/video-pace.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 12:40:24 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 08:05:35 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Wang", "Jiangliu", ""], ["Jiao", "Jianbo", ""], ["Liu", "Yun-Hui", ""]]}, {"id": "2008.05865", "submitter": "Ming Tao", "authors": "Ming Tao, Hao Tang, Songsong Wu, Nicu Sebe, Xiao-Yuan Jing, Fei Wu,\n  Bingkun Bao", "title": "DF-GAN: Deep Fusion Generative Adversarial Networks for Text-to-Image\n  Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing high-quality realistic images from text descriptions is a\nchallenging task. Almost all existing text-to-image Generative Adversarial\nNetworks employ stacked architecture as the backbone. They utilize cross-modal\nattention mechanisms to fuse text and image features, and introduce extra\nnetworks to ensure text-image semantic consistency. In this work, we propose a\nmuch simpler, but more effective text-to-image model than previous works.\nCorresponding to the above three limitations, we propose: 1) a novel one-stage\ntext-to-image backbone which is able to synthesize high-quality images directly\nby one pair of generator and discriminator, 2) a novel fusion module called\ndeep text-image fusion block which deepens the text-image fusion process in\ngenerator, 3) a novel target-aware discriminator composed of matching-aware\ngradient penalty and one-way output which promotes the generator to synthesize\nmore realistic and text-image semantic consistent images without introducing\nextra networks. Compared with existing text-to-image models, our proposed\nmethod (i.e., DF-GAN) is simpler but more efficient to synthesize realistic and\ntext-matching images and achieves better performance. Extensive experiments on\nboth Caltech-UCSD Birds 200 and COCO datasets demonstrate the superiority of\nthe proposed model in comparison to state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 12:51:17 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 12:10:58 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Tao", "Ming", ""], ["Tang", "Hao", ""], ["Wu", "Songsong", ""], ["Sebe", "Nicu", ""], ["Jing", "Xiao-Yuan", ""], ["Wu", "Fei", ""], ["Bao", "Bingkun", ""]]}, {"id": "2008.05872", "submitter": "Peter Kan", "authors": "Anna Sebernegg, Peter K\\'an, Hannes Kaufmann", "title": "Motion Similarity Modeling -- A State of the Art Report", "comments": null, "journal-ref": null, "doi": null, "report-no": "VR-TR-001", "categories": "cs.GR cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of human motion opens up a wide range of possibilities, such as\nrealistic training simulations or authentic motions in robotics or animation.\nOne of the problems underlying motion analysis is the meaningful comparison of\nactions based on similarity measures. Since the motion analysis is\napplication-dependent, it is essential to find the appropriate motion\nsimilarity method for the particular use case. This state of the art report\nprovides an overview of human motion analysis and different similarity modeling\nmethods, while mainly focusing on approaches that work with 3D motion data. The\nsurvey summarizes various similarity aspects and features of motion and\ndescribes approaches to measuring the similarity between two actions.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 13:08:30 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Sebernegg", "Anna", ""], ["K\u00e1n", "Peter", ""], ["Kaufmann", "Hannes", ""]]}, {"id": "2008.05892", "submitter": "Quan Meng", "authors": "Quan Meng, Jiakai Zhang, Qiang Hu, Xuming He, Jingyi Yu", "title": "LGNN: A Context-aware Line Segment Detector", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": "10.1145/3394171.3413784", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel real-time line segment detection scheme called Line Graph\nNeural Network (LGNN). Existing approaches require a computationally expensive\nverification or postprocessing step. Our LGNN employs a deep convolutional\nneural network (DCNN) for proposing line segment directly, with a graph neural\nnetwork (GNN) module for reasoning their connectivities. Specifically, LGNN\nexploits a new quadruplet representation for each line segment where the GNN\nmodule takes the predicted candidates as vertexes and constructs a sparse graph\nto enforce structural context. Compared with the state-of-the-art, LGNN\nachieves near real-time performance without compromising accuracy. LGNN further\nenables time-sensitive 3D applications. When a 3D point cloud is accessible, we\npresent a multi-modal line segment classification technique for extracting a 3D\nwireframe of the environment robustly and efficiently.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 13:23:18 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2020 03:43:06 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Meng", "Quan", ""], ["Zhang", "Jiakai", ""], ["Hu", "Qiang", ""], ["He", "Xuming", ""], ["Yu", "Jingyi", ""]]}, {"id": "2008.05924", "submitter": "Xingxun Jiang", "authors": "Xingxun Jiang, Yuan Zong, Wenming Zheng, Chuangao Tang, Wanchuang Xia,\n  Cheng Lu, Jiateng Liu", "title": "DFEW: A Large-Scale Database for Recognizing Dynamic Facial Expressions\n  in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, facial expression recognition (FER) in the wild has gained a lot of\nresearchers' attention because it is a valuable topic to enable the FER\ntechniques to move from the laboratory to the real applications. In this paper,\nwe focus on this challenging but interesting topic and make contributions from\nthree aspects. First, we present a new large-scale 'in-the-wild' dynamic facial\nexpression database, DFEW (Dynamic Facial Expression in the Wild), consisting\nof over 16,000 video clips from thousands of movies. These video clips contain\nvarious challenging interferences in practical scenarios such as extreme\nillumination, occlusions, and capricious pose changes. Second, we propose a\nnovel method called Expression-Clustered Spatiotemporal Feature Learning\n(EC-STFL) framework to deal with dynamic FER in the wild. Third, we conduct\nextensive benchmark experiments on DFEW using a lot of spatiotemporal deep\nfeature learning methods as well as our proposed EC-STFL. Experimental results\nshow that DFEW is a well-designed and challenging database, and the proposed\nEC-STFL can promisingly improve the performance of existing spatiotemporal deep\nneural networks in coping with the problem of dynamic FER in the wild. Our DFEW\ndatabase is publicly available and can be freely downloaded from\nhttps://dfew-dataset.github.io/.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 14:10:05 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Jiang", "Xingxun", ""], ["Zong", "Yuan", ""], ["Zheng", "Wenming", ""], ["Tang", "Chuangao", ""], ["Xia", "Wanchuang", ""], ["Lu", "Cheng", ""], ["Liu", "Jiateng", ""]]}, {"id": "2008.05927", "submitter": "Bin Yang", "authors": "Lingyun Luke Li, Bin Yang, Ming Liang, Wenyuan Zeng, Mengye Ren, Sean\n  Segal, Raquel Urtasun", "title": "End-to-end Contextual Perception and Prediction with Interaction\n  Transformer", "comments": "IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we tackle the problem of detecting objects in 3D and\nforecasting their future motion in the context of self-driving. Towards this\ngoal, we design a novel approach that explicitly takes into account the\ninteractions between actors. To capture their spatial-temporal dependencies, we\npropose a recurrent neural network with a novel Transformer architecture, which\nwe call the Interaction Transformer. Importantly, our model can be trained\nend-to-end, and runs in real-time. We validate our approach on two challenging\nreal-world datasets: ATG4D and nuScenes. We show that our approach can\noutperform the state-of-the-art on both datasets. In particular, we\nsignificantly improve the social compliance between the estimated future\ntrajectories, resulting in far fewer collisions between the predicted actors.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 14:30:12 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Li", "Lingyun Luke", ""], ["Yang", "Bin", ""], ["Liang", "Ming", ""], ["Zeng", "Wenyuan", ""], ["Ren", "Mengye", ""], ["Segal", "Sean", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2008.05930", "submitter": "Sergio Casas", "authors": "Abbas Sadat, Sergio Casas, Mengye Ren, Xinyu Wu, Pranaab Dhawan,\n  Raquel Urtasun", "title": "Perceive, Predict, and Plan: Safe Motion Planning Through Interpretable\n  Semantic Representations", "comments": "European Conference on Computer Vision (ECCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel end-to-end learnable network that performs\njoint perception, prediction and motion planning for self-driving vehicles and\nproduces interpretable intermediate representations. Unlike existing neural\nmotion planners, our motion planning costs are consistent with our perception\nand prediction estimates. This is achieved by a novel differentiable semantic\noccupancy representation that is explicitly used as cost by the motion planning\nprocess. Our network is learned end-to-end from human demonstrations. The\nexperiments in a large-scale manual-driving dataset and closed-loop simulation\nshow that the proposed model significantly outperforms state-of-the-art\nplanners in imitating the human behaviors while producing much safer\ntrajectories.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 14:40:46 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Sadat", "Abbas", ""], ["Casas", "Sergio", ""], ["Ren", "Mengye", ""], ["Wu", "Xinyu", ""], ["Dhawan", "Pranaab", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2008.05938", "submitter": "Andrea Ceccarelli", "authors": "Francesco Secci, Andrea Ceccarelli", "title": "RGB cameras failures and their effects in autonomous driving\n  applications", "comments": "submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RGB cameras are one of the most relevant sensors for autonomous driving\napplications. It is undeniable that failures of vehicle cameras may compromise\nthe autonomous driving task, possibly leading to unsafe behaviors when images\nthat are subsequently processed by the driving system are altered. To support\nthe definition of safe and robust vehicle architectures and intelligent\nsystems, in this paper we define the failure modes of a vehicle camera,\ntogether with an analysis of effects and known mitigations. Further, we build a\nsoftware library for the generation of the corresponding failed images and we\nfeed them to six object detectors for mono and stereo cameras and to the\nself-driving agent of an autonomous driving simulator. The resulting\nmisbehaviors with respect to operating with clean images allow a better\nunderstanding of failures effects and the related safety risks in image-based\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 14:47:50 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 15:06:58 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Secci", "Francesco", ""], ["Ceccarelli", "Andrea", ""]]}, {"id": "2008.05948", "submitter": "Radu Tudor Ionescu", "authors": "Nicolae-C\\u{a}t\\u{a}lin Ristea, Andrei Anghel, Radu Tudor Ionescu", "title": "Estimating Magnitude and Phase of Automotive Radar Signals under\n  Multiple Interference Sources with Fully Convolutional Networks", "comments": "arXiv admin note: text overlap with arXiv:2007.11102", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radar sensors are gradually becoming a wide-spread equipment for road\nvehicles, playing a crucial role in autonomous driving and road safety. The\nbroad adoption of radar sensors increases the chance of interference among\nsensors from different vehicles, generating corrupted range profiles and\nrange-Doppler maps. In order to extract distance and velocity of multiple\ntargets from range-Doppler maps, the interference affecting each range profile\nneeds to be mitigated. In this paper, we propose a fully convolutional neural\nnetwork for automotive radar interference mitigation. In order to train our\nnetwork in a real-world scenario, we introduce a new data set of realistic\nautomotive radar signals with multiple targets and multiple interferers. To our\nknowledge, this is the first work to mitigate interference from multiple\nsources. Furthermore, we introduce a new training regime that eliminates noisy\nweights, showing superior results compared to the widely-used dropout. While\nsome previous works successfully estimated the magnitude of automotive radar\nsignals, we are the first to propose a deep learning model that can accurately\nestimate the phase. For instance, our novel approach reduces the phase\nestimation error with respect to the commonly-adopted zeroing technique by\nhalf, from 12.55 degrees to 6.58 degrees. Considering the lack of databases for\nautomotive radar interference mitigation, we release as open source our\nlarge-scale data set that closely replicates the real-world automotive scenario\nfor multiple interference cases, allowing others to objectively compare their\nfuture work in this domain. Our data set is available for download at:\nhttp://github.com/ristea/arim-v2.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 18:50:38 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Ristea", "Nicolae-C\u0103t\u0103lin", ""], ["Anghel", "Andrei", ""], ["Ionescu", "Radu Tudor", ""]]}, {"id": "2008.05955", "submitter": "Mona Jalal", "authors": "Mona Jalal, Josef Spjut, Ben Boudaoud, Margrit Betke", "title": "SIDOD: A Synthetic Image Dataset for 3D Object Pose Recognition with\n  Distractors", "comments": "3 pages, 4 figures, 1 table, Accepted at CVPR 2019 Workshop", "journal-ref": null, "doi": "10.1109/CVPRW.2019.00063", "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new, publicly-available image dataset generated by the NVIDIA\nDeep Learning Data Synthesizer intended for use in object detection, pose\nestimation, and tracking applications. This dataset contains 144k stereo image\npairs that synthetically combine 18 camera viewpoints of three photorealistic\nvirtual environments with up to 10 objects (chosen randomly from the 21 object\nmodels of the YCB dataset [1]) and flying distractors. Object and camera pose,\nscene lighting, and quantity of objects and distractors were randomized. Each\nprovided view includes RGB, depth, segmentation, and surface normal images, all\npixel level. We describe our approach for domain randomization and provide\ninsight into the decisions that produced the dataset.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 00:14:19 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Jalal", "Mona", ""], ["Spjut", "Josef", ""], ["Boudaoud", "Ben", ""], ["Betke", "Margrit", ""]]}, {"id": "2008.05975", "submitter": "Ruizhi Liao", "authors": "Steven Horng, Ruizhi Liao, Xin Wang, Sandeep Dalal, Polina Golland,\n  Seth J Berkowitz", "title": "Deep Learning to Quantify Pulmonary Edema in Chest Radiographs", "comments": "The two first authors contributed equally", "journal-ref": null, "doi": "10.1148/ryai.2021190228", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To develop a machine learning model to classify the severity grades\nof pulmonary edema on chest radiographs.\n  Materials and Methods: In this retrospective study, 369,071 chest radiographs\nand associated radiology reports from 64,581 (mean age, 51.71; 54.51% women)\npatients from the MIMIC-CXR chest radiograph dataset were included. This\ndataset was split into patients with and without congestive heart failure\n(CHF). Pulmonary edema severity labels from the associated radiology reports\nwere extracted from patients with CHF as four different ordinal levels: 0, no\nedema; 1, vascular congestion; 2, interstitial edema; and 3, alveolar edema.\nDeep learning models were developed using two approaches: a semi-supervised\nmodel using a variational autoencoder and a pre-trained supervised learning\nmodel using a dense neural network. Receiver operating characteristic curve\nanalysis was performed on both models.\n  Results: The area under the receiver operating characteristic curve (AUC) for\ndifferentiating alveolar edema from no edema was 0.99 for the semi-supervised\nmodel and 0.87 for the pre-trained models. Performance of the algorithm was\ninversely related to the difficulty in categorizing milder states of pulmonary\nedema (shown as AUCs for semi-supervised model and pre-trained model,\nrespectively): 2 versus 0, 0.88 and 0.81; 1 versus 0, 0.79 and 0.66; 3 versus\n1, 0.93 and 0.82; 2 versus 1, 0.69 and 0.73; and, 3 versus 2, 0.88 and 0.63.\n  Conclusion: Deep learning models were trained on a large chest radiograph\ndataset and could grade the severity of pulmonary edema on chest radiographs\nwith high performance.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 15:45:44 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 16:12:01 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Horng", "Steven", ""], ["Liao", "Ruizhi", ""], ["Wang", "Xin", ""], ["Dalal", "Sandeep", ""], ["Golland", "Polina", ""], ["Berkowitz", "Seth J", ""]]}, {"id": "2008.05977", "submitter": "Ling-An Zeng", "authors": "Ling-An Zeng, Fa-Ting Hong, Wei-Shi Zheng, Qi-Zhi Yu, Wei Zeng,\n  Yao-Wei Wang, and Jian-Huang Lai", "title": "Hybrid Dynamic-static Context-aware Attention Network for Action\n  Assessment in Long Videos", "comments": "ACM International Conference on Multimedia 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of action quality assessment is to score sports videos.\nHowever, most existing works focus only on video dynamic information (i.e.,\nmotion information) but ignore the specific postures that an athlete is\nperforming in a video, which is important for action assessment in long videos.\nIn this work, we present a novel hybrid dynAmic-static Context-aware attenTION\nNETwork (ACTION-NET) for action assessment in long videos. To learn more\ndiscriminative representations for videos, we not only learn the video dynamic\ninformation but also focus on the static postures of the detected athletes in\nspecific frames, which represent the action quality at certain moments, along\nwith the help of the proposed hybrid dynamic-static architecture. Moreover, we\nleverage a context-aware attention module consisting of a temporal\ninstance-wise graph convolutional network unit and an attention unit for both\nstreams to extract more robust stream features, where the former is for\nexploring the relations between instances and the latter for assigning a proper\nweight to each instance. Finally, we combine the features of the two streams to\nregress the final video score, supervised by ground-truth scores given by\nexperts. Additionally, we have collected and annotated the new Rhythmic\nGymnastics dataset, which contains videos of four different types of gymnastics\nroutines, for evaluation of action quality assessment in long videos. Extensive\nexperimental results validate the efficacy of our proposed method, which\noutperforms related approaches. The codes and dataset are available at\n\\url{https://github.com/lingan1996/ACTION-NET}.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 15:51:42 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zeng", "Ling-An", ""], ["Hong", "Fa-Ting", ""], ["Zheng", "Wei-Shi", ""], ["Yu", "Qi-Zhi", ""], ["Zeng", "Wei", ""], ["Wang", "Yao-Wei", ""], ["Lai", "Jian-Huang", ""]]}, {"id": "2008.05981", "submitter": "Ziqi Wang", "authors": "Kanav Anand, Ziqi Wang, Marco Loog, Jan van Gemert", "title": "Black Magic in Deep Learning: How Human Skill Impacts Network Training", "comments": "presented at the British Machine Vision Conference, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How does a user's prior experience with deep learning impact accuracy? We\npresent an initial study based on 31 participants with different levels of\nexperience. Their task is to perform hyperparameter optimization for a given\ndeep learning architecture. The results show a strong positive correlation\nbetween the participant's experience and the final performance. They\nadditionally indicate that an experienced participant finds better solutions\nusing fewer resources on average. The data suggests furthermore that\nparticipants with no prior experience follow random strategies in their pursuit\nof optimal hyperparameters. Our study investigates the subjective human factor\nin comparisons of state of the art results and scientific reproducibility in\ndeep learning.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 15:56:14 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Anand", "Kanav", ""], ["Wang", "Ziqi", ""], ["Loog", "Marco", ""], ["van Gemert", "Jan", ""]]}, {"id": "2008.06020", "submitter": "Kelvin Wong", "authors": "Kelvin Wong, Qiang Zhang, Ming Liang, Bin Yang, Renjie Liao, Abbas\n  Sadat, Raquel Urtasun", "title": "Testing the Safety of Self-driving Vehicles by Simulating Perception and\n  Prediction", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for testing the safety of self-driving vehicles in\nsimulation. We propose an alternative to sensor simulation, as sensor\nsimulation is expensive and has large domain gaps. Instead, we directly\nsimulate the outputs of the self-driving vehicle's perception and prediction\nsystem, enabling realistic motion planning testing. Specifically, we use paired\ndata in the form of ground truth labels and real perception and prediction\noutputs to train a model that predicts what the online system will produce.\nImportantly, the inputs to our system consists of high definition maps,\nbounding boxes, and trajectories, which can be easily sketched by a test\nengineer in a matter of minutes. This makes our approach a much more scalable\nsolution. Quantitative results on two large-scale datasets demonstrate that we\ncan realistically test motion planning using our simulations.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 17:20:02 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Wong", "Kelvin", ""], ["Zhang", "Qiang", ""], ["Liang", "Ming", ""], ["Yang", "Bin", ""], ["Liao", "Renjie", ""], ["Sadat", "Abbas", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2008.06021", "submitter": "Matteo Testa", "authors": "Arslan Ali, Matteo Testa, Tiziano Bianchi, Enrico Magli", "title": "BioMetricNet: deep unconstrained face verification through learning of\n  metrics regularized onto Gaussian distributions", "comments": "Accepted at ECCV20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present BioMetricNet: a novel framework for deep unconstrained face\nverification which learns a regularized metric to compare facial features.\nDifferently from popular methods such as FaceNet, the proposed approach does\nnot impose any specific metric on facial features; instead, it shapes the\ndecision space by learning a latent representation in which matching and\nnon-matching pairs are mapped onto clearly separated and well-behaved target\ndistributions. In particular, the network jointly learns the best feature\nrepresentation, and the best metric that follows the target distributions, to\nbe used to discriminate face images. In this paper we present this general\nframework, first of its kind for facial verification, and tailor it to Gaussian\ndistributions. This choice enables the use of a simple linear decision boundary\nthat can be tuned to achieve the desired trade-off between false alarm and\ngenuine acceptance rate, and leads to a loss function that can be written in\nclosed form. Extensive analysis and experimentation on publicly available\ndatasets such as Labeled Faces in the wild (LFW), Youtube faces (YTF),\nCelebrities in Frontal-Profile in the Wild (CFP), and challenging datasets like\ncross-age LFW (CALFW), cross-pose LFW (CPLFW), In-the-wild Age Dataset (AgeDB)\nshow a significant performance improvement and confirms the effectiveness and\nsuperiority of BioMetricNet over existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 17:22:46 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Ali", "Arslan", ""], ["Testa", "Matteo", ""], ["Bianchi", "Tiziano", ""], ["Magli", "Enrico", ""]]}, {"id": "2008.06029", "submitter": "Burhaneddin Yaman", "authors": "Burhaneddin Yaman, Seyed Amir Hossein Hosseini, Steen Moeller, Jutta\n  Ellermann, K\\^amil U\\u{g}urbil, Mehmet Ak\\c{c}akaya", "title": "Multi-Mask Self-Supervised Learning for Physics-Guided Neural Networks\n  in Highly Accelerated MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To develop an improved self-supervised learning strategy that\nefficiently uses the acquired data for training a physics-guided reconstruction\nnetwork without a database of fully-sampled data.\n  Methods: Currently self-supervised learning for physics-guided reconstruction\nnetworks splits acquired undersampled data into two disjoint sets, where one is\nused for data consistency (DC) in the unrolled network and the other to define\nthe training loss. The proposed multi-mask self-supervised learning via data\nundersampling (SSDU) splits acquired measurements into multiple pairs of\ndisjoint sets for each training sample, while using one of these sets for DC\nunits and the other for defining loss, thereby more efficiently using the\nundersampled data. Multi-mask SSDU is applied on fully-sampled 3D knee and\nprospectively undersampled 3D brain MRI datasets, which are retrospectively\nsubsampled to acceleration rate (R)=8, and compared to CG-SENSE and single-mask\nSSDU DL-MRI, as well as supervised DL-MRI when fully-sampled data is available.\n  Results: Results on knee MRI show that the proposed multi-mask SSDU\noutperforms SSDU and performs closely with supervised DL-MRI, while\nsignificantly outperforming CG-SENSE. A clinical reader study further ranks the\nmulti-mask SSDU higher than supervised DL-MRI in terms of SNR and aliasing\nartifacts. Results on brain MRI show that multi-mask SSDU achieves better\nreconstruction quality compared to SSDU and CG-SENSE. Reader study demonstrates\nthat multi-mask SSDU at R=8 significantly improves reconstruction compared to\nsingle-mask SSDU at R=8, as well as CG-SENSE at R=2.\n  Conclusion: The proposed multi-mask SSDU approach enables improved training\nof physics-guided neural networks without fully-sampled data, by enabling\nefficient use of the undersampled data with multiple masks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 17:36:59 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Yaman", "Burhaneddin", ""], ["Hosseini", "Seyed Amir Hossein", ""], ["Moeller", "Steen", ""], ["Ellermann", "Jutta", ""], ["U\u011furbil", "K\u00e2mil", ""], ["Ak\u00e7akaya", "Mehmet", ""]]}, {"id": "2008.06035", "submitter": "Srikrishna Karanam", "authors": "Meng Zheng and Srikrishna Karanam and Terrence Chen and Richard J.\n  Radke and Ziyan Wu", "title": "Towards Visually Explaining Similarity Models", "comments": "13 pages, 10 figures, 4 tables. arXiv admin note: substantial text\n  overlap with arXiv:1911.07381", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of visually explaining similarity models, i.e.,\nexplaining why a model predicts two images to be similar in addition to\nproducing a scalar score. While much recent work in visual model\ninterpretability has focused on gradient-based attention, these methods rely on\na classification module to generate visual explanations. Consequently, they\ncannot readily explain other kinds of models that do not use or need\nclassification-like loss functions (e.g., similarity models trained with a\nmetric learning loss). In this work, we bridge this crucial gap, presenting a\nmethod to generate gradient-based visual attention for image similarity\npredictors. By relying solely on the learned feature embedding, we show that\nour approach can be applied to any kind of CNN-based similarity architecture,\nan important step towards generic visual explainability. We show that our\nresulting attention maps serve more than just interpretability; they can be\ninfused into the model learning process itself with new trainable constraints.\nWe show that the resulting similarity models perform, and can be visually\nexplained, better than the corresponding baseline models trained without these\nconstraints. We demonstrate our approach using extensive experiments on three\ndifferent kinds of tasks: generic image retrieval, person re-identification,\nand low-shot semantic segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 17:47:41 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 17:00:38 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Zheng", "Meng", ""], ["Karanam", "Srikrishna", ""], ["Chen", "Terrence", ""], ["Radke", "Richard J.", ""], ["Wu", "Ziyan", ""]]}, {"id": "2008.06041", "submitter": "Wenyuan Zeng", "authors": "Wenyuan Zeng, Shenlong Wang, Renjie Liao, Yun Chen, Bin Yang, Raquel\n  Urtasun", "title": "DSDNet: Deep Structured self-Driving Network", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the Deep Structured self-Driving Network (DSDNet),\nwhich performs object detection, motion prediction, and motion planning with a\nsingle neural network. Towards this goal, we develop a deep structured energy\nbased model which considers the interactions between actors and produces\nsocially consistent multimodal future predictions. Furthermore, DSDNet\nexplicitly exploits the predicted future distributions of actors to plan a safe\nmaneuver by using a structured planning cost. Our sample-based formulation\nallows us to overcome the difficulty in probabilistic inference of continuous\nrandom variables. Experiments on a number of large-scale self driving datasets\ndemonstrate that our model significantly outperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 17:54:06 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Zeng", "Wenyuan", ""], ["Wang", "Shenlong", ""], ["Liao", "Renjie", ""], ["Chen", "Yun", ""], ["Yang", "Bin", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2008.06046", "submitter": "Chris Rockwell", "authors": "Chris Rockwell, David F. Fouhey", "title": "Full-Body Awareness from Partial Observations", "comments": "In ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been great progress in human 3D mesh recovery and great interest in\nlearning about the world from consumer video data. Unfortunately current\nmethods for 3D human mesh recovery work rather poorly on consumer video data,\nsince on the Internet, unusual camera viewpoints and aggressive truncations are\nthe norm rather than a rarity. We study this problem and make a number of\ncontributions to address it: (i) we propose a simple but highly effective\nself-training framework that adapts human 3D mesh recovery systems to consumer\nvideos and demonstrate its application to two recent systems; (ii) we introduce\nevaluation protocols and keypoint annotations for 13K frames across four\nconsumer video datasets for studying this task, including evaluations on\nout-of-image keypoints; and (iii) we show that our method substantially\nimproves PCK and human-subject judgments compared to baselines, both on test\nvideos from the dataset it was trained on, as well as on three other datasets\nwithout further adaptation. Project website:\nhttps://crockwell.github.io/partial_humans\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 17:59:11 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Rockwell", "Chris", ""], ["Fouhey", "David F.", ""]]}, {"id": "2008.06069", "submitter": "Ali Shahin Shamsabadi", "authors": "Ali Shahin Shamsabadi, Changjae Oh, Andrea Cavallaro", "title": "Semantically Adversarial Learnable Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an adversarial framework to craft perturbations that mislead\nclassifiers by accounting for the image content and the semantics of the\nlabels. The proposed framework combines a structure loss and a semantic\nadversarial loss in a multi-task objective function to train a fully\nconvolutional neural network. The structure loss helps generate perturbations\nwhose type and magnitude are defined by a target image processing filter. The\nsemantic adversarial loss considers groups of (semantic) labels to craft\nperturbations that prevent the filtered image being classified with a label in\nthe same group. We validate our framework by selecting as target filters detail\nenhancement, log transformation and gamma correction filters, and evaluate the\nadversarially filtered images against three classifiers, ResNet50, ResNet18 and\nAlexNet, pre-trained on ImageNet. We show that the proposed framework generates\nfiltered images with a high success rate, robustness, and transferability to\nunseen classifiers.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 18:12:40 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 18:12:06 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Shamsabadi", "Ali Shahin", ""], ["Oh", "Changjae", ""], ["Cavallaro", "Andrea", ""]]}, {"id": "2008.06072", "submitter": "Arash Mohammadi", "authors": "Parnian Afshar, Farnoosh Naderkhani, Anastasia Oikonomou, Moezedin\n  Javad Rafiee, Arash Mohammadi, and Konstantinos N. Plataniotis", "title": "MIXCAPS: A Capsule Network-based Mixture of Experts for Lung Nodule\n  Malignancy Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung diseases including infections such as Pneumonia, Tuberculosis, and novel\nCoronavirus (COVID-19), together with Lung Cancer are significantly widespread\nand are, typically, considered life threatening. In particular, lung cancer is\namong the most common and deadliest cancers with a low 5-year survival rate.\nTimely diagnosis of lung cancer is, therefore, of paramount importance as it\ncan save countless lives. In this regard, deep learning radiomics solutions\nhave the promise of extracting the most useful features on their own in an\nend-to-end fashion without having access to the annotated boundaries. Among\ndifferent deep learning models, Capsule Networks are proposed to overcome\nshortcomings of the Convolutional Neural Networks (CNN) such as their inability\nto recognize detailed spatial relations. Capsule networks have so far shown\nsatisfying performance in medical imaging problems. Capitalizing on their\nsuccess, in this study, we propose a novel capsule network-based mixture of\nexperts, referred to as the MIXCAPS. The proposed MIXCAPS architecture takes\nadvantage of not only the capsule network's capabilities to handle small\ndatasets, but also automatically splitting dataset through a convolutional\ngating network. MIXCAPS enables capsule network experts to specialize on\ndifferent subsets of the data. Our results show that MIXCAPS outperforms a\nsingle capsule network and a mixture of CNNs, with an accuracy of 92.88%,\nsensitivity of 93.2%, specificity of 92.3% and area under the curve of 0.963.\nOur experiments also show that there is a relation between the gate outputs and\na couple of hand-crafted features, illustrating explainable nature of the\nproposed MIXCAPS. To further evaluate generalization capabilities of the\nproposed MIXCAPS architecture, additional experiments on a brain tumor dataset\nare performed showing potentials of MIXCAPS for detection of tumors related to\nother organs.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 18:16:07 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Afshar", "Parnian", ""], ["Naderkhani", "Farnoosh", ""], ["Oikonomou", "Anastasia", ""], ["Rafiee", "Moezedin Javad", ""], ["Mohammadi", "Arash", ""], ["Plataniotis", "Konstantinos N.", ""]]}, {"id": "2008.06094", "submitter": "Gukyeong Kwon", "authors": "Gukyeong Kwon, Mohit Prabhushankar, Dogancan Temel, Ghassan AlRegib", "title": "Novelty Detection Through Model-Based Characterization of Neural\n  Networks", "comments": "IEEE International Conference on Image Processing (ICIP) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a model-based characterization of neural networks\nto detect novel input types and conditions. Novelty detection is crucial to\nidentify abnormal inputs that can significantly degrade the performance of\nmachine learning algorithms. Majority of existing studies have focused on\nactivation-based representations to detect abnormal inputs, which limits the\ncharacterization of abnormality from a data perspective. However, a model\nperspective can also be informative in terms of the novelties and\nabnormalities. To articulate the significance of the model perspective in\nnovelty detection, we utilize backpropagated gradients. We conduct a\ncomprehensive analysis to compare the representation capability of gradients\nwith that of activation and show that the gradients outperform the activation\nin novel class and condition detection. We validate our approach using four\nimage recognition datasets including MNIST, Fashion-MNIST, CIFAR-10, and\nCURE-TSR. We achieve a significant improvement on all four datasets with an\naverage AUROC of 0.953, 0.918, 0.582, and 0.746, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 20:03:25 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Kwon", "Gukyeong", ""], ["Prabhushankar", "Mohit", ""], ["Temel", "Dogancan", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "2008.06098", "submitter": "Amir Alansary", "authors": "Vitalis Vosylius, Andy Wang, Cemlyn Waters, Alexey Zakharov, Francis\n  Ward, Loic Le Folgoc, John Cupitt, Antonios Makropoulos, Andreas Schuh,\n  Daniel Rueckert, Amir Alansary", "title": "Geometric Deep Learning for Post-Menstrual Age Prediction based on the\n  Neonatal White Matter Cortical Surface", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate estimation of the age in neonates is essential for measuring\nneurodevelopmental, medical, and growth outcomes. In this paper, we propose a\nnovel approach to predict the post-menstrual age (PA) at scan, using techniques\nfrom geometric deep learning, based on the neonatal white matter cortical\nsurface. We utilize and compare multiple specialized neural network\narchitectures that predict the age using different geometric representations of\nthe cortical surface; we compare MeshCNN, Pointnet++, GraphCNN, and a\nvolumetric benchmark. The dataset is part of the Developing Human Connectome\nProject (dHCP), and is a cohort of healthy and premature neonates. We evaluate\nour approach on 650 subjects (727scans) with PA ranging from 27 to 45 weeks.\nOur results show accurate prediction of the estimated PA, with mean error less\nthan one week.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 20:15:03 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 22:01:44 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Vosylius", "Vitalis", ""], ["Wang", "Andy", ""], ["Waters", "Cemlyn", ""], ["Zakharov", "Alexey", ""], ["Ward", "Francis", ""], ["Folgoc", "Loic Le", ""], ["Cupitt", "John", ""], ["Makropoulos", "Antonios", ""], ["Schuh", "Andreas", ""], ["Rueckert", "Daniel", ""], ["Alansary", "Amir", ""]]}, {"id": "2008.06106", "submitter": "Akin Yilmaz", "authors": "M. Akin Yilmaz and A. Murat Tekalp", "title": "Effect of Architectures and Training Methods on the Performance of\n  Learned Video Frame Prediction", "comments": "Accepted for publication at IEEE ICIP 2019", "journal-ref": null, "doi": "10.1109/ICIP.2019.8803624", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the performance of feedforward vs. recurrent neural network (RNN)\narchitectures and associated training methods for learned frame prediction. To\nthis effect, we trained a residual fully convolutional neural network (FCNN), a\nconvolutional RNN (CRNN), and a convolutional long short-term memory (CLSTM)\nnetwork for next frame prediction using the mean square loss. We performed both\nstateless and stateful training for recurrent networks. Experimental results\nshow that the residual FCNN architecture performs the best in terms of peak\nsignal to noise ratio (PSNR) at the expense of higher training and test\n(inference) computational complexity. The CRNN can be trained stably and very\nefficiently using the stateful truncated backpropagation through time\nprocedure, and it requires an order of magnitude less inference runtime to\nachieve near real-time frame prediction with an acceptable performance.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 20:45:28 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Yilmaz", "M. Akin", ""], ["Tekalp", "A. Murat", ""]]}, {"id": "2008.06120", "submitter": "Gabriel Bender", "authors": "Gabriel Bender, Hanxiao Liu, Bo Chen, Grace Chu, Shuyang Cheng,\n  Pieter-Jan Kindermans, Quoc Le", "title": "Can weight sharing outperform random architecture search? An\n  investigation with TuNAS", "comments": "Published at CVPR 2020", "journal-ref": "Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2020, pp. 14323-14332", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient Neural Architecture Search methods based on weight sharing have\nshown good promise in democratizing Neural Architecture Search for computer\nvision models. There is, however, an ongoing debate whether these efficient\nmethods are significantly better than random search. Here we perform a thorough\ncomparison between efficient and random search methods on a family of\nprogressively larger and more challenging search spaces for image\nclassification and detection on ImageNet and COCO. While the efficacies of both\nmethods are problem-dependent, our experiments demonstrate that there are\nlarge, realistic tasks where efficient search methods can provide substantial\ngains over random search. In addition, we propose and evaluate techniques which\nimprove the quality of searched architectures and reduce the need for manual\nhyper-parameter tuning.\n  Source code and experiment data are available at\nhttps://github.com/google-research/google-research/tree/master/tunas\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 21:32:40 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Bender", "Gabriel", ""], ["Liu", "Hanxiao", ""], ["Chen", "Bo", ""], ["Chu", "Grace", ""], ["Cheng", "Shuyang", ""], ["Kindermans", "Pieter-Jan", ""], ["Le", "Quoc", ""]]}, {"id": "2008.06133", "submitter": "Marc Badger", "authors": "Marc Badger, Yufu Wang, Adarsh Modh, Ammon Perkes, Nikos Kolotouros,\n  Bernd G. Pfrommer, Marc F. Schmidt, Kostas Daniilidis", "title": "3D Bird Reconstruction: a Dataset, Model, and Shape Recovery from a\n  Single View", "comments": "In ECCV 2020", "journal-ref": "ECCV 2020, vol 12363, pp 1-17", "doi": "10.1007/978-3-030-58523-5_1", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated capture of animal pose is transforming how we study neuroscience\nand social behavior. Movements carry important social cues, but current methods\nare not able to robustly estimate pose and shape of animals, particularly for\nsocial animals such as birds, which are often occluded by each other and\nobjects in the environment. To address this problem, we first introduce a model\nand multi-view optimization approach, which we use to capture the unique shape\nand pose space displayed by live birds. We then introduce a pipeline and\nexperiments for keypoint, mask, pose, and shape regression that recovers\naccurate avian postures from single views. Finally, we provide extensive\nmulti-view keypoint and mask annotations collected from a group of 15 social\nbirds housed together in an outdoor aviary. The project website with videos,\nresults, code, mesh model, and the Penn Aviary Dataset can be found at\nhttps://marcbadger.github.io/avian-mesh.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 23:29:04 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Badger", "Marc", ""], ["Wang", "Yufu", ""], ["Modh", "Adarsh", ""], ["Perkes", "Ammon", ""], ["Kolotouros", "Nikos", ""], ["Pfrommer", "Bernd G.", ""], ["Schmidt", "Marc F.", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "2008.06142", "submitter": "Hui Xue PhD", "authors": "Hui Xue, Jessica Artico, Marianna Fontana, James C Moon, Rhodri H\n  Davies, Peter Kellman", "title": "Landmark detection in Cardiac Magnetic Resonance Imaging Using A\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To develop a convolutional neural network (CNN) solution for robust\nlandmark detection in cardiac MR images.\n  Methods: This retrospective study included cine, LGE and T1 mapping scans\nfrom two hospitals. The training set included 2,329 patients and 34,019 images.\nA hold-out test set included 531 patients and 7,723 images. CNN models were\ndeveloped to detect two mitral valve plane and apical points on long-axis (LAX)\nimages. On short-axis (SAX) images, anterior and posterior RV insertion points\nand LV center were detected. Model outputs were compared to manual labels by\ntwo operators for accuracy with a t-test for statistical significance. The\ntrained model was deployed to MR scanners.\n  Results: For the LAX images, success detection was 99.8% for cine, 99.4% for\nLGE. For the SAX, success rate was 96.6%, 97.6% and 98.9% for cine, LGE and\nT1-mapping. The L2 distances between model and manual labels were 2 to 3.5 mm,\nindicating close agreement between model landmarks to manual labels. No\nsignificant differences were found for the anterior RV insertion angle and LV\nlength by the models and operators for all views and imaging sequences. Model\ninference on MR scanner took 610ms/5.6s on GPU/CPU, respectively, for a typical\ncardiac cine series.\n  Conclusions: This study developed, validated and deployed a CNN solution for\nrobust landmark detection in both long and short-axis CMR images for cine, LGE\nand T1 mapping sequences, with the accuracy comparable to the inter-operator\nvariation.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 00:25:59 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Xue", "Hui", ""], ["Artico", "Jessica", ""], ["Fontana", "Marianna", ""], ["Moon", "James C", ""], ["Davies", "Rhodri H", ""], ["Kellman", "Peter", ""]]}, {"id": "2008.06151", "submitter": "Emanuel Azcona", "authors": "Emanuel A. Azcona, Pierre Besson, Yunan Wu, Arjun Punjabi, Adam\n  Martersteck, Amil Dravid, Todd B. Parrish, S. Kathleen Bandt, Aggelos K.\n  Katsaggelos", "title": "Interpretation of Brain Morphology in Association to Alzheimer's Disease\n  Dementia Classification Using Graph Convolutional Networks on Triangulated\n  Meshes", "comments": "Accepted for the Shape in Medical Imaging (ShapeMI) workshop at\n  MICCAI International Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG math.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a mesh-based technique to aid in the classification of Alzheimer's\ndisease dementia (ADD) using mesh representations of the cortex and subcortical\nstructures. Deep learning methods for classification tasks that utilize\nstructural neuroimaging often require extensive learning parameters to\noptimize. Frequently, these approaches for automated medical diagnosis also\nlack visual interpretability for areas in the brain involved in making a\ndiagnosis. This work: (a) analyzes brain shape using surface information of the\ncortex and subcortical structures, (b) proposes a residual learning framework\nfor state-of-the-art graph convolutional networks which offer a significant\nreduction in learnable parameters, and (c) offers visual interpretability of\nthe network via class-specific gradient information that localizes important\nregions of interest in our inputs. With our proposed method leveraging the use\nof cortical and subcortical surface information, we outperform other machine\nlearning methods with a 96.35% testing accuracy for the ADD vs. healthy control\nproblem. We confirm the validity of our model by observing its performance in a\n25-trial Monte Carlo cross-validation. The generated visualization maps in our\nstudy show correspondences with current knowledge regarding the structural\nlocalization of pathological changes in the brain associated to dementia of the\nAlzheimer's type.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 01:10:39 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 23:59:56 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 06:58:20 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Azcona", "Emanuel A.", ""], ["Besson", "Pierre", ""], ["Wu", "Yunan", ""], ["Punjabi", "Arjun", ""], ["Martersteck", "Adam", ""], ["Dravid", "Amil", ""], ["Parrish", "Todd B.", ""], ["Bandt", "S. Kathleen", ""], ["Katsaggelos", "Aggelos K.", ""]]}, {"id": "2008.06164", "submitter": "Rihuan Ke", "authors": "Rihuan Ke and Carola-Bibiane Sch\\\"onlieb", "title": "Unsupervised Image Restoration Using Partially Linear Denoisers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network based methods are the state of the art in various image\nrestoration problems. Standard supervised learning frameworks require a set of\nnoisy measurement and clean image pairs for which a distance between the output\nof the restoration model and the ground truth, clean images is minimized. The\nground truth images, however, are often unavailable or very expensive to\nacquire in real-world applications. We circumvent this problem by proposing a\nclass of structured denoisers that can be decomposed as the sum of a nonlinear\nimage-dependent mapping, a linear noise-dependent term and a small residual\nterm. We show that these denoisers can be trained with only noisy images under\nthe condition that the noise has zero mean and known variance. The exact\ndistribution of the noise, however, is not assumed to be known. We show the\nsuperiority of our approach for image denoising, and demonstrate its extension\nto solving other restoration problems such as blind deblurring where the ground\ntruth is not available. Our method outperforms some recent unsupervised and\nself-supervised deep denoising models that do not require clean images for\ntheir training. For blind deblurring problems, the method, using only one noisy\nand blurry observation per image, reaches a quality not far away from its fully\nsupervised counterparts on a benchmark dataset.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 02:13:19 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 13:56:06 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Ke", "Rihuan", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "2008.06179", "submitter": "Ye Bi", "authors": "Ye Bi, Shuo Wang, Zhongrui Fan", "title": "A Multimodal Late Fusion Model for E-Commerce Product Classification", "comments": "4 pages, SIGIR 2020 E-commerce Workshop Data Challenge Technical\n  Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The cataloging of product listings is a fundamental problem for most\ne-commerce platforms. Despite promising results obtained by unimodal-based\nmethods, it can be expected that their performance can be further boosted by\nthe consideration of multimodal product information. In this study, we\ninvestigated a multimodal late fusion approach based on text and image\nmodalities to categorize e-commerce products on Rakuten. Specifically, we\ndeveloped modal specific state-of-the-art deep neural networks for each input\nmodal, and then fused them at the decision level. Experimental results on\nMultimodal Product Classification Task of SIGIR 2020 E-Commerce Workshop Data\nChallenge demonstrate the superiority and effectiveness of our proposed method\ncompared with unimodal and other multimodal methods. Our team named pa_curis\nwon the 1st place with a macro-F1 of 0.9144 on the final leaderboard.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 03:46:24 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Bi", "Ye", ""], ["Wang", "Shuo", ""], ["Fan", "Zhongrui", ""]]}, {"id": "2008.06181", "submitter": "Zhengxu Yu", "authors": "Zhengxu Yu, Yilun Zhao, Bin Hong, Zhongming Jin, Jianqiang Huang, Deng\n  Cai, Xiaofei He, Xian-Sheng Hua", "title": "Apparel-invariant Feature Learning for Apparel-changed Person\n  Re-identification", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of deep learning methods, person Re-Identification (ReID)\nperformance has been improved tremendously in many public datasets. However,\nmost public ReID datasets are collected in a short time window in which\npersons' appearance rarely changes. In real-world applications such as in a\nshopping mall, the same person's clothing may change, and different persons may\nwearing similar clothes. All these cases can result in an inconsistent ReID\nperformance, revealing a critical problem that current ReID models heavily rely\non person's apparels. Therefore, it is critical to learn an apparel-invariant\nperson representation under cases like cloth changing or several persons\nwearing similar clothes. In this work, we tackle this problem from the\nviewpoint of invariant feature representation learning. The main contributions\nof this work are as follows. (1) We propose the semi-supervised\nApparel-invariant Feature Learning (AIFL) framework to learn an\napparel-invariant pedestrian representation using images of the same person\nwearing different clothes. (2) To obtain images of the same person wearing\ndifferent clothes, we propose an unsupervised apparel-simulation GAN (AS-GAN)\nto synthesize cloth changing images according to the target cloth embedding.\nIt's worth noting that the images used in ReID tasks were cropped from\nreal-world low-quality CCTV videos, making it more challenging to synthesize\ncloth changing images. We conduct extensive experiments on several datasets\ncomparing with several baselines. Experimental results demonstrate that our\nproposal can improve the ReID performance of the baseline models.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 03:49:14 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 03:14:12 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Yu", "Zhengxu", ""], ["Zhao", "Yilun", ""], ["Hong", "Bin", ""], ["Jin", "Zhongming", ""], ["Huang", "Jianqiang", ""], ["Cai", "Deng", ""], ["He", "Xiaofei", ""], ["Hua", "Xian-Sheng", ""]]}, {"id": "2008.06189", "submitter": "Syed Ali Hassan", "authors": "Syed Ali Hassan, Tariq Rahim, Soo Young Shin", "title": "An Improved Deep Convolutional Neural Network-Based Autonomous Road\n  Inspection Scheme Using Unmanned Aerial Vehicles", "comments": "10 pages, 11 figures, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advancements in artificial intelligence (AI) gives a great opportunity to\ndevelop an autonomous devices. The contribution of this work is an improved\nconvolutional neural network (CNN) model and its implementation for the\ndetection of road cracks, potholes, and yellow lane in the road. The purpose of\nyellow lane detection and tracking is to realize autonomous navigation of\nunmanned aerial vehicle (UAV) by following yellow lane while detecting and\nreporting the road cracks and potholes to the server through WIFI or 5G medium.\nThe fabrication of own data set is a hectic and time-consuming task. The data\nset is created, labeled and trained using default and an improved model. The\nperformance of both these models is benchmarked with respect to accuracy, mean\naverage precision (mAP) and detection time. In the testing phase, it was\nobserved that the performance of the improved model is better in respect of\naccuracy and mAP. The improved model is implemented in UAV using the robot\noperating system for the autonomous detection of potholes and cracks in roads\nvia UAV front camera vision in real-time.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 04:35:10 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Hassan", "Syed Ali", ""], ["Rahim", "Tariq", ""], ["Shin", "Soo Young", ""]]}, {"id": "2008.06204", "submitter": "Wensheng Cheng", "authors": "Wensheng Cheng, Hao Luo, Wen Yang, Lei Yu, Wei Li", "title": "Structure-Aware Network for Lane Marker Extraction with Dynamic Vision\n  Sensor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lane marker extraction is a basic yet necessary task for autonomous driving.\nAlthough past years have witnessed major advances in lane marker extraction\nwith deep learning models, they all aim at ordinary RGB images generated by\nframe-based cameras, which limits their performance in extreme cases, like huge\nillumination change. To tackle this problem, we introduce Dynamic Vision Sensor\n(DVS), a type of event-based sensor to lane marker extraction task and build a\nhigh-resolution DVS dataset for lane marker extraction. We collect the raw\nevent data and generate 5,424 DVS images with a resolution of 1280$\\times$800\npixels, the highest one among all DVS datasets available now. All images are\nannotated with multi-class semantic segmentation format. We then propose a\nstructure-aware network for lane marker extraction in DVS images. It can\ncapture directional information comprehensively with multidirectional slice\nconvolution. We evaluate our proposed network with other state-of-the-art lane\nmarker extraction models on this dataset. Experimental results demonstrate that\nour method outperforms other competitors. The dataset is made publicly\navailable, including the raw event data, accumulated images and labels.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 06:28:20 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Cheng", "Wensheng", ""], ["Luo", "Hao", ""], ["Yang", "Wen", ""], ["Yu", "Lei", ""], ["Li", "Wei", ""]]}, {"id": "2008.06223", "submitter": "Haijun Liu", "authors": "Haijun Liu, Xiaoheng Tan and Xichuan Zhou", "title": "Parameter Sharing Exploration and Hetero-Center based Triplet Loss for\n  Visible-Thermal Person Re-Identification", "comments": "to be published in IEEE Transactions on Multimedia", "journal-ref": null, "doi": "10.1109/TMM.2020.3042080", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the visible-thermal cross-modality person\nre-identification (VT Re-ID) task, whose goal is to match person images between\nthe daytime visible modality and the nighttime thermal modality. The two-stream\nnetwork is usually adopted to address the cross-modality discrepancy, the most\nchallenging problem for VT Re-ID, by learning the multi-modality person\nfeatures. In this paper, we explore how many parameters of two-stream network\nshould share, which is still not well investigated in the existing literature.\nBy well splitting the ResNet50 model to construct the modality-specific feature\nextracting network and modality-sharing feature embedding network, we\nexperimentally demonstrate the effect of parameters sharing of two-stream\nnetwork for VT Re-ID. Moreover, in the framework of part-level person feature\nlearning, we propose the hetero-center based triplet loss to relax the strict\nconstraint of traditional triplet loss through replacing the comparison of\nanchor to all the other samples by anchor center to all the other centers. With\nthe extremely simple means, the proposed method can significantly improve the\nVT Re-ID performance. The experimental results on two datasets show that our\nproposed method distinctly outperforms the state-of-the-art methods by large\nmargins, especially on RegDB dataset achieving superior performance,\nrank1/mAP/mINP 91.05%/83.28%/68.84%. It can be a new baseline for VT Re-ID,\nwith a simple but effective strategy.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 07:40:35 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 01:36:49 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Liu", "Haijun", ""], ["Tan", "Xiaoheng", ""], ["Zhou", "Xichuan", ""]]}, {"id": "2008.06226", "submitter": "Xianghui Yang", "authors": "Xianghui Yang, Bairun Wang, Kaige Chen, Xinchi Zhou, Shuai Yi, Wanli\n  Ouyang, Luping Zhou", "title": "BriNet: Towards Bridging the Intra-class and Inter-class Gaps in\n  One-Shot Segmentation", "comments": "14 pages, 6 figures, BMVC2020(Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot segmentation focuses on the generalization of models to segment\nunseen object instances with limited training samples. Although tremendous\nimprovements have been achieved, existing methods are still constrained by two\nfactors. (1) The information interaction between query and support images is\nnot adequate, leaving intra-class gap. (2) The object categories at the\ntraining and inference stages have no overlap, leaving the inter-class gap.\nThus, we propose a framework, BriNet, to bridge these gaps. First, more\ninformation interactions are encouraged between the extracted features of the\nquery and support images, i.e., using an Information Exchange Module to\nemphasize the common objects. Furthermore, to precisely localize the query\nobjects, we design a multi-path fine-grained strategy which is able to make\nbetter use of the support feature representations. Second, a new online\nrefinement strategy is proposed to help the trained model adapt to unseen\nclasses, achieved by switching the roles of the query and the support images at\nthe inference stage. The effectiveness of our framework is demonstrated by\nexperimental results, which outperforms other competitive methods and leads to\na new state-of-the-art on both PASCAL VOC and MSCOCO dataset.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 07:45:50 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Yang", "Xianghui", ""], ["Wang", "Bairun", ""], ["Chen", "Kaige", ""], ["Zhou", "Xinchi", ""], ["Yi", "Shuai", ""], ["Ouyang", "Wanli", ""], ["Zhou", "Luping", ""]]}, {"id": "2008.06229", "submitter": "Varun Sundar", "authors": "Varun Sundar, Sumanth Hegde, Divya Kothandaraman and Kaushik Mitra", "title": "Deep Atrous Guided Filter for Image Restoration in Under Display Cameras", "comments": "To appear in ECCV 2020 RLQ Workshop. Supplementary material attached.\n  For project website, see\n  https://varun19299.github.io/deep-atrous-guided-filter/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under Display Cameras present a promising opportunity for phone manufacturers\nto achieve bezel-free displays by positioning the camera behind\nsemi-transparent OLED screens. Unfortunately, such imaging systems suffer from\nsevere image degradation due to light attenuation and diffraction effects. In\nthis work, we present Deep Atrous Guided Filter (DAGF), a two-stage, end-to-end\napproach for image restoration in UDC systems. A Low-Resolution Network first\nrestores image quality at low-resolution, which is subsequently used by the\nGuided Filter Network as a filtering input to produce a high-resolution output.\nBesides the initial downsampling, our low-resolution network uses multiple,\nparallel atrous convolutions to preserve spatial resolution and emulates\nmulti-scale processing. Our approach's ability to directly train on megapixel\nimages results in significant performance improvement. We additionally propose\na simple simulation scheme to pre-train our model and boost performance. Our\noverall framework ranks 2nd and 5th in the RLQ-TOD'20 UDC Challenge for POLED\nand TOLED displays, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 07:54:52 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 06:15:45 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Sundar", "Varun", ""], ["Hegde", "Sumanth", ""], ["Kothandaraman", "Divya", ""], ["Mitra", "Kaushik", ""]]}, {"id": "2008.06254", "submitter": "Ye Liu", "authors": "Ye Liu, Junsong Yuan, Chang Wen Chen", "title": "ConsNet: Learning Consistency Graph for Zero-Shot Human-Object\n  Interaction Detection", "comments": "Accepted to Proceedings of the 28th ACM International Conference on\n  Multimedia (ACM MM'20), Seattle, WA, USA", "journal-ref": null, "doi": "10.1145/3394171.3413600", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of Human-Object Interaction (HOI) Detection, which\naims to locate and recognize HOI instances in the form of <human, action,\nobject> in images. Most existing works treat HOIs as individual interaction\ncategories, thus can not handle the problem of long-tail distribution and\npolysemy of action labels. We argue that multi-level consistencies among\nobjects, actions and interactions are strong cues for generating semantic\nrepresentations of rare or previously unseen HOIs. Leveraging the compositional\nand relational peculiarities of HOI labels, we propose ConsNet, a\nknowledge-aware framework that explicitly encodes the relations among objects,\nactions and interactions into an undirected graph called consistency graph, and\nexploits Graph Attention Networks (GATs) to propagate knowledge among HOI\ncategories as well as their constituents. Our model takes visual features of\ncandidate human-object pairs and word embeddings of HOI labels as inputs, maps\nthem into visual-semantic joint embedding space and obtains detection results\nby measuring their similarities. We extensively evaluate our model on the\nchallenging V-COCO and HICO-DET datasets, and results validate that our\napproach outperforms state-of-the-arts under both fully-supervised and\nzero-shot settings. Code is available at https://github.com/yeliudev/ConsNet.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 09:11:18 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 05:35:03 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 20:38:46 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Liu", "Ye", ""], ["Yuan", "Junsong", ""], ["Chen", "Chang Wen", ""]]}, {"id": "2008.06255", "submitter": "Seung-Hun Nam", "authors": "Seung-Hun Nam, Wonhyuk Ahn, In-Jae Yu, Seung-Min Mun", "title": "WAN: Watermarking Attack Network", "comments": "Seung-Hun Nam and Wonhyuk Ahn contributed equally to this work.\n  Corresponding author: Seung-Hun Nam", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-bit watermarking (MW) has been developed to improve robustness against\nsignal processing operations and geometric distortions. To this end, benchmark\ntools that test robustness by applying simulated attacks on watermarked images\nare available. However, limitations in these general attacks exist since they\ncannot exploit specific characteristics of the targeted MW. In addition, these\nattacks are usually devised without consideration of visual quality, which\nrarely occurs in the real world. To address these limitations, we propose a\nwatermarking attack network (WAN), a fully trainable watermarking benchmark\ntool that utilizes the weak points of the target MW and induces an inversion of\nthe watermark bit, thereby considerably reducing the watermark extractability.\nTo hinder the extraction of hidden information while ensuring high visual\nquality, we utilize a residual dense blocks-based architecture specialized in\nlocal and global feature learning. A novel watermarking attack loss is\nintroduced to break the MW systems. We empirically demonstrate that the WAN can\nsuccessfully fool various block-based MW systems.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 09:11:46 GMT"}, {"version": "v2", "created": "Sat, 9 Jan 2021 21:32:50 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Nam", "Seung-Hun", ""], ["Ahn", "Wonhyuk", ""], ["Yu", "In-Jae", ""], ["Mun", "Seung-Min", ""]]}, {"id": "2008.06258", "submitter": "Herman Kamper", "authors": "Leanne Nortje, Herman Kamper", "title": "Unsupervised vs. transfer learning for multimodal one-shot matching of\n  speech and images", "comments": "Accepted at Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of multimodal one-shot speech-image matching. An agent\nis shown a picture along with a spoken word describing the object in the\npicture, e.g. cookie, broccoli and ice-cream. After observing one paired\nspeech-image example per class, it is shown a new set of unseen pictures, and\nasked to pick the \"ice-cream\". Previous work attempted to tackle this problem\nusing transfer learning: supervised models are trained on labelled background\ndata not containing any of the one-shot classes. Here we compare transfer\nlearning to unsupervised models trained on unlabelled in-domain data. On a\ndataset of paired isolated spoken and visual digits, we specifically compare\nunsupervised autoencoder-like models to supervised classifier and Siamese\nneural networks. In both unimodal and multimodal few-shot matching experiments,\nwe find that transfer learning outperforms unsupervised training. We also\npresent experiments towards combining the two methodologies, but find that\ntransfer learning still performs best (despite idealised experiments showing\nthe benefits of unsupervised learning).\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 09:13:37 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Nortje", "Leanne", ""], ["Kamper", "Herman", ""]]}, {"id": "2008.06262", "submitter": "Mareike Thies", "authors": "Mareike Thies, Jan-Nico Z\\\"ach, Cong Gao, Russell Taylor, Nassir\n  Navab, Andreas Maier, Mathias Unberath", "title": "A Learning-based Method for Online Adjustment of C-arm Cone-Beam CT\n  Source Trajectories for Artifact Avoidance", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During spinal fusion surgery, screws are placed close to critical nerves\nsuggesting the need for highly accurate screw placement. Verifying screw\nplacement on high-quality tomographic imaging is essential. C-arm Cone-beam CT\n(CBCT) provides intraoperative 3D tomographic imaging which would allow for\nimmediate verification and, if needed, revision. However, the reconstruction\nquality attainable with commercial CBCT devices is insufficient, predominantly\ndue to severe metal artifacts in the presence of pedicle screws. These\nartifacts arise from a mismatch between the true physics of image formation and\nan idealized model thereof assumed during reconstruction. Prospectively\nacquiring views onto anatomy that are least affected by this mismatch can,\ntherefore, improve reconstruction quality. We propose to adjust the C-arm CBCT\nsource trajectory during the scan to optimize reconstruction quality with\nrespect to a certain task, i.e. verification of screw placement. Adjustments\nare performed on-the-fly using a convolutional neural network that regresses a\nquality index for possible next views given the current x-ray image. Adjusting\nthe CBCT trajectory to acquire the recommended views results in non-circular\nsource orbits that avoid poor images, and thus, data inconsistencies. We\ndemonstrate that convolutional neural networks trained on realistically\nsimulated data are capable of predicting quality metrics that enable\nscene-specific adjustments of the CBCT source trajectory. Using both\nrealistically simulated data and real CBCT acquisitions of a\nsemi-anthropomorphic phantom, we show that tomographic reconstructions of the\nresulting scene-specific CBCT acquisitions exhibit improved image quality\nparticularly in terms of metal artifacts. Since the optimization objective is\nimplicitly encoded in a neural network, the proposed approach overcomes the\nneed for 3D information at run-time.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 09:23:50 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Thies", "Mareike", ""], ["Z\u00e4ch", "Jan-Nico", ""], ["Gao", "Cong", ""], ["Taylor", "Russell", ""], ["Navab", "Nassir", ""], ["Maier", "Andreas", ""], ["Unberath", "Mathias", ""]]}, {"id": "2008.06266", "submitter": "Jacob K\\\"onig", "authors": "Jacob K\\\"onig, Mark Jenkins, Mike Mannion, Peter Barrie, Gordon\n  Morison", "title": "Optimized Deep Encoder-Decoder Methods for Crack Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous maintenance of concrete infrastructure is an important task which\nis needed to continue safe operations of these structures. One kind of defect\nthat occurs on surfaces in these structures are cracks. Automatic detection of\nthose cracks poses a challenging computer vision task as background, shape,\ncolour and size of cracks vary. In this work we propose optimized deep\nencoder-decoder methods consisting of a combination of techniques which yield\nan increase in crack segmentation performance. Specifically, we propose a new\ndesign for the decoder-part in encoder-decoder based deep learning\narchitectures for semantic segmentation. We study its composition and how to\nachieve increased performance by exploring components such as deep supervision\nand upsampling strategies. Then we examine the optimal encoder to go in\nconjunction with this decoder and determine that pretrained encoders lead to an\nincrease in performance. We propose a data augmentation strategy to increase\nthe amount of available training data and carry out the performance evaluation\nof the designed architecture on four publicly available crack segmentation\ndatasets. Additionally, we introduce two techniques into the field of surface\ncrack segmentation, previously not used there: Generating results using\ntest-time-augmentation and performing a statistical result analysis over\nmultiple training runs. The former approach generally yields increased\nperformance results, whereas the latter allows for more reproducible and better\nrepresentability of a methods results. Using those aforementioned strategies\nwith our proposed encoder-decoder architecture we are able to achieve new state\nof the art results in all datasets.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 09:43:43 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["K\u00f6nig", "Jacob", ""], ["Jenkins", "Mark", ""], ["Mannion", "Mike", ""], ["Barrie", "Peter", ""], ["Morison", "Gordon", ""]]}, {"id": "2008.06284", "submitter": "Qiegen Liu", "authors": "Cong Quan, Jinjie Zhou, Yuanzheng Zhu, Yang Chen, Shanshan Wang, Dong\n  Liang, Qiegen Liu", "title": "Homotopic Gradients of Generative Density Priors for MR Image\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning, particularly the generative model, has demonstrated tremendous\npotential to significantly speed up image reconstruction with reduced\nmeasurements recently. Rather than the existing generative models that often\noptimize the density priors, in this work, by taking advantage of the denoising\nscore matching, homotopic gradients of generative density priors (HGGDP) are\nproposed for magnetic resonance imaging (MRI) reconstruction. More precisely,\nto tackle the low-dimensional manifold and low data density region issues in\ngenerative density prior, we estimate the target gradients in\nhigher-dimensional space. We train a more powerful noise conditional score\nnetwork by forming high-dimensional tensor as the network input at the training\nphase. More artificial noise is also injected in the embedding space. At the\nreconstruction stage, a homotopy method is employed to pursue the density\nprior, such as to boost the reconstruction performance. Experiment results\nimply the remarkable performance of HGGDP in terms of high reconstruction\naccuracy; only 10% of the k-space data can still generate images of high\nquality as effectively as standard MRI reconstruction with the fully sampled\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 10:30:12 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 16:22:56 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Quan", "Cong", ""], ["Zhou", "Jinjie", ""], ["Zhu", "Yuanzheng", ""], ["Chen", "Yang", ""], ["Wang", "Shanshan", ""], ["Liang", "Dong", ""], ["Liu", "Qiegen", ""]]}, {"id": "2008.06285", "submitter": "Zichen Zhu", "authors": "Shenyu Zhang, Zichen Zhu, Qingquan Bao", "title": "Rb-PaStaNet: A Few-Shot Human-Object Interaction Detection Based on\n  Rules and Part States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Existing Human-Object Interaction (HOI) Detection approaches have achieved\ngreat progress on nonrare classes while rare HOI classes are still not\nwell-detected. In this paper, we intend to apply human prior knowledge into the\nexisting work. So we add human-labeled rules to PaStaNet and propose\nRb-PaStaNet aimed at improving rare HOI classes detection. Our results show a\ncertain improvement of the rare classes, while the non-rare classes and the\noverall improvement is more considerable.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 10:32:15 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Zhang", "Shenyu", ""], ["Zhu", "Zichen", ""], ["Bao", "Qingquan", ""]]}, {"id": "2008.06286", "submitter": "Wei Zhang", "authors": "Weidong Zhang, Wei Zhang and Yinda Zhang", "title": "GeoLayout: Geometry Driven Room Layout Estimation Based on Depth Maps of\n  Planes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of room layout estimation is to locate the wall-floor, wall-ceiling,\nand wall-wall boundaries. Most recent methods solve this problem based on\nedge/keypoint detection or semantic segmentation. However, these approaches\nhave shown limited attention on the geometry of the dominant planes and the\nintersection between them, which has significant impact on room layout. In this\nwork, we propose to incorporate geometric reasoning to deep learning for layout\nestimation. Our approach learns to infer the depth maps of the dominant planes\nin the scene by predicting the pixel-level surface parameters, and the layout\ncan be generated by the intersection of the depth maps. Moreover, we present a\nnew dataset with pixel-level depth annotation of dominant planes. It is larger\nthan the existing datasets and contains both cuboid and non-cuboid rooms.\nExperimental results show that our approach produces considerable performance\ngains on both 2D and 3D datasets.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 10:34:24 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Zhang", "Weidong", ""], ["Zhang", "Wei", ""], ["Zhang", "Yinda", ""]]}, {"id": "2008.06318", "submitter": "Aishah Alsehaim", "authors": "Toby P. Breckon and Aishah Alsehaim", "title": "Not 3D Re-ID: a Simple Single Stream 2D Convolution for Robust Video\n  Re-identification", "comments": "have been submitted to ICPR 2020 and has been ACCEPTED for\n  presentation and inclusion in the proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based person re-identification has received increasing attention\nrecently, as it plays an important role within surveillance video analysis.\nVideo-based Re-ID is an expansion of earlier image-based re-identification\nmethods by learning features from a video via multiple image frames for each\nperson. Most contemporary video Re-ID methods utilise complex CNNbased network\narchitectures using 3D convolution or multibranch networks to extract\nspatial-temporal video features. By contrast, in this paper, we illustrate\nsuperior performance from a simple single stream 2D convolution network\nleveraging the ResNet50-IBN architecture to extract frame-level features\nfollowed by temporal attention for clip level features. These clip level\nfeatures can be generalised to extract video level features by averaging\nwithout any significant additional cost. Our approach uses best video Re-ID\npractice and transfer learning between datasets to outperform existing\nstate-of-the-art approaches on the MARS, PRID2011 and iLIDS-VID datasets with\n89:62%, 97:75%, 97:33% rank-1 accuracy respectively and with 84:61% mAP for\nMARS, without reliance on complex and memory intensive 3D convolutions or\nmulti-stream networks architectures as found in other contemporary work.\nConversely, our work shows that global features extracted by the 2D convolution\nnetwork are a sufficient representation for robust state of the art video\nRe-ID.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 12:19:32 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 10:49:24 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Breckon", "Toby P.", ""], ["Alsehaim", "Aishah", ""]]}, {"id": "2008.06330", "submitter": "Siqi Liu", "authors": "Eduardo Mortani Barbosa Jr., Warren B. Gefter, Rochelle Yang, Florin\n  C. Ghesu, Siqi Liu, Boris Mailhe, Awais Mansoor, Sasa Grbic, Sebastian Piat,\n  Guillaume Chabin, Vishwanath R S., Abishek Balachandran, Sebastian Vogt,\n  Valentin Ziebandt, Steffen Kappler, Dorin Comaniciu", "title": "Automated detection and quantification of COVID-19 airspace disease on\n  chest radiographs: A novel approach achieving radiologist-level performance\n  using a CNN trained on digital reconstructed radiographs (DRRs) from CT-based\n  ground-truth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To leverage volumetric quantification of airspace disease (AD)\nderived from a superior modality (CT) serving as ground truth, projected onto\ndigitally reconstructed radiographs (DRRs) to: 1) train a convolutional neural\nnetwork to quantify airspace disease on paired CXRs; and 2) compare the\nDRR-trained CNN to expert human readers in the CXR evaluation of patients with\nconfirmed COVID-19.\n  Materials and Methods: We retrospectively selected a cohort of 86 COVID-19\npatients (with positive RT-PCR), from March-May 2020 at a tertiary hospital in\nthe northeastern USA, who underwent chest CT and CXR within 48 hrs. The ground\ntruth volumetric percentage of COVID-19 related AD (POv) was established by\nmanual AD segmentation on CT. The resulting 3D masks were projected into 2D\nanterior-posterior digitally reconstructed radiographs (DRR) to compute\narea-based AD percentage (POa). A convolutional neural network (CNN) was\ntrained with DRR images generated from a larger-scale CT dataset of COVID-19\nand non-COVID-19 patients, automatically segmenting lungs, AD and quantifying\nPOa on CXR. CNN POa results were compared to POa quantified on CXR by two\nexpert readers and to the POv ground-truth, by computing correlations and mean\nabsolute errors.\n  Results: Bootstrap mean absolute error (MAE) and correlations between POa and\nPOv were 11.98% [11.05%-12.47%] and 0.77 [0.70-0.82] for average of expert\nreaders, and 9.56%-9.78% [8.83%-10.22%] and 0.78-0.81 [0.73-0.85] for the CNN,\nrespectively.\n  Conclusion: Our CNN trained with DRR using CT-derived airspace quantification\nachieved expert radiologist level of accuracy in the quantification of airspace\ndisease on CXR, in patients with positive RT-PCR for COVID-19.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 14:33:03 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Barbosa", "Eduardo Mortani", "Jr."], ["Gefter", "Warren B.", ""], ["Yang", "Rochelle", ""], ["Ghesu", "Florin C.", ""], ["Liu", "Siqi", ""], ["Mailhe", "Boris", ""], ["Mansoor", "Awais", ""], ["Grbic", "Sasa", ""], ["Piat", "Sebastian", ""], ["Chabin", "Guillaume", ""], ["S.", "Vishwanath R", ""], ["Balachandran", "Abishek", ""], ["Vogt", "Sebastian", ""], ["Ziebandt", "Valentin", ""], ["Kappler", "Steffen", ""], ["Comaniciu", "Dorin", ""]]}, {"id": "2008.06332", "submitter": "Beate Sick", "authors": "Lisa Herzog, Elvis Murina, Oliver D\\\"urr, Susanne Wegener, Beate Sick", "title": "Integrating uncertainty in deep neural networks for MRI based stroke\n  analysis", "comments": "21 pages, 13 figures", "journal-ref": "Medical Image Analysis (2020): 101790", "doi": "10.1016/j.media.2020.101790", "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At present, the majority of the proposed Deep Learning (DL) methods provide\npoint predictions without quantifying the models uncertainty. However, a\nquantification of the reliability of automated image analysis is essential, in\nparticular in medicine when physicians rely on the results for making critical\ntreatment decisions. In this work, we provide an entire framework to diagnose\nischemic stroke patients incorporating Bayesian uncertainty into the analysis\nprocedure. We present a Bayesian Convolutional Neural Network (CNN) yielding a\nprobability for a stroke lesion on 2D Magnetic Resonance (MR) images with\ncorresponding uncertainty information about the reliability of the prediction.\nFor patient-level diagnoses, different aggregation methods are proposed and\nevaluated, which combine the single image-level predictions. Those methods take\nadvantage of the uncertainty in image predictions and report model uncertainty\nat the patient-level. In a cohort of 511 patients, our Bayesian CNN achieved an\naccuracy of 95.33% at the image-level representing a significant improvement of\n2% over a non-Bayesian counterpart. The best patient aggregation method yielded\n95.89% of accuracy. Integrating uncertainty information about image predictions\nin aggregation models resulted in higher uncertainty measures to false patient\nclassifications, which enabled to filter critical patient diagnoses that are\nsupposed to be closer examined by a medical doctor. We therefore recommend\nusing Bayesian approaches not only for improved image-level prediction and\nuncertainty estimation but also for the detection of uncertain aggregations at\nthe patient-level.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 09:50:17 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Herzog", "Lisa", ""], ["Murina", "Elvis", ""], ["D\u00fcrr", "Oliver", ""], ["Wegener", "Susanne", ""], ["Sick", "Beate", ""]]}, {"id": "2008.06353", "submitter": "Milda Pocevi\\v{c}i\\=ut\\.e", "authors": "Milda Pocevi\\v{c}i\\=ut\\.e and Gabriel Eilertsen and Claes Lundstr\\\"om", "title": "Survey of XAI in digital pathology", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-50402-1_4", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence (AI) has shown great promise for diagnostic imaging\nassessments. However, the application of AI to support medical diagnostics in\nclinical routine comes with many challenges. The algorithms should have high\nprediction accuracy but also be transparent, understandable and reliable. Thus,\nexplainable artificial intelligence (XAI) is highly relevant for this domain.\nWe present a survey on XAI within digital pathology, a medical imaging\nsub-discipline with particular characteristics and needs. The review includes\nseveral contributions. Firstly, we give a thorough overview of current XAI\ntechniques of potential relevance for deep learning methods in pathology\nimaging, and categorise them from three different aspects. In doing so, we\nincorporate uncertainty estimation methods as an integral part of the XAI\nlandscape. We also connect the technical methods to the specific prerequisites\nin digital pathology and present findings to guide future research efforts. The\nsurvey is intended for both technical researchers and medical professionals,\none of the objectives being to establish a common ground for cross-disciplinary\ndiscussions.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 13:11:54 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Pocevi\u010di\u016bt\u0117", "Milda", ""], ["Eilertsen", "Gabriel", ""], ["Lundstr\u00f6m", "Claes", ""]]}, {"id": "2008.06365", "submitter": "Shruti Jadon", "authors": "Shruti Jadon", "title": "An Overview of Deep Learning Architectures in Few-Shot Learning Domain", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": "10.13140/RG.2.2.31573.24803/1", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since 2012, Deep learning has revolutionized Artificial Intelligence and has\nachieved state-of-the-art outcomes in different domains, ranging from Image\nClassification to Speech Generation. Though it has many potentials, our current\narchitectures come with the pre-requisite of large amounts of data. Few-Shot\nLearning (also known as one-shot learning) is a sub-field of machine learning\nthat aims to create such models that can learn the desired objective with less\ndata, similar to how humans learn. In this paper, we have reviewed some of the\nwell-known deep learning-based approaches towards few-shot learning. We have\ndiscussed the recent achievements, challenges, and possibilities of improvement\nof few-shot learning based deep learning architectures. Our aim for this paper\nis threefold: (i) Give a brief introduction to deep learning architectures for\nfew-shot learning with pointers to core references. (ii) Indicate how deep\nlearning has been applied to the low-data regime, from data preparation to\nmodel training. and, (iii) Provide a starting point for people interested in\nexperimenting and perhaps contributing to the field of few-shot learning by\npointing out some useful resources and open-source code. Our code is available\nat Github: https://github.com/shruti-jadon/Hands-on-One-Shot-Learning.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 06:58:45 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 03:06:33 GMT"}, {"version": "v3", "created": "Wed, 19 Aug 2020 18:23:15 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Jadon", "Shruti", ""]]}, {"id": "2008.06374", "submitter": "Yunlu Chen", "authors": "Yunlu Chen, Vincent Tao Hu, Efstratios Gavves, Thomas Mensink, Pascal\n  Mettes, Pengwan Yang and Cees G.M. Snoek", "title": "PointMixup: Augmentation for Point Clouds", "comments": "Accepted as Spotlight presentation at European Conference on Computer\n  Vision (ECCV), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces data augmentation for point clouds by interpolation\nbetween examples. Data augmentation by interpolation has shown to be a simple\nand effective approach in the image domain. Such a mixup is however not\ndirectly transferable to point clouds, as we do not have a one-to-one\ncorrespondence between the points of two different objects. In this paper, we\ndefine data augmentation between point clouds as a shortest path linear\ninterpolation. To that end, we introduce PointMixup, an interpolation method\nthat generates new examples through an optimal assignment of the path function\nbetween two point clouds. We prove that our PointMixup finds the shortest path\nbetween two point clouds and that the interpolation is assignment invariant and\nlinear. With the definition of interpolation, PointMixup allows to introduce\nstrong interpolation-based regularizers such as mixup and manifold mixup to the\npoint cloud domain. Experimentally, we show the potential of PointMixup for\npoint cloud classification, especially when examples are scarce, as well as\nincreased robustness to noise and geometric transformations to points. The code\nfor PointMixup and the experimental details are publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 13:57:20 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Chen", "Yunlu", ""], ["Hu", "Vincent Tao", ""], ["Gavves", "Efstratios", ""], ["Mensink", "Thomas", ""], ["Mettes", "Pascal", ""], ["Yang", "Pengwan", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "2008.06388", "submitter": "Michael Roberts", "authors": "Michael Roberts, Derek Driggs, Matthew Thorpe, Julian Gilbey, Michael\n  Yeung, Stephan Ursprung, Angelica I. Aviles-Rivero, Christian Etmann, Cathal\n  McCague, Lucian Beer, Jonathan R. Weir-McCall, Zhongzhao Teng, Effrossyni\n  Gkrania-Klotsas, James H.F. Rudd, Evis Sala, Carola-Bibiane Sch\\\"onlieb (on\n  behalf of the AIX-COVNET collaboration)", "title": "Common pitfalls and recommendations for using machine learning to detect\n  and prognosticate for COVID-19 using chest radiographs and CT scans", "comments": "35 pages, 3 figures, 2 tables, updated to the period 1 January 2020 -\n  3 October 2020", "journal-ref": "Nature Machine Intelligence 3, 199-217 (2021)", "doi": "10.1038/s42256-021-00307-0", "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning methods offer great promise for fast and accurate detection\nand prognostication of COVID-19 from standard-of-care chest radiographs (CXR)\nand computed tomography (CT) images. Many articles have been published in 2020\ndescribing new machine learning-based models for both of these tasks, but it is\nunclear which are of potential clinical utility. In this systematic review, we\nsearch EMBASE via OVID, MEDLINE via PubMed, bioRxiv, medRxiv and arXiv for\npublished papers and preprints uploaded from January 1, 2020 to October 3, 2020\nwhich describe new machine learning models for the diagnosis or prognosis of\nCOVID-19 from CXR or CT images. Our search identified 2,212 studies, of which\n415 were included after initial screening and, after quality screening, 61\nstudies were included in this systematic review. Our review finds that none of\nthe models identified are of potential clinical use due to methodological flaws\nand/or underlying biases. This is a major weakness, given the urgency with\nwhich validated COVID-19 models are needed. To address this, we give many\nrecommendations which, if followed, will solve these issues and lead to higher\nquality model development and well documented manuscripts.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 14:25:21 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 08:10:35 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2020 13:56:25 GMT"}, {"version": "v4", "created": "Tue, 5 Jan 2021 19:41:55 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Roberts", "Michael", "", "on\n  behalf of the AIX-COVNET collaboration"], ["Driggs", "Derek", "", "on\n  behalf of the AIX-COVNET collaboration"], ["Thorpe", "Matthew", "", "on\n  behalf of the AIX-COVNET collaboration"], ["Gilbey", "Julian", "", "on\n  behalf of the AIX-COVNET collaboration"], ["Yeung", "Michael", "", "on\n  behalf of the AIX-COVNET collaboration"], ["Ursprung", "Stephan", "", "on\n  behalf of the AIX-COVNET collaboration"], ["Aviles-Rivero", "Angelica I.", "", "on\n  behalf of the AIX-COVNET collaboration"], ["Etmann", "Christian", "", "on\n  behalf of the AIX-COVNET collaboration"], ["McCague", "Cathal", "", "on\n  behalf of the AIX-COVNET collaboration"], ["Beer", "Lucian", "", "on\n  behalf of the AIX-COVNET collaboration"], ["Weir-McCall", "Jonathan R.", "", "on\n  behalf of the AIX-COVNET collaboration"], ["Teng", "Zhongzhao", "", "on\n  behalf of the AIX-COVNET collaboration"], ["Gkrania-Klotsas", "Effrossyni", "", "on\n  behalf of the AIX-COVNET collaboration"], ["Rudd", "James H. F.", "", "on\n  behalf of the AIX-COVNET collaboration"], ["Sala", "Evis", "", "on\n  behalf of the AIX-COVNET collaboration"], ["Sch\u00f6nlieb", "Carola-Bibiane", "", "on\n  behalf of the AIX-COVNET collaboration"]]}, {"id": "2008.06392", "submitter": "Gnana Praveen Rajasekar", "authors": "Gnana Praveen R, Eric Granger, Patrick Cardinal", "title": "Deep Domain Adaptation for Ordinal Regression of Pain Intensity\n  Estimation Using Weakly-Labelled Videos", "comments": "Under review for a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of pain intensity from facial expressions captured in videos has\nan immense potential for health care applications. Given the challenges related\nto subjective variations of facial expressions, and operational capture\nconditions, the accuracy of state-of-the-art DL models for recognizing facial\nexpressions may decline. Domain adaptation has been widely explored to\nalleviate the problem of domain shifts that typically occur between video data\ncaptured across various source and target domains. Moreover, given the\nlaborious task of collecting and annotating videos, and subjective bias due to\nambiguity among adjacent intensity levels, weakly-supervised learning is\ngaining attention in such applications. State-of-the-art WSL models are\ntypically formulated as regression problems, and do not leverage the ordinal\nrelationship among pain intensity levels, nor temporal coherence of multiple\nconsecutive frames. This paper introduces a new DL model for weakly-supervised\nDA with ordinal regression that can be adapted using target domain videos with\ncoarse labels provided on a periodic basis. The WSDA-OR model enforces ordinal\nrelationships among intensity levels assigned to target sequences, and\nassociates multiple relevant frames to sequence-level labels. In particular, it\nlearns discriminant and domain-invariant feature representations by integrating\nmultiple instance learning with deep adversarial DA, where soft Gaussian labels\nare used to efficiently represent the weak ordinal sequence-level labels from\ntarget domain. The proposed approach was validated using RECOLA video dataset\nas fully-labeled source domain data, and UNBC-McMaster shoulder pain video\ndataset as weakly-labeled target domain data. We have also validated WSDA-OR on\nBIOVID and Fatigue datasets for sequence level estimation.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 15:42:23 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 02:24:36 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["R", "Gnana Praveen", ""], ["Granger", "Eric", ""], ["Cardinal", "Patrick", ""]]}, {"id": "2008.06399", "submitter": "Georgios Evangelidis", "authors": "Branislav Micusik, Georgios Evangelidis", "title": "Renormalization for Initialization of Rolling Shutter Visual-Inertial\n  Odometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we deal with the initialization problem of a visual-inertial\nodometry system with rolling shutter cameras. Initialization is a prerequisite\nfor using inertial signals and fusing them with visual data. We propose a novel\nstatistical solution to the initialization problem on visual and inertial data\nsimultaneously, by casting it into the renormalization scheme of Kanatani. The\nrenormalization is an optimization scheme which intends to reduce the inherent\nstatistical bias of common linear systems. We derive and present the necessary\nsteps and methodology specific to the initialization problem. Extensive\nevaluations on ground truth exhibit superior performance and a gain in accuracy\nof up to $20\\%$ over the originally proposed Least Squares solution. The\nrenormalization performs similarly to the optimal Maximum Likelihood estimate,\ndespite arriving at the solution by different means. With this paper we are\nadding to the set of Computer Vision problems which can be cast into the\nrenormalization scheme.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 14:54:15 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 19:46:11 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Micusik", "Branislav", ""], ["Evangelidis", "Georgios", ""]]}, {"id": "2008.06402", "submitter": "Stefanos Laskaridis", "authors": "Stefanos Laskaridis, Stylianos I. Venieris, Mario Almeida, Ilias\n  Leontiadis, Nicholas D. Lane", "title": "SPINN: Synergistic Progressive Inference of Neural Networks over Device\n  and Cloud", "comments": "Accepted at the 26th Annual International Conference on Mobile\n  Computing and Networking (MobiCom), 2020", "journal-ref": null, "doi": "10.1145/3372224.3419194", "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the soaring use of convolutional neural networks (CNNs) in mobile\napplications, uniformly sustaining high-performance inference on mobile has\nbeen elusive due to the excessive computational demands of modern CNNs and the\nincreasing diversity of deployed devices. A popular alternative comprises\noffloading CNN processing to powerful cloud-based servers. Nevertheless, by\nrelying on the cloud to produce outputs, emerging mission-critical and\nhigh-mobility applications, such as drone obstacle avoidance or interactive\napplications, can suffer from the dynamic connectivity conditions and the\nuncertain availability of the cloud. In this paper, we propose SPINN, a\ndistributed inference system that employs synergistic device-cloud computation\ntogether with a progressive inference method to deliver fast and robust CNN\ninference across diverse settings. The proposed system introduces a novel\nscheduler that co-optimises the early-exit policy and the CNN splitting at run\ntime, in order to adapt to dynamic conditions and meet user-defined\nservice-level requirements. Quantitative evaluation illustrates that SPINN\noutperforms its state-of-the-art collaborative inference counterparts by up to\n2x in achieved throughput under varying network conditions, reduces the server\ncost by up to 6.8x and improves accuracy by 20.7% under latency constraints,\nwhile providing robust operation under uncertain connectivity conditions and\nsignificant energy savings compared to cloud-centric execution.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 15:00:19 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 10:24:41 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Laskaridis", "Stefanos", ""], ["Venieris", "Stylianos I.", ""], ["Almeida", "Mario", ""], ["Leontiadis", "Ilias", ""], ["Lane", "Nicholas D.", ""]]}, {"id": "2008.06439", "submitter": "Manoj Acharya", "authors": "Manoj Acharya, Tyler L. Hayes, Christopher Kanan", "title": "RODEO: Replay for Online Object Detection", "comments": "Accepted for poster presentation at BMVC2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can incrementally learn to do new visual detection tasks, which is a\nhuge challenge for today's computer vision systems. Incrementally trained deep\nlearning models lack backwards transfer to previously seen classes and suffer\nfrom a phenomenon known as $\"catastrophic forgetting.\"$ In this paper, we\npioneer online streaming learning for object detection, where an agent must\nlearn examples one at a time with severe memory and computational constraints.\nIn object detection, a system must output all bounding boxes for an image with\nthe correct label. Unlike earlier work, the system described in this paper can\nlearn this task in an online manner with new classes being introduced over\ntime. We achieve this capability by using a novel memory replay mechanism that\nefficiently replays entire scenes. We achieve state-of-the-art results on both\nthe PASCAL VOC 2007 and MS COCO datasets.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 16:03:52 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Acharya", "Manoj", ""], ["Hayes", "Tyler L.", ""], ["Kanan", "Christopher", ""]]}, {"id": "2008.06447", "submitter": "Matteo Poggi", "authors": "Matteo Poggi, Filippo Aleotti, Fabio Tosi, Giulio Zaccaroni and\n  Stefano Mattoccia", "title": "Self-adapting confidence estimation for stereo", "comments": "ECCV 2020 (errata corrige: eq.6, k domain)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the confidence of disparity maps inferred by a stereo algorithm\nhas become a very relevant task in the years, due to the increasing number of\napplications leveraging such cue. Although self-supervised learning has\nrecently spread across many computer vision tasks, it has been barely\nconsidered in the field of confidence estimation. In this paper, we propose a\nflexible and lightweight solution enabling self-adapting confidence estimation\nagnostic to the stereo algorithm or network. Our approach relies on the minimum\ninformation available in any stereo setup (i.e., the input stereo pair and the\noutput disparity map) to learn an effective confidence measure. This strategy\nallows us not only a seamless integration with any stereo system, including\nconsumer and industrial devices equipped with undisclosed stereo perception\nmethods, but also, due to its self-adapting capability, for its out-of-the-box\ndeployment in the field. Exhaustive experimental results with different\nstandard datasets support our claims, showing how our solution is the\nfirst-ever enabling online learning of accurate confidence estimation for any\nstereo system and without any requirement for the end-user.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 16:17:28 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 18:01:13 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Poggi", "Matteo", ""], ["Aleotti", "Filippo", ""], ["Tosi", "Fabio", ""], ["Zaccaroni", "Giulio", ""], ["Mattoccia", "Stefano", ""]]}, {"id": "2008.06457", "submitter": "Avinash Kori", "authors": "Avinash Kori, Parth Natekar, Ganapathy Krishnamurthi, Balaji\n  Srinivasan", "title": "Abstracting Deep Neural Networks into Concept Graphs for Concept Level\n  Interpretability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The black-box nature of deep learning models prevents them from being\ncompletely trusted in domains like biomedicine. Most explainability techniques\ndo not capture the concept-based reasoning that human beings follow. In this\nwork, we attempt to understand the behavior of trained models that perform\nimage processing tasks in the medical domain by building a graphical\nrepresentation of the concepts they learn. Extracting such a graphical\nrepresentation of the model's behavior on an abstract, higher conceptual level\nwould unravel the learnings of these models and would help us to evaluate the\nsteps taken by the model for predictions. We show the application of our\nproposed implementation on two biomedical problems - brain tumor segmentation\nand fundus image classification. We provide an alternative graphical\nrepresentation of the model by formulating a concept level graph as discussed\nabove, which makes the problem of intervention to find active inference trails\nmore tractable. Understanding these trails would provide an understanding of\nthe hierarchy of the decision-making process followed by the model. [As well as\noverall nature of model]. Our framework is available at\nhttps://github.com/koriavinash1/BioExp\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 16:34:32 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 07:12:01 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Kori", "Avinash", ""], ["Natekar", "Parth", ""], ["Krishnamurthi", "Ganapathy", ""], ["Srinivasan", "Balaji", ""]]}, {"id": "2008.06471", "submitter": "Gal Metzer", "authors": "Gal Metzer, Rana Hanocka, Raja Giryes, Daniel Cohen-Or", "title": "Self-Sampling for Neural Point Cloud Consolidation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel technique for neural point cloud consolidation which\nlearns from only the input point cloud. Unlike other point upsampling methods\nwhich analyze shapes via local patches, in this work, we learn from global\nsubsets. We repeatedly self-sample the input point cloud with global subsets\nthat are used to train a deep neural network. Specifically, we define source\nand target subsets according to the desired consolidation criteria (e.g.,\ngenerating sharp points or points in sparse regions). The network learns a\nmapping from source to target subsets, and implicitly learns to consolidate the\npoint cloud. During inference, the network is fed with random subsets of points\nfrom the input, which it displaces to synthesize a consolidated point set. We\nleverage the inductive bias of neural networks to eliminate noise and outliers,\na notoriously difficult problem in point cloud consolidation. The shared\nweights of the network are optimized over the entire shape, learning non-local\nstatistics and exploiting the recurrence of local-scale geometries.\nSpecifically, the network encodes the distribution of the underlying shape\nsurface within a fixed set of local kernels, which results in the best\nexplanation of the underlying shape surface. We demonstrate the ability to\nconsolidate point sets from a variety of shapes, while eliminating outliers and\nnoise.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 17:16:02 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 17:09:31 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Metzer", "Gal", ""], ["Hanocka", "Rana", ""], ["Giryes", "Raja", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2008.06474", "submitter": "Hiroki Tsuda", "authors": "Hiroki Tsuda, Eisuke Shibuya and Kazuhiro Hotta", "title": "Feedback Attention for Cell Image Segmentation", "comments": "14 pages, 4 figures, Accepted by ECCV2020 Workshop \"BioImage\n  Computing (BIC)\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address cell image segmentation task by Feedback Attention\nmechanism like feedback processing. Unlike conventional neural network models\nof feedforward processing, we focused on the feedback processing in human brain\nand assumed that the network learns like a human by connecting feature maps\nfrom deep layers to shallow layers. We propose some Feedback Attentions which\nimitate human brain and feeds back the feature maps of output layer to close\nlayer to the input. U-Net with Feedback Attention showed better result than the\nconventional methods using only feedforward processing.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 17:23:32 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Tsuda", "Hiroki", ""], ["Shibuya", "Eisuke", ""], ["Hotta", "Kazuhiro", ""]]}, {"id": "2008.06520", "submitter": "Ruojin Cai", "authors": "Ruojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun Hao, Serge\n  Belongie, Noah Snavely, and Bharath Hariharan", "title": "Learning Gradient Fields for Shape Generation", "comments": "Published in ECCV 2020 (Spotlight); Project page:\n  https://www.cs.cornell.edu/~ruojin/ShapeGF/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel technique to generate shapes from point\ncloud data. A point cloud can be viewed as samples from a distribution of 3D\npoints whose density is concentrated near the surface of the shape. Point cloud\ngeneration thus amounts to moving randomly sampled points to high-density\nareas. We generate point clouds by performing stochastic gradient ascent on an\nunnormalized probability density, thereby moving sampled points toward the\nhigh-likelihood regions. Our model directly predicts the gradient of the log\ndensity field and can be trained with a simple objective adapted from\nscore-based generative models. We show that our method can reach\nstate-of-the-art performance for point cloud auto-encoding and generation,\nwhile also allowing for extraction of a high-quality implicit surface. Code is\navailable at https://github.com/RuojinCai/ShapeGF.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 18:06:15 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 04:34:18 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Cai", "Ruojin", ""], ["Yang", "Guandao", ""], ["Averbuch-Elor", "Hadar", ""], ["Hao", "Zekun", ""], ["Belongie", "Serge", ""], ["Snavely", "Noah", ""], ["Hariharan", "Bharath", ""]]}, {"id": "2008.06534", "submitter": "Selena Ling", "authors": "Benjamin Attal, Selena Ling, Aaron Gokaslan, Christian Richardt, and\n  James Tompkin", "title": "MatryODShka: Real-time 6DoF Video View Synthesis using Multi-Sphere\n  Images", "comments": "25 pages, 13 figures, Published at European Conference on Computer\n  Vision (ECCV 2020), Project Page: http://visual.cs.brown.edu/matryodshka", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to convert stereo 360{\\deg} (omnidirectional stereo)\nimagery into a layered, multi-sphere image representation for six\ndegree-of-freedom (6DoF) rendering. Stereo 360{\\deg} imagery can be captured\nfrom multi-camera systems for virtual reality (VR), but lacks motion parallax\nand correct-in-all-directions disparity cues. Together, these can quickly lead\nto VR sickness when viewing content. One solution is to try and generate a\nformat suitable for 6DoF rendering, such as by estimating depth. However, this\nraises questions as to how to handle disoccluded regions in dynamic scenes. Our\napproach is to simultaneously learn depth and disocclusions via a multi-sphere\nimage representation, which can be rendered with correct 6DoF disparity and\nmotion parallax in VR. This significantly improves comfort for the viewer, and\ncan be inferred and rendered in real time on modern GPU hardware. Together,\nthese move towards making VR video a more comfortable immersive medium.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 18:33:05 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Attal", "Benjamin", ""], ["Ling", "Selena", ""], ["Gokaslan", "Aaron", ""], ["Richardt", "Christian", ""], ["Tompkin", "James", ""]]}, {"id": "2008.06543", "submitter": "Fuxun Yu", "authors": "Fuxun Yu, Chenchen Liu, Di Wang, Yanzhi Wang, Xiang Chen", "title": "AntiDote: Attention-based Dynamic Optimization for Neural Network\n  Runtime Efficiency", "comments": "Accepted in DATE'2020 (Best Paper Nomination)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) achieved great cognitive performance at\nthe expense of considerable computation load. To relieve the computation load,\nmany optimization works are developed to reduce the model redundancy by\nidentifying and removing insignificant model components, such as weight\nsparsity and filter pruning. However, these works only evaluate model\ncomponents' static significance with internal parameter information, ignoring\ntheir dynamic interaction with external inputs. With per-input feature\nactivation, the model component significance can dynamically change, and thus\nthe static methods can only achieve sub-optimal results. Therefore, we propose\na dynamic CNN optimization framework in this work. Based on the neural network\nattention mechanism, we propose a comprehensive dynamic optimization framework\nincluding (1) testing-phase channel and column feature map pruning, as well as\n(2) training-phase optimization by targeted dropout. Such a dynamic\noptimization framework has several benefits: (1) First, it can accurately\nidentify and aggressively remove per-input feature redundancy with considering\nthe model-input interaction; (2) Meanwhile, it can maximally remove the feature\nmap redundancy in various dimensions thanks to the multi-dimension flexibility;\n(3) The training-testing co-optimization favors the dynamic pruning and helps\nmaintain the model accuracy even with very high feature pruning ratio.\nExtensive experiments show that our method could bring 37.4% to 54.5% FLOPs\nreduction with negligible accuracy drop on various of test networks.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 18:48:13 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Yu", "Fuxun", ""], ["Liu", "Chenchen", ""], ["Wang", "Di", ""], ["Wang", "Yanzhi", ""], ["Chen", "Xiang", ""]]}, {"id": "2008.06551", "submitter": "Aditay Tripathi", "authors": "Aditay Tripathi, Rajath R Dani, Anand Mishra, Anirban Chakraborty", "title": "Sketch-Guided Object Localization in Natural Images", "comments": "ECCV 2020 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the novel problem of localizing all the instances of an object\n(seen or unseen during training) in a natural image via sketch query. We refer\nto this problem as sketch-guided object localization. This problem is\ndistinctively different from the traditional sketch-based image retrieval task\nwhere the gallery set often contains images with only one object. The\nsketch-guided object localization proves to be more challenging when we\nconsider the following: (i) the sketches used as queries are abstract\nrepresentations with little information on the shape and salient attributes of\nthe object, (ii) the sketches have significant variability as they are\nhand-drawn by a diverse set of untrained human subjects, and (iii) there exists\na domain gap between sketch queries and target natural images as these are\nsampled from very different data distributions. To address the problem of\nsketch-guided object localization, we propose a novel cross-modal attention\nscheme that guides the region proposal network (RPN) to generate object\nproposals relevant to the sketch query. These object proposals are later scored\nagainst the query to obtain final localization. Our method is effective with as\nlittle as a single sketch query. Moreover, it also generalizes well to object\ncategories not seen during training and is effective in localizing multiple\nobject instances present in the image. Furthermore, we extend our framework to\na multi-query setting using novel feature fusion and attention fusion\nstrategies introduced in this paper. The localization performance is evaluated\non publicly available object detection benchmarks, viz. MS-COCO and PASCAL-VOC,\nwith sketch queries obtained from `Quick, Draw!'. The proposed method\nsignificantly outperforms related baselines on both single-query and\nmulti-query localization tasks.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 19:35:56 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Tripathi", "Aditay", ""], ["Dani", "Rajath R", ""], ["Mishra", "Anand", ""], ["Chakraborty", "Anirban", ""]]}, {"id": "2008.06559", "submitter": "R. Marc Lebel", "authors": "R. Marc Lebel", "title": "Performance characterization of a novel deep learning-based MR image\n  reconstruction pipeline", "comments": "18 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel deep learning-based magnetic resonance imaging reconstruction\npipeline was designed to address fundamental image quality limitations of\nconventional reconstruction to provide high-resolution, low-noise MR images.\nThis pipeline's unique aims were to convert truncation artifact into improved\nimage sharpness while jointly denoising images to improve image quality. This\nnew approach, now commercially available at AIR Recon DL (GE Healthcare,\nWaukesha, WI), includes a deep convolutional neural network (CNN) to aid in the\nreconstruction of raw data, ultimately producing clean, sharp images. Here we\ndescribe key features of this pipeline and its CNN, characterize its\nperformance in digital reference objects, phantoms, and in-vivo, and present\nsample images and protocol optimization strategies that leverage image quality\nimprovement for reduced scan time. This new deep learning-based reconstruction\npipeline represents a powerful new tool to increase the diagnostic and\noperational performance of an MRI scanner.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 19:54:08 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Lebel", "R. Marc", ""]]}, {"id": "2008.06581", "submitter": "Bin Duan", "authors": "Bin Duan, Hao Tang, Wei Wang, Ziliang Zong, Guowei Yang, Yan Yan", "title": "Audio-Visual Event Localization via Recursive Fusion by Joint\n  Co-Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The major challenge in audio-visual event localization task lies in how to\nfuse information from multiple modalities effectively. Recent works have shown\nthat attention mechanism is beneficial to the fusion process. In this paper, we\npropose a novel joint attention mechanism with multimodal fusion methods for\naudio-visual event localization. Particularly, we present a concise yet valid\narchitecture that effectively learns representations from multiple modalities\nin a joint manner. Initially, visual features are combined with auditory\nfeatures and then turned into joint representations. Next, we make use of the\njoint representations to attend to visual features and auditory features,\nrespectively. With the help of this joint co-attention, new visual and auditory\nfeatures are produced, and thus both features can enjoy the mutually improved\nbenefits from each other. It is worth noting that the joint co-attention unit\nis recursive meaning that it can be performed multiple times for obtaining\nbetter joint representations progressively. Extensive experiments on the public\nAVE dataset have shown that the proposed method achieves significantly better\nresults than the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 21:50:26 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Duan", "Bin", ""], ["Tang", "Hao", ""], ["Wang", "Wei", ""], ["Zong", "Ziliang", ""], ["Yang", "Guowei", ""], ["Yan", "Yan", ""]]}, {"id": "2008.06597", "submitter": "Siyang Yuan", "authors": "Siyang Yuan, Ke Bai, Liqun Chen, Yizhe Zhang, Chenyang Tao, Chunyuan\n  Li, Guoyin Wang, Ricardo Henao, Lawrence Carin", "title": "Weakly supervised cross-domain alignment with optimal transport", "comments": "Accepted to BMVC 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-domain alignment between image objects and text sequences is key to\nmany visual-language tasks, and it poses a fundamental challenge to both\ncomputer vision and natural language processing. This paper investigates a\nnovel approach for the identification and optimization of fine-grained semantic\nsimilarities between image and text entities, under a weakly-supervised setup,\nimproving performance over state-of-the-art solutions. Our method builds upon\nrecent advances in optimal transport (OT) to resolve the cross-domain matching\nproblem in a principled manner. Formulated as a drop-in regularizer, the\nproposed OT solution can be efficiently computed and used in combination with\nother existing approaches. We present empirical evidence to demonstrate the\neffectiveness of our approach, showing how it enables simpler model\narchitectures to outperform or be comparable with more sophisticated designs on\na range of vision-language tasks.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 22:48:36 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Yuan", "Siyang", ""], ["Bai", "Ke", ""], ["Chen", "Liqun", ""], ["Zhang", "Yizhe", ""], ["Tao", "Chenyang", ""], ["Li", "Chunyuan", ""], ["Wang", "Guoyin", ""], ["Henao", "Ricardo", ""], ["Carin", "Lawrence", ""]]}, {"id": "2008.06607", "submitter": "Jianbo Jiao", "authors": "Jianbo Jiao, Yifan Cai, Mohammad Alsharid, Lior Drukker, Aris\n  T.Papageorghiou, and J. Alison Noble", "title": "Self-supervised Contrastive Video-Speech Representation Learning for\n  Ultrasound", "comments": "MICCAI 2020 (early acceptance)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical imaging, manual annotations can be expensive to acquire and\nsometimes infeasible to access, making conventional deep learning-based models\ndifficult to scale. As a result, it would be beneficial if useful\nrepresentations could be derived from raw data without the need for manual\nannotations. In this paper, we propose to address the problem of\nself-supervised representation learning with multi-modal ultrasound\nvideo-speech raw data. For this case, we assume that there is a high\ncorrelation between the ultrasound video and the corresponding narrative speech\naudio of the sonographer. In order to learn meaningful representations, the\nmodel needs to identify such correlation and at the same time understand the\nunderlying anatomical features. We designed a framework to model the\ncorrespondence between video and audio without any kind of human annotations.\nWithin this framework, we introduce cross-modal contrastive learning and an\naffinity-aware self-paced learning scheme to enhance correlation modelling.\nExperimental evaluations on multi-modal fetal ultrasound video and audio show\nthat the proposed approach is able to learn strong representations and\ntransfers well to downstream tasks of standard plane detection and eye-gaze\nprediction.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 23:58:23 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Jiao", "Jianbo", ""], ["Cai", "Yifan", ""], ["Alsharid", "Mohammad", ""], ["Drukker", "Lior", ""], ["Papageorghiou", "Aris T.", ""], ["Noble", "J. Alison", ""]]}, {"id": "2008.06614", "submitter": "Xiangyun Zhao", "authors": "Xiangyun Zhao, Samuel Schulter, Gaurav Sharma, Yi-Hsuan Tsai, Manmohan\n  Chandraker, Ying Wu", "title": "Object Detection with a Unified Label Space from Multiple Datasets", "comments": "To appear in ECCV 2020, project page\n  http://www.nec-labs.com/~mas/UniDet/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given multiple datasets with different label spaces, the goal of this work is\nto train a single object detector predicting over the union of all the label\nspaces. The practical benefits of such an object detector are obvious and\nsignificant application-relevant categories can be picked and merged form\narbitrary existing datasets. However, naive merging of datasets is not possible\nin this case, due to inconsistent object annotations. Consider an object\ncategory like faces that is annotated in one dataset, but is not annotated in\nanother dataset, although the object itself appears in the latter images. Some\ncategories, like face here, would thus be considered foreground in one dataset,\nbut background in another. To address this challenge, we design a framework\nwhich works with such partial annotations, and we exploit a pseudo labeling\napproach that we adapt for our specific case. We propose loss functions that\ncarefully integrate partial but correct annotations with complementary but\nnoisy pseudo labels. Evaluation in the proposed novel setting requires full\nannotation on the test set. We collect the required annotations and define a\nnew challenging experimental setup for this task based one existing public\ndatasets. We show improved performances compared to competitive baselines and\nappropriate adaptations of existing work.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 00:51:27 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Zhao", "Xiangyun", ""], ["Schulter", "Samuel", ""], ["Sharma", "Gaurav", ""], ["Tsai", "Yi-Hsuan", ""], ["Chandraker", "Manmohan", ""], ["Wu", "Ying", ""]]}, {"id": "2008.06630", "submitter": "Igor Vasiljevic", "authors": "Igor Vasiljevic, Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Wolfram\n  Burgard, Greg Shakhnarovich, Adrien Gaidon", "title": "Neural Ray Surfaces for Self-Supervised Learning of Depth and Ego-motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning has emerged as a powerful tool for depth and\nego-motion estimation, leading to state-of-the-art results on benchmark\ndatasets. However, one significant limitation shared by current methods is the\nassumption of a known parametric camera model -- usually the standard pinhole\ngeometry -- leading to failure when applied to imaging systems that deviate\nsignificantly from this assumption (e.g., catadioptric cameras or underwater\nimaging). In this work, we show that self-supervision can be used to learn\naccurate depth and ego-motion estimation without prior knowledge of the camera\nmodel. Inspired by the geometric model of Grossberg and Nayar, we introduce\nNeural Ray Surfaces (NRS), convolutional networks that represent pixel-wise\nprojection rays, approximating a wide range of cameras. NRS are fully\ndifferentiable and can be learned end-to-end from unlabeled raw videos. We\ndemonstrate the use of NRS for self-supervised learning of visual odometry and\ndepth estimation from raw videos obtained using a wide variety of camera\nsystems, including pinhole, fisheye, and catadioptric.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 02:29:13 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Vasiljevic", "Igor", ""], ["Guizilini", "Vitor", ""], ["Ambrus", "Rares", ""], ["Pillai", "Sudeep", ""], ["Burgard", "Wolfram", ""], ["Shakhnarovich", "Greg", ""], ["Gaidon", "Adrien", ""]]}, {"id": "2008.06632", "submitter": "Zahra Anvari", "authors": "Zahra Anvari, Vassilis Athitsos", "title": "Dehaze-GLCGAN: Unpaired Single Image De-hazing via Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image de-hazing is a challenging problem, and it is far from solved.\nMost current solutions require paired image datasets that include both hazy\nimages and their corresponding haze-free ground-truth images. However, in\nreality, lighting conditions and other factors can produce a range of haze-free\nimages that can serve as ground truth for a hazy image, and a single ground\ntruth image cannot capture that range. This limits the scalability and\npracticality of paired image datasets in real-world applications. In this\npaper, we focus on unpaired single image de-hazing and we do not rely on the\nground truth image or physical scattering model. We reduce the image de-hazing\nproblem to an image-to-image translation problem and propose a dehazing\nGlobal-Local Cycle-consistent Generative Adversarial Network (Dehaze-GLCGAN).\nGenerator network of Dehaze-GLCGAN combines an encoder-decoder architecture\nwith residual blocks to better recover the haze free scene. We also employ a\nglobal-local discriminator structure to deal with spatially varying haze.\nThrough ablation study, we demonstrate the effectiveness of different factors\nin the performance of the proposed network. Our extensive experiments over\nthree benchmark datasets show that our network outperforms previous work in\nterms of PSNR and SSIM while being trained on smaller amount of data compared\nto other methods.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 02:43:00 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Anvari", "Zahra", ""], ["Athitsos", "Vassilis", ""]]}, {"id": "2008.06634", "submitter": "Yuqiao Liu", "authors": "Yuqiao Liu, Yanan Sun, Bing Xue, Mengjie Zhang", "title": "Evolving Deep Convolutional Neural Networks for Hyperspectral Image\n  Denoising", "comments": "8 pages, 4 figures, to be published in IJCNN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral images (HSIs) are susceptible to various noise factors leading\nto the loss of information, and the noise restricts the subsequent HSIs object\ndetection and classification tasks. In recent years, learning-based methods\nhave demonstrated their superior strengths in denoising the HSIs.\nUnfortunately, most of the methods are manually designed based on the extensive\nexpertise that is not necessarily available to the users interested. In this\npaper, we propose a novel algorithm to automatically build an optimal\nConvolutional Neural Network (CNN) to effectively denoise HSIs. Particularly,\nthe proposed algorithm focuses on the architectures and the initialization of\nthe connection weights of the CNN. The experiments of the proposed algorithm\nhave been well-designed and compared against the state-of-the-art peer\ncompetitors, and the experimental results demonstrate the competitive\nperformance of the proposed algorithm in terms of the different evaluation\nmetrics, visual assessments, and the computational complexity.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 03:04:11 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Liu", "Yuqiao", ""], ["Sun", "Yanan", ""], ["Xue", "Bing", ""], ["Zhang", "Mengjie", ""]]}, {"id": "2008.06651", "submitter": "Lichang Chen", "authors": "Lichang Chen, Guosheng Lin, Shijie Wang, Qingyao Wu", "title": "Graph Edit Distance Reward: Learning to Edit Scene Graph", "comments": "14 pages, 6 figures, ECCV camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene Graph, as a vital tool to bridge the gap between language domain and\nimage domain, has been widely adopted in the cross-modality task like VQA. In\nthis paper, we propose a new method to edit the scene graph according to the\nuser instructions, which has never been explored. To be specific, in order to\nlearn editing scene graphs as the semantics given by texts, we propose a Graph\nEdit Distance Reward, which is based on the Policy Gradient and Graph Matching\nalgorithm, to optimize neural symbolic model. In the context of text-editing\nimage retrieval, we validate the effectiveness of our method in CSS and CRIR\ndataset. Besides, CRIR is a new synthetic dataset generated by us, which we\nwill publish it soon for future use.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 04:52:16 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Chen", "Lichang", ""], ["Lin", "Guosheng", ""], ["Wang", "Shijie", ""], ["Wu", "Qingyao", ""]]}, {"id": "2008.06655", "submitter": "Yi Xu", "authors": "Xiang Li and Yuan Tian and Fuyao Zhang and Shuxue Quan and Yi Xu", "title": "Object Detection in the Context of Mobile Augmented Reality", "comments": "accepted to IEEE International Symposium on Mixed and Augmented\n  Reality (ISMAR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, numerous Deep Neural Network (DNN) models and\nframeworks have been developed to tackle the problem of real-time object\ndetection from RGB images. Ordinary object detection approaches process\ninformation from the images only, and they are oblivious to the camera pose\nwith regard to the environment and the scale of the environment. On the other\nhand, mobile Augmented Reality (AR) frameworks can continuously track a\ncamera's pose within the scene and can estimate the correct scale of the\nenvironment by using Visual-Inertial Odometry (VIO). In this paper, we propose\na novel approach that combines the geometric information from VIO with semantic\ninformation from object detectors to improve the performance of object\ndetection on mobile devices. Our approach includes three components: (1) an\nimage orientation correction method, (2) a scale-based filtering approach, and\n(3) an online semantic map. Each component takes advantage of the different\ncharacteristics of the VIO-based AR framework. We implemented the AR-enhanced\nfeatures using ARCore and the SSD Mobilenet model on Android phones. To\nvalidate our approach, we manually labeled objects in image sequences taken\nfrom 12 room-scale AR sessions. The results show that our approach can improve\non the accuracy of generic object detectors by 12% on our dataset.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 05:15:00 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Li", "Xiang", ""], ["Tian", "Yuan", ""], ["Zhang", "Fuyao", ""], ["Quan", "Shuxue", ""], ["Xu", "Yi", ""]]}, {"id": "2008.06672", "submitter": "Nanyu Li", "authors": "Nanyu Li, Yujuan Si, Duo Deng, Chunyu Yuan", "title": "ECG beats classification via online sparse dictionary and time pyramid\n  matching", "comments": "7 pages,5 figure", "journal-ref": "17th IEEE International Conference on Communication\n  Technology(ICCT 2017)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the Bag-Of-Word (BOW) algorithm provides efficient features and\npromotes the accuracy of the ECG classification system. However, BOW algorithm\nhas two shortcomings: (1). it has large quantization errors and poor\nreconstruction performance; (2). it loses heart beat's time information, and\nmay provide confusing features for different kinds of heart beats. Furthermore,\nECG classification system can be used for long time monitoring and analysis of\ncardiovascular patients, while a huge amount of data will be produced, so we\nurgently need an efficient compression algorithm. In view of the above\nproblems, we use the wavelet feature to construct the sparse dictionary, which\nlower the quantization error to a minimum. In order to reduce the complexity of\nour algorithm and adapt to large-scale heart beats operation, we combine the\nOnline Dictionary Learning with Feature-sign algorithm to update the dictionary\nand coefficients. Coefficients matrix is used to represent ECG beats, which\ngreatly reduces the memory consumption, and solve the problem of quantitative\nerror simultaneously. Finally, we construct the pyramid to match coefficients\nof each ECG beat. Thus, we obtain the features that contain the beat time\ninformation by time stochastic pooling. It is efficient to solve the problem of\nlosing time information. The experimental results show that: on the one hand,\nthe proposed algorithm has advantages of high reconstruction performance for\nBOW, this storage method is high fidelity and low memory consumption; on the\nother hand, our algorithm yields highest accuracy in ECG beats classification;\nso this method is more suitable for large-scale heart beats data storage and\nclassification.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 08:10:21 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Li", "Nanyu", ""], ["Si", "Yujuan", ""], ["Deng", "Duo", ""], ["Yuan", "Chunyu", ""]]}, {"id": "2008.06674", "submitter": "Yonghyun Kim", "authors": "Yonghyun Kim, Wonpyo Park, Jongju Shin", "title": "BroadFace: Looking at Tens of Thousands of People at Once for Face\n  Recognition", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The datasets of face recognition contain an enormous number of identities and\ninstances. However, conventional methods have difficulty in reflecting the\nentire distribution of the datasets because a mini-batch of small size contains\nonly a small portion of all identities. To overcome this difficulty, we propose\na novel method called BroadFace, which is a learning process to consider a\nmassive set of identities, comprehensively. In BroadFace, a linear classifier\nlearns optimal decision boundaries among identities from a large number of\nembedding vectors accumulated over past iterations. By referring more instances\nat once, the optimality of the classifier is naturally increased on the entire\ndatasets. Thus, the encoder is also globally optimized by referring the weight\nmatrix of the classifier. Moreover, we propose a novel compensation method to\nincrease the number of referenced instances in the training stage. BroadFace\ncan be easily applied on many existing methods to accelerate a learning process\nand obtain a significant improvement in accuracy without extra computational\nburden at inference stage. We perform extensive ablation studies and\nexperiments on various datasets to show the effectiveness of BroadFace, and\nalso empirically prove the validity of our compensation method. BroadFace\nachieves the state-of-the-art results with significant improvements on nine\ndatasets in 1:1 face verification and 1:N face identification tasks, and is\nalso effective in image retrieval.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 08:17:25 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Kim", "Yonghyun", ""], ["Park", "Wonpyo", ""], ["Shin", "Jongju", ""]]}, {"id": "2008.06698", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Maria Gonzalez-i-Calabuig, Carles Ventura and Xavier Gir\\'o-i-Nieto", "title": "Curriculum Learning for Recurrent Video Object Segmentation", "comments": "Extended abstract accepted at ECCV 2020 Women in Computer Vision\n  (WiCV) & Perception for Autonomous Driving (PAD) Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video object segmentation can be understood as a sequence-to-sequence task\nthat can benefit from the curriculum learning strategies for better and faster\ntraining of deep neural networks. This work explores different schedule\nsampling and frame skipping variations to significantly improve the performance\nof a recurrent architecture. Our results on the car class of the KITTI-MOTS\nchallenge indicate that, surprisingly, an inverse schedule sampling is a better\noption than a classic forward one. Also, that a progressive skipping of frames\nduring training is beneficial, but only when training with the ground truth\nmasks instead of the predicted ones. Source code and trained models are\navailable at http://imatge-upc.github.io/rvos-mots/.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 10:51:22 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Gonzalez-i-Calabuig", "Maria", ""], ["Ventura", "Carles", ""], ["Gir\u00f3-i-Nieto", "Xavier", ""]]}, {"id": "2008.06713", "submitter": "Dilip K. Prasad", "authors": "Ayush Singh, Ajay Bhave, Dilip K. Prasad", "title": "Single image dehazing for a variety of haze scenarios using back\n  projected pyramid network", "comments": "16 pages, 8 figures, to be published in Computer Vision ECCV 2020\n  Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to dehaze single hazy images, especially using a small training\ndataset is quite challenging. We propose a novel generative adversarial network\narchitecture for this problem, namely back projected pyramid network (BPPNet),\nthat gives good performance for a variety of challenging haze conditions,\nincluding dense haze and inhomogeneous haze. Our architecture incorporates\nlearning of multiple levels of complexities while retaining spatial context\nthrough iterative blocks of UNets and structural information of multiple scales\nthrough a novel pyramidal convolution block. These blocks together for the\ngenerator and are amenable to learning through back projection. We have shown\nthat our network can be trained without over-fitting using as few as 20 image\npairs of hazy and non-hazy images. We report the state of the art performances\non NTIRE 2018 homogeneous haze datasets for indoor and outdoor images, NTIRE\n2019 denseHaze dataset, and NTIRE 2020 non-homogeneous haze dataset.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 13:09:34 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Singh", "Ayush", ""], ["Bhave", "Ajay", ""], ["Prasad", "Dilip K.", ""]]}, {"id": "2008.06721", "submitter": "Tariq Rahim", "authors": "Tariq Rahim, Syed Ali Hassan, Soo Young Shin", "title": "A Deep Convolutional Neural Network for the Detection of Polyps in\n  Colonoscopy Images", "comments": "21Pages,7 Figues,Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computerized detection of colonic polyps remains an unsolved issue because of\nthe wide variation in the appearance, texture, color, size, and presence of the\nmultiple polyp-like imitators during colonoscopy. In this paper, we propose a\ndeep convolutional neural network based model for the computerized detection of\npolyps within colonoscopy images. The proposed model comprises 16 convolutional\nlayers with 2 fully connected layers, and a Softmax layer, where we implement a\nunique approach using different convolutional kernels within the same hidden\nlayer for deeper feature extraction. We applied two different activation\nfunctions, MISH and rectified linear unit activation functions for deeper\npropagation of information and self regularized smooth non-monotonicity.\nFurthermore, we used a generalized intersection of union, thus overcoming\nissues such as scale invariance, rotation, and shape. Data augmentation\ntechniques such as photometric and geometric distortions are adapted to\novercome the obstacles faced in polyp detection. Detailed benchmarked results\nare provided, showing better performance in terms of precision, sensitivity,\nF1- score, F2- score, and dice-coefficient, thus proving the efficacy of the\nproposed model.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 13:55:44 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Rahim", "Tariq", ""], ["Hassan", "Syed Ali", ""], ["Shin", "Soo Young", ""]]}, {"id": "2008.06775", "submitter": "Karan Goel", "authors": "Karan Goel, Albert Gu, Yixuan Li and Christopher R\\'e", "title": "Model Patching: Closing the Subgroup Performance Gap with Data\n  Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers in machine learning are often brittle when deployed. Particularly\nconcerning are models with inconsistent performance on specific subgroups of a\nclass, e.g., exhibiting disparities in skin cancer classification in the\npresence or absence of a spurious bandage. To mitigate these performance\ndifferences, we introduce model patching, a two-stage framework for improving\nrobustness that encourages the model to be invariant to subgroup differences,\nand focus on class information shared by subgroups. Model patching first models\nsubgroup features within a class and learns semantic transformations between\nthem, and then trains a classifier with data augmentations that deliberately\nmanipulate subgroup features. We instantiate model patching with CAMEL, which\n(1) uses a CycleGAN to learn the intra-class, inter-subgroup augmentations, and\n(2) balances subgroup performance using a theoretically-motivated subgroup\nconsistency regularizer, accompanied by a new robust objective. We demonstrate\nCAMEL's effectiveness on 3 benchmark datasets, with reductions in robust error\nof up to 33% relative to the best baseline. Lastly, CAMEL successfully patches\na model that fails due to spurious features on a real-world skin cancer\ndataset.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 20:01:23 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Goel", "Karan", ""], ["Gu", "Albert", ""], ["Li", "Yixuan", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "2008.06780", "submitter": "Francesco La Rosa", "authors": "Francesco La Rosa, Erin S Beck, Ahmed Abdulkadir, Jean-Philippe\n  Thiran, Daniel S Reich, Pascal Sati, Meritxell Bach Cuadra", "title": "Automated Detection of Cortical Lesions in Multiple Sclerosis Patients\n  with 7T MRI", "comments": "Accepted to MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automated detection of cortical lesions (CLs) in patients with multiple\nsclerosis (MS) is a challenging task that, despite its clinical relevance, has\nreceived very little attention. Accurate detection of the small and scarce\nlesions requires specialized sequences and high or ultra-high field MRI. For\nsupervised training based on multimodal structural MRI at 7T, two experts\ngenerated ground truth segmentation masks of 60 patients with 2014 CLs. We\nimplemented a simplified 3D U-Net with three resolution levels (3D U-Net-). By\nincreasing the complexity of the task (adding brain tissue segmentation), while\nrandomly dropping input channels during training, we improved the performance\ncompared to the baseline. Considering a minimum lesion size of 0.75 {\\mu}L, we\nachieved a lesion-wise cortical lesion detection rate of 67% and a false\npositive rate of 42%. However, 393 (24%) of the lesions reported as false\npositives were post-hoc confirmed as potential or definite lesions by an\nexpert. This indicates the potential of the proposed method to support experts\nin the tedious process of CL manual segmentation.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 20:35:12 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["La Rosa", "Francesco", ""], ["Beck", "Erin S", ""], ["Abdulkadir", "Ahmed", ""], ["Thiran", "Jean-Philippe", ""], ["Reich", "Daniel S", ""], ["Sati", "Pascal", ""], ["Cuadra", "Meritxell Bach", ""]]}, {"id": "2008.06810", "submitter": "Qiuyu Chen", "authors": "Qiuyu Chen, Wei Zhang, Jianping Fan", "title": "Cluster-level Feature Alignment for Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Instance-level alignment is widely exploited for person re-identification,\ne.g. spatial alignment, latent semantic alignment and triplet alignment. This\npaper probes another feature alignment modality, namely cluster-level feature\nalignment across whole dataset, where the model can see not only the sampled\nimages in local mini-batch but the global feature distribution of the whole\ndataset from distilled anchors. Towards this aim, we propose anchor loss and\ninvestigate many variants of cluster-level feature alignment, which consists of\niterative aggregation and alignment from the overview of dataset. Our extensive\nexperiments have demonstrated that our methods can provide consistent and\nsignificant performance improvement with small training efforts after the\nsaturation of traditional training. In both theoretical and experimental\naspects, our proposed methods can result in more stable and guided optimization\ntowards better representation and generalization for well-aligned embedding.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 23:47:47 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Chen", "Qiuyu", ""], ["Zhang", "Wei", ""], ["Fan", "Jianping", ""]]}, {"id": "2008.06814", "submitter": "Roy Miles", "authors": "Roy Miles and Krystian Mikolajczyk", "title": "Cascaded channel pruning using hierarchical self-distillation", "comments": "BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an approach for filter-level pruning with\nhierarchical knowledge distillation based on the teacher, teaching-assistant,\nand student framework. Our method makes use of teaching assistants at\nintermediate pruning levels that share the same architecture and weights as the\ntarget student. We propose to prune each model independently using the gradient\ninformation from its corresponding teacher. By considering the relative sizes\nof each student-teacher pair, this formulation provides a natural trade-off\nbetween the capacity gap for knowledge distillation and the bias of the filter\nsaliency updates. Our results show improvements in the attainable accuracy and\nmodel compression across the CIFAR10 and ImageNet classification tasks using\nthe VGG16and ResNet50 architectures. We provide an extensive evaluation that\ndemonstrates the benefits of using a varying number of teaching assistant\nmodels at different sizes.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 00:19:35 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Miles", "Roy", ""], ["Mikolajczyk", "Krystian", ""]]}, {"id": "2008.06822", "submitter": "Sizhe Chen", "authors": "Sizhe Chen, Fan He, Xiaolin Huang, Kun Zhang", "title": "Relevance Attack on Detectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on high-transferable adversarial attacks on detectors,\nwhich are hard to attack in a black-box manner, because of their\nmultiple-output characteristics and the diversity across architectures. To\npursue a high attack transferability, one plausible way is to find a common\nproperty across detectors, which facilitates the discovery of common\nweaknesses. We are the first to suggest that the relevance map from\ninterpreters for detectors is such a property. Based on it, we design a\nRelevance Attack on Detectors (RAD), which achieves a state-of-the-art\ntransferability, exceeding existing results by above 20%. On MS COCO, the\ndetection mAPs for all 8 black-box architectures are more than halved and the\nsegmentation mAPs are also significantly influenced. Given the great\ntransferability of RAD, we generate the first adversarial dataset for object\ndetection and instance segmentation, i.e., Adversarial Objects in COntext\n(AOCO), which helps to quickly evaluate and improve the robustness of\ndetectors.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 02:44:25 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 06:27:56 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Chen", "Sizhe", ""], ["He", "Fan", ""], ["Huang", "Xiaolin", ""], ["Zhang", "Kun", ""]]}, {"id": "2008.06826", "submitter": "Guan-An Wang", "authors": "Guan'an Wang, Shaogang Gong, Jian Cheng and Zengguang Hou", "title": "Faster Person Re-Identification", "comments": "accepted by ECCV2020, https://github.com/wangguanan/light-reid", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast person re-identification (ReID) aims to search person images quickly and\naccurately. The main idea of recent fast ReID methods is the hashing algorithm,\nwhich learns compact binary codes and performs fast Hamming distance and\ncounting sort. However, a very long code is needed for high accuracy (e.g.\n2048), which compromises search speed. In this work, we introduce a new\nsolution for fast ReID by formulating a novel Coarse-to-Fine (CtF) hashing code\nsearch strategy, which complementarily uses short and long codes, achieving\nboth faster speed and better accuracy. It uses shorter codes to coarsely rank\nbroad matching similarities and longer codes to refine only a few top\ncandidates for more accurate instance ReID. Specifically, we design an\nAll-in-One (AiO) framework together with a Distance Threshold Optimization\n(DTO) algorithm. In AiO, we simultaneously learn and enhance multiple codes of\ndifferent lengths in a single model. It learns multiple codes in a pyramid\nstructure, and encourage shorter codes to mimic longer codes by\nself-distillation. DTO solves a complex threshold search problem by a simple\noptimization process, and the balance between accuracy and speed is easily\ncontrolled by a single parameter. It formulates the optimization target as a\n$F_{\\beta}$ score that can be optimised by Gaussian cumulative distribution\nfunctions. Experimental results on 2 datasets show that our proposed method\n(CtF) is not only 8% more accurate but also 5x faster than contemporary hashing\nReID methods. Compared with non-hashing ReID methods, CtF is $50\\times$ faster\nwith comparable accuracy. Code is available at\nhttps://github.com/wangguanan/light-reid.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 03:02:49 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Wang", "Guan'an", ""], ["Gong", "Shaogang", ""], ["Cheng", "Jian", ""], ["Hou", "Zengguang", ""]]}, {"id": "2008.06828", "submitter": "Nguyen Quoc Khanh Le Dr.", "authors": "Hieu X. Le, Phuong D. Nguyen, Thang H. Nguyen, Khanh N.Q. Le, Thanh T.\n  Nguyen", "title": "A novel approach to remove foreign objects from chest X-ray images", "comments": "9 pages, 7 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initially proposed a deep learning approach for foreign objects inpainting\nin smartphone-camera captured chest radiographs utilizing the cheXphoto\ndataset. Foreign objects which can significantly affect the quality of a\ncomputer-aided diagnostic prediction are captured under various settings. In\nthis paper, we used multi-method to tackle both removal and inpainting chest\nradiographs. Firstly, an object detection model is trained to separate the\nforeign objects from the given image. Subsequently, the binary mask of each\nobject is extracted utilizing a segmentation model. Each pair of the binary\nmask and the extracted object are then used for inpainting purposes. Finally,\nthe in-painted regions are now merged back to the original image, resulting in\na clean and non-foreign-object-existing output. To conclude, we achieved\nstate-of-the-art accuracy. The experimental results showed a new approach to\nthe possible applications of this method for chest X-ray images detection.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 03:06:28 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Le", "Hieu X.", ""], ["Nguyen", "Phuong D.", ""], ["Nguyen", "Thang H.", ""], ["Le", "Khanh N. Q.", ""], ["Nguyen", "Thanh T.", ""]]}, {"id": "2008.06837", "submitter": "Matloob Khushi Dr", "authors": "Matloob Khushi, Georgina Edwards, Diego Alonso de Marcos, Jane E\n  Carpenter, J Dinny Graham and Christine L Clarke", "title": "Open source tools for management and archiving of digital microscopy\n  data to allow integration with patient pathology and treatment information", "comments": null, "journal-ref": "Diagnostic Pathology volume 8, Article number: 22 (2013)", "doi": "10.1186/1746-1596-8-22", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Virtual microscopy includes digitisation of histology slides and the use of\ncomputer technologies for complex investigation of diseases such as cancer.\nHowever, automated image analysis, or website publishing of such digital\nimages, is hampered by their large file sizes. We have developed two Java based\nopen source tools: Snapshot Creator and NDPI-Splitter. Snapshot Creator\nconverts a portion of a large digital slide into a desired quality JPEG image.\nThe image is linked to the patients clinical and treatment information in a\ncustomised open source cancer data management software (Caisis) in use at the\nAustralian Breast Cancer Tissue Bank (ABCTB) and then published on the ABCTB\nwebsite www.abctb.org.au using Deep Zoom open source technology. Using the\nABCTB online search engine, digital images can be searched by defining various\ncriteria such as cancer type, or biomarkers expressed. NDPI-Splitter splits a\nlarge image file into smaller sections of TIFF images so that they can be\neasily analysed by image analysis software such as Metamorph or Matlab.\nNDPI-Splitter also has the capacity to filter out empty images. Snapshot\nCreator and NDPI-Splitter are novel open source Java tools. They convert\ndigital slides into files of smaller size for further processing. In\nconjunction with other open source tools such as Deep Zoom and Caisis, this\nsuite of tools is used for the management and archiving of digital microscopy\nimages, enabling digitised images to be explored and zoomed online. Our online\nimage repository also has the capacity to be used as a teaching resource. These\ntools also enable large files to be sectioned for image analysis.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 04:28:53 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Khushi", "Matloob", ""], ["Edwards", "Georgina", ""], ["de Marcos", "Diego Alonso", ""], ["Carpenter", "Jane E", ""], ["Graham", "J Dinny", ""], ["Clarke", "Christine L", ""]]}, {"id": "2008.06840", "submitter": "Hengli Wang", "authors": "Rui Fan, Hengli Wang, Mohammud J. Bocus, Ming Liu", "title": "We Learn Better Road Pothole Detection: from Attention Aggregation to\n  Adversarial Domain Adaptation", "comments": "16 pages, 7 figures and 2 tables. This paper is accepted by ECCV\n  Workshops 2020", "journal-ref": null, "doi": "10.1007/978-3-030-66823-5_17", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manual visual inspection performed by certified inspectors is still the main\nform of road pothole detection. This process is, however, not only tedious,\ntime-consuming and costly, but also dangerous for the inspectors. Furthermore,\nthe road pothole detection results are always subjective, because they depend\nentirely on the individual experience. Our recently introduced disparity (or\ninverse depth) transformation algorithm allows better discrimination between\ndamaged and undamaged road areas, and it can be easily deployed to any semantic\nsegmentation network for better road pothole detection results. To boost the\nperformance, we propose a novel attention aggregation (AA) framework, which\ntakes the advantages of different types of attention modules. In addition, we\ndevelop an effective training set augmentation technique based on adversarial\ndomain adaptation, where the synthetic road RGB images and transformed road\ndisparity (or inverse depth) images are generated to enhance the training of\nsemantic segmentation networks. The experimental results demonstrate that,\nfirstly, the transformed disparity (or inverse depth) images become more\ninformative; secondly, AA-UNet and AA-RTFNet, our best performing\nimplementations, respectively outperform all other state-of-the-art\nsingle-modal and data-fusion networks for road pothole detection; and finally,\nthe training set augmentation technique based on adversarial domain adaptation\nnot only improves the accuracy of the state-of-the-art semantic segmentation\nnetworks, but also accelerates their convergence.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 05:17:42 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 10:32:39 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Fan", "Rui", ""], ["Wang", "Hengli", ""], ["Bocus", "Mohammud J.", ""], ["Liu", "Ming", ""]]}, {"id": "2008.06841", "submitter": "Matloob Khushi Dr", "authors": "Zhiwen Zeng and Matloob Khushi", "title": "Wavelet Denoising and Attention-based RNN-ARIMA Model to Predict Forex\n  Price", "comments": null, "journal-ref": "IJCNN 2020", "doi": null, "report-no": null, "categories": "cs.CE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every change of trend in the forex market presents a great opportunity as\nwell as a risk for investors. Accurate forecasting of forex prices is a crucial\nelement in any effective hedging or speculation strategy. However, the complex\nnature of the forex market makes the predicting problem challenging, which has\nprompted extensive research from various academic disciplines. In this paper, a\nnovel approach that integrates the wavelet denoising, Attention-based Recurrent\nNeural Network (ARNN), and Autoregressive Integrated Moving Average (ARIMA) are\nproposed. Wavelet transform removes the noise from the time series to stabilize\nthe data structure. ARNN model captures the robust and non-linear relationships\nin the sequence and ARIMA can well fit the linear correlation of the sequential\ninformation. By hybridization of the three models, the methodology is capable\nof modelling dynamic systems such as the forex market. Our experiments on\nUSD/JPY five-minute data outperforms the baseline methods.\nRoot-Mean-Squared-Error (RMSE) of the hybrid approach was found to be 1.65 with\na directional accuracy of ~76%.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 05:32:40 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Zeng", "Zhiwen", ""], ["Khushi", "Matloob", ""]]}, {"id": "2008.06843", "submitter": "Yuxiang Wei", "authors": "Yuxiang Wei, Ming Liu, Haolin Wang, Ruifeng Zhu, Guosheng Hu, Wangmeng\n  Zuo", "title": "Learning Flow-based Feature Warping for Face Frontalization with\n  Illumination Inconsistent Supervision", "comments": "ECCV 2020. Code is available at: https://github.com/csyxwei/FFWM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent advances in deep learning-based face frontalization methods,\nphoto-realistic and illumination preserving frontal face synthesis is still\nchallenging due to large pose and illumination discrepancy during training. We\npropose a novel Flow-based Feature Warping Model (FFWM) which can learn to\nsynthesize photo-realistic and illumination preserving frontal images with\nillumination inconsistent supervision. Specifically, an Illumination Preserving\nModule (IPM) is proposed to learn illumination preserving image synthesis from\nillumination inconsistent image pairs. IPM includes two pathways which\ncollaborate to ensure the synthesized frontal images are illumination\npreserving and with fine details. Moreover, a Warp Attention Module (WAM) is\nintroduced to reduce the pose discrepancy in the feature level, and hence to\nsynthesize frontal images more effectively and preserve more details of profile\nimages. The attention mechanism in WAM helps reduce the artifacts caused by the\ndisplacements between the profile and the frontal images. Quantitative and\nqualitative experimental results show that our FFWM can synthesize\nphoto-realistic and illumination preserving frontal images and performs\nfavorably against the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 06:07:00 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 12:51:26 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Wei", "Yuxiang", ""], ["Liu", "Ming", ""], ["Wang", "Haolin", ""], ["Zhu", "Ruifeng", ""], ["Hu", "Guosheng", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "2008.06861", "submitter": "Koteswar Rao Jerripothula", "authors": "Daksh Goyal, Koteswar Rao Jerripothula, Ankush Mittal", "title": "Detection of Gait Abnormalities caused by Neurological Disorders", "comments": "6 pages, 5 figures, Accepted by IEEE Workshop on Multimedia Signal\n  Processing (MMSP), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we leverage gait to potentially detect some of the important\nneurological disorders, namely Parkinson's disease, Diplegia, Hemiplegia, and\nHuntington's Chorea. Persons with these neurological disorders often have a\nvery abnormal gait, which motivates us to target gait for their potential\ndetection. Some of the abnormalities involve the circumduction of legs,\nforward-bending, involuntary movements, etc. To detect such abnormalities in\ngait, we develop gait features from the key-points of the human pose, namely\nshoulders, elbows, hips, knees, ankles, etc. To evaluate the effectiveness of\nour gait features in detecting the abnormalities related to these diseases, we\nbuild a synthetic video dataset of persons mimicking the gait of persons with\nsuch disorders, considering the difficulty in finding a sufficient number of\npeople with these disorders. We name it \\textit{NeuroSynGait} video dataset.\nExperiments demonstrated that our gait features were indeed successful in\ndetecting these abnormalities.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 09:00:36 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Goyal", "Daksh", ""], ["Jerripothula", "Koteswar Rao", ""], ["Mittal", "Ankush", ""]]}, {"id": "2008.06866", "submitter": "Angel Ayala", "authors": "Angel Ayala, Bruno Fernandes, Francisco Cruz, David Mac\\^edo, Adriano\n  L. I. Oliveira, and Cleber Zanchettin", "title": "KutralNet: A Portable Deep Learning Model for Fire Recognition", "comments": "Accepted in the IEEE International Joint Conference on Neural\n  Networks (IJCNN), 2020", "journal-ref": "2020 International Joint Conference on Neural Networks (IJCNN)", "doi": "10.1109/IJCNN48605.2020.9207202", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the automatic fire alarm systems detect the fire presence through\nsensors like thermal, smoke, or flame. One of the new approaches to the problem\nis the use of images to perform the detection. The image approach is promising\nsince it does not need specific sensors and can be easily embedded in different\ndevices. However, besides the high performance, the computational cost of the\nused deep learning methods is a challenge to their deployment in portable\ndevices. In this work, we propose a new deep learning architecture that\nrequires fewer floating-point operations (flops) for fire recognition.\nAdditionally, we propose a portable approach for fire recognition and the use\nof modern techniques such as inverted residual block, convolutions like\ndepth-wise, and octave, to reduce the model's computational cost. The\nexperiments show that our model keeps high accuracy while substantially\nreducing the number of parameters and flops. One of our models presents 71\\%\nfewer parameters than FireNet, while still presenting competitive accuracy and\nAUROC performance. The proposed methods are evaluated on FireNet and FiSmo\ndatasets. The obtained results are promising for the implementation of the\nmodel in a mobile device, considering the reduced number of flops and\nparameters acquired.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 09:35:25 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Ayala", "Angel", ""], ["Fernandes", "Bruno", ""], ["Cruz", "Francisco", ""], ["Mac\u00eado", "David", ""], ["Oliveira", "Adriano L. I.", ""], ["Zanchettin", "Cleber", ""]]}, {"id": "2008.06872", "submitter": "Sergey Prokudin", "authors": "Sergey Prokudin, Michael J. Black, Javier Romero", "title": "SMPLpix: Neural Avatars from 3D Human Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep generative models have led to an unprecedented level\nof realism for synthetically generated images of humans. However, one of the\nremaining fundamental limitations of these models is the ability to flexibly\ncontrol the generative process, e.g.~change the camera and human pose while\nretaining the subject identity. At the same time, deformable human body models\nlike SMPL and its successors provide full control over pose and shape but rely\non classic computer graphics pipelines for rendering. Such rendering pipelines\nrequire explicit mesh rasterization that (a) does not have the potential to fix\nartifacts or lack of realism in the original 3D geometry and (b) until\nrecently, were not fully incorporated into deep learning frameworks. In this\nwork, we propose to bridge the gap between classic geometry-based rendering and\nthe latest generative networks operating in pixel space. We train a network\nthat directly converts a sparse set of 3D mesh vertices into photorealistic\nimages, alleviating the need for traditional rasterization mechanism. We train\nour model on a large corpus of human 3D models and corresponding real photos,\nand show the advantage over conventional differentiable renderers both in terms\nof the level of photorealism and rendering efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 10:22:00 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 11:08:09 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Prokudin", "Sergey", ""], ["Black", "Michael J.", ""], ["Romero", "Javier", ""]]}, {"id": "2008.06880", "submitter": "Shengyu Zhang", "authors": "Shengyu Zhang, Ziqi Tan, Jin Yu, Zhou Zhao, Kun Kuang, Jie Liu,\n  Jingren Zhou, Hongxia Yang, Fei Wu", "title": "Poet: Product-oriented Video Captioner for E-commerce", "comments": "10 pages, 3 figures, to appear in ACM MM 2020 proceedings", "journal-ref": null, "doi": "10.1145/3394171.3413880", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In e-commerce, a growing number of user-generated videos are used for product\npromotion. How to generate video descriptions that narrate the user-preferred\nproduct characteristics depicted in the video is vital for successful\npromoting. Traditional video captioning methods, which focus on routinely\ndescribing what exists and happens in a video, are not amenable for\nproduct-oriented video captioning. To address this problem, we propose a\nproduct-oriented video captioner framework, abbreviated as Poet. Poet firstly\nrepresents the videos as product-oriented spatial-temporal graphs. Then, based\non the aspects of the video-associated product, we perform knowledge-enhanced\nspatial-temporal inference on those graphs for capturing the dynamic change of\nfine-grained product-part characteristics. The knowledge leveraging module in\nPoet differs from the traditional design by performing knowledge filtering and\ndynamic memory modeling. We show that Poet achieves consistent performance\nimprovement over previous methods concerning generation quality, product\naspects capturing, and lexical diversity. Experiments are performed on two\nproduct-oriented video captioning datasets, buyer-generated fashion video\ndataset (BFVD) and fan-generated fashion video dataset (FFVD), collected from\nMobile Taobao. We will release the desensitized datasets to promote further\ninvestigations on both video captioning and general video analysis problems.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 10:53:46 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Zhang", "Shengyu", ""], ["Tan", "Ziqi", ""], ["Yu", "Jin", ""], ["Zhao", "Zhou", ""], ["Kuang", "Kun", ""], ["Liu", "Jie", ""], ["Zhou", "Jingren", ""], ["Yang", "Hongxia", ""], ["Wu", "Fei", ""]]}, {"id": "2008.06883", "submitter": "Junbing Li", "authors": "Junbing Li, Changqing Zhang, Pengfei Zhu, Baoyuan Wu, Lei Chen,\n  Qinghua Hu", "title": "SPL-MLL: Selecting Predictable Landmarks for Multi-Label Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although significant progress achieved, multi-label classification is still\nchallenging due to the complexity of correlations among different labels.\nFurthermore, modeling the relationships between input and some (dull) classes\nfurther increases the difficulty of accurately predicting all possible labels.\nIn this work, we propose to select a small subset of labels as landmarks which\nare easy to predict according to input (predictable) and can well recover the\nother possible labels (representative). Different from existing methods which\nseparate the landmark selection and landmark prediction in the 2-step manner,\nthe proposed algorithm, termed Selecting Predictable Landmarks for Multi-Label\nLearning (SPL-MLL), jointly conducts landmark selection, landmark prediction,\nand label recovery in a unified framework, to ensure both the\nrepresentativeness and predictableness for selected landmarks. We employ the\nAlternating Direction Method (ADM) to solve our problem. Empirical studies on\nreal-world datasets show that our method achieves superior classification\nperformance over other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 11:07:44 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Li", "Junbing", ""], ["Zhang", "Changqing", ""], ["Zhu", "Pengfei", ""], ["Wu", "Baoyuan", ""], ["Chen", "Lei", ""], ["Hu", "Qinghua", ""]]}, {"id": "2008.06884", "submitter": "Shengyu Zhang", "authors": "Shengyu Zhang, Tan Jiang, Tan Wang, Kun Kuang, Zhou Zhao, Jianke Zhu,\n  Jin Yu, Hongxia Yang, Fei Wu", "title": "DeVLBert: Learning Deconfounded Visio-Linguistic Representations", "comments": "10 pages, 4 figures, to appear in ACM MM 2020 proceedings", "journal-ref": null, "doi": "10.1145/3394171.3413518", "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to investigate the problem of out-of-domain\nvisio-linguistic pretraining, where the pretraining data distribution differs\nfrom that of downstream data on which the pretrained model will be fine-tuned.\nExisting methods for this problem are purely likelihood-based, leading to the\nspurious correlations and hurt the generalization ability when transferred to\nout-of-domain downstream tasks. By spurious correlation, we mean that the\nconditional probability of one token (object or word) given another one can be\nhigh (due to the dataset biases) without robust (causal) relationships between\nthem. To mitigate such dataset biases, we propose a Deconfounded\nVisio-Linguistic Bert framework, abbreviated as DeVLBert, to perform\nintervention-based learning. We borrow the idea of the backdoor adjustment from\nthe research field of causality and propose several neural-network based\narchitectures for Bert-style out-of-domain pretraining. The quantitative\nresults on three downstream tasks, Image Retrieval (IR), Zero-shot IR, and\nVisual Question Answering, show the effectiveness of DeVLBert by boosting\ngeneralization ability.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 11:09:22 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 12:00:56 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Zhang", "Shengyu", ""], ["Jiang", "Tan", ""], ["Wang", "Tan", ""], ["Kuang", "Kun", ""], ["Zhao", "Zhou", ""], ["Zhu", "Jianke", ""], ["Yu", "Jin", ""], ["Yang", "Hongxia", ""], ["Wu", "Fei", ""]]}, {"id": "2008.06893", "submitter": "Zhangxuan Gu", "authors": "Zhangxuan Gu, and Siyuan Zhou, and Li Niu, and Zihan Zhao, and Liqing\n  Zhang", "title": "Context-aware Feature Generation for Zero-shot Semantic Segmentation", "comments": "Accepted by ACM MM 2020", "journal-ref": null, "doi": "10.1145/3394171.3413593", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing semantic segmentation models heavily rely on dense pixel-wise\nannotations. To reduce the annotation pressure, we focus on a challenging task\nnamed zero-shot semantic segmentation, which aims to segment unseen objects\nwith zero annotations. This task can be accomplished by transferring knowledge\nacross categories via semantic word embeddings. In this paper, we propose a\nnovel context-aware feature generation method for zero-shot segmentation named\nCaGNet. In particular, with the observation that a pixel-wise feature highly\ndepends on its contextual information, we insert a contextual module in a\nsegmentation network to capture the pixel-wise contextual information, which\nguides the process of generating more diverse and context-aware features from\nsemantic word embeddings. Our method achieves state-of-the-art results on three\nbenchmark datasets for zero-shot segmentation. Codes are available at:\nhttps://github.com/bcmi/CaGNet-Zero-Shot-Semantic-Segmentation.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 12:20:49 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Gu", "Zhangxuan", ""], ["Zhou", "Siyuan", ""], ["Niu", "Li", ""], ["Zhao", "Zihan", ""], ["Zhang", "Liqing", ""]]}, {"id": "2008.06909", "submitter": "Da Chen", "authors": "Da Chen, Jian Zhu, Xinxin Zhang, Minglei Shu and Laurent D. Cohen", "title": "Geodesic Paths for Image Segmentation with Implicit Region-based\n  Homogeneity Enhancement", "comments": "Published in IEEE Trans. Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2021.3078106", "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimal paths are regarded as a powerful and efficient tool for boundary\ndetection and image segmentation due to its global optimality and the\nwell-established numerical solutions such as fast marching method. In this\npaper, we introduce a flexible interactive image segmentation model based on\nthe Eikonal partial differential equation (PDE) framework in conjunction with\nregion-based homogeneity enhancement. A key ingredient in the introduced model\nis the construction of local geodesic metrics, which are capable of integrating\nanisotropic and asymmetric edge features, implicit region-based homogeneity\nfeatures and/or curvature regularization. The incorporation of the region-based\nhomogeneity features into the metrics considered relies on an implicit\nrepresentation of these features, which is one of the contributions of this\nwork. Moreover, we also introduce a way to build simple closed contours as the\nconcatenation of two disjoint open curves. Experimental results prove that the\nproposed model indeed outperforms state-of-the-art minimal paths-based image\nsegmentation approaches.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 13:29:11 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 01:32:42 GMT"}, {"version": "v3", "created": "Tue, 4 May 2021 09:16:37 GMT"}, {"version": "v4", "created": "Thu, 6 May 2021 14:07:42 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Chen", "Da", ""], ["Zhu", "Jian", ""], ["Zhang", "Xinxin", ""], ["Shu", "Minglei", ""], ["Cohen", "Laurent D.", ""]]}, {"id": "2008.06910", "submitter": "Andrei Zanfir", "authors": "Andrei Zanfir, Eduard Gabriel Bazavan, Mihai Zanfir, William T.\n  Freeman, Rahul Sukthankar, Cristian Sminchisescu", "title": "Neural Descent for Visual 3D Human Pose and Shape", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present deep neural network methodology to reconstruct the 3d pose and\nshape of people, given an input RGB image. We rely on a recently introduced,\nexpressivefull body statistical 3d human model, GHUM, trained end-to-end, and\nlearn to reconstruct its pose and shape state in a self-supervised regime.\nCentral to our methodology, is a learning to learn and optimize approach,\nreferred to as HUmanNeural Descent (HUND), which avoids both second-order\ndifferentiation when training the model parameters,and expensive state gradient\ndescent in order to accurately minimize a semantic differentiable rendering\nloss at test time. Instead, we rely on novel recurrent stages to update the\npose and shape parameters such that not only losses are minimized effectively,\nbut the process is meta-regularized in order to ensure end-progress. HUND's\nsymmetry between training and testing makes it the first 3d human sensing\narchitecture to natively support different operating regimes including\nself-supervised ones. In diverse tests, we show that HUND achieves very\ncompetitive results in datasets like H3.6M and 3DPW, aswell as good quality 3d\nreconstructions for complex imagery collected in-the-wild.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 13:38:41 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 09:10:36 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Zanfir", "Andrei", ""], ["Bazavan", "Eduard Gabriel", ""], ["Zanfir", "Mihai", ""], ["Freeman", "William T.", ""], ["Sukthankar", "Rahul", ""], ["Sminchisescu", "Cristian", ""]]}, {"id": "2008.06939", "submitter": "Richard Granger", "authors": "Elijah Bowen, Antonio Rodriguez, Damian Sowinski, Richard Granger", "title": "Visual stream connectivity predicts assessments of image quality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Some biological mechanisms of early vision are comparatively well understood,\nbut they have yet to be evaluated for their ability to accurately predict and\nexplain human judgments of image similarity. From well-studied simple\nconnectivity patterns in early vision, we derive a novel formalization of the\npsychophysics of similarity, showing the differential geometry that provides\naccurate and explanatory accounts of perceptual similarity judgments. These\npredictions then are further improved via simple regression on human behavioral\nreports, which in turn are used to construct more elaborate hypothesized neural\nconnectivity patterns. Both approaches outperform standard successful measures\nof perceived image fidelity from the literature, as well as providing\nexplanatory principles of similarity perception.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 15:38:17 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Bowen", "Elijah", ""], ["Rodriguez", "Antonio", ""], ["Sowinski", "Damian", ""], ["Granger", "Richard", ""]]}, {"id": "2008.06941", "submitter": "Zhu Zhang", "authors": "Zhu Zhang, Zhou Zhao, Zhijie Lin, Baoxing Huai and Nicholas Jing Yuan", "title": "Object-Aware Multi-Branch Relation Networks for Spatio-Temporal Video\n  Grounding", "comments": "Accepted by IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal video grounding aims to retrieve the spatio-temporal tube of\na queried object according to the given sentence. Currently, most existing\ngrounding methods are restricted to well-aligned segment-sentence pairs. In\nthis paper, we explore spatio-temporal video grounding on unaligned data and\nmulti-form sentences. This challenging task requires to capture critical object\nrelations to identify the queried target. However, existing approaches cannot\ndistinguish notable objects and remain in ineffective relation modeling between\nunnecessary objects. Thus, we propose a novel object-aware multi-branch\nrelation network for object-aware relation discovery. Concretely, we first\ndevise multiple branches to develop object-aware region modeling, where each\nbranch focuses on a crucial object mentioned in the sentence. We then propose\nmulti-branch relation reasoning to capture critical object relationships\nbetween the main branch and auxiliary branches. Moreover, we apply a diversity\nloss to make each branch only pay attention to its corresponding object and\nboost multi-branch learning. The extensive experiments show the effectiveness\nof our proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 15:39:56 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2020 11:11:32 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Zhang", "Zhu", ""], ["Zhao", "Zhou", ""], ["Lin", "Zhijie", ""], ["Huai", "Baoxing", ""], ["Yuan", "Nicholas Jing", ""]]}, {"id": "2008.06959", "submitter": "Daniyar Turmukhambetov", "authors": "Iaroslav Melekhov, Gabriel J. Brostow, Juho Kannala, Daniyar\n  Turmukhambetov", "title": "Image Stylization for Robust Features", "comments": "v1.1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local features that are robust to both viewpoint and appearance changes are\ncrucial for many computer vision tasks. In this work we investigate if\nphotorealistic image stylization improves robustness of local features to not\nonly day-night, but also weather and season variations. We show that image\nstylization in addition to color augmentation is a powerful method of learning\nrobust features. We evaluate learned features on visual localization\nbenchmarks, outperforming state of the art baseline models despite training\nwithout ground-truth 3D correspondences using synthetic homographies only.\n  We use trained feature networks to compete in Long-Term Visual Localization\nand Map-based Localization for Autonomous Driving challenges achieving\ncompetitive scores.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 17:03:03 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Melekhov", "Iaroslav", ""], ["Brostow", "Gabriel J.", ""], ["Kannala", "Juho", ""], ["Turmukhambetov", "Daniyar", ""]]}, {"id": "2008.06963", "submitter": "Shizhen Zhao", "authors": "Shizhen Zhao, Changxin Gao, Jun Zhang, Hao Cheng, Chuchu Han, Xinyang\n  Jiang, Xiaowei Guo, Wei-Shi Zheng, Nong Sang, Xing Sun", "title": "Do Not Disturb Me: Person Re-identification Under the Interference of\n  Other Pedestrians", "comments": "Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the conventional person Re-ID setting, it is widely assumed that cropped\nperson images are for each individual. However, in a crowded scene,\noff-shelf-detectors may generate bounding boxes involving multiple people,\nwhere the large proportion of background pedestrians or human occlusion exists.\nThe representation extracted from such cropped images, which contain both the\ntarget and the interference pedestrians, might include distractive information.\nThis will lead to wrong retrieval results. To address this problem, this paper\npresents a novel deep network termed Pedestrian-Interference Suppression\nNetwork (PISNet). PISNet leverages a Query-Guided Attention Block (QGAB) to\nenhance the feature of the target in the gallery, under the guidance of the\nquery. Furthermore, the involving Guidance Reversed Attention Module and the\nMulti-Person Separation Loss promote QGAB to suppress the interference of other\npedestrians. Our method is evaluated on two new pedestrian-interference\ndatasets and the results show that the proposed method performs favorably\nagainst existing Re-ID methods.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 17:45:14 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Zhao", "Shizhen", ""], ["Gao", "Changxin", ""], ["Zhang", "Jun", ""], ["Cheng", "Hao", ""], ["Han", "Chuchu", ""], ["Jiang", "Xinyang", ""], ["Guo", "Xiaowei", ""], ["Zheng", "Wei-Shi", ""], ["Sang", "Nong", ""], ["Sun", "Xing", ""]]}, {"id": "2008.06966", "submitter": "Jeremy Tan", "authors": "Jeremy Tan, Anselm Au, Qingjie Meng, Sandy FinesilverSmith, John\n  Simpson, Daniel Rueckert, Reza Razavi, Thomas Day, David Lloyd, Bernhard\n  Kainz", "title": "Automated Detection of Congenital Heart Disease in Fetal Ultrasound\n  Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prenatal screening with ultrasound can lower neonatal mortality significantly\nfor selected cardiac abnormalities. However, the need for human expertise,\ncoupled with the high volume of screening cases, limits the practically\nachievable detection rates. In this paper we discuss the potential for deep\nlearning techniques to aid in the detection of congenital heart disease (CHD)\nin fetal ultrasound. We propose a pipeline for automated data curation and\nclassification. During both training and inference, we exploit an auxiliary\nview classification task to bias features toward relevant cardiac structures.\nThis bias helps to improve in F1-scores from 0.72 and 0.77 to 0.87 and 0.85 for\nhealthy and CHD classes respectively.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 17:53:41 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 01:29:45 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Tan", "Jeremy", ""], ["Au", "Anselm", ""], ["Meng", "Qingjie", ""], ["FinesilverSmith", "Sandy", ""], ["Simpson", "John", ""], ["Rueckert", "Daniel", ""], ["Razavi", "Reza", ""], ["Day", "Thomas", ""], ["Lloyd", "David", ""], ["Kainz", "Bernhard", ""]]}, {"id": "2008.06967", "submitter": "Yu Feng", "authors": "Yu Feng, Boyuan Tian, Tiancheng Xu, Paul Whatmough, Yuhao Zhu", "title": "Mesorasi: Architecture Support for Point Cloud Analytics via\n  Delayed-Aggregation", "comments": null, "journal-ref": "Proceedings of the 53nd (2020) Annual IEEE/ACM International\n  Symposium on Microarchitecture", "doi": null, "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud analytics is poised to become a key workload on battery-powered\nembedded and mobile platforms in a wide range of emerging application domains,\nsuch as autonomous driving, robotics, and augmented reality, where efficiency\nis paramount. This paper proposes Mesorasi, an algorithm-architecture\nco-designed system that simultaneously improves the performance and energy\nefficiency of point cloud analytics while retaining its accuracy. Our extensive\ncharacterizations of state-of-the-art point cloud algorithms show that, while\nstructurally reminiscent of convolutional neural networks (CNNs), point cloud\nalgorithms exhibit inherent compute and memory inefficiencies due to the unique\ncharacteristics of point cloud data. We propose delayed-aggregation, a new\nalgorithmic primitive for building efficient point cloud algorithms.\nDelayed-aggregation hides the performance bottlenecks and reduces the compute\nand memory redundancies by exploiting the approximately distributive property\nof key operations in point cloud algorithms. Delayed-aggregation let point\ncloud algorithms achieve 1.6x speedup and 51.1% energy reduction on a mobile\nGPU while retaining the accuracy (-0.9% loss to 1.2% gains). To maximize the\nalgorithmic benefits, we propose minor extensions to contemporary CNN\naccelerators, which can be integrated into a mobile Systems-on-a-Chip (SoC)\nwithout modifying other SoC components. With additional hardware support,\nMesorasi achieves up to 3.6x speedup.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 18:11:19 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Feng", "Yu", ""], ["Tian", "Boyuan", ""], ["Xu", "Tiancheng", ""], ["Whatmough", "Paul", ""], ["Zhu", "Yuhao", ""]]}, {"id": "2008.06981", "submitter": "Zhipeng Bao", "authors": "Zhipeng Bao, Yu-Xiong Wang and Martial Hebert", "title": "Bowtie Networks: Generative Modeling for Joint Few-Shot Recognition and\n  Novel-View Synthesis", "comments": "Accepted as a Poster paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel task of joint few-shot recognition and novel-view\nsynthesis: given only one or few images of a novel object from arbitrary views\nwith only category annotation, we aim to simultaneously learn an object\nclassifier and generate images of that type of object from new viewpoints.\nWhile existing work copes with two or more tasks mainly by multi-task learning\nof shareable feature representations, we take a different perspective. We focus\non the interaction and cooperation between a generative model and a\ndiscriminative model, in a way that facilitates knowledge to flow across tasks\nin complementary directions. To this end, we propose bowtie networks that\njointly learn 3D geometric and semantic representations with a feedback loop.\nExperimental evaluation on challenging fine-grained recognition datasets\ndemonstrates that our synthesized images are realistic from multiple viewpoints\nand significantly improve recognition performance as ways of data augmentation,\nespecially in the low-data regime. Code and pre-trained models are released at\nhttps://github.com/zpbao/bowtie_networks.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 19:40:56 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 19:18:51 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Bao", "Zhipeng", ""], ["Wang", "Yu-Xiong", ""], ["Hebert", "Martial", ""]]}, {"id": "2008.06982", "submitter": "Khoi Nguyen", "authors": "Khoi Nguyen, Sinisa Todorovic", "title": "A Self-supervised GAN for Unsupervised Few-shot Object Recognition", "comments": "To be appeared in ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses unsupervised few-shot object recognition, where all\ntraining images are unlabeled, and test images are divided into queries and a\nfew labeled support images per object class of interest. The training and test\nimages do not share object classes. We extend the vanilla GAN with two loss\nfunctions, both aimed at self-supervised learning. The first is a\nreconstruction loss that enforces the discriminator to reconstruct the\nprobabilistically sampled latent code which has been used for generating the\n\"fake\" image. The second is a triplet loss that enforces the discriminator to\noutput image encodings that are closer for more similar images. Evaluation,\ncomparisons, and detailed ablation studies are done in the context of few-shot\nclassification. Our approach significantly outperforms the state of the art on\nthe Mini-Imagenet and Tiered-Imagenet datasets.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 19:47:26 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 18:05:25 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Nguyen", "Khoi", ""], ["Todorovic", "Sinisa", ""]]}, {"id": "2008.06986", "submitter": "Subrata Goswami", "authors": "Subrata Goswami", "title": "False Detection (Positives and Negatives) in Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is a very important function of visual perception systems.\nSince the early days of classical object detection based on HOG to modern deep\nlearning based detectors, object detection has improved in accuracy. Two stage\ndetectors usually have higher accuracy than single stage ones. Both types of\ndetectors use some form of quantization of the search space of rectangular\nregions of image. There are far more of the quantized elements than true\nobjects. The way these bounding boxes are filtered out possibly results in the\nfalse positive and false negatives. This empirical experimental study explores\nways of reducing false positives and negatives with labelled data.. In the\nprocess also discovered insufficient labelling in Openimage 2019 Object\nDetection dataset.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 20:09:05 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Goswami", "Subrata", ""]]}, {"id": "2008.06989", "submitter": "V\\'itor Albiero", "authors": "V\\'itor Albiero and Kevin W. Bowyer", "title": "Is Face Recognition Sexist? No, Gendered Hairstyles and Biology Are", "comments": "Paper will appear on BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent news articles have accused face recognition of being \"biased\",\n\"sexist\" or \"racist\". There is consensus in the research literature that face\nrecognition accuracy is lower for females, who often have both a higher false\nmatch rate and a higher false non-match rate. However, there is little\npublished research aimed at identifying the cause of lower accuracy for\nfemales. For instance, the 2019 Face Recognition Vendor Test that documents\nlower female accuracy across a broad range of algorithms and datasets also\nlists \"Analyze cause and effect\" under the heading \"What we did not do\". We\npresent the first experimental analysis to identify major causes of lower face\nrecognition accuracy for females on datasets where previous research has\nobserved this result. Controlling for equal amount of visible face in the test\nimages reverses the apparent higher false non-match rate for females. Also,\nprincipal component analysis indicates that images of two different females are\ninherently more similar than of two different males, potentially accounting for\na difference in false match rates.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 20:29:05 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Albiero", "V\u00edtor", ""], ["Bowyer", "Kevin W.", ""]]}, {"id": "2008.06997", "submitter": "Hanqing Chao", "authors": "Hanqing Chao, Hongming Shan, Fatemeh Homayounieh, Ramandeep Singh,\n  Ruhani Doda Khera, Hengtao Guo, Timothy Su, Ge Wang, Mannudeep K. Kalra,\n  Pingkun Yan", "title": "Deep Learning Predicts Cardiovascular Disease Risks from Lung Cancer\n  Screening Low Dose Computed Tomography", "comments": null, "journal-ref": null, "doi": "10.1038/s41467-021-23235-4", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cancer patients have a higher risk of cardiovascular disease (CVD) mortality\nthan the general population. Low dose computed tomography (LDCT) for lung\ncancer screening offers an opportunity for simultaneous CVD risk estimation in\nat-risk patients. Our deep learning CVD risk prediction model, trained with\n30,286 LDCTs from the National Lung Cancer Screening Trial, achieved an area\nunder the curve (AUC) of 0.871 on a separate test set of 2,085 subjects and\nidentified patients with high CVD mortality risks (AUC of 0.768). We validated\nour model against ECG-gated cardiac CT based markers, including coronary artery\ncalcification (CAC) score, CAD-RADS score, and MESA 10-year risk score from an\nindependent dataset of 335 subjects. Our work shows that, in high-risk\npatients, deep learning can convert LDCT for lung cancer screening into a\ndual-screening quantitative tool for CVD risk estimation.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 21:07:01 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 15:15:03 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Chao", "Hanqing", ""], ["Shan", "Hongming", ""], ["Homayounieh", "Fatemeh", ""], ["Singh", "Ramandeep", ""], ["Khera", "Ruhani Doda", ""], ["Guo", "Hengtao", ""], ["Su", "Timothy", ""], ["Wang", "Ge", ""], ["Kalra", "Mannudeep K.", ""], ["Yan", "Pingkun", ""]]}, {"id": "2008.06999", "submitter": "Marc Gantenbein", "authors": "Marc Gantenbein and Ertunc Erdil and Ender Konukoglu", "title": "RevPHiSeg: A Memory-Efficient Neural Network for Uncertainty\n  Quantification in Medical Image Segmentation", "comments": "Accepted to UNSURE by MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying segmentation uncertainty has become an important issue in medical\nimage analysis due to the inherent ambiguity of anatomical structures and its\npathologies. Recently, neural network-based uncertainty quantification methods\nhave been successfully applied to various problems. One of the main limitations\nof the existing techniques is the high memory requirement during training;\nwhich limits their application to processing smaller field-of-views (FOVs)\nand/or using shallower architectures. In this paper, we investigate the effect\nof using reversible blocks for building memory-efficient neural network\narchitectures for quantification of segmentation uncertainty. The reversible\narchitecture achieves memory saving by exactly computing the activations from\nthe outputs of the subsequent layers during backpropagation instead of storing\nthe activations for each layer. We incorporate the reversible blocks into a\nrecently proposed architecture called PHiSeg that is developed for uncertainty\nquantification in medical image segmentation. The reversible architecture,\nRevPHiSeg, allows training neural networks for quantifying segmentation\nuncertainty on GPUs with limited memory and processing larger FOVs. We perform\nexperiments on the LIDC-IDRI dataset and an in-house prostate dataset, and\npresent comparisons with PHiSeg. The results demonstrate that RevPHiSeg\nconsumes ~30% less memory compared to PHiSeg while achieving very similar\nsegmentation accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 21:16:19 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 08:18:44 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Gantenbein", "Marc", ""], ["Erdil", "Ertunc", ""], ["Konukoglu", "Ender", ""]]}, {"id": "2008.07000", "submitter": "Szymon P{\\l}otka", "authors": "Tomasz W{\\l}odarczyk, Szymon P{\\l}otka, Przemys{\\l}aw Rokita, Nicole\n  Sochacki-W\\'ojcicka, Jakub W\\'ojcicki, Micha{\\l} Lipa, Tomasz Trzci\\'nski", "title": "Spontaneous preterm birth prediction using convolutional neural networks", "comments": "Accepted at MICCAI Workshop on Perinatal, Preterm and Paediatric\n  Image analysis (PIPPI) 2020, Lima, Peru", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An estimated 15 million babies are born too early every year. Approximately 1\nmillion children die each year due to complications of preterm birth (PTB).\nMany survivors face a lifetime of disability, including learning disabilities\nand visual and hearing problems. Although manual analysis of ultrasound images\n(US) is still prevalent, it is prone to errors due to its subjective component\nand complex variations in the shape and position of organs across patients. In\nthis work, we introduce a conceptually simple convolutional neural network\n(CNN) trained for segmenting prenatal ultrasound images and classifying task\nfor the purpose of preterm birth detection. Our method efficiently segments\ndifferent types of cervixes in transvaginal ultrasound images while\nsimultaneously predicting a preterm birth based on extracted image features\nwithout human oversight. We employed three popular network models: U-Net, Fully\nConvolutional Network, and Deeplabv3 for the cervix segmentation task. Based on\nthe conducted results and model efficiency, we decided to extend U-Net by\nadding a parallel branch for classification task. The proposed model is trained\nand evaluated on a dataset consisting of 354 2D transvaginal ultrasound images\nand achieved a segmentation accuracy with a mean Jaccard coefficient index of\n0.923 $\\pm$ 0.081 and a classification sensitivity of 0.677 $\\pm$ 0.042 with a\n3.49\\% false positive rate. Our method obtained better results in the\nprediction of preterm birth based on transvaginal ultrasound images compared to\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 21:21:33 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 19:35:33 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["W\u0142odarczyk", "Tomasz", ""], ["P\u0142otka", "Szymon", ""], ["Rokita", "Przemys\u0142aw", ""], ["Sochacki-W\u00f3jcicka", "Nicole", ""], ["W\u00f3jcicki", "Jakub", ""], ["Lipa", "Micha\u0142", ""], ["Trzci\u0144ski", "Tomasz", ""]]}, {"id": "2008.07001", "submitter": "Marah Halawa", "authors": "Marah Halawa, Manuel W\\\"ollhaf, Eduardo Vellasques, Urko S\\'anchez\n  Sanz, and Olaf Hellwich", "title": "Learning Disentangled Expression Representations from Facial Images", "comments": "Accepted at ECCV2020 workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face images are subject to many different factors of variation, especially in\nunconstrained in-the-wild scenarios. For most tasks involving such images, e.g.\nexpression recognition from video streams, having enough labeled data is\nprohibitively expensive. One common strategy to tackle such a problem is to\nlearn disentangled representations for the different factors of variation of\nthe observed data using adversarial learning. In this paper, we use a\nformulation of the adversarial loss to learn disentangled representations for\nface images. The used model facilitates learning on single-task datasets and\nimproves the state-of-the-art in expression recognition with an accuracy\nof60.53%on the AffectNetdataset, without using any additional data.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 21:23:32 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 06:58:13 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Halawa", "Marah", ""], ["W\u00f6llhaf", "Manuel", ""], ["Vellasques", "Eduardo", ""], ["Sanz", "Urko S\u00e1nchez", ""], ["Hellwich", "Olaf", ""]]}, {"id": "2008.07008", "submitter": "Senthil Yogamani", "authors": "Eslam Mohamed, Mahmoud Ewaisha, Mennatullah Siam, Hazem Rashed,\n  Senthil Yogamani, Waleed Hamdy, Muhammad Helmi and Ahmad El-Sallab", "title": "Monocular Instance Motion Segmentation for Autonomous Driving: KITTI\n  InstanceMotSeg Dataset and Multi-task Baseline", "comments": "Accepted for presentation at IEEE IV 2021 (Intelligent Vehicles\n  Symposium) conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moving object segmentation is a crucial task for autonomous vehicles as it\ncan be used to segment objects in a class agnostic manner based on their motion\ncues. It enables the detection of unseen objects during training (e.g., moose\nor a construction truck) based on their motion and independent of their\nappearance. Although pixel-wise motion segmentation has been studied in\nautonomous driving literature, it has been rarely addressed at the instance\nlevel, which would help separate connected segments of moving objects leading\nto better trajectory planning. As the main issue is the lack of large public\ndatasets, we create a new InstanceMotSeg dataset comprising of 12.9K samples\nimproving upon our KITTIMoSeg dataset. In addition to providing instance level\nannotations, we have added 4 additional classes which is crucial for studying\nclass agnostic motion segmentation. We adapt YOLACT and implement a\nmotion-based class agnostic instance segmentation model which would act as a\nbaseline for the dataset. We also extend it to an efficient multi-task model\nwhich additionally provides semantic instance segmentation sharing the encoder.\nThe model then learns separate prototype coefficients within the class agnostic\nand semantic heads providing two independent paths of object detection for\nredundant safety. To obtain real-time performance, we study different efficient\nencoders and obtain 39 fps on a Titan Xp GPU using MobileNetV2 with an\nimprovement of 10% mAP relative to the baseline. Our model improves the\nprevious state of the art motion segmentation method by 3.3%. The dataset and\nqualitative results video are shared in our website at\nhttps://sites.google.com/view/instancemotseg/.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 21:47:09 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 18:01:09 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2021 22:58:48 GMT"}, {"version": "v4", "created": "Wed, 26 May 2021 15:12:49 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Mohamed", "Eslam", ""], ["Ewaisha", "Mahmoud", ""], ["Siam", "Mennatullah", ""], ["Rashed", "Hazem", ""], ["Yogamani", "Senthil", ""], ["Hamdy", "Waleed", ""], ["Helmi", "Muhammad", ""], ["El-Sallab", "Ahmad", ""]]}, {"id": "2008.07012", "submitter": "Yanchao Yang", "authors": "Yanchao Yang, Brian Lai and Stefano Soatto", "title": "DyStaB: Unsupervised Object Segmentation via Dynamic-Static\n  Bootstrapping", "comments": "camera-ready version for CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an unsupervised method to detect and segment portions of images\nof live scenes that, at some point in time, are seen moving as a coherent\nwhole, which we refer to as objects. Our method first partitions the motion\nfield by minimizing the mutual information between segments. Then, it uses the\nsegments to learn object models that can be used for detection in a static\nimage. Static and dynamic models are represented by deep neural networks\ntrained jointly in a bootstrapping strategy, which enables extrapolation to\npreviously unseen objects. While the training process requires motion, the\nresulting object segmentation network can be used on either static images or\nvideos at inference time. As the volume of seen videos grows, more and more\nobjects are seen moving, priming their detection, which then serves as a\nregularizer for new objects, turning our method into unsupervised continual\nlearning to segment objects. Our models are compared to the state of the art in\nboth video object segmentation and salient object detection. In the six\nbenchmark datasets tested, our models compare favorably even to those using\npixel-level supervision, despite requiring no manual annotation.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 22:05:13 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 06:25:46 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Yang", "Yanchao", ""], ["Lai", "Brian", ""], ["Soatto", "Stefano", ""]]}, {"id": "2008.07015", "submitter": "Elahe Arani", "authors": "Elahe Arani, Fahad Sarfraz and Bahram Zonooz", "title": "Adversarial Concurrent Training: Optimizing Robustness and Accuracy\n  Trade-off of Deep Neural Networks", "comments": "Accepted at 31st British Machine Vision Conference (BMVC) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training has been proven to be an effective technique for\nimproving the adversarial robustness of models. However, there seems to be an\ninherent trade-off between optimizing the model for accuracy and robustness. To\nthis end, we propose Adversarial Concurrent Training (ACT), which employs\nadversarial training in a collaborative learning framework whereby we train a\nrobust model in conjunction with a natural model in a minimax game. ACT\nencourages the two models to align their feature space by using the\ntask-specific decision boundaries and explore the input space more broadly.\nFurthermore, the natural model acts as a regularizer, enforcing priors on\nfeatures that the robust model should learn. Our analyses on the behavior of\nthe models show that ACT leads to a robust model with lower model complexity,\nhigher information compression in the learned representations, and high\nposterior entropy solutions indicative of convergence to a flatter minima. We\ndemonstrate the effectiveness of the proposed approach across different\ndatasets and network architectures. On ImageNet, ACT achieves 68.20% standard\naccuracy and 44.29% robustness accuracy under a 100-iteration untargeted\nattack, improving upon the standard adversarial training method's 65.70%\nstandard accuracy and 42.36% robustness.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 22:14:48 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 18:31:40 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Arani", "Elahe", ""], ["Sarfraz", "Fahad", ""], ["Zonooz", "Bahram", ""]]}, {"id": "2008.07018", "submitter": "Xinyu Gong", "authors": "Xinyu Gong, Wuyang Chen, Yifan Jiang, Ye Yuan, Xianming Liu, Qian\n  Zhang, Yuan Li, Zhangyang Wang", "title": "AutoPose: Searching Multi-Scale Branch Aggregation for Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present AutoPose, a novel neural architecture search(NAS) framework that\nis capable of automatically discovering multiple parallel branches of\ncross-scale connections towards accurate and high-resolution 2D human pose\nestimation. Recently, high-performance hand-crafted convolutional networks for\npose estimation show growing demands on multi-scale fusion and high-resolution\nrepresentations. However, current NAS works exhibit limited flexibility on\nscale searching, they dominantly adopt simplified search spaces of\nsingle-branch architectures. Such simplification limits the fusion of\ninformation at different scales and fails to maintain high-resolution\nrepresentations. The presentedAutoPose framework is able to search for\nmulti-branch scales and network depth, in addition to the cell-level\nmicrostructure. Motivated by the search space, a novel bi-level optimization\nmethod is presented, where the network-level architecture is searched via\nreinforcement learning, and the cell-level search is conducted by the\ngradient-based method. Within 2.5 GPU days, AutoPose is able to find very\ncompetitive architectures on the MS COCO dataset, that are also transferable to\nthe MPII dataset. Our code is available at\nhttps://github.com/VITA-Group/AutoPose.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 22:27:43 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Gong", "Xinyu", ""], ["Chen", "Wuyang", ""], ["Jiang", "Yifan", ""], ["Yuan", "Ye", ""], ["Liu", "Xianming", ""], ["Zhang", "Qian", ""], ["Li", "Yuan", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2008.07030", "submitter": "Chun Kit Wong", "authors": "Chun Kit Wong, Stephanie Marchesseau, Maria Kalimeri, Tiang Siew Yap,\n  Serena S. H. Teo, Lingaraj Krishna, Alfredo Franco-Obreg\\'on, Stacey K. H.\n  Tay, Chin Meng Khoo, Philip T. H. Lee, Melvin K. S. Leow, John J. Totman,\n  Mary C. Stephenson", "title": "Training CNN Classifiers for Semantic Segmentation using Partially\n  Annotated Images: with Application on Human Thigh and Calf MRI", "comments": "Submitted to IEEE Transactions on Medical Imaging (Special Issue on\n  Annotation-Efficient Deep Learning for Medical Imaging)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Medical image datasets with pixel-level labels tend to have a\nlimited number of organ or tissue label classes annotated, even when the images\nhave wide anatomical coverage. With supervised learning, multiple classifiers\nare usually needed given these partially annotated datasets. In this work, we\npropose a set of strategies to train one single classifier in segmenting all\nlabel classes that are heterogeneously annotated across multiple datasets\nwithout moving into semi-supervised learning. Methods: Masks were first created\nfrom each label image through a process we termed presence masking. Three\npresence masking modes were evaluated, differing mainly in weightage assigned\nto the annotated and unannotated classes. These masks were then applied to the\nloss function during training to remove the influence of unannotated classes.\nResults: Evaluation against publicly available CT datasets shows that presence\nmasking is a viable method for training class-generic classifiers. Our\nclass-generic classifier can perform as well as multiple class-specific\nclassifiers combined, while the training duration is similar to that required\nfor one class-specific classifier. Furthermore, the class-generic classifier\ncan outperform the class-specific classifiers when trained on smaller datasets.\nFinally, consistent results are observed from evaluations against human thigh\nand calf MRI datasets collected in-house. Conclusion: The evaluation outcomes\nshow that presence masking is capable of significantly improving both training\nand inference efficiency across imaging modalities and anatomical regions.\nImproved performance may even be observed on small datasets. Significance:\nPresence masking strategies can reduce the computational resources and costs\ninvolved in manual medical image annotations. All codes are publicly available\nat https://github.com/wong-ck/DeepSegment.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 23:38:02 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Wong", "Chun Kit", ""], ["Marchesseau", "Stephanie", ""], ["Kalimeri", "Maria", ""], ["Yap", "Tiang Siew", ""], ["Teo", "Serena S. H.", ""], ["Krishna", "Lingaraj", ""], ["Franco-Obreg\u00f3n", "Alfredo", ""], ["Tay", "Stacey K. H.", ""], ["Khoo", "Chin Meng", ""], ["Lee", "Philip T. H.", ""], ["Leow", "Melvin K. S.", ""], ["Totman", "John J.", ""], ["Stephenson", "Mary C.", ""]]}, {"id": "2008.07043", "submitter": "Jingru Yi", "authors": "Jingru Yi, Pengxiang Wu, Bo Liu, Qiaoying Huang, Hui Qu, Dimitris\n  Metaxas", "title": "Oriented Object Detection in Aerial Images with Box Boundary-Aware\n  Vectors", "comments": "Accepted to WACV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Oriented object detection in aerial images is a challenging task as the\nobjects in aerial images are displayed in arbitrary directions and are usually\ndensely packed. Current oriented object detection methods mainly rely on\ntwo-stage anchor-based detectors. However, the anchor-based detectors typically\nsuffer from a severe imbalance issue between the positive and negative anchor\nboxes. To address this issue, in this work we extend the horizontal\nkeypoint-based object detector to the oriented object detection task. In\nparticular, we first detect the center keypoints of the objects, based on which\nwe then regress the box boundary-aware vectors (BBAVectors) to capture the\noriented bounding boxes. The box boundary-aware vectors are distributed in the\nfour quadrants of a Cartesian coordinate system for all arbitrarily oriented\nobjects. To relieve the difficulty of learning the vectors in the corner cases,\nwe further classify the oriented bounding boxes into horizontal and rotational\nbounding boxes. In the experiment, we show that learning the box boundary-aware\nvectors is superior to directly predicting the width, height, and angle of an\noriented bounding box, as adopted in the baseline method. Besides, the proposed\nmethod competes favorably with state-of-the-art methods. Code is available at\nhttps://github.com/yijingru/BBAVectors-Oriented-Object-Detection.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 00:56:50 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2020 06:38:18 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Yi", "Jingru", ""], ["Wu", "Pengxiang", ""], ["Liu", "Bo", ""], ["Huang", "Qiaoying", ""], ["Qu", "Hui", ""], ["Metaxas", "Dimitris", ""]]}, {"id": "2008.07049", "submitter": "Yang Wu", "authors": "Yuzheng Xu, Yang Wu, Nur Sabrina binti Zuraimi, Shohei Nobuhara and Ko\n  Nishino", "title": "Video Region Annotation with Sparse Bounding Boxes", "comments": "Accepted for publication in BMVC 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Video analysis has been moving towards more detailed interpretation (e.g.\nsegmentation) with encouraging progresses. These tasks, however, increasingly\nrely on densely annotated training data both in space and time. Since such\nannotation is labour-intensive, few densely annotated video data with detailed\nregion boundaries exist. This work aims to resolve this dilemma by learning to\nautomatically generate region boundaries for all frames of a video from\nsparsely annotated bounding boxes of target regions. We achieve this with a\nVolumetric Graph Convolutional Network (VGCN), which learns to iteratively find\nkeypoints on the region boundaries using the spatio-temporal volume of\nsurrounding appearance and motion. The global optimization of VGCN makes it\nsignificantly stronger and generalize better than existing solutions.\nExperimental results using two latest datasets (one real and one synthetic),\nincluding ablation studies, demonstrate the effectiveness and superiority of\nour method.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 01:27:20 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Xu", "Yuzheng", ""], ["Wu", "Yang", ""], ["Zuraimi", "Nur Sabrina binti", ""], ["Nobuhara", "Shohei", ""], ["Nishino", "Ko", ""]]}, {"id": "2008.07064", "submitter": "Shuhan Chen", "authors": "Shuhan Chen, Yun Fu", "title": "Progressively Guided Alternate Refinement Network for RGB-D Salient\n  Object Detection", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to develop an efficient and compact deep network for\nRGB-D salient object detection, where the depth image provides complementary\ninformation to boost performance in complex scenarios. Starting from a coarse\ninitial prediction by a multi-scale residual block, we propose a progressively\nguided alternate refinement network to refine it. Instead of using ImageNet\npre-trained backbone network, we first construct a lightweight depth stream by\nlearning from scratch, which can extract complementary features more\nefficiently with less redundancy. Then, different from the existing fusion\nbased methods, RGB and depth features are fed into proposed guided residual\n(GR) blocks alternately to reduce their mutual degradation. By assigning\nprogressive guidance in the stacked GR blocks within each side-output, the\nfalse detection and missing parts can be well remedied. Extensive experiments\non seven benchmark datasets demonstrate that our model outperforms existing\nstate-of-the-art approaches by a large margin, and also shows superiority in\nefficiency (71 FPS) and model size (64.9 MB).\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 02:55:06 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Chen", "Shuhan", ""], ["Fu", "Yun", ""]]}, {"id": "2008.07071", "submitter": "Dewen Zeng", "authors": "Dewen Zeng, Weiwen Jiang, Tianchen Wang, Xiaowei Xu, Haiyun Yuan,\n  Meiping Huang, Jian Zhuang, Jingtong Hu, Yiyu Shi", "title": "Towards Cardiac Intervention Assistance: Hardware-aware Neural\n  Architecture Exploration for Real-Time 3D Cardiac Cine MRI Segmentation", "comments": "8 pages, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time cardiac magnetic resonance imaging (MRI) plays an increasingly\nimportant role in guiding various cardiac interventions. In order to provide\nbetter visual assistance, the cine MRI frames need to be segmented on-the-fly\nto avoid noticeable visual lag. In addition, considering reliability and\npatient data privacy, the computation is preferably done on local hardware.\nState-of-the-art MRI segmentation methods mostly focus on accuracy only, and\ncan hardly be adopted for real-time application or on local hardware. In this\nwork, we present the first hardware-aware multi-scale neural architecture\nsearch (NAS) framework for real-time 3D cardiac cine MRI segmentation. The\nproposed framework incorporates a latency regularization term into the loss\nfunction to handle real-time constraints, with the consideration of underlying\nhardware. In addition, the formulation is fully differentiable with respect to\nthe architecture parameters, so that stochastic gradient descent (SGD) can be\nused for optimization to reduce the computation cost while maintaining\noptimization quality. Experimental results on ACDC MICCAI 2017 dataset\ndemonstrate that our hardware-aware multi-scale NAS framework can reduce the\nlatency by up to 3.5 times and satisfy the real-time constraints, while still\nachieving competitive segmentation accuracy, compared with the state-of-the-art\nNAS segmentation framework.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 03:22:57 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 01:11:50 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Zeng", "Dewen", ""], ["Jiang", "Weiwen", ""], ["Wang", "Tianchen", ""], ["Xu", "Xiaowei", ""], ["Yuan", "Haiyun", ""], ["Huang", "Meiping", ""], ["Zhuang", "Jian", ""], ["Hu", "Jingtong", ""], ["Shi", "Yiyu", ""]]}, {"id": "2008.07073", "submitter": "Nadine Chang", "authors": "Nadine Chang, Jayanth Koushik, Michael J. Tarr, Martial Hebert,\n  Yu-Xiong Wang", "title": "Alpha Net: Adaptation with Composition in Classifier Space", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning classification models typically train poorly on classes with\nsmall numbers of examples. Motivated by the human ability to solve this task,\nmodels have been developed that transfer knowledge from classes with many\nexamples to learn classes with few examples. Critically, the majority of these\nmodels transfer knowledge within model feature space. In this work, we\ndemonstrate that transferring knowledge within classified space is more\neffective and efficient. Specifically, by linearly combining strong nearest\nneighbor classifiers along with a weak classifier, we are able to compose a\nstronger classifier. Uniquely, our model can be implemented on top of any\nexisting classification model that includes a classifier layer. We showcase the\nsuccess of our approach in the task of long-tailed recognition, whereby the\nclasses with few examples, otherwise known as the \"tail\" classes, suffer the\nmost in performance and are the most challenging classes to learn. Using\nclassifier-level knowledge transfer, we are able to drastically improve - by a\nmargin as high as 12.6% - the state-of-the-art performance on the \"tail\"\ncategories.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 03:31:39 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Chang", "Nadine", ""], ["Koushik", "Jayanth", ""], ["Tarr", "Michael J.", ""], ["Hebert", "Martial", ""], ["Wang", "Yu-Xiong", ""]]}, {"id": "2008.07083", "submitter": "Haneul Ko", "authors": "Seung Wook Kim, Keunsoo Ko, Haneul Ko, Victor C. M. Leung", "title": "Edge Network-Assisted Real-Time Object Detection Framework for\n  Autonomous Driving", "comments": "This paper will be published in IEEE Network", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous vehicles (AVs) can achieve the desired results within a short\nduration by offloading tasks even requiring high computational power (e.g.,\nobject detection (OD)) to edge clouds. However, although edge clouds are\nexploited, real-time OD cannot always be guaranteed due to dynamic channel\nquality. To mitigate this problem, we propose an edge network-assisted\nreal-time OD framework~(EODF). In an EODF, AVs extract the region of\ninterests~(RoIs) of the captured image when the channel quality is not\nsufficiently good for supporting real-time OD. Then, AVs compress the image\ndata on the basis of the RoIs and transmit the compressed one to the edge\ncloud. In so doing, real-time OD can be achieved owing to the reduced\ntransmission latency. To verify the feasibility of our framework, we evaluate\nthe probability that the results of OD are not received within the inter-frame\nduration (i.e., outage probability) and their accuracy. From the evaluation, we\ndemonstrate that the proposed EODF provides the results to AVs in real-time and\nachieves satisfactory accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 04:35:20 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Kim", "Seung Wook", ""], ["Ko", "Keunsoo", ""], ["Ko", "Haneul", ""], ["Leung", "Victor C. M.", ""]]}, {"id": "2008.07090", "submitter": "Carlo Russo", "authors": "Carlo Russo, Sidong Liu, Antonio Di Ieva", "title": "Spherical coordinates transformation pre-processing in Deep Convolution\n  Neural Networks for brain tumor segmentation in MRI", "comments": "26 pages, 5 figures. Submitted to Computer Methods and Programs in\n  Biomedicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance Imaging (MRI) is used in everyday clinical practice to\nassess brain tumors. Several automatic or semi-automatic segmentation\nalgorithms have been introduced to segment brain tumors and achieve an\nexpert-like accuracy. Deep Convolutional Neural Networks (DCNN) have recently\nshown very promising results, however, DCNN models are still far from achieving\nclinically meaningful results mainly because of the lack of generalization of\nthe models. DCNN models need large annotated datasets to achieve good\nperformance. Models are often optimized on the domain dataset on which they\nhave been trained, and then fail the task when the same model is applied to\ndifferent datasets from different institutions. One of the reasons is due to\nthe lack of data standardization to adjust for different models and MR\nmachines. In this work, a 3D Spherical coordinates transform during the\npre-processing phase has been hypothesized to improve DCNN models' accuracy and\nto allow more generalizable results even when the model is trained on small and\nheterogeneous datasets and translated into different domains. Indeed, the\nspherical coordinate system avoids several standardization issues since it\nworks independently of resolution and imaging settings. Both Cartesian and\nspherical volumes were evaluated in two DCNN models with the same network\nstructure using the BraTS 2019 dataset. The model trained on spherical\ntransform pre-processed inputs resulted in superior performance over the\nCartesian-input trained model on predicting gliomas' segmentation on tumor core\nand enhancing tumor classes (increase of 0.011 and 0.014 respectively on the\nvalidation dataset), achieving a further improvement in accuracy by merging the\ntwo models together. Furthermore, the spherical transform is not\nresolution-dependent and achieve same results on different input resolution.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 05:11:05 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Russo", "Carlo", ""], ["Liu", "Sidong", ""], ["Di Ieva", "Antonio", ""]]}, {"id": "2008.07109", "submitter": "George Retsinas", "authors": "George Retsinas, Giorgos Sfikas, Petros Maragos", "title": "WSRNet: Joint Spotting and Recognition of Handwritten Words", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a unified model that can handle both Keyword\nSpotting and Word Recognition with the same network architecture. The proposed\nnetwork is comprised of a non-recurrent CTC branch and a Seq2Seq branch that is\nfurther augmented with an Autoencoding module. The related joint loss leads to\na boost in recognition performance, while the Seq2Seq branch is used to create\nefficient word representations. We show how to further process these\nrepresentations with binarization and a retraining scheme to provide compact\nand highly efficient descriptors, suitable for keyword spotting. Numerical\nresults validate the usefulness of the proposed architecture, as our method\noutperforms the previous state-of-the-art in keyword spotting, and provides\nresults in the ballpark of the leading methods for word recognition.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 06:22:05 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Retsinas", "George", ""], ["Sfikas", "Giorgos", ""], ["Maragos", "Petros", ""]]}, {"id": "2008.07119", "submitter": "Namwoo Kang", "authors": "Seowoo Jang, Soyoung Yoo, Namwoo Kang", "title": "Generative Design by Reinforcement Learning: Enhancing the Diversity of\n  Topology Optimization Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative design refers to computational design methods that can\nautomatically conduct design exploration under constraints defined by\ndesigners. Among many approaches, topology optimization-based generative\ndesigns aim to explore diverse topology designs, which cannot be represented by\nconventional parametric design approaches. Recently, data-driven topology\noptimization research has started to exploit artificial intelligence, such as\ndeep learning or machine learning, to improve the capability of design\nexploration. This study proposes a reinforcement learning (RL) based generative\ndesign process, with reward functions maximizing the diversity of topology\ndesigns. We formulate generative design as a sequential problem of finding\noptimal design parameter combinations in accordance with a given reference\ndesign. Proximal Policy Optimization is used as the learning framework, which\nis demonstrated in the case study of an automotive wheel design problem. To\nreduce the heavy computational burden of the wheel topology optimization\nprocess required by our RL formulation, we approximate the optimization process\nwith neural networks. With efficient data preprocessing/augmentation and neural\narchitecture, the neural networks achieve a generalized performance and\nsymmetricity-reserving characteristics. We show that RL-based generative design\nproduces a large number of diverse designs within a short inference time by\nexploiting GPU in a fully automated manner. It is different from the previous\napproach using CPU which takes much more processing time and involving human\nintervention.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 06:50:47 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 07:04:49 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Jang", "Seowoo", ""], ["Yoo", "Soyoung", ""], ["Kang", "Namwoo", ""]]}, {"id": "2008.07130", "submitter": "Matteo Poggi", "authors": "Filippo Aleotti, Fabio Tosi, Li Zhang, Matteo Poggi, Stefano Mattoccia", "title": "Reversing the cycle: self-supervised deep stereo through enhanced\n  monocular distillation", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many fields, self-supervised learning solutions are rapidly evolving and\nfilling the gap with supervised approaches. This fact occurs for depth\nestimation based on either monocular or stereo, with the latter often providing\na valid source of self-supervision for the former. In contrast, to soften\ntypical stereo artefacts, we propose a novel self-supervised paradigm reversing\nthe link between the two. Purposely, in order to train deep stereo networks, we\ndistill knowledge through a monocular completion network. This architecture\nexploits single-image clues and few sparse points, sourced by traditional\nstereo algorithms, to estimate dense yet accurate disparity maps by means of a\nconsensus mechanism over multiple estimations. We thoroughly evaluate with\npopular stereo datasets the impact of different supervisory signals showing how\nstereo networks trained with our paradigm outperform existing self-supervised\nframeworks. Finally, our proposal achieves notable generalization capabilities\ndealing with domain shift issues. Code available at\nhttps://github.com/FilippoAleotti/Reversing\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 07:40:22 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Aleotti", "Filippo", ""], ["Tosi", "Fabio", ""], ["Zhang", "Li", ""], ["Poggi", "Matteo", ""], ["Mattoccia", "Stefano", ""]]}, {"id": "2008.07132", "submitter": "Tianyang Shi", "authors": "Tianyang Shi (1), Zhengxia Zou (2), Yi Yuan (1), Changjie Fan (1) ((1)\n  NetEase Fuxi AI Lab, (2) University of Michigan)", "title": "Fast and Robust Face-to-Parameter Translation for Game Character\n  Auto-Creation", "comments": "Accepted by AAAI 2020 with supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of Role-Playing Games (RPGs), players are now\nallowed to edit the facial appearance of their in-game characters with their\npreferences rather than using default templates. This paper proposes a game\ncharacter auto-creation framework that generates in-game characters according\nto a player's input face photo. Different from the previous methods that are\ndesigned based on neural style transfer or monocular 3D face reconstruction, we\nre-formulate the character auto-creation process in a different point of view:\nby predicting a large set of physically meaningful facial parameters under a\nself-supervised learning paradigm. Instead of updating facial parameters\niteratively at the input end of the renderer as suggested by previous methods,\nwhich are time-consuming, we introduce a facial parameter translator so that\nthe creation can be done efficiently through a single forward propagation from\nthe face embeddings to parameters, with a considerable 1000x computational\nspeedup. Despite its high efficiency, the interactivity is preserved in our\nmethod where users are allowed to optionally fine-tune the facial parameters on\nour creation according to their needs. Our approach also shows better\nrobustness than previous methods, especially for those photos with head-pose\nvariance. Comparison results and ablation analysis on seven public face\nverification datasets suggest the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 07:45:31 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Shi", "Tianyang", ""], ["Zou", "Zhengxia", ""], ["Yuan", "Yi", ""], ["Fan", "Changjie", ""]]}, {"id": "2008.07139", "submitter": "Junjie Huang", "authors": "Junjie Huang, Zheng Zhu, Guan Huang, Dalong Du", "title": "AID: Pushing the Performance Boundary of Human Pose Estimation with\n  Information Dropping Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both appearance cue and constraint cue are vital for human pose estimation.\nHowever, there is a tendency in most existing works to overfitting the former\nand overlook the latter. In this paper, we propose Augmentation by Information\nDropping (AID) to verify and tackle this dilemma. Alone with AID as a\nprerequisite for effectively exploiting its potential, we propose customized\ntraining schedules, which are designed by analyzing the pattern of loss and\nperformance in training process from the perspective of information supplying.\nIn experiments, as a model-agnostic approach, AID promotes various\nstate-of-the-art methods in both bottom-up and top-down paradigms with\ndifferent input sizes, frameworks, backbones, training and testing sets. On\npopular COCO human pose estimation test set, AID consistently boosts the\nperformance of different configurations by around 0.6 AP in top-down paradigm\nand up to 1.5 AP in bottom-up paradigm. On more challenging CrowdPose dataset,\nthe improvement is more than 1.5 AP. As AID successfully pushes the performance\nboundary of human pose estimation problem by considerable margin and sets a new\nstate-of-the-art, we hope AID to be a regular configuration for training human\npose estimators. The source code will be publicly available for further\nresearch.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 08:05:16 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 08:07:12 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Huang", "Junjie", ""], ["Zhu", "Zheng", ""], ["Huang", "Guan", ""], ["Du", "Dalong", ""]]}, {"id": "2008.07149", "submitter": "Rui Huang", "authors": "Rui Huang, Yuanjie Zheng, Zhiqiang Hu, Shaoting Zhang, Hongsheng Li", "title": "Multi-organ Segmentation via Co-training Weight-averaged Models from\n  Few-organ Datasets", "comments": "Accepted by MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-organ segmentation has extensive applications in many clinical\napplications. To segment multiple organs of interest, it is generally quite\ndifficult to collect full annotations of all the organs on the same images, as\nsome medical centers might only annotate a portion of the organs due to their\nown clinical practice. In most scenarios, one might obtain annotations of a\nsingle or a few organs from one training set, and obtain annotations of the the\nother organs from another set of training images. Existing approaches mostly\ntrain and deploy a single model for each subset of organs, which are memory\nintensive and also time inefficient. In this paper, we propose to co-train\nweight-averaged models for learning a unified multi-organ segmentation network\nfrom few-organ datasets. We collaboratively train two networks and let the\ncoupled networks teach each other on un-annotated organs. To alleviate the\nnoisy teaching supervisions between the networks, the weighted-averaged models\nare adopted to produce more reliable soft labels. In addition, a novel region\nmask is utilized to selectively apply the consistent constraint on the\nun-annotated organ regions that require collaborative teaching, which further\nboosts the performance. Extensive experiments on three public available\nsingle-organ datasets LiTS, KiTS, Pancreas and manually-constructed\nsingle-organ datasets from MOBA show that our method can better utilize the\nfew-organ datasets and achieves superior performance with less inference\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 08:39:16 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Huang", "Rui", ""], ["Zheng", "Yuanjie", ""], ["Hu", "Zhiqiang", ""], ["Zhang", "Shaoting", ""], ["Li", "Hongsheng", ""]]}, {"id": "2008.07154", "submitter": "Tianyang Shi", "authors": "Tianyang Shi (1), Zhengxia Zou (2), Xinhui Song (1), Zheng Song (1),\n  Changjian Gu (1), Changjie Fan (1), Yi Yuan (1) ((1) NetEase Fuxi AI Lab, (2)\n  University of Michigan)", "title": "Neutral Face Game Character Auto-Creation via PokerFace-GAN", "comments": "Accepted by ACMMM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game character customization is one of the core features of many recent\nRole-Playing Games (RPGs), where players can edit the appearance of their\nin-game characters with their preferences. This paper studies the problem of\nautomatically creating in-game characters with a single photo. In recent\nliterature on this topic, neural networks are introduced to make game engine\ndifferentiable and the self-supervised learning is used to predict facial\ncustomization parameters. However, in previous methods, the expression\nparameters and facial identity parameters are highly coupled with each other,\nmaking it difficult to model the intrinsic facial features of the character.\nBesides, the neural network based renderer used in previous methods is also\ndifficult to be extended to multi-view rendering cases. In this paper,\nconsidering the above problems, we propose a novel method named \"PokerFace-GAN\"\nfor neutral face game character auto-creation. We first build a differentiable\ncharacter renderer which is more flexible than the previous methods in\nmulti-view rendering cases. We then take advantage of the adversarial training\nto effectively disentangle the expression parameters from the identity\nparameters and thus generate player-preferred neutral face (expression-less)\ncharacters. Since all components of our method are differentiable, our method\ncan be easily trained under a multi-task self-supervised learning paradigm.\nExperiment results show that our method can generate vivid neutral face game\ncharacters that are highly similar to the input photos. The effectiveness of\nour method is verified by comparison results and ablation studies.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 08:43:48 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Shi", "Tianyang", ""], ["Zou", "Zhengxia", ""], ["Song", "Xinhui", ""], ["Song", "Zheng", ""], ["Gu", "Changjian", ""], ["Fan", "Changjie", ""], ["Yuan", "Yi", ""]]}, {"id": "2008.07173", "submitter": "Chu-Tak Li", "authors": "Chu-Tak Li, Wan-Chi Siu, Zhi-Song Liu, Li-Wen Wang, and Daniel\n  Pak-Kong Lun", "title": "DeepGIN: Deep Generative Inpainting Network for Extreme Image Inpainting", "comments": "17 pages (14 pages of main content + 3 pages references), 6 figures,\n  2 tables, submitted to ECCV'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The degree of difficulty in image inpainting depends on the types and sizes\nof the missing parts. Existing image inpainting approaches usually encounter\ndifficulties in completing the missing parts in the wild with pleasing visual\nand contextual results as they are trained for either dealing with one specific\ntype of missing patterns (mask) or unilaterally assuming the shapes and/or\nsizes of the masked areas. We propose a deep generative inpainting network,\nnamed DeepGIN, to handle various types of masked images. We design a Spatial\nPyramid Dilation (SPD) ResNet block to enable the use of distant features for\nreconstruction. We also employ Multi-Scale Self-Attention (MSSA) mechanism and\nBack Projection (BP) technique to enhance our inpainting results. Our DeepGIN\noutperforms the state-of-the-art approaches generally, including two publicly\navailable datasets (FFHQ and Oxford Buildings), both quantitatively and\nqualitatively. We also demonstrate that our model is capable of completing\nmasked images in the wild.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 09:30:28 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Li", "Chu-Tak", ""], ["Siu", "Wan-Chi", ""], ["Liu", "Zhi-Song", ""], ["Wang", "Li-Wen", ""], ["Lun", "Daniel Pak-Kong", ""]]}, {"id": "2008.07181", "submitter": "Na Dong", "authors": "Na Dong, Meng-die Zhai, Jian-fang Chang and Chun-ho Wu", "title": "White blood cell classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel automatic classification framework for the\nrecognition of five types of white blood cells. Segmenting complete white blood\ncells from blood smears images and extracting advantageous features from them\nremain challenging tasks in the classification of white blood cells. Therefore,\nwe present an adaptive threshold segmentation method to deal with blood smears\nimages with non-uniform color and uneven illumination, which is designed based\non color space information and threshold segmentation. Subsequently, after\nsuccessfully separating the white blood cell from the blood smear image, a\nlarge number of nonlinear features including geometrical, color and texture\nfeatures are extracted. Nevertheless, redundant features can affect the\nclassification speed and efficiency, and in view of that, a feature selection\nalgorithm based on classification and regression trees (CART) is designed.\nThrough in-depth analysis of the nonlinear relationship between features, the\nirrelevant and redundant features are successfully removed from the initial\nnonlinear features. Afterwards, the selected prominent features are fed into\nparticle swarm optimization support vector machine (PSO-SVM) classifier to\nrecognize the types of the white blood cells. Finally, to evaluate the\nperformance of the proposed white blood cell classification methodology, we\nbuild a white blood cell data set containing 500 blood smear images for\nexperiments. By comparing with the ground truth obtained manually, the proposed\nsegmentation method achieves an average of 95.98% and 97.57% dice similarity\nfor segmented nucleus and cell regions respectively. Furthermore, the proposed\nmethodology achieves 99.76% classification accuracy, which well demonstrates\nits effectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 09:48:12 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 03:06:29 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Dong", "Na", ""], ["Zhai", "Meng-die", ""], ["Chang", "Jian-fang", ""], ["Wu", "Chun-ho", ""]]}, {"id": "2008.07203", "submitter": "Diego Rodriguez", "authors": "Diego Rodriguez, Florian Huber, Sven Behnke", "title": "Category-Level 3D Non-Rigid Registration from Single-View RGB Images", "comments": "Accepted final version. In 2020 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach to solve the 3D non-rigid\nregistration problem from RGB images using Convolutional Neural Networks\n(CNNs). Our objective is to find a deformation field (typically used for\ntransferring knowledge between instances, e.g., grasping skills) that warps a\ngiven 3D canonical model into a novel instance observed by a single-view RGB\nimage. This is done by training a CNN that infers a deformation field for the\nvisible parts of the canonical model and by employing a learned shape (latent)\nspace for inferring the deformations of the occluded parts. As result of the\nregistration, the observed model is reconstructed. Because our method does not\nneed depth information, it can register objects that are typically hard to\nperceive with RGB-D sensors, e.g. with transparent or shiny surfaces. Even\nwithout depth data, our approach outperforms the Coherent Point Drift (CPD)\nregistration method for the evaluated object categories.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 10:35:19 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Rodriguez", "Diego", ""], ["Huber", "Florian", ""], ["Behnke", "Sven", ""]]}, {"id": "2008.07234", "submitter": "Ines Rieger", "authors": "Jaspar Pahl, Ines Rieger, Dominik Seuss", "title": "Multi-label Learning with Missing Values using Combined Facial Action\n  Unit Datasets", "comments": "Presented at the first Workshop on the Art of Learning with Missing\n  Values (Artemiss) hosted by the 37th International Conference on Machine\n  Learning (ICML) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial action units allow an objective, standardized description of facial\nmicro movements which can be used to describe emotions in human faces.\nAnnotating data for action units is an expensive and time-consuming task, which\nleads to a scarce data situation. By combining multiple datasets from different\nstudies, the amount of training data for a machine learning algorithm can be\nincreased in order to create robust models for automated, multi-label action\nunit detection. However, every study annotates different action units, leading\nto a tremendous amount of missing labels in a combined database. In this work,\nwe examine this challenge and present our approach to create a combined\ndatabase and an algorithm capable of learning under the presence of missing\nlabels without inferring their values. Our approach shows competitive\nperformance compared to recent competitions in action unit detection.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 11:58:06 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Pahl", "Jaspar", ""], ["Rieger", "Ines", ""], ["Seuss", "Dominik", ""]]}, {"id": "2008.07246", "submitter": "Boitumelo Ruf", "authors": "Max Hermann, Boitumelo Ruf, Martin Weinmann, Stefan Hinz", "title": "Self-Supervised Learning for Monocular Depth Estimation from Aerial\n  Imagery", "comments": null, "journal-ref": "ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., V-2-2020,\n  357-364, 2020", "doi": "10.5194/isprs-annals-V-2-2020-357-2020", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Supervised learning based methods for monocular depth estimation usually\nrequire large amounts of extensively annotated training data. In the case of\naerial imagery, this ground truth is particularly difficult to acquire.\nTherefore, in this paper, we present a method for self-supervised learning for\nmonocular depth estimation from aerial imagery that does not require annotated\ntraining data. For this, we only use an image sequence from a single moving\ncamera and learn to simultaneously estimate depth and pose information. By\nsharing the weights between pose and depth estimation, we achieve a relatively\nsmall model, which favors real-time application. We evaluate our approach on\nthree diverse datasets and compare the results to conventional methods that\nestimate depth maps based on multi-view geometry. We achieve an accuracy\n{\\delta}1.25 of up to 93.5 %. In addition, we have paid particular attention to\nthe generalization of a trained model to unknown data and the self-improving\ncapabilities of our approach. We conclude that, even though the results of\nmonocular depth estimation are inferior to those achieved by conventional\nmethods, they are well suited to provide a good initialization for methods that\nrely on image matching or to provide estimates in regions where image matching\nfails, e.g. occluded or texture-less regions.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 12:20:46 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Hermann", "Max", ""], ["Ruf", "Boitumelo", ""], ["Weinmann", "Martin", ""], ["Hinz", "Stefan", ""]]}, {"id": "2008.07254", "submitter": "Soufien Hamrouni", "authors": "Soufien Hamrouni, Hakim Ghazzai, Hamid Menouar and Yahya Massoud", "title": "An Improved Dilated Convolutional Network for Herd Counting in Crowded\n  Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Crowd management technologies that leverage computer vision are widespread in\ncontemporary times. There exists many security-related applications of these\nmethods, including, but not limited to: following the flow of an array of\npeople and monitoring large gatherings. In this paper, we propose an accurate\nmonitoring system composed of two concatenated convolutional deep learning\narchitectures. The first part called Front-end, is responsible for converting\nbi-dimensional signals and delivering high-level features. The second part,\ncalled the Back-end, is a dilated Convolutional Neural Network (CNN) used to\nreplace pooling layers. It is responsible for enlarging the receptive field of\nthe whole network and converting the descriptors provided by the first network\nto a saliency map that will be utilized to estimate the number of people in\nhighly congested images. We also propose to utilize a genetic algorithm in\norder to find an optimized dilation rate configuration in the back-end. The\nproposed model is shown to converge 30\\% faster than state-of-the-art\napproaches. It is also shown that it achieves 20\\% lower Mean Absolute Error\n(MAE) when applied to the Shanghai data~set.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 12:31:10 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Hamrouni", "Soufien", ""], ["Ghazzai", "Hakim", ""], ["Menouar", "Hamid", ""], ["Massoud", "Yahya", ""]]}, {"id": "2008.07263", "submitter": "Jing Zhang", "authors": "Jing Zhang, Deng Liang, Aiping Liu, Min Gao, Xiang Chen, Xu Zhang, Xun\n  Chen", "title": "MLBF-Net: A Multi-Lead-Branch Fusion Network for Multi-Class Arrhythmia\n  Classification Using 12-Lead ECG", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic arrhythmia detection using 12-lead electrocardiogram (ECG) signal\nplays a critical role in early prevention and diagnosis of cardiovascular\ndiseases. In the previous studies on automatic arrhythmia detection, most\nmethods concatenated 12 leads of ECG into a matrix, and then input the matrix\nto a variety of feature extractors or deep neural networks for extracting\nuseful information. Under such frameworks, these methods had the ability to\nextract comprehensive features (known as integrity) of 12-lead ECG since the\ninformation of each lead interacts with each other during training. However,\nthe diverse lead-specific features (known as diversity) among 12 leads were\nneglected, causing inadequate information learning for 12-lead ECG. To maximize\nthe information learning of multi-lead ECG, the information fusion of\ncomprehensive features with integrity and lead-specific features with diversity\nshould be taken into account. In this paper, we propose a novel\nMulti-Lead-Branch Fusion Network (MLBF-Net) architecture for arrhythmia\nclassification by integrating multi-loss optimization to jointly learning\ndiversity and integrity of multi-lead ECG. MLBF-Net is composed of three\ncomponents: 1) multiple lead-specific branches for learning the diversity of\nmulti-lead ECG; 2) cross-lead features fusion by concatenating the output\nfeature maps of all branches for learning the integrity of multi-lead ECG; 3)\nmulti-loss co-optimization for all the individual branches and the concatenated\nnetwork. We demonstrate our MLBF-Net on China Physiological Signal Challenge\n2018 which is an open 12-lead ECG dataset. The experimental results show that\nMLBF-Net obtains an average $F_1$ score of 0.855, reaching the highest\narrhythmia classification performance. The proposed method provides a promising\nsolution for multi-lead ECG analysis from an information fusion perspective.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 12:51:39 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Zhang", "Jing", ""], ["Liang", "Deng", ""], ["Liu", "Aiping", ""], ["Gao", "Min", ""], ["Chen", "Xiang", ""], ["Zhang", "Xu", ""], ["Chen", "Xun", ""]]}, {"id": "2008.07275", "submitter": "Damian Borth", "authors": "L\\'ea Steinacker, Miriam Meckel, Genia Kostka, Damian Borth", "title": "Facial Recognition: A cross-national Survey on Public Acceptance,\n  Privacy, and Discrimination", "comments": "ICML 2020 - Law and Machine Learning Workshop, Vienna, Austria", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With rapid advances in machine learning (ML), more of this technology is\nbeing deployed into the real world interacting with us and our environment. One\nof the most widely applied application of ML is facial recognition as it is\nrunning on millions of devices. While being useful for some people, others\nperceive it as a threat when used by public authorities. This discrepancy and\nthe lack of policy increases the uncertainty in the ML community about the\nfuture direction of facial recognition research and development. In this paper\nwe present results from a cross-national survey about public acceptance,\nprivacy, and discrimination of the use of facial recognition technology (FRT)\nin the public. This study provides insights about the opinion towards FRT from\nChina, Germany, the United Kingdom (UK), and the United States (US), which can\nserve as input for policy makers and legal regulators.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 14:17:21 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Steinacker", "L\u00e9a", ""], ["Meckel", "Miriam", ""], ["Kostka", "Genia", ""], ["Borth", "Damian", ""]]}, {"id": "2008.07294", "submitter": "Kean Chen", "authors": "Kean Chen, Weiyao Lin, Jianguo Li, John See, Ji Wang, Junni Zou", "title": "AP-Loss for Accurate One-Stage Object Detection", "comments": "Accepted to IEEE TPAMI. arXiv admin note: substantial text overlap\n  with arXiv:1904.06373", "journal-ref": null, "doi": "10.1109/TPAMI.2020.2991457", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-stage object detectors are trained by optimizing classification-loss and\nlocalization-loss simultaneously, with the former suffering much from extreme\nforeground-background class imbalance issue due to the large number of anchors.\nThis paper alleviates this issue by proposing a novel framework to replace the\nclassification task in one-stage detectors with a ranking task, and adopting\nthe Average-Precision loss (AP-loss) for the ranking problem. Due to its\nnon-differentiability and non-convexity, the AP-loss cannot be optimized\ndirectly. For this purpose, we develop a novel optimization algorithm, which\nseamlessly combines the error-driven update scheme in perceptron learning and\nbackpropagation algorithm in deep networks. We provide in-depth analyses on the\ngood convergence property and computational complexity of the proposed\nalgorithm, both theoretically and empirically. Experimental results demonstrate\nnotable improvement in addressing the imbalance issue in object detection over\nexisting AP-based optimization algorithms. An improved state-of-the-art\nperformance is achieved in one-stage detectors based on AP-loss over detectors\nusing classification-losses on various standard benchmarks. The proposed\nframework is also highly versatile in accommodating different network\narchitectures. Code is available at https://github.com/cccorn/AP-loss .\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 13:22:01 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Chen", "Kean", ""], ["Lin", "Weiyao", ""], ["Li", "Jianguo", ""], ["See", "John", ""], ["Wang", "Ji", ""], ["Zou", "Junni", ""]]}, {"id": "2008.07357", "submitter": "Mikhail Belyaev", "authors": "Boris Shirokikh and Ivan Zakazov and Alexey Chernyavskiy and Irina\n  Fedulova and Mikhail Belyaev", "title": "First U-Net Layers Contain More Domain Specific Information Than The\n  Last Ones", "comments": "Accepted to DART workshop at MICCAI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MRI scans appearance significantly depends on scanning protocols and,\nconsequently, the data-collection institution. These variations between\nclinical sites result in dramatic drops of CNN segmentation quality on unseen\ndomains. Many of the recently proposed MRI domain adaptation methods operate\nwith the last CNN layers to suppress domain shift. At the same time, the core\nmanifestation of MRI variability is a considerable diversity of image\nintensities. We hypothesize that these differences can be eliminated by\nmodifying the first layers rather than the last ones. To validate this simple\nidea, we conducted a set of experiments with brain MRI scans from six domains.\nOur results demonstrate that 1) domain-shift may deteriorate the quality even\nfor a simple brain extraction segmentation task (surface Dice Score drops from\n0.85-0.89 even to 0.09); 2) fine-tuning of the first layers significantly\noutperforms fine-tuning of the last layers in almost all supervised domain\nadaptation setups. Moreover, fine-tuning of the first layers is a better\nstrategy than fine-tuning of the whole network, if the amount of annotated data\nfrom the new domain is strictly limited.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 14:31:10 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Shirokikh", "Boris", ""], ["Zakazov", "Ivan", ""], ["Chernyavskiy", "Alexey", ""], ["Fedulova", "Irina", ""], ["Belyaev", "Mikhail", ""]]}, {"id": "2008.07358", "submitter": "Yida Wang", "authors": "Yida Wang, David Joseph Tan, Nassir Navab, Federico Tombari", "title": "SoftPoolNet: Shape Descriptor for Point Cloud Completion and\n  Classification", "comments": "accepted in ECCV 2020 as oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds are often the default choice for many applications as they\nexhibit more flexibility and efficiency than volumetric data. Nevertheless,\ntheir unorganized nature -- points are stored in an unordered way -- makes them\nless suited to be processed by deep learning pipelines. In this paper, we\npropose a method for 3D object completion and classification based on point\nclouds. We introduce a new way of organizing the extracted features based on\ntheir activations, which we name soft pooling. For the decoder stage, we\npropose regional convolutions, a novel operator aimed at maximizing the global\nactivation entropy. Furthermore, inspired by the local refining procedure in\nPoint Completion Network (PCN), we also propose a patch-deforming operation to\nsimulate deconvolutional operations for point clouds. This paper proves that\nour regional activation can be incorporated in many point cloud architectures\nlike AtlasNet and PCN, leading to better performance for geometric completion.\nWe evaluate our approach on different 3D tasks such as object completion and\nclassification, achieving state-of-the-art accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 14:32:35 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Wang", "Yida", ""], ["Tan", "David Joseph", ""], ["Navab", "Nassir", ""], ["Tombari", "Federico", ""]]}, {"id": "2008.07360", "submitter": "Veronica Rotemberg", "authors": "Veronica Rotemberg, Nicholas Kurtansky, Brigid Betz-Stablein, Liam\n  Caffery, Emmanouil Chousakos, Noel Codella, Marc Combalia, Stephen Dusza,\n  Pascale Guitera, David Gutman, Allan Halpern, Harald Kittler, Kivanc Kose,\n  Steve Langer, Konstantinos Lioprys, Josep Malvehy, Shenara Musthaq, Jabpani\n  Nanda, Ofer Reiter, George Shih, Alexander Stratigos, Philipp Tschandl,\n  Jochen Weber, and H. Peter Soyer", "title": "A Patient-Centric Dataset of Images and Metadata for Identifying\n  Melanomas Using Clinical Context", "comments": "Figures: 3, Tables: 2, Pages: 12", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.CY physics.med-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prior skin image datasets have not addressed patient-level information\nobtained from multiple skin lesions from the same patient. Though artificial\nintelligence classification algorithms have achieved expert-level performance\nin controlled studies examining single images, in practice dermatologists base\ntheir judgment holistically from multiple lesions on the same patient. The 2020\nSIIM-ISIC Melanoma Classification challenge dataset described herein was\nconstructed to address this discrepancy between prior challenges and clinical\npractice, providing for each image in the dataset an identifier allowing\nlesions from the same patient to be mapped to one another. This patient-level\ncontextual information is frequently used by clinicians to diagnose melanoma\nand is especially useful in ruling out false positives in patients with many\natypical nevi. The dataset represents 2,056 patients from three continents with\nan average of 16 lesions per patient, consisting of 33,126 dermoscopic images\nand 584 histopathologically confirmed melanomas compared with benign melanoma\nmimickers.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 20:22:23 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Rotemberg", "Veronica", ""], ["Kurtansky", "Nicholas", ""], ["Betz-Stablein", "Brigid", ""], ["Caffery", "Liam", ""], ["Chousakos", "Emmanouil", ""], ["Codella", "Noel", ""], ["Combalia", "Marc", ""], ["Dusza", "Stephen", ""], ["Guitera", "Pascale", ""], ["Gutman", "David", ""], ["Halpern", "Allan", ""], ["Kittler", "Harald", ""], ["Kose", "Kivanc", ""], ["Langer", "Steve", ""], ["Lioprys", "Konstantinos", ""], ["Malvehy", "Josep", ""], ["Musthaq", "Shenara", ""], ["Nanda", "Jabpani", ""], ["Reiter", "Ofer", ""], ["Shih", "George", ""], ["Stratigos", "Alexander", ""], ["Tschandl", "Philipp", ""], ["Weber", "Jochen", ""], ["Soyer", "H. Peter", ""]]}, {"id": "2008.07393", "submitter": "Vinay Prabhu", "authors": "Bowen Jing, Vinay Prabhu, Angela Gu, John Whaley", "title": "Rotation-Invariant Gait Identification with Quaternion Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A desireable property of accelerometric gait-based identification systems is\nrobustness to new device orientations presented by users during testing but\nunseen during the training phase. However, traditional Convolutional neural\nnetworks (CNNs) used in these systems compensate poorly for such\ntransformations. In this paper, we target this problem by introducing\nQuaternion CNN, a network architecture which is intrinsically layer-wise\nequivariant and globally invariant under 3D rotations of an array of input\nvectors. We show empirically that this network indeed significantly outperforms\na traditional CNN in a multi-user rotation-invariant gait classification\nsetting .Lastly, we demonstrate how the kernels learned by this QCNN can also\nbe visualized as basis-independent but origin- and chirality-dependent\ntrajectory fragments in the euclidean space, thus yielding a novel mode of\nfeature visualization and extraction.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 23:22:12 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Jing", "Bowen", ""], ["Prabhu", "Vinay", ""], ["Gu", "Angela", ""], ["Whaley", "John", ""]]}, {"id": "2008.07404", "submitter": "Chiara Plizzari", "authors": "Chiara Plizzari, Marco Cannici, Matteo Matteucci", "title": "Skeleton-based Action Recognition via Spatial and Temporal Transformer\n  Networks", "comments": "Accepted at Computer Vision and Image Understanding (CVIU) 12 pages,\n  8 figures", "journal-ref": "Computer Vision and Image Understanding, Volumes 208-209 (2021),\n  103219, ISSN 1077-3142", "doi": "10.1016/j.cviu.2021.103219", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Skeleton-based Human Activity Recognition has achieved great interest in\nrecent years as skeleton data has demonstrated being robust to illumination\nchanges, body scales, dynamic camera views, and complex background. In\nparticular, Spatial-Temporal Graph Convolutional Networks (ST-GCN) demonstrated\nto be effective in learning both spatial and temporal dependencies on\nnon-Euclidean data such as skeleton graphs. Nevertheless, an effective encoding\nof the latent information underlying the 3D skeleton is still an open problem,\nespecially when it comes to extracting effective information from joint motion\npatterns and their correlations. In this work, we propose a novel\nSpatial-Temporal Transformer network (ST-TR) which models dependencies between\njoints using the Transformer self-attention operator. In our ST-TR model, a\nSpatial Self-Attention module (SSA) is used to understand intra-frame\ninteractions between different body parts, and a Temporal Self-Attention module\n(TSA) to model inter-frame correlations. The two are combined in a two-stream\nnetwork, whose performance is evaluated on three large-scale datasets,\nNTU-RGB+D 60, NTU-RGB+D 120, and Kinetics Skeleton 400, consistently improving\nbackbone results. Compared with methods that use the same input data, the\nproposed ST-TR achieves state-of-the-art performance on all datasets when using\njoints' coordinates as input, and results on-par with state-of-the-art when\nadding bones information.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 15:25:40 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 13:29:18 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2020 14:49:47 GMT"}, {"version": "v4", "created": "Tue, 22 Jun 2021 15:29:28 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Plizzari", "Chiara", ""], ["Cannici", "Marco", ""], ["Matteucci", "Matteo", ""]]}, {"id": "2008.07418", "submitter": "Marc Bosch", "authors": "Marc Bosch and Christian Conroy and Benjamin Ortiz and Philip Bogden", "title": "Improving Emergency Response during Hurricane Season using Computer\n  Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed a framework for crisis response and management that\nincorporates the latest technologies in computer vision (CV), inland flood\nprediction, damage assessment and data visualization. The framework uses data\ncollected before, during, and after the crisis to enable rapid and informed\ndecision making during all phases of disaster response. Our computer-vision\nmodel analyzes spaceborne and airborne imagery to detect relevant features\nduring and after a natural disaster and creates metadata that is transformed\ninto actionable information through web-accessible mapping tools. In\nparticular, we have designed an ensemble of models to identify features\nincluding water, roads, buildings, and vegetation from the imagery. We have\ninvestigated techniques to bootstrap and reduce dependency on large data\nannotation efforts by adding use of open source labels including OpenStreetMaps\nand adding complementary data sources including Height Above Nearest Drainage\n(HAND) as a side channel to the network's input to encourage it to learn other\nfeatures orthogonal to visual characteristics. Modeling efforts include\nmodification of connected U-Nets for (1) semantic segmentation, (2) flood line\ndetection, and (3) for damage assessment. In particular for the case of damage\nassessment, we added a second encoder to U-Net so that it could learn pre-event\nand post-event image features simultaneously. Through this method, the network\nis able to learn the difference between the pre- and post-disaster images, and\ntherefore more effectively classify the level of damage. We have validated our\napproaches using publicly available data from the National Oceanic and\nAtmospheric Administration (NOAA)'s Remote Sensing Division, which displays the\ncity and street-level details as mosaic tile images as well as data released as\npart of the Xview2 challenge.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 15:42:02 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 19:51:37 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Bosch", "Marc", ""], ["Conroy", "Christian", ""], ["Ortiz", "Benjamin", ""], ["Bogden", "Philip", ""]]}, {"id": "2008.07424", "submitter": "Jean Ogier Du Terrail", "authors": "Mathieu Andreux, Jean Ogier du Terrail, Constance Beguier, Eric W.\n  Tramel", "title": "Siloed Federated Learning for Multi-Centric Histopathology Datasets", "comments": "Accepted to MICCAI 2020 DCL workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While federated learning is a promising approach for training deep learning\nmodels over distributed sensitive datasets, it presents new challenges for\nmachine learning, especially when applied in the medical domain where\nmulti-centric data heterogeneity is common. Building on previous domain\nadaptation works, this paper proposes a novel federated learning approach for\ndeep learning architectures via the introduction of local-statistic batch\nnormalization (BN) layers, resulting in collaboratively-trained, yet\ncenter-specific models. This strategy improves robustness to data heterogeneity\nwhile also reducing the potential for information leaks by not sharing the\ncenter-specific layer activation statistics. We benchmark the proposed method\non the classification of tumorous histopathology image patches extracted from\nthe Camelyon16 and Camelyon17 datasets. We show that our approach compares\nfavorably to previous state-of-the-art methods, especially for transfer\nlearning across datasets.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 15:49:30 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Andreux", "Mathieu", ""], ["Terrail", "Jean Ogier du", ""], ["Beguier", "Constance", ""], ["Tramel", "Eric W.", ""]]}, {"id": "2008.07426", "submitter": "Matias Valdenegro-Toro", "authors": "Maryam Matin and Matias Valdenegro-Toro", "title": "Hey Human, If your Facial Emotions are Uncertain, You Should Use\n  Bayesian Neural Networks!", "comments": "10 pages, 7 figures, Women in Computer Vision @ ECCV 2020 camera\n  ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial emotion recognition is the task to classify human emotions in face\nimages. It is a difficult task due to high aleatoric uncertainty and visual\nambiguity. A large part of the literature aims to show progress by increasing\naccuracy on this task, but this ignores the inherent uncertainty and ambiguity\nin the task. In this paper we show that Bayesian Neural Networks, as\napproximated using MC-Dropout, MC-DropConnect, or an Ensemble, are able to\nmodel the aleatoric uncertainty in facial emotion recognition, and produce\noutput probabilities that are closer to what a human expects. We also show that\ncalibration metrics show strange behaviors for this task, due to the multiple\nclasses that can be considered correct, which motivates future work. We believe\nour work will motivate other researchers to move away from Classical and into\nBayesian Neural Networks.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 15:50:40 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Matin", "Maryam", ""], ["Valdenegro-Toro", "Matias", ""]]}, {"id": "2008.07443", "submitter": "Udit Maniyar", "authors": "Udit Maniyar, Joseph K J, Aniket Anand Deshmukh, Urun Dogan, Vineeth N\n  Balasubramanian", "title": "Zero Shot Domain Generalization", "comments": "Accepted to BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Standard supervised learning setting assumes that training data and test data\ncome from the same distribution (domain). Domain generalization (DG) methods\ntry to learn a model that when trained on data from multiple domains, would\ngeneralize to a new unseen domain. We extend DG to an even more challenging\nsetting, where the label space of the unseen domain could also change. We\nintroduce this problem as Zero-Shot Domain Generalization (to the best of our\nknowledge, the first such effort), where the model generalizes across new\ndomains and also across new classes in those domains. We propose a simple\nstrategy which effectively exploits semantic information of classes, to adapt\nexisting DG methods to meet the demands of Zero-Shot Domain Generalization. We\nevaluate the proposed methods on CIFAR-10, CIFAR-100, F-MNIST and PACS\ndatasets, establishing a strong baseline to foster interest in this new\nresearch direction.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 16:04:03 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Maniyar", "Udit", ""], ["J", "Joseph K", ""], ["Deshmukh", "Aniket Anand", ""], ["Dogan", "Urun", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "2008.07514", "submitter": "Yunzhong Hou", "authors": "Yunzhong Hou, Liang Zheng", "title": "Source Free Domain Adaptation with Image Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effort in releasing large-scale datasets may be compromised by privacy and\nintellectual property considerations. A feasible alternative is to release\npre-trained models instead. While these models are strong on their original\ntask (source domain), their performance might degrade significantly when\ndeployed directly in a new environment (target domain), which might not contain\nlabels for training under realistic settings. Domain adaptation (DA) is a known\nsolution to the domain gap problem, but usually requires labeled source data.\nIn this paper, we study the problem of source free domain adaptation (SFDA),\nwhose distinctive feature is that the source domain only provides a pre-trained\nmodel, but no source data. Being source free adds significant challenges to DA,\nespecially when considering that the target dataset is unlabeled. To solve the\nSFDA problem, we propose an image translation approach that transfers the style\nof target images to that of unseen source images. To this end, we align the\nbatch-wise feature statistics of generated images to that stored in batch\nnormalization layers of the pre-trained model. Compared with directly\nclassifying target images, higher accuracy is obtained with these style\ntransferred images using the pre-trained model. On several image classification\ndatasets, we show that the above-mentioned improvements are consistent and\nstatistically significant.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 17:57:33 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 07:11:57 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Hou", "Yunzhong", ""], ["Zheng", "Liang", ""]]}, {"id": "2008.07519", "submitter": "Sivabalan Manivasagam", "authors": "Tsun-Hsuan Wang, Sivabalan Manivasagam, Ming Liang, Bin Yang, Wenyuan\n  Zeng, James Tu, Raquel Urtasun", "title": "V2VNet: Vehicle-to-Vehicle Communication for Joint Perception and\n  Prediction", "comments": "ECCV 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the use of vehicle-to-vehicle (V2V) communication\nto improve the perception and motion forecasting performance of self-driving\nvehicles. By intelligently aggregating the information received from multiple\nnearby vehicles, we can observe the same scene from different viewpoints. This\nallows us to see through occlusions and detect actors at long range, where the\nobservations are very sparse or non-existent. We also show that our approach of\nsending compressed deep feature map activations achieves high accuracy while\nsatisfying communication bandwidth requirements.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 17:58:26 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Wang", "Tsun-Hsuan", ""], ["Manivasagam", "Sivabalan", ""], ["Liang", "Ming", ""], ["Yang", "Bin", ""], ["Zeng", "Wenyuan", ""], ["Tu", "James", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2008.07579", "submitter": "Xiao Chen", "authors": "Pingjun Chen, Xiao Chen, Eric Z. Chen, Hanchao Yu, Terrence Chen,\n  Shanhui Sun", "title": "Anatomy-Aware Cardiac Motion Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiac motion estimation is critical to the assessment of cardiac function.\nMyocardium feature tracking (FT) can directly estimate cardiac motion from cine\nMRI, which requires no special scanning procedure. However, current deep\nlearning-based FT methods may result in unrealistic myocardium shapes since the\nlearning is solely guided by image intensities without considering anatomy. On\nthe other hand, motion estimation through learning is challenging because\nground-truth motion fields are almost impossible to obtain. In this study, we\npropose a novel Anatomy-Aware Tracker (AATracker) for cardiac motion estimation\nthat preserves anatomy by weak supervision. A convolutional variational\nautoencoder (VAE) is trained to encapsulate realistic myocardium shapes. A\nbaseline dense motion tracker is trained to approximate the motion fields and\nthen refined to estimate anatomy-aware motion fields under the weak supervision\nfrom the VAE. We evaluate the proposed method on long-axis cardiac cine MRI,\nwhich has more complex myocardium appearances and motions than short-axis.\nCompared with other methods, AATracker significantly improves the tracking\nperformance and provides visually more realistic tracking results,\ndemonstrating the effectiveness of the proposed weakly-supervision scheme in\ncardiac motion estimation.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 19:14:32 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Chen", "Pingjun", ""], ["Chen", "Xiao", ""], ["Chen", "Eric Z.", ""], ["Yu", "Hanchao", ""], ["Chen", "Terrence", ""], ["Sun", "Shanhui", ""]]}, {"id": "2008.07588", "submitter": "Abhinav Sagar", "authors": "Abhinav Sagar", "title": "Uncertainty Quantification using Variational Inference for Biomedical\n  Image Segmentation", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning motivated by convolutional neural networks has been highly\nsuccessful in a range of medical imaging problems like image classification,\nimage segmentation, image synthesis etc. However for validation and\ninterpretability, not only do we need the predictions made by the model but\nalso how confident it is while making those predictions. This is important in\nsafety critical applications for the people to accept it. In this work, we used\nan encoder decoder architecture based on variational inference techniques for\nsegmenting brain tumour images. We compare different backbones architectures\nlike U-Net, V-Net and FCN as sampling data from the conditional distribution\nfor the encoder. We evaluate our work on the publicly available BRATS dataset\nusing Dice Similarity Coefficient (DSC) and Intersection Over Union (IOU) as\nthe evaluation metrics. Our model outperforms previous state of the art results\nwhile making use of uncertainty quantification in a principled bayesian manner.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 20:08:04 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Sagar", "Abhinav", ""]]}, {"id": "2008.07592", "submitter": "Divyansh Singh", "authors": "Divyansh Singh", "title": "Polyth-Net: Classification of Polythene Bags for Garbage Segregation\n  Using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polythene has always been a threat to the environment since its invention. It\nis non-biodegradable and very difficult to recycle. Even after many awareness\ncampaigns and practices, Separation of polythene bags from waste has been a\nchallenge for human civilization. The primary method of segregation deployed is\nmanual handpicking, which causes a dangerous health hazards to the workers and\nis also highly inefficient due to human errors. In this paper I have designed\nand researched on image-based classification of polythene bags using a\ndeep-learning model and its efficiency. This paper focuses on the architecture\nand statistical analysis of its performance on the data set as well as problems\nexperienced in the classification. It also suggests a modified loss function to\nspecifically detect polythene irrespective of its individual features. It aims\nto help the current environment protection endeavours and save countless lives\nlost to the hazards caused by current methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 19:00:56 GMT"}, {"version": "v2", "created": "Sat, 26 Sep 2020 03:08:11 GMT"}, {"version": "v3", "created": "Sun, 20 Dec 2020 05:06:35 GMT"}, {"version": "v4", "created": "Sat, 23 Jan 2021 11:23:16 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Singh", "Divyansh", ""]]}, {"id": "2008.07623", "submitter": "Yipeng Zhang", "authors": "Yipeng Zhang, Haofu Liao, Jin Xiao, Nisreen Al Jallad, Oriana\n  Ly-Mapes, Jiebo Luo", "title": "A Smartphone-based System for Real-time Early Childhood Caries Diagnosis", "comments": "MICCAI 2020 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early childhood caries (ECC) is the most common, yet preventable chronic\ndisease in children under the age of 6. Treatments on severe ECC are extremely\nexpensive and unaffordable for socioeconomically disadvantaged families. The\nidentification of ECC in an early stage usually requires expertise in the\nfield, and hence is often ignored by parents. Therefore, early prevention\nstrategies and easy-to-adopt diagnosis techniques are desired. In this study,\nwe propose a multistage deep learning-based system for cavity detection. We\ncreate a dataset containing RGB oral images labeled manually by dental\npractitioners. We then investigate the effectiveness of different deep learning\nmodels on the dataset. Furthermore, we integrate the deep learning system into\nan easy-to-use mobile application that can diagnose ECC from an early stage and\nprovide real-time results to untrained users.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 21:11:19 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Zhang", "Yipeng", ""], ["Liao", "Haofu", ""], ["Xiao", "Jin", ""], ["Jallad", "Nisreen Al", ""], ["Ly-Mapes", "Oriana", ""], ["Luo", "Jiebo", ""]]}, {"id": "2008.07628", "submitter": "Xu Han", "authors": "Xu Han, Zhengyang Shen, Zhenlin Xu, Spyridon Bakas, Hamed Akbari,\n  Michel Bilello, Christos Davatzikos, Marc Niethammer", "title": "A Deep Network for Joint Registration and Reconstruction of Images with\n  Pathologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Registration of images with pathologies is challenging due to tissue\nappearance changes and missing correspondences caused by the pathologies.\nMoreover, mass effects as observed for brain tumors may displace tissue,\ncreating larger deformations over time than what is observed in a healthy\nbrain. Deep learning models have successfully been applied to image\nregistration to offer dramatic speed up and to use surrogate information (e.g.,\nsegmentations) during training. However, existing approaches focus on learning\nregistration models using images from healthy patients. They are therefore not\ndesigned for the registration of images with strong pathologies for example in\nthe context of brain tumors, and traumatic brain injuries. In this work, we\nexplore a deep learning approach to register images with brain tumors to an\natlas. Our model learns an appearance mapping from images with tumors to the\natlas, while simultaneously predicting the transformation to atlas space. Using\nseparate decoders, the network disentangles the tumor mass effect from the\nreconstruction of quasi-normal images. Results on both synthetic and real brain\ntumor scans show that our approach outperforms cost function masking for\nregistration to the atlas and that reconstructed quasi-normal images can be\nused for better longitudinal registrations.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 21:26:02 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Han", "Xu", ""], ["Shen", "Zhengyang", ""], ["Xu", "Zhenlin", ""], ["Bakas", "Spyridon", ""], ["Akbari", "Hamed", ""], ["Bilello", "Michel", ""], ["Davatzikos", "Christos", ""], ["Niethammer", "Marc", ""]]}, {"id": "2008.07641", "submitter": "Pau Riba", "authors": "Pau Riba, Andreas Fischer, Josep Llad\\'os and Alicia Forn\\'es", "title": "Learning Graph Edit Distance by Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The emergence of geometric deep learning as a novel framework to deal with\ngraph-based representations has faded away traditional approaches in favor of\ncompletely new methodologies. In this paper, we propose a new framework able to\ncombine the advances on deep metric learning with traditional approximations of\nthe graph edit distance. Hence, we propose an efficient graph distance based on\nthe novel field of geometric deep learning. Our method employs a message\npassing neural network to capture the graph structure, and thus, leveraging\nthis information for its use on a distance computation. The performance of the\nproposed graph distance is validated on two different scenarios. On the one\nhand, in a graph retrieval of handwritten words~\\ie~keyword spotting, showing\nits superior performance when compared with (approximate) graph edit distance\nbenchmarks. On the other hand, demonstrating competitive results for graph\nsimilarity learning when compared with the current state-of-the-art on a recent\nbenchmark dataset.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 21:49:59 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Riba", "Pau", ""], ["Fischer", "Andreas", ""], ["Llad\u00f3s", "Josep", ""], ["Forn\u00e9s", "Alicia", ""]]}, {"id": "2008.07643", "submitter": "Fajrian Yunus", "authors": "Fajrian Yunus, Chlo\\'e Clavel, Catherine Pelachaud", "title": "Sequence-to-Sequence Predictive Model: From Prosody To Communicative\n  Gestures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communicative gestures and speech acoustic are tightly linked. Our objective\nis to predict the timing of gestures according to the acoustic. That is, we\nwant to predict when a certain gesture occurs. We develop a model based on a\nrecurrent neural network with attention mechanism. The model is trained on a\ncorpus of natural dyadic interaction where the speech acoustic and the gesture\nphases and types have been annotated. The input of the model is a sequence of\nspeech acoustic and the output is a sequence of gesture classes. The classes we\nare using for the model output is based on a combination of gesture phases and\ngesture types. We use a sequence comparison technique to evaluate the model\nperformance. We find that the model can predict better certain gesture classes\nthan others. We also perform ablation studies which reveal that fundamental\nfrequency is a relevant feature for gesture prediction task. In another\nsub-experiment, we find that including eyebrow movements as acting as beat\ngesture improves the performance. Besides, we also find that a model trained on\nthe data of one given speaker also works for the other speaker of the same\nconversation. We also perform a subjective experiment to measure how\nrespondents judge the naturalness, the time consistency, and the semantic\nconsistency of the generated gesture timing of a virtual agent. Our respondents\nrate the output of our model favorably.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 21:55:22 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 21:03:40 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Yunus", "Fajrian", ""], ["Clavel", "Chlo\u00e9", ""], ["Pelachaud", "Catherine", ""]]}, {"id": "2008.07644", "submitter": "Peleg Harel", "authors": "Peleg Harel and Ohad Ben-Shahar", "title": "Lazy caterer jigsaw puzzles: Models, properties, and a mechanical\n  system-based solver", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Jigsaw puzzle solving, the problem of constructing a coherent whole from a\nset of non-overlapping unordered fragments, is fundamental to numerous\napplications, and yet most of the literature has focused thus far on less\nrealistic puzzles whose pieces are identical squares. Here we formalize a new\ntype of jigsaw puzzle where the pieces are general convex polygons generated by\ncutting through a global polygonal shape with an arbitrary number of straight\ncuts, a generation model inspired by the celebrated Lazy caterer's sequence. We\nanalyze the theoretical properties of such puzzles, including the inherent\nchallenges in solving them once pieces are contaminated with geometrical noise.\nTo cope with such difficulties and obtain tractable solutions, we abstract the\nproblem as a multi-body spring-mass dynamical system endowed with hierarchical\nloop constraints and a layered reconstruction process. We define evaluation\nmetrics and present experimental results to indicate that such puzzles are\nsolvable completely automatically.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 22:07:40 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Harel", "Peleg", ""], ["Ben-Shahar", "Ohad", ""]]}, {"id": "2008.07651", "submitter": "Mehmet Kerim Yucel", "authors": "Mehmet Kerim Yucel, Ramazan Gokberk Cinbis, Pinar Duygulu", "title": "A Deep Dive into Adversarial Robustness in Zero-Shot Learning", "comments": "To appear in ECCV 2020, Workshop on Adversarial Robustness in the\n  Real World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) systems have introduced significant advances in various\nfields, due to the introduction of highly complex models. Despite their\nsuccess, it has been shown multiple times that machine learning models are\nprone to imperceptible perturbations that can severely degrade their accuracy.\nSo far, existing studies have primarily focused on models where supervision\nacross all classes were available. In constrast, Zero-shot Learning (ZSL) and\nGeneralized Zero-shot Learning (GZSL) tasks inherently lack supervision across\nall classes. In this paper, we present a study aimed on evaluating the\nadversarial robustness of ZSL and GZSL models. We leverage the well-established\nlabel embedding model and subject it to a set of established adversarial\nattacks and defenses across multiple datasets. In addition to creating possibly\nthe first benchmark on adversarial robustness of ZSL models, we also present\nanalyses on important points that require attention for better interpretation\nof ZSL robustness results. We hope these points, along with the benchmark, will\nhelp researchers establish a better understanding what challenges lie ahead and\nhelp guide their work.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 22:26:06 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Yucel", "Mehmet Kerim", ""], ["Cinbis", "Ramazan Gokberk", ""], ["Duygulu", "Pinar", ""]]}, {"id": "2008.07665", "submitter": "Azade Farshad", "authors": "Yousef Yeganeh, Azade Farshad, Nassir Navab, Shadi Albarqouni", "title": "Inverse Distance Aggregation for Federated Learning with Non-IID Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) has been a promising approach in the field of medical\nimaging in recent years. A critical problem in FL, specifically in medical\nscenarios is to have a more accurate shared model which is robust to noisy and\nout-of distribution clients. In this work, we tackle the problem of statistical\nheterogeneity in data for FL which is highly plausible in medical data where\nfor example the data comes from different sites with different scanner\nsettings. We propose IDA (Inverse Distance Aggregation), a novel adaptive\nweighting approach for clients based on meta-information which handles\nunbalanced and non-iid data. We extensively analyze and evaluate our method\nagainst the well-known FL approach, Federated Averaging as a baseline.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 23:20:01 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Yeganeh", "Yousef", ""], ["Farshad", "Azade", ""], ["Navab", "Nassir", ""], ["Albarqouni", "Shadi", ""]]}, {"id": "2008.07668", "submitter": "Hooman Hedayati", "authors": "Hooman Hedayati, Annika Muehlbradt, Daniel J. Szafir, Sean Andrist", "title": "REFORM: Recognizing F-formations for Social Robots", "comments": "IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing and understanding conversational groups, or F-formations, is a\ncritical task for situated agents designed to interact with humans.\nF-formations contain complex structures and dynamics, yet are used intuitively\nby people in everyday face-to-face conversations. Prior research exploring ways\nof identifying F-formations has largely relied on heuristic algorithms that may\nnot capture the rich dynamic behaviors employed by humans. We introduce REFORM\n(REcognize F-FORmations with Machine learning), a data-driven approach for\ndetecting F-formations given human and agent positions and orientations. REFORM\ndecomposes the scene into all possible pairs and then reconstructs F-formations\nwith a voting-based scheme. We evaluated our approach across three datasets:\nthe SALSA dataset, a newly collected human-only dataset, and a new set of acted\nhuman-robot scenarios, and found that REFORM yielded improved accuracy over a\nstate-of-the-art F-formation detection algorithm. We also introduce symmetry\nand tightness as quantitative measures to characterize F-formations.\nSupplementary video: https://youtu.be/Fp7ETdkKvdA , Dataset available at:\ngithub.com/cu-ironlab/Babble\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 23:32:05 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Hedayati", "Hooman", ""], ["Muehlbradt", "Annika", ""], ["Szafir", "Daniel J.", ""], ["Andrist", "Sean", ""]]}, {"id": "2008.07711", "submitter": "Shanjiaoyang Huang", "authors": "Shanjiaoyang Huang, Weiqi Peng, Zhiwei Jia, Zhuowen Tu", "title": "One-pixel Signature: Characterizing CNN Models for Backdoor Detection", "comments": "Accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the convolution neural networks (CNNs) backdoor detection problem\nby proposing a new representation called one-pixel signature. Our task is to\ndetect/classify if a CNN model has been maliciously inserted with an unknown\nTrojan trigger or not. Here, each CNN model is associated with a signature that\nis created by generating, pixel-by-pixel, an adversarial value that is the\nresult of the largest change to the class prediction. The one-pixel signature\nis agnostic to the design choice of CNN architectures, and how they were\ntrained. It can be computed efficiently for a black-box CNN model without\naccessing the network parameters. Our proposed one-pixel signature demonstrates\na substantial improvement (by around 30% in the absolute detection accuracy)\nover the existing competing methods for backdoored CNN\ndetection/classification. One-pixel signature is a general representation that\ncan be used to characterize CNN models beyond backdoor detection.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 02:54:47 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Huang", "Shanjiaoyang", ""], ["Peng", "Weiqi", ""], ["Jia", "Zhiwei", ""], ["Tu", "Zhuowen", ""]]}, {"id": "2008.07712", "submitter": "Pan Zhang", "authors": "Pan Zhang, Wilfredo Torres Calderon, Bokyung Lee, Alex Tessier, Jacky\n  Bibliowicz, Liviu Calin, Michael Lee", "title": "Contact Area Detector using Cross View Projection Consistency for\n  COVID-19 Projects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to determine what parts of objects and surfaces people touch as\nthey go about their daily lives would be useful in understanding how the\nCOVID-19 virus spreads. To determine whether a person has touched an object or\nsurface using visual data, images, or videos, is a hard problem. Computer\nvision 3D reconstruction approaches project objects and the human body from the\n2D image domain to 3D and perform 3D space intersection directly. However, this\nsolution would not meet the accuracy requirement in applications due to\nprojection error. Another standard approach is to train a neural network to\ninfer touch actions from the collected visual data. This strategy would require\nsignificant amounts of training data to generalize over scale and viewpoint\nvariations. A different approach to this problem is to identify whether a\nperson has touched a defined object. In this work, we show that the solution to\nthis problem can be straightforward. Specifically, we show that the contact\nbetween an object and a static surface can be identified by projecting the\nobject onto the static surface through two different viewpoints and analyzing\ntheir 2D intersection. The object contacts the surface when the projected\npoints are close to each other; we call this cross view projection consistency.\nInstead of doing 3D scene reconstruction or transfer learning from deep\nnetworks, a mapping from the surface in the two camera views to the surface\nspace is the only requirement. For planar space, this mapping is the Homography\ntransformation. This simple method can be easily adapted to real-life\napplications. In this paper, we apply our method to do office occupancy\ndetection for studying the COVID-19 transmission pattern from an office desk in\na meeting room using the contact information.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 02:57:26 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Zhang", "Pan", ""], ["Calderon", "Wilfredo Torres", ""], ["Lee", "Bokyung", ""], ["Tessier", "Alex", ""], ["Bibliowicz", "Jacky", ""], ["Calin", "Liviu", ""], ["Lee", "Michael", ""]]}, {"id": "2008.07714", "submitter": "Maliha Arif", "authors": "Maliha Arif, Abhijit Mahalanobis", "title": "Multiple View Generation and Classification of Mid-wave Infrared Images\n  using Deep Learning", "comments": "5 pages, 5 figures, to be submitted in a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel study of generating unseen arbitrary viewpoints for\ninfrared imagery in the non-linear feature subspace . Current methods use\nsynthetic images and often result in blurry and distorted outputs. Our approach\non the contrary understands the semantic information in natural images and\nencapsulates it such that our predicted unseen views possess good 3D\nrepresentations. We further explore the non-linear feature subspace and\nconclude that our network does not operate in the Euclidean subspace but rather\nin the Riemannian subspace. It does not learn the geometric transformation for\npredicting the position of the pixel in the new image but rather learns the\nmanifold. To this end, we use t-SNE visualisations to conduct a detailed\nanalysis of our network and perform classification of generated images as a\nlow-shot learning task.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 02:58:21 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Arif", "Maliha", ""], ["Mahalanobis", "Abhijit", ""]]}, {"id": "2008.07724", "submitter": "Pulkit Khandelwal", "authors": "Pulkit Khandelwal and Paul Yushkevich", "title": "Domain Generalizer: A Few-shot Meta Learning Framework for Domain\n  Generalization in Medical Imaging", "comments": "Medical Image Computing and Computer Assisted Interventions (MICCAI)\n  2020 to be presented at DART 2020. Supplementary material and link to code\n  included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning models perform best when tested on target (test) data domains\nwhose distribution is similar to the set of source (train) domains. However,\nmodel generalization can be hindered when there is significant difference in\nthe underlying statistics between the target and source domains. In this work,\nwe adapt a domain generalization method based on a model-agnostic meta-learning\nframework to biomedical imaging. The method learns a domain-agnostic feature\nrepresentation to improve generalization of models to the unseen test\ndistribution. The method can be used for any imaging task, as it does not\ndepend on the underlying model architecture. We validate the approach through a\ncomputed tomography (CT) vertebrae segmentation task across healthy and\npathological cases on three datasets. Next, we employ few-shot learning, i.e.\ntraining the generalized model using very few examples from the unseen domain,\nto quickly adapt the model to new unseen data distribution. Our results suggest\nthat the method could help generalize models across different medical centers,\nimage acquisition protocols, anatomies, different regions in a given scan,\nhealthy and diseased populations across varied imaging modalities.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 03:35:56 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Khandelwal", "Pulkit", ""], ["Yushkevich", "Paul", ""]]}, {"id": "2008.07725", "submitter": "Wei-Chih Hung", "authors": "Wei-Chih Hung, Henrik Kretzschmar, Tsung-Yi Lin, Yuning Chai, Ruichi\n  Yu, Ming-Hsuan Yang, Dragomir Anguelov", "title": "SoDA: Multi-Object Tracking with Soft Data Association", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust multi-object tracking (MOT) is a prerequisite fora safe deployment of\nself-driving cars. Tracking objects, however, remains a highly challenging\nproblem, especially in cluttered autonomous driving scenes in which objects\ntend to interact with each other in complex ways and frequently get occluded.\nWe propose a novel approach to MOT that uses attention to compute track\nembeddings that encode the spatiotemporal dependencies between observed\nobjects. This attention measurement encoding allows our model to relax hard\ndata associations, which may lead to unrecoverable errors. Instead, our model\naggregates information from all object detections via soft data associations.\nThe resulting latent space representation allows our model to learn to reason\nabout occlusions in a holistic data-driven way and maintain track estimates for\nobjects even when they are occluded. Our experimental results on the Waymo\nOpenDataset suggest that our approach leverages modern large-scale datasets and\nperforms favorably compared to the state of the art in visual multi-object\ntracking.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 03:40:25 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 17:46:22 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Hung", "Wei-Chih", ""], ["Kretzschmar", "Henrik", ""], ["Lin", "Tsung-Yi", ""], ["Chai", "Yuning", ""], ["Yu", "Ruichi", ""], ["Yang", "Ming-Hsuan", ""], ["Anguelov", "Dragomir", ""]]}, {"id": "2008.07728", "submitter": "Le Yang", "authors": "Tao Zhao, Junwei Han, Le Yang, Dingwen Zhang", "title": "Equivalent Classification Mapping for Weakly Supervised Temporal Action\n  Localization", "comments": "12 pages, 6 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised temporal action localization is a newly emerging yet widely\nstudied topic in recent years. The existing methods can be categorized into two\nlocalization-by-classification pipelines, i.e., the pre-classification pipeline\nand the post-classification pipeline. The pre-classification pipeline first\nperforms classification on each video snippet and then aggregate the\nsnippet-level classification scores to obtain the video-level classification\nscore. In contrast, the post-classification pipeline aggregates the\nsnippet-level features first and then predicts the video-level classification\nscore based on the aggregated feature. Although the classifiers in these two\npipelines are used in different ways, the role they play is exactly the\nsame---to classify the given features to identify the corresponding action\ncategories. To this end, an ideal classifier can make both pipelines work. This\ninspires us to simultaneously learn these two pipelines in a unified framework\nto obtain an effective classifier. Specifically, in the proposed learning\nframework, we implement two parallel network streams to model the two\nlocalization-by-classification pipelines simultaneously and make the two\nnetwork streams share the same classifier. This achieves the novel Equivalent\nClassification Mapping (ECM) mechanism. Moreover, we discover that an ideal\nclassifier may possess two characteristics: 1) The frame-level classification\nscores obtained from the pre-classification stream and the feature aggregation\nweights in the post-classification stream should be consistent; 2) The\nclassification results of these two streams should be identical. Based on these\ntwo characteristics, we further introduce a weight-transition module and an\nequivalent training strategy into the proposed learning framework, which\nassists to thoroughly mine the equivalence mechanism.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 03:54:56 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 02:17:18 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Zhao", "Tao", ""], ["Han", "Junwei", ""], ["Yang", "Le", ""], ["Zhang", "Dingwen", ""]]}, {"id": "2008.07742", "submitter": "Yuqian Zhou", "authors": "Yuqian Zhou, Michael Kwan, Kyle Tolentino, Neil Emerton, Sehoon Lim,\n  Tim Large, Lijiang Fu, Zhihong Pan, Baopu Li, Qirui Yang, Yihao Liu, Jigang\n  Tang, Tao Ku, Shibin Ma, Bingnan Hu, Jiarong Wang, Densen Puthussery,\n  Hrishikesh P S, Melvin Kuriakose, Jiji C V, Varun Sundar, Sumanth Hegde,\n  Divya Kothandaraman, Kaushik Mitra, Akashdeep Jassal, Nisarg A. Shah, Sabari\n  Nathan, Nagat Abdalla Esiad Rahel, Dafan Chen, Shichao Nie, Shuting Yin,\n  Chengconghui Ma, Haoran Wang, Tongtong Zhao, Shanshan Zhao, Joshua Rego,\n  Huaijin Chen, Shuai Li, Zhenhua Hu, Kin Wai Lau, Lai-Man Po, Dahai Yu, Yasar\n  Abbas Ur Rehman, Yiqun Li, Lianping Xing", "title": "UDC 2020 Challenge on Image Restoration of Under-Display Camera: Methods\n  and Results", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is the report of the first Under-Display Camera (UDC) image\nrestoration challenge in conjunction with the RLQ workshop at ECCV 2020. The\nchallenge is based on a newly-collected database of Under-Display Camera. The\nchallenge tracks correspond to two types of display: a 4k Transparent OLED\n(T-OLED) and a phone Pentile OLED (P-OLED). Along with about 150 teams\nregistered the challenge, eight and nine teams submitted the results during the\ntesting phase for each track. The results in the paper are state-of-the-art\nrestoration performance of Under-Display Camera Restoration. Datasets and paper\nare available at https://yzhouas.github.io/projects/UDC/udc.html.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 04:48:39 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Zhou", "Yuqian", ""], ["Kwan", "Michael", ""], ["Tolentino", "Kyle", ""], ["Emerton", "Neil", ""], ["Lim", "Sehoon", ""], ["Large", "Tim", ""], ["Fu", "Lijiang", ""], ["Pan", "Zhihong", ""], ["Li", "Baopu", ""], ["Yang", "Qirui", ""], ["Liu", "Yihao", ""], ["Tang", "Jigang", ""], ["Ku", "Tao", ""], ["Ma", "Shibin", ""], ["Hu", "Bingnan", ""], ["Wang", "Jiarong", ""], ["Puthussery", "Densen", ""], ["S", "Hrishikesh P", ""], ["Kuriakose", "Melvin", ""], ["C", "Jiji", "V"], ["Sundar", "Varun", ""], ["Hegde", "Sumanth", ""], ["Kothandaraman", "Divya", ""], ["Mitra", "Kaushik", ""], ["Jassal", "Akashdeep", ""], ["Shah", "Nisarg A.", ""], ["Nathan", "Sabari", ""], ["Rahel", "Nagat Abdalla Esiad", ""], ["Chen", "Dafan", ""], ["Nie", "Shichao", ""], ["Yin", "Shuting", ""], ["Ma", "Chengconghui", ""], ["Wang", "Haoran", ""], ["Zhao", "Tongtong", ""], ["Zhao", "Shanshan", ""], ["Rego", "Joshua", ""], ["Chen", "Huaijin", ""], ["Li", "Shuai", ""], ["Hu", "Zhenhua", ""], ["Lau", "Kin Wai", ""], ["Po", "Lai-Man", ""], ["Yu", "Dahai", ""], ["Rehman", "Yasar Abbas Ur", ""], ["Li", "Yiqun", ""], ["Xing", "Lianping", ""]]}, {"id": "2008.07760", "submitter": "Minhyuk Sung", "authors": "Jiahui Lei, Srinath Sridhar, Paul Guerrero, Minhyuk Sung, Niloy Mitra,\n  Leonidas J. Guibas", "title": "Pix2Surf: Learning Parametric 3D Surface Models of Objects from Images", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of learning to generate 3D parametric surface\nrepresentations for novel object instances, as seen from one or more views.\nPrevious work on learning shape reconstruction from multiple views uses\ndiscrete representations such as point clouds or voxels, while continuous\nsurface generation approaches lack multi-view consistency. We address these\nissues by designing neural networks capable of generating high-quality\nparametric 3D surfaces which are also consistent between views. Furthermore,\nthe generated 3D surfaces preserve accurate image pixel to 3D surface point\ncorrespondences, allowing us to lift texture information to reconstruct shapes\nwith rich geometry and appearance. Our method is supervised and trained on a\npublic dataset of shapes from common object categories. Quantitative results\nindicate that our method significantly outperforms previous work, while\nqualitative results demonstrate the high quality of our reconstructions.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 06:33:40 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Lei", "Jiahui", ""], ["Sridhar", "Srinath", ""], ["Guerrero", "Paul", ""], ["Sung", "Minhyuk", ""], ["Mitra", "Niloy", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "2008.07770", "submitter": "Xiaoran Zhang", "authors": "Xiaoran Zhang and Michelle Noga and Kumaradevan Punithakumar", "title": "Fully automated deep learning based segmentation of normal, infarcted\n  and edema regions from multiple cardiac MRI sequences", "comments": "Accepted by MyoPS 2020 Challenge in conjunction with MICCAI and\n  STACOM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Myocardial characterization is essential for patients with myocardial\ninfarction and other myocardial diseases, and the assessment is often performed\nusing cardiac magnetic resonance (CMR) sequences. In this study, we propose a\nfully automated approach using deep convolutional neural networks (CNN) for\ncardiac pathology segmentation, including left ventricular (LV) blood pool,\nright ventricular blood pool, LV normal myocardium, LV myocardial edema (ME)\nand LV myocardial scars (MS). The input to the network consists of three CMR\nsequences, namely, late gadolinium enhancement (LGE), T2 and balanced steady\nstate free precession (bSSFP). The proposed approach utilized the data provided\nby the MyoPS challenge hosted by MICCAI 2020 in conjunction with STACOM. The\ntraining set for the CNN model consists of images acquired from 25 cases, and\nthe gold standard labels are provided by trained raters and validated by\nradiologists. The proposed approach introduces a data augmentation module,\nlinear encoder and decoder module and a network module to increase the number\nof training samples and improve the prediction accuracy for LV ME and MS. The\nproposed approach is evaluated by the challenge organizers with a test set\nincluding 20 cases and achieves a mean dice score of $46.8\\%$ for LV MS and\n$55.7\\%$ for LV ME+MS\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 07:01:24 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Zhang", "Xiaoran", ""], ["Noga", "Michelle", ""], ["Punithakumar", "Kumaradevan", ""]]}, {"id": "2008.07783", "submitter": "Guangming Yao", "authors": "Guangming Yao, Yi Yuan, Tianjia Shao, Kun Zhou", "title": "Mesh Guided One-shot Face Reenactment using Graph Convolutional Networks", "comments": "9 pages, 8 figures,accepted by ACM MM2020", "journal-ref": null, "doi": "10.1145/3394171.3413865", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face reenactment aims to animate a source face image to a different pose and\nexpression provided by a driving image. Existing approaches are either designed\nfor a specific identity, or suffer from the identity preservation problem in\nthe one-shot or few-shot scenarios. In this paper, we introduce a method for\none-shot face reenactment, which uses the reconstructed 3D meshes (i.e., the\nsource mesh and driving mesh) as guidance to learn the optical flow needed for\nthe reenacted face synthesis. Technically, we explicitly exclude the driving\nface's identity information in the reconstructed driving mesh. In this way, our\nnetwork can focus on the motion estimation for the source face without the\ninterference of driving face shape. We propose a motion net to learn the face\nmotion, which is an asymmetric autoencoder. The encoder is a graph\nconvolutional network (GCN) that learns a latent motion vector from the meshes,\nand the decoder serves to produce an optical flow image from the latent vector\nwith CNNs. Compared to previous methods using sparse keypoints to guide the\noptical flow learning, our motion net learns the optical flow directly from 3D\ndense meshes, which provide the detailed shape and pose information for the\noptical flow, so it can achieve more accurate expression and pose on the\nreenacted face. Extensive experiments show that our method can generate\nhigh-quality results and outperforms state-of-the-art methods in both\nqualitative and quantitative comparisons.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 07:41:40 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 11:21:18 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Yao", "Guangming", ""], ["Yuan", "Yi", ""], ["Shao", "Tianjia", ""], ["Zhou", "Kun", ""]]}, {"id": "2008.07792", "submitter": "Chengshu Li", "authors": "Fei Xia, Chengshu Li, Roberto Mart\\'in-Mart\\'in, Or Litany, Alexander\n  Toshev, Silvio Savarese", "title": "ReLMoGen: Leveraging Motion Generation in Reinforcement Learning for\n  Mobile Manipulation", "comments": "First two authors contributed equally. Access project website at\n  http://svl.stanford.edu/projects/relmogen", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Reinforcement Learning (RL) approaches use joint control signals\n(positions, velocities, torques) as action space for continuous control tasks.\nWe propose to lift the action space to a higher level in the form of subgoals\nfor a motion generator (a combination of motion planner and trajectory\nexecutor). We argue that, by lifting the action space and by leveraging\nsampling-based motion planners, we can efficiently use RL to solve complex,\nlong-horizon tasks that could not be solved with existing RL methods in the\noriginal action space. We propose ReLMoGen -- a framework that combines a\nlearned policy to predict subgoals and a motion generator to plan and execute\nthe motion needed to reach these subgoals. To validate our method, we apply\nReLMoGen to two types of tasks: 1) Interactive Navigation tasks, navigation\nproblems where interactions with the environment are required to reach the\ndestination, and 2) Mobile Manipulation tasks, manipulation tasks that require\nmoving the robot base. These problems are challenging because they are usually\nlong-horizon, hard to explore during training, and comprise alternating phases\nof navigation and interaction. Our method is benchmarked on a diverse set of\nseven robotics tasks in photo-realistic simulation environments. In all\nsettings, ReLMoGen outperforms state-of-the-art Reinforcement Learning and\nHierarchical Reinforcement Learning baselines. ReLMoGen also shows outstanding\ntransferability between different motion generators at test time, indicating a\ngreat potential to transfer to real robots.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 08:05:15 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 04:44:22 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Xia", "Fei", ""], ["Li", "Chengshu", ""], ["Mart\u00edn-Mart\u00edn", "Roberto", ""], ["Litany", "Or", ""], ["Toshev", "Alexander", ""], ["Savarese", "Silvio", ""]]}, {"id": "2008.07816", "submitter": "Anbang Yao", "authors": "Anbang Yao, Dawei Sun", "title": "Knowledge Transfer via Dense Cross-Layer Mutual-Distillation", "comments": "Accepted by ECCV 2020. The code is available at\n  https://github.com/sundw2014/DCM, which is based on the implementation of our\n  DKS work https://github.com/sundw2014/DKS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Distillation (KD) based methods adopt the one-way Knowledge\nTransfer (KT) scheme in which training a lower-capacity student network is\nguided by a pre-trained high-capacity teacher network. Recently, Deep Mutual\nLearning (DML) presented a two-way KT strategy, showing that the student\nnetwork can be also helpful to improve the teacher network. In this paper, we\npropose Dense Cross-layer Mutual-distillation (DCM), an improved two-way KT\nmethod in which the teacher and student networks are trained collaboratively\nfrom scratch. To augment knowledge representation learning, well-designed\nauxiliary classifiers are added to certain hidden layers of both teacher and\nstudent networks. To boost KT performance, we introduce dense bidirectional KD\noperations between the layers appended with classifiers. After training, all\nauxiliary classifiers are discarded, and thus there are no extra parameters\nintroduced to final models. We test our method on a variety of KT tasks,\nshowing its superiorities over related methods. Code is available at\nhttps://github.com/sundw2014/DCM\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 09:25:08 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Yao", "Anbang", ""], ["Sun", "Dawei", ""]]}, {"id": "2008.07817", "submitter": "Tomu Tahara", "authors": "Tomu Tahara, Takashi Seno, Gaku Narita, Tomoya Ishikawa", "title": "Retargetable AR: Context-aware Augmented Reality in Indoor Scenes based\n  on 3D Scene Graph", "comments": "11 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present Retargetable AR, a novel AR framework that yields\nan AR experience that is aware of scene contexts set in various real\nenvironments, achieving natural interaction between the virtual and real\nworlds. To this end, we characterize scene contexts with relationships among\nobjects in 3D space, not with coordinates transformations. A context assumed by\nan AR content and a context formed by a real environment where users experience\nAR are represented as abstract graph representations, i.e. scene graphs. From\nRGB-D streams, our framework generates a volumetric map in which geometric and\nsemantic information of a scene are integrated. Moreover, using the semantic\nmap, we abstract scene objects as oriented bounding boxes and estimate their\norientations. With such a scene representation, our framework constructs, in an\nonline fashion, a 3D scene graph characterizing the context of a real\nenvironment for AR. The correspondence between the constructed graph and an AR\nscene graph denoting the context of AR content provides a semantically\nregistered content arrangement, which facilitates natural interaction between\nthe virtual and real worlds. We performed extensive evaluations on our\nprototype system through quantitative evaluation of the performance of the\noriented bounding box estimation, subjective evaluation of the AR content\narrangement based on constructed 3D scene graphs, and an online AR\ndemonstration. The results of these evaluations showed the effectiveness of our\nframework, demonstrating that it can provide a context-aware AR experience in a\nvariety of real scenes.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 09:25:55 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Tahara", "Tomu", ""], ["Seno", "Takashi", ""], ["Narita", "Gaku", ""], ["Ishikawa", "Tomoya", ""]]}, {"id": "2008.07819", "submitter": "Tianqi Ma", "authors": "Tianqi Ma, Lin Zhang, Xiumin Diao, Ou Ma", "title": "ConvGRU in Fine-grained Pitching Action Recognition for Action Outcome\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction of the action outcome is a new challenge for a robot\ncollaboratively working with humans. With the impressive progress in video\naction recognition in recent years, fine-grained action recognition from video\ndata turns into a new concern. Fine-grained action recognition detects subtle\ndifferences of actions in more specific granularity and is significant in many\nfields such as human-robot interaction, intelligent traffic management, sports\ntraining, health caring. Considering that the different outcomes are closely\nconnected to the subtle differences in actions, fine-grained action recognition\nis a practical method for action outcome prediction. In this paper, we explore\nthe performance of convolutional gate recurrent unit (ConvGRU) method on a\nfine-grained action recognition tasks: predicting outcomes of ball-pitching.\nBased on sequences of RGB images of human actions, the proposed approach\nachieved the performance of 79.17% accuracy, which exceeds the current\nstate-of-the-art result. We also compared different network implementations and\nshowed the influence of different image sampling methods, different fusion\nmethods and pre-training, etc. Finally, we discussed the advantages and\nlimitations of ConvGRU in such action outcome prediction and fine-grained\naction recognition tasks.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 09:27:17 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Ma", "Tianqi", ""], ["Zhang", "Lin", ""], ["Diao", "Xiumin", ""], ["Ma", "Ou", ""]]}, {"id": "2008.07828", "submitter": "Luk\\'a\\v{s} Picek", "authors": "Miroslav Valan and Luk\\'a\\v{s} Picek", "title": "Mastering Large Scale Multi-label Image Recognition with high efficiency\n  overCamera trap images", "comments": "Hakuna Ma-data Challenge: 1st place submission description.\n  (Fine-Grained Visual Categorization Workshop - CVPR2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Camera traps are crucial in biodiversity motivated studies, however dealing\nwith large number of images while annotating these data sets is a tedious and\ntime consuming task. To speed up this process, Machine Learning approaches are\na reasonable asset. In this article we are proposing an easy, accessible,\nlight-weight, fast and efficient approach based on our winning submission to\nthe \"Hakuna Ma-data - Serengeti Wildlife Identification challenge\". Our system\nachieved an Accuracy of 97% and outperformed the human level performance. We\nshow that, given relatively large data sets, it is effective to look at each\nimage only once with little or no augmentation. By utilizing such a simple, yet\neffective baseline we were able to avoid over-fitting without extensive\nregularization techniques and to train a top scoring system on a very limited\nhardware featuring single GPU (1080Ti) despite the large training set (6.7M\nimages and 6TB).\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 09:51:34 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Valan", "Miroslav", ""], ["Picek", "Luk\u00e1\u0161", ""]]}, {"id": "2008.07831", "submitter": "Malek El Husseini", "authors": "Malek Husseini, Anjany Sekuboyina, Maximilian Loeffler, Fernando\n  Navarro, Bjoern H. Menze, Jan S. Kirschke", "title": "Grading Loss: A Fracture Grade-based Metric Loss for Vertebral Fracture\n  Detection", "comments": "To be presented at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Osteoporotic vertebral fractures have a severe impact on patients' overall\nwell-being but are severely under-diagnosed. These fractures present themselves\nat various levels of severity measured using the Genant's grading scale.\nInsufficient annotated datasets, severe data-imbalance, and minor difference in\nappearances between fractured and healthy vertebrae make naive classification\napproaches result in poor discriminatory performance. Addressing this, we\npropose a representation learning-inspired approach for automated vertebral\nfracture detection, aimed at learning latent representations efficient for\nfracture detection. Building on state-of-art metric losses, we present a novel\nGrading Loss for learning representations that respect Genant's fracture\ngrading scheme. On a publicly available spine dataset, the proposed loss\nfunction achieves a fracture detection F1 score of 81.5%, a 10% increase over a\nnaive classification baseline.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 10:03:45 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Husseini", "Malek", ""], ["Sekuboyina", "Anjany", ""], ["Loeffler", "Maximilian", ""], ["Navarro", "Fernando", ""], ["Menze", "Bjoern H.", ""], ["Kirschke", "Jan S.", ""]]}, {"id": "2008.07832", "submitter": "Tzu-Jui Julius Wang", "authors": "Tzu-Jui Julius Wang, Selen Pehlivan, Jorma Laaksonen", "title": "Tackling the Unannotated: Scene Graph Generation with Bias-Reduced\n  Models", "comments": "accepted to BMVC2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting a scene graph that captures visual entities and their interactions\nin an image has been considered a crucial step towards full scene\ncomprehension. Recent scene graph generation (SGG) models have shown their\ncapability of capturing the most frequent relations among visual entities.\nHowever, the state-of-the-art results are still far from satisfactory, e.g.\nmodels can obtain 31% in overall recall R@100, whereas the likewise important\nmean class-wise recall mR@100 is only around 8% on Visual Genome (VG). The\ndiscrepancy between R and mR results urges to shift the focus from pursuing a\nhigh R to a high mR with a still competitive R. We suspect that the observed\ndiscrepancy stems from both the annotation bias and sparse annotations in VG,\nin which many visual entity pairs are either not annotated at all or only with\na single relation when multiple ones could be valid. To address this particular\nissue, we propose a novel SGG training scheme that capitalizes on self-learned\nknowledge. It involves two relation classifiers, one offering a less biased\nsetting for the other to base on. The proposed scheme can be applied to most of\nthe existing SGG models and is straightforward to implement. We observe\nsignificant relative improvements in mR (between +6.6% and +20.4%) and\ncompetitive or better R (between -2.4% and 0.3%) across all standard SGG tasks.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 10:04:51 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Wang", "Tzu-Jui Julius", ""], ["Pehlivan", "Selen", ""], ["Laaksonen", "Jorma", ""]]}, {"id": "2008.07839", "submitter": "Raghav Bali", "authors": "Kartik Chaudhary and Raghav Bali", "title": "EASTER: Efficient and Scalable Text Recognizer", "comments": "9 pages, fixed typos and minor edits", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent progress in deep learning has led to the development of Optical\nCharacter Recognition (OCR) systems which perform remarkably well. Most\nresearch has been around recurrent networks as well as complex gated layers\nwhich make the overall solution complex and difficult to scale. In this paper,\nwe present an Efficient And Scalable TExt Recognizer (EASTER) to perform\noptical character recognition on both machine printed and handwritten text. Our\nmodel utilises 1-D convolutional layers without any recurrence which enables\nparallel training with considerably less volume of data. We experimented with\nmultiple variations of our architecture and one of the smallest variant (depth\nand number of parameter wise) performs comparably to RNN based complex choices.\nOur 20-layered deepest variant outperforms RNN architectures with a good margin\non benchmarking datasets like IIIT-5k and SVT. We also showcase improvements\nover the current best results on offline handwritten text recognition task. We\nalso present data generation pipelines with augmentation setup to generate\nsynthetic datasets for both handwritten and machine printed text.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 10:26:03 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 14:02:50 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Chaudhary", "Kartik", ""], ["Bali", "Raghav", ""]]}, {"id": "2008.07853", "submitter": "Ovi Paul", "authors": "Ovi Paul", "title": "Image Pre-processing on NumtaDB for Bengali Handwritten Digit\n  Recognition", "comments": "5 pages, 8 figures and 4 tables", "journal-ref": "2018 International Conference on Bangla Speech and Language\n  Processing (ICBSLP), Sylhet, 2018, pp. 1-6", "doi": "10.1109/ICBSLP.2018.8554910", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NumtaDB is by far the largest data-set collection for handwritten digits in\nBengali. This is a diverse dataset containing more than 85000 images. But this\ndiversity also makes this dataset very difficult to work with. The goal of this\npaper is to find the benchmark for pre-processed images which gives good\naccuracy on any machine learning models. The reason being, there are no\navailable pre-processed data for Bengali digit recognition to work with like\nthe English digits for MNIST.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 11:02:25 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Paul", "Ovi", ""]]}, {"id": "2008.07861", "submitter": "Yuri Feldman", "authors": "Yuri Feldman, Yoel Shapiro and Dotan Di Castro", "title": "Depth Completion with RGB Prior", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth cameras are a prominent perception system for robotics, especially when\noperating in natural unstructured environments. Industrial applications,\nhowever, typically involve reflective objects under harsh lighting conditions,\na challenging scenario for depth cameras, as it induces numerous reflections\nand deflections, leading to loss of robustness and deteriorated accuracy. Here,\nwe developed a deep model to correct the depth channel in RGBD images, aiming\nto restore the depth information to the required accuracy. To train the model,\nwe created a novel industrial dataset that we now present to the public. The\ndata was collected with low-end depth cameras and the ground truth depth was\ngenerated by multi-view fusion.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 11:22:20 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Feldman", "Yuri", ""], ["Shapiro", "Yoel", ""], ["Di Castro", "Dotan", ""]]}, {"id": "2008.07872", "submitter": "Amirhossein Kardoost", "authors": "Amirhossein Kardoost, Kalun Ho, Peter Ochs, Margret Keuper", "title": "Self-supervised Sparse to Dense Motion Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observable motion in videos can give rise to the definition of objects moving\nwith respect to the scene. The task of segmenting such moving objects is\nreferred to as motion segmentation and is usually tackled either by aggregating\nmotion information in long, sparse point trajectories, or by directly producing\nper frame dense segmentations relying on large amounts of training data. In\nthis paper, we propose a self supervised method to learn the densification of\nsparse motion segmentations from single video frames. While previous approaches\ntowards motion segmentation build upon pre-training on large surrogate datasets\nand use dense motion information as an essential cue for the pixelwise\nsegmentation, our model does not require pre-training and operates at test time\non single frames. It can be trained in a sequence specific way to produce high\nquality dense segmentations from sparse and noisy input. We evaluate our method\non the well-known motion segmentation datasets FBMS59 and DAVIS16.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 11:40:18 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Kardoost", "Amirhossein", ""], ["Ho", "Kalun", ""], ["Ochs", "Peter", ""], ["Keuper", "Margret", ""]]}, {"id": "2008.07884", "submitter": "Kejun Wang", "authors": "Meichen Liu, Kejun Wang, Juihang Ji and Shuzhi Sam Ge", "title": "Person image generation with semantic attention network for person\n  re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pose variation is one of the key factors which prevents the network from\nlearning a robust person re-identification (Re-ID) model. To address this\nissue, we propose a novel person pose-guided image generation method, which is\ncalled the semantic attention network. The network consists of several semantic\nattention blocks, where each block attends to preserve and update the pose code\nand the clothing textures. The introduction of the binary segmentation mask and\nthe semantic parsing is important for seamlessly stitching foreground and\nbackground in the pose-guided image generation. Compared with other methods,\nour network can characterize better body shape and keep clothing attributes,\nsimultaneously. Our synthesized image can obtain better appearance and shape\nconsistency related to the original image. Experimental results show that our\napproach is competitive with respect to both quantitative and qualitative\nresults on Market-1501 and DeepFashion. Furthermore, we conduct extensive\nevaluations by using person re-identification (Re-ID) systems trained with the\npose-transferred person based augmented data. The experiment shows that our\napproach can significantly enhance the person Re-ID accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 12:18:51 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Liu", "Meichen", ""], ["Wang", "Kejun", ""], ["Ji", "Juihang", ""], ["Ge", "Shuzhi Sam", ""]]}, {"id": "2008.07928", "submitter": "Jingyang Zhang", "authors": "Jingyang Zhang, Yao Yao, Shiwei Li, Zixin Luo, Tian Fang", "title": "Visibility-aware Multi-view Stereo Network", "comments": "Accepted to BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based multi-view stereo (MVS) methods have demonstrated promising\nresults. However, very few existing networks explicitly take the pixel-wise\nvisibility into consideration, resulting in erroneous cost aggregation from\noccluded pixels. In this paper, we explicitly infer and integrate the\npixel-wise occlusion information in the MVS network via the matching\nuncertainty estimation. The pair-wise uncertainty map is jointly inferred with\nthe pair-wise depth map, which is further used as weighting guidance during the\nmulti-view cost volume fusion. As such, the adverse influence of occluded\npixels is suppressed in the cost fusion. The proposed framework Vis-MVSNet\nsignificantly improves depth accuracies in the scenes with severe occlusion.\nExtensive experiments are performed on DTU, BlendedMVS, and Tanks and Temples\ndatasets to justify the effectiveness of the proposed framework.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 13:47:36 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 02:40:59 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Zhang", "Jingyang", ""], ["Yao", "Yao", ""], ["Li", "Shiwei", ""], ["Luo", "Zixin", ""], ["Fang", "Tian", ""]]}, {"id": "2008.07930", "submitter": "Philipp Gruening", "authors": "Philipp Gr\\\"uning, Thomas Martinetz, Erhardt Barth", "title": "Feature Products Yield Efficient Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Feature-Product networks (FP-nets) as a novel deep-network\narchitecture based on a new building block inspired by principles of biological\nvision. For each input feature map, a so-called FP-block learns two different\nfilters, the outputs of which are then multiplied. Such FP-blocks are inspired\nby models of end-stopped neurons, which are common in cortical areas V1 and\nespecially in V2. Convolutional neural networks can be transformed into\nparameter-efficient FP-nets by substituting conventional blocks of regular\nconvolutions with FP-blocks. In this way, we create several novel FP-nets based\non state-of-the-art networks and evaluate them on the Cifar-10 and ImageNet\nchallenges. We show that the use of FP-blocks reduces the number of parameters\nsignificantly without decreasing generalization capability. Since so far\nheuristics and search algorithms have been used to find more efficient\nnetworks, it seems remarkable that we can obtain even more efficient networks\nbased on a novel bio-inspired design principle.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 13:47:56 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Gr\u00fcning", "Philipp", ""], ["Martinetz", "Thomas", ""], ["Barth", "Erhardt", ""]]}, {"id": "2008.07931", "submitter": "Qing Shuai", "authors": "Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou,\n  Hujun Bao", "title": "Motion Capture from Internet Videos", "comments": "ECCV 2020 (Oral), project page: https://zju3dv.github.io/iMoCap/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in image-based human pose estimation make it possible to\ncapture 3D human motion from a single RGB video. However, the inherent depth\nambiguity and self-occlusion in a single view prohibit the recovery of as\nhigh-quality motion as multi-view reconstruction. While multi-view videos are\nnot common, the videos of a celebrity performing a specific action are usually\nabundant on the Internet. Even if these videos were recorded at different time\ninstances, they would encode the same motion characteristics of the person.\nTherefore, we propose to capture human motion by jointly analyzing these\nInternet videos instead of using single videos separately. However, this new\ntask poses many new challenges that cannot be addressed by existing methods, as\nthe videos are unsynchronized, the camera viewpoints are unknown, the\nbackground scenes are different, and the human motions are not exactly the same\namong videos. To address these challenges, we propose a novel\noptimization-based framework and experimentally demonstrate its ability to\nrecover much more precise and detailed motion from multiple videos, compared\nagainst monocular motion capture methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 13:48:37 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 02:56:48 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Dong", "Junting", ""], ["Shuai", "Qing", ""], ["Zhang", "Yuanqing", ""], ["Liu", "Xian", ""], ["Zhou", "Xiaowei", ""], ["Bao", "Hujun", ""]]}, {"id": "2008.07935", "submitter": "Ye Zhu", "authors": "Ye Zhu, Yu Wu, Yi Yang, and Yan Yan", "title": "Describing Unseen Videos via Multi-Modal Cooperative Dialog Agents", "comments": "ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the arising concerns for the AI systems provided with direct access to\nabundant sensitive information, researchers seek to develop more reliable AI\nwith implicit information sources. To this end, in this paper, we introduce a\nnew task called video description via two multi-modal cooperative dialog\nagents, whose ultimate goal is for one conversational agent to describe an\nunseen video based on the dialog and two static frames. Specifically, one of\nthe intelligent agents - Q-BOT - is given two static frames from the beginning\nand the end of the video, as well as a finite number of opportunities to ask\nrelevant natural language questions before describing the unseen video. A-BOT,\nthe other agent who has already seen the entire video, assists Q-BOT to\naccomplish the goal by providing answers to those questions. We propose a\nQA-Cooperative Network with a dynamic dialog history update learning mechanism\nto transfer knowledge from A-BOT to Q-BOT, thus helping Q-BOT to better\ndescribe the video. Extensive experiments demonstrate that Q-BOT can\neffectively learn to describe an unseen video by the proposed model and the\ncooperative learning method, achieving the promising performance where Q-BOT is\ngiven the full ground truth history dialog.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 14:01:09 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2020 12:58:39 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Zhu", "Ye", ""], ["Wu", "Yu", ""], ["Yang", "Yi", ""], ["Yan", "Yan", ""]]}, {"id": "2008.07960", "submitter": "Chenlong Liu", "authors": "Shuqiang Jiang, Yaohui Zhu, Chenlong Liu, Xinhang Song, Xiangyang Li,\n  and Weiqing Min", "title": "Dataset Bias in Few-shot Image Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of few-shot image recognition (FSIR) is to identify novel categories\nwith a small number of annotated samples by exploiting transferable knowledge\nfrom training data (base categories). Most current studies assume that the\ntransferable knowledge can be well used to identify novel categories. However,\nsuch transferable capability may be impacted by the dataset bias, and this\nproblem has rarely been investigated before. Besides, most of few-shot learning\nmethods are biased to different datasets, which is also an important issue that\nneeds to be investigated deeply. In this paper, we first investigate the impact\nof transferable capabilities learned from base categories. Specifically, we use\nthe relevance to measure relationships between base categories and novel\ncategories. Distributions of base categories are depicted via the instance\ndensity and category diversity. The FSIR model learns better transferable\nknowledge from relevant training data. In the relevant data, dense instances or\ndiverse categories can further enrich the learned knowledge. Experimental\nresults on different sub-datasets of ImagNet demonstrate category relevance,\ninstance density and category diversity can depict transferable bias from base\ncategories. Second, we investigate performance differences on different\ndatasets from dataset structures and different few-shot learning methods.\nSpecifically, we introduce image complexity, intra-concept visual consistency,\nand inter-concept visual similarity to quantify characteristics of dataset\nstructures. We use these quantitative characteristics and four few-shot\nlearning methods to analyze performance differences on five different datasets.\nBased on the experimental analysis, some insightful observations are obtained\nfrom the perspective of both dataset structures and few-shot learning methods.\nWe hope these observations are useful to guide future FSIR research.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 14:46:23 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 02:37:18 GMT"}, {"version": "v3", "created": "Tue, 16 Mar 2021 03:23:18 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Jiang", "Shuqiang", ""], ["Zhu", "Yaohui", ""], ["Liu", "Chenlong", ""], ["Song", "Xinhang", ""], ["Li", "Xiangyang", ""], ["Min", "Weiqing", ""]]}, {"id": "2008.07961", "submitter": "Ye Zhu", "authors": "Ye Zhu, Yan Yan, and Oleg Komogortsev", "title": "Hierarchical HMM for Eye Movement Classification", "comments": "ECCV2020 Workshop, OpenEyes: Eye Gaze in AR, VR, and in the Wild", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we tackle the problem of ternary eye movement classification,\nwhich aims to separate fixations, saccades and smooth pursuits from the raw eye\npositional data. The efficient classification of these different types of eye\nmovements helps to better analyze and utilize the eye tracking data. Different\nfrom the existing methods that detect eye movement by several pre-defined\nthreshold values, we propose a hierarchical Hidden Markov Model (HMM)\nstatistical algorithm for detecting fixations, saccades and smooth pursuits.\nThe proposed algorithm leverages different features from the recorded raw eye\ntracking data with a hierarchical classification strategy, separating one type\nof eye movement each time. Experimental results demonstrate the effectiveness\nand robustness of the proposed method by achieving competitive or better\nperformance compared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 14:47:23 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Zhu", "Ye", ""], ["Yan", "Yan", ""], ["Komogortsev", "Oleg", ""]]}, {"id": "2008.07981", "submitter": "Arjun Haridas Pallath", "authors": "Arjun Haridas Pallath, Martin Dyrba", "title": "Comparison of Convolutional neural network training parameters for\n  detecting Alzheimers disease and effect on visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) have become a powerful tool for detecting\npatterns in image data. Recent papers report promising results in the domain of\ndisease detection using brain MRI data. Despite the high accuracy obtained from\nCNN models for MRI data so far, almost no papers provided information on the\nfeatures or image regions driving this accuracy as adequate methods were\nmissing or challenging to apply. Recently, the toolbox iNNvestigate has become\navailable, implementing various state of the art methods for deep learning\nvisualizations. Currently, there is a great demand for a comparison of\nvisualization algorithms to provide an overview of the practical usefulness and\ncapability of these algorithms.\n  Therefore, this thesis has two goals: 1. To systematically evaluate the\ninfluence of CNN hyper-parameters on model accuracy. 2. To compare various\nvisualization methods with respect to the quality (i.e. randomness/focus,\nsoundness).\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 15:21:50 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Pallath", "Arjun Haridas", ""], ["Dyrba", "Martin", ""]]}, {"id": "2008.07989", "submitter": "Jascha Kolberg", "authors": "Jascha Kolberg and Marcel Grimmer and Marta Gomez-Barrero and\n  Christoph Busch", "title": "Anomaly Detection with Convolutional Autoencoders for Fingerprint\n  Presentation Attack Detection", "comments": null, "journal-ref": null, "doi": "10.1109/TBIOM.2021.3050036", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the popularity of fingerprint-based biometric authentication\nsystems significantly increased. However, together with many advantages,\nbiometric systems are still vulnerable to presentation attacks (PAs). In\nparticular, this applies for unsupervised applications, where new attacks\nunknown to the system operator may occur. Therefore, presentation attack\ndetection (PAD) methods are used to determine whether samples stem from a bona\nfide subject or from a presentation attack instrument (PAI). In this context,\nmost works are dedicated to solve PAD as a two-class classification problem,\nwhich includes training a model on both bona fide and PA samples. In spite of\nthe good detection rates reported, these methods still face difficulties\ndetecting PAIs from unknown materials. To address this issue, we propose a new\nPAD technique based on autoencoders (AEs) trained only on bona fide samples\n(i.e. one-class), which are captured in the short wave infrared domain. On the\nexperimental evaluation over a database of 19,711 bona fide and 4,339 PA images\nincluding 45 different PAI species, a detection equal error rate (D-EER) of\n2.00% was achieved. Additionally, our best performing AE model is compared to\nfurther one-class classifiers (support vector machine, Gaussian mixture model).\nThe results show the effectiveness of the AE model as it significantly\noutperforms the previously proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 15:33:41 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 14:08:47 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Kolberg", "Jascha", ""], ["Grimmer", "Marcel", ""], ["Gomez-Barrero", "Marta", ""], ["Busch", "Christoph", ""]]}, {"id": "2008.08001", "submitter": "Bo Yang", "authors": "Bo Yang, Xuelin Cao, Chau Yuen, Lijun Qian", "title": "Offloading Optimization in Edge Computing for Deep Learning Enabled\n  Target Tracking by Internet-of-UAVs", "comments": "Accepted by IEEE IoTJ", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The empowering unmanned aerial vehicles (UAVs) have been extensively used in\nproviding intelligence such as target tracking. In our field experiments, a\npre-trained convolutional neural network (CNN) is deployed at the UAV to\nidentify a target (a vehicle) from the captured video frames and enable the UAV\nto keep tracking. However, this kind of visual target tracking demands a lot of\ncomputational resources due to the desired high inference accuracy and\nstringent delay requirement. This motivates us to consider offloading this type\nof deep learning (DL) tasks to a mobile edge computing (MEC) server due to\nlimited computational resource and energy budget of the UAV, and further\nimprove the inference accuracy. Specifically, we propose a novel hierarchical\nDL tasks distribution framework, where the UAV is embedded with lower layers of\nthe pre-trained CNN model, while the MEC server with rich computing resources\nwill handle the higher layers of the CNN model. An optimization problem is\nformulated to minimize the weighted-sum cost including the tracking delay and\nenergy consumption introduced by communication and computing of the UAVs, while\ntaking into account the quality of data (e.g., video frames) input to the DL\nmodel and the inference errors. Analytical results are obtained and insights\nare provided to understand the tradeoff between the weighted-sum cost and\ninference error rate in the proposed framework. Numerical results demonstrate\nthe effectiveness of the proposed offloading framework.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 16:00:36 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Yang", "Bo", ""], ["Cao", "Xuelin", ""], ["Yuen", "Chau", ""], ["Qian", "Lijun", ""]]}, {"id": "2008.08005", "submitter": "Siddharth Nayak", "authors": "Siddharth Nayak and Balaraman Ravindran", "title": "Reinforcement Learning for Improving Object Detection", "comments": "14 pages, 6 figures, 4 tables. Accepted in the RLQ-TOD workshop at\n  ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The performance of a trained object detection neural network depends a lot on\nthe image quality. Generally, images are pre-processed before feeding them into\nthe neural network and domain knowledge about the image dataset is used to\nchoose the pre-processing techniques. In this paper, we introduce an algorithm\ncalled ObjectRL to choose the amount of a particular pre-processing to be\napplied to improve the object detection performances of pre-trained networks.\nThe main motivation for ObjectRL is that an image which looks good to a human\neye may not necessarily be the optimal one for a pre-trained object detector to\ndetect objects.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 16:20:04 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Nayak", "Siddharth", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "2008.08012", "submitter": "Gouthaman Kv", "authors": "Gouthaman KV, Athira Nambiar, Kancheti Sai Srinivas, Anurag Mittal", "title": "Linguistically-aware Attention for Reducing the Semantic-Gap in\n  Vision-Language Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention models are widely used in Vision-language (V-L) tasks to perform\nthe visual-textual correlation. Humans perform such a correlation with a strong\nlinguistic understanding of the visual world. However, even the best performing\nattention model in V-L tasks lacks such a high-level linguistic understanding,\nthus creating a semantic gap between the modalities. In this paper, we propose\nan attention mechanism - Linguistically-aware Attention (LAT) - that leverages\nobject attributes obtained from generic object detectors along with pre-trained\nlanguage models to reduce this semantic gap. LAT represents visual and textual\nmodalities in a common linguistically-rich space, thus providing linguistic\nawareness to the attention process. We apply and demonstrate the effectiveness\nof LAT in three V-L tasks: Counting-VQA, VQA, and Image captioning. In\nCounting-VQA, we propose a novel counting-specific VQA model to predict an\nintuitive count and achieve state-of-the-art results on five datasets. In VQA\nand Captioning, we show the generic nature and effectiveness of LAT by adapting\nit into various baselines and consistently improving their performance.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 16:29:49 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["KV", "Gouthaman", ""], ["Nambiar", "Athira", ""], ["Srinivas", "Kancheti Sai", ""], ["Mittal", "Anurag", ""]]}, {"id": "2008.08016", "submitter": "Adnane Cabani", "authors": "Adnane Cabani, Karim Hammoudi, Halim Benhabiles and Mahmoud Melkemi", "title": "MaskedFace-Net -- A Dataset of Correctly/Incorrectly Masked Face Images\n  in the Context of COVID-19", "comments": null, "journal-ref": null, "doi": "10.1016/j.smhl.2020.100144", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The wearing of the face masks appears as a solution for limiting the spread\nof COVID-19. In this context, efficient recognition systems are expected for\nchecking that people faces are masked in regulated areas. To perform this task,\na large dataset of masked faces is necessary for training deep learning models\ntowards detecting people wearing masks and those not wearing masks. Some large\ndatasets of masked faces are available in the literature. However, at the\nmoment, there are no available large dataset of masked face images that permits\nto check if detected masked faces are correctly worn or not. Indeed, many\npeople are not correctly wearing their masks due to bad practices, bad\nbehaviors or vulnerability of individuals (e.g., children, old people). For\nthese reasons, several mask wearing campaigns intend to sensitize people about\nthis problem and good practices. In this sense, this work proposes three types\nof masked face detection dataset; namely, the Correctly Masked Face Dataset\n(CMFD), the Incorrectly Masked Face Dataset (IMFD) and their combination for\nthe global masked face detection (MaskedFace-Net). Realistic masked face\ndatasets are proposed with a twofold objective: i) to detect people having\ntheir faces masked or not masked, ii) to detect faces having their masks\ncorrectly worn or incorrectly worn (e.g.; at airport portals or in crowds). To\nthe best of our knowledge, no large dataset of masked faces provides such a\ngranularity of classification towards permitting mask wearing analysis.\nMoreover, this work globally presents the applied mask-to-face deformable model\nfor permitting the generation of other masked face images, notably with\nspecific masks. Our datasets of masked face images (137,016 images) are\navailable at https://github.com/cabani/MaskedFace-Net.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 16:38:11 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Cabani", "Adnane", ""], ["Hammoudi", "Karim", ""], ["Benhabiles", "Halim", ""], ["Melkemi", "Mahmoud", ""]]}, {"id": "2008.08023", "submitter": "Jatin Gupta GUPTA", "authors": "Jatin Gupta and Vandana Saini and Kamaldeep Garg", "title": "Multilanguage Number Plate Detection using Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object Detection is a popular field of research for recent technologies. In\nrecent years, profound learning performance attracts the researchers to use it\nin many applications. Number plate (NP) detection and classification is\nanalyzed over decades however, it needs approaches which are more precise and\nstate, language and design independent since cars are now moving from state to\nanother easily. In this paperwe suggest a new strategy to detect NP and\ncomprehend the nation, language and layout of NPs. YOLOv2 sensor with ResNet\nattribute extractor heart is proposed for NP detection and a brand new\nconvolutional neural network architecture is suggested to classify NPs. The\ndetector achieves average precision of 99.57% and country, language and layout\nclassification precision of 99.33%. The results outperforms the majority of the\nprevious works and can move the area forward toward international NP detection\nand recognition.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 16:50:47 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Gupta", "Jatin", ""], ["Saini", "Vandana", ""], ["Garg", "Kamaldeep", ""]]}, {"id": "2008.08024", "submitter": "Neel Dey", "authors": "Guillaume Gisbert, Neel Dey, Hiroshi Ishikawa, Joel Schuman, James\n  Fishbaugh, Guido Gerig", "title": "Self-supervised Denoising via Diffeomorphic Template Estimation:\n  Application to Optical Coherence Tomography", "comments": "To be published in MICCAI Ophthalmic Medical Image Analysis 2020. 11\n  pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical Coherence Tomography (OCT) is pervasive in both the research and\nclinical practice of Ophthalmology. However, OCT images are strongly corrupted\nby noise, limiting their interpretation. Current OCT denoisers leverage\nassumptions on noise distributions or generate targets for training deep\nsupervised denoisers via averaging of repeat acquisitions. However, recent\nself-supervised advances allow the training of deep denoising networks using\nonly repeat acquisitions without clean targets as ground truth, reducing the\nburden of supervised learning. Despite the clear advantages of self-supervised\nmethods, their use is precluded as OCT shows strong structural deformations\neven between sequential scans of the same subject due to involuntary eye\nmotion. Further, direct nonlinear alignment of repeats induces correlation of\nthe noise between images. In this paper, we propose a joint diffeomorphic\ntemplate estimation and denoising framework which enables the use of\nself-supervised denoising for motion deformed repeat acquisitions, without\nempirically registering their noise realizations. Strong qualitative and\nquantitative improvements are achieved in denoising OCT images, with generic\nutility in any imaging modality amenable to multiple exposures.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 16:52:10 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Gisbert", "Guillaume", ""], ["Dey", "Neel", ""], ["Ishikawa", "Hiroshi", ""], ["Schuman", "Joel", ""], ["Fishbaugh", "James", ""], ["Gerig", "Guido", ""]]}, {"id": "2008.08030", "submitter": "Jinsol Lee", "authors": "Jinsol Lee and Ghassan AlRegib", "title": "Gradients as a Measure of Uncertainty in Neural Networks", "comments": "IEEE International Conference on Image Processing (ICIP) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite tremendous success of modern neural networks, they are known to be\noverconfident even when the model encounters inputs with unfamiliar conditions.\nDetecting such inputs is vital to preventing models from making naive\npredictions that may jeopardize real-world applications of neural networks. In\nthis paper, we address the challenging problem of devising a simple yet\neffective measure of uncertainty in deep neural networks. Specifically, we\npropose to utilize backpropagated gradients to quantify the uncertainty of\ntrained models. Gradients depict the required amount of change for a model to\nproperly represent given inputs, thus providing a valuable insight into how\nfamiliar and certain the model is regarding the inputs. We demonstrate the\neffectiveness of gradients as a measure of model uncertainty in applications of\ndetecting unfamiliar inputs, including out-of-distribution and corrupted\nsamples. We show that our gradient-based method outperforms state-of-the-art\nmethods by up to 4.8% of AUROC score in out-of-distribution detection and 35.7%\nin corrupted input detection.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 16:58:46 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 19:47:36 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Lee", "Jinsol", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "2008.08046", "submitter": "Fuqiang Gu Dr", "authors": "Fuqiang Gu, Weicong Sng, Tasbolat Taunyazov and Harold Soh", "title": "TactileSGNet: A Spiking Graph Neural Network for Event-based Tactile\n  Object Recognition", "comments": "IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tactile perception is crucial for a variety of robot tasks including grasping\nand in-hand manipulation. New advances in flexible, event-driven, electronic\nskins may soon endow robots with touch perception capabilities similar to\nhumans. These electronic skins respond asynchronously to changes (e.g., in\npressure, temperature), and can be laid out irregularly on the robot's body or\nend-effector. However, these unique features may render current deep learning\napproaches such as convolutional feature extractors unsuitable for tactile\nlearning. In this paper, we propose a novel spiking graph neural network for\nevent-based tactile object recognition. To make use of local connectivity of\ntaxels, we present several methods for organizing the tactile data in a graph\nstructure. Based on the constructed graphs, we develop a spiking graph\nconvolutional network. The event-driven nature of spiking neural network makes\nit arguably more suitable for processing the event-based data. Experimental\nresults on two tactile datasets show that the proposed method outperforms other\nstate-of-the-art spiking methods, achieving high accuracies of approximately\n90\\% when classifying a variety of different household objects.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 03:35:15 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Gu", "Fuqiang", ""], ["Sng", "Weicong", ""], ["Taunyazov", "Tasbolat", ""], ["Soh", "Harold", ""]]}, {"id": "2008.08055", "submitter": "Amir Alansary", "authors": "Guy Leroy, Daniel Rueckert, and Amir Alansary", "title": "Communicative Reinforcement Learning Agents for Landmark Detection in\n  Brain Images", "comments": "Accepted for the MLCN workshop, MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate detection of anatomical landmarks is an essential step in several\nmedical imaging tasks. We propose a novel communicative multi-agent\nreinforcement learning (C-MARL) system to automatically detect landmarks in 3D\nbrain images. C-MARL enables the agents to learn explicit communication\nchannels, as well as implicit communication signals by sharing certain weights\nof the architecture among all the agents. The proposed approach is evaluated on\ntwo brain imaging datasets from adult magnetic resonance imaging (MRI) and\nfetal ultrasound scans. Our experiments show that involving multiple\ncooperating agents by learning their communication with each other outperforms\nprevious approaches using single agents.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 17:36:56 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 22:11:48 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Leroy", "Guy", ""], ["Rueckert", "Daniel", ""], ["Alansary", "Amir", ""]]}, {"id": "2008.08063", "submitter": "Xinshuo Weng", "authors": "Xinshuo Weng, Jianren Wang, David Held, Kris Kitani", "title": "AB3DMOT: A Baseline for 3D Multi-Object Tracking and New Evaluation\n  Metrics", "comments": "Extended abstract that will be presented at ECCV 2020 Workshops.\n  Project website: http://www.xinshuoweng.com/projects/AB3DMOT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D multi-object tracking (MOT) is essential to applications such as\nautonomous driving. Recent work focuses on developing accurate systems giving\nless attention to computational cost and system complexity. In contrast, this\nwork proposes a simple real-time 3D MOT system with strong performance. Our\nsystem first obtains 3D detections from a LiDAR point cloud. Then, a\nstraightforward combination of a 3D Kalman filter and the Hungarian algorithm\nis used for state estimation and data association. Additionally, 3D MOT\ndatasets such as KITTI evaluate MOT methods in 2D space and standardized 3D MOT\nevaluation tools are missing for a fair comparison of 3D MOT methods. We\npropose a new 3D MOT evaluation tool along with three new metrics to\ncomprehensively evaluate 3D MOT methods. We show that, our proposed method\nachieves strong 3D MOT performance on KITTI and runs at a rate of $207.4$ FPS\non the KITTI dataset, achieving the fastest speed among modern 3D MOT systems.\nOur code is publicly available at http://www.xinshuoweng.com/projects/AB3DMOT.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 17:45:56 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Weng", "Xinshuo", ""], ["Wang", "Jianren", ""], ["Held", "David", ""], ["Kitani", "Kris", ""]]}, {"id": "2008.08072", "submitter": "Michael S. Ryoo", "authors": "Michael S. Ryoo, AJ Piergiovanni, Juhana Kangaspunta, Anelia Angelova", "title": "AssembleNet++: Assembling Modality Representations via Attention\n  Connections", "comments": "ECCV 2020 camera-ready version", "journal-ref": "ECCV 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We create a family of powerful video models which are able to: (i) learn\ninteractions between semantic object information and raw appearance and motion\nfeatures, and (ii) deploy attention in order to better learn the importance of\nfeatures at each convolutional block of the network. A new network component\nnamed peer-attention is introduced, which dynamically learns the attention\nweights using another block or input modality. Even without pre-training, our\nmodels outperform the previous work on standard public activity recognition\ndatasets with continuous videos, establishing new state-of-the-art. We also\nconfirm that our findings of having neural connections from the object modality\nand the use of peer-attention is generally applicable for different existing\narchitectures, improving their performances. We name our model explicitly as\nAssembleNet++. The code will be available at:\nhttps://sites.google.com/corp/view/assemblenet/\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 17:54:08 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Ryoo", "Michael S.", ""], ["Piergiovanni", "AJ", ""], ["Kangaspunta", "Juhana", ""], ["Angelova", "Anelia", ""]]}, {"id": "2008.08115", "submitter": "Daniel Bolya", "authors": "Daniel Bolya, Sean Foley, James Hays, Judy Hoffman", "title": "TIDE: A General Toolbox for Identifying Object Detection Errors", "comments": "Updated LVIS results with the v1.0.1 error calculation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce TIDE, a framework and associated toolbox for analyzing the\nsources of error in object detection and instance segmentation algorithms.\nImportantly, our framework is applicable across datasets and can be applied\ndirectly to output prediction files without required knowledge of the\nunderlying prediction system. Thus, our framework can be used as a drop-in\nreplacement for the standard mAP computation while providing a comprehensive\nanalysis of each model's strengths and weaknesses. We segment errors into six\ntypes and, crucially, are the first to introduce a technique for measuring the\ncontribution of each error in a way that isolates its effect on overall\nperformance. We show that such a representation is critical for drawing\naccurate, comprehensive conclusions through in-depth analysis across 4 datasets\nand 7 recognition models. Available at https://dbolya.github.io/tide/\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 18:30:53 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 19:06:51 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Bolya", "Daniel", ""], ["Foley", "Sean", ""], ["Hays", "James", ""], ["Hoffman", "Judy", ""]]}, {"id": "2008.08136", "submitter": "Ramy Battrawy", "authors": "Rishav, Ramy Battrawy, Ren\\'e Schuster, Oliver Wasenm\\\"uller and\n  Didier Stricker", "title": "DeepLiDARFlow: A Deep Learning Architecture For Scene Flow Estimation\n  Using Monocular Camera and Sparse LiDAR", "comments": "This paper is accepted to IROS2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene flow is the dense 3D reconstruction of motion and geometry of a scene.\nMost state-of-the-art methods use a pair of stereo images as input for full\nscene reconstruction. These methods depend a lot on the quality of the RGB\nimages and perform poorly in regions with reflective objects, shadows,\nill-conditioned light environment and so on. LiDAR measurements are much less\nsensitive to the aforementioned conditions but LiDAR features are in general\nunsuitable for matching tasks due to their sparse nature. Hence, using both\nLiDAR and RGB can potentially overcome the individual disadvantages of each\nsensor by mutual improvement and yield robust features which can improve the\nmatching process. In this paper, we present DeepLiDARFlow, a novel deep\nlearning architecture which fuses high level RGB and LiDAR features at multiple\nscales in a monocular setup to predict dense scene flow. Its performance is\nmuch better in the critical regions where image-only and LiDAR-only methods are\ninaccurate. We verify our DeepLiDARFlow using the established data sets KITTI\nand FlyingThings3D and we show strong robustness compared to several\nstate-of-the-art methods which used other input modalities. The code of our\npaper is available at https://github.com/dfki-av/DeepLiDARFlow.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 19:51:08 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Rishav", "", ""], ["Battrawy", "Ramy", ""], ["Schuster", "Ren\u00e9", ""], ["Wasenm\u00fcller", "Oliver", ""], ["Stricker", "Didier", ""]]}, {"id": "2008.08143", "submitter": "Amanda Duarte", "authors": "Amanda Duarte, Shruti Palaskar, Lucas Ventura, Deepti Ghadiyaram,\n  Kenneth DeHaan, Florian Metze, Jordi Torres and Xavier Giro-i-Nieto", "title": "How2Sign: A Large-scale Multimodal Dataset for Continuous American Sign\n  Language", "comments": "Accepted at CVPR 2021. Dataset website: http://how2sign.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the factors that have hindered progress in the areas of sign language\nrecognition, translation, and production is the absence of large annotated\ndatasets. Towards this end, we introduce How2Sign, a multimodal and multiview\ncontinuous American Sign Language (ASL) dataset, consisting of a parallel\ncorpus of more than 80 hours of sign language videos and a set of corresponding\nmodalities including speech, English transcripts, and depth. A three-hour\nsubset was further recorded in the Panoptic studio enabling detailed 3D pose\nestimation. To evaluate the potential of How2Sign for real-world impact, we\nconduct a study with ASL signers and show that synthesized videos using our\ndataset can indeed be understood. The study further gives insights on\nchallenges that computer vision should address in order to make progress in\nthis field.\n  Dataset website: http://how2sign.github.io/\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 20:22:16 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 16:54:42 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Duarte", "Amanda", ""], ["Palaskar", "Shruti", ""], ["Ventura", "Lucas", ""], ["Ghadiyaram", "Deepti", ""], ["DeHaan", "Kenneth", ""], ["Metze", "Florian", ""], ["Torres", "Jordi", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "2008.08145", "submitter": "Xu Chen", "authors": "Xu Chen, Zijian Dong, Jie Song, Andreas Geiger, Otmar Hilliges", "title": "Category Level Object Pose Estimation via Neural Analysis-by-Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many object pose estimation algorithms rely on the analysis-by-synthesis\nframework which requires explicit representations of individual object\ninstances. In this paper we combine a gradient-based fitting procedure with a\nparametric neural image synthesis module that is capable of implicitly\nrepresenting the appearance, shape and pose of entire object categories, thus\nrendering the need for explicit CAD models per object instance unnecessary. The\nimage synthesis network is designed to efficiently span the pose configuration\nspace so that model capacity can be used to capture the shape and local\nappearance (i.e., texture) variations jointly. At inference time the\nsynthesized images are compared to the target via an appearance based loss and\nthe error signal is backpropagated through the network to the input parameters.\nKeeping the network parameters fixed, this allows for iterative optimization of\nthe object pose, shape and appearance in a joint manner and we experimentally\nshow that the method can recover orientation of objects with high accuracy from\n2D images alone. When provided with depth measurements, to overcome scale\nambiguities, the method can accurately recover the full 6DOF pose successfully.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 20:30:47 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Chen", "Xu", ""], ["Dong", "Zijian", ""], ["Song", "Jie", ""], ["Geiger", "Andreas", ""], ["Hilliges", "Otmar", ""]]}, {"id": "2008.08148", "submitter": "Saket Dingliwal", "authors": "Hai Pham, Amrith Setlur, Saket Dingliwal, Tzu-Hsiang Lin, Barnabas\n  Poczos, Kang Huang, Zhuo Li, Jae Lim, Collin McCormack, Tam Vu", "title": "Robust Handwriting Recognition with Limited and Noisy Data", "comments": "icfhr2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the advent of deep learning in computer vision, the general\nhandwriting recognition problem is far from solved. Most existing approaches\nfocus on handwriting datasets that have clearly written text and carefully\nsegmented labels. In this paper, we instead focus on learning handwritten\ncharacters from maintenance logs, a constrained setting where data is very\nlimited and noisy. We break the problem into two consecutive stages of word\nsegmentation and word recognition respectively and utilize data augmentation\ntechniques to train both stages. Extensive comparisons with popular baselines\nfor scene-text detection and word recognition show that our system achieves a\nlower error rate and is more suited to handle noisy and difficult documents\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 20:33:23 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Pham", "Hai", ""], ["Setlur", "Amrith", ""], ["Dingliwal", "Saket", ""], ["Lin", "Tzu-Hsiang", ""], ["Poczos", "Barnabas", ""], ["Huang", "Kang", ""], ["Li", "Zhuo", ""], ["Lim", "Jae", ""], ["McCormack", "Collin", ""], ["Vu", "Tam", ""]]}, {"id": "2008.08170", "submitter": "Feihu Huang", "authors": "Feihu Huang, Shangqian Gao, Jian Pei, Heng Huang", "title": "Accelerated Zeroth-Order and First-Order Momentum Methods from Mini to\n  Minimax Optimization", "comments": "66 pages. In this version, we change the Lyapunov functions for our\n  Acc-ZOMDA and Acc-MDA methods in the convergence analysis. Then our Acc-ZOMDA\n  method obtains a lower query complexity and our Acc-MDA method achieves a\n  lower gradient complexity", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper, we propose a class of accelerated zeroth-order and first-order\nmomentum methods for both nonconvex mini-optimization and minimax-optimization.\nSpecifically, we propose a new accelerated zeroth-order momentum (Acc-ZOM)\nmethod to solve stochastic mini-optimization problems. We prove that the\nAcc-ZOM method achieves a lower query complexity of\n$\\tilde{O}(d^{3/4}\\epsilon^{-3})$ for finding an $\\epsilon$-stationary point,\nwhich improves the best known result by a factor of $O(d^{1/4})$ where $d$\ndenotes the parameter dimension. In particular, the Acc-ZOM does not require\nlarge batches required in the existing zeroth-order stochastic algorithms. At\nthe same time, we propose an accelerated zeroth-order momentum descent ascent\n(Acc-ZOMDA) method for black-box minimax-optimization. We prove that the\nAcc-ZOMDA method reaches a lower query complexity of\n$\\tilde{O}((d_1+d_2)^{9/10}\\kappa_y^{3}\\epsilon^{-3})$ for finding an\n$\\epsilon$-stationary point, which improves the best known result by a factor\nof $O((d_1+d_2)^{1/10})$ where $d_1$ and $d_2$ denote dimensions of\noptimization parameters and $\\kappa_y$ is condition number. Moreover, we\npropose an accelerated first-order momentum descent ascent (Acc-MDA) method for\nsolving white-box minimax problems, and prove that it achieves a lower gradient\ncomplexity of $\\tilde{O}(\\kappa_y^{(3-\\nu/2)}\\epsilon^{-3})$ with $\\nu>0$ for\nfinding an $\\epsilon$-stationary point, which improves the best known result by\na factor of $O(\\kappa_y^{\\nu/2})$. Extensive experimental results on the\nblack-box adversarial attack to deep neural networks (DNNs) and poisoning\nattack demonstrate the efficiency of our algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 22:19:29 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 21:48:49 GMT"}, {"version": "v3", "created": "Mon, 1 Mar 2021 02:33:46 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Huang", "Feihu", ""], ["Gao", "Shangqian", ""], ["Pei", "Jian", ""], ["Huang", "Heng", ""]]}, {"id": "2008.08171", "submitter": "Jiaman Li", "authors": "Jiaman Li, Yihang Yin, Hang Chu, Yi Zhou, Tingwu Wang, Sanja Fidler,\n  Hao Li", "title": "Learning to Generate Diverse Dance Motions with Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ongoing pandemic, virtual concerts and live events using digitized\nperformances of musicians are getting traction on massive multiplayer online\nworlds. However, well choreographed dance movements are extremely complex to\nanimate and would involve an expensive and tedious production process. In\naddition to the use of complex motion capture systems, it typically requires a\ncollaborative effort between animators, dancers, and choreographers. We\nintroduce a complete system for dance motion synthesis, which can generate\ncomplex and highly diverse dance sequences given an input music sequence. As\nmotion capture data is limited for the range of dance motions and styles, we\nintroduce a massive dance motion data set that is created from YouTube videos.\nWe also present a novel two-stream motion transformer generative model, which\ncan generate motion sequences with high flexibility. We also introduce new\nevaluation metrics for the quality of synthesized dance motions, and\ndemonstrate that our system can outperform state-of-the-art methods. Our system\nprovides high-quality animations suitable for large crowds for virtual concerts\nand can also be used as reference for professional animation pipelines. Most\nimportantly, we show that vast online videos can be effective in training dance\nmotion models.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 22:29:40 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Li", "Jiaman", ""], ["Yin", "Yihang", ""], ["Chu", "Hang", ""], ["Zhou", "Yi", ""], ["Wang", "Tingwu", ""], ["Fidler", "Sanja", ""], ["Li", "Hao", ""]]}, {"id": "2008.08173", "submitter": "Jianren Wang", "authors": "Jianren Wang, Siddharth Ancha, Yi-Ting Chen, David Held", "title": "Uncertainty-aware Self-supervised 3D Data Association", "comments": null, "journal-ref": "2020 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object trackers usually require training on large amounts of annotated\ndata that is expensive and time-consuming to collect. Instead, we propose\nleveraging vast unlabeled datasets by self-supervised metric learning of 3D\nobject trackers, with a focus on data association. Large scale annotations for\nunlabeled data are cheaply obtained by automatic object detection and\nassociation across frames. We show how these self-supervised annotations can be\nused in a principled manner to learn point-cloud embeddings that are effective\nfor 3D tracking. We estimate and incorporate uncertainty in self-supervised\ntracking to learn more robust embeddings, without needing any labeled data. We\ndesign embeddings to differentiate objects across frames, and learn them using\nuncertainty-aware self-supervised training. Finally, we demonstrate their\nability to perform accurate data association across frames, towards effective\nand accurate 3D tracking. Project videos and code are at\nhttps://jianrenw.github.io/Self-Supervised-3D-Data-Association.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 22:34:07 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Wang", "Jianren", ""], ["Ancha", "Siddharth", ""], ["Chen", "Yi-Ting", ""], ["Held", "David", ""]]}, {"id": "2008.08178", "submitter": "Grace Chu", "authors": "Grace Chu, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton,\n  Pieter-Jan Kindermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, Andrew Howard", "title": "Discovering Multi-Hardware Mobile Models via Architecture Search", "comments": "CVPR Workshop 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware-aware neural architecture designs have been predominantly focusing\non optimizing model performance on single hardware and model development\ncomplexity, where another important factor, model deployment complexity, has\nbeen largely ignored. In this paper, we argue that, for applications that may\nbe deployed on multiple hardware, having different single-hardware models\nacross the deployed hardware makes it hard to guarantee consistent outputs\nacross hardware and duplicates engineering work for debugging and fixing. To\nminimize such deployment cost, we propose an alternative solution,\nmulti-hardware models, where a single architecture is developed for multiple\nhardware. With thoughtful search space design and incorporating the proposed\nmulti-hardware metrics in neural architecture search, we discover\nmulti-hardware models that give state-of-the-art (SoTA) performance across\nmultiple hardware in both average and worse case scenarios. For performance on\nindividual hardware, the single multi-hardware model yields similar or better\nresults than SoTA performance on accelerators like GPU, DSP and EdgeTPU which\nwas achieved by different models, while having similar performance with\nMobilenetV3 Large Minimalistic model on mobile CPU.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 22:58:17 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 20:42:27 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Chu", "Grace", ""], ["Arikan", "Okan", ""], ["Bender", "Gabriel", ""], ["Wang", "Weijun", ""], ["Brighton", "Achille", ""], ["Kindermans", "Pieter-Jan", ""], ["Liu", "Hanxiao", ""], ["Akin", "Berkin", ""], ["Gupta", "Suyog", ""], ["Howard", "Andrew", ""]]}, {"id": "2008.08186", "submitter": "Xiaoyan Han", "authors": "Vardan Papyan, X.Y. Han, David L. Donoho", "title": "Prevalence of Neural Collapse during the terminal phase of deep learning\n  training", "comments": null, "journal-ref": null, "doi": "10.1073/pnas.2015509117", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern practice for training classification deepnets involves a Terminal\nPhase of Training (TPT), which begins at the epoch where training error first\nvanishes; During TPT, the training error stays effectively zero while training\nloss is pushed towards zero. Direct measurements of TPT, for three prototypical\ndeepnet architectures and across seven canonical classification datasets,\nexpose a pervasive inductive bias we call Neural Collapse, involving four\ndeeply interconnected phenomena: (NC1) Cross-example within-class variability\nof last-layer training activations collapses to zero, as the individual\nactivations themselves collapse to their class-means; (NC2) The class-means\ncollapse to the vertices of a Simplex Equiangular Tight Frame (ETF); (NC3) Up\nto rescaling, the last-layer classifiers collapse to the class-means, or in\nother words to the Simplex ETF, i.e. to a self-dual configuration; (NC4) For a\ngiven activation, the classifier's decision collapses to simply choosing\nwhichever class has the closest train class-mean, i.e. the Nearest Class Center\n(NCC) decision rule. The symmetric and very simple geometry induced by the TPT\nconfers important benefits, including better generalization performance, better\nrobustness, and better interpretability.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 23:12:54 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 16:15:50 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Papyan", "Vardan", ""], ["Han", "X. Y.", ""], ["Donoho", "David L.", ""]]}, {"id": "2008.08189", "submitter": "Xuewen Yang", "authors": "Xuewen Yang, Dongliang Xie, Xin Wang, Jiangbo Yuan, Wanying Ding,\n  Pengyun Yan", "title": "Learning Tuple Compatibility for Conditional OutfitRecommendation", "comments": null, "journal-ref": "ACM Multimedia 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outfit recommendation requires the answers of some challenging outfit\ncompatibility questions such as 'Which pair of boots and school bag go well\nwith my jeans and sweater?'. It is more complicated than conventional\nsimilarity search, and needs to consider not only visual aesthetics but also\nthe intrinsic fine-grained and multi-category nature of fashion items. Some\nexisting approaches solve the problem through sequential models or learning\npair-wise distances between items. However, most of them only consider coarse\ncategory information in defining fashion compatibility while neglecting the\nfine-grained category information often desired in practical applications. To\nbetter define the fashion compatibility and more flexibly meet different needs,\nwe propose a novel problem of learning compatibility among multiple tuples\n(each consisting of an item and category pair), and recommending fashion items\nfollowing the category choices from customers. Our contributions include: 1)\nDesigning a Mixed Category Attention Net (MCAN) which integrates both\nfine-grained and coarse category information into recommendation and learns the\ncompatibility among fashion tuples. MCAN can explicitly and effectively\ngenerate diverse and controllable recommendations based on need. 2)\nContributing a new dataset IQON, which follows eastern culture and can be used\nto test the generalization of recommendation systems. Our extensive experiments\non a reference dataset Polyvore and our dataset IQON demonstrate that our\nmethod significantly outperforms state-of-the-art recommendation methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 23:22:16 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Yang", "Xuewen", ""], ["Xie", "Dongliang", ""], ["Wang", "Xin", ""], ["Yuan", "Jiangbo", ""], ["Ding", "Wanying", ""], ["Yan", "Pengyun", ""]]}, {"id": "2008.08194", "submitter": "Meng Ye", "authors": "Meng Ye, Qiaoying Huang, Dong Yang, Pengxiang Wu, Jingru Yi, Leon\n  Axel, Dimitris Metaxas", "title": "PC-U Net: Learning to Jointly Reconstruct and Segment the Cardiac Walls\n  in 3D from CT Data", "comments": null, "journal-ref": "STACOM 2020", "doi": "10.1007/978-3-030-68107-4_12", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 3D volumetric shape of the heart's left ventricle (LV) myocardium (MYO)\nwall provides important information for diagnosis of cardiac disease and\ninvasive procedure navigation. Many cardiac image segmentation methods have\nrelied on detection of region-of-interest as a pre-requisite for shape\nsegmentation and modeling. With segmentation results, a 3D surface mesh and a\ncorresponding point cloud of the segmented cardiac volume can be reconstructed\nfor further analyses. Although state-of-the-art methods (e.g., U-Net) have\nachieved decent performance on cardiac image segmentation in terms of accuracy,\nthese segmentation results can still suffer from imaging artifacts and noise,\nwhich will lead to inaccurate shape modeling results. In this paper, we propose\na PC-U net that jointly reconstructs the point cloud of the LV MYO wall\ndirectly from volumes of 2D CT slices and generates its segmentation masks from\nthe predicted 3D point cloud. Extensive experimental results show that by\nincorporating a shape prior from the point cloud, the segmentation masks are\nmore accurate than the state-of-the-art U-Net results in terms of Dice's\ncoefficient and Hausdorff distance.The proposed joint learning framework of our\nPC-U net is beneficial for automatic cardiac image analysis tasks because it\ncan obtain simultaneously the 3D shape and segmentation of the LV MYO walls.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 23:37:05 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Ye", "Meng", ""], ["Huang", "Qiaoying", ""], ["Yang", "Dong", ""], ["Wu", "Pengxiang", ""], ["Yi", "Jingru", ""], ["Axel", "Leon", ""], ["Metaxas", "Dimitris", ""]]}, {"id": "2008.08213", "submitter": "Gyeongsik Moon", "authors": "Gyeongsik Moon, Takaaki Shiratori, Kyoung Mu Lee", "title": "DeepHandMesh: A Weakly-supervised Deep Encoder-Decoder Framework for\n  High-fidelity Hand Mesh Modeling", "comments": "Published at ECCV 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human hands play a central role in interacting with other people and objects.\nFor realistic replication of such hand motions, high-fidelity hand meshes have\nto be reconstructed. In this study, we firstly propose DeepHandMesh, a\nweakly-supervised deep encoder-decoder framework for high-fidelity hand mesh\nmodeling. We design our system to be trained in an end-to-end and\nweakly-supervised manner; therefore, it does not require groundtruth meshes.\nInstead, it relies on weaker supervisions such as 3D joint coordinates and\nmulti-view depth maps, which are easier to get than groundtruth meshes and do\nnot dependent on the mesh topology. Although the proposed DeepHandMesh is\ntrained in a weakly-supervised way, it provides significantly more realistic\nhand mesh than previous fully-supervised hand models. Our newly introduced\npenetration avoidance loss further improves results by replicating physical\ninteraction between hand parts. Finally, we demonstrate that our system can\nalso be applied successfully to the 3D hand mesh estimation from general\nimages. Our hand model, dataset, and codes are publicly available at\nhttps://mks0601.github.io/DeepHandMesh/.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 00:59:51 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Moon", "Gyeongsik", ""], ["Shiratori", "Takaaki", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "2008.08218", "submitter": "Xiaoyu Zhang", "authors": "Xiaoyu Zhang, Wei Wang, Xianyu Qi and Ziwei Liao", "title": "Stereo Plane SLAM Based on Intersecting Lines", "comments": "Accepted at IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plane feature is a kind of stable landmark to reduce drift error in SLAM\nsystem. It is easy and fast to extract planes from dense point cloud, which is\ncommonly acquired from RGB-D camera or lidar. But for stereo camera, it is hard\nto compute dense point cloud accurately and efficiently. In this paper, we\npropose a novel method to compute plane parameters using intersecting lines\nwhich are extracted from the stereo image. The plane features commonly exist on\nthe surface of man-made objects and structure, which have regular shape and\nstraight edge lines. In 3D space, two intersecting lines can determine such a\nplane. Thus we extract line segments from both stereo left and right image. By\nstereo matching, we compute the endpoints and line directions in 3D space, and\nthen the planes from two intersecting lines. We discard those inaccurate plane\nfeatures in the frame tracking. Adding such plane features in stereo SLAM\nsystem reduces the drift error and refines the performance. We test our\nproposed system on public datasets and demonstrate its robust and accurate\nestimation results, compared with state-of-the-art SLAM systems. To benefit the\nresearch of plane-based SLAM, we release our codes at\nhttps://github.com/fishmarch/Stereo-Plane-SLAM.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 01:35:49 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 07:17:56 GMT"}, {"version": "v3", "created": "Thu, 29 Jul 2021 02:25:33 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Zhang", "Xiaoyu", ""], ["Wang", "Wei", ""], ["Qi", "Xianyu", ""], ["Liao", "Ziwei", ""]]}, {"id": "2008.08220", "submitter": "Zhaoyuan Fang", "authors": "Zhaoyuan Fang, Adam Czajka", "title": "Open Source Iris Recognition Hardware and Software with Presentation\n  Attack Detection", "comments": "Accepted to IJCB 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the first known to us open source hardware and software\niris recognition system with presentation attack detection (PAD), which can be\neasily assembled for about 75 USD using Raspberry Pi board and a few\nperipherals. The primary goal of this work is to offer a low-cost baseline for\nspoof-resistant iris recognition, which may (a) stimulate research in iris PAD\nand allow for easy prototyping of secure iris recognition systems, (b) offer a\nlow-cost secure iris recognition alternative to more sophisticated systems, and\n(c) serve as an educational platform. We propose a lightweight image\ncomplexity-guided convolutional network for fast and accurate iris\nsegmentation, domain-specific human-inspired Binarized Statistical Image\nFeatures (BSIF) to build an iris template, and to combine 2D (iris texture) and\n3D (photometric stereo-based) features for PAD. The proposed iris recognition\nruns in about 3.2 seconds and the proposed PAD runs in about 4.5 seconds on\nRaspberry Pi 3B+. The hardware specifications and all source codes of the\nentire pipeline are made available along with this paper.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 02:02:16 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Fang", "Zhaoyuan", ""], ["Czajka", "Adam", ""]]}, {"id": "2008.08242", "submitter": "Jianzhao Liu", "authors": "Jianzhao Liu, Jianxin Lin, Xin Li, Wei Zhou, Sen Liu, Zhibo Chen", "title": "LIRA: Lifelong Image Restoration from Unknown Blended Distortions", "comments": "ECCV2020 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing image restoration networks are designed in a disposable way and\ncatastrophically forget previously learned distortions when trained on a new\ndistortion removal task. To alleviate this problem, we raise the novel lifelong\nimage restoration problem for blended distortions. We first design a base\nfork-join model in which multiple pre-trained expert models specializing in\nindividual distortion removal task work cooperatively and adaptively to handle\nblended distortions. When the input is degraded by a new distortion, inspired\nby adult neurogenesis in human memory system, we develop a neural growing\nstrategy where the previously trained model can incorporate a new expert branch\nand continually accumulate new knowledge without interfering with learned\nknowledge. Experimental results show that the proposed approach can not only\nachieve state-of-the-art performance on blended distortions removal tasks in\nboth PSNR/SSIM metrics, but also maintain old expertise while learning new\nrestoration tasks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 03:35:45 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Liu", "Jianzhao", ""], ["Lin", "Jianxin", ""], ["Li", "Xin", ""], ["Zhou", "Wei", ""], ["Liu", "Sen", ""], ["Chen", "Zhibo", ""]]}, {"id": "2008.08246", "submitter": "Jialun Pei", "authors": "Jialun Pei, He Tang, Tianyang Cheng, Chuanbo Chen", "title": "Salient Instance Segmentation with Region and Box-level Annotations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salient instance segmentation is a new challenging task that received\nwidespread attention in the saliency detection area. The new generation of\nsaliency detection provides a strong theoretical and technical basis for video\nsurveillance. Due to the limited scale of the existing dataset and the high\nmask annotations cost, plenty of supervision source is urgently needed to train\na well-performing salient instance model. In this paper, we aim to train a\nnovel salient instance segmentation framework by an inexact supervision without\nresorting to laborious labeling. To this end, we present a cyclic global\ncontext salient instance segmentation network (CGCNet), which is supervised by\nthe combination of salient regions and bounding boxes from the ready-made\nsalient object detection datasets. To locate salient instance more accurately,\na global feature refining layer is proposed that dilates the features of the\nregion of interest (ROI) to the global context in a scene. Meanwhile, a\nlabeling updating scheme is embedded in the proposed framework to update the\ncoarse-grained labels for next iteration. Experiment results demonstrate that\nthe proposed end-to-end framework trained by inexact supervised annotations can\nbe competitive to the existing fully supervised salient instance segmentation\nmethods. Without bells and whistles, our proposed method achieves a mask AP of\n58.3% in the test set of Dataset1K that outperforms the mainstream\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 03:43:45 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 06:41:52 GMT"}, {"version": "v3", "created": "Wed, 28 Apr 2021 07:38:49 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Pei", "Jialun", ""], ["Tang", "He", ""], ["Cheng", "Tianyang", ""], ["Chen", "Chuanbo", ""]]}, {"id": "2008.08248", "submitter": "Qiaoying Huang", "authors": "Qiaoying Huang, Dong Yang, Yikun Xian, Pengxiang Wu, Jingru Yi, Hui\n  Qu, Dimitris Metaxas", "title": "Enhanced MRI Reconstruction Network using Neural Architecture Search", "comments": "10 pages. Code will be released soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accurate reconstruction of under-sampled magnetic resonance imaging (MRI)\ndata using modern deep learning technology, requires significant effort to\ndesign the necessary complex neural network architectures. The cascaded network\narchitecture for MRI reconstruction has been widely used, while it suffers from\nthe \"vanishing gradient\" problem when the network becomes deep. In addition,\nhomogeneous architecture degrades the representation capacity of the network.\nIn this work, we present an enhanced MRI reconstruction network using a\nresidual in residual basic block. For each cell in the basic block, we use the\ndifferentiable neural architecture search (NAS) technique to automatically\nchoose the optimal operation among eight variants of the dense block. This new\nheterogeneous network is evaluated on two publicly available datasets and\noutperforms all current state-of-the-art methods, which demonstrates the\neffectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 03:44:31 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Huang", "Qiaoying", ""], ["Yang", "Dong", ""], ["Xian", "Yikun", ""], ["Wu", "Pengxiang", ""], ["Yi", "Jingru", ""], ["Qu", "Hui", ""], ["Metaxas", "Dimitris", ""]]}, {"id": "2008.08250", "submitter": "Ke-Yue Zhang", "authors": "Ke-Yue Zhang, Taiping Yao, Jian Zhang, Ying Tai, Shouhong Ding, Jilin\n  Li, Feiyue Huang, Haichuan Song, Lizhuang Ma", "title": "Face Anti-Spoofing Via Disentangled Representation Learning", "comments": "To appear in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face anti-spoofing is crucial to security of face recognition systems.\nPrevious approaches focus on developing discriminative models based on the\nfeatures extracted from images, which may be still entangled between spoof\npatterns and real persons. In this paper, motivated by the disentangled\nrepresentation learning, we propose a novel perspective of face anti-spoofing\nthat disentangles the liveness features and content features from images, and\nthe liveness features is further used for classification. We also put forward a\nConvolutional Neural Network (CNN) architecture with the process of\ndisentanglement and combination of low-level and high-level supervision to\nimprove the generalization capabilities. We evaluate our method on public\nbenchmark datasets and extensive experimental results demonstrate the\neffectiveness of our method against the state-of-the-art competitors. Finally,\nwe further visualize some results to help understand the effect and advantage\nof disentanglement.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 03:54:23 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Zhang", "Ke-Yue", ""], ["Yao", "Taiping", ""], ["Zhang", "Jian", ""], ["Tai", "Ying", ""], ["Ding", "Shouhong", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""], ["Song", "Haichuan", ""], ["Ma", "Lizhuang", ""]]}, {"id": "2008.08251", "submitter": "Arjun Sharma", "authors": "Arjun Sharma, Adway Mitra, Vishal Vasan, Rama Govindarajan", "title": "Spatio-temporal relationships between rainfall and convective clouds\n  during Indian Monsoon through a discrete lens", "comments": "20 pages, 13 figures, 5 tables. Abstract slightly modified from the\n  accepted version at International Journal of Climatology", "journal-ref": "International Journal of Climatology, 2020", "doi": "10.1002/joc.6812", "report-no": null, "categories": "physics.ao-ph cs.CV physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Indian monsoon, a multi-variable process causing heavy rains during\nJune-September every year, is very heterogeneous in space and time. We study\nthe relationship between rainfall and Outgoing Longwave Radiation (OLR,\nconvective cloud cover) for monsoon between 2004-2010. To identify, classify\nand visualize spatial patterns of rainfall and OLR we use a discrete and\nspatio-temporally coherent representation of the data, created using a\nstatistical model based on Markov Random Field. Our approach clusters the days\nwith similar spatial distributions of rainfall and OLR into a small number of\nspatial patterns. We find that eight daily spatial patterns each in rainfall\nand OLR, and seven joint patterns of rainfall and OLR, describe over 90\\% of\nall days. Through these patterns, we find that OLR generally has a strong\nnegative correlation with precipitation, but with significant spatial\nvariations. In particular, peninsular India (except west coast) is under\nsignificant convective cloud cover over a majority of days but remains\nrainless. We also find that much of the monsoon rainfall co-occurs with low\nOLR, but some amount of rainfall in Eastern and North-western India in June\noccurs on OLR days, presumably from shallow clouds. To study day-to-day\nvariations of both quantities, we identify spatial patterns in the temporal\ngradients computed from the observations. We find that changes in convective\ncloud activity across India most commonly occur due to the establishment of a\nnorth-south OLR gradient which persists for 1-2 days and shifts the convective\ncloud cover from light to deep or vice versa. Such changes are also accompanied\nby changes in the spatial distribution of precipitation. The present work thus\nprovides a highly reduced description of the complex spatial patterns and their\nday-to-day variations, and could form a useful tool for future simplified\ndescriptions of this process.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 04:02:39 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Sharma", "Arjun", ""], ["Mitra", "Adway", ""], ["Vasan", "Vishal", ""], ["Govindarajan", "Rama", ""]]}, {"id": "2008.08255", "submitter": "Hao Liu", "authors": "Hao Liu, Xue-Cheng Tai, Ron Kimmel, Roland Glowinski", "title": "A Color Elastica Model for Vector-Valued Image Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models related to the Euler's elastica energy have proven to be useful for\nmany applications including image processing. Extending elastica models to\ncolor images and multi-channel data is a challenging task, as stable and\nconsistent numerical solvers for these geometric models often involve high\norder derivatives. Like the single channel Euler's elastica model and the total\nvariation (TV) models, geometric measures that involve high order derivatives\ncould help when considering image formation models that minimize elastic\nproperties. In the past, the Polyakov action from high energy physics has been\nsuccessfully applied to color image processing. Here, we introduce an addition\nto the Polyakov action for color images that minimizes the color manifold\ncurvature. The color image curvature is computed by applying of the\nLaplace-Beltrami operator to the color image channels. When reduced to\ngray-scale images, while selecting appropriate scaling between space and color,\nthe proposed model minimizes the Euler's elastica operating on the image level\nsets. Finding a minimizer for the proposed nonlinear geometric model is a\nchallenge we address in this paper. Specifically, we present an\noperator-splitting method to minimize the proposed functional. The\nnon-linearity is decoupled by introducing three vector-valued and matrix-valued\nvariables. The problem is then converted into solving for the steady state of\nan associated initial-value problem. The initial-value problem is time-split\ninto three fractional steps, such that each sub-problem has a closed form\nsolution, or can be solved by fast algorithms. The efficiency and robustness of\nthe proposed method are demonstrated by systematic numerical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 04:18:35 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 00:42:25 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Liu", "Hao", ""], ["Tai", "Xue-Cheng", ""], ["Kimmel", "Ron", ""], ["Glowinski", "Roland", ""]]}, {"id": "2008.08257", "submitter": "Zhu Zhang", "authors": "Zhu Zhang, Zhijie Lin, Zhou Zhao, Jieming Zhu and Xiuqiang He", "title": "Regularized Two-Branch Proposal Networks for Weakly-Supervised Moment\n  Retrieval in Videos", "comments": "ACM MM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video moment retrieval aims to localize the target moment in an video\naccording to the given sentence. The weak-supervised setting only provides the\nvideo-level sentence annotations during training. Most existing weak-supervised\nmethods apply a MIL-based framework to develop inter-sample confrontment, but\nignore the intra-sample confrontment between moments with semantically similar\ncontents. Thus, these methods fail to distinguish the target moment from\nplausible negative moments. In this paper, we propose a novel Regularized\nTwo-Branch Proposal Network to simultaneously consider the inter-sample and\nintra-sample confrontments. Concretely, we first devise a language-aware filter\nto generate an enhanced video stream and a suppressed video stream. We then\ndesign the sharable two-branch proposal module to generate positive proposals\nfrom the enhanced stream and plausible negative proposals from the suppressed\none for sufficient confrontment. Further, we apply the proposal regularization\nto stabilize the training process and improve model performance. The extensive\nexperiments show the effectiveness of our method. Our code is released at here.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 04:42:46 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Zhang", "Zhu", ""], ["Lin", "Zhijie", ""], ["Zhao", "Zhou", ""], ["Zhu", "Jieming", ""], ["He", "Xiuqiang", ""]]}, {"id": "2008.08261", "submitter": "Kun Yuan", "authors": "Kun Yuan, Quanquan Li, Jing Shao, Junjie Yan", "title": "Learning Connectivity of Neural Networks from a Topological Perspective", "comments": "17 pages, 5 figures, to be published in ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seeking effective neural networks is a critical and practical field in deep\nlearning. Besides designing the depth, type of convolution, normalization, and\nnonlinearities, the topological connectivity of neural networks is also\nimportant. Previous principles of rule-based modular design simplify the\ndifficulty of building an effective architecture, but constrain the possible\ntopologies in limited spaces. In this paper, we attempt to optimize the\nconnectivity in neural networks. We propose a topological perspective to\nrepresent a network into a complete graph for analysis, where nodes carry out\naggregation and transformation of features, and edges determine the flow of\ninformation. By assigning learnable parameters to the edges which reflect the\nmagnitude of connections, the learning process can be performed in a\ndifferentiable manner. We further attach auxiliary sparsity constraint to the\ndistribution of connectedness, which promotes the learned topology focus on\ncritical connections. This learning process is compatible with existing\nnetworks and owns adaptability to larger search spaces and different tasks.\nQuantitative results of experiments reflect the learned connectivity is\nsuperior to traditional rule-based ones, such as random, residual, and\ncomplete. In addition, it obtains significant improvements in image\nclassification and object detection without introducing excessive computation\nburden.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 04:53:31 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Yuan", "Kun", ""], ["Li", "Quanquan", ""], ["Shao", "Jing", ""], ["Yan", "Junjie", ""]]}, {"id": "2008.08278", "submitter": "Yaxiong Wang", "authors": "Yaxiong Wang, Yunchao Wei, Xueming Qian, Li Zhu, and Yi Yang", "title": "DONet: Dual Objective Networks for Skin Lesion Segmentation", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin lesion segmentation is a crucial step in the computer-aided diagnosis of\ndermoscopic images. In the last few years, deep learning based semantic\nsegmentation methods have significantly advanced the skin lesion segmentation\nresults. However, the current performance is still unsatisfactory due to some\nchallenging factors such as large variety of lesion scale and ambiguous\ndifference between lesion region and background. In this paper, we propose a\nsimple yet effective framework, named Dual Objective Networks (DONet), to\nimprove the skin lesion segmentation. Our DONet adopts two symmetric decoders\nto produce different predictions for approaching different objectives.\nConcretely, the two objectives are actually defined by different loss\nfunctions. In this way, the two decoders are encouraged to produce\ndifferentiated probability maps to match different optimization targets,\nresulting in complementary predictions accordingly. The complementary\ninformation learned by these two objectives are further aggregated together to\nmake the final prediction, by which the uncertainty existing in segmentation\nmaps can be significantly alleviated. Besides, to address the challenge of\nlarge variety of lesion scales and shapes in dermoscopic images, we\nadditionally propose a recurrent context encoding module (RCEM) to model the\ncomplex correlation among skin lesions, where the features with different scale\ncontexts are efficiently integrated to form a more robust representation.\nExtensive experiments on two popular benchmarks well demonstrate the\neffectiveness of the proposed DONet. In particular, our DONet achieves 0.881\nand 0.931 dice score on ISIC 2018 and $\\text{PH}^2$, respectively. Code will be\nmade public available.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 06:02:46 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Wang", "Yaxiong", ""], ["Wei", "Yunchao", ""], ["Qian", "Xueming", ""], ["Zhu", "Li", ""], ["Yang", "Yi", ""]]}, {"id": "2008.08281", "submitter": "Shengnan Hu", "authors": "Shengnan Hu, Yang Zhang, Sumit Laha, Ankit Sharma, Hassan Foroosh", "title": "CCA: Exploring the Possibility of Contextual Camouflage Attack on Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network based object detection hasbecome the cornerstone of many\nreal-world applications. Alongwith this success comes concerns about its\nvulnerability tomalicious attacks. To gain more insight into this issue, we\nproposea contextual camouflage attack (CCA for short) algorithm to in-fluence\nthe performance of object detectors. In this paper, we usean evolutionary\nsearch strategy and adversarial machine learningin interactions with a\nphoto-realistic simulated environment tofind camouflage patterns that are\neffective over a huge varietyof object locations, camera poses, and lighting\nconditions. Theproposed camouflages are validated effective to most of the\nstate-of-the-art object detectors.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 06:16:10 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Hu", "Shengnan", ""], ["Zhang", "Yang", ""], ["Laha", "Sumit", ""], ["Sharma", "Ankit", ""], ["Foroosh", "Hassan", ""]]}, {"id": "2008.08284", "submitter": "Qian Xu", "authors": "Xu Qian, Victor Li, Crews Darren", "title": "Channel-wise Hessian Aware trace-Weighted Quantization of Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Second-order information has proven to be very effective in determining the\nredundancy of neural network weights and activations. Recent paper proposes to\nuse Hessian traces of weights and activations for mixed-precision quantization\nand achieves state-of-the-art results. However, prior works only focus on\nselecting bits for each layer while the redundancy of different channels within\na layer also differ a lot. This is mainly because the complexity of determining\nbits for each channel is too high for original methods. Here, we introduce\nChannel-wise Hessian Aware trace-Weighted Quantization (CW-HAWQ). CW-HAWQ uses\nHessian trace to determine the relative sensitivity order of different channels\nof activations and weights. What's more, CW-HAWQ proposes to use deep\nReinforcement learning (DRL) Deep Deterministic Policy Gradient (DDPG)-based\nagent to find the optimal ratios of different quantization bits and assign bits\nto channels according to the Hessian trace order. The number of states in\nCW-HAWQ is much smaller compared with traditional AutoML based mix-precision\nmethods since we only need to search ratios for the quantization bits. Compare\nCW-HAWQ with state-of-the-art shows that we can achieve better results for\nmultiple networks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 06:34:56 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Qian", "Xu", ""], ["Li", "Victor", ""], ["Darren", "Crews", ""]]}, {"id": "2008.08290", "submitter": "Jiuniu Wang", "authors": "Wenjia Xu, Yongqin Xian, Jiuniu Wang, Bernt Schiele, Zeynep Akata", "title": "Attribute Prototype Network for Zero-Shot Learning", "comments": "NeurIPS 2020. The code is publicly available at\n  https://wenjiaxu.github.io/APN-ZSL/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From the beginning of zero-shot learning research, visual attributes have\nbeen shown to play an important role. In order to better transfer\nattribute-based knowledge from known to unknown classes, we argue that an image\nrepresentation with integrated attribute localization ability would be\nbeneficial for zero-shot learning. To this end, we propose a novel zero-shot\nrepresentation learning framework that jointly learns discriminative global and\nlocal features using only class-level attributes. While a visual-semantic\nembedding layer learns global features, local features are learned through an\nattribute prototype network that simultaneously regresses and decorrelates\nattributes from intermediate features. We show that our locality augmented\nimage representations achieve a new state-of-the-art on three zero-shot\nlearning benchmarks. As an additional benefit, our model points to the visual\nevidence of the attributes in an image, e.g. for the CUB dataset, confirming\nthe improved attribute localization ability of our image representation.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 06:46:35 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 09:46:00 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2021 02:26:42 GMT"}, {"version": "v4", "created": "Thu, 6 May 2021 09:13:08 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Xu", "Wenjia", ""], ["Xian", "Yongqin", ""], ["Wang", "Jiuniu", ""], ["Schiele", "Bernt", ""], ["Akata", "Zeynep", ""]]}, {"id": "2008.08294", "submitter": "Hang Zhao", "authors": "Hang Zhao, Jiyang Gao, Tian Lan, Chen Sun, Benjamin Sapp, Balakrishnan\n  Varadarajan, Yue Shen, Yi Shen, Yuning Chai, Cordelia Schmid, Congcong Li,\n  Dragomir Anguelov", "title": "TNT: Target-driveN Trajectory Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the future behavior of moving agents is essential for real world\napplications. It is challenging as the intent of the agent and the\ncorresponding behavior is unknown and intrinsically multimodal. Our key insight\nis that for prediction within a moderate time horizon, the future modes can be\neffectively captured by a set of target states. This leads to our target-driven\ntrajectory prediction (TNT) framework. TNT has three stages which are trained\nend-to-end. It first predicts an agent's potential target states $T$ steps into\nthe future, by encoding its interactions with the environment and the other\nagents. TNT then generates trajectory state sequences conditioned on targets. A\nfinal stage estimates trajectory likelihoods and a final compact set of\ntrajectory predictions is selected. This is in contrast to previous work which\nmodels agent intents as latent variables, and relies on test-time sampling to\ngenerate diverse trajectories. We benchmark TNT on trajectory prediction of\nvehicles and pedestrians, where we outperform state-of-the-art on Argoverse\nForecasting, INTERACTION, Stanford Drone and an in-house\nPedestrian-at-Intersection dataset.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 06:52:46 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 07:33:10 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Zhao", "Hang", ""], ["Gao", "Jiyang", ""], ["Lan", "Tian", ""], ["Sun", "Chen", ""], ["Sapp", "Benjamin", ""], ["Varadarajan", "Balakrishnan", ""], ["Shen", "Yue", ""], ["Shen", "Yi", ""], ["Chai", "Yuning", ""], ["Schmid", "Cordelia", ""], ["Li", "Congcong", ""], ["Anguelov", "Dragomir", ""]]}, {"id": "2008.08298", "submitter": "Li-Wen Wang", "authors": "Li-Wen Wang, Wan-Chi Siu, Zhi-Song Liu, Chu-Tak Li, Daniel P.K. Lun", "title": "Deep Relighting Networks for Image Light Source Manipulation", "comments": "The 2020 European Conference on Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manipulating the light source of given images is an interesting task and\nuseful in various applications, including photography and cinematography.\nExisting methods usually require additional information like the geometric\nstructure of the scene, which may not be available for most images. In this\npaper, we formulate the single image relighting task and propose a novel Deep\nRelighting Network (DRN) with three parts: 1) scene reconversion, which aims to\nreveal the primary scene structure through a deep auto-encoder network, 2)\nshadow prior estimation, to predict light effect from the new light direction\nthrough adversarial learning, and 3) re-renderer, to combine the primary\nstructure with the reconstructed shadow view to form the required estimation\nunder the target light source. Experimental results show that the proposed\nmethod outperforms other possible methods, both qualitatively and\nquantitatively. Specifically, the proposed DRN has achieved the best PSNR in\nthe \"AIM2020 - Any to one relighting challenge\" of the 2020 ECCV conference.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 07:03:23 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 04:02:17 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Wang", "Li-Wen", ""], ["Siu", "Wan-Chi", ""], ["Liu", "Zhi-Song", ""], ["Li", "Chu-Tak", ""], ["Lun", "Daniel P. K.", ""]]}, {"id": "2008.08311", "submitter": "Seokwoo Jung", "authors": "Seokwoo Jung, Sungha Choi, Mohammad Azam Khan, Jaegul Choo", "title": "Towards Lightweight Lane Detection by Optimizing Spatial Embedding", "comments": "Preprint - work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of lane detection methods depend on a proposal-free instance\nsegmentation because of its adaptability to flexible object shape, occlusion,\nand real-time application. This paper addresses the problem that pixel\nembedding in proposal-free instance segmentation based lane detection is\ndifficult to optimize. A translation invariance of convolution, which is one of\nthe supposed strengths, causes challenges in optimizing pixel embedding. In\nthis work, we propose a lane detection method based on proposal-free instance\nsegmentation, directly optimizing spatial embedding of pixels using image\ncoordinate. Our proposed method allows the post-processing step for center\nlocalization and optimizes clustering in an end-to-end manner. The proposed\nmethod enables real-time lane detection through the simplicity of\npost-processing and the adoption of a lightweight backbone. Our proposed method\ndemonstrates competitive performance on public lane detection datasets.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 07:37:04 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 06:45:20 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Jung", "Seokwoo", ""], ["Choi", "Sungha", ""], ["Khan", "Mohammad Azam", ""], ["Choo", "Jaegul", ""]]}, {"id": "2008.08324", "submitter": "Yu Rong", "authors": "Yu Rong, Takaaki Shiratori, Hanbyul Joo", "title": "FrankMocap: Fast Monocular 3D Hand and Body Motion Capture by Regression\n  and Integration", "comments": "Demo, Code and Models are available at\n  https://penincillin.github.io/frank_mocap", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the essential nuance of human motion is often conveyed as a\ncombination of body movements and hand gestures, the existing monocular motion\ncapture approaches mostly focus on either body motion capture only ignoring\nhand parts or hand motion capture only without considering body motion. In this\npaper, we present FrankMocap, a motion capture system that can estimate both 3D\nhand and body motion from in-the-wild monocular inputs with faster speed (9.5\nfps) and better accuracy than previous work. Our method works in near real-time\n(9.5 fps) and produces 3D body and hand motion capture outputs as a unified\nparametric model structure. Our method aims to capture 3D body and hand motion\nsimultaneously from challenging in-the-wild monocular videos. To construct\nFrankMocap, we build the state-of-the-art monocular 3D \"hand\" motion capture\nmethod by taking the hand part of the whole body parametric model (SMPL-X). Our\n3D hand motion capture output can be efficiently integrated to monocular body\nmotion capture output, producing whole body motion results in a unified\nparrametric model structure. We demonstrate the state-of-the-art performance of\nour hand motion capture system in public benchmarks, and show the high quality\nof our whole body motion capture result in various challenging real-world\nscenes, including a live demo scenario.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 08:22:30 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Rong", "Yu", ""], ["Shiratori", "Takaaki", ""], ["Joo", "Hanbyul", ""]]}, {"id": "2008.08332", "submitter": "Yuxi Li", "authors": "Yuxi Li, Weiyao Lin, John See, Ning Xu, Shugong Xu, Ke Yan and Cong\n  Yang", "title": "CFAD: Coarse-to-Fine Action Detector for Spatiotemporal Action\n  Localization", "comments": "7 figures, 3 tables; ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Most current pipelines for spatio-temporal action localization connect\nframe-wise or clip-wise detection results to generate action proposals, where\nonly local information is exploited and the efficiency is hindered by dense\nper-frame localization. In this paper, we propose Coarse-to-Fine Action\nDetector (CFAD),an original end-to-end trainable framework for efficient\nspatio-temporal action localization. The CFAD introduces a new paradigm that\nfirst estimates coarse spatio-temporal action tubes from video streams, and\nthen refines the tubes' location based on key timestamps. This concept is\nimplemented by two key components, the Coarse and Refine Modules in our\nframework. The parameterized modeling of long temporal information in the\nCoarse Module helps obtain accurate initial tube estimation, while the Refine\nModule selectively adjusts the tube location under the guidance of key\ntimestamps. Against other methods, theproposed CFAD achieves competitive\nresults on action detection benchmarks of UCF101-24, UCFSports and JHMDB-21\nwith inference speed that is 3.3x faster than the nearest competitors.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 08:47:50 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Li", "Yuxi", ""], ["Lin", "Weiyao", ""], ["See", "John", ""], ["Xu", "Ning", ""], ["Xu", "Shugong", ""], ["Yan", "Ke", ""], ["Yang", "Cong", ""]]}, {"id": "2008.08336", "submitter": "Sheng Ren", "authors": "Sheng Ren, Yan He, Neal N. Xiong and Kehua Guo", "title": "Towards Class-incremental Object Detection with Nearest Mean of\n  Exemplars", "comments": "There are inaccuracies in the statement of the article, and we need\n  to withdraw and correct the inaccurate statement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental learning is a form of online learning. Incremental learning can\nmodify the parameters and structure of the deep learning model so that the\nmodel does not forget the old knowledge while learning new knowledge.\nPreventing catastrophic forgetting is the most important task of incremental\nlearning. However, the current incremental learning is often only for one type\nof input. For example, if the input images are of the same type, the current\nincremental model can learn new knowledge while not forgetting old knowledge.\nHowever, if several categories are added to the input graphics, the current\nmodel will not be able to deal with it correctly, and the accuracy will drop\nsignificantly. Therefore, this paper proposes a kind of incremental method,\nwhich adjusts the parameters of the model by identifying the prototype vector\nand increasing the distance of the vector, so that the model can learn new\nknowledge without catastrophic forgetting. Experiments show the effectiveness\nof our proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 08:56:04 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 00:41:30 GMT"}, {"version": "v3", "created": "Fri, 9 Oct 2020 07:23:34 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Ren", "Sheng", ""], ["He", "Yan", ""], ["Xiong", "Neal N.", ""], ["Guo", "Kehua", ""]]}, {"id": "2008.08345", "submitter": "Dominik Engel", "authors": "Dominik Engel, Timo Ropinski", "title": "Deep Volumetric Ambient Occlusion", "comments": "IEEE VIS SciVis 2020", "journal-ref": null, "doi": "10.1109/TVCG.2020.3030344", "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel deep learning based technique for volumetric ambient\nocclusion in the context of direct volume rendering. Our proposed Deep\nVolumetric Ambient Occlusion (DVAO) approach can predict per-voxel ambient\nocclusion in volumetric data sets, while considering global information\nprovided through the transfer function. The proposed neural network only needs\nto be executed upon change of this global information, and thus supports\nreal-time volume interaction. Accordingly, we demonstrate DVAOs ability to\npredict volumetric ambient occlusion, such that it can be applied interactively\nwithin direct volume rendering. To achieve the best possible results, we\npropose and analyze a variety of transfer function representations and\ninjection strategies for deep neural networks. Based on the obtained results we\nalso give recommendations applicable in similar volume learning scenarios.\nLastly, we show that DVAO generalizes to a variety of modalities, despite being\ntrained on computed tomography data only.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 09:19:08 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 06:13:14 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Engel", "Dominik", ""], ["Ropinski", "Timo", ""]]}, {"id": "2008.08360", "submitter": "Junyan Wang", "authors": "Junyan Wang, Yang Bai, Yang Long, Bingzhang Hu, Zhenhua Chai, Yu Guan\n  and Xiaolin Wei", "title": "Query Twice: Dual Mixture Attention Meta Learning for Video\n  Summarization", "comments": "This manuscript has been accepted at ACM MM 2020", "journal-ref": null, "doi": "10.1145/3394171.3414064", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video summarization aims to select representative frames to retain high-level\ninformation, which is usually solved by predicting the segment-wise importance\nscore via a softmax function. However, softmax function suffers in retaining\nhigh-rank representations for complex visual or sequential information, which\nis known as the Softmax Bottleneck problem. In this paper, we propose a novel\nframework named Dual Mixture Attention (DMASum) model with Meta Learning for\nvideo summarization that tackles the softmax bottleneck problem, where the\nMixture of Attention layer (MoA) effectively increases the model capacity by\nemploying twice self-query attention that can capture the second-order changes\nin addition to the initial query-key attention, and a novel Single Frame Meta\nLearning rule is then introduced to achieve more generalization to small\ndatasets with limited training sources. Furthermore, the DMASum significantly\nexploits both visual and sequential attention that connects local key-frame and\nglobal attention in an accumulative way. We adopt the new evaluation protocol\non two public datasets, SumMe, and TVSum. Both qualitative and quantitative\nexperiments manifest significant improvements over the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 10:12:52 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Wang", "Junyan", ""], ["Bai", "Yang", ""], ["Long", "Yang", ""], ["Hu", "Bingzhang", ""], ["Chai", "Zhenhua", ""], ["Guan", "Yu", ""], ["Wei", "Xiaolin", ""]]}, {"id": "2008.08369", "submitter": "Artjoms Gorpincenko", "authors": "Artjoms Gorpincenko, Geoffrey French, Michal Mackiewicz", "title": "Virtual Adversarial Training in Feature Space to Improve Unsupervised\n  Video Domain Adaptation", "comments": "Submitted to the EI conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Adversarial Training has recently seen a lot of success in\nsemi-supervised learning, as well as unsupervised Domain Adaptation. However,\nso far it has been used on input samples in the pixel space, whereas we propose\nto apply it directly to feature vectors. We also discuss the unstable behaviour\nof entropy minimization and Decision-Boundary Iterative Refinement Training\nWith a Teacher in Domain Adaptation, and suggest substitutes that achieve\nsimilar behaviour. By adding the aforementioned techniques to the state of the\nart model TA$^3$N, we either maintain competitive results or outperform prior\nart in multiple unsupervised video Domain Adaptation tasks\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 10:30:31 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Gorpincenko", "Artjoms", ""], ["French", "Geoffrey", ""], ["Mackiewicz", "Michal", ""]]}, {"id": "2008.08384", "submitter": "Alfred Laugros", "authors": "Alfred Laugros, Alice Caplier, Matthieu Ospici", "title": "Addressing Neural Network Robustness with Mixup and Targeted Labeling\n  Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their performance, Artificial Neural Networks are not reliable enough\nfor most of industrial applications. They are sensitive to noises, rotations,\nblurs and adversarial examples. There is a need to build defenses that protect\nagainst a wide range of perturbations, covering the most traditional common\ncorruptions and adversarial examples. We propose a new data augmentation\nstrategy called M-TLAT and designed to address robustness in a broad sense. Our\napproach combines the Mixup augmentation and a new adversarial training\nalgorithm called Targeted Labeling Adversarial Training (TLAT). The idea of\nTLAT is to interpolate the target labels of adversarial examples with the\nground-truth labels. We show that M-TLAT can increase the robustness of image\nclassifiers towards nineteen common corruptions and five adversarial attacks,\nwithout reducing the accuracy on clean samples.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 11:34:11 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Laugros", "Alfred", ""], ["Caplier", "Alice", ""], ["Ospici", "Matthieu", ""]]}, {"id": "2008.08391", "submitter": "Zhigang Li", "authors": "Zhigang Li, Yinlin Hu, Mathieu Salzmann, and Xiangyang Ji", "title": "Robust RGB-based 6-DoF Pose Estimation without Real Pose Annotations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While much progress has been made in 6-DoF object pose estimation from a\nsingle RGB image, the current leading approaches heavily rely on\nreal-annotation data. As such, they remain sensitive to severe occlusions,\nbecause covering all possible occlusions with annotated data is intractable. In\nthis paper, we introduce an approach to robustly and accurately estimate the\n6-DoF pose in challenging conditions and without using any real pose\nannotations. To this end, we leverage the intuition that the poses predicted by\na network from an image and from its counterpart synthetically altered to mimic\nocclusion should be consistent, and translate this to a self-supervised loss\nfunction. Our experiments on LINEMOD, Occluded-LINEMOD, YCB and new\nRandomization LINEMOD dataset evidence the robustness of our approach. We\nachieve state of the art performance on LINEMOD, and OccludedLINEMOD in without\nreal-pose setting, even outperforming methods that rely on real annotations\nduring training on Occluded-LINEMOD.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 12:07:01 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Li", "Zhigang", ""], ["Hu", "Yinlin", ""], ["Salzmann", "Mathieu", ""], ["Ji", "Xiangyang", ""]]}, {"id": "2008.08407", "submitter": "Yun Wang", "authors": "Yun Wang, Tong Zhang, Zhen Cui, Chunyan Xu, Jian Yang", "title": "Instance-Aware Graph Convolutional Network for Multi-Label\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional neural network (GCN) has effectively boosted the\nmulti-label image recognition task by introducing label dependencies based on\nstatistical label co-occurrence of data. However, in previous methods, label\ncorrelation is computed based on statistical information of data and therefore\nthe same for all samples, and this makes graph inference on labels insufficient\nto handle huge variations among numerous image instances. In this paper, we\npropose an instance-aware graph convolutional neural network (IA-GCN) framework\nfor multi-label classification. As a whole, two fused branches of sub-networks\nare involved in the framework: a global branch modeling the whole image and a\nregion-based branch exploring dependencies among regions of interests (ROIs).\nFor label diffusion of instance-awareness in graph convolution, rather than\nusing the statistical label correlation alone, an image-dependent label\ncorrelation matrix (LCM), fusing both the statistical LCM and an individual one\nof each image instance, is constructed for graph inference on labels to inject\nadaptive information of label-awareness into the learned features of the model.\nSpecifically, the individual LCM of each image is obtained by mining the label\ndependencies based on the scores of labels about detected ROIs. In this\nprocess, considering the contribution differences of ROIs to multi-label\nclassification, variational inference is introduced to learn adaptive scaling\nfactors for those ROIs by considering their complex distribution. Finally,\nextensive experiments on MS-COCO and VOC datasets show that our proposed\napproach outperforms existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 12:49:28 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Wang", "Yun", ""], ["Zhang", "Tong", ""], ["Cui", "Zhen", ""], ["Xu", "Chunyan", ""], ["Yang", "Jian", ""]]}, {"id": "2008.08414", "submitter": "Alexander Krull", "authors": "Anna S. Goncharova, Alf Honigmann, Florian Jug, Alexander Krull", "title": "Improving Blind Spot Denoising for Microscopy", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many microscopy applications are limited by the total amount of usable light\nand are consequently challenged by the resulting levels of noise in the\nacquired images. This problem is often addressed via (supervised) deep learning\nbased denoising. Recently, by making assumptions about the noise statistics,\nself-supervised methods have emerged. Such methods are trained directly on the\nimages that are to be denoised and do not require additional paired training\ndata. While achieving remarkable results, self-supervised methods can produce\nhigh-frequency artifacts and achieve inferior results compared to supervised\napproaches. Here we present a novel way to improve the quality of\nself-supervised denoising. Considering that light microscopy images are usually\ndiffraction-limited, we propose to include this knowledge in the denoising\nprocess. We assume the clean image to be the result of a convolution with a\npoint spread function (PSF) and explicitly include this operation at the end of\nour neural network. As a consequence, we are able to eliminate high-frequency\nartifacts and achieve self-supervised results that are very close to the ones\nachieved with traditional supervised methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 13:06:24 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Goncharova", "Anna S.", ""], ["Honigmann", "Alf", ""], ["Jug", "Florian", ""], ["Krull", "Alexander", ""]]}, {"id": "2008.08416", "submitter": "JooYeol Yun", "authors": "JooYeol Yun, JungWoo Oh, and IlDong Yun", "title": "Gradually Applying Weakly Supervised and Active Learning for Mass\n  Detection in Breast Ultrasound Images", "comments": null, "journal-ref": "Appl. Sci. 2020, 10(13), 4519", "doi": "10.3390/app10134519", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for effectively utilizing weakly annotated image data in\nan object detection tasks of breast ultrasound images. Given the problem\nsetting where a small, strongly annotated dataset and a large, weakly annotated\ndataset with no bounding box information are available, training an object\ndetection model becomes a non-trivial problem. We suggest a controlled weight\nfor handling the effect of weakly annotated images in a two stage object\ndetection model. We~also present a subsequent active learning scheme for safely\nassigning weakly annotated images a strong annotation using the trained model.\nExperimental results showed a 24\\% point increase in correct localization\n(CorLoc) measure, which is the ratio of correctly localized and classified\nimages, by assigning the properly controlled weight. Performing active learning\nafter a model is trained showed an additional increase in CorLoc. We tested the\nproposed method on the Stanford Dog datasets to assure that it can be applied\nto general cases, where strong annotations are insufficient to obtain\nresembling results. The presented method showed that higher performance is\nachievable with lesser annotation effort.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 13:09:00 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Yun", "JooYeol", ""], ["Oh", "JungWoo", ""], ["Yun", "IlDong", ""]]}, {"id": "2008.08418", "submitter": "M. Saquib Sarfraz", "authors": "Alexander Wolpert, Michael Teutsch, M. Saquib Sarfraz, Rainer\n  Stiefelhagen", "title": "Anchor-free Small-scale Multispectral Pedestrian Detection", "comments": "BMVC2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multispectral images consisting of aligned visual-optical (VIS) and thermal\ninfrared (IR) image pairs are well-suited for practical applications like\nautonomous driving or visual surveillance. Such data can be used to increase\nthe performance of pedestrian detection especially for weakly illuminated,\nsmall-scaled, or partially occluded instances. The current state-of-the-art is\nbased on variants of Faster R-CNN and thus passes through two stages: a\nproposal generator network with handcrafted anchor boxes for object\nlocalization and a classification network for verifying the object category. In\nthis paper we propose a method for effective and efficient multispectral fusion\nof the two modalities in an adapted single-stage anchor-free base architecture.\nWe aim at learning pedestrian representations based on object center and scale\nrather than direct bounding box predictions. In this way, we can both simplify\nthe network architecture and achieve higher detection performance, especially\nfor pedestrians under occlusion or at low object resolution. In addition, we\nprovide a study on well-suited multispectral data augmentation techniques that\nimprove the commonly used augmentations. The results show our method's\neffectiveness in detecting small-scaled pedestrians. We achieve 5.68%\nlog-average miss rate in comparison to the best current state-of-the-art of\n7.49% (25% improvement) on the challenging KAIST Multispectral Pedestrian\nDetection Benchmark.\n  Code: https://github.com/HensoldtOptronicsCV/MultispectralPedestrianDetection\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 13:13:01 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 15:01:59 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Wolpert", "Alexander", ""], ["Teutsch", "Michael", ""], ["Sarfraz", "M. Saquib", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "2008.08424", "submitter": "Harkirat Behl", "authors": "Harkirat Singh Behl, At{\\i}l{\\i}m G\\\"une\\c{s} Baydin, Ran Gal, Philip\n  H.S. Torr, Vibhav Vineet", "title": "AutoSimulate: (Quickly) Learning Synthetic Data Generation", "comments": "ECCV 2020", "journal-ref": "European Conference on Computer Vision (ECCV) 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation is increasingly being used for generating large labelled datasets\nin many machine learning problems. Recent methods have focused on adjusting\nsimulator parameters with the goal of maximising accuracy on a validation task,\nusually relying on REINFORCE-like gradient estimators. However these approaches\nare very expensive as they treat the entire data generation, model training,\nand validation pipeline as a black-box and require multiple costly objective\nevaluations at each iteration. We propose an efficient alternative for optimal\nsynthetic data generation, based on a novel differentiable approximation of the\nobjective. This allows us to optimize the simulator, which may be\nnon-differentiable, requiring only one objective evaluation at each iteration\nwith a little overhead. We demonstrate on a state-of-the-art photorealistic\nrenderer that the proposed method finds the optimal data distribution faster\n(up to $50\\times$), with significantly reduced training data generation (up to\n$30\\times$) and better accuracy ($+8.7\\%$) on real-world test datasets than\nprevious methods.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 11:36:11 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Behl", "Harkirat Singh", ""], ["Baydin", "At\u0131l\u0131m G\u00fcne\u015f", ""], ["Gal", "Ran", ""], ["Torr", "Philip H. S.", ""], ["Vineet", "Vibhav", ""]]}, {"id": "2008.08432", "submitter": "Gael Kamdem De Teyou Dr", "authors": "Gael Kamdem De Teyou, Yuliya Tarabalka, Isabelle Manighetti, Rafael\n  Almar, Sebastien Tripod", "title": "Deep Neural Networks for automatic extraction of features in time series\n  satellite images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many earth observation programs such as Landsat, Sentinel, SPOT, and Pleiades\nproduce huge volume of medium to high resolution multi-spectral images every\nday that can be organized in time series. In this work, we exploit both\ntemporal and spatial information provided by these images to generate land\ncover maps. For this purpose, we combine a fully convolutional neural network\nwith a convolutional long short-term memory. Implementation details of the\nproposed spatio-temporal neural network architecture are provided. Experimental\nresults show that the temporal information provided by time series images\nallows increasing the accuracy of land cover classification, thus producing\nup-to-date maps that can help in identifying changes on earth.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 09:26:52 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["De Teyou", "Gael Kamdem", ""], ["Tarabalka", "Yuliya", ""], ["Manighetti", "Isabelle", ""], ["Almar", "Rafael", ""], ["Tripod", "Sebastien", ""]]}, {"id": "2008.08433", "submitter": "Qingjie Meng", "authors": "Qingjie Meng and Daniel Rueckert and Bernhard Kainz", "title": "Unsupervised Cross-domain Image Classification by Distance Metric Guided\n  Feature Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning deep neural networks that are generalizable across different domains\nremains a challenge due to the problem of domain shift. Unsupervised domain\nadaptation is a promising avenue which transfers knowledge from a source domain\nto a target domain without using any labels in the target domain. Contemporary\ntechniques focus on extracting domain-invariant features using domain\nadversarial training. However, these techniques neglect to learn discriminative\nclass boundaries in the latent representation space on a target domain and\nyield limited adaptation performance. To address this problem, we propose\ndistance metric guided feature alignment (MetFA) to extract discriminative as\nwell as domain-invariant features on both source and target domains. The\nproposed MetFA method explicitly and directly learns the latent representation\nwithout using domain adversarial training. Our model integrates class\ndistribution alignment to transfer semantic knowledge from a source domain to a\ntarget domain. We evaluate the proposed method on fetal ultrasound datasets for\ncross-device image classification. Experimental results demonstrate that the\nproposed method outperforms the state-of-the-art and enables model\ngeneralization.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 13:36:57 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Meng", "Qingjie", ""], ["Rueckert", "Daniel", ""], ["Kainz", "Bernhard", ""]]}, {"id": "2008.08452", "submitter": "Taufiq Hasan", "authors": "Asif Shahriyar Sushmit, Partho Ghosh, Md.Abrar Istiak, Nayeeb Rashid,\n  Ahsan Habib Akash, Taufiq Hasan", "title": "SegCodeNet: Color-Coded Segmentation Masks for Activity Detection from\n  Wearable Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activity detection from first-person videos (FPV) captured using a wearable\ncamera is an active research field with potential applications in many sectors,\nincluding healthcare, law enforcement, and rehabilitation. State-of-the-art\nmethods use optical flow-based hybrid techniques that rely on features derived\nfrom the motion of objects from consecutive frames. In this work, we developed\na two-stream network, the \\emph{SegCodeNet}, that uses a network branch\ncontaining video-streams with color-coded semantic segmentation masks of\nrelevant objects in addition to the original RGB video-stream. We also include\na stream-wise attention gating that prioritizes between the two streams and a\nframe-wise attention module that prioritizes the video frames that contain\nrelevant features. Experiments are conducted on an FPV dataset containing $18$\nactivity classes in office environments. In comparison to a single-stream\nnetwork, the proposed two-stream method achieves an absolute improvement of\n$14.366\\%$ and $10.324\\%$ for averaged F1 score and accuracy, respectively,\nwhen average results are compared for three different frame sizes\n$224\\times224$, $112\\times112$, and $64\\times64$. The proposed method provides\nsignificant performance gains for lower-resolution images with absolute\nimprovements of $17\\%$ and $26\\%$ in F1 score for input dimensions of\n$112\\times112$ and $64\\times64$, respectively. The best performance is achieved\nfor a frame size of $224\\times224$ yielding an F1 score and accuracy of\n$90.176\\%$ and $90.799\\%$ which outperforms the state-of-the-art Inflated 3D\nConvNet (I3D) \\cite{carreira2017quo} method by an absolute margin of $4.529\\%$\nand $2.419\\%$, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 14:01:53 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Sushmit", "Asif Shahriyar", ""], ["Ghosh", "Partho", ""], ["Istiak", "Md. Abrar", ""], ["Rashid", "Nayeeb", ""], ["Akash", "Ahsan Habib", ""], ["Hasan", "Taufiq", ""]]}, {"id": "2008.08454", "submitter": "XiangTong Wang", "authors": "Dali Wang", "title": "MineNav: An Expandable Synthetic Dataset Based on Minecraft for Aircraft\n  Visual Navigation", "comments": "6 pages,10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simply method to generate high quality synthetic dataset based\non open-source game Minecraft includes rendered image, Depth map, surface\nnormal map, and 6-dof camera trajectory. This dataset has a perfect\nground-truth generated by plug-in program, and thanks for the large game's\ncommunity, there is an extremely large number of 3D open-world environment,\nusers can find suitable scenes for shooting and build data sets through it and\nthey can also build scenes in-game. as such, We don't need to worry about\nmanual over fitting caused by too small datasets. what's more, there is also a\nshader community which We can use to minimize data bias between rendered images\nand real-images as little as possible. Last but not least, we now provide three\ntools to generate the data for depth prediction ,surface normal prediction and\nvisual odometry, user can also develop the plug-in module for other vision task\nlike segmentation or optical flow prediction.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 14:03:17 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Wang", "Dali", ""]]}, {"id": "2008.08465", "submitter": "Yann Labb\\'e", "authors": "Yann Labb\\'e, Justin Carpentier, Mathieu Aubry, Josef Sivic", "title": "CosyPose: Consistent multi-view multi-object 6D pose estimation", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an approach for recovering the 6D pose of multiple known objects\nin a scene captured by a set of input images with unknown camera viewpoints.\nFirst, we present a single-view single-object 6D pose estimation method, which\nwe use to generate 6D object pose hypotheses. Second, we develop a robust\nmethod for matching individual 6D object pose hypotheses across different input\nimages in order to jointly estimate camera viewpoints and 6D poses of all\nobjects in a single consistent scene. Our approach explicitly handles object\nsymmetries, does not require depth measurements, is robust to missing or\nincorrect object hypotheses, and automatically recovers the number of objects\nin the scene. Third, we develop a method for global scene refinement given\nmultiple object hypotheses and their correspondences across views. This is\nachieved by solving an object-level bundle adjustment problem that refines the\nposes of cameras and objects to minimize the reprojection error in all views.\nWe demonstrate that the proposed method, dubbed CosyPose, outperforms current\nstate-of-the-art results for single-view and multi-view 6D object pose\nestimation by a large margin on two challenging benchmarks: the YCB-Video and\nT-LESS datasets. Code and pre-trained models are available on the project\nwebpage https://www.di.ens.fr/willow/research/cosypose/.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 14:11:56 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Labb\u00e9", "Yann", ""], ["Carpentier", "Justin", ""], ["Aubry", "Mathieu", ""], ["Sivic", "Josef", ""]]}, {"id": "2008.08473", "submitter": "Benjamin Riggan", "authors": "Cedric Nimpa Fondje, Shuowen Hu, Nathaniel J. Short, Benjamin S.\n  Riggan", "title": "Cross-Domain Identification for Thermal-to-Visible Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recent advances in domain adaptation, especially those applied to\nheterogeneous facial recognition, typically rely upon restrictive Euclidean\nloss functions (e.g., $L_2$ norm) which perform best when images from two\ndifferent domains (e.g., visible and thermal) are co-registered and temporally\nsynchronized. This paper proposes a novel domain adaptation framework that\ncombines a new feature mapping sub-network with existing deep feature models,\nwhich are based on modified network architectures (e.g., VGG16 or Resnet50).\nThis framework is optimized by introducing new cross-domain identity and domain\ninvariance loss functions for thermal-to-visible face recognition, which\nalleviates the requirement for precisely co-registered and synchronized\nimagery. We provide extensive analysis of both features and loss functions\nused, and compare the proposed domain adaptation framework with\nstate-of-the-art feature based domain adaptation models on a difficult dataset\ncontaining facial imagery collected at varying ranges, poses, and expressions.\nMoreover, we analyze the viability of the proposed framework for more\nchallenging tasks, such as non-frontal thermal-to-visible face recognition.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 14:24:04 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Fondje", "Cedric Nimpa", ""], ["Hu", "Shuowen", ""], ["Short", "Nathaniel J.", ""], ["Riggan", "Benjamin S.", ""]]}, {"id": "2008.08474", "submitter": "Xu Chen", "authors": "Jie Song, Xu Chen, Otmar Hilliges", "title": "Human Body Model Fitting by Learned Gradient Descent", "comments": "First two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel algorithm for the fitting of 3D human shape to images.\nCombining the accuracy and refinement capabilities of iterative gradient-based\noptimization techniques with the robustness of deep neural networks, we propose\na gradient descent algorithm that leverages a neural network to predict the\nparameter update rule for each iteration. This per-parameter and state-aware\nupdate guides the optimizer towards a good solution in very few steps,\nconverging in typically few steps. During training our approach only requires\nMoCap data of human poses, parametrized via SMPL. From this data the network\nlearns a subspace of valid poses and shapes in which optimization is performed\nmuch more efficiently. The approach does not require any hard to acquire\nimage-to-3D correspondences. At test time we only optimize the 2D joint\nre-projection error without the need for any further priors or regularization\nterms. We show empirically that this algorithm is fast (avg. 120ms\nconvergence), robust to initialization and dataset, and achieves\nstate-of-the-art results on public evaluation datasets including the\nchallenging 3DPW in-the-wild benchmark (improvement over SMPLify 45%) and also\napproaches using image-to-3D correspondences\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 14:26:47 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Song", "Jie", ""], ["Chen", "Xu", ""], ["Hilliges", "Otmar", ""]]}, {"id": "2008.08496", "submitter": "Saul Calderon Ramirez", "authors": "Saul Calderon-Ramirez, Shengxiang-Yang, Armaghan Moemeni, David\n  Elizondo, Simon Colreavy-Donnelly, Luis Fernando Chavarria-Estrada, Miguel A.\n  Molina-Cabello", "title": "Correcting Data Imbalance for Semi-Supervised Covid-19 Detection Using\n  X-ray Chest Images", "comments": "Under journal review", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Corona Virus (COVID-19) is an internationalpandemic that has quickly\npropagated throughout the world. The application of deep learning for image\nclassification of chest X-ray images of Covid-19 patients, could become a novel\npre-diagnostic detection methodology. However, deep learning architectures\nrequire large labelled datasets. This is often a limitation when the subject of\nresearch is relatively new as in the case of the virus outbreak, where dealing\nwith small labelled datasets is a challenge. Moreover, in the context of a new\nhighly infectious disease, the datasets are also highly imbalanced,with few\nobservations from positive cases of the new disease. In this work we evaluate\nthe performance of the semi-supervised deep learning architecture known as\nMixMatch using a very limited number of labelled observations and highly\nimbalanced labelled dataset. We propose a simple approach for correcting data\nimbalance, re-weight each observationin the loss function, giving a higher\nweight to the observationscorresponding to the under-represented class. For\nunlabelled observations, we propose the usage of the pseudo and augmentedlabels\ncalculated by MixMatch to choose the appropriate weight. The MixMatch method\ncombined with the proposed pseudo-label based balance correction improved\nclassification accuracy by up to 10%, with respect to the non balanced MixMatch\nalgorithm, with statistical significance. We tested our proposed approach with\nseveral available datasets using 10, 15 and 20 labelledobservations.\nAdditionally, a new dataset is included among thetested datasets, composed of\nchest X-ray images of Costa Rican adult patients\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 15:16:57 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 20:53:08 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Calderon-Ramirez", "Saul", ""], ["Shengxiang-Yang", "", ""], ["Moemeni", "Armaghan", ""], ["Elizondo", "David", ""], ["Colreavy-Donnelly", "Simon", ""], ["Chavarria-Estrada", "Luis Fernando", ""], ["Molina-Cabello", "Miguel A.", ""]]}, {"id": "2008.08502", "submitter": "Lezi Wang", "authors": "Lezi Wang, Dong Liu, Rohit Puri, and Dimitris N. Metaxas", "title": "Learning Trailer Moments in Full-Length Movies", "comments": "Accepted by ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A movie's key moments stand out of the screenplay to grab an audience's\nattention and make movie browsing efficient. But a lack of annotations makes\nthe existing approaches not applicable to movie key moment detection. To get\nrid of human annotations, we leverage the officially-released trailers as the\nweak supervision to learn a model that can detect the key moments from\nfull-length movies. We introduce a novel ranking network that utilizes the\nCo-Attention between movies and trailers as guidance to generate the training\npairs, where the moments highly corrected with trailers are expected to be\nscored higher than the uncorrelated moments. Additionally, we propose a\nContrastive Attention module to enhance the feature representations such that\nthe comparative contrast between features of the key and non-key moments are\nmaximized. We construct the first movie-trailer dataset, and the proposed\nCo-Attention assisted ranking network shows superior performance even over the\nsupervised approach. The effectiveness of our Contrastive Attention module is\nalso demonstrated by the performance improvement over the state-of-the-art on\nthe public benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 15:23:25 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Wang", "Lezi", ""], ["Liu", "Dong", ""], ["Puri", "Rohit", ""], ["Metaxas", "Dimitris N.", ""]]}, {"id": "2008.08523", "submitter": "Hang Du", "authors": "Anna Zhu, Hang Du, Shengwu Xiong", "title": "Scene Text Detection with Selected Anchor", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object proposal technique with dense anchoring scheme for scene text\ndetection were applied frequently to achieve high recall. It results in the\nsignificant improvement in accuracy but waste of computational searching,\nregression and classification. In this paper, we propose an anchor\nselection-based region proposal network (AS-RPN) using effective selected\nanchors instead of dense anchors to extract text proposals. The center, scales,\naspect ratios and orientations of anchors are learnable instead of fixing,\nwhich leads to high recall and greatly reduced numbers of anchors. By replacing\nthe anchor-based RPN in Faster RCNN, the AS-RPN-based Faster RCNN can achieve\ncomparable performance with previous state-of-the-art text detecting approaches\non standard benchmarks, including COCO-Text, ICDAR2013, ICDAR2015 and\nMSRA-TD500 when using single-scale and single model (ResNet50) testing only.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 16:03:13 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Zhu", "Anna", ""], ["Du", "Hang", ""], ["Xiong", "Shengwu", ""]]}, {"id": "2008.08525", "submitter": "Giorgio Pietro Biondetti", "authors": "Giorgio Pietro Biondetti, Romane Gauriau, Christopher P. Bridge,\n  Charles Lu, Katherine P. Andriole", "title": "\"Name that manufacturer\". Relating image acquisition bias with task\n  complexity when training deep learning models: experiments on head CT", "comments": "15 pages, 4 figures. This paper has been submitted to the Journal of\n  Digital Imaging (Springer Journal) and it is now in review", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As interest in applying machine learning techniques for medical images\ncontinues to grow at a rapid pace, models are starting to be developed and\ndeployed for clinical applications. In the clinical AI model development\nlifecycle (described by Lu et al. [1]), a crucial phase for machine learning\nscientists and clinicians is the proper design and collection of the data\ncohort. The ability to recognize various forms of biases and distribution\nshifts in the dataset is critical at this step. While it remains difficult to\naccount for all potential sources of bias, techniques can be developed to\nidentify specific types of bias in order to mitigate their impact. In this work\nwe analyze how the distribution of scanner manufacturers in a dataset can\ncontribute to the overall bias of deep learning models. We evaluate\nconvolutional neural networks (CNN) for both classification and segmentation\ntasks, specifically two state-of-the-art models: ResNet [2] for classification\nand U-Net [3] for segmentation. We demonstrate that CNNs can learn to\ndistinguish the imaging scanner manufacturer and that this bias can\nsubstantially impact model performance for both classification and segmentation\ntasks. By creating an original synthesis dataset of brain data mimicking the\npresence of more or less subtle lesions we also show that this bias is related\nto the difficulty of the task. Recognition of such bias is critical to develop\nrobust, generalizable models that will be crucial for clinical applications in\nreal-world data distributions.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 16:05:58 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Biondetti", "Giorgio Pietro", ""], ["Gauriau", "Romane", ""], ["Bridge", "Christopher P.", ""], ["Lu", "Charles", ""], ["Andriole", "Katherine P.", ""]]}, {"id": "2008.08526", "submitter": "Feifan Yang", "authors": "Xiaoguang Li, Feifan Yang, Kin Man Lam, Li Zhuo, Jiafeng Li", "title": "Blur-Attention: A boosting mechanism for non-uniform blurred image\n  restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic scene deblurring is a challenging problem in computer vision. It is\ndifficult to accurately estimate the spatially varying blur kernel by\ntraditional methods. Data-driven-based methods usually employ kernel-free\nend-to-end mapping schemes, which are apt to overlook the kernel estimation. To\naddress this issue, we propose a blur-attention module to dynamically capture\nthe spatially varying features of non-uniform blurred images. The module\nconsists of a DenseBlock unit and a spatial attention unit with multi-pooling\nfeature fusion, which can effectively extract complex spatially varying blur\nfeatures. We design a multi-level residual connection structure to connect\nmultiple blur-attention modules to form a blur-attention network. By\nintroducing the blur-attention network into a conditional generation\nadversarial framework, we propose an end-to-end blind motion deblurring method,\nnamely Blur-Attention-GAN (BAG), for a single image. Our method can adaptively\nselect the weights of the extracted features according to the spatially varying\nblur features, and dynamically restore the images. Experimental results show\nthat the deblurring capability of our method achieved outstanding objective\nperformance in terms of PSNR, SSIM, and subjective visual quality. Furthermore,\nby visualizing the features extracted by the blur-attention module,\ncomprehensive discussions are provided on its effectiveness.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 16:07:06 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Li", "Xiaoguang", ""], ["Yang", "Feifan", ""], ["Lam", "Kin Man", ""], ["Zhuo", "Li", ""], ["Li", "Jiafeng", ""]]}, {"id": "2008.08528", "submitter": "Boqiang Xu", "authors": "Boqiang Xu, Lingxiao He, Xingyu Liao, Wu Liu, Zhenan Sun, Tao Mei", "title": "Black Re-ID: A Head-shoulder Descriptor for the Challenging Problem of\n  Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (Re-ID) aims at retrieving an input person image\nfrom a set of images captured by multiple cameras. Although recent Re-ID\nmethods have made great success, most of them extract features in terms of the\nattributes of clothing (e.g., color, texture). However, it is common for people\nto wear black clothes or be captured by surveillance systems in low light\nillumination, in which cases the attributes of the clothing are severely\nmissing. We call this problem the Black Re-ID problem. To solve this problem,\nrather than relying on the clothing information, we propose to exploit\nhead-shoulder features to assist person Re-ID. The head-shoulder adaptive\nattention network (HAA) is proposed to learn the head-shoulder feature and an\ninnovative ensemble method is designed to enhance the generalization of our\nmodel. Given the input person image, the ensemble method would focus on the\nhead-shoulder feature by assigning a larger weight if the individual insides\nthe image is in black clothing. Due to the lack of a suitable benchmark dataset\nfor studying the Black Re-ID problem, we also contribute the first Black-reID\ndataset, which contains 1274 identities in training set. Extensive evaluations\non the Black-reID, Market1501 and DukeMTMC-reID datasets show that our model\nachieves the best result compared with the state-of-the-art Re-ID methods on\nboth Black and conventional Re-ID problems. Furthermore, our method is also\nproved to be effective in dealing with person Re-ID in similar clothing. Our\ncode and dataset are avaliable on https://github.com/xbq1994/.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 16:10:36 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Xu", "Boqiang", ""], ["He", "Lingxiao", ""], ["Liao", "Xingyu", ""], ["Liu", "Wu", ""], ["Sun", "Zhenan", ""], ["Mei", "Tao", ""]]}, {"id": "2008.08535", "submitter": "Timo Bolkart", "authors": "Ahmed A. A. Osman, Timo Bolkart, Michael J. Black", "title": "STAR: Sparse Trained Articulated Human Body Regressor", "comments": "ECCV 2020", "journal-ref": null, "doi": "10.1007/978-3-030-58539-6_36", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SMPL body model is widely used for the estimation, synthesis, and\nanalysis of 3D human pose and shape. While popular, we show that SMPL has\nseveral limitations and introduce STAR, which is quantitatively and\nqualitatively superior to SMPL. First, SMPL has a huge number of parameters\nresulting from its use of global blend shapes. These dense pose-corrective\noffsets relate every vertex on the mesh to all the joints in the kinematic\ntree, capturing spurious long-range correlations. To address this, we define\nper-joint pose correctives and learn the subset of mesh vertices that are\ninfluenced by each joint movement. This sparse formulation results in more\nrealistic deformations and significantly reduces the number of model parameters\nto 20% of SMPL. When trained on the same data as SMPL, STAR generalizes better\ndespite having many fewer parameters. Second, SMPL factors pose-dependent\ndeformations from body shape while, in reality, people with different shapes\ndeform differently. Consequently, we learn shape-dependent pose-corrective\nblend shapes that depend on both body pose and BMI. Third, we show that the\nshape space of SMPL is not rich enough to capture the variation in the human\npopulation. We address this by training STAR with an additional 10,000 scans of\nmale and female subjects, and show that this results in better model\ngeneralization. STAR is compact, generalizes better to new bodies and is a\ndrop-in replacement for SMPL. STAR is publicly available for research purposes\nat http://star.is.tue.mpg.de.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 16:27:55 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Osman", "Ahmed A. A.", ""], ["Bolkart", "Timo", ""], ["Black", "Michael J.", ""]]}, {"id": "2008.08561", "submitter": "Baoliang Chen", "authors": "Baoliang Chen, Haoliang Li, Hongfei Fan and Shiqi Wang", "title": "No-reference Screen Content Image Quality Assessment with Unsupervised\n  Domain Adaptation", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2021.3084750", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we quest the capability of transferring the quality of natural\nscene images to the images that are not acquired by optical cameras (e.g.,\nscreen content images, SCIs), rooted in the widely accepted view that the human\nvisual system has adapted and evolved through the perception of natural\nenvironment. Here, we develop the first unsupervised domain adaptation based no\nreference quality assessment method for SCIs, leveraging rich subjective\nratings of the natural images (NIs). In general, it is a non-trivial task to\ndirectly transfer the quality prediction model from NIs to a new type of\ncontent (i.e., SCIs) that holds dramatically different statistical\ncharacteristics. Inspired by the transferability of pair-wise relationship, the\nproposed quality measure operates based on the philosophy of improving the\ntransferability and discriminability simultaneously. In particular, we\nintroduce three types of losses which complementarily and explicitly regularize\nthe feature space of ranking in a progressive manner. Regarding feature\ndiscriminatory capability enhancement, we propose a center based loss to\nrectify the classifier and improve its prediction capability not only for\nsource domain (NI) but also the target domain (SCI). For feature discrepancy\nminimization, the maximum mean discrepancy (MMD) is imposed on the extracted\nranking features of NIs and SCIs. Furthermore, to further enhance the feature\ndiversity, we introduce the correlation penalization between different feature\ndimensions, leading to the features with lower rank and higher diversity.\nExperiments show that our method can achieve higher performance on different\nsource-target settings based on a light-weight convolution neural network. The\nproposed method also sheds light on learning quality assessment measures for\nunseen application-specific content without the cumbersome and costing\nsubjective evaluations.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 17:31:23 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 09:44:37 GMT"}, {"version": "v3", "created": "Sat, 15 May 2021 16:45:10 GMT"}, {"version": "v4", "created": "Wed, 26 May 2021 03:00:56 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Chen", "Baoliang", ""], ["Li", "Haoliang", ""], ["Fan", "Hongfei", ""], ["Wang", "Shiqi", ""]]}, {"id": "2008.08563", "submitter": "Ying Qu", "authors": "Ying Qu, Razieh Kaviani Baghbaderani, Wei Li, Lianru Gao, Hairong Qi", "title": "Physically-Constrained Transfer Learning through Shared Abundance Space\n  for Hyperspectral Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral image (HSI) classification is one of the most active research\ntopics and has achieved promising results boosted by the recent development of\ndeep learning. However, most state-of-the-art approaches tend to perform poorly\nwhen the training and testing images are on different domains, e.g., source\ndomain and target domain, respectively, due to the spectral variability caused\nby different acquisition conditions. Transfer learning-based methods address\nthis problem by pre-training in the source domain and fine-tuning on the target\ndomain. Nonetheless, a considerable amount of data on the target domain has to\nbe labeled and non-negligible computational resources are required to retrain\nthe whole network. In this paper, we propose a new transfer learning scheme to\nbridge the gap between the source and target domains by projecting the HSI data\nfrom the source and target domains into a shared abundance space based on their\nown physical characteristics. In this way, the domain discrepancy would be\nlargely reduced such that the model trained on the source domain could be\napplied on the target domain without extra efforts for data labeling or network\nretraining. The proposed method is referred to as physically-constrained\ntransfer learning through shared abundance space (PCTL-SAS). Extensive\nexperimental results demonstrate the superiority of the proposed method as\ncompared to the state-of-the-art. The success of this endeavor would largely\nfacilitate the deployment of HSI classification for real-world sensing\nscenarios.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 17:41:37 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 11:05:13 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Qu", "Ying", ""], ["Baghbaderani", "Razieh Kaviani", ""], ["Li", "Wei", ""], ["Gao", "Lianru", ""], ["Qi", "Hairong", ""]]}, {"id": "2008.08574", "submitter": "Cheng-Chun Hsu", "authors": "Cheng-Chun Hsu, Yi-Hsuan Tsai, Yen-Yu Lin, Ming-Hsuan Yang", "title": "Every Pixel Matters: Center-aware Feature Alignment for Domain Adaptive\n  Object Detector", "comments": "Accepted in ECCV'20. Project page:\n  https://chengchunhsu.github.io/EveryPixelMatters/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A domain adaptive object detector aims to adapt itself to unseen domains that\nmay contain variations of object appearance, viewpoints or backgrounds. Most\nexisting methods adopt feature alignment either on the image level or instance\nlevel. However, image-level alignment on global features may tangle\nforeground/background pixels at the same time, while instance-level alignment\nusing proposals may suffer from the background noise. Different from existing\nsolutions, we propose a domain adaptation framework that accounts for each\npixel via predicting pixel-wise objectness and centerness. Specifically, the\nproposed method carries out center-aware alignment by paying more attention to\nforeground pixels, hence achieving better adaptation across domains. We\ndemonstrate our method on numerous adaptation settings with extensive\nexperimental results and show favorable performance against existing\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 17:57:03 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Hsu", "Cheng-Chun", ""], ["Tsai", "Yi-Hsuan", ""], ["Lin", "Yen-Yu", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2008.08579", "submitter": "Tanishq Abraham", "authors": "Tanishq Abraham, Andrew Shaw, Daniel O'Connor, Austin Todd, Richard\n  Levenson", "title": "Slide-free MUSE Microscopy to H&E Histology Modality Conversion via\n  Unpaired Image-to-Image Translation GAN Models", "comments": "4 pages plus 1 page references. Presented at the ICML Computational\n  Biology Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MUSE is a novel slide-free imaging technique for histological examination of\ntissues that can serve as an alternative to traditional histology. In order to\nbridge the gap between MUSE and traditional histology, we aim to convert MUSE\nimages to resemble authentic hematoxylin- and eosin-stained (H&E) images. We\nevaluated four models: a non-machine-learning-based color-mapping\nunmixing-based tool, CycleGAN, DualGAN, and GANILLA. CycleGAN and GANILLA\nprovided visually compelling results that appropriately transferred H&E style\nand preserved MUSE content. Based on training an automated critic on real and\ngenerated H&E images, we determined that CycleGAN demonstrated the best\nperformance. We have also found that MUSE color inversion may be a necessary\nstep for accurate modality conversion to H&E. We believe that our MUSE-to-H&E\nmodel can help improve adoption of novel slide-free methods by bridging a\nperceptual gap between MUSE imaging and traditional histology.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 17:59:08 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Abraham", "Tanishq", ""], ["Shaw", "Andrew", ""], ["O'Connor", "Daniel", ""], ["Todd", "Austin", ""], ["Levenson", "Richard", ""]]}, {"id": "2008.08622", "submitter": "Steven Zucker", "authors": "Steven W Zucker", "title": "On Qualitative Shape Inferences: a journey from geometry to topology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape inference is classically ill-posed, because it involves a map from the\n(2D) image domain to the (3D) world. Standard approaches regularize this\nproblem by either assuming a prior on lighting and rendering or restricting the\ndomain, and develop differential equations or optimization solutions. While\nelegant, the solutions that emerge in these situations are remarkably fragile.\nWe exploit the observation that people infer shape qualitatively; that there\nare quantitative differences between individuals. The consequence is a\ntopological approach based on critical contours and the Morse-Smale complex.\nThis paper provides a developmental review of that theory, emphasizing the\nmotivation at different stages of the research.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 18:32:54 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Zucker", "Steven W", ""]]}, {"id": "2008.08633", "submitter": "Guangyi Zhang", "authors": "Guangyi Zhang and Ali Etemad", "title": "RFNet: Riemannian Fusion Network for EEG-based Brain-Computer Interfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the novel Riemannian Fusion Network (RFNet), a deep\nneural architecture for learning spatial and temporal information from\nElectroencephalogram (EEG) for a number of different EEG-based Brain Computer\nInterface (BCI) tasks and applications. The spatial information relies on\nSpatial Covariance Matrices (SCM) of multi-channel EEG, whose space form a\nRiemannian Manifold due to the Symmetric and Positive Definite structure. We\nexploit a Riemannian approach to map spatial information onto feature vectors\nin Euclidean space. The temporal information characterized by features based on\ndifferential entropy and logarithm power spectrum density is extracted from\ndifferent windows through time. Our network then learns the temporal\ninformation by employing a deep long short-term memory network with a soft\nattention mechanism. The output of the attention mechanism is used as the\ntemporal feature vector. To effectively fuse spatial and temporal information,\nwe use an effective fusion strategy, which learns attention weights applied to\nembedding-specific features for decision making. We evaluate our proposed\nframework on four public datasets from three popular fields of BCI, notably\nemotion recognition, vigilance estimation, and motor imagery classification,\ncontaining various types of tasks such as binary classification, multi-class\nclassification, and regression. RFNet approaches the state-of-the-art on one\ndataset (SEED) and outperforms other methods on the other three datasets\n(SEED-VIG, BCI-IV 2A, and BCI-IV 2B), setting new state-of-the-art values and\nshowing the robustness of our framework in EEG representation learning.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 18:56:49 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Zhang", "Guangyi", ""], ["Etemad", "Ali", ""]]}, {"id": "2008.08673", "submitter": "Md Yousuf Harun", "authors": "Md Yousuf Harun, M Arifur Rahman, Joshua Mellinger, Willy Chang,\n  Thomas Huang, Brienne Walker, Kristen Hori, and Aaron T. Ohta", "title": "Image Segmentation of Zona-Ablated Human Blastocysts", "comments": null, "journal-ref": "IEEE 13th International Conference on Nano/Molecular Medicine &\n  Engineering (NANOMED), Gwangju, Korea (South), 2019, pp. 208-213", "doi": "10.1109/NANOMED49242.2019.9130621", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automating human preimplantation embryo grading offers the potential for\nhigher success rates with in vitro fertilization (IVF) by providing new\nquantitative and objective measures of embryo quality. Current IVF procedures\ntypically use only qualitative manual grading, which is limited in the\nidentification of genetically abnormal embryos. The automatic quantitative\nassessment of blastocyst expansion can potentially improve sustained pregnancy\nrates and reduce health risks from abnormal pregnancies through a more accurate\nidentification of genetic abnormality. The expansion rate of a blastocyst is an\nimportant morphological feature to determine the quality of a developing\nembryo. In this work, a deep learning based human blastocyst image segmentation\nmethod is presented, with the goal of facilitating the challenging task of\nsegmenting irregularly shaped blastocysts. The type of blastocysts evaluated\nhere has undergone laser ablation of the zona pellucida, which is required\nprior to trophectoderm biopsy. This complicates the manual measurements of the\nexpanded blastocyst's size, which shows a correlation with genetic\nabnormalities. The experimental results on the test set demonstrate\nsegmentation greatly improves the accuracy of expansion measurements, resulting\nin up to 99.4% accuracy, 98.1% precision, 98.8% recall, a 98.4% Dice\nCoefficient, and a 96.9% Jaccard Index.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 21:20:02 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Harun", "Md Yousuf", ""], ["Rahman", "M Arifur", ""], ["Mellinger", "Joshua", ""], ["Chang", "Willy", ""], ["Huang", "Thomas", ""], ["Walker", "Brienne", ""], ["Hori", "Kristen", ""], ["Ohta", "Aaron T.", ""]]}, {"id": "2008.08676", "submitter": "Md Yousuf Harun", "authors": "Md Yousuf Harun, Thomas Huang, and Aaron T. Ohta", "title": "Inner Cell Mass and Trophectoderm Segmentation in Human Blastocyst\n  Images using Deep Neural Network", "comments": null, "journal-ref": "IEEE 13th International Conference on Nano/Molecular Medicine &\n  Engineering (NANOMED), Gwangju, Korea (South), 2019, pp. 214-219", "doi": "10.1109/NANOMED49242.2019.9130618", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embryo quality assessment based on morphological attributes is important for\nachieving higher pregnancy rates from in vitro fertilization (IVF). The\naccurate segmentation of the embryo's inner cell mass (ICM) and trophectoderm\nepithelium (TE) is important, as these parameters can help to predict the\nembryo viability and live birth potential. However, segmentation of the ICM and\nTE is difficult due to variations in their shape and similarities in their\ntextures, both with each other and with their surroundings. To tackle this\nproblem, a deep neural network (DNN) based segmentation approach was\nimplemented. The DNN can identify the ICM region with 99.1% accuracy, 94.9%\nprecision, 93.8% recall, a 94.3% Dice Coefficient, and a 89.3% Jaccard Index.\nIt can extract the TE region with 98.3% accuracy, 91.8% precision, 93.2%\nrecall, a 92.5% Dice Coefficient, and a 85.3% Jaccard Index.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 21:23:16 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Harun", "Md Yousuf", ""], ["Huang", "Thomas", ""], ["Ohta", "Aaron T.", ""]]}, {"id": "2008.08698", "submitter": "Jianbo Jiao", "authors": "Jianbo Jiao, Ana I.L. Namburete, Aris T. Papageorghiou, J. Alison\n  Noble", "title": "Self-Supervised Ultrasound to MRI Fetal Brain Image Synthesis", "comments": "IEEE Transactions on Medical Imaging 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fetal brain magnetic resonance imaging (MRI) offers exquisite images of the\ndeveloping brain but is not suitable for second-trimester anomaly screening,\nfor which ultrasound (US) is employed. Although expert sonographers are adept\nat reading US images, MR images which closely resemble anatomical images are\nmuch easier for non-experts to interpret. Thus in this paper we propose to\ngenerate MR-like images directly from clinical US images. In medical image\nanalysis such a capability is potentially useful as well, for instance for\nautomatic US-MRI registration and fusion. The proposed model is end-to-end\ntrainable and self-supervised without any external annotations. Specifically,\nbased on an assumption that the US and MRI data share a similar anatomical\nlatent space, we first utilise a network to extract the shared latent features,\nwhich are then used for MRI synthesis. Since paired data is unavailable for our\nstudy (and rare in practice), pixel-level constraints are infeasible to apply.\nWe instead propose to enforce the distributions to be statistically\nindistinguishable, by adversarial learning in both the image domain and feature\nspace. To regularise the anatomical structures between US and MRI during\nsynthesis, we further propose an adversarial structural constraint. A new\ncross-modal attention technique is proposed to utilise non-local spatial\ninformation, by encouraging multi-modal knowledge fusion and propagation. We\nextend the approach to consider the case where 3D auxiliary information (e.g.,\n3D neighbours and a 3D location index) from volumetric data is also available,\nand show that this improves image synthesis. The proposed approach is evaluated\nquantitatively and qualitatively with comparison to real fetal MR images and\nother approaches to synthesis, demonstrating its feasibility of synthesising\nrealistic MR images.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 22:56:36 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Jiao", "Jianbo", ""], ["Namburete", "Ana I. L.", ""], ["Papageorghiou", "Aris T.", ""], ["Noble", "J. Alison", ""]]}, {"id": "2008.08701", "submitter": "Jin Sun", "authors": "Jin Sun, Hadar Averbuch-Elor, Qianqian Wang, and Noah Snavely", "title": "Hidden Footprints: Learning Contextual Walkability from 3D Human Trails", "comments": "European Conference on Computer Vision (ECCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting where people can walk in a scene is important for many tasks,\nincluding autonomous driving systems and human behavior analysis. Yet learning\na computational model for this purpose is challenging due to semantic ambiguity\nand a lack of labeled data: current datasets only tell you where people are,\nnot where they could be. We tackle this problem by leveraging information from\nexisting datasets, without additional labeling. We first augment the set of\nvalid, labeled walkable regions by propagating person observations between\nimages, utilizing 3D information to create what we call hidden footprints.\nHowever, this augmented data is still sparse. We devise a training strategy\ndesigned for such sparse labels, combining a class-balanced classification loss\nwith a contextual adversarial loss. Using this strategy, we demonstrate a model\nthat learns to predict a walkability map from a single image. We evaluate our\nmodel on the Waymo and Cityscapes datasets, demonstrating superior performance\ncompared to baselines and state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 23:19:08 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Sun", "Jin", ""], ["Averbuch-Elor", "Hadar", ""], ["Wang", "Qianqian", ""], ["Snavely", "Noah", ""]]}, {"id": "2008.08716", "submitter": "Sudipta Paul", "authors": "Sudipta Paul, Niluthpol Chowdhury Mithun, and Amit K. Roy-Chowdhury", "title": "Text-based Localization of Moments in a Video Corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior works on text-based video moment localization focus on temporally\ngrounding the textual query in an untrimmed video. These works assume that the\nrelevant video is already known and attempt to localize the moment on that\nrelevant video only. Different from such works, we relax this assumption and\naddress the task of localizing moments in a corpus of videos for a given\nsentence query. This task poses a unique challenge as the system is required to\nperform: (i) retrieval of the relevant video where only a segment of the video\ncorresponds with the queried sentence, and (ii) temporal localization of moment\nin the relevant video based on sentence query. Towards overcoming this\nchallenge, we propose Hierarchical Moment Alignment Network (HMAN) which learns\nan effective joint embedding space for moments and sentences. In addition to\nlearning subtle differences between intra-video moments, HMAN focuses on\ndistinguishing inter-video global semantic concepts based on sentence queries.\nQualitative and quantitative results on three benchmark text-based video moment\nretrieval datasets - Charades-STA, DiDeMo, and ActivityNet Captions -\ndemonstrate that our method achieves promising performance on the proposed task\nof temporal localization of moments in a corpus of videos.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 00:05:45 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Paul", "Sudipta", ""], ["Mithun", "Niluthpol Chowdhury", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "2008.08730", "submitter": "Wanyi Fu", "authors": "Wanyi Fu, Shobhit Sharma, Ehsan Abadi, Alexandros-Stavros Iliopoulos,\n  Qi Wang, Joseph Y. Lo, Xiaobai Sun, William P. Segars, Ehsan Samei", "title": "iPhantom: a framework for automated creation of individualized\n  computational phantoms and its application to CT organ dosimetry", "comments": "Main text: 11 pages, 8 figures; Supplement material: 7 pages, 5\n  figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: This study aims to develop and validate a novel framework,\niPhantom, for automated creation of patient-specific phantoms or digital-twins\n(DT) using patient medical images. The framework is applied to assess radiation\ndose to radiosensitive organs in CT imaging of individual patients. Method:\nFrom patient CT images, iPhantom segments selected anchor organs (e.g. liver,\nbones, pancreas) using a learning-based model developed for multi-organ CT\nsegmentation. Organs challenging to segment (e.g. intestines) are incorporated\nfrom a matched phantom template, using a diffeomorphic registration model\ndeveloped for multi-organ phantom-voxels. The resulting full-patient phantoms\nare used to assess organ doses during routine CT exams. Result: iPhantom was\nvalidated on both the XCAT (n=50) and an independent clinical (n=10) dataset\nwith similar accuracy. iPhantom precisely predicted all organ locations with\ngood accuracy of Dice Similarity Coefficients (DSC) >0.6 for anchor organs and\nDSC of 0.3-0.9 for all other organs. iPhantom showed less than 10% dose errors\nfor the majority of organs, which was notably superior to the state-of-the-art\nbaseline method (20-35% dose errors). Conclusion: iPhantom enables automated\nand accurate creation of patient-specific phantoms and, for the first time,\nprovides sufficient and automated patient-specific dose estimates for CT\ndosimetry. Significance: The new framework brings the creation and application\nof CHPs to the level of individual CHPs through automation, achieving a wider\nand precise organ localization, paving the way for clinical monitoring, and\npersonalized optimization, and large-scale research.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 01:50:49 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Fu", "Wanyi", ""], ["Sharma", "Shobhit", ""], ["Abadi", "Ehsan", ""], ["Iliopoulos", "Alexandros-Stavros", ""], ["Wang", "Qi", ""], ["Lo", "Joseph Y.", ""], ["Sun", "Xiaobai", ""], ["Segars", "William P.", ""], ["Samei", "Ehsan", ""]]}, {"id": "2008.08735", "submitter": "Sarah Ostadabbas", "authors": "Shuangjun Liu, Xiaofei Huang, Nihang Fu, Cheng Li, Zhongnan Su, and\n  Sarah Ostadabbas", "title": "Simultaneously-Collected Multimodal Lying Pose Dataset: Towards In-Bed\n  Human Pose Monitoring under Adverse Vision Conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision (CV) has achieved great success in interpreting semantic\nmeanings from images, yet CV algorithms can be brittle for tasks with adverse\nvision conditions and the ones suffering from data/label pair limitation. One\nof this tasks is in-bed human pose estimation, which has significant values in\nmany healthcare applications. In-bed pose monitoring in natural settings could\ninvolve complete darkness or full occlusion. Furthermore, the lack of publicly\navailable in-bed pose datasets hinders the use of many successful pose\nestimation algorithms for this task. In this paper, we introduce our\nSimultaneously-collected multimodal Lying Pose (SLP) dataset, which includes\nin-bed pose images from 109 participants captured using multiple imaging\nmodalities including RGB, long wave infrared, depth, and pressure map. We also\npresent a physical hyper parameter tuning strategy for ground truth pose label\ngeneration under extreme conditions such as lights off and being fully covered\nby a sheet/blanket. SLP design is compatible with the mainstream human pose\ndatasets, therefore, the state-of-the-art 2D pose estimation models can be\ntrained effectively with SLP data with promising performance as high as 95% at\nPCKh@0.5 on a single modality. The pose estimation performance can be further\nimproved by including additional modalities through collaboration.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 02:20:35 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Liu", "Shuangjun", ""], ["Huang", "Xiaofei", ""], ["Fu", "Nihang", ""], ["Li", "Cheng", ""], ["Su", "Zhongnan", ""], ["Ostadabbas", "Sarah", ""]]}, {"id": "2008.08756", "submitter": "Dahuin Jung", "authors": "Dahuin Jung, Jonghyun Lee, Jihun Yi, and Sungroh Yoon", "title": "iCaps: An Interpretable Classifier via Disentangled Capsule Networks", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an interpretable Capsule Network, iCaps, for image classification.\nA capsule is a group of neurons nested inside each layer, and the one in the\nlast layer is called a class capsule, which is a vector whose norm indicates a\npredicted probability for the class. Using the class capsule, existing Capsule\nNetworks already provide some level of interpretability. However, there are two\nlimitations which degrade its interpretability: 1) the class capsule also\nincludes classification-irrelevant information, and 2) entities represented by\nthe class capsule overlap. In this work, we address these two limitations using\na novel class-supervised disentanglement algorithm and an additional\nregularizer, respectively. Through quantitative and qualitative evaluations on\nthree datasets, we demonstrate that the resulting classifier, iCaps, provides a\nprediction along with clear rationales behind it with no performance\ndegradation.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 03:44:26 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Jung", "Dahuin", ""], ["Lee", "Jonghyun", ""], ["Yi", "Jihun", ""], ["Yoon", "Sungroh", ""]]}, {"id": "2008.08766", "submitter": "Prarthana Bhattacharyya", "authors": "Prarthana Bhattacharyya and Krzysztof Czarnecki", "title": "Deformable PV-RCNN: Improving 3D Object Detection with Learned\n  Deformations", "comments": "Accepted at ECCV 2020 Workshop on Perception for Autonomous Driving", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Deformable PV-RCNN, a high-performing point-cloud based 3D object\ndetector. Currently, the proposal refinement methods used by the\nstate-of-the-art two-stage detectors cannot adequately accommodate differing\nobject scales, varying point-cloud density, part-deformation and clutter. We\npresent a proposal refinement module inspired by 2D deformable convolution\nnetworks that can adaptively gather instance-specific features from locations\nwhere informative content exists. We also propose a simple context gating\nmechanism which allows the keypoints to select relevant context information for\nthe refinement stage. We show state-of-the-art results on the KITTI dataset.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 04:11:17 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Bhattacharyya", "Prarthana", ""], ["Czarnecki", "Krzysztof", ""]]}, {"id": "2008.08767", "submitter": "WeiLei Wen", "authors": "Ben Niu, Weilei Wen, Wenqi Ren, Xiangde Zhang, Lianping Yang, Shuzhen\n  Wang, Kaihao Zhang, Xiaochun Cao and Haifeng Shen", "title": "Single Image Super-Resolution via a Holistic Attention Network", "comments": "16 pages, 6 figures, IEEE International Conference on Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Informative features play a crucial role in the single image super-resolution\ntask. Channel attention has been demonstrated to be effective for preserving\ninformation-rich features in each layer. However, channel attention treats each\nconvolution layer as a separate process that misses the correlation among\ndifferent layers. To address this problem, we propose a new holistic attention\nnetwork (HAN), which consists of a layer attention module (LAM) and a\nchannel-spatial attention module (CSAM), to model the holistic\ninterdependencies among layers, channels, and positions. Specifically, the\nproposed LAM adaptively emphasizes hierarchical features by considering\ncorrelations among layers. Meanwhile, CSAM learns the confidence at all the\npositions of each channel to selectively capture more informative features.\nExtensive experiments demonstrate that the proposed HAN performs favorably\nagainst the state-of-the-art single image super-resolution approaches.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 04:13:15 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Niu", "Ben", ""], ["Wen", "Weilei", ""], ["Ren", "Wenqi", ""], ["Zhang", "Xiangde", ""], ["Yang", "Lianping", ""], ["Wang", "Shuzhen", ""], ["Zhang", "Kaihao", ""], ["Cao", "Xiaochun", ""], ["Shen", "Haifeng", ""]]}, {"id": "2008.08775", "submitter": "Qingsong Xu", "authors": "Qingsong Xu, Xin Yuan, Chaojun Ouyang, Yue Zeng", "title": "Spatial--spectral FFPNet: Attention-Based Pyramid Network for\n  Segmentation and Classification of Remote Sensing Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of segmentation and classification of high-resolution\nand hyperspectral remote sensing images. Unlike conventional natural (RGB)\nimages, the inherent large scale and complex structures of remote sensing\nimages pose major challenges such as spatial object distribution diversity and\nspectral information extraction when existing models are directly applied for\nimage classification. In this study, we develop an attention-based pyramid\nnetwork for segmentation and classification of remote sensing datasets.\nAttention mechanisms are used to develop the following modules: i) a novel and\nrobust attention-based multi-scale fusion method effectively fuses useful\nspatial or spectral information at different and same scales; ii) a region\npyramid attention mechanism using region-based attention addresses the target\ngeometric size diversity in large-scale remote sensing images; and iii\ncross-scale attention} in our adaptive atrous spatial pyramid pooling network\nadapts to varied contents in a feature-embedded space. Different forms of\nfeature fusion pyramid frameworks are established by combining these\nattention-based modules. First, a novel segmentation framework, called the\nheavy-weight spatial feature fusion pyramid network (FFPNet), is proposed to\naddress the spatial problem of high-resolution remote sensing images. Second,\nan end-to-end spatial--spectral FFPNet is presented for classifying\nhyperspectral images. Experiments conducted on ISPRS Vaihingen and ISPRS\nPotsdam high-resolution datasets demonstrate the competitive segmentation\naccuracy achieved by the proposed heavy-weight spatial FFPNet. Furthermore,\nexperiments on the Indian Pines and the University of Pavia hyperspectral\ndatasets indicate that the proposed spatial--spectral FFPNet outperforms the\ncurrent state-of-the-art methods in hyperspectral image classification.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 04:55:34 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Xu", "Qingsong", ""], ["Yuan", "Xin", ""], ["Ouyang", "Chaojun", ""], ["Zeng", "Yue", ""]]}, {"id": "2008.08791", "submitter": "Monica Perusquia-Hernandez", "authors": "Monica Perusquia-Hernandez, Felix Dollack, Chun Kwang Tan, Shushi\n  Namba, Saho Ayabe-Kanamura, Kenji Suzuki", "title": "Facial movement synergies and Action Unit detection from distal wearable\n  Electromyography and Computer Vision", "comments": "11 pages, 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distal facial Electromyography (EMG) can be used to detect smiles and frowns\nwith reasonable accuracy. It capitalizes on volume conduction to detect\nrelevant muscle activity, even when the electrodes are not placed directly on\nthe source muscle. The main advantage of this method is to prevent occlusion\nand obstruction of the facial expression production, whilst allowing EMG\nmeasurements. However, measuring EMG distally entails that the exact source of\nthe facial movement is unknown. We propose a novel method to estimate specific\nFacial Action Units (AUs) from distal facial EMG and Computer Vision (CV). This\nmethod is based on Independent Component Analysis (ICA), Non-Negative Matrix\nFactorization (NNMF), and sorting of the resulting components to determine\nwhich is the most likely to correspond to each CV-labeled action unit (AU).\nPerformance on the detection of AU06 (Orbicularis Oculi) and AU12 (Zygomaticus\nMajor) was estimated by calculating the agreement with Human Coders. The\nresults of our proposed algorithm showed an accuracy of 81% and a Cohen's Kappa\nof 0.49 for AU6; and accuracy of 82% and a Cohen's Kappa of 0.53 for AU12. This\ndemonstrates the potential of distal EMG to detect individual facial movements.\nUsing this multimodal method, several AU synergies were identified. We\nquantified the co-occurrence and timing of AU6 and AU12 in posed and\nspontaneous smiles using the human-coded labels, and for comparison, using the\ncontinuous CV-labels. The co-occurrence analysis was also performed on the\nEMG-based labels to uncover the relationship between muscle synergies and the\nkinematics of visible facial movement.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 06:09:03 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Perusquia-Hernandez", "Monica", ""], ["Dollack", "Felix", ""], ["Tan", "Chun Kwang", ""], ["Namba", "Shushi", ""], ["Ayabe-Kanamura", "Saho", ""], ["Suzuki", "Kenji", ""]]}, {"id": "2008.08817", "submitter": "Haiyue Zhu", "authors": "Haiyue Zhu, Yiting Li, Fengjun Bai, Wenjie Chen, Xiaocong Li, Jun Ma,\n  Chek Sing Teo, Pey Yuen Tao, and Wei Lin", "title": "Grasping Detection Network with Uncertainty Estimation for\n  Confidence-Driven Semi-Supervised Domain Adaptation", "comments": "6 pages, 7 figures, accepted in IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-efficient domain adaptation with only a few labelled data is desired for\nmany robotic applications, e.g., in grasping detection, the inference skill\nlearned from a grasping dataset is not universal enough to directly apply on\nvarious other daily/industrial applications. This paper presents an approach\nenabling the easy domain adaptation through a novel grasping detection network\nwith confidence-driven semi-supervised learning, where these two components\ndeeply interact with each other. The proposed grasping detection network\nspecially provides a prediction uncertainty estimation mechanism by leveraging\non Feature Pyramid Network (FPN), and the mean-teacher semi-supervised learning\nutilizes such uncertainty information to emphasizing the consistency loss only\nfor those unlabelled data with high confidence, which we referred it as the\nconfidence-driven mean teacher. This approach largely prevents the student\nmodel to learn the incorrect/harmful information from the consistency loss,\nwhich speeds up the learning progress and improves the model accuracy. Our\nresults show that the proposed network can achieve high success rate on the\nCornell grasping dataset, and for domain adaptation with very limited data, the\nconfidence-driven mean teacher outperforms the original mean teacher and direct\ntraining by more than 10% in evaluation loss especially for avoiding the\noverfitting and model diverging.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 07:42:45 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Zhu", "Haiyue", ""], ["Li", "Yiting", ""], ["Bai", "Fengjun", ""], ["Chen", "Wenjie", ""], ["Li", "Xiaocong", ""], ["Ma", "Jun", ""], ["Teo", "Chek Sing", ""], ["Tao", "Pey Yuen", ""], ["Lin", "Wei", ""]]}, {"id": "2008.08823", "submitter": "Georgios Albanis", "authors": "Georgios Albanis, Nikolaos Zioulis, Anastasios Dimou, Dimitrios\n  Zarpalas, Petros Daras", "title": "DronePose: Photorealistic UAV-Assistant Dataset Synthesis for 3D Pose\n  Estimation via a Smooth Silhouette Loss", "comments": "Accepted in ECCVW 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.RO eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work we consider UAVs as cooperative agents supporting human users in\ntheir operations. In this context, the 3D localisation of the UAV assistant is\nan important task that can facilitate the exchange of spatial information\nbetween the user and the UAV. To address this in a data-driven manner, we\ndesign a data synthesis pipeline to create a realistic multimodal dataset that\nincludes both the exocentric user view, and the egocentric UAV view. We then\nexploit the joint availability of photorealistic and synthesized inputs to\ntrain a single-shot monocular pose estimation model. During training we\nleverage differentiable rendering to supplement a state-of-the-art direct\nregression objective with a novel smooth silhouette loss. Our results\ndemonstrate its qualitative and quantitative performance gains over traditional\nsilhouette objectives. Our data and code are available at\nhttps://vcl3d.github.io/DronePose\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 07:54:56 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 06:14:34 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Albanis", "Georgios", ""], ["Zioulis", "Nikolaos", ""], ["Dimou", "Anastasios", ""], ["Zarpalas", "Dimitrios", ""], ["Daras", "Petros", ""]]}, {"id": "2008.08826", "submitter": "ShiJie Sun", "authors": "ShiJie Sun, Naveed Akhtar, XiangYu Song, HuanSheng Song, Ajmal Mian,\n  Mubarak Shah", "title": "Simultaneous Detection and Tracking with Motion Modelling for Multiple\n  Object Tracking", "comments": "25 pages, 9 figures, has been accepted by the ECCCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning-based Multiple Object Tracking (MOT) currently relies on\noff-the-shelf detectors for tracking-by-detection.This results in deep models\nthat are detector biased and evaluations that are detector influenced. To\nresolve this issue, we introduce Deep Motion Modeling Network (DMM-Net) that\ncan estimate multiple objects' motion parameters to perform joint detection and\nassociation in an end-to-end manner. DMM-Net models object features over\nmultiple frames and simultaneously infers object classes, visibility, and their\nmotion parameters. These outputs are readily used to update the tracklets for\nefficient MOT. DMM-Net achieves PR-MOTA score of 12.80 @ 120+ fps for the\npopular UA-DETRAC challenge, which is better performance and orders of\nmagnitude faster. We also contribute a synthetic large-scale public dataset\nOmni-MOT for vehicle tracking that provides precise ground-truth annotations to\neliminate the detector influence in MOT evaluation. This 14M+ frames dataset is\nextendable with our public script (Code at Dataset\n<https://github.com/shijieS/OmniMOTDataset>, Dataset Recorder\n<https://github.com/shijieS/OMOTDRecorder>, Omni-MOT Source\n<https://github.com/shijieS/DMMN>). We demonstrate the suitability of Omni-MOT\nfor deep learning with DMMNet and also make the source code of our network\npublic.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 08:05:33 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Sun", "ShiJie", ""], ["Akhtar", "Naveed", ""], ["Song", "XiangYu", ""], ["Song", "HuanSheng", ""], ["Mian", "Ajmal", ""], ["Shah", "Mubarak", ""]]}, {"id": "2008.08837", "submitter": "Max-Heinrich Laves M. Sc.", "authors": "Max-Heinrich Laves and Malte T\\\"olle and Tobias Ortmaier", "title": "Uncertainty Estimation in Medical Image Denoising with Bayesian Deep\n  Image Prior", "comments": "Accepted at UNSURE workshop (MICCAI 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty quantification in inverse medical imaging tasks with deep\nlearning has received little attention. However, deep models trained on large\ndata sets tend to hallucinate and create artifacts in the reconstructed output\nthat are not anatomically present. We use a randomly initialized convolutional\nnetwork as parameterization of the reconstructed image and perform gradient\ndescent to match the observation, which is known as deep image prior. In this\ncase, the reconstruction does not suffer from hallucinations as no prior\ntraining is performed. We extend this to a Bayesian approach with Monte Carlo\ndropout to quantify both aleatoric and epistemic uncertainty. The presented\nmethod is evaluated on the task of denoising different medical imaging\nmodalities. The experimental results show that our approach yields\nwell-calibrated uncertainty. That is, the predictive uncertainty correlates\nwith the predictive error. This allows for reliable uncertainty estimates and\ncan tackle the problem of hallucinations and artifacts in inverse medical\nimaging tasks.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 08:34:51 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Laves", "Max-Heinrich", ""], ["T\u00f6lle", "Malte", ""], ["Ortmaier", "Tobias", ""]]}, {"id": "2008.08847", "submitter": "Qizhang Li", "authors": "Qizhang Li, Yiwen Guo, Hao Chen", "title": "Yet Another Intermediate-Level Attack", "comments": "Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The transferability of adversarial examples across deep neural network (DNN)\nmodels is the crux of a spectrum of black-box attacks. In this paper, we\npropose a novel method to enhance the black-box transferability of baseline\nadversarial examples. By establishing a linear mapping of the\nintermediate-level discrepancies (between a set of adversarial inputs and their\nbenign counterparts) for predicting the evoked adversarial loss, we aim to take\nfull advantage of the optimization procedure of multi-step baseline attacks. We\nconducted extensive experiments to verify the effectiveness of our method on\nCIFAR-100 and ImageNet. Experimental results demonstrate that it outperforms\nprevious state-of-the-arts considerably. Our code is at\nhttps://github.com/qizhangli/ila-plus-plus.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 09:14:04 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Li", "Qizhang", ""], ["Guo", "Yiwen", ""], ["Chen", "Hao", ""]]}, {"id": "2008.08862", "submitter": "Xinhui Song", "authors": "Xinhui Song, Tianyang Shi, Zunlei Feng, Mingli Song, Jackie Lin,\n  Chuanjie Lin, Changjie Fan, Yi Yuan", "title": "Unsupervised Learning Facial Parameter Regressor for Action Unit\n  Intensity Estimation via Differentiable Renderer", "comments": null, "journal-ref": null, "doi": "10.1145/3394171.3413955", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial action unit (AU) intensity is an index to describe all visually\ndiscernible facial movements. Most existing methods learn intensity estimator\nwith limited AU data, while they lack generalization ability out of the\ndataset. In this paper, we present a framework to predict the facial parameters\n(including identity parameters and AU parameters) based on a bone-driven face\nmodel (BDFM) under different views. The proposed framework consists of a\nfeature extractor, a generator, and a facial parameter regressor. The regressor\ncan fit the physical meaning parameters of the BDFM from a single face image\nwith the help of the generator, which maps the facial parameters to the\ngame-face images as a differentiable renderer. Besides, identity loss, loopback\nloss, and adversarial loss can improve the regressive results. Quantitative\nevaluations are performed on two public databases BP4D and DISFA, which\ndemonstrates that the proposed method can achieve comparable or better\nperformance than the state-of-the-art methods. What's more, the qualitative\nresults also demonstrate the validity of our method in the wild.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 09:49:13 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Song", "Xinhui", ""], ["Shi", "Tianyang", ""], ["Feng", "Zunlei", ""], ["Song", "Mingli", ""], ["Lin", "Jackie", ""], ["Lin", "Chuanjie", ""], ["Fan", "Changjie", ""], ["Yuan", "Yi", ""]]}, {"id": "2008.08871", "submitter": "Aydogan Ozcan", "authors": "Kevin de Haan, Yijie Zhang, Tairan Liu, Anthony E. Sisk, Miguel F. P.\n  Diaz, Jonathan E. Zuckerman, Yair Rivenson, W. Dean Wallace, Aydogan Ozcan", "title": "Deep learning-based transformation of the H&E stain into special stains\n  improves kidney disease diagnosis", "comments": "22 Pages, 5 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pathology is practiced by visual inspection of histochemically stained\nslides. Most commonly, the hematoxylin and eosin (H&E) stain is used in the\ndiagnostic workflow and it is the gold standard for cancer diagnosis. However,\nin many cases, especially for non-neoplastic diseases, additional \"special\nstains\" are used to provide different levels of contrast and color to tissue\ncomponents and allow pathologists to get a clearer diagnostic picture. In this\nstudy, we demonstrate the utility of supervised learning-based computational\nstain transformation from H&E to different special stains (Masson's Trichrome,\nperiodic acid-Schiff and Jones silver stain) using tissue sections from kidney\nneedle core biopsies. Based on evaluation by three renal pathologists, followed\nby adjudication by a fourth renal pathologist, we show that the generation of\nvirtual special stains from existing H&E images improves the diagnosis in\nseveral non-neoplastic kidney diseases, sampled from 16 unique subjects.\nAdjudication of N=48 diagnoses from the three pathologists revealed that the\nvirtually generated special stains yielded 22 improvements (45.8%), 23\nconcordances (47.9%) and 3 discordances (6.3%), when compared against the use\nof H&E stained tissue only. As the virtual transformation of H&E images into\nspecial stains can be achieved in less than 1 min per patient core specimen\nslide, this stain-to-stain transformation framework can improve the quality of\nthe preliminary diagnosis when additional special stains are needed, along with\nsignificant savings in time and cost, reducing the burden on healthcare system\nand patients.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 10:12:03 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["de Haan", "Kevin", ""], ["Zhang", "Yijie", ""], ["Liu", "Tairan", ""], ["Sisk", "Anthony E.", ""], ["Diaz", "Miguel F. P.", ""], ["Zuckerman", "Jonathan E.", ""], ["Rivenson", "Yair", ""], ["Wallace", "W. Dean", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "2008.08880", "submitter": "Soshi Shimada", "authors": "Soshi Shimada, Vladislav Golyanik, Weipeng Xu, Christian Theobalt", "title": "PhysCap: Physically Plausible Monocular 3D Motion Capture in Real Time", "comments": "16 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marker-less 3D human motion capture from a single colour camera has seen\nsignificant progress. However, it is a very challenging and severely ill-posed\nproblem. In consequence, even the most accurate state-of-the-art approaches\nhave significant limitations. Purely kinematic formulations on the basis of\nindividual joints or skeletons, and the frequent frame-wise reconstruction in\nstate-of-the-art methods greatly limit 3D accuracy and temporal stability\ncompared to multi-view or marker-based motion capture. Further, captured 3D\nposes are often physically incorrect and biomechanically implausible, or\nexhibit implausible environment interactions (floor penetration, foot skating,\nunnatural body leaning and strong shifting in depth), which is problematic for\nany use case in computer graphics. We, therefore, present PhysCap, the first\nalgorithm for physically plausible, real-time and marker-less human 3D motion\ncapture with a single colour camera at 25 fps. Our algorithm first captures 3D\nhuman poses purely kinematically. To this end, a CNN infers 2D and 3D joint\npositions, and subsequently, an inverse kinematics step finds space-time\ncoherent joint angles and global 3D pose. Next, these kinematic reconstructions\nare used as constraints in a real-time physics-based pose optimiser that\naccounts for environment constraints (e.g., collision handling and floor\nplacement), gravity, and biophysical plausibility of human postures. Our\napproach employs a combination of ground reaction force and residual force for\nplausible root control, and uses a trained neural network to detect foot\ncontact events in images. Our method captures physically plausible and\ntemporally stable global 3D human motion, without physically implausible\npostures, floor penetrations or foot skating, from video in real time and in\ngeneral scenes. The video is available at\nhttp://gvv.mpi-inf.mpg.de/projects/PhysCap\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 10:46:32 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 14:18:55 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Shimada", "Soshi", ""], ["Golyanik", "Vladislav", ""], ["Xu", "Weipeng", ""], ["Theobalt", "Christian", ""]]}, {"id": "2008.08882", "submitter": "Jaehoon Oh", "authors": "Jaehoon Oh, Hyungjun Yoo, ChangHwan Kim, Se-Young Yun", "title": "BOIL: Towards Representation Change for Few-shot Learning", "comments": "24 pages, 26 figures, 19 tables, ICLR 2021 published", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model Agnostic Meta-Learning (MAML) is one of the most representative of\ngradient-based meta-learning algorithms. MAML learns new tasks with a few data\nsamples using inner updates from a meta-initialization point and learns the\nmeta-initialization parameters with outer updates. It has recently been\nhypothesized that representation reuse, which makes little change in efficient\nrepresentations, is the dominant factor in the performance of the\nmeta-initialized model through MAML in contrast to representation change, which\ncauses a significant change in representations. In this study, we investigate\nthe necessity of representation change for the ultimate goal of few-shot\nlearning, which is solving domain-agnostic tasks. To this aim, we propose a\nnovel meta-learning algorithm, called BOIL (Body Only update in Inner Loop),\nwhich updates only the body (extractor) of the model and freezes the head\n(classifier) during inner loop updates. BOIL leverages representation change\nrather than representation reuse. This is because feature vectors\n(representations) have to move quickly to their corresponding frozen head\nvectors. We visualize this property using cosine similarity, CKA, and empirical\nresults without the head. BOIL empirically shows significant performance\nimprovement over MAML, particularly on cross-domain tasks. The results imply\nthat representation change in gradient-based meta-learning approaches is a\ncritical component.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 10:52:23 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 05:16:52 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Oh", "Jaehoon", ""], ["Yoo", "Hyungjun", ""], ["Kim", "ChangHwan", ""], ["Yun", "Se-Young", ""]]}, {"id": "2008.08884", "submitter": "Lev Teplyakov", "authors": "Lev Teplyakov, Kirill Kaymakov, Evgeny Shvets, Dmitry Nikolaev", "title": "Line detection via a lightweight CNN with a Hough Layer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Line detection is an important computer vision task traditionally solved by\nHough Transform. With the advance of deep learning, however, trainable\napproaches to line detection became popular. In this paper we propose a\nlightweight CNN for line detection with an embedded parameter-free Hough layer,\nwhich allows the network neurons to have global strip-like receptive fields. We\nargue that traditional convolutional networks have two inherent problems when\napplied to the task of line detection and show how insertion of a Hough layer\ninto the network solves them. Additionally, we point out some major\ninconsistencies in the current datasets used for line detection.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 10:54:31 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 13:05:58 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Teplyakov", "Lev", ""], ["Kaymakov", "Kirill", ""], ["Shvets", "Evgeny", ""], ["Nikolaev", "Dmitry", ""]]}, {"id": "2008.08899", "submitter": "Minesh Mathew", "authors": "Minesh Mathew, Ruben Tito, Dimosthenis Karatzas, R. Manmatha, C.V.\n  Jawahar", "title": "Document Visual Question Answering Challenge 2020", "comments": "to be published as a short paper in DAS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents results of Document Visual Question Answering Challenge\norganized as part of \"Text and Documents in the Deep Learning Era\" workshop, in\nCVPR 2020. The challenge introduces a new problem - Visual Question Answering\non document images. The challenge comprised two tasks. The first task concerns\nwith asking questions on a single document image. On the other hand, the second\ntask is set as a retrieval task where the question is posed over a collection\nof images. For the task 1 a new dataset is introduced comprising 50,000\nquestions-answer(s) pairs defined over 12,767 document images. For task 2\nanother dataset has been created comprising 20 questions over 14,362 document\nimages which share the same document template.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 11:36:36 GMT"}, {"version": "v2", "created": "Sun, 18 Jul 2021 03:09:51 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Mathew", "Minesh", ""], ["Tito", "Ruben", ""], ["Karatzas", "Dimosthenis", ""], ["Manmatha", "R.", ""], ["Jawahar", "C. V.", ""]]}, {"id": "2008.08909", "submitter": "Guangshuai Gao", "authors": "Guangshuai Gao, Wenting Zhao, Qingjie Liu, Yunhong Wang", "title": "Co-Saliency Detection with Co-Attention Fully Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-saliency detection aims to detect common salient objects from a group of\nrelevant images. Some attempts have been made with the Fully Convolutional\nNetwork (FCN) framework and achieve satisfactory detection results. However,\ndue to stacking convolution layers and pooling operation, the boundary details\ntend to be lost. In addition, existing models often utilize the extracted\nfeatures without discrimination, leading to redundancy in representation since\nactually not all features are helpful to the final prediction and some even\nbring distraction. In this paper, we propose a co-attention module embedded FCN\nframework, called as Co-Attention FCN (CA-FCN). Specifically, the co-attention\nmodule is plugged into the high-level convolution layers of FCN, which can\nassign larger attention weights on the common salient objects and smaller ones\non the background and uncommon distractors to boost final detection\nperformance. Extensive experiments on three popular co-saliency benchmark\ndatasets demonstrate the superiority of the proposed CA-FCN, which outperforms\nstate-of-the-arts in most cases. Besides, the effectiveness of our new\nco-attention module is also validated with ablation studies.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 11:52:40 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Gao", "Guangshuai", ""], ["Zhao", "Wenting", ""], ["Liu", "Qingjie", ""], ["Wang", "Yunhong", ""]]}, {"id": "2008.08930", "submitter": "Ziqiang Li", "authors": "Ziqiang Li, Xintian Wu, Muhammad Usman, Rentuo Tao, Pengfei Xia,\n  Huanhuan Chen, Bin Li", "title": "A Systematic Survey of Regularization and Normalization in GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have been widely applied in different\nscenarios thanks to the development of deep neural networks. The original GAN\nwas proposed based on the non-parametric assumption of the infinite capacity of\nnetworks. However, it is still unknown whether GANs can generate realistic\nsamples without any prior information. Due to the overconfident assumption,\nmany issues remain unaddressed in GANs' training, such as non-convergence, mode\ncollapses, gradient vanishing. Regularization and normalization are common\nmethods of introducing prior information to stabilize training and improve\ndiscrimination. Although a handful number of regularization and normalization\nmethods have been proposed for GANs, to the best of our knowledge, there exists\nno comprehensive survey which primarily focuses on objectives and development\nof these methods, apart from some in-comprehensive and limited scope studies.\nIn this work, we conduct a comprehensive survey on the regularization and\nnormalization techniques from different perspectives of GANs training. First,\nwe systematically describe different perspectives of GANs training and thus\nobtain the different objectives of regularization and normalization. Based on\nthese objectives, we propose a new taxonomy. Furthermore, we compare the\nperformance of the mainstream methods on different datasets and investigate the\nregularization and normalization techniques that have been frequently employed\nin SOTA GANs. Finally, we highlight potential future directions of research in\nthis domain.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 12:52:10 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 03:15:54 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2020 02:06:58 GMT"}, {"version": "v4", "created": "Sat, 29 May 2021 15:52:22 GMT"}, {"version": "v5", "created": "Mon, 21 Jun 2021 07:46:30 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Li", "Ziqiang", ""], ["Wu", "Xintian", ""], ["Usman", "Muhammad", ""], ["Tao", "Rentuo", ""], ["Xia", "Pengfei", ""], ["Chen", "Huanhuan", ""], ["Li", "Bin", ""]]}, {"id": "2008.08944", "submitter": "Hui Lv", "authors": "Hui Lv, Chuanwei Zhou, Chunyan Xu, Zhen Cui, Jian Yang", "title": "Localizing Anomalies from Weakly-Labeled Videos", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2021.3072863", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video anomaly detection under video-level labels is currently a challenging\ntask. Previous works have made progresses on discriminating whether a video\nsequencecontains anomalies. However, most of them fail to accurately localize\nthe anomalous events within videos in the temporal domain. In this paper, we\npropose a Weakly Supervised Anomaly Localization (WSAL) method focusing on\ntemporally localizing anomalous segments within anomalous videos. Inspired by\nthe appearance difference in anomalous videos, the evolution of adjacent\ntemporal segments is evaluated for the localization of anomalous segments. To\nthis end, a high-order context encoding model is proposed to not only extract\nsemantic representations but also measure the dynamic variations so that the\ntemporal context could be effectively utilized. In addition, in order to fully\nutilize the spatial context information, the immediate semantics are directly\nderived from the segment representations. The dynamic variations as well as the\nimmediate semantics, are efficiently aggregated to obtain the final anomaly\nscores. An enhancement strategy is further proposed to deal with noise\ninterference and the absence of localization guidance in anomaly detection.\nMoreover, to facilitate the diversity requirement for anomaly detection\nbenchmarks, we also collect a new traffic anomaly (TAD) dataset which specifies\nin the traffic conditions, differing greatly from the current popular anomaly\ndetection evaluation benchmarks.Extensive experiments are conducted to verify\nthe effectiveness of different components, and our proposed method achieves new\nstate-of-the-art performance on the UCF-Crime and TAD datasets.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 12:58:03 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 08:14:02 GMT"}, {"version": "v3", "created": "Wed, 14 Apr 2021 11:51:29 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Lv", "Hui", ""], ["Zhou", "Chuanwei", ""], ["Xu", "Chunyan", ""], ["Cui", "Zhen", ""], ["Yang", "Jian", ""]]}, {"id": "2008.08946", "submitter": "Wangbin Ding", "authors": "Wangbin Ding, Lei Li, Xiahai Zhuang, Liqin Huang", "title": "Cross-Modality Multi-Atlas Segmentation Using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both image registration and label fusion in the multi-atlas segmentation\n(MAS) rely on the intensity similarity between target and atlas images.\nHowever, such similarity can be problematic when target and atlas images are\nacquired using different imaging protocols. High-level structure information\ncan provide reliable similarity measurement for cross-modality images when\ncooperating with deep neural networks (DNNs). This work presents a new MAS\nframework for cross-modality images, where both image registration and label\nfusion are achieved by DNNs. For image registration, we propose a consistent\nregistration network, which can jointly estimate forward and backward dense\ndisplacement fields (DDFs). Additionally, an invertible constraint is employed\nin the network to reduce the correspondence ambiguity of the estimated DDFs.\nFor label fusion, we adapt a few-shot learning network to measure the\nsimilarity of atlas and target patches. Moreover, the network can be seamlessly\nintegrated into the patch-based label fusion. The proposed framework is\nevaluated on the MM-WHS dataset of MICCAI 2017. Results show that the framework\nis effective in both cross-modality registration and segmentation.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 02:57:23 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Ding", "Wangbin", ""], ["Li", "Lei", ""], ["Zhuang", "Xiahai", ""], ["Huang", "Liqin", ""]]}, {"id": "2008.08956", "submitter": "Manikandan Ravikiran", "authors": "Siddharth Vohra, Manikandan Ravikiran", "title": "Investigating the Effect of Intraclass Variability in Temporal\n  Ensembling", "comments": "Preliminary Results; More Experiments to be added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal Ensembling is a semi-supervised approach that allows training deep\nneural network models with a small number of labeled images. In this paper, we\npresent our preliminary study on the effect of intraclass variability on\ntemporal ensembling, with a focus on seed size and seed type, respectively.\nThrough our experiments we find that (a) there is a significant drop in\naccuracy with datasets that offer high intraclass variability, (b) more seed\nimages offer consistently higher accuracy across the datasets, and (c) seed\ntype indeed has an impact on the overall efficiency, where it produces a\nspectrum of accuracy both lower and higher. Additionally, based on our\nexperiments, we also find KMNIST to be a competitive baseline for temporal\nensembling.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 13:24:51 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 09:12:55 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Vohra", "Siddharth", ""], ["Ravikiran", "Manikandan", ""]]}, {"id": "2008.08974", "submitter": "Jiaming Zhang", "authors": "Jiaming Zhang, Kailun Yang and Rainer Stiefelhagen", "title": "ISSAFE: Improving Semantic Segmentation in Accidents by Fusing\n  Event-based Data", "comments": "8 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To bring autonomous vehicles closer to real-world applications, a major task\nis to ensure the safety of all traffic participants. In addition to the high\naccuracy under controlled conditions, the assistance system is still supposed\nto obtain robust perception against extreme situations, especially in accident\nscenarios, which involve object collisions, deformations, overturns, etc.\nHowever, models trained on common datasets may suffer from a large performance\ndegradation when applied in these challenging scenes. To tackle this issue, we\npresent a rarely addressed task regarding semantic segmentation in accident\nscenarios, along with an associated large-scale dataset DADA-seg. Our dataset\ncontains 313 sequences with 40 frames each, of which the time windows are\nlocated before and during a traffic accident. For benchmarking the segmentation\nperformance, every 11th frame is manually annotated with reference to\nCityscapes. Furthermore, we propose a novel event-based multi-modal\nsegmentation architecture ISSAFE. Our experiments indicate that event-based\ndata can provide complementary information to stabilize semantic segmentation\nunder adverse conditions by preserving fine-grain motion of fast-moving\nforeground (crash objects) in accidents. Compared with state-of-the-art models,\nour approach achieves 30.0% mIoU with 9.9% performance gain on the proposed\nevaluation set.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 14:03:34 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Zhang", "Jiaming", ""], ["Yang", "Kailun", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "2008.08976", "submitter": "Naitik Bhise", "authors": "Naitik Bhise, Zhenfei Zhang, Tien D. Bui", "title": "Improving Text to Image Generation using Mode-seeking Function", "comments": "changes : changed the title of the research for submission to CVIU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have long been used to understand the\nsemantic relationship between the text and image. However, there are problems\nwith mode collapsing in the image generation that causes some preferred output\nmodes. Our aim is to improve the training of the network by using a specialized\nmode-seeking loss function to avoid this issue. In the text to image synthesis,\nour loss function differentiates two points in latent space for the generation\nof distinct images. We validate our model on the Caltech Birds (CUB) dataset\nand the Microsoft COCO dataset by changing the intensity of the loss function\nduring the training. Experimental results demonstrate that our model works very\nwell compared to some state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 12:58:32 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 21:52:21 GMT"}, {"version": "v3", "created": "Fri, 4 Sep 2020 14:08:31 GMT"}, {"version": "v4", "created": "Fri, 18 Sep 2020 20:00:56 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Bhise", "Naitik", ""], ["Zhang", "Zhenfei", ""], ["Bui", "Tien D.", ""]]}, {"id": "2008.08977", "submitter": "MingFei Wang", "authors": "Yuan Zhou, Mingfei Wang, Ruolin Wang, Shuwei Huo", "title": "Generating Adjacency Matrix for Video-Query based Video Moment Retrieval", "comments": "arXiv admin note: substantial text overlap with arXiv:2007.09877", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we continue our work on Video-Query based Video Moment\nretrieval task. Based on using graph convolution to extract intra-video and\ninter-video frame features, we improve the method by using similarity-metric\nbased graph convolution, whose weighted adjacency matrix is achieved by\ncalculating similarity metric between features of any two different timesteps\nin the graph. Experiments on ActivityNet v1.2 and Thumos14 dataset shows the\neffectiveness of this improvement, and it outperforms the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 13:52:36 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Zhou", "Yuan", ""], ["Wang", "Mingfei", ""], ["Wang", "Ruolin", ""], ["Huo", "Shuwei", ""]]}, {"id": "2008.08999", "submitter": "Qian Zheng", "authors": "Qian Zheng, Weikai Wu, Hanting Pan, Niloy Mitra, Daniel Cohen-Or, Hui\n  Huang", "title": "Object Properties Inferring from and Transfer for Human Interaction\n  Motions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans regularly interact with their surrounding objects. Such interactions\noften result in strongly correlated motion between humans and the interacting\nobjects. We thus ask: \"Is it possible to infer object properties from skeletal\nmotion alone, even without seeing the interacting object itself?\" In this\npaper, we present a fine-grained action recognition method that learns to infer\nsuch latent object properties from human interaction motion alone. This\ninference allows us to disentangle the motion from the object property and\ntransfer object properties to a given motion. We collected a large number of\nvideos and 3D skeletal motions of the performing actors using an inertial\nmotion capture device. We analyze similar actions and learn subtle differences\namong them to reveal latent properties of the interacting objects. In\nparticular, we learn to identify the interacting object, by estimating its\nweight, or its fragility or delicacy. Our results clearly demonstrate that the\ninteraction motions and interacting objects are highly correlated and indeed\nrelative object latent properties can be inferred from the 3D skeleton\nsequences alone, leading to new synthesis possibilities for human interaction\nmotions. Dataset will be available at http://vcc.szu.edu.cn/research/2020/IT.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 14:36:34 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Zheng", "Qian", ""], ["Wu", "Weikai", ""], ["Pan", "Hanting", ""], ["Mitra", "Niloy", ""], ["Cohen-Or", "Daniel", ""], ["Huang", "Hui", ""]]}, {"id": "2008.09010", "submitter": "Marco Maggipinto", "authors": "Marco Maggipinto and Matteo Terzi and Gian Antonio Susto", "title": "$\\beta$-Variational Classifiers Under Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural networks have gained lots of attention in recent years thanks to\nthe breakthroughs obtained in the field of Computer Vision. However, despite\ntheir popularity, it has been shown that they provide limited robustness in\ntheir predictions. In particular, it is possible to synthesise small\nadversarial perturbations that imperceptibly modify a correctly classified\ninput data, making the network confidently misclassify it. This has led to a\nplethora of different methods to try to improve robustness or detect the\npresence of these perturbations. In this paper, we perform an analysis of\n$\\beta$-Variational Classifiers, a particular class of methods that not only\nsolve a specific classification task, but also provide a generative component\nthat is able to generate new samples from the input distribution. More in\ndetails, we study their robustness and detection capabilities, together with\nsome novel insights on the generative part of the model.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 14:57:22 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Maggipinto", "Marco", ""], ["Terzi", "Matteo", ""], ["Susto", "Gian Antonio", ""]]}, {"id": "2008.09037", "submitter": "Matthew Hutchinson", "authors": "Matthew Hutchinson, Siddharth Samsi, William Arcand, David Bestor,\n  Bill Bergeron, Chansup Byun, Micheal Houle, Matthew Hubbell, Micheal Jones,\n  Jeremy Kepner, Andrew Kirby, Peter Michaleas, Lauren Milechin, Julie Mullen,\n  Andrew Prout, Antonio Rosa, Albert Reuther, Charles Yee, Vijay Gadepally", "title": "Accuracy and Performance Comparison of Video Action Recognition\n  Approaches", "comments": "Accepted for publication at IEEE HPEC 2020", "journal-ref": null, "doi": "10.1109/HPEC43674.2020.9286249", "report-no": null, "categories": "cs.CV cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, there has been significant interest in video action\nrecognition systems and models. However, direct comparison of accuracy and\ncomputational performance results remain clouded by differing training\nenvironments, hardware specifications, hyperparameters, pipelines, and\ninference methods. This article provides a direct comparison between fourteen\noff-the-shelf and state-of-the-art models by ensuring consistency in these\ntraining characteristics in order to provide readers with a meaningful\ncomparison across different types of video action recognition algorithms.\nAccuracy of the models is evaluated using standard Top-1 and Top-5 accuracy\nmetrics in addition to a proposed new accuracy metric. Additionally, we compare\ncomputational performance of distributed training from two to sixty-four GPUs\non a state-of-the-art HPC system.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 15:42:37 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Hutchinson", "Matthew", ""], ["Samsi", "Siddharth", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "Bill", ""], ["Byun", "Chansup", ""], ["Houle", "Micheal", ""], ["Hubbell", "Matthew", ""], ["Jones", "Micheal", ""], ["Kepner", "Jeremy", ""], ["Kirby", "Andrew", ""], ["Michaleas", "Peter", ""], ["Milechin", "Lauren", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Reuther", "Albert", ""], ["Yee", "Charles", ""], ["Gadepally", "Vijay", ""]]}, {"id": "2008.09041", "submitter": "Ziqiang Li", "authors": "Ziqiang Li, Pengfei Xia, Rentuo Tao, Hongjing Niu, Bin Li", "title": "Direct Adversarial Training: An Adaptive Method to Penalize Lipschitz\n  Continuity of the Discriminator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are the most popular image generation\nmodels that have achieved remarkable performance on various tasks. However,\ntraining instability is still one of the open problems for all GAN-based\nalgorithms. In order to stabilize GANs training, some regularization and\nnormalization techniques have been proposed to make the discriminator meet the\nLipschitz continuity. In this paper, a new approach inspired by works on\nadversarial is proposed to stabilize the training process of GANs. It is found\nthat sometimes the images generated by the generator play a role just like\nadversarial examples for discriminator during the training process, which might\nbe a part of the reason for the unstable training of GANs. With this discovery,\nwe propose a Direct Adversarial Training (DAT) method for the training process\nof GANs to improve its performance. We prove that the DAT method can minimize\nthe Lipschitz constant of the discriminator adaptively. The advanced\nperformance of the proposed method is verified on multiple baseline and SOTA\nnetworks, such as DCGAN, Spectral Normalization GAN, Self-supervised GAN, and\nInformation Maximum GAN. Code will be available at\n\\url{https://github.com/iceli1007/DAT-GAN}\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 02:36:53 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 07:45:34 GMT"}, {"version": "v3", "created": "Fri, 15 Jan 2021 07:49:51 GMT"}, {"version": "v4", "created": "Thu, 6 May 2021 03:29:53 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Li", "Ziqiang", ""], ["Xia", "Pengfei", ""], ["Tao", "Rentuo", ""], ["Niu", "Hongjing", ""], ["Li", "Bin", ""]]}, {"id": "2008.09047", "submitter": "Hongsuk Choi", "authors": "Hongsuk Choi, Gyeongsik Moon, Kyoung Mu Lee", "title": "Pose2Mesh: Graph Convolutional Network for 3D Human Pose and Mesh\n  Recovery from a 2D Human Pose", "comments": "Accepted to ECCV 2020, 22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Most of the recent deep learning-based 3D human pose and mesh estimation\nmethods regress the pose and shape parameters of human mesh models, such as\nSMPL and MANO, from an input image. The first weakness of these methods is an\nappearance domain gap problem, due to different image appearance between train\ndata from controlled environments, such as a laboratory, and test data from\nin-the-wild environments. The second weakness is that the estimation of the\npose parameters is quite challenging owing to the representation issues of 3D\nrotations. To overcome the above weaknesses, we propose Pose2Mesh, a novel\ngraph convolutional neural network (GraphCNN)-based system that estimates the\n3D coordinates of human mesh vertices directly from the 2D human pose. The 2D\nhuman pose as input provides essential human body articulation information,\nwhile having a relatively homogeneous geometric property between the two\ndomains. Also, the proposed system avoids the representation issues, while\nfully exploiting the mesh topology using a GraphCNN in a coarse-to-fine manner.\nWe show that our Pose2Mesh outperforms the previous 3D human pose and mesh\nestimation methods on various benchmark datasets. For the codes, see\nhttps://github.com/hongsukchoi/Pose2Mesh_RELEASE.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 16:01:56 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 09:34:49 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 08:48:41 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Choi", "Hongsuk", ""], ["Moon", "Gyeongsik", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "2008.09062", "submitter": "Vasileios Choutas", "authors": "Vasileios Choutas, Georgios Pavlakos, Timo Bolkart, Dimitrios Tzionas,\n  Michael J. Black", "title": "Monocular Expressive Body Regression through Body-Driven Attention", "comments": "Accepted in ECCV'20. Project page: http://expose.is.tue.mpg.de", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand how people look, interact, or perform tasks, we need to quickly\nand accurately capture their 3D body, face, and hands together from an RGB\nimage. Most existing methods focus only on parts of the body. A few recent\napproaches reconstruct full expressive 3D humans from images using 3D body\nmodels that include the face and hands. These methods are optimization-based\nand thus slow, prone to local optima, and require 2D keypoints as input. We\naddress these limitations by introducing ExPose (EXpressive POse and Shape\nrEgression), which directly regresses the body, face, and hands, in SMPL-X\nformat, from an RGB image. This is a hard problem due to the high\ndimensionality of the body and the lack of expressive training data.\nAdditionally, hands and faces are much smaller than the body, occupying very\nfew image pixels. This makes hand and face estimation hard when body images are\ndownscaled for neural networks. We make three main contributions. First, we\naccount for the lack of training data by curating a dataset of SMPL-X fits on\nin-the-wild images. Second, we observe that body estimation localizes the face\nand hands reasonably well. We introduce body-driven attention for face and hand\nregions in the original image to extract higher-resolution crops that are fed\nto dedicated refinement modules. Third, these modules exploit part-specific\nknowledge from existing face- and hand-only datasets. ExPose estimates\nexpressive 3D humans more accurately than existing optimization methods at a\nsmall fraction of the computational cost. Our data, model and code are\navailable for research at https://expose.is.tue.mpg.de .\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 16:33:47 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Choutas", "Vasileios", ""], ["Pavlakos", "Georgios", ""], ["Bolkart", "Timo", ""], ["Tzionas", "Dimitrios", ""], ["Black", "Michael J.", ""]]}, {"id": "2008.09072", "submitter": "Muhammad Sabih", "authors": "Muhammad Sabih, Frank Hannig and Juergen Teich", "title": "Utilizing Explainable AI for Quantization and Pruning of Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many applications, utilizing DNNs (Deep Neural Networks) requires their\nimplementation on a target architecture in an optimized manner concerning\nenergy consumption, memory requirement, throughput, etc. DNN compression is\nused to reduce the memory footprint and complexity of a DNN before its\ndeployment on hardware. Recent efforts to understand and explain AI (Artificial\nIntelligence) methods have led to a new research area, termed as explainable\nAI. Explainable AI methods allow us to understand better the inner working of\nDNNs, such as the importance of different neurons and features. The concepts\nfrom explainable AI provide an opportunity to improve DNN compression methods\nsuch as quantization and pruning in several ways that have not been\nsufficiently explored so far. In this paper, we utilize explainable AI methods:\nmainly DeepLIFT method. We use these methods for (1) pruning of DNNs; this\nincludes structured and unstructured pruning of \\ac{CNN} filters pruning as\nwell as pruning weights of fully connected layers, (2) non-uniform quantization\nof DNN weights using clustering algorithm; this is also referred to as Weight\nSharing, and (3) integer-based mixed-precision quantization; this is where each\nlayer of a DNN may use a different number of integer bits. We use typical image\nclassification datasets with common deep learning image classification models\nfor evaluation. In all these three cases, we demonstrate significant\nimprovements as well as new insights and opportunities from the use of\nexplainable AI in DNN compression.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 16:52:58 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Sabih", "Muhammad", ""], ["Hannig", "Frank", ""], ["Teich", "Juergen", ""]]}, {"id": "2008.09088", "submitter": "Wentao Yuan", "authors": "Wentao Yuan, Ben Eckart, Kihwan Kim, Varun Jampani, Dieter Fox, Jan\n  Kautz", "title": "DeepGMR: Learning Latent Gaussian Mixture Models for Registration", "comments": "ECCV 2020 spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud registration is a fundamental problem in 3D computer vision,\ngraphics and robotics. For the last few decades, existing registration\nalgorithms have struggled in situations with large transformations, noise, and\ntime constraints. In this paper, we introduce Deep Gaussian Mixture\nRegistration (DeepGMR), the first learning-based registration method that\nexplicitly leverages a probabilistic registration paradigm by formulating\nregistration as the minimization of KL-divergence between two probability\ndistributions modeled as mixtures of Gaussians. We design a neural network that\nextracts pose-invariant correspondences between raw point clouds and Gaussian\nMixture Model (GMM) parameters and two differentiable compute blocks that\nrecover the optimal transformation from matched GMM parameters. This\nconstruction allows the network learn an SE(3)-invariant feature space,\nproducing a global registration method that is real-time, generalizable, and\nrobust to noise. Across synthetic and real-world data, our proposed method\nshows favorable performance when compared with state-of-the-art geometry-based\nand learning-based registration methods.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 17:25:16 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Yuan", "Wentao", ""], ["Eckart", "Ben", ""], ["Kim", "Kihwan", ""], ["Jampani", "Varun", ""], ["Fox", "Dieter", ""], ["Kautz", "Jan", ""]]}, {"id": "2008.09092", "submitter": "Amlan Kar", "authors": "Jeevan Devaranjan, Amlan Kar, Sanja Fidler", "title": "Meta-Sim2: Unsupervised Learning of Scene Structure for Synthetic Data\n  Generation", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Procedural models are being widely used to synthesize scenes for graphics,\ngaming, and to create (labeled) synthetic datasets for ML. In order to produce\nrealistic and diverse scenes, a number of parameters governing the procedural\nmodels have to be carefully tuned by experts. These parameters control both the\nstructure of scenes being generated (e.g. how many cars in the scene), as well\nas parameters which place objects in valid configurations. Meta-Sim aimed at\nautomatically tuning parameters given a target collection of real images in an\nunsupervised way. In Meta-Sim2, we aim to learn the scene structure in addition\nto parameters, which is a challenging problem due to its discrete nature.\nMeta-Sim2 proceeds by learning to sequentially sample rule expansions from a\ngiven probabilistic scene grammar. Due to the discrete nature of the problem,\nwe use Reinforcement Learning to train our model, and design a feature space\ndivergence between our synthesized and target images that is key to successful\ntraining. Experiments on a real driving dataset show that, without any\nsupervision, we can successfully learn to generate data that captures discrete\nstructural statistics of objects, such as their frequency, in real images. We\nalso show that this leads to downstream improvement in the performance of an\nobject detector trained on our generated dataset as opposed to other baseline\nsimulation methods. Project page:\nhttps://nv-tlabs.github.io/meta-sim-structure/.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 17:28:45 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Devaranjan", "Jeevan", ""], ["Kar", "Amlan", ""], ["Fidler", "Sanja", ""]]}, {"id": "2008.09103", "submitter": "Chenglizhao Chen", "authors": "Yunxiao Li, Shuai Li, Chenglizhao Chen, Aimin Hao, Hong Qin", "title": "A Plug-and-play Scheme to Adapt Image Saliency Deep Model for Video Data", "comments": "12 pages, 10 figures, and, this paper is currently in peer review in\n  IEEE TCSVT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of deep learning techniques, image saliency deep\nmodels trained solely by spatial information have occasionally achieved\ndetection performance for video data comparable to that of the models trained\nby both spatial and temporal information. However, due to the lesser\nconsideration of temporal information, the image saliency deep models may\nbecome fragile in the video sequences dominated by temporal information. Thus,\nthe most recent video saliency detection approaches have adopted the network\narchitecture starting with a spatial deep model that is followed by an\nelaborately designed temporal deep model. However, such methods easily\nencounter the performance bottleneck arising from the single stream learning\nmethodology, so the overall detection performance is largely determined by the\nspatial deep model. In sharp contrast to the current mainstream methods, this\npaper proposes a novel plug-and-play scheme to weakly retrain a pretrained\nimage saliency deep model for video data by using the newly sensed and coded\ntemporal information. Thus, the retrained image saliency deep model will be\nable to maintain temporal saliency awareness, achieving much improved detection\nperformance. Moreover, our method is simple yet effective for adapting any\noff-the-shelf pre-trained image saliency deep model to obtain high-quality\nvideo saliency detection. Additionally, both the data and source code of our\nmethod are publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 13:23:14 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Li", "Yunxiao", ""], ["Li", "Shuai", ""], ["Chen", "Chenglizhao", ""], ["Hao", "Aimin", ""], ["Qin", "Hong", ""]]}, {"id": "2008.09104", "submitter": "S. Kevin Zhou", "authors": "S. Kevin Zhou, Hayit Greenspan, Christos Davatzikos, James S. Duncan,\n  Bram van Ginneken, Anant Madabhushi, Jerry L. Prince, Daniel Rueckert, Ronald\n  M. Summers", "title": "A review of deep learning in medical imaging: Imaging traits, technology\n  trends, case studies with progress highlights, and future promises", "comments": "20 pages, 7 figures", "journal-ref": "Proceedings of the IEEE (2021)", "doi": "10.1109/JPROC.2021.3054390", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its renaissance, deep learning has been widely used in various medical\nimaging tasks and has achieved remarkable success in many medical imaging\napplications, thereby propelling us into the so-called artificial intelligence\n(AI) era. It is known that the success of AI is mostly attributed to the\navailability of big data with annotations for a single task and the advances in\nhigh performance computing. However, medical imaging presents unique challenges\nthat confront deep learning approaches. In this survey paper, we first present\ntraits of medical imaging, highlight both clinical needs and technical\nchallenges in medical imaging, and describe how emerging trends in deep\nlearning are addressing these issues. We cover the topics of network\narchitecture, sparse and noisy labels, federating learning, interpretability,\nuncertainty quantification, etc. Then, we present several case studies that are\ncommonly found in clinical practice, including digital pathology and chest,\nbrain, cardiovascular, and abdominal imaging. Rather than presenting an\nexhaustive literature survey, we instead describe some prominent research\nhighlights related to these case study applications. We conclude with a\ndiscussion and presentation of promising future directions.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 14:26:13 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 13:17:05 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Zhou", "S. Kevin", ""], ["Greenspan", "Hayit", ""], ["Davatzikos", "Christos", ""], ["Duncan", "James S.", ""], ["van Ginneken", "Bram", ""], ["Madabhushi", "Anant", ""], ["Prince", "Jerry L.", ""], ["Rueckert", "Daniel", ""], ["Summers", "Ronald M.", ""]]}, {"id": "2008.09105", "submitter": "Mingkui Tan", "authors": "Deng Huang, Peihao Chen, Runhao Zeng, Qing Du, Mingkui Tan, Chuang Gan", "title": "Location-aware Graph Convolutional Networks for Video Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We addressed the challenging task of video question answering, which requires\nmachines to answer questions about videos in a natural language form. Previous\nstate-of-the-art methods attempt to apply spatio-temporal attention mechanism\non video frame features without explicitly modeling the location and relations\namong object interaction occurred in videos. However, the relations between\nobject interaction and their location information are very critical for both\naction recognition and question reasoning. In this work, we propose to\nrepresent the contents in the video as a location-aware graph by incorporating\nthe location information of an object into the graph construction. Here, each\nnode is associated with an object represented by its appearance and location\nfeatures. Based on the constructed graph, we propose to use graph convolution\nto infer both the category and temporal locations of an action. As the graph is\nbuilt on objects, our method is able to focus on the foreground action contents\nfor better video question answering. Lastly, we leverage an attention mechanism\nto combine the output of graph convolution and encoded question features for\nfinal answer reasoning. Extensive experiments demonstrate the effectiveness of\nthe proposed methods. Specifically, our method significantly outperforms\nstate-of-the-art methods on TGIF-QA, Youtube2Text-QA, and MSVD-QA datasets.\nCode and pre-trained models are publicly available at:\nhttps://github.com/SunDoge/L-GCN\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 02:12:56 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Huang", "Deng", ""], ["Chen", "Peihao", ""], ["Zeng", "Runhao", ""], ["Du", "Qing", ""], ["Tan", "Mingkui", ""], ["Gan", "Chuang", ""]]}, {"id": "2008.09106", "submitter": "Tewodros Habtegebrial", "authors": "Tewodros Habtegebrial, Varun Jampani, Orazio Gallo, Didier Stricker", "title": "Generative View Synthesis: From Single-view Semantics to Novel-view\n  Images", "comments": "Accepted at Neurips-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content creation, central to applications such as virtual reality, can be a\ntedious and time-consuming. Recent image synthesis methods simplify this task\nby offering tools to generate new views from as little as a single input image,\nor by converting a semantic map into a photorealistic image. We propose to push\nthe envelope further, and introduce Generative View Synthesis (GVS), which can\nsynthesize multiple photorealistic views of a scene given a single semantic\nmap. We show that the sequential application of existing techniques, e.g.,\nsemantics-to-image translation followed by monocular view synthesis, fail at\ncapturing the scene's structure. In contrast, we solve the semantics-to-image\ntranslation in concert with the estimation of the 3D layout of the scene, thus\nproducing geometrically consistent novel views that preserve semantic\nstructures. We first lift the input 2D semantic map onto a 3D layered\nrepresentation of the scene in feature space, thereby preserving the semantic\nlabels of 3D geometric structures. We then project the layered features onto\nthe target views to generate the final novel-view images. We verify the\nstrengths of our method and compare it with several advanced baselines on three\ndifferent datasets. Our approach also allows for style manipulation and image\nediting operations, such as the addition or removal of objects, with simple\nmanipulations of the input style images and semantic maps respectively. Visit\nthe project page at https://gvsnet.github.io.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 17:48:16 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 12:09:09 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Habtegebrial", "Tewodros", ""], ["Jampani", "Varun", ""], ["Gallo", "Orazio", ""], ["Stricker", "Didier", ""]]}, {"id": "2008.09110", "submitter": "Jiayuan Gu", "authors": "Jiayuan Gu, Wei-Chiu Ma, Sivabalan Manivasagam, Wenyuan Zeng, Zihao\n  Wang, Yuwen Xiong, Hao Su, Raquel Urtasun", "title": "Weakly-supervised 3D Shape Completion in the Wild", "comments": "Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D shape completion for real data is important but challenging, since partial\npoint clouds acquired by real-world sensors are usually sparse, noisy and\nunaligned. Different from previous methods, we address the problem of learning\n3D complete shape from unaligned and real-world partial point clouds. To this\nend, we propose a weakly-supervised method to estimate both 3D canonical shape\nand 6-DoF pose for alignment, given multiple partial observations associated\nwith the same instance. The network jointly optimizes canonical shapes and\nposes with multi-view geometry constraints during training, and can infer the\ncomplete shape given a single partial point cloud. Moreover, learned pose\nestimation can facilitate partial point cloud registration. Experiments on both\nsynthetic and real data show that it is feasible and promising to learn 3D\nshape completion through large-scale data without shape and pose supervision.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 17:53:42 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Gu", "Jiayuan", ""], ["Ma", "Wei-Chiu", ""], ["Manivasagam", "Sivabalan", ""], ["Zeng", "Wenyuan", ""], ["Wang", "Zihao", ""], ["Xiong", "Yuwen", ""], ["Su", "Hao", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2008.09150", "submitter": "Iacer Calixto", "authors": "Houda Alberts, Teresa Huang, Yash Deshpande, Yibo Liu, Kyunghyun Cho,\n  Clara Vania, Iacer Calixto", "title": "VisualSem: a high-quality knowledge graph for vision and language", "comments": "11 pages, 5 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue that the next frontier in natural language understanding (NLU) and\ngeneration (NLG) will include models that can efficiently access external\nstructured knowledge repositories. In order to support the development of such\nmodels, we release the VisualSem knowledge graph (KG) which includes nodes with\nmultilingual glosses and multiple illustrative images and visually relevant\nrelations. We also release a neural multi-modal retrieval model that can use\nimages or sentences as inputs and retrieves entities in the KG. This\nmulti-modal retrieval model can be integrated into any (neural network) model\npipeline and we encourage the research community to use VisualSem for data\naugmentation and/or as a source of grounding, among other possible uses.\nVisualSem as well as the multi-modal retrieval model are publicly available and\ncan be downloaded in: https://github.com/iacercalixto/visualsem.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 18:20:29 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Alberts", "Houda", ""], ["Huang", "Teresa", ""], ["Deshpande", "Yash", ""], ["Liu", "Yibo", ""], ["Cho", "Kyunghyun", ""], ["Vania", "Clara", ""], ["Calixto", "Iacer", ""]]}, {"id": "2008.09152", "submitter": "Iacer Calixto", "authors": "Houda Alberts and Iacer Calixto", "title": "ImagiFilter: A resource to enable the semi-automatic mining of images at\n  scale", "comments": "10 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets (semi-)automatically collected from the web can easily scale to\nmillions of entries, but a dataset's usefulness is directly related to how\nclean and high-quality its examples are. In this paper, we describe and\npublicly release an image dataset along with pretrained models designed to\n(semi-)automatically filter out undesirable images from very large image\ncollections, possibly obtained from the web. Our dataset focusses on\nphotographic and/or natural images, a very common use-case in computer vision\nresearch. We provide annotations for coarse prediction, i.e. photographic vs.\nnon-photographic, and smaller fine-grained prediction tasks where we further\nbreak down the non-photographic class into five classes: maps, drawings,\ngraphs, icons, and sketches. Results on held out validation data show that a\nmodel architecture with reduced memory footprint achieves over 96% accuracy on\ncoarse-prediction. Our best model achieves 88% accuracy on the hardest\nfine-grained classification task available. Dataset and pretrained models are\navailable at: https://github.com/houda96/imagi-filter.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 18:31:52 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Alberts", "Houda", ""], ["Calixto", "Iacer", ""]]}, {"id": "2008.09154", "submitter": "Athanasios Vlontzos", "authors": "Athanasios Vlontzos, Henrique Bergallo Rocha, Daniel Rueckert,\n  Bernhard Kainz", "title": "Causal Future Prediction in a Minkowski Space-Time", "comments": "Includes supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating future events is a difficult task. Unlike humans, machine learning\napproaches are not regularized by a natural understanding of physics. In the\nwild, a plausible succession of events is governed by the rules of causality,\nwhich cannot easily be derived from a finite training set. In this paper we\npropose a novel theoretical framework to perform causal future prediction by\nembedding spatiotemporal information on a Minkowski space-time. We utilize the\nconcept of a light cone from special relativity to restrict and traverse the\nlatent space of an arbitrary model. We demonstrate successful applications in\ncausal image synthesis and future video frame prediction on a dataset of\nimages. Our framework is architecture- and task-independent and comes with\nstrong theoretical guarantees of causal capabilities.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 18:45:55 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 17:08:17 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Vlontzos", "Athanasios", ""], ["Rocha", "Henrique Bergallo", ""], ["Rueckert", "Daniel", ""], ["Kainz", "Bernhard", ""]]}, {"id": "2008.09162", "submitter": "Shi-Jie Li", "authors": "Shijie Li, Xieyuanli Chen, Yun Liu, Dengxin Dai, Cyrill Stachniss,\n  Juergen Gall", "title": "Multi-scale Interaction for Real-time LiDAR Data Segmentation on an\n  Embedded Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time semantic segmentation of LiDAR data is crucial for autonomously\ndriving vehicles, which are usually equipped with an embedded platform and have\nlimited computational resources. Approaches that operate directly on the point\ncloud use complex spatial aggregation operations, which are very expensive and\ndifficult to optimize for embedded platforms. They are therefore not suitable\nfor real-time applications with embedded systems. As an alternative,\nprojection-based methods are more efficient and can run on embedded platforms.\nHowever, the current state-of-the-art projection-based methods do not achieve\nthe same accuracy as point-based methods and use millions of parameters. In\nthis paper, we therefore propose a projection-based method, called Multi-scale\nInteraction Network (MINet), which is very efficient and accurate. The network\nuses multiple paths with different scales and balances the computational\nresources between the scales. Additional dense interactions between the scales\navoid redundant computations and make the network highly efficient. The\nproposed network outperforms point-based, image-based, and projection-based\nmethods in terms of accuracy, number of parameters, and runtime. Moreover, the\nnetwork processes more than 24 scans per second on an embedded platform, which\nis higher than the framerates of LiDAR sensors. The network is therefore\nsuitable for autonomous vehicles.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 19:06:11 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Li", "Shijie", ""], ["Chen", "Xieyuanli", ""], ["Liu", "Yun", ""], ["Dai", "Dengxin", ""], ["Stachniss", "Cyrill", ""], ["Gall", "Juergen", ""]]}, {"id": "2008.09164", "submitter": "Kevin Musgrave", "authors": "Kevin Musgrave, Serge Belongie, Ser-Nam Lim", "title": "PyTorch Metric Learning", "comments": "Code and documentation is available at\n  https://www.github.com/KevinMusgrave/pytorch-metric-learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep metric learning algorithms have a wide variety of applications, but\nimplementing these algorithms can be tedious and time consuming. PyTorch Metric\nLearning is an open source library that aims to remove this barrier for both\nresearchers and practitioners. The modular and flexible design allows users to\neasily try out different combinations of algorithms in their existing code. It\nalso comes with complete train/test workflows, for users who want results fast.\nCode and documentation is available at\nhttps://www.github.com/KevinMusgrave/pytorch-metric-learning.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 19:08:56 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Musgrave", "Kevin", ""], ["Belongie", "Serge", ""], ["Lim", "Ser-Nam", ""]]}, {"id": "2008.09180", "submitter": "Jerry Liu", "authors": "Jerry Liu, Shenlong Wang, Wei-Chiu Ma, Meet Shah, Rui Hu, Pranaab\n  Dhawan, and Raquel Urtasun", "title": "Conditional Entropy Coding for Efficient Video Compression", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a very simple and efficient video compression framework that only\nfocuses on modeling the conditional entropy between frames. Unlike prior\nlearning-based approaches, we reduce complexity by not performing any form of\nexplicit transformations between frames and assume each frame is encoded with\nan independent state-of-the-art deep image compressor. We first show that a\nsimple architecture modeling the entropy between the image latent codes is as\ncompetitive as other neural video compression works and video codecs while\nbeing much faster and easier to implement. We then propose a novel internal\nlearning extension on top of this architecture that brings an additional 10%\nbitrate savings without trading off decoding speed. Importantly, we show that\nour approach outperforms H.265 and other deep learning baselines in MS-SSIM on\nhigher bitrate UVG video, and against all video codecs on lower framerates,\nwhile being thousands of times faster in decoding than deep models utilizing an\nautoregressive entropy model.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 20:01:59 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Liu", "Jerry", ""], ["Wang", "Shenlong", ""], ["Ma", "Wei-Chiu", ""], ["Shah", "Meet", ""], ["Hu", "Rui", ""], ["Dhawan", "Pranaab", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2008.09188", "submitter": "Ethan Weber", "authors": "Ethan Weber, Nuria Marzo, Dim P. Papadopoulos, Aritro Biswas, Agata\n  Lapedriza, Ferda Ofli, Muhammad Imran and Antonio Torralba", "title": "Detecting natural disasters, damage, and incidents in the wild", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Responding to natural disasters, such as earthquakes, floods, and wildfires,\nis a laborious task performed by on-the-ground emergency responders and\nanalysts. Social media has emerged as a low-latency data source to quickly\nunderstand disaster situations. While most studies on social media are limited\nto text, images offer more information for understanding disaster and incident\nscenes. However, no large-scale image datasets for incident detection exists.\nIn this work, we present the Incidents Dataset, which contains 446,684 images\nannotated by humans that cover 43 incidents across a variety of scenes. We\nemploy a baseline classification model that mitigates false-positive errors and\nwe perform image filtering experiments on millions of social media images from\nFlickr and Twitter. Through these experiments, we show how the Incidents\nDataset can be used to detect images with incidents in the wild. Code, data,\nand models are available online at http://incidentsdataset.csail.mit.edu.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 20:09:42 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Weber", "Ethan", ""], ["Marzo", "Nuria", ""], ["Papadopoulos", "Dim P.", ""], ["Biswas", "Aritro", ""], ["Lapedriza", "Agata", ""], ["Ofli", "Ferda", ""], ["Imran", "Muhammad", ""], ["Torralba", "Antonio", ""]]}, {"id": "2008.09194", "submitter": "Baiwu Zhang", "authors": "Baiwu Zhang, Jin Peng Zhou, Ilia Shumailov, Nicolas Papernot", "title": "On Attribution of Deepfakes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progress in generative modelling, especially generative adversarial networks,\nhave made it possible to efficiently synthesize and alter media at scale.\nMalicious individuals now rely on these machine-generated media, or deepfakes,\nto manipulate social discourse. In order to ensure media authenticity, existing\nresearch is focused on deepfake detection. Yet, the adversarial nature of\nframeworks used for generative modeling suggests that progress towards\ndetecting deepfakes will enable more realistic deepfake generation. Therefore,\nit comes at no surprise that developers of generative models are under the\nscrutiny of stakeholders dealing with misinformation campaigns. At the same\ntime, generative models have a lot of positive applications. As such, there is\na clear need to develop tools that ensure the transparent use of generative\nmodeling, while minimizing the harm caused by malicious applications.\n  Our technique optimizes over the source of entropy of each generative model\nto probabilistically attribute a deepfake to one of the models. We evaluate our\nmethod on the seminal example of face synthesis, demonstrating that our\napproach achieves 97.62% attribution accuracy, and is less sensitive to\nperturbations and adversarial examples. We discuss the ethical implications of\nour work, identify where our technique can be used, and highlight that a more\nmeaningful legislative framework is required for a more transparent and ethical\nuse of generative modeling. Finally, we argue that model developers should be\ncapable of claiming plausible deniability and propose a second framework to do\nso -- this allows a model developer to produce evidence that they did not\nproduce media that they are being accused of having produced.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 20:25:18 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 21:41:33 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Zhang", "Baiwu", ""], ["Zhou", "Jin Peng", ""], ["Shumailov", "Ilia", ""], ["Papernot", "Nicolas", ""]]}, {"id": "2008.09228", "submitter": "Linhui Dai", "authors": "Linhui Dai, Xiaohong Liu, Chengqi Li, and Jun Chen", "title": "AWNet: Attentive Wavelet Network for Image ISP", "comments": "17 pages, 6 figures, accepted in ECCVW 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the revolutionary improvement being made on the performance of smartphones\nover the last decade, mobile photography becomes one of the most common\npractices among the majority of smartphone users. However, due to the limited\nsize of camera sensors on phone, the photographed image is still visually\ndistinct to the one taken by the digital single-lens reflex (DSLR) camera. To\nnarrow this performance gap, one is to redesign the camera image signal\nprocessor (ISP) to improve the image quality. Owing to the rapid rise of deep\nlearning, recent works resort to the deep convolutional neural network (CNN) to\ndevelop a sophisticated data-driven ISP that directly maps the phone-captured\nimage to the DSLR-captured one. In this paper, we introduce a novel network\nthat utilizes the attention mechanism and wavelet transform, dubbed AWNet, to\ntackle this learnable image ISP problem. By adding the wavelet transform, our\nproposed method enables us to restore favorable image details from RAW\ninformation and achieve a larger receptive field while remaining high\nefficiency in terms of computational cost. The global context block is adopted\nin our method to learn the non-local color mapping for the generation of\nappealing RGB images. More importantly, this block alleviates the influence of\nimage misalignment occurred on the provided dataset. Experimental results\nindicate the advances of our design in both qualitative and quantitative\nmeasurements. The code is available publically.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 23:28:41 GMT"}, {"version": "v2", "created": "Sun, 13 Sep 2020 17:38:47 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Dai", "Linhui", ""], ["Liu", "Xiaohong", ""], ["Li", "Chengqi", ""], ["Chen", "Jun", ""]]}, {"id": "2008.09229", "submitter": "Quoc-Huy Tran", "authors": "Bingbing Zhuang and Quoc-Huy Tran", "title": "Image Stitching and Rectification for Hand-Held Cameras", "comments": "ECCV 2020. Project web: https://www.nec-labs.com/~mas/RS-APAP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we derive a new differential homography that can account for\nthe scanline-varying camera poses in Rolling Shutter (RS) cameras, and\ndemonstrate its application to carry out RS-aware image stitching and\nrectification at one stroke. Despite the high complexity of RS geometry, we\nfocus in this paper on a special yet common input -- two consecutive frames\nfrom a video stream, wherein the inter-frame motion is restricted from being\narbitrarily large. This allows us to adopt simpler differential motion model,\nleading to a straightforward and practical minimal solver. To deal with\nnon-planar scene and camera parallax in stitching, we further propose an\nRS-aware spatially-varying homography field in the principle of\nAs-Projective-As-Possible (APAP). We show superior performance over\nstate-of-the-art methods both in RS image stitching and rectification,\nespecially for images captured by hand-held shaking cameras.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 23:31:23 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Zhuang", "Bingbing", ""], ["Tran", "Quoc-Huy", ""]]}, {"id": "2008.09234", "submitter": "Romero Morais", "authors": "Romero Morais, Vuong Le, Truyen Tran, Svetha Venkatesh", "title": "Learning to Abstract and Predict Human Actions", "comments": "Accepted for publication in BMVC'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activities are naturally structured as hierarchies unrolled over time.\nFor action prediction, temporal relations in event sequences are widely\nexploited by current methods while their semantic coherence across different\nlevels of abstraction has not been well explored. In this work we model the\nhierarchical structure of human activities in videos and demonstrate the power\nof such structure in action prediction. We propose Hierarchical\nEncoder-Refresher-Anticipator, a multi-level neural machine that can learn the\nstructure of human activities by observing a partial hierarchy of events and\nroll-out such structure into a future prediction in multiple levels of\nabstraction. We also introduce a new coarse-to-fine action annotation on the\nBreakfast Actions videos to create a comprehensive, consistent, and cleanly\nstructured video hierarchical activity dataset. Through our experiments, we\nexamine and rethink the settings and metrics of activity prediction tasks\ntoward unbiased evaluation of prediction systems, and demonstrate the role of\nhierarchical modeling toward reliable and detailed long-term action\nforecasting.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 23:57:58 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Morais", "Romero", ""], ["Le", "Vuong", ""], ["Tran", "Truyen", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "2008.09241", "submitter": "Tushar Nagarajan", "authors": "Tushar Nagarajan and Kristen Grauman", "title": "Learning Affordance Landscapes for Interaction Exploration in 3D\n  Environments", "comments": "To be published in NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embodied agents operating in human spaces must be able to master how their\nenvironment works: what objects can the agent use, and how can it use them? We\nintroduce a reinforcement learning approach for exploration for interaction,\nwhereby an embodied agent autonomously discovers the affordance landscape of a\nnew unmapped 3D environment (such as an unfamiliar kitchen). Given an\negocentric RGB-D camera and a high-level action space, the agent is rewarded\nfor maximizing successful interactions while simultaneously training an\nimage-based affordance segmentation model. The former yields a policy for\nacting efficiently in new environments to prepare for downstream interaction\ntasks, while the latter yields a convolutional neural network that maps image\nregions to the likelihood they permit each action, densifying the rewards for\nexploration. We demonstrate our idea with AI2-iTHOR. The results show agents\ncan learn how to use new home environments intelligently and that it prepares\nthem to rapidly address various downstream tasks like \"find a knife and put it\nin the drawer.\" Project page:\nhttp://vision.cs.utexas.edu/projects/interaction-exploration/\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 00:29:36 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 02:44:08 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Nagarajan", "Tushar", ""], ["Grauman", "Kristen", ""]]}, {"id": "2008.09269", "submitter": "Jun Gao", "authors": "Jun Gao, Zian Wang, Jinchen Xuan, Sanja Fidler", "title": "Beyond Fixed Grid: Learning Geometric Image Representation with a\n  Deformable Grid", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In modern computer vision, images are typically represented as a fixed\nuniform grid with some stride and processed via a deep convolutional neural\nnetwork. We argue that deforming the grid to better align with the\nhigh-frequency image content is a more effective strategy. We introduce\n\\emph{Deformable Grid} DefGrid, a learnable neural network module that predicts\nlocation offsets of vertices of a 2-dimensional triangular grid, such that the\nedges of the deformed grid align with image boundaries. We showcase our DefGrid\nin a variety of use cases, i.e., by inserting it as a module at various levels\nof processing. We utilize DefGrid as an end-to-end \\emph{learnable geometric\ndownsampling} layer that replaces standard pooling methods for reducing feature\nresolution when feeding images into a deep CNN. We show significantly improved\nresults at the same grid resolution compared to using CNNs on uniform grids for\nthe task of semantic segmentation. We also utilize DefGrid at the output layers\nfor the task of object mask annotation, and show that reasoning about object\nboundaries on our predicted polygonal grid leads to more accurate results over\nexisting pixel-wise and curve-based approaches. We finally showcase DefGrid as\na standalone module for unsupervised image partitioning, showing superior\nperformance over existing approaches. Project website:\nhttp://www.cs.toronto.edu/~jungao/def-grid\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 02:22:06 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Gao", "Jun", ""], ["Wang", "Zian", ""], ["Xuan", "Jinchen", ""], ["Fidler", "Sanja", ""]]}, {"id": "2008.09285", "submitter": "Santhosh Kumar Ramakrishnan", "authors": "Santhosh K. Ramakrishnan, Ziad Al-Halah, Kristen Grauman", "title": "Occupancy Anticipation for Efficient Exploration and Navigation", "comments": "Accepted in ECCV 2020. 19 pages, 6 figures, appendix at end", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art navigation methods leverage a spatial memory to generalize\nto new environments, but their occupancy maps are limited to capturing the\ngeometric structures directly observed by the agent. We propose occupancy\nanticipation, where the agent uses its egocentric RGB-D observations to infer\nthe occupancy state beyond the visible regions. In doing so, the agent builds\nits spatial awareness more rapidly, which facilitates efficient exploration and\nnavigation in 3D environments. By exploiting context in both the egocentric\nviews and top-down maps our model successfully anticipates a broader map of the\nenvironment, with performance significantly better than strong baselines.\nFurthermore, when deployed for the sequential decision-making tasks of\nexploration and navigation, our model outperforms state-of-the-art methods on\nthe Gibson and Matterport3D datasets. Our approach is the winning entry in the\n2020 Habitat PointNav Challenge. Project page:\nhttp://vision.cs.utexas.edu/projects/occupancy_anticipation/\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 03:16:51 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 16:36:11 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Ramakrishnan", "Santhosh K.", ""], ["Al-Halah", "Ziad", ""], ["Grauman", "Kristen", ""]]}, {"id": "2008.09289", "submitter": "Nathaniel Bloomfield", "authors": "Nathaniel J. Bloomfield and Susan Wei and Bartholomew Woodham and\n  Peter Wilkinson and Andrew Robinson", "title": "Automating the assessment of biofouling in images using expert agreement\n  as a gold standard", "comments": "12 pages", "journal-ref": "Sci Rep 11, 2739 (2021)", "doi": "10.1038/s41598-021-81011-2", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biofouling is the accumulation of organisms on surfaces immersed in water. It\nis of particular concern to the international shipping industry because it\nincreases fuel costs and presents a biosecurity risk by providing a pathway for\nnon-indigenous marine species to establish in new areas. There is growing\ninterest within jurisdictions to strengthen biofouling risk-management\nregulations, but it is expensive to conduct in-water inspections and assess the\ncollected data to determine the biofouling state of vessel hulls. Machine\nlearning is well suited to tackle the latter challenge, and here we apply deep\nlearning to automate the classification of images from in-water inspections to\nidentify the presence and severity of fouling. We combined several datasets to\nobtain over 10,000 images collected from in-water surveys which were annotated\nby a group biofouling experts. We compared the annotations from three experts\non a 120-sample subset of these images, and found that they showed 89%\nagreement (95% CI: 87-92%). Subsequent labelling of the whole dataset by one of\nthese experts achieved similar levels of agreement with this group of experts,\nwhich we defined as performing at most 5% worse (p=0.009-0.054). Using these\nexpert labels, we were able to train a deep learning model that also agreed\nsimilarly with the group of experts (p=0.001-0.014), demonstrating that\nautomated analysis of biofouling in images is feasible and effective using this\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 03:30:45 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 03:59:09 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Bloomfield", "Nathaniel J.", ""], ["Wei", "Susan", ""], ["Woodham", "Bartholomew", ""], ["Wilkinson", "Peter", ""], ["Robinson", "Andrew", ""]]}, {"id": "2008.09304", "submitter": "Dou Xu", "authors": "Dou Xu, Chang Cai, Chaowei Fang, Bin Kong, Jihua Zhu, Zhongyu Li", "title": "Graph Neural Networks for UnsupervisedDomain Adaptation of\n  Histopathological ImageAnalytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotating histopathological images is a time-consuming andlabor-intensive\nprocess, which requires broad-certificated pathologistscarefully examining\nlarge-scale whole-slide images from cells to tissues.Recent frontiers of\ntransfer learning techniques have been widely investi-gated for image\nunderstanding tasks with limited annotations. However,when applied for the\nanalytics of histology images, few of them can effec-tively avoid the\nperformance degradation caused by the domain discrep-ancy between the source\ntraining dataset and the target dataset, suchas different tissues, staining\nappearances, and imaging devices. To thisend, we present a novel method for the\nunsupervised domain adaptationin histopathological image analysis, based on a\nbackbone for embeddinginput images into a feature space, and a graph neural\nlayer for propa-gating the supervision signals of images with labels. The graph\nmodel isset up by connecting every image with its close neighbors in the\nembed-ded feature space. Then graph neural network is employed to synthesizenew\nfeature representation from every image. During the training stage,target\nsamples with confident inferences are dynamically allocated withpseudo labels.\nThe cross-entropy loss function is used to constrain thepredictions of source\nsamples with manually marked labels and targetsamples with pseudo labels.\nFurthermore, the maximum mean diversityis adopted to facilitate the extraction\nof domain-invariant feature repre-sentations, and contrastive learning is\nexploited to enhance the categorydiscrimination of learned features. In\nexperiments of the unsupervised do-main adaptation for histopathological image\nclassification, our methodachieves state-of-the-art performance on four public\ndatasets\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 04:53:44 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Xu", "Dou", ""], ["Cai", "Chang", ""], ["Fang", "Chaowei", ""], ["Kong", "Bin", ""], ["Zhu", "Jihua", ""], ["Li", "Zhongyu", ""]]}, {"id": "2008.09305", "submitter": "Hengli Wang", "authors": "Hengli Wang, Yuxuan Liu, Huaiyang Huang, Yuheng Pan, Wenbin Yu, Jialin\n  Jiang, Dianbin Lyu, Mohammud J. Bocus, Ming Liu, Ioannis Pitas, Rui Fan", "title": "ATG-PVD: Ticketing Parking Violations on A Drone", "comments": "17 pages, 11 figures and 3 tables. This paper is accepted by ECCV\n  Workshops 2020", "journal-ref": null, "doi": "10.1007/978-3-030-66823-5_32", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel suspect-and-investigate framework, which\ncan be easily embedded in a drone for automated parking violation detection\n(PVD). Our proposed framework consists of: 1) SwiftFlow, an efficient and\naccurate convolutional neural network (CNN) for unsupervised optical flow\nestimation; 2) Flow-RCNN, a flow-guided CNN for car detection and\nclassification; and 3) an illegally parked car (IPC) candidate investigation\nmodule developed based on visual SLAM. The proposed framework was successfully\nembedded in a drone from ATG Robotics. The experimental results demonstrate\nthat, firstly, our proposed SwiftFlow outperforms all other state-of-the-art\nunsupervised optical flow estimation approaches in terms of both speed and\naccuracy; secondly, IPC candidates can be effectively and efficiently detected\nby our proposed Flow-RCNN, with a better performance than our baseline network,\nFaster-RCNN; finally, the actual IPCs can be successfully verified by our\ninvestigation module after drone re-localization.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 05:07:21 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Wang", "Hengli", ""], ["Liu", "Yuxuan", ""], ["Huang", "Huaiyang", ""], ["Pan", "Yuheng", ""], ["Yu", "Wenbin", ""], ["Jiang", "Jialin", ""], ["Lyu", "Dianbin", ""], ["Bocus", "Mohammud J.", ""], ["Liu", "Ming", ""], ["Pitas", "Ioannis", ""], ["Fan", "Rui", ""]]}, {"id": "2008.09306", "submitter": "Shirley Liu", "authors": "Shirley Liu, Charles Lehman and Ghassan AlRegib", "title": "Robustness and Overfitting Behavior of Implicit Background Models", "comments": "6 pages, 3 figures, accepted to IEEE International Conference on\n  Image Processing (ICIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we examine the overfitting behavior of image classification\nmodels modified with Implicit Background Estimation (SCrIBE), which transforms\nthem into weakly supervised segmentation models that provide spatial domain\nvisualizations without affecting performance. Using the segmentation masks, we\nderive an overfit detection criterion that does not require testing labels. In\naddition, we assess the change in model performance, calibration, and\nsegmentation masks after applying data augmentations as overfitting reduction\nmeasures and testing on various types of distorted images.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 05:08:33 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Liu", "Shirley", ""], ["Lehman", "Charles", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "2008.09309", "submitter": "Gyeongsik Moon", "authors": "Gyeongsik Moon, Shoou-i Yu, He Wen, Takaaki Shiratori, Kyoung Mu Lee", "title": "InterHand2.6M: A Dataset and Baseline for 3D Interacting Hand Pose\n  Estimation from a Single RGB Image", "comments": "Published at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of hand-hand interactions is a crucial step towards better\nunderstanding human behavior. However, most researches in 3D hand pose\nestimation have focused on the isolated single hand case. Therefore, we firstly\npropose (1) a large-scale dataset, InterHand2.6M, and (2) a baseline network,\nInterNet, for 3D interacting hand pose estimation from a single RGB image. The\nproposed InterHand2.6M consists of \\textbf{2.6M labeled single and interacting\nhand frames} under various poses from multiple subjects. Our InterNet\nsimultaneously performs 3D single and interacting hand pose estimation. In our\nexperiments, we demonstrate big gains in 3D interacting hand pose estimation\naccuracy when leveraging the interacting hand data in InterHand2.6M. We also\nreport the accuracy of InterNet on InterHand2.6M, which serves as a strong\nbaseline for this new dataset. Finally, we show 3D interacting hand pose\nestimation results from general images. Our code and dataset are available at\nhttps://mks0601.github.io/InterHand2.6M/.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 05:15:58 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Moon", "Gyeongsik", ""], ["Yu", "Shoou-i", ""], ["Wen", "He", ""], ["Shiratori", "Takaaki", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "2008.09310", "submitter": "Sungyong Baik", "authors": "Sungyong Baik, Hyo Jin Kim, Tianwei Shen, Eddy Ilg, Kyoung Mu Lee,\n  Chris Sweeney", "title": "Domain Adaptation of Learned Features for Visual Localization", "comments": "BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of visual localization under changing conditions, such\nas time of day, weather, and seasons. Recent learned local features based on\ndeep neural networks have shown superior performance over classical\nhand-crafted local features. However, in a real-world scenario, there often\nexists a large domain gap between training and target images, which can\nsignificantly degrade the localization accuracy. While existing methods utilize\na large amount of data to tackle the problem, we present a novel and practical\napproach, where only a few examples are needed to reduce the domain gap. In\nparticular, we propose a few-shot domain adaptation framework for learned local\nfeatures that deals with varying conditions in visual localization. The\nexperimental results demonstrate the superior performance over baselines, while\nusing a scarce number of training examples from the target domain.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 05:17:32 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Baik", "Sungyong", ""], ["Kim", "Hyo Jin", ""], ["Shen", "Tianwei", ""], ["Ilg", "Eddy", ""], ["Lee", "Kyoung Mu", ""], ["Sweeney", "Chris", ""]]}, {"id": "2008.09315", "submitter": "Seyed Amir Tafrishi", "authors": "Seyed Amir Tafrishi and Xiaotian Dai and Vahid Esmaeilzadeh Kandjani", "title": "Line-Circle-Square (LCS): A Multilayered Geometric Filter for Edge-Based\n  Detection", "comments": "17 pages, 19 figures, accepted at the Elsevier's Robotics and\n  Autonomous Systems journal", "journal-ref": "Journal: Robotics and Autonomous Systems, publisher: Elsevier,\n  volume number: 137, year: 2021, page number: 103732", "doi": "10.1016/j.robot.2021.103732", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a state-of-the-art filter that reduces the complexity in\nobject detection, tracking and mapping applications. Existing edge detection\nand tracking methods are proposed to create suitable autonomy for mobile\nrobots, however, many of them face overconfidence and large computations at the\nentrance to scenarios with an immense number of landmarks. The method in this\nwork, the Line-Circle-Square (LCS) filter, claims that mobile robots without a\nlarge database for object recognition and highly advanced prediction methods\ncan deal with incoming objects that the camera captures in real-time. The\nproposed filter applies detection, tracking and learning to each defined expert\nto extract higher level information for judging scenes without\nover-calculation. The interactive learning feed between each expert increases\nthe consistency of detected landmarks that works against overwhelming detected\nfeatures in crowded scenes. Our experts are dependent on trust factors'\ncovariance under the geometric definitions to ignore, emerge and compare\ndetected landmarks. The experiment validates the effectiveness of the proposed\nfilter in terms of detection precision and resource usage in both experimental\nand real-world scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 05:28:12 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 03:37:17 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2021 04:30:04 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Tafrishi", "Seyed Amir", ""], ["Dai", "Xiaotian", ""], ["Kandjani", "Vahid Esmaeilzadeh", ""]]}, {"id": "2008.09326", "submitter": "Wang Zheng", "authors": "Zheng Wang, Jianwu Li and Ge Song", "title": "DTDN: Dual-task De-raining Network", "comments": null, "journal-ref": null, "doi": "10.1145/3343031.3350945", "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Removing rain streaks from rainy images is necessary for many tasks in\ncomputer vision, such as object detection and recognition. It needs to address\ntwo mutually exclusive objectives: removing rain streaks and reserving\nrealistic details. Balancing them is critical for de-raining methods. We\npropose an end-to-end network, called dual-task de-raining network (DTDN),\nconsisting of two sub-networks: generative adversarial network (GAN) and\nconvolutional neural network (CNN), to remove rain streaks via coordinating the\ntwo mutually exclusive objectives self-adaptively. DTDN-GAN is mainly used to\nremove structural rain streaks, and DTDN-CNN is designed to recover details in\noriginal images. We also design a training algorithm to train these two\nsub-networks of DTDN alternatively, which share same weights but use different\ntraining sets. We further enrich two existing datasets to approximate the\ndistribution of real rain streaks. Experimental results show that our method\noutperforms several recent state-of-the-art methods, based on both benchmark\ntesting datasets and real rainy images.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 06:32:42 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Wang", "Zheng", ""], ["Li", "Jianwu", ""], ["Song", "Ge", ""]]}, {"id": "2008.09342", "submitter": "Dingheng Wang", "authors": "Dingheng Wang and Bijiao Wu and Guangshe Zhao and Hengnu Chen and Lei\n  Deng and Tianyi Yan and Guoqi Li", "title": "Kronecker CP Decomposition with Fast Multiplication for Compressing RNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) are powerful in the tasks oriented to\nsequential data, such as natural language processing and video recognition.\nHowever, since the modern RNNs, including long-short term memory (LSTM) and\ngated recurrent unit (GRU) networks, have complex topologies and expensive\nspace/computation complexity, compressing them becomes a hot and promising\ntopic in recent years. Among plenty of compression methods, tensor\ndecomposition, e.g., tensor train (TT), block term (BT), tensor ring (TR) and\nhierarchical Tucker (HT), appears to be the most amazing approach since a very\nhigh compression ratio might be obtained. Nevertheless, none of these tensor\ndecomposition formats can provide both the space and computation efficiency. In\nthis paper, we consider to compress RNNs based on a novel Kronecker\nCANDECOMP/PARAFAC (KCP) decomposition, which is derived from Kronecker tensor\n(KT) decomposition, by proposing two fast algorithms of multiplication between\nthe input and the tensor-decomposed weight. According to our experiments based\non UCF11, Youtube Celebrities Face and UCF50 datasets, it can be verified that\nthe proposed KCP-RNNs have comparable performance of accuracy with those in\nother tensor-decomposed formats, and even 278,219x compression ratio could be\nobtained by the low rank KCP. More importantly, KCP-RNNs are efficient in both\nspace and computation complexity compared with other tensor-decomposed ones\nunder similar ranks. Besides, we find KCP has the best potential for parallel\ncomputing to accelerate the calculations in neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 07:29:45 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Wang", "Dingheng", ""], ["Wu", "Bijiao", ""], ["Zhao", "Guangshe", ""], ["Chen", "Hengnu", ""], ["Deng", "Lei", ""], ["Yan", "Tianyi", ""], ["Li", "Guoqi", ""]]}, {"id": "2008.09346", "submitter": "Ren\\'e Schuster", "authors": "Ren\\'e Schuster, Oliver Wasenm\\\"uller, Christian Unger, Didier\n  Stricker", "title": "SSGP: Sparse Spatial Guided Propagation for Robust and Generic\n  Interpolation", "comments": "Accepted to WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpolation of sparse pixel information towards a dense target resolution\nfinds its application across multiple disciplines in computer vision.\nState-of-the-art interpolation of motion fields applies model-based\ninterpolation that makes use of edge information extracted from the target\nimage. For depth completion, data-driven learning approaches are widespread.\nOur work is inspired by latest trends in depth completion that tackle the\nproblem of dense guidance for sparse information. We extend these ideas and\ncreate a generic cross-domain architecture that can be applied for a multitude\nof interpolation problems like optical flow, scene flow, or depth completion.\nIn our experiments, we show that our proposed concept of Sparse Spatial Guided\nPropagation (SSGP) achieves improvements to robustness, accuracy, or speed\ncompared to specialized algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 07:39:41 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 09:06:51 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Schuster", "Ren\u00e9", ""], ["Wasenm\u00fcller", "Oliver", ""], ["Unger", "Christian", ""], ["Stricker", "Didier", ""]]}, {"id": "2008.09352", "submitter": "Jiehua Zhang", "authors": "Zhang Li, Jiehua Zhang, Tao Tan, Xichao Teng, Xiaoliang Sun, Yang Li,\n  Lihong Liu, Yang Xiao, Byungjae Lee, Yilong Li, Qianni Zhang, Shujiao Sun,\n  Yushan Zheng, Junyu Yan, Ni Li, Yiyu Hong, Junsu Ko, Hyun Jung, Yanling Liu,\n  Yu-cheng Chen, Ching-wei Wang, Vladimir Yurovskiy, Pavel Maevskikh, Vahid\n  Khanagha, Yi Jiang, Xiangjun Feng, Zhihong Liu, Daiqiang Li, Peter J.\n  Sch\\\"uffler, Qifeng Yu, Hui Chen, Yuling Tang, Geert Litjens", "title": "Deep Learning Methods for Lung Cancer Segmentation in Whole-slide\n  Histopathology Images -- the ACDC@LungHP Challenge 2019", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmentation of lung cancer in pathology slides is a critical step\nin improving patient care. We proposed the ACDC@LungHP (Automatic Cancer\nDetection and Classification in Whole-slide Lung Histopathology) challenge for\nevaluating different computer-aided diagnosis (CADs) methods on the automatic\ndiagnosis of lung cancer. The ACDC@LungHP 2019 focused on segmentation\n(pixel-wise detection) of cancer tissue in whole slide imaging (WSI), using an\nannotated dataset of 150 training images and 50 test images from 200 patients.\nThis paper reviews this challenge and summarizes the top 10 submitted methods\nfor lung cancer segmentation. All methods were evaluated using the false\npositive rate, false negative rate, and DICE coefficient (DC). The DC ranged\nfrom 0.7354$\\pm$0.1149 to 0.8372$\\pm$0.0858. The DC of the best method was\nclose to the inter-observer agreement (0.8398$\\pm$0.0890). All methods were\nbased on deep learning and categorized into two groups: multi-model method and\nsingle model method. In general, multi-model methods were significantly better\n($\\textit{p}$<$0.01$) than single model methods, with mean DC of 0.7966 and\n0.7544, respectively. Deep learning based methods could potentially help\npathologists find suspicious regions for further analysis of lung cancer in\nWSI.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 07:52:11 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Li", "Zhang", ""], ["Zhang", "Jiehua", ""], ["Tan", "Tao", ""], ["Teng", "Xichao", ""], ["Sun", "Xiaoliang", ""], ["Li", "Yang", ""], ["Liu", "Lihong", ""], ["Xiao", "Yang", ""], ["Lee", "Byungjae", ""], ["Li", "Yilong", ""], ["Zhang", "Qianni", ""], ["Sun", "Shujiao", ""], ["Zheng", "Yushan", ""], ["Yan", "Junyu", ""], ["Li", "Ni", ""], ["Hong", "Yiyu", ""], ["Ko", "Junsu", ""], ["Jung", "Hyun", ""], ["Liu", "Yanling", ""], ["Chen", "Yu-cheng", ""], ["Wang", "Ching-wei", ""], ["Yurovskiy", "Vladimir", ""], ["Maevskikh", "Pavel", ""], ["Khanagha", "Vahid", ""], ["Jiang", "Yi", ""], ["Feng", "Xiangjun", ""], ["Liu", "Zhihong", ""], ["Li", "Daiqiang", ""], ["Sch\u00fcffler", "Peter J.", ""], ["Yu", "Qifeng", ""], ["Chen", "Hui", ""], ["Tang", "Yuling", ""], ["Litjens", "Geert", ""]]}, {"id": "2008.09359", "submitter": "Weifeng Liu", "authors": "Jinfeng Li, Weifeng Liu, Yicong Zhou, Jun Yu, Dapeng Tao", "title": "Learning Domain-invariant Graph for Adaptive Semi-supervised Domain\n  Adaptation with Few Labeled Source Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation aims to generalize a model from a source domain to tackle\ntasks in a related but different target domain. Traditional domain adaptation\nalgorithms assume that enough labeled data, which are treated as the prior\nknowledge are available in the source domain. However, these algorithms will be\ninfeasible when only a few labeled data exist in the source domain, and thus\nthe performance decreases significantly. To address this challenge, we propose\na Domain-invariant Graph Learning (DGL) approach for domain adaptation with\nonly a few labeled source samples. Firstly, DGL introduces the Nystrom method\nto construct a plastic graph that shares similar geometric property as the\ntarget domain. And then, DGL flexibly employs the Nystrom approximation error\nto measure the divergence between plastic graph and source graph to formalize\nthe distribution mismatch from the geometric perspective. Through minimizing\nthe approximation error, DGL learns a domain-invariant geometric graph to\nbridge source and target domains. Finally, we integrate the learned\ndomain-invariant graph with the semi-supervised learning and further propose an\nadaptive semi-supervised model to handle the cross-domain problems. The results\nof extensive experiments on popular datasets verify the superiority of DGL,\nespecially when only a few labeled source samples are available.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 08:13:25 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Li", "Jinfeng", ""], ["Liu", "Weifeng", ""], ["Zhou", "Yicong", ""], ["Yu", "Jun", ""], ["Tao", "Dapeng", ""]]}, {"id": "2008.09370", "submitter": "Ke-Chi Chang", "authors": "Ke-Chi Chang, Ren Wang, Hung-Jin Lin, Yu-Lun Liu, Chia-Ping Chen,\n  Yu-Lin Chang, Hwann-Tzong Chen", "title": "Learning Camera-Aware Noise Models", "comments": "Accepted by ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling imaging sensor noise is a fundamental problem for image processing\nand computer vision applications. While most previous works adopt statistical\nnoise models, real-world noise is far more complicated and beyond what these\nmodels can describe. To tackle this issue, we propose a data-driven approach,\nwhere a generative noise model is learned from real-world noise. The proposed\nnoise model is camera-aware, that is, different noise characteristics of\ndifferent camera sensors can be learned simultaneously, and a single learned\nnoise model can generate different noise for different camera sensors.\nExperimental results show that our method quantitatively and qualitatively\noutperforms existing statistical noise models and learning-based methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 08:25:14 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Chang", "Ke-Chi", ""], ["Wang", "Ren", ""], ["Lin", "Hung-Jin", ""], ["Liu", "Yu-Lun", ""], ["Chen", "Chia-Ping", ""], ["Chang", "Yu-Lin", ""], ["Chen", "Hwann-Tzong", ""]]}, {"id": "2008.09381", "submitter": "Julia Lust", "authors": "Julia Lust and Alexandru Paul Condurache", "title": "A Survey on Assessing the Generalization Envelope of Deep Neural\n  Networks at Inference Time for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) achieve state-of-the-art performance on numerous\napplications. However, it is difficult to tell beforehand if a DNN receiving an\ninput will deliver the correct output since their decision criteria are usually\nnontransparent. A DNN delivers the correct output if the input is within the\narea enclosed by its generalization envelope. In this case, the information\ncontained in the input sample is processed reasonably by the network. It is of\nlarge practical importance to assess at inference time if a DNN generalizes\ncorrectly. Currently, the approaches to achieve this goal are investigated in\ndifferent problem set-ups rather independently from one another, leading to\nthree main research and literature fields: predictive uncertainty,\nout-of-distribution detection and adversarial example detection. This survey\nconnects the three fields within the larger framework of investigating the\ngeneralization performance of machine learning methods and in particular DNNs.\nWe underline the common ground, point at the most promising approaches and give\na structured overview of the methods that provide at inference time means to\nestablish if the current input is within the generalization envelope of a DNN.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 09:12:52 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 19:54:19 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Lust", "Julia", ""], ["Condurache", "Alexandru Paul", ""]]}, {"id": "2008.09388", "submitter": "Shiming Chen", "authors": "Shiming Chen and Wenjie Wang and Beihao Xia and Xinge You and Zehong\n  Cao and Weiping Ding", "title": "CDE-GAN: Cooperative Dual Evolution Based Generative Adversarial Network", "comments": "15 pages,6 figures,4 tables. Accepted by IEEE Transactions on\n  Evolutionary Computation", "journal-ref": "IEEE Transactions on Evolutionary Computation, 2021", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have been a popular deep generative\nmodel for real-world applications. Despite many recent efforts on GANs that\nhave been contributed, mode collapse and instability of GANs are still open\nproblems caused by their adversarial optimization difficulties. In this paper,\nmotivated by the cooperative co-evolutionary algorithm, we propose a\nCooperative Dual Evolution based Generative Adversarial Network (CDE-GAN) to\ncircumvent these drawbacks. In essence, CDE-GAN incorporates dual evolution\nwith respect to the generator(s) and discriminators into a unified evolutionary\nadversarial framework to conduct effective adversarial multi-objective\noptimization. Thus it exploits the complementary properties and injects dual\nmutation diversity into training to steadily diversify the estimated density in\ncapturing multi-modes and improve generative performance. Specifically, CDE-GAN\ndecomposes the complex adversarial optimization problem into two subproblems\n(generation and discrimination), and each subproblem is solved with a separated\nsubpopulation (E-Generator} and E-Discriminators), evolved by its own\nevolutionary algorithm. Additionally, we further propose a Soft Mechanism to\nbalance the trade-off between E-Generators and E-Discriminators to conduct\nsteady training for CDE-GAN. Extensive experiments on one synthetic dataset and\nthree real-world benchmark image datasets demonstrate that the proposed CDE-GAN\nachieves a competitive and superior performance in generating good quality and\ndiverse samples over baselines. The code and more generated results are\navailable at our project homepage:\nhttps://shiming-chen.github.io/CDE-GAN-website/CDE-GAN.html.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 09:39:53 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 13:06:29 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Chen", "Shiming", ""], ["Wang", "Wenjie", ""], ["Xia", "Beihao", ""], ["You", "Xinge", ""], ["Cao", "Zehong", ""], ["Ding", "Weiping", ""]]}, {"id": "2008.09397", "submitter": "Jiaming Han", "authors": "Jiaming Han, Jian Ding, Jie Li, Gui-Song Xia", "title": "Align Deep Features for Oriented Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past decade has witnessed significant progress on detecting objects in\naerial images that are often distributed with large scale variations and\narbitrary orientations. However most of existing methods rely on heuristically\ndefined anchors with different scales, angles and aspect ratios and usually\nsuffer from severe misalignment between anchor boxes and axis-aligned\nconvolutional features, which leads to the common inconsistency between the\nclassification score and localization accuracy. To address this issue, we\npropose a Single-shot Alignment Network (S$^2$A-Net) consisting of two modules:\na Feature Alignment Module (FAM) and an Oriented Detection Module (ODM). The\nFAM can generate high-quality anchors with an Anchor Refinement Network and\nadaptively align the convolutional features according to the anchor boxes with\na novel Alignment Convolution. The ODM first adopts active rotating filters to\nencode the orientation information and then produces orientation-sensitive and\norientation-invariant features to alleviate the inconsistency between\nclassification score and localization accuracy. Besides, we further explore the\napproach to detect objects in large-size images, which leads to a better\ntrade-off between speed and accuracy. Extensive experiments demonstrate that\nour method can achieve state-of-the-art performance on two commonly used aerial\nobjects datasets (i.e., DOTA and HRSC2016) while keeping high efficiency. The\ncode is available at https://github.com/csuhan/s2anet.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 09:55:13 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 13:34:03 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 03:26:49 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Han", "Jiaming", ""], ["Ding", "Jian", ""], ["Li", "Jie", ""], ["Xia", "Gui-Song", ""]]}, {"id": "2008.09403", "submitter": "Tommaso Campari", "authors": "Tommaso Campari, Paolo Eccher, Luciano Serafini and Lamberto Ballan", "title": "Exploiting Scene-specific Features for Object Goal Navigation", "comments": "Accepted at ACVR2020 ECCV2020 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can the intrinsic relation between an object and the room in which it is\nusually located help agents in the Visual Navigation Task? We study this\nquestion in the context of Object Navigation, a problem in which an agent has\nto reach an object of a specific class while moving in a complex domestic\nenvironment. In this paper, we introduce a new reduced dataset that speeds up\nthe training of navigation models, a notoriously complex task. Our proposed\ndataset permits the training of models that do not exploit online-built maps in\nreasonable times even without the use of huge computational resources.\nTherefore, this reduced dataset guarantees a significant benchmark and it can\nbe used to identify promising models that could be then tried on bigger and\nmore challenging datasets. Subsequently, we propose the SMTSC model, an\nattention-based model capable of exploiting the correlation between scenes and\nobjects contained in them, highlighting quantitatively how the idea is correct.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 10:16:01 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Campari", "Tommaso", ""], ["Eccher", "Paolo", ""], ["Serafini", "Luciano", ""], ["Ballan", "Lamberto", ""]]}, {"id": "2008.09412", "submitter": "Zitong Yu", "authors": "Zitong Yu, Benjia Zhou, Jun Wan, Pichao Wang, Haoyu Chen, Xin Liu,\n  Stan Z. Li, Guoying Zhao", "title": "Searching Multi-Rate and Multi-Modal Temporal Enhanced Networks for\n  Gesture Recognition", "comments": "Submitted to IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2021.3087348", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gesture recognition has attracted considerable attention owing to its great\npotential in applications. Although the great progress has been made recently\nin multi-modal learning methods, existing methods still lack effective\nintegration to fully explore synergies among spatio-temporal modalities\neffectively for gesture recognition. The problems are partially due to the fact\nthat the existing manually designed network architectures have low efficiency\nin the joint learning of multi-modalities. In this paper, we propose the first\nneural architecture search (NAS)-based method for RGB-D gesture recognition.\nThe proposed method includes two key components: 1) enhanced temporal\nrepresentation via the proposed 3D Central Difference Convolution (3D-CDC)\nfamily, which is able to capture rich temporal context via aggregating temporal\ndifference information; and 2) optimized backbones for multi-sampling-rate\nbranches and lateral connections among varied modalities. The resultant\nmulti-modal multi-rate network provides a new perspective to understand the\nrelationship between RGB and depth modalities and their temporal dynamics.\nComprehensive experiments are performed on three benchmark datasets (IsoGD,\nNvGesture, and EgoGesture), demonstrating the state-of-the-art performance in\nboth single- and multi-modality settings.The code is available at\nhttps://github.com/ZitongYu/3DCDC-NAS\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 10:45:09 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Yu", "Zitong", ""], ["Zhou", "Benjia", ""], ["Wan", "Jun", ""], ["Wang", "Pichao", ""], ["Chen", "Haoyu", ""], ["Liu", "Xin", ""], ["Li", "Stan Z.", ""], ["Zhao", "Guoying", ""]]}, {"id": "2008.09416", "submitter": "Alexander Neergaard Olesen", "authors": "Alexander Neergaard Olesen, Poul Jennum, Emmanuel Mignot, Helge B D\n  Sorensen", "title": "Automatic sleep stage classification with deep residual networks in a\n  mixed-cohort setting", "comments": "Author's original version. This article has been accepted for\n  publication in SLEEP published by Oxford University Press", "journal-ref": null, "doi": "10.1093/sleep/zsaa161", "report-no": null, "categories": "cs.CV eess.SP stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Study Objectives: Sleep stage scoring is performed manually by sleep experts\nand is prone to subjective interpretation of scoring rules with low intra- and\ninterscorer reliability. Many automatic systems rely on few small-scale\ndatabases for developing models, and generalizability to new datasets is thus\nunknown. We investigated a novel deep neural network to assess the\ngeneralizability of several large-scale cohorts.\n  Methods: A deep neural network model was developed using 15684\npolysomnography studies from five different cohorts. We applied four different\nscenarios: 1) impact of varying time-scales in the model; 2) performance of a\nsingle cohort on other cohorts of smaller, greater or equal size relative to\nthe performance of other cohorts on a single cohort; 3) varying the fraction of\nmixed-cohort training data compared to using single-origin data; and 4)\ncomparing models trained on combinations of data from 2, 3, and 4 cohorts.\n  Results: Overall classification accuracy improved with increasing fractions\nof training data (0.25$\\%$: 0.782 $\\pm$ 0.097, 95$\\%$ CI [0.777-0.787];\n100$\\%$: 0.869 $\\pm$ 0.064, 95$\\%$ CI [0.864-0.872]), and with increasing\nnumber of data sources (2: 0.788 $\\pm$ 0.102, 95$\\%$ CI [0.787-0.790]; 3: 0.808\n$\\pm$ 0.092, 95$\\%$ CI [0.807-0.810]; 4: 0.821 $\\pm$ 0.085, 95$\\%$ CI\n[0.819-0.823]). Different cohorts show varying levels of generalization to\nother cohorts.\n  Conclusions: Automatic sleep stage scoring systems based on deep learning\nalgorithms should consider as much data as possible from as many sources\navailable to ensure proper generalization. Public datasets for benchmarking\nshould be made available for future research.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 10:48:35 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Olesen", "Alexander Neergaard", ""], ["Jennum", "Poul", ""], ["Mignot", "Emmanuel", ""], ["Sorensen", "Helge B D", ""]]}, {"id": "2008.09417", "submitter": "Yi Xiao", "authors": "Yi Xiao, Felipe Codevilla, Christopher Pal, Antonio M. Lopez", "title": "Action-Based Representation Learning for Autonomous Driving", "comments": "This paper has been accepted to the Conference on Robot Learning\n  (CoRL 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human drivers produce a vast amount of data which could, in principle, be\nused to improve autonomous driving systems. Unfortunately, seemingly\nstraightforward approaches for creating end-to-end driving models that map\nsensor data directly into driving actions are problematic in terms of\ninterpretability, and typically have significant difficulty dealing with\nspurious correlations. Alternatively, we propose to use this kind of\naction-based driving data for learning representations. Our experiments show\nthat an affordance-based driving model pre-trained with this approach can\nleverage a relatively small amount of weakly annotated imagery and outperform\npure end-to-end driving models, while being more interpretable. Further, we\ndemonstrate how this strategy outperforms previous methods based on learning\ninverse dynamics models as well as other methods based on heavy human\nsupervision (ImageNet).\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 10:49:13 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 15:45:26 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Xiao", "Yi", ""], ["Codevilla", "Felipe", ""], ["Pal", "Christopher", ""], ["Lopez", "Antonio M.", ""]]}, {"id": "2008.09418", "submitter": "Hemanth Nadipineni", "authors": "Hemanth Nadipineni", "title": "Method to Classify Skin Lesions using Dermoscopic images", "comments": "16 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin cancer is the most common cancer in the existing world constituting\none-third of the cancer cases. Benign skin cancers are not fatal, can be cured\nwith proper medication. But it is not the same as the malignant skin cancers.\nIn the case of malignant melanoma, in its peak stage, the maximum life\nexpectancy is less than or equal to 5 years. But, it can be cured if detected\nin early stages. Though there are numerous clinical procedures, the accuracy of\ndiagnosis falls between 49% to 81% and is time-consuming. So, dermoscopy has\nbeen brought into the picture. It helped in increasing the accuracy of\ndiagnosis but could not demolish the error-prone behaviour. A quick and less\nerror-prone solution is needed to diagnose this majorly growing skin cancer.\nThis project deals with the usage of deep learning in skin lesion\nclassification. In this project, an automated model for skin lesion\nclassification using dermoscopic images has been developed with CNN(Convolution\nNeural Networks) as a training model. Convolution neural networks are known for\ncapturing features of an image. So, they are preferred in analyzing medical\nimages to find the characteristics that drive the model towards success.\nTechniques like data augmentation for tackling class imbalance, segmentation\nfor focusing on the region of interest and 10-fold cross-validation to make the\nmodel robust have been brought into the picture. This project also includes\nusage of certain preprocessing techniques like brightening the images using\npiece-wise linear transformation function, grayscale conversion of the image,\nresize the image. This project throws a set of valuable insights on how the\naccuracy of the model hikes with the bringing of new input strategies,\npreprocessing techniques. The best accuracy this model could achieve is 0.886\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 10:58:33 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Nadipineni", "Hemanth", ""]]}, {"id": "2008.09435", "submitter": "Haocong Rao", "authors": "Haocong Rao, Siqi Wang, Xiping Hu, Mingkui Tan, Huang Da, Jun Cheng,\n  Bin Hu", "title": "Self-Supervised Gait Encoding with Locality-Aware Attention for Person\n  Re-Identification", "comments": "Accepted at IJCAI 2020 Main Track. Sole copyright holder is IJCAI.\n  Codes are available at https://github.com/Kali-Hac/SGE-LA", "journal-ref": "In IJCAI, pages 898-905, 2020", "doi": "10.24963/ijcai.2020/125", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gait-based person re-identification (Re-ID) is valuable for safety-critical\napplications, and using only 3D skeleton data to extract discriminative gait\nfeatures for person Re-ID is an emerging open topic. Existing methods either\nadopt hand-crafted features or learn gait features by traditional supervised\nlearning paradigms. Unlike previous methods, we for the first time propose a\ngeneric gait encoding approach that can utilize unlabeled skeleton data to\nlearn gait representations in a self-supervised manner. Specifically, we first\npropose to introduce self-supervision by learning to reconstruct input skeleton\nsequences in reverse order, which facilitates learning richer high-level\nsemantics and better gait representations. Second, inspired by the fact that\nmotion's continuity endows temporally adjacent skeletons with higher\ncorrelations (\"locality\"), we propose a locality-aware attention mechanism that\nencourages learning larger attention weights for temporally adjacent skeletons\nwhen reconstructing current skeleton, so as to learn locality when encoding\ngait. Finally, we propose Attention-based Gait Encodings (AGEs), which are\nbuilt using context vectors learned by locality-aware attention, as final gait\nrepresentations. AGEs are directly utilized to realize effective person Re-ID.\nOur approach typically improves existing skeleton-based methods by 10-20%\nRank-1 accuracy, and it achieves comparable or even superior performance to\nmulti-modal methods with extra RGB or depth information. Our codes are\navailable at https://github.com/Kali-Hac/SGE-LA.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 12:03:17 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Rao", "Haocong", ""], ["Wang", "Siqi", ""], ["Hu", "Xiping", ""], ["Tan", "Mingkui", ""], ["Da", "Huang", ""], ["Cheng", "Jun", ""], ["Hu", "Bin", ""]]}, {"id": "2008.09448", "submitter": "Sajad Amouei Sheshkal", "authors": "Sajad Amouei Sheshkal, Kazim Fouladi-Ghaleh, Hossein Aghababa", "title": "An Improved Person Re-identification Method by light-weight\n  convolutional neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person Re-identification is defined as a recognizing process where the person\nis observed by non-overlapping cameras at different places. In the last decade,\nthe rise in the applications and importance of Person Re-identification for\nsurveillance systems popularized this subject in different areas of computer\nvision. Person Re-identification is faced with challenges such as low\nresolution, varying poses, illumination, background clutter, and occlusion,\nwhich could affect the result of recognizing process. The present paper aims to\nimprove Person Re-identification using transfer learning and application of\nverification loss function within the framework of Siamese network. The Siamese\nnetwork receives image pairs as inputs and extract their features via a\npre-trained model. EfficientNet was employed to obtain discriminative features\nand reduce the demands for data. The advantages of verification loss were used\nin the network learning. Experiments showed that the proposed model performs\nbetter than state-of-the-art methods on the CUHK01 dataset. For example, rank5\naccuracies are 95.2% (+5.7) for the CUHK01 datasets. It also achieved an\nacceptable percentage in Rank 1. Because of the small size of the pre-trained\nmodel parameters, learning speeds up and there will be a need for less hardware\nand data.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 12:34:15 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Sheshkal", "Sajad Amouei", ""], ["Fouladi-Ghaleh", "Kazim", ""], ["Aghababa", "Hossein", ""]]}, {"id": "2008.09457", "submitter": "Philippe Weinzaepfel", "authors": "Philippe Weinzaepfel, Romain Br\\'egier, Hadrien Combaluzier, Vincent\n  Leroy, Gr\\'egory Rogez", "title": "DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in\n  the wild", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce DOPE, the first method to detect and estimate whole-body 3D\nhuman poses, including bodies, hands and faces, in the wild. Achieving this\nlevel of details is key for a number of applications that require understanding\nthe interactions of the people with each other or with the environment. The\nmain challenge is the lack of in-the-wild data with labeled whole-body 3D\nposes. In previous work, training data has been annotated or generated for\nsimpler tasks focusing on bodies, hands or faces separately. In this work, we\npropose to take advantage of these datasets to train independent experts for\neach part, namely a body, a hand and a face expert, and distill their knowledge\ninto a single deep network designed for whole-body 2D-3D pose detection. In\npractice, given a training image with partial or no annotation, each part\nexpert detects its subset of keypoints in 2D and 3D and the resulting\nestimations are combined to obtain whole-body pseudo ground-truth poses. A\ndistillation loss encourages the whole-body predictions to mimic the experts'\noutputs. Our results show that this approach significantly outperforms the same\nwhole-body model trained without distillation while staying close to the\nperformance of the experts. Importantly, DOPE is computationally less demanding\nthan the ensemble of experts and can achieve real-time performance. Test code\nand models are available at\nhttps://europe.naverlabs.com/research/computer-vision/dope.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 12:54:26 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Weinzaepfel", "Philippe", ""], ["Br\u00e9gier", "Romain", ""], ["Combaluzier", "Hadrien", ""], ["Leroy", "Vincent", ""], ["Rogez", "Gr\u00e9gory", ""]]}, {"id": "2008.09471", "submitter": "Matloob Khushi Dr", "authors": "Zezheng Zhang and Matloob Khushi", "title": "GA-MSSR: Genetic Algorithm Maximizing Sharpe and Sterling Ratio Method\n  for RoboTrading", "comments": null, "journal-ref": "IJCNN 2020", "doi": null, "report-no": null, "categories": "q-fin.ST cs.CE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Foreign exchange is the largest financial market in the world, and it is also\none of the most volatile markets. Technical analysis plays an important role in\nthe forex market and trading algorithms are designed utilizing machine learning\ntechniques. Most literature used historical price information and technical\nindicators for training. However, the noisy nature of the market affects the\nconsistency and profitability of the algorithms. To address this problem, we\ndesigned trading rule features that are derived from technical indicators and\ntrading rules. The parameters of technical indicators are optimized to maximize\ntrading performance. We also proposed a novel cost function that computes the\nrisk-adjusted return, Sharpe and Sterling Ratio (SSR), in an effort to reduce\nthe variance and the magnitude of drawdowns. An automatic robotic trading\n(RoboTrading) strategy is designed with the proposed Genetic Algorithm\nMaximizing Sharpe and Sterling Ratio model (GA-MSSR) model. The experiment was\nconducted on intraday data of 6 major currency pairs from 2018 to 2019. The\nresults consistently showed significant positive returns and the performance of\nthe trading system is superior using the optimized rule-based features. The\nhighest return obtained was 320% annually using 5-minute AUDUSD currency pair.\nBesides, the proposed model achieves the best performance on risk factors,\nincluding maximum drawdowns and variance in return, comparing to benchmark\nmodels. The code can be accessed at\nhttps://github.com/zzzac/rule-based-forextrading-system\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 05:33:35 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Zhang", "Zezheng", ""], ["Khushi", "Matloob", ""]]}, {"id": "2008.09474", "submitter": "Zexi Chen", "authors": "Zexi Chen, Xuecheng Xu, Yue Wang, Rong Xiong", "title": "Deep Phase Correlation for End-to-End Heterogeneous Sensor Measurements\n  Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The crucial step for localization is to match the current observation to the\nmap. When the two sensor modalities are significantly different, matching\nbecomes challenging. In this paper, we present an end-to-end deep phase\ncorrelation network (DPCN) to match heterogeneous sensor measurements. In DPCN,\nthe primary component is a differentiable correlation-based estimator that\nback-propagates the pose error to learnable feature extractors, which addresses\nthe problem that there are no direct common features for supervision. Also, it\neliminates the exhaustive evaluation in some previous methods, improving\nefficiency. With the interpretable modeling, the network is light-weighted and\npromising for better generalization. We evaluate the system on both the\nsimulation data and Aero-Ground Dataset which consists of heterogeneous sensor\nimages and aerial images acquired by satellites or aerial robots. The results\nshow that our method is able to match the heterogeneous sensor measurements,\noutperforming the comparative traditional phase correlation and other\nlearning-based methods. Code is available at\nhttps://github.com/jessychen1016/DPCN .\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 13:42:25 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 05:53:14 GMT"}, {"version": "v3", "created": "Mon, 26 Oct 2020 08:58:19 GMT"}, {"version": "v4", "created": "Mon, 2 Nov 2020 11:00:11 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Chen", "Zexi", ""], ["Xu", "Xuecheng", ""], ["Wang", "Yue", ""], ["Xiong", "Rong", ""]]}, {"id": "2008.09497", "submitter": "Carl Toft", "authors": "Carl Toft, Daniyar Turmukhambetov, Torsten Sattler, Fredrik Kahl,\n  Gabriel Brostow", "title": "Single-Image Depth Prediction Makes Feature Matching Easier", "comments": "14 pages, 7 figures, accepted for publication at the European\n  conference on computer vision (ECCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Good local features improve the robustness of many 3D re-localization and\nmulti-view reconstruction pipelines. The problem is that viewing angle and\ndistance severely impact the recognizability of a local feature. Attempts to\nimprove appearance invariance by choosing better local feature points or by\nleveraging outside information, have come with pre-requisites that made some of\nthem impractical. In this paper, we propose a surprisingly effective\nenhancement to local feature extraction, which improves matching. We show that\nCNN-based depths inferred from single RGB images are quite helpful, despite\ntheir flaws. They allow us to pre-warp images and rectify perspective\ndistortions, to significantly enhance SIFT and BRISK features, enabling more\ngood matches, even when cameras are looking at the same scene but in opposite\ndirections.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 14:25:36 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Toft", "Carl", ""], ["Turmukhambetov", "Daniyar", ""], ["Sattler", "Torsten", ""], ["Kahl", "Fredrik", ""], ["Brostow", "Gabriel", ""]]}, {"id": "2008.09506", "submitter": "Xinshuo Weng", "authors": "Xinshuo Weng, Yongxin Wang, Yunze Man, and Kris Kitani", "title": "Graph Neural Networks for 3D Multi-Object Tracking", "comments": "ECCV 2020 workshop paper. Project website:\n  http://www.xinshuoweng.com/projects/GNN3DMOT. arXiv admin note: substantial\n  text overlap with arXiv:2006.07327", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MA cs.MM cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D Multi-object tracking (MOT) is crucial to autonomous systems. Recent work\noften uses a tracking-by-detection pipeline, where the feature of each object\nis extracted independently to compute an affinity matrix. Then, the affinity\nmatrix is passed to the Hungarian algorithm for data association. A key process\nof this pipeline is to learn discriminative features for different objects in\norder to reduce confusion during data association. To that end, we propose two\ninnovative techniques: (1) instead of obtaining the features for each object\nindependently, we propose a novel feature interaction mechanism by introducing\nGraph Neural Networks; (2) instead of obtaining the features from either 2D or\n3D space as in prior work, we propose a novel joint feature extractor to learn\nappearance and motion features from 2D and 3D space. Through experiments on the\nKITTI dataset, our proposed method achieves state-of-the-art 3D MOT\nperformance. Our project website is at\nhttp://www.xinshuoweng.com/projects/GNN3DMOT.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 17:55:41 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Weng", "Xinshuo", ""], ["Wang", "Yongxin", ""], ["Man", "Yunze", ""], ["Kitani", "Kris", ""]]}, {"id": "2008.09527", "submitter": "Xueqian Li", "authors": "Xueqian Li, Jhony Kaesemodel Pontes, Simon Lucey", "title": "PointNetLK Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the generalization ability of recent learning-based point cloud\nregistration methods. Despite their success, these approaches tend to have poor\nperformance when applied to mismatched conditions that are not well-represented\nin the training set, such as unseen object categories, different complex\nscenes, or unknown depth sensors. In these circumstances, it has often been\nbetter to rely on classical non-learning methods (e.g., Iterative Closest\nPoint), which have better generalization ability. Hybrid learning methods, that\nuse learning for predicting point correspondences and then a deterministic step\nfor alignment, have offered some respite, but are still limited in their\ngeneralization abilities. We revisit a recent innovation -- PointNetLK -- and\nshow that the inclusion of an analytical Jacobian can exhibit remarkable\ngeneralization properties while reaping the inherent fidelity benefits of a\nlearning framework. Our approach not only outperforms the state-of-the-art in\nmismatched conditions but also produces results competitive with current\nlearning methods when operating on real-world test data close to the training\nset.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 15:09:28 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 02:11:50 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Li", "Xueqian", ""], ["Pontes", "Jhony Kaesemodel", ""], ["Lucey", "Simon", ""]]}, {"id": "2008.09561", "submitter": "Martin Menchon", "authors": "Martin Menchon, Estefania Talavera, Jose M Massa and Petia Radeva", "title": "Behavioural pattern discovery from collections of egocentric\n  photo-streams", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-66823-5_28", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic discovery of behaviour is of high importance when aiming to\nassess and improve the quality of life of people. Egocentric images offer a\nrich and objective description of the daily life of the camera wearer. This\nwork proposes a new method to identify a person's patterns of behaviour from\ncollected egocentric photo-streams. Our model characterizes time-frames based\non the context (place, activities and environment objects) that define the\nimages composition. Based on the similarity among the time-frames that describe\nthe collected days for a user, we propose a new unsupervised greedy method to\ndiscover the behavioural pattern set based on a novel semantic clustering\napproach. Moreover, we present a new score metric to evaluate the performance\nof the proposed algorithm. We validate our method on 104 days and more than\n100k images extracted from 7 users. Results show that behavioural patterns can\nbe discovered to characterize the routine of individuals and consequently their\nlifestyle.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 16:05:45 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Menchon", "Martin", ""], ["Talavera", "Estefania", ""], ["Massa", "Jose M", ""], ["Radeva", "Petia", ""]]}, {"id": "2008.09567", "submitter": "Md Abul Bashar", "authors": "Md Abul Bashar, Richi Nayak", "title": "TAnoGAN: Time Series Anomaly Detection with Generative Adversarial\n  Networks", "comments": "Made some minor changes. This is the accepted version of the paper at\n  AusDM'20", "journal-ref": "2020 IEEE Symposium Series on Computational Intelligence (SSCI)", "doi": "10.1109/SSCI47803.2020.9308512", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection in time series data is a significant problem faced in many\napplication areas such as manufacturing, medical imaging and cyber-security.\nRecently, Generative Adversarial Networks (GAN) have gained attention for\ngeneration and anomaly detection in image domain. In this paper, we propose a\nnovel GAN-based unsupervised method called TAnoGan for detecting anomalies in\ntime series when a small number of data points are available. We evaluate\nTAnoGan with 46 real-world time series datasets that cover a variety of\ndomains. Extensive experimental results show that TAnoGan performs better than\ntraditional and neural network models.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 16:24:51 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 01:50:33 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Bashar", "Md Abul", ""], ["Nayak", "Richi", ""]]}, {"id": "2008.09585", "submitter": "Nick Byrne", "authors": "Nick Byrne, James R. Clough, Giovanni Montana, Andrew P. King", "title": "A persistent homology-based topological loss function for multi-class\n  CNN segmentation of cardiac MRI", "comments": "To be presented at the STACOM workshop at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With respect to spatial overlap, CNN-based segmentation of short axis\ncardiovascular magnetic resonance (CMR) images has achieved a level of\nperformance consistent with inter observer variation. However, conventional\ntraining procedures frequently depend on pixel-wise loss functions, limiting\noptimisation with respect to extended or global features. As a result, inferred\nsegmentations can lack spatial coherence, including spurious connected\ncomponents or holes. Such results are implausible, violating the anticipated\ntopology of image segments, which is frequently known a priori. Addressing this\nchallenge, published work has employed persistent homology, constructing\ntopological loss functions for the evaluation of image segments against an\nexplicit prior. Building a richer description of segmentation topology by\nconsidering all possible labels and label pairs, we extend these losses to the\ntask of multi-class segmentation. These topological priors allow us to resolve\nall topological errors in a subset of 150 examples from the ACDC short axis CMR\ntraining data set, without sacrificing overlap performance.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 17:09:13 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Byrne", "Nick", ""], ["Clough", "James R.", ""], ["Montana", "Giovanni", ""], ["King", "Andrew P.", ""]]}, {"id": "2008.09604", "submitter": "Xueyan Zou", "authors": "Xueyan Zou, Fanyi Xiao, Zhiding Yu, Yong Jae Lee", "title": "Delving Deeper into Anti-aliasing in ConvNets", "comments": "[Accepted in BMVC2020] code: https://maureenzou.github.io/ddac/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aliasing refers to the phenomenon that high frequency signals degenerate into\ncompletely different ones after sampling. It arises as a problem in the context\nof deep learning as downsampling layers are widely adopted in deep\narchitectures to reduce parameters and computation. The standard solution is to\napply a low-pass filter (e.g., Gaussian blur) before downsampling. However, it\ncan be suboptimal to apply the same filter across the entire content, as the\nfrequency of feature maps can vary across both spatial locations and feature\nchannels. To tackle this, we propose an adaptive content-aware low-pass\nfiltering layer, which predicts separate filter weights for each spatial\nlocation and channel group of the input feature maps. We investigate the\neffectiveness and generalization of the proposed method across multiple tasks\nincluding ImageNet classification, COCO instance segmentation, and Cityscapes\nsemantic segmentation. Qualitative and quantitative results demonstrate that\nour approach effectively adapts to the different feature frequencies to avoid\naliasing while preserving useful information for recognition. Code is available\nat https://maureenzou.github.io/ddac/.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 17:56:04 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Zou", "Xueyan", ""], ["Xiao", "Fanyi", ""], ["Yu", "Zhiding", ""], ["Lee", "Yong Jae", ""]]}, {"id": "2008.09622", "submitter": "Changan Chen", "authors": "Changan Chen, Sagnik Majumder, Ziad Al-Halah, Ruohan Gao, Santhosh\n  Kumar Ramakrishnan, Kristen Grauman", "title": "Learning to Set Waypoints for Audio-Visual Navigation", "comments": "Accepted to ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In audio-visual navigation, an agent intelligently travels through a complex,\nunmapped 3D environment using both sights and sounds to find a sound source\n(e.g., a phone ringing in another room). Existing models learn to act at a\nfixed granularity of agent motion and rely on simple recurrent aggregations of\nthe audio observations. We introduce a reinforcement learning approach to\naudio-visual navigation with two key novel elements: 1) waypoints that are\ndynamically set and learned end-to-end within the navigation policy, and 2) an\nacoustic memory that provides a structured, spatially grounded record of what\nthe agent has heard as it moves. Both new ideas capitalize on the synergy of\naudio and visual data for revealing the geometry of an unmapped space. We\ndemonstrate our approach on two challenging datasets of real-world 3D scenes,\nReplica and Matterport3D. Our model improves the state of the art by a\nsubstantial margin, and our experiments reveal that learning the links between\nsights, sounds, and space is essential for audio-visual navigation. Project:\nhttp://vision.cs.utexas.edu/projects/audio_visual_waypoints.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 18:00:33 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 16:47:31 GMT"}, {"version": "v3", "created": "Thu, 11 Feb 2021 18:36:45 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Chen", "Changan", ""], ["Majumder", "Sagnik", ""], ["Al-Halah", "Ziad", ""], ["Gao", "Ruohan", ""], ["Ramakrishnan", "Santhosh Kumar", ""], ["Grauman", "Kristen", ""]]}, {"id": "2008.09634", "submitter": "Morteza Ghahremani", "authors": "Morteza Ghahremani, Bernard Tiddeman, Yonghuai Liu and Ardhendu Behera", "title": "Orderly Disorder in Point Cloud Domain", "comments": null, "journal-ref": "16th European Conference on Computer Vision (ECCV2020)", "doi": "10.1007/978-3-030-58604-1_30", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the real world, out-of-distribution samples, noise and distortions exist\nin test data. Existing deep networks developed for point cloud data analysis\nare prone to overfitting and a partial change in test data leads to\nunpredictable behaviour of the networks. In this paper, we propose a smart yet\nsimple deep network for analysis of 3D models using `orderly disorder' theory.\nOrderly disorder is a way of describing the complex structure of disorders\nwithin complex systems. Our method extracts the deep patterns inside a 3D\nobject via creating a dynamic link to seek the most stable patterns and at\nonce, throws away the unstable ones. Patterns are more robust to changes in\ndata distribution, especially those that appear in the top layers. Features are\nextracted via an innovative cloning decomposition technique and then linked to\neach other to form stable complex patterns. Our model alleviates the\nvanishing-gradient problem, strengthens dynamic link propagation and\nsubstantially reduces the number of parameters. Extensive experiments on\nchallenging benchmark datasets verify the superiority of our light network on\nthe segmentation and classification tasks, especially in the presence of noise\nwherein our network's performance drops less than 10% while the\nstate-of-the-art networks fail to work.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 18:18:09 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Ghahremani", "Morteza", ""], ["Tiddeman", "Bernard", ""], ["Liu", "Yonghuai", ""], ["Behera", "Ardhendu", ""]]}, {"id": "2008.09644", "submitter": "Martin Barczyk", "authors": "Pranoy Panda, Martin Barczyk", "title": "Blending of Learning-based Tracking and Object Detection for Monocular\n  Camera-based Target Following", "comments": "Accepted in 24th International Symposium on Mathematical Theory of\n  Networks and Systems (MTNS 2020): Cambridge, UK (updated conference date:\n  23-27 August 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has recently started being applied to visual tracking of\ngeneric objects in video streams. For the purposes of robotics applications, it\nis very important for a target tracker to recover its track if it is lost due\nto heavy or prolonged occlusions or motion blur of the target. We present a\nreal-time approach which fuses a generic target tracker and object detection\nmodule with a target re-identification module. Our work focuses on improving\nthe performance of Convolutional Recurrent Neural Network-based object trackers\nin cases where the object of interest belongs to the category of\n\\emph{familiar} objects. Our proposed approach is sufficiently lightweight to\ntrack objects at 85-90 FPS while attaining competitive results on challenging\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 18:44:35 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Panda", "Pranoy", ""], ["Barczyk", "Martin", ""]]}, {"id": "2008.09646", "submitter": "Abhinav Sagar", "authors": "Abhinav Sagar", "title": "HRVGAN: High Resolution Video Generation using Spatio-Temporal GAN", "comments": "The design of neural network was based on assumptions which was found\n  to be wrong", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel network for high resolution video\ngeneration. Our network uses ideas from Wasserstein GANs by enforcing\nk-Lipschitz constraint on the loss term and Conditional GANs using class labels\nfor training and testing. We present Generator and Discriminator network\nlayerwise details along with the combined network architecture, optimization\ndetails and algorithm used in this work. Our network uses a combination of two\nloss terms: mean square pixel loss and an adversarial loss. The datasets used\nfor training and testing our network are UCF101, Golf and Aeroplane Datasets.\nUsing Inception Score and Fr\\'echet Inception Distance as the evaluation\nmetrics, our network outperforms previous state of the art networks on\nunsupervised video generation.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 20:45:59 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 05:47:07 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Sagar", "Abhinav", ""]]}, {"id": "2008.09647", "submitter": "Meida Chen", "authors": "Meida Chen, Andrew Feng, Kyle McCullough, Pratusha Bhuvana Prasad,\n  Ryan McAlinden, Lucio Soibelman", "title": "Generating synthetic photogrammetric data for training deep learning\n  based 3D point cloud segmentation models", "comments": null, "journal-ref": "Interservice/Industry Training, Simulation, and Education\n  Conference (I/ITSEC) 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At I/ITSEC 2019, the authors presented a fully-automated workflow to segment\n3D photogrammetric point-clouds/meshes and extract object information,\nincluding individual tree locations and ground materials (Chen et al., 2019).\nThe ultimate goal is to create realistic virtual environments and provide the\nnecessary information for simulation. We tested the generalizability of the\npreviously proposed framework using a database created under the U.S. Army's\nOne World Terrain (OWT) project with a variety of landscapes (i.e., various\nbuildings styles, types of vegetation, and urban density) and different data\nqualities (i.e., flight altitudes and overlap between images). Although the\ndatabase is considerably larger than existing databases, it remains unknown\nwhether deep-learning algorithms have truly achieved their full potential in\nterms of accuracy, as sizable data sets for training and validation are\ncurrently lacking. Obtaining large annotated 3D point-cloud databases is\ntime-consuming and labor-intensive, not only from a data annotation perspective\nin which the data must be manually labeled by well-trained personnel, but also\nfrom a raw data collection and processing perspective. Furthermore, it is\ngenerally difficult for segmentation models to differentiate objects, such as\nbuildings and tree masses, and these types of scenarios do not always exist in\nthe collected data set. Thus, the objective of this study is to investigate\nusing synthetic photogrammetric data to substitute real-world data in training\ndeep-learning algorithms. We have investigated methods for generating synthetic\nUAV-based photogrammetric data to provide a sufficiently sized database for\ntraining a deep-learning algorithm with the ability to enlarge the data size\nfor scenarios in which deep-learning models have difficulties.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 18:50:42 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Chen", "Meida", ""], ["Feng", "Andrew", ""], ["McCullough", "Kyle", ""], ["Prasad", "Pratusha Bhuvana", ""], ["McAlinden", "Ryan", ""], ["Soibelman", "Lucio", ""]]}, {"id": "2008.09648", "submitter": "Meida Chen", "authors": "Meida Chen, Andrew Feng, Kyle McCullough, Pratusha Bhuvana Prasad,\n  Ryan McAlinden, Lucio Soibelman", "title": "Semantic Segmentation and Data Fusion of Microsoft Bing 3D Cities and\n  Small UAV-based Photogrammetric Data", "comments": null, "journal-ref": "Interservice/Industry Training, Simulation, and Education\n  Conference (I/ITSEC) 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With state-of-the-art sensing and photogrammetric techniques, Microsoft Bing\nMaps team has created over 125 highly detailed 3D cities from 11 different\ncountries that cover hundreds of thousands of square kilometer areas. The 3D\ncity models were created using the photogrammetric technique with\nhigh-resolution images that were captured from aircraft-mounted cameras. Such a\nlarge 3D city database has caught the attention of the US Army for creating\nvirtual simulation environments to support military operations. However, the 3D\ncity models do not have semantic information such as buildings, vegetation, and\nground and cannot allow sophisticated user-level and system-level interaction.\nAt I/ITSEC 2019, the authors presented a fully automated data segmentation and\nobject information extraction framework for creating simulation terrain using\nUAV-based photogrammetric data. This paper discusses the next steps in\nextending our designed data segmentation framework for segmenting 3D city data.\nIn this study, the authors first investigated the strengths and limitations of\nthe existing framework when applied to the Bing data. The main differences\nbetween UAV-based and aircraft-based photogrammetric data are highlighted. The\ndata quality issues in the aircraft-based photogrammetric data, which can\nnegatively affect the segmentation performance, are identified. Based on the\nfindings, a workflow was designed specifically for segmenting Bing data while\nconsidering its characteristics. In addition, since the ultimate goal is to\ncombine the use of both small unmanned aerial vehicle (UAV) collected data and\nthe Bing data in a virtual simulation environment, data from these two sources\nneeded to be aligned and registered together. To this end, the authors also\nproposed a data registration workflow that utilized the traditional iterative\nclosest point (ICP) with the extracted semantic information.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 18:56:05 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Chen", "Meida", ""], ["Feng", "Andrew", ""], ["McCullough", "Kyle", ""], ["Prasad", "Pratusha Bhuvana", ""], ["McAlinden", "Ryan", ""], ["Soibelman", "Lucio", ""]]}, {"id": "2008.09655", "submitter": "Elizaveta Logacheva", "authors": "Elizaveta Logacheva, Roman Suvorov, Oleg Khomenko, Anton Mashikhin and\n  Victor Lempitsky", "title": "DeepLandscape: Adversarial Modeling of Landscape Video", "comments": "Accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build a new model of landscape videos that can be trained on a mixture of\nstatic landscape images as well as landscape animations. Our architecture\nextends StyleGAN model by augmenting it with parts that allow to model dynamic\nchanges in a scene. Once trained, our model can be used to generate realistic\ntime-lapse landscape videos with moving objects and time-of-the-day changes.\nFurthermore, by fitting the learned models to a static landscape image, the\nlatter can be reenacted in a realistic way. We propose simple but necessary\nmodifications to StyleGAN inversion procedure, which lead to in-domain latent\ncodes and allow to manipulate real images. Quantitative comparisons and user\nstudies suggest that our model produces more compelling animations of given\nphotographs than previously proposed methods. The results of our approach\nincluding comparisons with prior art can be seen in supplementary materials and\non the project page https://saic-mdal.github.io/deep-landscape\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 19:14:19 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Logacheva", "Elizaveta", ""], ["Suvorov", "Roman", ""], ["Khomenko", "Oleg", ""], ["Mashikhin", "Anton", ""], ["Lempitsky", "Victor", ""]]}, {"id": "2008.09661", "submitter": "Jonathan Siegel", "authors": "Jonathan W. Siegel, Jianhong Chen, Pengchuan Zhang, Jinchao Xu", "title": "Training Sparse Neural Networks using Compressed Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pruning the weights of neural networks is an effective and widely-used\ntechnique for reducing model size and inference complexity. We develop and test\na novel method based on compressed sensing which combines the pruning and\ntraining into a single step. Specifically, we utilize an adaptively weighted\n$\\ell^1$ penalty on the weights during training, which we combine with a\ngeneralization of the regularized dual averaging (RDA) algorithm in order to\ntrain sparse neural networks. The adaptive weighting we introduce corresponds\nto a novel regularizer based on the logarithm of the absolute value of the\nweights. We perform a series of ablation studies demonstrating the improvement\nprovided by the adaptive weighting and generalized RDA algorithm. Furthermore,\nnumerical experiments on the CIFAR-10, CIFAR-100, and ImageNet datasets\ndemonstrate that our method 1) trains sparser, more accurate networks than\nexisting state-of-the-art methods; 2) can be used to train sparse networks from\nscratch, i.e. from a random initialization, as opposed to initializing with a\nwell-trained base model; 3) acts as an effective regularizer, improving\ngeneralization accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 19:35:54 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 04:14:08 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Siegel", "Jonathan W.", ""], ["Chen", "Jianhong", ""], ["Zhang", "Pengchuan", ""], ["Xu", "Jinchao", ""]]}, {"id": "2008.09672", "submitter": "Jorge Beltr\\'an", "authors": "Jorge Beltr\\'an, Carlos Guindel, Irene Cort\\'es, Alejandro Barrera,\n  Armando Astudillo, Jes\\'us Urdiales, Mario \\'Alvarez, Farid Bekka, Vicente\n  Milan\\'es, and Fernando Garc\\'ia", "title": "Towards Autonomous Driving: a Multi-Modal 360$^{\\circ}$ Perception\n  Proposal", "comments": "Accepted for publication in IEEE ITSC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a multi-modal 360$^{\\circ}$ framework for 3D object detection\nand tracking for autonomous vehicles is presented. The process is divided into\nfour main stages. First, images are fed into a CNN network to obtain instance\nsegmentation of the surrounding road participants. Second, LiDAR-to-image\nassociation is performed for the estimated mask proposals. Then, the isolated\npoints of every object are processed by a PointNet ensemble to compute their\ncorresponding 3D bounding boxes and poses. Lastly, a tracking stage based on\nUnscented Kalman Filter is used to track the agents along time. The solution,\nbased on a novel sensor fusion configuration, provides accurate and reliable\nroad environment detection. A wide variety of tests of the system, deployed in\nan autonomous vehicle, have successfully assessed the suitability of the\nproposed perception stack in a real autonomous driving application.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 20:36:21 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Beltr\u00e1n", "Jorge", ""], ["Guindel", "Carlos", ""], ["Cort\u00e9s", "Irene", ""], ["Barrera", "Alejandro", ""], ["Astudillo", "Armando", ""], ["Urdiales", "Jes\u00fas", ""], ["\u00c1lvarez", "Mario", ""], ["Bekka", "Farid", ""], ["Milan\u00e9s", "Vicente", ""], ["Garc\u00eda", "Fernando", ""]]}, {"id": "2008.09688", "submitter": "Xi Wang", "authors": "Xi Wang, Zoya Bylinskii, Aaron Hertzmann, Robert Pepperell", "title": "Toward Quantifying Ambiguities in Artistic Images", "comments": null, "journal-ref": "ACM Trans. Applied Perception, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has long been hypothesized that perceptual ambiguities play an important\nrole in aesthetic experience: a work with some ambiguity engages a viewer more\nthan one that does not. However, current frameworks for testing this theory are\nlimited by the availability of stimuli and data collection methods. This paper\npresents an approach to measuring the perceptual ambiguity of a collection of\nimages. Crowdworkers are asked to describe image content, after different\nviewing durations. Experiments are performed using images created with\nGenerative Adversarial Networks, using the Artbreeder website. We show that\ntext processing of viewer responses can provide a fine-grained way to measure\nand describe image ambiguities.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 21:40:16 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Wang", "Xi", ""], ["Bylinskii", "Zoya", ""], ["Hertzmann", "Aaron", ""], ["Pepperell", "Robert", ""]]}, {"id": "2008.09694", "submitter": "Carlo Biffi", "authors": "Carlo Biffi, Steven McDonagh, Philip Torr, Ales Leonardis, Sarah\n  Parisot", "title": "Many-shot from Low-shot: Learning to Annotate using Mixed Supervision\n  for Object Detection", "comments": "Accepted at ECCV 2020. Camera-ready version and Appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection has witnessed significant progress by relying on large,\nmanually annotated datasets. Annotating such datasets is highly time consuming\nand expensive, which motivates the development of weakly supervised and\nfew-shot object detection methods. However, these methods largely underperform\nwith respect to their strongly supervised counterpart, as weak training signals\n\\emph{often} result in partial or oversized detections. Towards solving this\nproblem we introduce, for the first time, an online annotation module (OAM)\nthat learns to generate a many-shot set of \\emph{reliable} annotations from a\nlarger volume of weakly labelled images. Our OAM can be jointly trained with\nany fully supervised two-stage object detection method, providing additional\ntraining annotations on the fly. This results in a fully end-to-end strategy\nthat only requires a low-shot set of fully annotated images. The integration of\nthe OAM with Fast(er) R-CNN improves their performance by $17\\%$ mAP, $9\\%$\nAP50 on PASCAL VOC 2007 and MS-COCO benchmarks, and significantly outperforms\ncompeting methods using mixed supervision.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 22:06:43 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 17:26:13 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Biffi", "Carlo", ""], ["McDonagh", "Steven", ""], ["Torr", "Philip", ""], ["Leonardis", "Ales", ""], ["Parisot", "Sarah", ""]]}, {"id": "2008.09695", "submitter": "Huiqi Deng", "authors": "Huiqi Deng, Na Zou, Mengnan Du, Weifu Chen, Guocan Feng, and Xia Hu", "title": "A Unified Taylor Framework for Revisiting Attribution Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attribution methods have been developed to understand the decision-making\nprocess of machine learning models, especially deep neural networks, by\nassigning importance scores to individual features. Existing attribution\nmethods often built upon empirical intuitions and heuristics. There still lacks\na general and theoretical framework that not only can unify these attribution\nmethods, but also theoretically reveal their rationales, fidelity, and\nlimitations. To bridge the gap, in this paper, we propose a Taylor attribution\nframework and reformulate seven mainstream attribution methods into the\nframework. Based on reformulations, we analyze the attribution methods in terms\nof rationale, fidelity, and limitation. Moreover, We establish three principles\nfor a good attribution in the Taylor attribution framework, i.e., low\napproximation error, correct contribution assignment, and unbiased baseline\nselection. Finally, we empirically validate the Taylor reformulations and\nreveal a positive correlation between the attribution performance and the\nnumber of principles followed by the attribution method via benchmarking on\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 22:07:06 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 11:38:08 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 09:00:51 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Deng", "Huiqi", ""], ["Zou", "Na", ""], ["Du", "Mengnan", ""], ["Chen", "Weifu", ""], ["Feng", "Guocan", ""], ["Hu", "Xia", ""]]}, {"id": "2008.09697", "submitter": "Long Chen", "authors": "Long Chen, Zheheng Jiang, Lei Tong, Zhihua Liu, Aite Zhao, Qianni\n  Zhang, Junyu Dong, and Huiyu Zhou", "title": "Perceptual underwater image enhancement with deep learning and physical\n  priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underwater image enhancement, as a pre-processing step to improve the\naccuracy of the following object detection task, has drawn considerable\nattention in the field of underwater navigation and ocean exploration. However,\nmost of the existing underwater image enhancement strategies tend to consider\nenhancement and detection as two independent modules with no interaction, and\nthe practice of separate optimization does not always help the underwater\nobject detection task. In this paper, we propose two perceptual enhancement\nmodels, each of which uses a deep enhancement model with a detection perceptor.\nThe detection perceptor provides coherent information in the form of gradients\nto the enhancement model, guiding the enhancement model to generate patch level\nvisually pleasing images or detection favourable images. In addition, due to\nthe lack of training data, a hybrid underwater image synthesis model, which\nfuses physical priors and data-driven cues, is proposed to synthesize training\ndata and generalise our enhancement model for real-world underwater images.\nExperimental results show the superiority of our proposed method over several\nstate-of-the-art methods on both real-world and synthetic underwater datasets.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 22:11:34 GMT"}, {"version": "v2", "created": "Sat, 26 Sep 2020 21:30:53 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Chen", "Long", ""], ["Jiang", "Zheheng", ""], ["Tong", "Lei", ""], ["Liu", "Zhihua", ""], ["Zhao", "Aite", ""], ["Zhang", "Qianni", ""], ["Dong", "Junyu", ""], ["Zhou", "Huiyu", ""]]}, {"id": "2008.09713", "submitter": "Arshad Khan Dr", "authors": "Muhammad Aleem, Rahul Raj and Arshad Khan", "title": "Comparative performance analysis of the ResNet backbones of Mask RCNN to\n  segment the signs of COVID-19 in chest CT scans", "comments": "11 pages, 10 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19 has been detrimental in terms of the number of fatalities and rising\nnumber of critical patients across the world. According to the UNDP (United\nNational Development Programme) Socio-Economic programme, aimed at the COVID-19\ncrisis, the pandemic is far more than a health crisis: it is affecting\nsocieties and economies at their core. There has been greater developments\nrecently in the chest X-ray-based imaging technique as part of the COVID-19\ndiagnosis especially using Convolution Neural Networks (CNN) for recognising\nand classifying images. However, given the limitation of supervised labelled\nimaging data, the classification and predictive risk modelling of medical\ndiagnosis tend to compromise. This paper aims to identify and monitor the\neffects of COVID-19 on the human lungs by employing Deep Neural Networks on\naxial CT (Chest Computed Tomography) scan of lungs. We have adopted Mask RCNN,\nwith ResNet50 and ResNet101 as its backbone, to segment the regions, affected\nby COVID-19 coronavirus. Using the regions of human lungs, where symptoms have\nmanifested, the model classifies condition of the patient as either \"Mild\" or\n\"Alarming\". Moreover, the model is deployed on the Google Cloud Platform (GCP)\nto simulate the online usage of the model for performance evaluation and\naccuracy improvement. The ResNet101 backbone model produces an F1 score of 0.85\nand faster prediction scores with an average time of 9.04 seconds per\ninference.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 23:42:08 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Aleem", "Muhammad", ""], ["Raj", "Rahul", ""], ["Khan", "Arshad", ""]]}, {"id": "2008.09721", "submitter": "Huan Ling", "authors": "Bowen Chen, Huan Ling, Xiaohui Zeng, Gao Jun, Ziyue Xu, Sanja Fidler", "title": "ScribbleBox: Interactive Annotation Framework for Video Object\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Manually labeling video datasets for segmentation tasks is extremely time\nconsuming. In this paper, we introduce ScribbleBox, a novel interactive\nframework for annotating object instances with masks in videos. In particular,\nwe split annotation into two steps: annotating objects with tracked boxes, and\nlabeling masks inside these tracks. We introduce automation and interaction in\nboth steps. Box tracks are annotated efficiently by approximating the\ntrajectory using a parametric curve with a small number of control points which\nthe annotator can interactively correct. Our approach tolerates a modest amount\nof noise in the box placements, thus typically only a few clicks are needed to\nannotate tracked boxes to a sufficient accuracy. Segmentation masks are\ncorrected via scribbles which are efficiently propagated through time. We show\nsignificant performance gains in annotation efficiency over past work. We show\nthat our ScribbleBox approach reaches 88.92% J&F on DAVIS2017 with 9.14 clicks\nper box track, and 4 frames of scribble annotation.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 00:33:10 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Chen", "Bowen", ""], ["Ling", "Huan", ""], ["Zeng", "Xiaohui", ""], ["Jun", "Gao", ""], ["Xu", "Ziyue", ""], ["Fidler", "Sanja", ""]]}, {"id": "2008.09742", "submitter": "Chaowei Fang", "authors": "Feida Zhu, Chaowei Fang, Kai-Kuang Ma", "title": "PNEN: Pyramid Non-Local Enhanced Networks", "comments": "Accepted by Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2020.3019644", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing neural networks proposed for low-level image processing tasks are\nusually implemented by stacking convolution layers with limited kernel size.\nEvery convolution layer merely involves in context information from a small\nlocal neighborhood. More contextual features can be explored as more\nconvolution layers are adopted. However it is difficult and costly to take full\nadvantage of long-range dependencies. We propose a novel non-local module,\nPyramid Non-local Block, to build up connection between every pixel and all\nremain pixels. The proposed module is capable of efficiently exploiting\npairwise dependencies between different scales of low-level structures. The\ntarget is fulfilled through first learning a query feature map with full\nresolution and a pyramid of reference feature maps with downscaled resolutions.\nThen correlations with multi-scale reference features are exploited for\nenhancing pixel-level feature representation. The calculation procedure is\neconomical considering memory consumption and computational cost. Based on the\nproposed module, we devise a Pyramid Non-local Enhanced Networks for\nedge-preserving image smoothing which achieves state-of-the-art performance in\nimitating three classical image smoothing algorithms. Additionally, the pyramid\nnon-local block can be directly incorporated into convolution neural networks\nfor other image restoration tasks. We integrate it into two existing methods\nfor image denoising and single image super-resolution, achieving consistently\nimproved performance.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 03:10:48 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Zhu", "Feida", ""], ["Fang", "Chaowei", ""], ["Ma", "Kai-Kuang", ""]]}, {"id": "2008.09743", "submitter": "Guanghao Yin", "authors": "Guanghao Yin, Shouqian Sun, Dian Yu and Kejun Zhang", "title": "A Efficient Multimodal Framework for Large Scale Emotion Recognition by\n  Fusing Music and Electrodermal Activity Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considerable attention has been paid for physiological signal-based emotion\nrecognition in field of affective computing. For the reliability and user\nfriendly acquisition, Electrodermal Activity (EDA) has great advantage in\npractical applications. However, the EDA-based emotion recognition with\nhundreds of subjects still lacks effective solution. In this paper, our work\nmakes an attempt to fuse the subject individual EDA features and the external\nevoked music features. And we propose an end-to-end multimodal framework, the\n1-dimensional residual temporal and channel attention network (RTCAN-1D). For\nEDA features, the novel convex optimization-based EDA (CvxEDA) method is\napplied to decompose EDA signals into pahsic and tonic signals for mining the\ndynamic and steady features. The channel-temporal attention mechanism for\nEDA-based emotion recognition is firstly involved to improve the temporal- and\nchannel-wise representation. For music features, we process the music signal\nwith the open source toolkit openSMILE to obtain external feature vectors. The\nindividual emotion features from EDA signals and external emotion benchmarks\nfrom music are fused in the classifing layers. We have conducted systematic\ncomparisons on three multimodal datasets (PMEmo, DEAP, AMIGOS) for 2-classes\nvalance/arousal emotion recognition. Our proposed RTCAN-1D outperforms the\nexisting state-of-the-art models, which also validate that our work provides an\nreliable and efficient solution for large scale emotion recognition. Our code\nhas been released at https://github.com/guanghaoyin/RTCAN-1D.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 03:13:20 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Yin", "Guanghao", ""], ["Sun", "Shouqian", ""], ["Yu", "Dian", ""], ["Zhang", "Kejun", ""]]}, {"id": "2008.09747", "submitter": "Zeeshan Ahmad", "authors": "Zeeshan Ahmad and Naimul Khan", "title": "Towards Improved Human Action Recognition Using Convolutional Neural\n  Networks and Multimodal Fusion of Depth and Inertial Sensor Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper attempts at improving the accuracy of Human Action Recognition\n(HAR) by fusion of depth and inertial sensor data. Firstly, we transform the\ndepth data into Sequential Front view Images(SFI) and fine-tune the pre-trained\nAlexNet on these images. Then, inertial data is converted into Signal Images\n(SI) and another convolutional neural network (CNN) is trained on these images.\nFinally, learned features are extracted from both CNN, fused together to make a\nshared feature layer, and these features are fed to the classifier. We\nexperiment with two classifiers, namely Support Vector Machines (SVM) and\nsoftmax classifier and compare their performances. The recognition accuracies\nof each modality, depth data alone and sensor data alone are also calculated\nand compared with fusion based accuracies to highlight the fact that fusion of\nmodalities yields better results than individual modalities. Experimental\nresults on UTD-MHAD and Kinect 2D datasets show that proposed method achieves\nstate of the art results when compared to other recently proposed\nvisual-inertial action recognition methods.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 03:41:34 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Ahmad", "Zeeshan", ""], ["Khan", "Naimul", ""]]}, {"id": "2008.09748", "submitter": "Zeeshan Ahmad", "authors": "Zeeshan Ahmad and Naimul Khan", "title": "Multidomain Multimodal Fusion For Human Action Recognition Using\n  Inertial Sensors", "comments": null, "journal-ref": null, "doi": "10.1109/BigMM.2019.00074", "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the major reasons for misclassification of multiplex actions during\naction recognition is the unavailability of complementary features that provide\nthe semantic information about the actions. In different domains these features\nare present with different scales and intensities. In existing literature,\nfeatures are extracted independently in different domains, but the benefits\nfrom fusing these multidomain features are not realized. To address this\nchallenge and to extract complete set of complementary information, in this\npaper, we propose a novel multidomain multimodal fusion framework that extracts\ncomplementary and distinct features from different domains of the input\nmodality. We transform input inertial data into signal images, and then make\nthe input modality multidomain and multimodal by transforming spatial domain\ninformation into frequency and time-spectrum domain using Discrete Fourier\nTransform (DFT) and Gabor wavelet transform (GWT) respectively. Features in\ndifferent domains are extracted by Convolutional Neural networks (CNNs) and\nthen fused by Canonical Correlation based Fusion (CCF) for improving the\naccuracy of human action recognition. Experimental results on three inertial\ndatasets show the superiority of the proposed method when compared to the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 03:46:12 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Ahmad", "Zeeshan", ""], ["Khan", "Naimul", ""]]}, {"id": "2008.09753", "submitter": "Tai-Xiang Jiang", "authors": "Yi-Si Luo, Xi-Le Zhao, Tai-Xiang Jiang, Yu-Bang Zheng, Yi Chang", "title": "Unsupervised Hyperspectral Mixed Noise Removal Via Spatial-Spectral\n  Constrained Deep Image Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recently, convolutional neural network (CNN)-based methods are proposed for\nhyperspectral images (HSIs) denoising. Among them, unsupervised methods such as\nthe deep image prior (DIP) have received much attention because these methods\ndo not require any training data. However, DIP suffers from the\nsemi-convergence behavior, i.e., the iteration of DIP needs to terminate by\nreferring to the ground-truth image at the optimal iteration point. In this\npaper, we propose the spatial-spectral constrained deep image prior (S2DIP) for\nHSI mixed noise removal. Specifically, we incorporate DIP with a\nspatial-spectral total variation (SSTV) term to fully preserve the\nspatial-spectral local smoothness of the HSI and an $\\ell_1$-norm term to\ncapture the complex sparse noise. The proposed S2DIP jointly leverages the\nexpressive power brought from the deep CNN without any training data and\nexploits the HSI and noise structures via hand-crafted priors. Thus, our method\navoids the semi-convergence behavior, showing higher stabilities than DIP.\nMeanwhile, our method largely enhances the HSI denoising ability of DIP. To\ntackle the proposed denoising model, we develop an alternating direction\nmultiplier method algorithm. Extensive experiments demonstrate that the\nproposed S2DIP outperforms optimization-based and supervised CNN-based\nstate-of-the-art HSI denoising methods.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 04:25:08 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 14:22:11 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Luo", "Yi-Si", ""], ["Zhao", "Xi-Le", ""], ["Jiang", "Tai-Xiang", ""], ["Zheng", "Yu-Bang", ""], ["Chang", "Yi", ""]]}, {"id": "2008.09772", "submitter": "Yi Zhou", "authors": "Yi Zhou, Boyang Wang, Lei Huang, Shanshan Cui and Ling Shao", "title": "A Benchmark for Studying Diabetic Retinopathy: Segmentation, Grading,\n  and Transferability", "comments": null, "journal-ref": "IEEE Transactions on Medical Imaging (2020)", "doi": "10.1109/TMI.2020.3037771", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People with diabetes are at risk of developing an eye disease called diabetic\nretinopathy (DR). This disease occurs when high blood glucose levels cause\ndamage to blood vessels in the retina. Computer-aided DR diagnosis is a\npromising tool for early detection of DR and severity grading, due to the great\nsuccess of deep learning. However, most current DR diagnosis systems do not\nachieve satisfactory performance or interpretability for ophthalmologists, due\nto the lack of training data with consistent and fine-grained annotations. To\naddress this problem, we construct a large fine-grained annotated DR dataset\ncontaining 2,842 images (FGADR). This dataset has 1,842 images with pixel-level\nDR-related lesion annotations, and 1,000 images with image-level labels graded\nby six board-certified ophthalmologists with intra-rater consistency. The\nproposed dataset will enable extensive studies on DR diagnosis. We set up three\nbenchmark tasks for evaluation: 1. DR lesion segmentation; 2. DR grading by\njoint classification and segmentation; 3. Transfer learning for ocular\nmulti-disease identification. Moreover, a novel inductive transfer learning\nmethod is introduced for the third task. Extensive experiments using different\nstate-of-the-art methods are conducted on our FGADR dataset, which can serve as\nbaselines for future research.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 07:48:04 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 10:52:36 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 10:49:15 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Zhou", "Yi", ""], ["Wang", "Boyang", ""], ["Huang", "Lei", ""], ["Cui", "Shanshan", ""], ["Shao", "Ling", ""]]}, {"id": "2008.09773", "submitter": "Yoav Goldstein", "authors": "Yoav Goldstein, Martin Sch\\\"atz and Mireille Avigal", "title": "Chest Area Segmentation in Depth Images of Sleeping Patients", "comments": "13 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the field of sleep study has greatly developed over the recent\nyears, the most common and efficient way to detect sleep issues remains a sleep\nexamination performed in a sleep laboratory, in a procedure called\nPolysomnography (PSG). This examination measures several vital signals during a\nfull night's sleep using multiple sensors connected to the patient's body. Yet,\ndespite being the golden standard, the connection of the sensors and the\nunfamiliar environment inevitably impact the quality of the patient's sleep and\nthe examination itself. Therefore, with the novel development of more accurate\nand affordable 3D sensing devices, new approaches for non-contact sleep study\nemerged. These methods utilize different techniques with the purpose to extract\nthe same sleep parameters, but remotely, eliminating the need of any physical\nconnections to the patient's body. However, in order to enable reliable remote\nextraction, these methods require accurate identification of the basic Region\nof Interest (ROI) i.e. the chest area of the patient, a task that is currently\nholding back the development process, as it is performed manually for each\npatient. In this study, we propose an automatic chest area segmentation\nalgorithm, that given an input set of 3D frames of a sleeping patient, outputs\na segmentation image with the pixels that correspond to the chest area, and can\nthen be used as an input to subsequent sleep analysis algorithms. Except for\nsignificantly speeding up the development process of the non-contact methods,\naccurate automatic segmentation can also enable a more precise feature\nextraction and it is shown it is already improving sensitivity of prior\nsolutions on average 46.9% better compared to manual ROI selection. All\nmentioned will place the extraction algorithms of the non-contact methods as a\nleading candidate to replace the existing traditional methods used today.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 07:54:32 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Goldstein", "Yoav", ""], ["Sch\u00e4tz", "Martin", ""], ["Avigal", "Mireille", ""]]}, {"id": "2008.09785", "submitter": "Hung-Min Hsu", "authors": "Hung-Min Hsu, Yizhou Wang, Jenq-Neng Hwang", "title": "Traffic-Aware Multi-Camera Tracking of Vehicles Based on ReID and Camera\n  Link Model", "comments": "Accepted by ACM International Conference on Multimedia 2020", "journal-ref": null, "doi": "10.1145/3394171.3413863", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-target multi-camera tracking (MTMCT), i.e., tracking multiple targets\nacross multiple cameras, is a crucial technique for smart city applications. In\nthis paper, we propose an effective and reliable MTMCT framework for vehicles,\nwhich consists of a traffic-aware single camera tracking (TSCT) algorithm, a\ntrajectory-based camera link model (CLM) for vehicle re-identification (ReID),\nand a hierarchical clustering algorithm to obtain the cross camera vehicle\ntrajectories. First, the TSCT, which jointly considers vehicle appearance,\ngeometric features, and some common traffic scenarios, is proposed to track the\nvehicles in each camera separately. Second, the trajectory-based CLM is adopted\nto facilitate the relationship between each pair of adjacently connected\ncameras and add spatio-temporal constraints for the subsequent vehicle ReID\nwith temporal attention. Third, the hierarchical clustering algorithm is used\nto merge the vehicle trajectories among all the cameras to obtain the final\nMTMCT results. Our proposed MTMCT is evaluated on the CityFlow dataset and\nachieves a new state-of-the-art performance with IDF1 of 74.93%.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 08:54:47 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 04:47:55 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Hsu", "Hung-Min", ""], ["Wang", "Yizhou", ""], ["Hwang", "Jenq-Neng", ""]]}, {"id": "2008.09791", "submitter": "Jae Sung Park", "authors": "Jae Sung Park, Trevor Darrell, Anna Rohrbach", "title": "Identity-Aware Multi-Sentence Video Description", "comments": "Project link at\n  https://sites.google.com/site/describingmovies/lsmdc-2019/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard video and movie description tasks abstract away from person\nidentities, thus failing to link identities across sentences. We propose a\nmulti-sentence Identity-Aware Video Description task, which overcomes this\nlimitation and requires to re-identify persons locally within a set of\nconsecutive clips. We introduce an auxiliary task of Fill-in the Identity, that\naims to predict persons' IDs consistently within a set of clips, when the video\ndescriptions are given. Our proposed approach to this task leverages a\nTransformer architecture allowing for coherent joint prediction of multiple\nIDs. One of the key components is a gender-aware textual representation as well\nan additional gender prediction objective in the main model. This auxiliary\ntask allows us to propose a two-stage approach to Identity-Aware Video\nDescription. We first generate multi-sentence video descriptions, and then\napply our Fill-in the Identity model to establish links between the predicted\nperson entities. To be able to tackle both tasks, we augment the Large Scale\nMovie Description Challenge (LSMDC) benchmark with new annotations suited for\nour problem statement. Experiments show that our proposed Fill-in the Identity\nmodel is superior to several baselines and recent works, and allows us to\ngenerate descriptions with locally re-identified people.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 09:50:43 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Park", "Jae Sung", ""], ["Darrell", "Trevor", ""], ["Rohrbach", "Anna", ""]]}, {"id": "2008.09809", "submitter": "Jialun Liu", "authors": "Jialun Liu, Jingwei Zhang, Yi yang, Wenhui Li, Chi Zhang and Yifan Sun", "title": "Memory-based Jitter: Improving Visual Recognition on Long-tailed Data\n  with Diversity In Memory", "comments": "We modify the method based on the arXiv version. We don't want to\n  publish it ,now. So we want to withdraw the arXiv version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers deep visual recognition on long-tailed data. To be\ngeneral, we consider two applied scenarios, \\ie, deep classification and deep\nmetric learning. Under the long-tailed data distribution, the majority classes\n(\\ie, tail classes) only occupy relatively few samples and are prone to lack of\nwithin-class diversity. A radical solution is to augment the tail classes with\nhigher diversity. To this end, we introduce a simple and reliable method named\nMemory-based Jitter (MBJ). We observe that during training, the deep model\nconstantly changes its parameters after every iteration, yielding the\nphenomenon of \\emph{weight jitters}. Consequentially, given a same image as the\ninput, two historical editions of the model generate two different features in\nthe deeply-embedded space, resulting in \\emph{feature jitters}. Using a memory\nbank, we collect these (model or feature) jitters across multiple training\niterations and get the so-called Memory-based Jitter. The accumulated jitters\nenhance the within-class diversity for the tail classes and consequentially\nimproves long-tailed visual recognition. With slight modifications, MBJ is\napplicable for two fundamental visual recognition tasks, \\emph{i.e.}, deep\nimage classification and deep metric learning (on long-tailed data). Extensive\nexperiments on five long-tailed classification benchmarks and two deep metric\nlearning benchmarks demonstrate significant improvement. Moreover, the achieved\nperformance are on par with the state of the art on both tasks.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 11:01:46 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 02:44:05 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2020 03:52:29 GMT"}, {"version": "v4", "created": "Mon, 7 Dec 2020 08:52:34 GMT"}, {"version": "v5", "created": "Fri, 19 Mar 2021 08:44:00 GMT"}, {"version": "v6", "created": "Tue, 6 Jul 2021 07:49:04 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Liu", "Jialun", ""], ["Zhang", "Jingwei", ""], ["yang", "Yi", ""], ["Li", "Wenhui", ""], ["Zhang", "Chi", ""], ["Sun", "Yifan", ""]]}, {"id": "2008.09824", "submitter": "Iman Saberi", "authors": "Iman Saberi, Fathiyeh Faghih", "title": "Self-Competitive Neural Networks", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have improved the accuracy of classification\nproblems in lots of applications. One of the challenges in training a DNN is\nits need to be fed by an enriched dataset to increase its accuracy and avoid it\nsuffering from overfitting. One way to improve the generalization of DNNs is to\naugment the training data with new synthesized adversarial samples. Recently,\nresearchers have worked extensively to propose methods for data augmentation.\nIn this paper, we generate adversarial samples to refine the Domains of\nAttraction (DoAs) of each class. In this approach, at each stage, we use the\nmodel learned by the primary and generated adversarial data (up to that stage)\nto manipulate the primary data in a way that look complicated to the DNN. The\nDNN is then retrained using the augmented data and then it again generates\nadversarial data that are hard to predict for itself. As the DNN tries to\nimprove its accuracy by competing with itself (generating hard samples and then\nlearning them), the technique is called Self-Competitive Neural Network (SCNN).\nTo generate such samples, we pose the problem as an optimization task, where\nthe network weights are fixed and use a gradient descent based method to\nsynthesize adversarial samples that are on the boundary of their true labels\nand the nearest wrong labels. Our experimental results show that data\naugmentation using SCNNs can significantly increase the accuracy of the\noriginal network. As an example, we can mention improving the accuracy of a CNN\ntrained with 1000 limited training data of MNIST dataset from 94.26% to 98.25%.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 12:28:35 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Saberi", "Iman", ""], ["Faghih", "Fathiyeh", ""]]}, {"id": "2008.09831", "submitter": "Filipa Valdeira", "authors": "Filipa Valdeira, Ricardo Ferreira, Alessandra Micheletti, Cl\\'audia\n  Soares", "title": "RANSIP : From noisy point clouds to complete ear models, unsupervised", "comments": "15 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ears are a particularly difficult region of the human face to model, not only\ndue to the non-rigid deformations existing between shapes but also to the\nchallenges in processing the retrieved data. The first step towards obtaining a\ngood model is to have complete scans in correspondence, but these usually\npresent a higher amount of occlusions, noise and outliers when compared to most\nface regions, thus requiring a specific procedure. Therefore, we propose a\ncomplete pipeline taking as input unordered 3D point clouds with the\naforementioned problems, and producing as output a dataset in correspondence,\nwith completion of the missing data. We provide a comparison of several\nstate-of-the-art registration methods and propose a new approach for one of the\nsteps of the pipeline, with better performance for our data.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 13:20:43 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Valdeira", "Filipa", ""], ["Ferreira", "Ricardo", ""], ["Micheletti", "Alessandra", ""], ["Soares", "Cl\u00e1udia", ""]]}, {"id": "2008.09837", "submitter": "Le Yang", "authors": "Le Yang, Houwen Peng, Dingwen Zhang, Jianlong Fu, Junwei Han", "title": "Revisiting Anchor Mechanisms for Temporal Action Localization", "comments": "Accept by TIP, code: https://github.com/LeYangNwpu/A2Net", "journal-ref": null, "doi": "10.1109/TIP.2020.3016486", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the current action localization methods follow an anchor-based\npipeline: depicting action instances by pre-defined anchors, learning to select\nthe anchors closest to the ground truth, and predicting the confidence of\nanchors with refinements. Pre-defined anchors set prior about the location and\nduration for action instances, which facilitates the localization for common\naction instances but limits the flexibility for tackling action instances with\ndrastic varieties, especially for extremely short or extremely long ones. To\naddress this problem, this paper proposes a novel anchor-free action\nlocalization module that assists action localization by temporal points.\nSpecifically, this module represents an action instance as a point with its\ndistances to the starting boundary and ending boundary, alleviating the\npre-defined anchor restrictions in terms of action localization and duration.\nThe proposed anchor-free module is capable of predicting the action instances\nwhose duration is either extremely short or extremely long. By combining the\nproposed anchor-free module with a conventional anchor-based module, we propose\na novel action localization framework, called A2Net. The cooperation between\nanchor-free and anchor-based modules achieves superior performance to the\nstate-of-the-art on THUMOS14 (45.5% vs. 42.8%). Furthermore, comprehensive\nexperiments demonstrate the complementarity between the anchor-free and the\nanchor-based module, making A2Net simple but effective.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 13:39:29 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Yang", "Le", ""], ["Peng", "Houwen", ""], ["Zhang", "Dingwen", ""], ["Fu", "Jianlong", ""], ["Han", "Junwei", ""]]}, {"id": "2008.09849", "submitter": "Alex Falcon", "authors": "Alex Falcon, Oswald Lanz, Giuseppe Serra", "title": "Data augmentation techniques for the Video Question Answering task", "comments": "16 pages, 5 figures; to be published in Egocentric Perception,\n  Interaction and Computing (EPIC) Workshop Proceedings, at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video Question Answering (VideoQA) is a task that requires a model to analyze\nand understand both the visual content given by the input video and the textual\npart given by the question, and the interaction between them in order to\nproduce a meaningful answer. In our work we focus on the Egocentric VideoQA\ntask, which exploits first-person videos, because of the importance of such\ntask which can have impact on many different fields, such as those pertaining\nthe social assistance and the industrial training. Recently, an Egocentric\nVideoQA dataset, called EgoVQA, has been released. Given its small size, models\ntend to overfit quickly. To alleviate this problem, we propose several\naugmentation techniques which give us a +5.5% improvement on the final accuracy\nover the considered baseline.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 14:34:55 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Falcon", "Alex", ""], ["Lanz", "Oswald", ""], ["Serra", "Giuseppe", ""]]}, {"id": "2008.09860", "submitter": "Aritra Chowdhury", "authors": "Aritra Chowdhury, Alberto Santamaria-Pang, James R. Kubricht, Peter Tu", "title": "Emergent symbolic language based deep medical image classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep learning systems for medical image classification have\ndemonstrated exceptional capabilities for distinguishing between image based\nmedical categories. However, they are severely hindered by their ina-bility to\nexplain the reasoning behind their decision making. This is partly due to the\nuninterpretable continuous latent representations of neural net-works. Emergent\nlanguages (EL) have recently been shown to enhance the capabilities of neural\nnetworks by equipping them with symbolic represen-tations in the framework of\nreferential games. Symbolic representations are one of the cornerstones of\nhighly explainable good old fashioned AI (GOFAI) systems. In this work, we\ndemonstrate for the first time, the emer-gence of deep symbolic representations\nof emergent language in the frame-work of image classification. We show that EL\nbased classification models can perform as well as, if not better than state of\nthe art deep learning mod-els. In addition, they provide a symbolic\nrepresentation that opens up an entire field of possibilities of interpretable\nGOFAI methods involving symbol manipulation. We demonstrate the EL\nclassification framework on immune cell marker based cell classification and\nchest X-ray classification using the CheXpert dataset. Code is available online\nat https://github.com/AriChow/EL.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 15:53:29 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Chowdhury", "Aritra", ""], ["Santamaria-Pang", "Alberto", ""], ["Kubricht", "James R.", ""], ["Tu", "Peter", ""]]}, {"id": "2008.09866", "submitter": "Aritra Chowdhury", "authors": "Aritra Chowdhury, Alberto Santamaria-Pang, James R. Kubricht, Jianwei\n  Qiu, Peter Tu", "title": "Symbolic Semantic Segmentation and Interpretation of COVID-19 Lung\n  Infections in Chest CT volumes based on Emergent Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coronavirus disease (COVID-19) has resulted in a pandemic crippling the a\nbreadth of services critical to daily life. Segmentation of lung infections in\ncomputerized tomography (CT) slices could be be used to improve diagnosis and\nunderstanding of COVID-19 in patients. Deep learning systems lack\ninterpretability because of their black box nature. Inspired by human\ncommunication of complex ideas through language, we propose a symbolic\nframework based on emergent languages for the segmentation of COVID-19\ninfections in CT scans of lungs. We model the cooperation between two\nartificial agents - a Sender and a Receiver. These agents synergistically\ncooperate using emergent symbolic language to solve the task of semantic\nsegmentation. Our game theoretic approach is to model the cooperation between\nagents unlike Generative Adversarial Networks (GANs). The Sender retrieves\ninformation from one of the higher layers of the deep network and generates a\nsymbolic sentence sampled from a categorical distribution of vocabularies. The\nReceiver ingests the stream of symbols and cogenerates the segmentation mask. A\nprivate emergent language is developed that forms the communication channel\nused to describe the task of segmentation of COVID infections. We augment\nexisting state of the art semantic segmentation architectures with our symbolic\ngenerator to form symbolic segmentation models. Our symbolic segmentation\nframework achieves state of the art performance for segmentation of lung\ninfections caused by COVID-19. Our results show direct interpretation of\nsymbolic sentences to discriminate between normal and infected regions,\ninfection morphology and image characteristics. We show state of the art\nresults for segmentation of COVID-19 lung infections in CT.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 16:19:11 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Chowdhury", "Aritra", ""], ["Santamaria-Pang", "Alberto", ""], ["Kubricht", "James R.", ""], ["Qiu", "Jianwei", ""], ["Tu", "Peter", ""]]}, {"id": "2008.09872", "submitter": "Xuanji Xiao", "authors": "Xuanji Xiao, Huabin Chen, Yuzhen Liu, Xing Yao, Pei Liu, Chaosheng\n  Fan, Nian Ji, Xirong Jiang", "title": "LT4REC:A Lottery Ticket Hypothesis Based Multi-task Practice for Video\n  Recommendation System", "comments": "6 pages,4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-through rate prediction (CTR) and post-click conversion rate prediction\n(CVR) play key roles across all industrial ranking systems, such as\nrecommendation systems, online advertising, and search engines. Different from\nthe extensive research on CTR, there is much less research on CVR estimation,\nwhose main challenge is extreme data sparsity with one or two orders of\nmagnitude reduction in the number of samples than CTR. People try to solve this\nproblem with the paradigm of multi-task learning with the sufficient samples of\nCTR, but the typical hard sharing method can't effectively solve this problem,\nbecause it is difficult to analyze which parts of network components can be\nshared and which parts are in conflict, i.e., there is a large inaccuracy with\nartificially designed neurons sharing. In this paper, we model CVR in a\nbrand-new method by adopting the lottery-ticket-hypothesis-based sparse sharing\nmulti-task learning, which can automatically and flexibly learn which neuron\nweights to be shared without artificial experience. Experiments on the dataset\ngathered from traffic logs of Tencent video's recommendation system demonstrate\nthat sparse sharing in the CVR model significantly outperforms competitive\nmethods. Due to the nature of weight sparsity in sparse sharing, it can also\nsignificantly reduce computational complexity and memory usage which are very\nimportant in the industrial recommendation system.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 16:48:08 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Xiao", "Xuanji", ""], ["Chen", "Huabin", ""], ["Liu", "Yuzhen", ""], ["Yao", "Xing", ""], ["Liu", "Pei", ""], ["Fan", "Chaosheng", ""], ["Ji", "Nian", ""], ["Jiang", "Xirong", ""]]}, {"id": "2008.09880", "submitter": "Ujjal Kr Dutta", "authors": "Ujjal Kr Dutta, Mehrtash Harandi and Chellu Chandra Sekhar", "title": "Unsupervised Deep Metric Learning via Orthogonality based Probabilistic\n  Loss", "comments": "In the IEEE Transactions on Artificial Intelligence (IEEE TAI)", "journal-ref": null, "doi": "10.1109/TAI.2020.3026982", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric learning is an important problem in machine learning. It aims to group\nsimilar examples together. Existing state-of-the-art metric learning approaches\nrequire class labels to learn a metric. As obtaining class labels in all\napplications is not feasible, we propose an unsupervised approach that learns a\nmetric without making use of class labels. The lack of class labels is\ncompensated by obtaining pseudo-labels of data using a graph-based clustering\napproach. The pseudo-labels are used to form triplets of examples, which guide\nthe metric learning. We propose a probabilistic loss that minimizes the chances\nof each triplet violating an angular constraint. A weight function, and an\northogonality constraint in the objective speeds up the convergence and avoids\na model collapse. We also provide a stochastic formulation of our method to\nscale up to large-scale datasets. Our studies demonstrate the competitiveness\nof our approach against state-of-the-art methods. We also thoroughly study the\neffect of the different components of our method.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 17:13:33 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Dutta", "Ujjal Kr", ""], ["Harandi", "Mehrtash", ""], ["Sekhar", "Chellu Chandra", ""]]}, {"id": "2008.09884", "submitter": "Ruizhi Liao", "authors": "Geeticka Chauhan, Ruizhi Liao, William Wells, Jacob Andreas, Xin Wang,\n  Seth Berkowitz, Steven Horng, Peter Szolovits, Polina Golland", "title": "Joint Modeling of Chest Radiographs and Radiology Reports for Pulmonary\n  Edema Assessment", "comments": "The two first authors contributed equally. To be published in the\n  proceedings of MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and demonstrate a novel machine learning algorithm that assesses\npulmonary edema severity from chest radiographs. While large publicly available\ndatasets of chest radiographs and free-text radiology reports exist, only\nlimited numerical edema severity labels can be extracted from radiology\nreports. This is a significant challenge in learning such models for image\nclassification. To take advantage of the rich information present in the\nradiology reports, we develop a neural network model that is trained on both\nimages and free-text to assess pulmonary edema severity from chest radiographs\nat inference time. Our experimental results suggest that the joint image-text\nrepresentation learning improves the performance of pulmonary edema assessment\ncompared to a supervised model trained on images only. We also show the use of\nthe text for explaining the image classification by the joint model. To the\nbest of our knowledge, our approach is the first to leverage free-text\nradiology reports for improving the image model performance in this\napplication. Our code is available at\nhttps://github.com/RayRuizhiLiao/joint_chestxray.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 17:28:39 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Chauhan", "Geeticka", ""], ["Liao", "Ruizhi", ""], ["Wells", "William", ""], ["Andreas", "Jacob", ""], ["Wang", "Xin", ""], ["Berkowitz", "Seth", ""], ["Horng", "Steven", ""], ["Szolovits", "Peter", ""], ["Golland", "Polina", ""]]}, {"id": "2008.09890", "submitter": "Dima Damen", "authors": "Dima Damen and Michael Wray", "title": "Supervision Levels Scale (SLS)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We propose a three-dimensional discrete and incremental scale to encode a\nmethod's level of supervision - i.e. the data and labels used when training a\nmodel to achieve a given performance. We capture three aspects of supervision,\nthat are known to give methods an advantage while requiring additional costs:\npre-training, training labels and training data. The proposed three-dimensional\nscale can be included in result tables or leaderboards to handily compare\nmethods not only by their performance, but also by the level of data\nsupervision utilised by each method. The Supervision Levels Scale (SLS) is\nfirst presented generally fo any task/dataset/challenge. It is then applied to\nthe EPIC-KITCHENS-100 dataset, to be used for the various leaderboards and\nchallenges associated with this dataset.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 18:03:20 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Damen", "Dima", ""], ["Wray", "Michael", ""]]}, {"id": "2008.09891", "submitter": "Hossein Kashiani", "authors": "Hossein Kashiani, Amir Abbas Hamidi Imani, Shahriar Baradaran\n  Shokouhi, Ahmad Ayatollahi", "title": "Online Visual Tracking with One-Shot Context-Aware Domain Adaptation", "comments": "36 pages, 1 algorithm, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online learning policy makes visual trackers more robust against different\ndistortions through learning domain-specific cues. However, the trackers\nadopting this policy fail to fully leverage the discriminative context of the\nbackground areas. Moreover, owing to the lack of sufficient data at each time\nstep, the online learning approach can also make the trackers prone to\nover-fitting to the background regions. In this paper, we propose a domain\nadaptation approach to strengthen the contributions of the semantic background\ncontext. The domain adaptation approach is backboned with only an off-the-shelf\ndeep model. The strength of the proposed approach comes from its discriminative\nability to handle severe occlusion and background clutter challenges. We\nfurther introduce a cost-sensitive loss alleviating the dominance of\nnon-semantic background candidates over the semantic candidates, thereby\ndealing with the data imbalance issue. Experimental results demonstrate that\nour tracker achieves competitive results at real-time speed compared to the\nstate-of-the-art trackers.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 18:13:12 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 22:31:27 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kashiani", "Hossein", ""], ["Imani", "Amir Abbas Hamidi", ""], ["Shokouhi", "Shahriar Baradaran", ""], ["Ayatollahi", "Ahmad", ""]]}, {"id": "2008.09892", "submitter": "Yan Xu", "authors": "Vivek Roy, Yan Xu, Yu-Xiong Wang, Kris Kitani, Ruslan Salakhutdinov,\n  and Martial Hebert", "title": "Few-Shot Learning with Intra-Class Knowledge Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the few-shot classification task with an unbalanced dataset, in\nwhich some classes have sufficient training samples while other classes only\nhave limited training samples. Recent works have proposed to solve this task by\naugmenting the training data of the few-shot classes using generative models\nwith the few-shot training samples as the seeds. However, due to the limited\nnumber of the few-shot seeds, the generated samples usually have small\ndiversity, making it difficult to train a discriminative classifier for the\nfew-shot classes. To enrich the diversity of the generated samples, we propose\nto leverage the intra-class knowledge from the neighbor many-shot classes with\nthe intuition that neighbor classes share similar statistical information. Such\nintra-class information is obtained with a two-step mechanism. First, a\nregressor trained only on the many-shot classes is used to evaluate the\nfew-shot class means from only a few samples. Second, superclasses are\nclustered, and the statistical mean and feature variance of each superclass are\nused as transferable knowledge inherited by the children few-shot classes. Such\nknowledge is then used by a generator to augment the sparse training data to\nhelp the downstream classification tasks. Extensive experiments show that our\nmethod achieves state-of-the-art across different datasets and $n$-shot\nsettings.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 18:15:38 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Roy", "Vivek", ""], ["Xu", "Yan", ""], ["Wang", "Yu-Xiong", ""], ["Kitani", "Kris", ""], ["Salakhutdinov", "Ruslan", ""], ["Hebert", "Martial", ""]]}, {"id": "2008.09916", "submitter": "Ting-Wu Chin", "authors": "Ting-Wu Chin, Pierce I-Jen Chuang, Vikas Chandra, Diana Marculescu", "title": "One Weight Bitwidth to Rule Them All", "comments": "Accepted at ECCV 2020 Embedded Vision Workshop (Best paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weight quantization for deep ConvNets has shown promising results for\napplications such as image classification and semantic segmentation and is\nespecially important for applications where memory storage is limited. However,\nwhen aiming for quantization without accuracy degradation, different tasks may\nend up with different bitwidths. This creates complexity for software and\nhardware support and the complexity accumulates when one considers\nmixed-precision quantization, in which case each layer's weights use a\ndifferent bitwidth. Our key insight is that optimizing for the least bitwidth\nsubject to no accuracy degradation is not necessarily an optimal strategy. This\nis because one cannot decide optimality between two bitwidths if one has a\nsmaller model size while the other has better accuracy. In this work, we take\nthe first step to understand if some weight bitwidth is better than others by\naligning all to the same model size using a width-multiplier. Under this\nsetting, somewhat surprisingly, we show that using a single bitwidth for the\nwhole network can achieve better accuracy compared to mixed-precision\nquantization targeting zero accuracy degradation when both have the same model\nsize. In particular, our results suggest that when the number of channels\nbecomes a target hyperparameter, a single weight bitwidth throughout the\nnetwork shows superior results for model compression.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 21:40:22 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 18:49:48 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Chin", "Ting-Wu", ""], ["Chuang", "Pierce I-Jen", ""], ["Chandra", "Vikas", ""], ["Marculescu", "Diana", ""]]}, {"id": "2008.09918", "submitter": "Oscar Koller", "authors": "Oscar Koller", "title": "Quantitative Survey of the State of the Art in Sign Language Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a meta study covering around 300 published sign language\nrecognition papers with over 400 experimental results. It includes most papers\nbetween the start of the field in 1983 and 2020. Additionally, it covers a\nfine-grained analysis on over 25 studies that have compared their recognition\napproaches on RWTH-PHOENIX-Weather 2014, the standard benchmark task of the\nfield. Research in the domain of sign language recognition has progressed\nsignificantly in the last decade, reaching a point where the task attracts much\nmore attention than ever before. This study compiles the state of the art in a\nconcise way to help advance the field and reveal open questions. Moreover, all\nof this meta study's source data is made public, easing future work with it and\nfurther expansion. The analyzed papers have been manually labeled with a set of\ncategories. The data reveals many insights, such as, among others, shifts in\nthe field from intrusive to non-intrusive capturing, from local to global\nfeatures and the lack of non-manual parameters included in medium and larger\nvocabulary recognition systems. Surprisingly, RWTH-PHOENIX-Weather with a\nvocabulary of 1080 signs represents the only resource for large vocabulary\ncontinuous sign language recognition benchmarking world wide.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 21:57:48 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2020 10:49:47 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Koller", "Oscar", ""]]}, {"id": "2008.09942", "submitter": "Guizhong Liu", "authors": "Jianyi Li and Guizhong Liu", "title": "Few-Shot Image Classification via Contrastive Self-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most previous few-shot learning algorithms are based on meta-training with\nfake few-shot tasks as training samples, where large labeled base classes are\nrequired. The trained model is also limited by the type of tasks. In this paper\nwe propose a new paradigm of unsupervised few-shot learning to repair the\ndeficiencies. We solve the few-shot tasks in two phases: meta-training a\ntransferable feature extractor via contrastive self-supervised learning and\ntraining a classifier using graph aggregation, self-distillation and manifold\naugmentation. Once meta-trained, the model can be used in any type of tasks\nwith a task-dependent classifier training. Our method achieves state of-the-art\nperformance in a variety of established few-shot tasks on the standard few-shot\nvisual classification datasets, with an 8- 28% increase compared to the\navailable unsupervised few-shot learning methods.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 02:24:31 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Li", "Jianyi", ""], ["Liu", "Guizhong", ""]]}, {"id": "2008.09958", "submitter": "Kaiyu Yue", "authors": "Kaiyu Yue, Jiangfan Deng, Feng Zhou", "title": "Matching Guided Distillation", "comments": "ECCV 2020 Camera-Ready. Project: http://kaiyuyue.com/mgd", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature distillation is an effective way to improve the performance for a\nsmaller student model, which has fewer parameters and lower computation cost\ncompared to the larger teacher model. Unfortunately, there is a common obstacle\n- the gap in semantic feature structure between the intermediate features of\nteacher and student. The classic scheme prefers to transform intermediate\nfeatures by adding the adaptation module, such as naive convolutional,\nattention-based or more complicated one. However, this introduces two problems:\na) The adaptation module brings more parameters into training. b) The\nadaptation module with random initialization or special transformation isn't\nfriendly for distilling a pre-trained student. In this paper, we present\nMatching Guided Distillation (MGD) as an efficient and parameter-free manner to\nsolve these problems. The key idea of MGD is to pose matching the teacher\nchannels with students' as an assignment problem. We compare three solutions of\nthe assignment problem to reduce channels from teacher features with partial\ndistillation loss. The overall training takes a coordinate-descent approach\nbetween two optimization objects - assignments update and parameters update.\nSince MGD only contains normalization or pooling operations with negligible\ncomputation cost, it is flexible to plug into network with other distillation\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 04:57:31 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 02:58:25 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Yue", "Kaiyu", ""], ["Deng", "Jiangfan", ""], ["Zhou", "Feng", ""]]}, {"id": "2008.09965", "submitter": "Zirui Wang", "authors": "Zirui Wang, Victor Adrian Prisacariu", "title": "Neighbourhood-Insensitive Point Cloud Normal Estimation Network", "comments": "Accepted in BMVC 2020 as oral presentation. Code available at\n  https://code.active.vision and project page at http://ninormal.active.vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel self-attention-based normal estimation network that is\nable to focus softly on relevant points and adjust the softness by learning a\ntemperature parameter, making it able to work naturally and effectively within\na large neighbourhood range. As a result, our model outperforms all existing\nnormal estimation algorithms by a large margin, achieving 94.1% accuracy in\ncomparison with the previous state of the art of 91.2%, with a 25x smaller\nmodel and 12x faster inference time. We also use point-to-plane Iterative\nClosest Point (ICP) as an application case to show that our normal estimations\nlead to faster convergence than normal estimations from other methods, without\nmanually fine-tuning neighbourhood range parameters. Code available at\nhttps://code.active.vision.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 05:46:58 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 02:36:33 GMT"}, {"version": "v3", "created": "Fri, 15 Jan 2021 11:01:58 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Wang", "Zirui", ""], ["Prisacariu", "Victor Adrian", ""]]}, {"id": "2008.09993", "submitter": "Zhida Huang", "authors": "Zhida Huang, Kaiyu Yue, Jiangfan Deng, Feng Zhou", "title": "Visible Feature Guidance for Crowd Pedestrian Detection", "comments": "Technical report; To appear at ECCV 2020 RLQ Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heavy occlusion and dense gathering in crowd scene make pedestrian detection\nbecome a challenging problem, because it's difficult to guess a precise full\nbounding box according to the invisible human part. To crack this nut, we\npropose a mechanism called Visible Feature Guidance (VFG) for both training and\ninference. During training, we adopt visible feature to regress the\nsimultaneous outputs of visible bounding box and full bounding box. Then we\nperform NMS only on visible bounding boxes to achieve the best fitting full box\nin inference. This manner can alleviate the incapable influence brought by NMS\nin crowd scene and make full bounding box more precisely. Furthermore, in order\nto ease feature association in the post application process, such as pedestrian\ntracking, we apply Hungarian algorithm to associate parts for a human instance.\nOur proposed method can stably bring about 2~3% improvements in mAP and AP50\nfor both two-stage and one-stage detector. It's also more effective for MR-2\nespecially with the stricter IoU. Experiments on Crowdhuman, Cityperson,\nCaltech and KITTI datasets show that visible feature guidance can help detector\nachieve promisingly better performances. Moreover, parts association produces a\nstrong benchmark on Crowdhuman for the vision community.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 08:52:52 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 11:23:38 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Huang", "Zhida", ""], ["Yue", "Kaiyu", ""], ["Deng", "Jiangfan", ""], ["Zhou", "Feng", ""]]}, {"id": "2008.09994", "submitter": "You-Wei Luo", "authors": "Chuan-Xian Ren, You-Wei Luo, Xiao-Lin Xu, Dao-Qing Dai and Hong Yan", "title": "Discriminative Residual Analysis for Image Set Classification with\n  Posture and Age Variations", "comments": null, "journal-ref": "IEEE Transactions on Image Processing, vol. 29, pp. 2875-2888,\n  2020", "doi": "10.1109/TIP.2019.2954176", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image set recognition has been widely applied in many practical problems like\nreal-time video retrieval and image caption tasks. Due to its superior\nperformance, it has grown into a significant topic in recent years. However,\nimages with complicated variations, e.g., postures and human ages, are\ndifficult to address, as these variations are continuous and gradual with\nrespect to image appearance. Consequently, the crucial point of image set\nrecognition is to mine the intrinsic connection or structural information from\nthe image batches with variations. In this work, a Discriminant Residual\nAnalysis (DRA) method is proposed to improve the classification performance by\ndiscovering discriminant features in related and unrelated groups.\nSpecifically, DRA attempts to obtain a powerful projection which casts the\nresidual representations into a discriminant subspace. Such a projection\nsubspace is expected to magnify the useful information of the input space as\nmuch as possible, then the relation between the training set and the test set\ndescribed by the given metric or distance will be more precise in the\ndiscriminant subspace. We also propose a nonfeasance strategy by defining\nanother approach to construct the unrelated groups, which help to reduce\nfurthermore the cost of sampling errors. Two regularization approaches are used\nto deal with the probable small sample size problem. Extensive experiments are\nconducted on benchmark databases, and the results show superiority and\nefficiency of the new methods.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 08:53:06 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Ren", "Chuan-Xian", ""], ["Luo", "You-Wei", ""], ["Xu", "Xiao-Lin", ""], ["Dai", "Dao-Qing", ""], ["Yan", "Hong", ""]]}, {"id": "2008.10010", "submitter": "Prajwal K R", "authors": "K R Prajwal, Rudrabha Mukhopadhyay, Vinay Namboodiri, C V Jawahar", "title": "A Lip Sync Expert Is All You Need for Speech to Lip Generation In The\n  Wild", "comments": "9 pages (including references), 3 figures, Accepted in ACM\n  Multimedia, 2020", "journal-ref": null, "doi": "10.1145/3394171.3413532", "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate the problem of lip-syncing a talking face video\nof an arbitrary identity to match a target speech segment. Current works excel\nat producing accurate lip movements on a static image or videos of specific\npeople seen during the training phase. However, they fail to accurately morph\nthe lip movements of arbitrary identities in dynamic, unconstrained talking\nface videos, resulting in significant parts of the video being out-of-sync with\nthe new audio. We identify key reasons pertaining to this and hence resolve\nthem by learning from a powerful lip-sync discriminator. Next, we propose new,\nrigorous evaluation benchmarks and metrics to accurately measure lip\nsynchronization in unconstrained videos. Extensive quantitative evaluations on\nour challenging benchmarks show that the lip-sync accuracy of the videos\ngenerated by our Wav2Lip model is almost as good as real synced videos. We\nprovide a demo video clearly showing the substantial impact of our Wav2Lip\nmodel and evaluation benchmarks on our website:\n\\url{cvit.iiit.ac.in/research/projects/cvit-projects/a-lip-sync-expert-is-all-you-need-for-speech-to-lip-generation-in-the-wild}.\nThe code and models are released at this GitHub repository:\n\\url{github.com/Rudrabha/Wav2Lip}. You can also try out the interactive demo at\nthis link: \\url{bhaasha.iiit.ac.in/lipsync}.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 11:01:25 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Prajwal", "K R", ""], ["Mukhopadhyay", "Rudrabha", ""], ["Namboodiri", "Vinay", ""], ["Jawahar", "C V", ""]]}, {"id": "2008.10030", "submitter": "You-Wei Luo", "authors": "You-Wei Luo, Chuan-Xian Ren, Dao-Qing Dai and Hong Yan", "title": "Unsupervised Domain Adaptation via Discriminative Manifold Propagation", "comments": "To be published in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": "10.1109/TPAMI.2020.3014218", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation is effective in leveraging rich information\nfrom a labeled source domain to an unlabeled target domain. Though deep\nlearning and adversarial strategy made a significant breakthrough in the\nadaptability of features, there are two issues to be further studied. First,\nhard-assigned pseudo labels on the target domain are arbitrary and error-prone,\nand direct application of them may destroy the intrinsic data structure.\nSecond, batch-wise training of deep learning limits the characterization of the\nglobal structure. In this paper, a Riemannian manifold learning framework is\nproposed to achieve transferability and discriminability simultaneously. For\nthe first issue, this framework establishes a probabilistic discriminant\ncriterion on the target domain via soft labels. Based on pre-built prototypes,\nthis criterion is extended to a global approximation scheme for the second\nissue. Manifold metric alignment is adopted to be compatible with the embedding\nspace. The theoretical error bounds of different alignment metrics are derived\nfor constructive guidance. The proposed method can be used to tackle a series\nof variants of domain adaptation problems, including both vanilla and partial\nsettings. Extensive experiments have been conducted to investigate the method\nand a comparative study shows the superiority of the discriminative manifold\nlearning framework.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 12:31:37 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Luo", "You-Wei", ""], ["Ren", "Chuan-Xian", ""], ["Dai", "Dao-Qing", ""], ["Yan", "Hong", ""]]}, {"id": "2008.10032", "submitter": "Jiaqi Wang", "authors": "Jiaqi Wang, Wenwei Zhang, Yuhang Zang, Yuhang Cao, Jiangmiao Pang, Tao\n  Gong, Kai Chen, Ziwei Liu, Chen Change Loy, Dahua Lin", "title": "Seesaw Loss for Long-Tailed Instance Segmentation", "comments": "CVPR 2021 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance segmentation has witnessed a remarkable progress on class-balanced\nbenchmarks. However, they fail to perform as accurately in real-world\nscenarios, where the category distribution of objects naturally comes with a\nlong tail. Instances of head classes dominate a long-tailed dataset and they\nserve as negative samples of tail categories. The overwhelming gradients of\nnegative samples on tail classes lead to a biased learning process for\nclassifiers. Consequently, objects of tail categories are more likely to be\nmisclassified as backgrounds or head categories. To tackle this problem, we\npropose Seesaw Loss to dynamically re-balance gradients of positive and\nnegative samples for each category, with two complementary factors, i.e.,\nmitigation factor and compensation factor. The mitigation factor reduces\npunishments to tail categories w.r.t. the ratio of cumulative training\ninstances between different categories. Meanwhile, the compensation factor\nincreases the penalty of misclassified instances to avoid false positives of\ntail categories. We conduct extensive experiments on Seesaw Loss with\nmainstream frameworks and different data sampling strategies. With a simple\nend-to-end training pipeline, Seesaw Loss obtains significant gains over\nCross-Entropy Loss, and achieves state-of-the-art performance on LVIS dataset\nwithout bells and whistles. Code is available at\nhttps://github.com/open-mmlab/mmdetection.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 12:44:45 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 12:38:15 GMT"}, {"version": "v3", "created": "Thu, 24 Dec 2020 13:16:00 GMT"}, {"version": "v4", "created": "Thu, 17 Jun 2021 15:13:10 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Wang", "Jiaqi", ""], ["Zhang", "Wenwei", ""], ["Zang", "Yuhang", ""], ["Cao", "Yuhang", ""], ["Pang", "Jiangmiao", ""], ["Gong", "Tao", ""], ["Chen", "Kai", ""], ["Liu", "Ziwei", ""], ["Loy", "Chen Change", ""], ["Lin", "Dahua", ""]]}, {"id": "2008.10038", "submitter": "Pengfei Ge", "authors": "Pengfei Ge, Chuan-Xian Ren, Jiashi Feng, Shuicheng Yan", "title": "Dual Adversarial Auto-Encoders for Clustering", "comments": null, "journal-ref": null, "doi": "10.1109/TNNLS.2019.2919948", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a powerful approach for exploratory data analysis, unsupervised clustering\nis a fundamental task in computer vision and pattern recognition. Many\nclustering algorithms have been developed, but most of them perform\nunsatisfactorily on the data with complex structures. Recently, Adversarial\nAuto-Encoder (AAE) shows effectiveness on tackling such data by combining\nAuto-Encoder (AE) and adversarial training, but it cannot effectively extract\nclassification information from the unlabeled data. In this work, we propose\nDual Adversarial Auto-encoder (Dual-AAE) which simultaneously maximizes the\nlikelihood function and mutual information between observed examples and a\nsubset of latent variables. By performing variational inference on the\nobjective function of Dual-AAE, we derive a new reconstruction loss which can\nbe optimized by training a pair of Auto-encoders. Moreover, to avoid mode\ncollapse, we introduce the clustering regularization term for the category\nvariable. Experiments on four benchmarks show that Dual-AAE achieves superior\nperformance over state-of-the-art clustering methods. Besides, by adding a\nreject option, the clustering accuracy of Dual-AAE can reach that of supervised\nCNN algorithms. Dual-AAE can also be used for disentangling style and content\nof images without using supervised information.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 13:16:34 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Ge", "Pengfei", ""], ["Ren", "Chuan-Xian", ""], ["Feng", "Jiashi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "2008.10041", "submitter": "Krzysztof Maziarz", "authors": "Zbigniew Wojna, Krzysztof Maziarz, {\\L}ukasz Jocz, Robert Pa{\\l}uba,\n  Robert Kozikowski, Iasonas Kokkinos", "title": "Holistic Multi-View Building Analysis in the Wild with Projection\n  Pooling", "comments": "Accepted for publication at the 35th AAAI Conference on Artificial\n  Intelligence (AAAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address six different classification tasks related to fine-grained\nbuilding attributes: construction type, number of floors, pitch and geometry of\nthe roof, facade material, and occupancy class. Tackling such a remote building\nanalysis problem became possible only recently due to growing large-scale\ndatasets of urban scenes. To this end, we introduce a new benchmarking dataset,\nconsisting of 49426 images (top-view and street-view) of 9674 buildings. These\nphotos are further assembled, together with the geometric metadata. The dataset\nshowcases various real-world challenges, such as occlusions, blur, partially\nvisible objects, and a broad spectrum of buildings. We propose a new projection\npooling layer, creating a unified, top-view representation of the top-view and\nthe side views in a high-dimensional space. It allows us to utilize the\nbuilding and imagery metadata seamlessly. Introducing this layer improves\nclassification accuracy -- compared to highly tuned baseline models --\nindicating its suitability for building analysis.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 13:49:22 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 23:04:21 GMT"}, {"version": "v3", "created": "Sat, 19 Dec 2020 20:59:07 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Wojna", "Zbigniew", ""], ["Maziarz", "Krzysztof", ""], ["Jocz", "\u0141ukasz", ""], ["Pa\u0142uba", "Robert", ""], ["Kozikowski", "Robert", ""], ["Kokkinos", "Iasonas", ""]]}, {"id": "2008.10060", "submitter": "Haoyi Zhu", "authors": "Haoyi Zhu, Cheng Jie, Shaofei Jiang", "title": "Multi-Person Full Body Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-person pose estimation plays an important role in many fields. Although\nprevious works have researched a lot on different parts of human pose\nestimation, full body pose estimation for multi-person still needs further\nresearch. Our work has developed an integrated model through knowledge\ndistillation which can estimate full body poses. Trained based on the AlphaPose\nsystem and MSCOCO2017 dataset, our model achieves 51.5 mAP on the validation\ndataset annotated manually by ourselves. Related resources are available at\nhttps://esflfei.github.io/esflfei.gethub.io/website.html.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 15:47:13 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Zhu", "Haoyi", ""], ["Jie", "Cheng", ""], ["Jiang", "Shaofei", ""]]}, {"id": "2008.10078", "submitter": "Chayan Sarkar", "authors": "Hrishav Bakul Barua, Pradip Pramanick, Chayan Sarkar, Theint Haythi Mg", "title": "Let me join you! Real-time F-formation recognition by a socially aware\n  robot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel architecture to detect social groups in real-time\nfrom a continuous image stream of an ego-vision camera. F-formation defines\nsocial orientations in space where two or more person tends to communicate in a\nsocial place. Thus, essentially, we detect F-formations in social gatherings\nsuch as meetings, discussions, etc. and predict the robot's approach angle if\nit wants to join the social group. Additionally, we also detect outliers, i.e.,\nthe persons who are not part of the group under consideration. Our proposed\npipeline consists of -- a) a skeletal key points estimator (a total of 17) for\nthe detected human in the scene, b) a learning model (using a feature vector\nbased on the skeletal points) using CRF to detect groups of people and outlier\nperson in a scene, and c) a separate learning model using a multi-class Support\nVector Machine (SVM) to predict the exact F-formation of the group of people in\nthe current scene and the angle of approach for the viewing robot. The system\nis evaluated using two data-sets. The results show that the group and outlier\ndetection in a scene using our method establishes an accuracy of 91%. We have\nmade rigorous comparisons of our systems with a state-of-the-art F-formation\ndetection system and found that it outperforms the state-of-the-art by 29% for\nformation detection and 55% for combined detection of the formation and\napproach angle.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 17:46:08 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Barua", "Hrishav Bakul", ""], ["Pramanick", "Pradip", ""], ["Sarkar", "Chayan", ""], ["Mg", "Theint Haythi", ""]]}, {"id": "2008.10106", "submitter": "Ian McDiarmid-Sterling", "authors": "Ian McDiarmid-Sterling and Allan Moser", "title": "Developing and Defeating Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breakthroughs in machine learning have resulted in state-of-the-art deep\nneural networks (DNNs) performing classification tasks in safety-critical\napplications. Recent research has demonstrated that DNNs can be attacked\nthrough adversarial examples, which are small perturbations to input data that\ncause the DNN to misclassify objects. The proliferation of DNNs raises\nimportant safety concerns about designing systems that are robust to\nadversarial examples. In this work we develop adversarial examples to attack\nthe Yolo V3 object detector [1] and then study strategies to detect and\nneutralize these examples. Python code for this project is available at\nhttps://github.com/ianmcdiarmidsterling/adversarial\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 21:00:33 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["McDiarmid-Sterling", "Ian", ""], ["Moser", "Allan", ""]]}, {"id": "2008.10112", "submitter": "Abhinav Valada", "authors": "Rohit Mohan and Abhinav Valada", "title": "Robust Vision Challenge 2020 -- 1st Place Report for Panoptic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report, we present key details of our winning panoptic\nsegmentation architecture EffPS_b1bs4_RVC. Our network is a lightweight version\nof our state-of-the-art EfficientPS architecture that consists of our proposed\nshared backbone with a modified EfficientNet-B5 model as the encoder, followed\nby the 2-way FPN to learn semantically rich multi-scale features. It consists\nof two task-specific heads, a modified Mask R-CNN instance head and our novel\nsemantic segmentation head that processes features of different scales with\nspecialized modules for coherent feature refinement. Finally, our proposed\npanoptic fusion module adaptively fuses logits from each of the heads to yield\nthe panoptic segmentation output. The Robust Vision Challenge 2020 benchmarking\nresults show that our model is ranked #1 on Microsoft COCO, VIPER and WildDash,\nand is ranked #2 on Cityscapes and Mapillary Vistas, thereby achieving the\noverall rank #1 for the panoptic segmentation task.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 21:41:43 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Mohan", "Rohit", ""], ["Valada", "Abhinav", ""]]}, {"id": "2008.10123", "submitter": "Yipu Zhao", "authors": "Yipu Zhao, Justin S. Smith, Patricio A. Vela", "title": "Good Graph to Optimize: Cost-Effective, Budget-Aware Bundle Adjustment\n  in Visual SLAM", "comments": "20 pages, 14 figures, 8 tables. Submitted to IEEE Transactions on\n  Robotics, for the provided open-source software see\n  https://github.com/ivalab/gf_orb_slam2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cost-efficiency of visual(-inertial) SLAM (VSLAM) is a critical\ncharacteristic of resource-limited applications. While hardware and algorithm\nadvances have been significantly improved the cost-efficiency of VSLAM\nfront-ends, the cost-efficiency of VSLAM back-ends remains a bottleneck. This\npaper describes a novel, rigorous method to improve the cost-efficiency of\nlocal BA in a BA-based VSLAM back-end. An efficient algorithm, called Good\nGraph, is developed to select size-reduced graphs optimized in local BA with\ncondition preservation. To better suit BA-based VSLAM back-ends, the Good Graph\npredicts future estimation needs, dynamically assigns an appropriate size\nbudget, and selects a condition-maximized subgraph for BA estimation.\nEvaluations are conducted on two scenarios: 1) VSLAM as standalone process, and\n2) VSLAM as part of closed-loop navigation system. Results from the first\nscenario show Good Graph improves accuracy and robustness of VSLAM estimation,\nwhen computational limits exist. Results from the second scenario, indicate\nthat Good Graph benefits the trajectory tracking performance of VSLAM-based\nclosed-loop navigation systems, which is a primary application of VSLAM.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 22:41:35 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Zhao", "Yipu", ""], ["Smith", "Justin S.", ""], ["Vela", "Patricio A.", ""]]}, {"id": "2008.10134", "submitter": "Salman Maqbool", "authors": "Salman Maqbool, Aqsa Riaz, Hasan Sajid, Osman Hasan", "title": "m2caiSeg: Semantic Segmentation of Laparoscopic Images using\n  Convolutional Neural Networks", "comments": "16 pages, 5 figures, Code available at:\n  https://github.com/salmanmaq/segmentationNetworks, Dataset available at:\n  https://www.kaggle.com/salmanmaq/m2caiseg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Autonomous surgical procedures, in particular minimal invasive surgeries, are\nthe next frontier for Artificial Intelligence research. However, the existing\nchallenges include precise identification of the human anatomy and the surgical\nsettings, and modeling the environment for training of an autonomous agent. To\naddress the identification of human anatomy and the surgical settings, we\npropose a deep learning based semantic segmentation algorithm to identify and\nlabel the tissues and organs in the endoscopic video feed of the human torso\nregion. We present an annotated dataset, m2caiSeg, created from endoscopic\nvideo feeds of real-world surgical procedures. Overall, the data consists of\n307 images, each of which is annotated for the organs and different surgical\ninstruments present in the scene. We propose and train a deep convolutional\nneural network for the semantic segmentation task. To cater for the low\nquantity of annotated data, we use unsupervised pre-training and data\naugmentation. The trained model is evaluated on an independent test set of the\nproposed dataset. We obtained a F1 score of 0.33 while using all the labeled\ncategories for the semantic segmentation task. Secondly, we labeled all\ninstruments into an 'Instruments' superclass to evaluate the model's\nperformance on discerning the various organs and obtained a F1 score of 0.57.\nWe propose a new dataset and a deep learning method for pixel level\nidentification of various organs and instruments in a endoscopic surgical\nscene. Surgical scene understanding is one of the first steps towards\nautomating surgical procedures.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 23:30:15 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 21:34:59 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Maqbool", "Salman", ""], ["Riaz", "Aqsa", ""], ["Sajid", "Hasan", ""], ["Hasan", "Osman", ""]]}, {"id": "2008.10162", "submitter": "Jingwei Xu", "authors": "Jingwei Xu, Huazhe Xu, Bingbing Ni, Xiaokang Yang, Xiaolong Wang,\n  Trevor Darrell", "title": "Hierarchical Style-based Networks for Motion Synthesis", "comments": "ECCV 2020, Project Page:\\<https://sites.google.com/view/hsnms>", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating diverse and natural human motion is one of the long-standing goals\nfor creating intelligent characters in the animated world. In this paper, we\npropose a self-supervised method for generating long-range, diverse and\nplausible behaviors to achieve a specific goal location. Our proposed method\nlearns to model the motion of human by decomposing a long-range generation task\nin a hierarchical manner. Given the starting and ending states, a memory bank\nis used to retrieve motion references as source material for short-range clip\ngeneration. We first propose to explicitly disentangle the provided motion\nmaterial into style and content counterparts via bi-linear transformation\nmodelling, where diverse synthesis is achieved by free-form combination of\nthese two components. The short-range clips are then connected to form a\nlong-range motion sequence. Without ground truth annotation, we propose a\nparameterized bi-directional interpolation scheme to guarantee the physical\nvalidity and visual naturalness of generated results. On large-scale skeleton\ndataset, we show that the proposed method is able to synthesise long-range,\ndiverse and plausible motion, which is also generalizable to unseen motion data\nduring testing. Moreover, we demonstrate the generated sequences are useful as\nsubgoals for actual physical execution in the animated world.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 02:11:02 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Xu", "Jingwei", ""], ["Xu", "Huazhe", ""], ["Ni", "Bingbing", ""], ["Yang", "Xiaokang", ""], ["Wang", "Xiaolong", ""], ["Darrell", "Trevor", ""]]}, {"id": "2008.10165", "submitter": "Pengfei Ge", "authors": "Chuan-Xian Ren, Pengfei Ge, Dao-Qing Dai, Hong Yan", "title": "Learning Kernel for Conditional Moment-Matching Discrepancy-based Image\n  Classification", "comments": null, "journal-ref": null, "doi": "10.1109/TCYB.2019.2916198", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional Maximum Mean Discrepancy (CMMD) can capture the discrepancy\nbetween conditional distributions by drawing support from nonlinear kernel\nfunctions, thus it has been successfully used for pattern classification.\nHowever, CMMD does not work well on complex distributions, especially when the\nkernel function fails to correctly characterize the difference between\nintra-class similarity and inter-class similarity. In this paper, a new kernel\nlearning method is proposed to improve the discrimination performance of CMMD.\nIt can be operated with deep network features iteratively and thus denoted as\nKLN for abbreviation. The CMMD loss and an auto-encoder (AE) are used to learn\nan injective function. By considering the compound kernel, i.e., the injective\nfunction with a characteristic kernel, the effectiveness of CMMD for data\ncategory description is enhanced. KLN can simultaneously learn a more\nexpressive kernel and label prediction distribution, thus, it can be used to\nimprove the classification performance in both supervised and semi-supervised\nlearning scenarios. In particular, the kernel-based similarities are\niteratively learned on the deep network features, and the algorithm can be\nimplemented in an end-to-end manner. Extensive experiments are conducted on\nfour benchmark datasets, including MNIST, SVHN, CIFAR-10 and CIFAR-100. The\nresults indicate that KLN achieves state-of-the-art classification performance.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 02:35:50 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Ren", "Chuan-Xian", ""], ["Ge", "Pengfei", ""], ["Dai", "Dao-Qing", ""], ["Yan", "Hong", ""]]}, {"id": "2008.10174", "submitter": "Egor Zakharov", "authors": "Egor Zakharov, Aleksei Ivakhnenko, Aliaksandra Shysheya, Victor\n  Lempitsky", "title": "Fast Bi-layer Neural Synthesis of One-Shot Realistic Head Avatars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural rendering-based system that creates head avatars from a\nsingle photograph. Our approach models a person's appearance by decomposing it\ninto two layers. The first layer is a pose-dependent coarse image that is\nsynthesized by a small neural network. The second layer is defined by a\npose-independent texture image that contains high-frequency details. The\ntexture image is generated offline, warped and added to the coarse image to\nensure a high effective resolution of synthesized head views. We compare our\nsystem to analogous state-of-the-art systems in terms of visual quality and\nspeed. The experiments show significant inference speedup over previous neural\nhead avatar models for a given visual quality. We also report on a real-time\nsmartphone-based implementation of our system.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 03:23:59 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Zakharov", "Egor", ""], ["Ivakhnenko", "Aleksei", ""], ["Shysheya", "Aliaksandra", ""], ["Lempitsky", "Victor", ""]]}, {"id": "2008.10183", "submitter": "Skyler Seto", "authors": "Skyler Seto, Martin T. Wells, Wenyu Zhang", "title": "HALO: Learning to Prune Neural Networks with Shrinkage", "comments": "Accepted at SDM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks achieve state-of-the-art performance in a variety of\ntasks by extracting a rich set of features from unstructured data, however this\nperformance is closely tied to model size. Modern techniques for inducing\nsparsity and reducing model size are (1) network pruning, (2) training with a\nsparsity inducing penalty, and (3) training a binary mask jointly with the\nweights of the network. We study different sparsity inducing penalties from the\nperspective of Bayesian hierarchical models and present a novel penalty called\nHierarchical Adaptive Lasso (HALO) which learns to adaptively sparsify weights\nof a given network via trainable parameters. When used to train\nover-parametrized networks, our penalty yields small subnetworks with high\naccuracy without fine-tuning. Empirically, on image recognition tasks, we find\nthat HALO is able to learn highly sparse network (only 5% of the parameters)\nwith significant gains in performance over state-of-the-art magnitude pruning\nmethods at the same level of sparsity. Code is available at\nhttps://github.com/skyler120/sparsity-halo.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 04:08:48 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 01:47:29 GMT"}, {"version": "v3", "created": "Sun, 28 Feb 2021 04:26:09 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Seto", "Skyler", ""], ["Wells", "Martin T.", ""], ["Zhang", "Wenyu", ""]]}, {"id": "2008.10191", "submitter": "Pengfei Xiong", "authors": "Xinyan Zhang, Yunfeng Wang, Pengfei Xiong", "title": "Affinity-aware Compression and Expansion Network for Human Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a fine-grained segmentation task, human parsing is still faced with two\nchallenges: inter-part indistinction and intra-part inconsistency, due to the\nambiguous definitions and confusing relationships between similar human parts.\nTo tackle these two problems, this paper proposes a novel\n\\textit{Affinity-aware Compression and Expansion} Network (ACENet), which\nmainly consists of two modules: Local Compression Module (LCM) and Global\nExpansion Module (GEM). Specifically, LCM compresses parts-correlation\ninformation through structural skeleton points, obtained from an extra skeleton\nbranch. It can decrease the inter-part interference, and strengthen structural\nrelationships between ambiguous parts. Furthermore, GEM expands semantic\ninformation of each part into a complete piece by incorporating the spatial\naffinity with boundary guidance, which can effectively enhance the semantic\nconsistency of intra-part as well. ACENet achieves new state-of-the-art\nperformance on the challenging LIP and Pascal-Person-Part datasets. In\nparticular, 58.1% mean IoU is achieved on the LIP benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 05:16:08 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Zhang", "Xinyan", ""], ["Wang", "Yunfeng", ""], ["Xiong", "Pengfei", ""]]}, {"id": "2008.10208", "submitter": "Youwei Liang", "authors": "Youwei Liang, Dong Huang, Chang-Dong Wang, and Philip S. Yu", "title": "Multi-view Graph Learning by Joint Modeling of Consistency and\n  Inconsistency", "comments": "Preprint, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph learning has emerged as a promising technique for multi-view clustering\nwith its ability to learn a unified and robust graph from multiple views.\nHowever, existing graph learning methods mostly focus on the multi-view\nconsistency issue, yet often neglect the inconsistency across multiple views,\nwhich makes them vulnerable to possibly low-quality or noisy datasets. To\novercome this limitation, we propose a new multi-view graph learning framework,\nwhich for the first time simultaneously and explicitly models multi-view\nconsistency and multi-view inconsistency in a unified objective function,\nthrough which the consistent and inconsistent parts of each single-view graph\nas well as the unified graph that fuses the consistent parts can be iteratively\nlearned. Though optimizing the objective function is NP-hard, we design a\nhighly efficient optimization algorithm which is able to obtain an approximate\nsolution with linear time complexity in the number of edges in the unified\ngraph. Furthermore, our multi-view graph learning approach can be applied to\nboth similarity graphs and dissimilarity graphs, which lead to two graph\nfusion-based variants in our framework. Experiments on twelve multi-view\ndatasets have demonstrated the robustness and efficiency of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 06:11:29 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 10:02:51 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Liang", "Youwei", ""], ["Huang", "Dong", ""], ["Wang", "Chang-Dong", ""], ["Yu", "Philip S.", ""]]}, {"id": "2008.10236", "submitter": "Akansel Cosgun", "authors": "Sunny Goondram, Akansel Cosgun and Dana Kulic", "title": "Strawberry Detection using Mixed Training on Simulated and Real Data", "comments": "DICTA 2020 Short Paper Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper demonstrates how simulated images can be useful for object\ndetection tasks in the agricultural sector, where labeled data can be scarce\nand costly to collect. We consider training on mixed datasets with real and\nsimulated data for strawberry detection in real images. Our results show that\nusing the real dataset augmented by the simulated dataset resulted in slightly\nhigher accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 07:37:12 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Goondram", "Sunny", ""], ["Cosgun", "Akansel", ""], ["Kulic", "Dana", ""]]}, {"id": "2008.10238", "submitter": "Sunjae Yoon", "authors": "Minuk Ma, Sunjae Yoon, Junyeong Kim, Youngjoon Lee, Sunghun Kang, and\n  Chang D. Yoo", "title": "VLANet: Video-Language Alignment Network for Weakly-Supervised Video\n  Moment Retrieval", "comments": "16 pages, 6 figures, European Conference on Computer Vision, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video Moment Retrieval (VMR) is a task to localize the temporal moment in\nuntrimmed video specified by natural language query. For VMR, several methods\nthat require full supervision for training have been proposed. Unfortunately,\nacquiring a large number of training videos with labeled temporal boundaries\nfor each query is a labor-intensive process. This paper explores methods for\nperforming VMR in a weakly-supervised manner (wVMR): training is performed\nwithout temporal moment labels but only with the text query that describes a\nsegment of the video. Existing methods on wVMR generate multi-scale proposals\nand apply query-guided attention mechanisms to highlight the most relevant\nproposal. To leverage the weak supervision, contrastive learning is used which\npredicts higher scores for the correct video-query pairs than for the incorrect\npairs. It has been observed that a large number of candidate proposals, coarse\nquery representation, and one-way attention mechanism lead to blurry attention\nmaps which limit the localization performance. To handle this issue,\nVideo-Language Alignment Network (VLANet) is proposed that learns sharper\nattention by pruning out spurious candidate proposals and applying a\nmulti-directional attention mechanism with fine-grained query representation.\nThe Surrogate Proposal Selection module selects a proposal based on the\nproximity to the query in the joint embedding space, and thus substantially\nreduces candidate proposals which leads to lower computation load and sharper\nattention. Next, the Cascaded Cross-modal Attention module considers dense\nfeature interactions and multi-directional attention flow to learn the\nmulti-modal alignment. VLANet is trained end-to-end using contrastive loss\nwhich enforces semantically similar videos and queries to gather. The\nexperiments show that the method achieves state-of-the-art performance on\nCharades-STA and DiDeMo datasets.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 07:54:59 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Ma", "Minuk", ""], ["Yoon", "Sunjae", ""], ["Kim", "Junyeong", ""], ["Lee", "Youngjoon", ""], ["Kang", "Sunghun", ""], ["Yoo", "Chang D.", ""]]}, {"id": "2008.10244", "submitter": "Marc Aubreville", "authors": "Marc Aubreville, Christof A. Bertram, Taryn A. Donovan, Christian\n  Marzahl, Andreas Maier, and Robert Klopfleisch", "title": "A completely annotated whole slide image dataset of canine breast cancer\n  to aid human breast cancer research", "comments": "12 pages, 5 figures", "journal-ref": "Sci Data 7, 417 (2020)", "doi": "10.1038/s41597-020-00756-z", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Canine mammary carcinoma (CMC) has been used as a model to investigate the\npathogenesis of human breast cancer and the same grading scheme is commonly\nused to assess tumor malignancy in both. One key component of this grading\nscheme is the density of mitotic figures (MF). Current publicly available\ndatasets on human breast cancer only provide annotations for small subsets of\nwhole slide images (WSIs). We present a novel dataset of 21 WSIs of CMC\ncompletely annotated for MF. For this, a pathologist screened all WSIs for\npotential MF and structures with a similar appearance. A second expert blindly\nassigned labels, and for non-matching labels, a third expert assigned the final\nlabels. Additionally, we used machine learning to identify previously\nundetected MF. Finally, we performed representation learning and\ntwo-dimensional projection to further increase the consistency of the\nannotations. Our dataset consists of 13,907 MF and 36,379 hard negatives. We\nachieved a mean F1-score of 0.791 on the test set and of up to 0.696 on a human\nbreast cancer dataset.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 08:06:55 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 11:49:32 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Aubreville", "Marc", ""], ["Bertram", "Christof A.", ""], ["Donovan", "Taryn A.", ""], ["Marzahl", "Christian", ""], ["Maier", "Andreas", ""], ["Klopfleisch", "Robert", ""]]}, {"id": "2008.10247", "submitter": "Mallikarjun Byrasandra Ramalinga Reddy", "authors": "Mallikarjun B R. (1), Ayush Tewari (1), Tae-Hyun Oh (2), Tim Weyrich\n  (3), Bernd Bickel (4), Hans-Peter Seidel (1), Hanspeter Pfister (5), Wojciech\n  Matusik (6), Mohamed Elgharib (1), Christian Theobalt (1) ((1) Max Planck\n  Institute for Informatics, Saarland Informatics Campus, (2) POSTECH, (3)\n  University College London, (4) IST Austria, (5) Harvard University, (6) MIT\n  CSAIL)", "title": "Monocular Reconstruction of Neural Face Reflectance Fields", "comments": "Project page -\n  http://gvv.mpi-inf.mpg.de/projects/FaceReflectanceFields/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reflectance field of a face describes the reflectance properties\nresponsible for complex lighting effects including diffuse, specular,\ninter-reflection and self shadowing. Most existing methods for estimating the\nface reflectance from a monocular image assume faces to be diffuse with very\nfew approaches adding a specular component. This still leaves out important\nperceptual aspects of reflectance as higher-order global illumination effects\nand self-shadowing are not modeled. We present a new neural representation for\nface reflectance where we can estimate all components of the reflectance\nresponsible for the final appearance from a single monocular image. Instead of\nmodeling each component of the reflectance separately using parametric models,\nour neural representation allows us to generate a basis set of faces in a\ngeometric deformation-invariant space, parameterized by the input light\ndirection, viewpoint and face geometry. We learn to reconstruct this\nreflectance field of a face just from a monocular image, which can be used to\nrender the face from any viewpoint in any light condition. Our method is\ntrained on a light-stage training dataset, which captures 300 people\nilluminated with 150 light conditions from 8 viewpoints. We show that our\nmethod outperforms existing monocular reflectance reconstruction methods, in\nterms of photorealism due to better capturing of physical premitives, such as\nsub-surface scattering, specularities, self-shadows and other higher-order\neffects.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 08:19:05 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["R.", "Mallikarjun B", ""], ["Tewari", "Ayush", ""], ["Oh", "Tae-Hyun", ""], ["Weyrich", "Tim", ""], ["Bickel", "Bernd", ""], ["Seidel", "Hans-Peter", ""], ["Pfister", "Hanspeter", ""], ["Matusik", "Wojciech", ""], ["Elgharib", "Mohamed", ""], ["Theobalt", "Christian", ""]]}, {"id": "2008.10254", "submitter": "Micha{\\l} Romaszewski", "authors": "Micha{\\l} Romaszewski, Przemys{\\l}aw G{\\l}omb, Arkadiusz Sochan and\n  Micha{\\l} Cholewa", "title": "A Dataset for Evaluating Blood Detection in Hyperspectral Images", "comments": null, "journal-ref": null, "doi": "10.1016/j.forsciint.2021.110701", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sensitivity of imaging spectroscopy to haemoglobin derivatives makes it a\npromising tool for detecting blood. However, due to complexity and high\ndimensionality of hyperspectral images, the development of hyperspectral blood\ndetection algorithms is challenging. To facilitate their development, we\npresent a new hyperspectral blood detection dataset. This dataset, published in\naccordance to open access mandate, consist of multiple detection scenarios with\nvarying levels of complexity. It allows to test the performance of Machine\nLearning methods in relation to different acquisition environments, types of\nbackground, age of blood and presence of other blood-like substances. We\nexplored the dataset with blood detection experiments. We used hyperspectral\ntarget detection algorithm based on the well-known Matched Filter detector. Our\nresults and their discussion highlight the challenges of blood detection in\nhyperspectral data and form a reference for further works.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 08:38:00 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 12:02:45 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Romaszewski", "Micha\u0142", ""], ["G\u0142omb", "Przemys\u0142aw", ""], ["Sochan", "Arkadiusz", ""], ["Cholewa", "Micha\u0142", ""]]}, {"id": "2008.10268", "submitter": "Aniket Joshi", "authors": "Aniket Joshi, Gaurav Mishra, Jayanthi Sivaswamy", "title": "Explainable Disease Classification via weakly-supervised segmentation", "comments": null, "journal-ref": "Interpretable and Annotation-Efficient Learning for Medical Image\n  Computing. IMIMIC 2020, MIL3ID 2020, LABELS 2020. Lecture Notes in Computer\n  Science, vol 12446. Springer, Cham", "doi": "10.1007/978-3-030-61166-8_6", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based approaches to Computer Aided Diagnosis (CAD) typically\npose the problem as an image classification (Normal or Abnormal) problem. These\nsystems achieve high to very high accuracy in specific disease detection for\nwhich they are trained but lack in terms of an explanation for the provided\ndecision/classification result. The activation maps which correspond to\ndecisions do not correlate well with regions of interest for specific diseases.\nThis paper examines this problem and proposes an approach which mimics the\nclinical practice of looking for an evidence prior to diagnosis. A CAD model is\nlearnt using a mixed set of information: class labels for the entire training\nset of images plus a rough localisation of suspect regions as an extra input\nfor a smaller subset of training images for guiding the learning. The proposed\napproach is illustrated with detection of diabetic macular edema (DME) from OCT\nslices. Results of testing on on a large public dataset show that with just a\nthird of images with roughly segmented fluid filled regions, the classification\naccuracy is on par with state of the art methods while providing a good\nexplanation in the form of anatomically accurate heatmap /region of interest.\nThe proposed solution is then adapted to Breast Cancer detection from\nmammographic images. Good evaluation results on public datasets underscores the\ngeneralisability of the proposed solution.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 09:00:30 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Joshi", "Aniket", ""], ["Mishra", "Gaurav", ""], ["Sivaswamy", "Jayanthi", ""]]}, {"id": "2008.10271", "submitter": "Bharath Comandur", "authors": "Bharath Comandur and Avinash C. Kak", "title": "Semantic Labeling of Large-Area Geographic Regions Using Multi-View and\n  Multi-Date Satellite Images and Noisy OSM Training Labels", "comments": "This work has been accepted by the IEEE for publication. Copyright\n  may be transferred without notice, after which this version may no longer be\n  accessible", "journal-ref": null, "doi": "10.1109/JSTARS.2021.3066944", "report-no": null, "categories": "cs.CV cs.DC cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel multi-view training framework and CNN architecture for\ncombining information from multiple overlapping satellite images and noisy\ntraining labels derived from OpenStreetMap (OSM) to semantically label\nbuildings and roads across large geographic regions (100 km$^2$). Our approach\nto multi-view semantic segmentation yields a 4-7% improvement in the per-class\nIoU scores compared to the traditional approaches that use the views\nindependently of one another. A unique (and, perhaps, surprising) property of\nour system is that modifications that are added to the tail-end of the CNN for\nlearning from the multi-view data can be discarded at the time of inference\nwith a relatively small penalty in the overall performance. This implies that\nthe benefits of training using multiple views are absorbed by all the layers of\nthe network. Additionally, our approach only adds a small overhead in terms of\nthe GPU-memory consumption even when training with as many as 32 views per\nscene. The system we present is end-to-end automated, which facilitates\ncomparing the classifiers trained directly on true orthophotos vis-a-vis first\ntraining them on the off-nadir images and subsequently translating the\npredicted labels to geographical coordinates. With no human supervision, our\nIoU scores for the buildings and roads classes are 0.8 and 0.64 respectively\nwhich are better than state-of-the-art approaches that use OSM labels and that\nare not completely automated.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 09:03:31 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 14:17:24 GMT"}, {"version": "v3", "created": "Sat, 28 Nov 2020 15:43:32 GMT"}, {"version": "v4", "created": "Thu, 1 Apr 2021 07:10:30 GMT"}, {"version": "v5", "created": "Sun, 27 Jun 2021 02:50:21 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Comandur", "Bharath", ""], ["Kak", "Avinash C.", ""]]}, {"id": "2008.10292", "submitter": "David Bruggemann", "authors": "David Bruggemann, Menelaos Kanakis, Stamatios Georgoulis, Luc Van Gool", "title": "Automated Search for Resource-Efficient Branched Multi-Task Networks", "comments": "British Machine Vision Conference (BMVC) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The multi-modal nature of many vision problems calls for neural network\narchitectures that can perform multiple tasks concurrently. Typically, such\narchitectures have been handcrafted in the literature. However, given the size\nand complexity of the problem, this manual architecture exploration likely\nexceeds human design abilities. In this paper, we propose a principled\napproach, rooted in differentiable neural architecture search, to automatically\ndefine branching (tree-like) structures in the encoding stage of a multi-task\nneural network. To allow flexibility within resource-constrained environments,\nwe introduce a proxyless, resource-aware loss that dynamically controls the\nmodel size. Evaluations across a variety of dense prediction tasks show that\nour approach consistently finds high-performing branching structures within\nlimited resource budgets.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 09:49:19 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 17:58:18 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Bruggemann", "David", ""], ["Kanakis", "Menelaos", ""], ["Georgoulis", "Stamatios", ""], ["Van Gool", "Luc", ""]]}, {"id": "2008.10293", "submitter": "Dimitrios Bariamis", "authors": "Armin Runge (1) and Thomas Wenzel (2) and Dimitrios Bariamis (2) and\n  Benedikt Sebastian Staffler (3) and Lucas Rego Drumond (2) and Michael\n  Pfeiffer (3) ((1) Department of Advanced Digital Technologies, Bosch\n  Corporate Research, Renningen, Germany, (2) Computer Vision Lab, Bosch\n  Corporate Research, Hildesheim, Germany, (3) Bosch Center for Artificial\n  Intelligence, Renningen, Germany)", "title": "Bosch Deep Learning Hardware Benchmark", "comments": "Presented in MLBench: Workshop on Benchmarking Machine Learning\n  Workloads (https://sites.google.com/g.harvard.edu/mlbench/home)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread use of Deep Learning (DL) applications in science and industry\nhas created a large demand for efficient inference systems. This has resulted\nin a rapid increase of available Hardware Accelerators (HWAs) making comparison\nchallenging and laborious. To address this, several DL hardware benchmarks have\nbeen proposed aiming at a comprehensive comparison for many models, tasks, and\nhardware platforms. Here, we present our DL hardware benchmark which has been\nspecifically developed for inference on embedded HWAs and tasks required for\nautonomous driving. In addition to previous benchmarks, we propose a new\ngranularity level to evaluate common submodules of DL models, a twofold\nbenchmark procedure that accounts for hardware and model optimizations done by\nHWA manufacturers, and an extended set of performance indicators that can help\nto identify a mismatch between a HWA and the DL models used in our benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 09:50:24 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Runge", "Armin", ""], ["Wenzel", "Thomas", ""], ["Bariamis", "Dimitrios", ""], ["Staffler", "Benedikt Sebastian", ""], ["Drumond", "Lucas Rego", ""], ["Pfeiffer", "Michael", ""]]}, {"id": "2008.10298", "submitter": "Robin Kips", "authors": "Robin Kips, Pietro Gori, Matthieu Perrot, Isabelle Bloch", "title": "CA-GAN: Weakly Supervised Color Aware GAN for Controllable Makeup\n  Transfer", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-67070-2_17", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While existing makeup style transfer models perform an image synthesis whose\nresults cannot be explicitly controlled, the ability to modify makeup color\ncontinuously is a desirable property for virtual try-on applications. We\npropose a new formulation for the makeup style transfer task, with the\nobjective to learn a color controllable makeup style synthesis. We introduce\nCA-GAN, a generative model that learns to modify the color of specific objects\n(e.g. lips or eyes) in the image to an arbitrary target color while preserving\nbackground. Since color labels are rare and costly to acquire, our method\nleverages weakly supervised learning for conditional GANs. This enables to\nlearn a controllable synthesis of complex objects, and only requires a weak\nproxy of the image attribute that we desire to modify. Finally, we present for\nthe first time a quantitative analysis of makeup style transfer and color\ncontrol performance.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 10:11:17 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Kips", "Robin", ""], ["Gori", "Pietro", ""], ["Perrot", "Matthieu", ""], ["Bloch", "Isabelle", ""]]}, {"id": "2008.10309", "submitter": "Guohao Li", "authors": "Guohao Li, Mengmeng Xu, Silvio Giancola, Ali Thabet, Bernard Ghanem", "title": "LC-NAS: Latency Constrained Neural Architecture Search for Point Cloud\n  Networks", "comments": "Originally submitted to ECCV'2020 but rejected. This work was filed\n  with the United States Patent and Trademark Office (USPTO) on May 19, 2020\n  and assigned Serial No. 63/027,241", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud architecture design has become a crucial problem for 3D deep\nlearning. Several efforts exist to manually design architectures with high\naccuracy in point cloud tasks such as classification, segmentation, and\ndetection. Recent progress in automatic Neural Architecture Search (NAS)\nminimizes the human effort in network design and optimizes high performing\narchitectures. However, these efforts fail to consider important factors such\nas latency during inference. Latency is of high importance in time critical\napplications like self-driving cars, robot navigation, and mobile applications,\nthat are generally bound by the available hardware. In this paper, we introduce\na new NAS framework, dubbed LC-NAS, where we search for point cloud\narchitectures that are constrained to a target latency. We implement a novel\nlatency constraint formulation to trade-off between accuracy and latency in our\narchitecture search. Contrary to previous works, our latency loss guarantees\nthat the final network achieves latency under a specified target value. This is\ncrucial when the end task is to be deployed in a limited hardware setting.\nExtensive experiments show that LC-NAS is able to find state-of-the-art\narchitectures for point cloud classification in ModelNet40 with minimal\ncomputational cost. We also show how our searched architectures achieve any\ndesired latency with a reasonably low drop in accuracy. Finally, we show how\nour searched architectures easily transfer to a different task, part\nsegmentation on PartNet, where we achieve state-of-the-art results while\nlowering latency by a factor of 10.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 10:30:21 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Li", "Guohao", ""], ["Xu", "Mengmeng", ""], ["Giancola", "Silvio", ""], ["Thabet", "Ali", ""], ["Ghanem", "Bernard", ""]]}, {"id": "2008.10312", "submitter": "Evgenii Zheltonozhskii", "authors": "Evgenii Zheltonozhskii, Chaim Baskin, Alex M. Bronstein, Avi Mendelson", "title": "Self-Supervised Learning for Large-Scale Unsupervised Image Clustering", "comments": "accepted to NeurIPS 2020 Workshop: Self-Supervised Learning - Theory\n  and Practice", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Unsupervised learning has always been appealing to machine learning\nresearchers and practitioners, allowing them to avoid an expensive and\ncomplicated process of labeling the data. However, unsupervised learning of\ncomplex data is challenging, and even the best approaches show much weaker\nperformance than their supervised counterparts. Self-supervised deep learning\nhas become a strong instrument for representation learning in computer vision.\nHowever, those methods have not been evaluated in a fully unsupervised setting.\nIn this paper, we propose a simple scheme for unsupervised classification based\non self-supervised representations. We evaluate the proposed approach with\nseveral recent self-supervised methods showing that it achieves competitive\nresults for ImageNet classification (39% accuracy on ImageNet with 1000\nclusters and 46% with overclustering). We suggest adding the unsupervised\nevaluation to a set of standard benchmarks for self-supervised learning. The\ncode is available at https://github.com/Randl/kmeans_selfsuper\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 10:39:19 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 16:14:04 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Zheltonozhskii", "Evgenii", ""], ["Baskin", "Chaim", ""], ["Bronstein", "Alex M.", ""], ["Mendelson", "Avi", ""]]}, {"id": "2008.10313", "submitter": "Yixiao Ge", "authors": "Yixiao Ge, Shijie Yu, Dapeng Chen", "title": "Improved Mutual Mean-Teaching for Unsupervised Domain Adaptive Re-ID", "comments": "2nd place solution to VisDA-2020 Challenge (ECCVW). Code&Models are\n  available at https://github.com/yxgeee/VisDA-ECCV20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report, we present our submission to the VisDA Challenge in\nECCV 2020 and we achieved one of the top-performing results on the leaderboard.\nOur solution is based on Structured Domain Adaptation (SDA) and Mutual\nMean-Teaching (MMT) frameworks. SDA, a domain-translation-based framework,\nfocuses on carefully translating the source-domain images to the target domain.\nMMT, a pseudo-label-based framework, focuses on conducting pseudo label\nrefinery with robust soft labels. Specifically, there are three main steps in\nour training pipeline. (i) We adopt SDA to generate source-to-target translated\nimages, and (ii) such images serve as informative training samples to pre-train\nthe network. (iii) The pre-trained network is further fine-tuned by MMT on the\ntarget domain. Note that we design an improved MMT (dubbed MMT+) to further\nmitigate the label noise by modeling inter-sample relations across two domains\nand maintaining the instance discrimination. Our proposed method achieved\n74.78% accuracies in terms of mAP, ranked the 2nd place out of 153 teams.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 10:43:40 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Ge", "Yixiao", ""], ["Yu", "Shijie", ""], ["Chen", "Dapeng", ""]]}, {"id": "2008.10314", "submitter": "Shoma Iwai", "authors": "Shoma Iwai, Tomo Miyazaki, Yoshihiro Sugaya and Shinichiro Omachi", "title": "Fidelity-Controllable Extreme Image Compression with Generative\n  Adversarial Networks", "comments": "8 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a GAN-based image compression method working at extremely low\nbitrates below 0.1bpp. Most existing learned image compression methods suffer\nfrom blur at extremely low bitrates. Although GAN can help to reconstruct sharp\nimages, there are two drawbacks. First, GAN makes training unstable. Second,\nthe reconstructions often contain unpleasing noise or artifacts. To address\nboth of the drawbacks, our method adopts two-stage training and network\ninterpolation. The two-stage training is effective to stabilize the training.\nMoreover, the network interpolation utilizes the models in both stages and\nreduces undesirable noise and artifacts, while maintaining important edges.\nHence, we can control the trade-off between perceptual quality and fidelity\nwithout re-training models. The experimental results show that our model can\nreconstruct high quality images. Furthermore, our user study confirms that our\nreconstructions are preferable to state-of-the-art GAN-based image compression\nmodel. The code will be available.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 10:45:19 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Iwai", "Shoma", ""], ["Miyazaki", "Tomo", ""], ["Sugaya", "Yoshihiro", ""], ["Omachi", "Shinichiro", ""]]}, {"id": "2008.10320", "submitter": "Fanhua Shang", "authors": "Hongying Liu, Zhubo Ruan, Chaowei Fang, Peng Zhao, Fanhua Shang,\n  Yuanyuan Liu, Lijun Wang", "title": "A Single Frame and Multi-Frame Joint Network for 360-degree Panorama\n  Video Super-Resolution", "comments": "10 pages, 5 figures, submitted to an international peer-review\n  journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spherical videos, also known as \\ang{360} (panorama) videos, can be viewed\nwith various virtual reality devices such as computers and head-mounted\ndisplays. They attract large amount of interest since awesome immersion can be\nexperienced when watching spherical videos. However, capturing, storing and\ntransmitting high-resolution spherical videos are extremely expensive. In this\npaper, we propose a novel single frame and multi-frame joint network (SMFN) for\nrecovering high-resolution spherical videos from low-resolution inputs. To take\nadvantage of pixel-level inter-frame consistency, deformable convolutions are\nused to eliminate the motion difference between feature maps of the target\nframe and its neighboring frames. A mixed attention mechanism is devised to\nenhance the feature representation capability. The dual learning strategy is\nexerted to constrain the space of solution so that a better solution can be\nfound. A novel loss function based on the weighted mean square error is\nproposed to emphasize on the super-resolution of the equatorial regions. This\nis the first attempt to settle the super-resolution of spherical videos, and we\ncollect a novel dataset from the Internet, MiG Panorama Video, which includes\n204 videos. Experimental results on 4 representative video clips demonstrate\nthe efficacy of the proposed method. The dataset and code are available at\nhttps://github.com/lovepiano/SMFN_For_360VSR.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 11:09:54 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Liu", "Hongying", ""], ["Ruan", "Zhubo", ""], ["Fang", "Chaowei", ""], ["Zhao", "Peng", ""], ["Shang", "Fanhua", ""], ["Liu", "Yuanyuan", ""], ["Wang", "Lijun", ""]]}, {"id": "2008.10325", "submitter": "Pavan A", "authors": "Pavan A, Adithya Bennur, Mohit Gaggar, Shylaja S S", "title": "LCA-Net: Light Convolutional Autoencoder for Image Dehazing", "comments": "4 pages, 7 figures in .pdf format and 1 figure in .jpg format", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image dehazing is a crucial image pre-processing task aimed at removing the\nincoherent noise generated by haze to improve the visual appeal of the image.\nThe existing models use sophisticated networks and custom loss functions which\nare computationally inefficient and requires heavy hardware to run. Time is of\nthe essence in image pre-processing since real time outputs can be obtained\ninstantly. To overcome these problems, our proposed generic model uses a very\nlight convolutional encoder-decoder network which does not depend on any\natmospheric models. The network complexity-image quality trade off is handled\nwell in this neural network and the performance of this network is not limited\nby low-spec systems. This network achieves optimum dehazing performance at a\nmuch faster rate, on several standard datasets, comparable to the\nstate-of-the-art methods in terms of image quality.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 11:20:52 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["A", "Pavan", ""], ["Bennur", "Adithya", ""], ["Gaggar", "Mohit", ""], ["S", "Shylaja S", ""]]}, {"id": "2008.10329", "submitter": "Guoqing Zhang", "authors": "Jianwei Zhang and zhenxing Wang and yuhui Zheng and Guoqing Zhang", "title": "Cascade Convolutional Neural Network for Image Super-Resolution", "comments": "12 page,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of the super-resolution convolutional neural network\n(SRCNN), deep learning technique has been widely applied in the field of image\nsuper-resolution. Previous works mainly focus on optimizing the structure of\nSRCNN, which have been achieved well performance in speed and restoration\nquality for image super-resolution. However, most of these approaches only\nconsider a specific scale image during the training process, while ignoring the\nrelationship between different scales of images. Motivated by this concern, in\nthis paper, we propose a cascaded convolution neural network for image\nsuper-resolution (CSRCNN), which includes three cascaded Fast SRCNNs and each\nFast SRCNN can process a specific scale image. Images of different scales can\nbe trained simultaneously and the learned network can make full use of the\ninformation resided in different scales of images. Extensive experiments show\nthat our network can achieve well performance for image SR.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 11:34:03 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 09:31:23 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Zhang", "Jianwei", ""], ["Wang", "zhenxing", ""], ["Zheng", "yuhui", ""], ["Zhang", "Guoqing", ""]]}, {"id": "2008.10351", "submitter": "Lucas Hu", "authors": "Lucas Hu, Caleb Robinson, Bistra Dilkina", "title": "Model Generalization in Deep Learning Applications for Land Cover\n  Mapping", "comments": "9 pages, 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that deep learning models can be used to classify\nland-use data from geospatial satellite imagery. We show that when these deep\nlearning models are trained on data from specific continents/seasons, there is\na high degree of variability in model performance on out-of-sample\ncontinents/seasons. This suggests that just because a model accurately predicts\nland-use classes in one continent or season does not mean that the model will\naccurately predict land-use classes in a different continent or season. We then\nuse clustering techniques on satellite imagery from different continents to\nvisualize the differences in landscapes that make geospatial generalization\nparticularly difficult, and summarize our takeaways for future satellite\nimagery-related applications.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 01:50:52 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 02:04:42 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 19:04:16 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Hu", "Lucas", ""], ["Robinson", "Caleb", ""], ["Dilkina", "Bistra", ""]]}, {"id": "2008.10356", "submitter": "Shengjun Liu", "authors": "Shengjun Liu, Ningkang Jiang, Yuanbin Wu", "title": "Visual Attack and Defense on Text", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modifying characters of a piece of text to their visual similar ones often\nap-pear in spam in order to fool inspection systems and other conditions, which\nwe regard as a kind of adversarial attack to neural models. We pro-pose a way\nof generating such visual text attack and show that the attacked text are\nreadable by humans but mislead a neural classifier greatly. We ap-ply a\nvision-based model and adversarial training to defense the attack without\nlosing the ability to understand normal text. Our results also show that visual\nattack is extremely sophisticated and diverse, more work needs to be done to\nsolve this.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 15:44:58 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Liu", "Shengjun", ""], ["Jiang", "Ningkang", ""], ["Wu", "Yuanbin", ""]]}, {"id": "2008.10399", "submitter": "Abhinav Sagar", "authors": "Abhinav Sagar", "title": "Generate High Resolution Images With Generative Variational Autoencoder", "comments": "The network architecture used in this paper while training the model\n  is not correct", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel neural network to generate high resolution\nimages. We replace the decoder of VAE with a discriminator while using the\nencoder as it is. The encoder is fed data from a normal distribution while the\ngenerator is fed from a gaussian distribution. The combination from both is\ngiven to a discriminator which tells whether the generated image is correct or\nnot. We evaluate our network on 3 different datasets: MNIST, LSUN and CelebA\ndataset. Our network beats the previous state of the art using MMD, SSIM, log\nlikelihood, reconstruction error, ELBO and KL divergence as the evaluation\nmetrics while generating much sharper images. This work is potentially very\nexciting as we are able to combine the advantages of generative models and\ninference models in a principled bayesian manner.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 20:15:34 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 17:08:46 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 18:15:20 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Sagar", "Abhinav", ""]]}, {"id": "2008.10400", "submitter": "Sanghyeon An", "authors": "Sanghyeon An, Minjun Lee, Sanglee Park, Heerin Yang, Jungmin So", "title": "An Ensemble of Simple Convolutional Neural Network Models for MNIST\n  Digit Recognition", "comments": "10 pages, 12 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report that a very high accuracy on the MNIST test set can be achieved by\nusing simple convolutional neural network (CNN) models. We use three different\nmodels with 3x3, 5x5, and 7x7 kernel size in the convolution layers. Each model\nconsists of a set of convolution layers followed by a single fully connected\nlayer. Every convolution layer uses batch normalization and ReLU activation,\nand pooling is not used. Rotation and translation is used to augment training\ndata, which is frequently used in most image classification tasks. A majority\nvoting using the three models independently trained on the training data set\ncan achieve up to 99.87% accuracy on the test set, which is one of the\nstate-of-the-art results. A two-layer ensemble, a heterogeneous ensemble of\nthree homogeneous ensemble networks, can achieve up to 99.91% test accuracy.\nThe results can be reproduced by using the code at:\nhttps://github.com/ansh941/MnistSimpleCNN\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 09:27:05 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 03:49:48 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["An", "Sanghyeon", ""], ["Lee", "Minjun", ""], ["Park", "Sanglee", ""], ["Yang", "Heerin", ""], ["So", "Jungmin", ""]]}, {"id": "2008.10418", "submitter": "Grzegorz Jacenk\\'ow", "authors": "Grzegorz Jacenk\\'ow, Alison Q. O'Neil, Brian Mohr, Sotirios A.\n  Tsaftaris", "title": "INSIDE: Steering Spatial Attention with Non-Imaging Information in CNNs", "comments": "Accepted at International Conference on Medical Image Computing and\n  Computer Assisted Intervention (MICCAI) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of integrating non-imaging information into\nsegmentation networks to improve performance. Conditioning layers such as FiLM\nprovide the means to selectively amplify or suppress the contribution of\ndifferent feature maps in a linear fashion. However, spatial dependency is\ndifficult to learn within a convolutional paradigm. In this paper, we propose a\nmechanism to allow for spatial localisation conditioned on non-imaging\ninformation, using a feature-wise attention mechanism comprising a\ndifferentiable parametrised function (e.g. Gaussian), prior to applying the\nfeature-wise modulation. We name our method INstance modulation with SpatIal\nDEpendency (INSIDE). The conditioning information might comprise any factors\nthat relate to spatial or spatio-temporal information such as lesion location,\nsize, and cardiac cycle phase. Our method can be trained end-to-end and does\nnot require additional supervision. We evaluate the method on two datasets: a\nnew CLEVR-Seg dataset where we segment objects based on location, and the ACDC\ndataset conditioned on cardiac phase and slice location within the volume. Code\nand the CLEVR-Seg dataset are available at https://github.com/jacenkow/inside.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 13:32:05 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Jacenk\u00f3w", "Grzegorz", ""], ["O'Neil", "Alison Q.", ""], ["Mohr", "Brian", ""], ["Tsaftaris", "Sotirios A.", ""]]}, {"id": "2008.10428", "submitter": "Hezhen Hu", "authors": "Hezhen Hu, Wengang Zhou, Junfu Pu, Houqiang Li", "title": "Global-local Enhancement Network for NMFs-aware Sign Language\n  Recognition", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign language recognition (SLR) is a challenging problem, involving complex\nmanual features, i.e., hand gestures, and fine-grained non-manual features\n(NMFs), i.e., facial expression, mouth shapes, etc. Although manual features\nare dominant, non-manual features also play an important role in the expression\nof a sign word. Specifically, many sign words convey different meanings due to\nnon-manual features, even though they share the same hand gestures. This\nambiguity introduces great challenges in the recognition of sign words. To\ntackle the above issue, we propose a simple yet effective architecture called\nGlobal-local Enhancement Network (GLE-Net), including two mutually promoted\nstreams towards different crucial aspects of SLR. Of the two streams, one\ncaptures the global contextual relationship, while the other models the\ndiscriminative fine-grained cues. Moreover, due to the lack of datasets\nexplicitly focusing on this kind of features, we introduce the first non-manual\nfeatures-aware isolated Chinese sign language dataset (NMFs-CSL) with a total\nvocabulary size of 1,067 sign words in daily life. Extensive experiments on\nNMFs-CSL and SLR500 datasets demonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 13:28:55 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Hu", "Hezhen", ""], ["Zhou", "Wengang", ""], ["Pu", "Junfu", ""], ["Li", "Houqiang", ""]]}, {"id": "2008.10436", "submitter": "Ming Zhu", "authors": "Ming Zhu, Chao Ma, Pan Ji, Xiaokang Yang", "title": "Cross-Modality 3D Object Detection", "comments": "Accepted by WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on exploring the fusion of images and point clouds\nfor 3D object detection in view of the complementary nature of the two\nmodalities, i.e., images possess more semantic information while point clouds\nspecialize in distance sensing. To this end, we present a novel two-stage\nmulti-modal fusion network for 3D object detection, taking both binocular\nimages and raw point clouds as input. The whole architecture facilitates\ntwo-stage fusion. The first stage aims at producing 3D proposals through sparse\npoint-wise feature fusion. Within the first stage, we further exploit a joint\nanchor mechanism that enables the network to utilize 2D-3D classification and\nregression simultaneously for better proposal generation.\n  The second stage works on the 2D and 3D proposal regions and fuses their\ndense features. In addition, we propose to use pseudo LiDAR points from stereo\nmatching as a data augmentation method to densify the LiDAR points, as we\nobserve that objects missed by the detection network mostly have too few points\nespecially for far-away objects. Our experiments on the KITTI dataset show that\nthe proposed multi-stage fusion helps the network to learn better\nrepresentations.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 11:01:20 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Zhu", "Ming", ""], ["Ma", "Chao", ""], ["Ji", "Pan", ""], ["Yang", "Xiaokang", ""]]}, {"id": "2008.10444", "submitter": "Hui Wen", "authors": "Hui Wen, Yue Wu, Chenming Yang, Jingjing Li, Yue Zhu, Xu Jiang,\n  Hancong Duan", "title": "Transferring Inter-Class Correlation", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Teacher-Student (T-S) framework is widely utilized in the classification\ntasks, through which the performance of one neural network (the student) can be\nimproved by transferring knowledge from another trained neural network (the\nteacher). Since the transferring knowledge is related to the network capacities\nand structures between the teacher and the student, how to define efficient\nknowledge remains an open question. To address this issue, we design a novel\ntransferring knowledge, the Self-Attention based Inter-Class Correlation (ICC)\nmap in the output layer, and propose our T-S framework, Inter-Class Correlation\nTransfer (ICCT).\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 13:24:44 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Wen", "Hui", ""], ["Wu", "Yue", ""], ["Yang", "Chenming", ""], ["Li", "Jingjing", ""], ["Zhu", "Yue", ""], ["Jiang", "Xu", ""], ["Duan", "Hancong", ""]]}, {"id": "2008.10454", "submitter": "Sebastiano Verde", "authors": "Sebastiano Verde, Paolo Bestagini, Simone Milani, Giancarlo Calvagno\n  and Stefano Tubaro", "title": "FOCAL: A Forgery Localization Framework based on Video Coding\n  Self-Consistency", "comments": null, "journal-ref": null, "doi": "10.1109/OJSP.2021.3074298", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forgery operations on video contents are nowadays within the reach of anyone,\nthanks to the availability of powerful and user-friendly editing software.\nIntegrity verification and authentication of videos represent a major interest\nin both journalism (e.g., fake news debunking) and legal environments dealing\nwith digital evidence (e.g., a court of law). While several strategies and\ndifferent forensics traces have been proposed in recent years, latest solutions\naim at increasing the accuracy by combining multiple detectors and features.\nThis paper presents a video forgery localization framework that verifies the\nself-consistency of coding traces between and within video frames, by fusing\nthe information derived from a set of independent feature descriptors. The\nfeature extraction step is carried out by means of an explainable convolutional\nneural network architecture, specifically designed to look for and classify\ncoding artifacts. The overall framework was validated in two typical forgery\nscenarios: temporal and spatial splicing. Experimental results show an\nimprovement to the state-of-the-art on temporal splicing localization and also\npromising performance in the newly tackled case of spatial splicing, on both\nsynthetic and real-world videos.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 13:55:14 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 07:55:11 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Verde", "Sebastiano", ""], ["Bestagini", "Paolo", ""], ["Milani", "Simone", ""], ["Calvagno", "Giancarlo", ""], ["Tubaro", "Stefano", ""]]}, {"id": "2008.10464", "submitter": "Jiahua Dong", "authors": "Jiahua Dong, Yang Cong, Gan Sun, Yuyang Liu, Xiaowei Xu", "title": "CSCL: Critical Semantic-Consistent Learning for Unsupervised Domain\n  Adaptation", "comments": "Accepted to Proceedings of the European Conference on Computer Vision\n  2020 (ECCV 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation without consuming annotation process for\nunlabeled target data attracts appealing interests in semantic segmentation.\nHowever, 1) existing methods neglect that not all semantic representations\nacross domains are transferable, which cripples domain-wise transfer with\nuntransferable knowledge; 2) they fail to narrow category-wise distribution\nshift due to category-agnostic feature alignment. To address above challenges,\nwe develop a new Critical Semantic-Consistent Learning (CSCL) model, which\nmitigates the discrepancy of both domain-wise and category-wise distributions.\nSpecifically, a critical transfer based adversarial framework is designed to\nhighlight transferable domain-wise knowledge while neglecting untransferable\nknowledge. Transferability-critic guides transferability-quantizer to maximize\npositive transfer gain under reinforcement learning manner, although negative\ntransfer of untransferable knowledge occurs. Meanwhile, with the help of\nconfidence-guided pseudo labels generator of target samples, a symmetric soft\ndivergence loss is presented to explore inter-class relationships and\nfacilitate category-wise distribution alignment. Experiments on several\ndatasets demonstrate the superiority of our model.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 14:12:04 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Dong", "Jiahua", ""], ["Cong", "Yang", ""], ["Sun", "Gan", ""], ["Liu", "Yuyang", ""], ["Xu", "Xiaowei", ""]]}, {"id": "2008.10480", "submitter": "Ke Mei", "authors": "Ke Mei, Lei li, Jinchang Xu, Yanhua Cheng, Yugeng Lin", "title": "3rd Place Solution to \"Google Landmark Retrieval 2020\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image retrieval is a fundamental problem in computer vision. This paper\npresents our 3rd place detailed solution to the Google Landmark Retrieval 2020\nchallenge. We focus on the exploration of data cleaning and models with metric\nlearning. We use a data cleaning strategy based on embedding clustering.\nBesides, we employ a data augmentation method called Corner-Cutmix, which\nimproves the model's ability to recognize multi-scale and occluded landmark\nimages. We show in detail the ablation experiments and results of our method.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 14:39:51 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 03:44:05 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Mei", "Ke", ""], ["li", "Lei", ""], ["Xu", "Jinchang", ""], ["Cheng", "Yanhua", ""], ["Lin", "Yugeng", ""]]}, {"id": "2008.10486", "submitter": "Abdelaziz Djelouah", "authors": "Leonhard Helminger, Abdelaziz Djelouah, Markus Gross, Christopher\n  Schroers", "title": "Lossy Image Compression with Normalizing Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Deep learning based image compression has recently witnessed exciting\nprogress and in some cases even managed to surpass transform coding based\napproaches that have been established and refined over many decades. However,\nstate-of-the-art solutions for deep image compression typically employ\nautoencoders which map the input to a lower dimensional latent space and thus\nirreversibly discard information already before quantization. Due to that, they\ninherently limit the range of quality levels that can be covered. In contrast,\ntraditional approaches in image compression allow for a larger range of quality\nlevels. Interestingly, they employ an invertible transformation before\nperforming the quantization step which explicitly discards information.\nInspired by this, we propose a deep image compression method that is able to go\nfrom low bit-rates to near lossless quality by leveraging normalizing flows to\nlearn a bijective mapping from the image space to a latent representation. In\naddition to this, we demonstrate further advantages unique to our solution,\nsuch as the ability to maintain constant quality results through re-encoding,\neven when performed multiple times. To the best of our knowledge, this is the\nfirst work to explore the opportunities for leveraging normalizing flows for\nlossy image compression.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 14:46:23 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Helminger", "Leonhard", ""], ["Djelouah", "Abdelaziz", ""], ["Gross", "Markus", ""], ["Schroers", "Christopher", ""]]}, {"id": "2008.10487", "submitter": "Jianbo Liu", "authors": "Jianbo Liu, Junjun He, Jiawei Zhang, Jimmy S. Ren, Hongsheng Li", "title": "EfficientFCN: Holistically-guided Decoding for Semantic Segmentation", "comments": "Accepted in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both performance and efficiency are important to semantic segmentation.\nState-of-the-art semantic segmentation algorithms are mostly based on dilated\nFully Convolutional Networks (dilatedFCN), which adopt dilated convolutions in\nthe backbone networks to extract high-resolution feature maps for achieving\nhigh-performance segmentation performance. However, due to many convolution\noperations are conducted on the high-resolution feature maps, such\ndilatedFCN-based methods result in large computational complexity and memory\nconsumption. To balance the performance and efficiency, there also exist\nencoder-decoder structures that gradually recover the spatial information by\ncombining multi-level feature maps from the encoder. However, the performances\nof existing encoder-decoder methods are far from comparable with the\ndilatedFCN-based methods. In this paper, we propose the EfficientFCN, whose\nbackbone is a common ImageNet pre-trained network without any dilated\nconvolution. A holistically-guided decoder is introduced to obtain the\nhigh-resolution semantic-rich feature maps via the multi-scale features from\nthe encoder. The decoding task is converted to novel codebook generation and\ncodeword assembly task, which takes advantages of the high-level and low-level\nfeatures from the encoder. Such a framework achieves comparable or even better\nperformance than state-of-the-art methods with only 1/3 of the computational\ncost. Extensive experiments on PASCAL Context, PASCAL VOC, ADE20K validate the\neffectiveness of the proposed EfficientFCN.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 14:48:23 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 02:42:38 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Liu", "Jianbo", ""], ["He", "Junjun", ""], ["Zhang", "Jiawei", ""], ["Ren", "Jimmy S.", ""], ["Li", "Hongsheng", ""]]}, {"id": "2008.10534", "submitter": "Kenneth Lai", "authors": "Kenneth Lai and Svetlana N. Yanushkevich", "title": "Decision Support for Video-based Detection of Flu Symptoms", "comments": "8 pages, 7 figures, submitted to IEEE SMC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of decision support systems is a growing domain that can be\napplied in the area of disease control and diagnostics. Using video-based\nsurveillance data, skeleton features are extracted to perform action\nrecognition, specifically the detection and recognition of coughing and\nsneezing motions. Providing evidence of flu-like symptoms, a decision support\nsystem based on causal networks is capable of providing the operator with vital\ninformation for decision-making. A modified residual temporal convolutional\nnetwork is proposed for action recognition using skeleton features. This paper\naddresses the capability of using results from a machine-learning model as\nevidence for a cognitive decision support system. We propose risk and trust\nmeasures as a metric to bridge between machine-learning and machine-reasoning.\nWe provide experiments on evaluating the performance of the proposed network\nand how these performance measures can be combined with risk to generate trust.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 16:16:38 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Lai", "Kenneth", ""], ["Yanushkevich", "Svetlana N.", ""]]}, {"id": "2008.10542", "submitter": "Young-Keun Kim", "authors": "Ji-Hwan You, Seon Taek Oh, Jae-Eun Park, Azim Eskandarian, and\n  Young-Keun Kim", "title": "Automatic LiDAR Extrinsic Calibration System using Photodetector and\n  Planar Board for Large-scale Applications", "comments": "prepost for IEEE journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel automatic calibration system to estimate the\nextrinsic parameters of LiDAR mounted on a mobile platform for sensor\nmisalignment inspection in the large-scale production of highly automated\nvehicles. To obtain subdegree and subcentimeter accuracy levels of extrinsic\ncalibration, this study proposed a new concept of a target board with embedded\nphotodetector arrays, named the PD-target system, to find the precise position\nof the correspondence laser beams on the target surface. Furthermore, the\nproposed system requires only the simple design of the target board at the\nfixed pose in a close range to be readily applicable in the automobile\nmanufacturing environment. The experimental evaluation of the proposed system\non low-resolution LiDAR showed that the LiDAR offset pose can be estimated\nwithin 0.1 degree and 3 mm levels of precision. The high accuracy and\nsimplicity of the proposed calibration system make it practical for large-scale\napplications for the reliability and safety of autonomous systems.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 16:28:40 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["You", "Ji-Hwan", ""], ["Oh", "Seon Taek", ""], ["Park", "Jae-Eun", ""], ["Eskandarian", "Azim", ""], ["Kim", "Young-Keun", ""]]}, {"id": "2008.10544", "submitter": "Martin Gerdzhev", "authors": "Martin Gerdzhev, Ryan Razani, Ehsan Taghavi, Bingbing Liu", "title": "TORNADO-Net: mulTiview tOtal vaRiatioN semAntic segmentation with\n  Diamond inceptiOn module", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation of point clouds is a key component of scene\nunderstanding for robotics and autonomous driving. In this paper, we introduce\nTORNADO-Net - a neural network for 3D LiDAR point cloud semantic segmentation.\nWe incorporate a multi-view (bird-eye and range) projection feature extraction\nwith an encoder-decoder ResNet architecture with a novel diamond context block.\nCurrent projection-based methods do not take into account that neighboring\npoints usually belong to the same class. To better utilize this local\nneighbourhood information and reduce noisy predictions, we introduce a\ncombination of Total Variation, Lovasz-Softmax, and Weighted Cross-Entropy\nlosses. We also take advantage of the fact that the LiDAR data encompasses 360\ndegrees field of view and uses circular padding. We demonstrate\nstate-of-the-art results on the SemanticKITTI dataset and also provide thorough\nquantitative evaluations and ablation results.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 16:32:41 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Gerdzhev", "Martin", ""], ["Razani", "Ryan", ""], ["Taghavi", "Ehsan", ""], ["Liu", "Bingbing", ""]]}, {"id": "2008.10545", "submitter": "Yalong Bai", "authors": "Yalong Bai, Yuxiang Chen, Wei Yu, Linfang Wang, and Wei Zhang", "title": "Products-10K: A Large-scale Product Recognition Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of electronic commerce, the way of shopping has\nexperienced a revolutionary evolution. To fully meet customers' massive and\ndiverse online shopping needs with quick response, the retailing AI system\nneeds to automatically recognize products from images and videos at the\nstock-keeping unit (SKU) level with high accuracy. However, product recognition\nis still a challenging task, since many of SKU-level products are fine-grained\nand visually similar by a rough glimpse. Although there are already some\nproducts benchmarks available, these datasets are either too small (limited\nnumber of products) or noisy-labeled (lack of human labeling). In this paper,\nwe construct a human-labeled product image dataset named \"Products-10K\", which\ncontains 10,000 fine-grained SKU-level products frequently bought by online\ncustomers in JD.com. Based on our new database, we also introduced several\nuseful tips and tricks for fine-grained product recognition. The products-10K\ndataset is available via https://products-10k.github.io/.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 16:33:37 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Bai", "Yalong", ""], ["Chen", "Yuxiang", ""], ["Yu", "Wei", ""], ["Wang", "Linfang", ""], ["Zhang", "Wei", ""]]}, {"id": "2008.10548", "submitter": "Eldad Klaiman", "authors": "Jacob Gildenblat, Ido Ben-Shaul, Zvi Lapp, and Eldad Klaiman", "title": "Certainty Pooling for Multiple Instance Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Instance Learning is a form of weakly supervised learning in which\nthe data is arranged in sets of instances called bags with one label assigned\nper bag. The bag level class prediction is derived from the multiple instances\nthrough application of a permutation invariant pooling operator on instance\npredictions or embeddings. We present a novel pooling operator called\n\\textbf{Certainty Pooling} which incorporates the model certainty into bag\npredictions resulting in a more robust and explainable model. We compare our\nproposed method with other pooling operators in controlled experiments with low\nevidence ratio bags based on MNIST, as well as on a real life histopathology\ndataset - Camelyon16. Our method outperforms other methods in both bag level\nand instance level prediction, especially when only small training sets are\navailable. We discuss the rationale behind our approach and the reasons for its\nsuperiority for these types of datasets.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 16:38:46 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Gildenblat", "Jacob", ""], ["Ben-Shaul", "Ido", ""], ["Lapp", "Zvi", ""], ["Klaiman", "Eldad", ""]]}, {"id": "2008.10559", "submitter": "Luis Guillermo Rold\\~ao Jimenez", "authors": "Luis Rold\\~ao, Raoul de Charette, Anne Verroust-Blondet", "title": "LMSCNet: Lightweight Multiscale 3D Semantic Completion", "comments": "Accepted at 3DV 2020 (Oral). For a demo video, see\n  http://tiny.cc/lmscnet. Code is available at\n  https://github.com/cv-rits/LMSCNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach for multiscale 3Dsemantic scene completion from\nvoxelized sparse 3D LiDAR scans. As opposed to the literature, we use a 2D UNet\nbackbone with comprehensive multiscale skip connections to enhance feature\nflow, along with 3D segmentation heads. On the SemanticKITTI benchmark, our\nmethod performs on par on semantic completion and better on occupancy\ncompletion than all other published methods -- while being significantly\nlighter and faster. As such it provides a great performance/speed trade-off for\nmobile-robotics applications. The ablation studies demonstrate our method is\nrobust to lower density inputs, and that it enables very high speed semantic\ncompletion at the coarsest level. Our code is available at\nhttps://github.com/cv-rits/LMSCNet.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 16:52:16 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 15:26:18 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Rold\u00e3o", "Luis", ""], ["de Charette", "Raoul", ""], ["Verroust-Blondet", "Anne", ""]]}, {"id": "2008.10580", "submitter": "Krushi Patel", "authors": "Brian McClannahan, Krushi Patel, Usman Sajid, Cuncong Zhong, Guanghui\n  Wang", "title": "Classification of Noncoding RNA Elements Using Deep Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes to employ deep convolutional neural networks (CNNs) to\nclassify noncoding RNA (ncRNA) sequences. To this end, we first propose an\nefficient approach to convert the RNA sequences into images characterizing\ntheir base-pairing probability. As a result, classifying RNA sequences is\nconverted to an image classification problem that can be efficiently solved by\navailable CNN-based classification models. The paper also considers the folding\npotential of the ncRNAs in addition to their primary sequence. Based on the\nproposed approach, a benchmark image classification dataset is generated from\nthe RFAM database of ncRNA sequences. In addition, three classical CNN models\nhave been implemented and compared to demonstrate the superior performance and\nefficiency of the proposed approach. Extensive experimental results show the\ngreat potential of using deep learning approaches for RNA classification.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 17:43:50 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["McClannahan", "Brian", ""], ["Patel", "Krushi", ""], ["Sajid", "Usman", ""], ["Zhong", "Cuncong", ""], ["Wang", "Guanghui", ""]]}, {"id": "2008.10584", "submitter": "Young-Keun Kim", "authors": "Seontake Oh, Ji-Hwan You, Azim Eskandarian, Young-Keun Kim", "title": "Accurate Alignment Inspection System for Low-resolution Automotive and\n  Mobility LiDAR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A misalignment of LiDAR as low as a few degrees could cause a significant\nerror in obstacle detection and mapping that could cause safety and quality\nissues. In this paper, an accurate inspection system is proposed for estimating\na LiDAR alignment error after sensor attachment on a mobility system such as a\nvehicle or robot. The proposed method uses only a single target board at the\nfixed position to estimate the three orientations (roll, tilt, and yaw) and the\nhorizontal position of the LiDAR attachment with sub-degree and millimeter\nlevel accuracy. After the proposed preprocessing steps, the feature beam points\nthat are the closest to each target corner are extracted and used to calculate\nthe sensor attachment pose with respect to the target board frame using a\nnonlinear optimization method and with a low computational cost. The\nperformance of the proposed method is evaluated using a test bench that can\ncontrol the reference yaw and horizontal translation of LiDAR within ranges of\n3 degrees and 30 millimeters, respectively. The experimental results for a\nlow-resolution 16 channel LiDAR (Velodyne VLP-16) confirmed that misalignment\ncould be estimated with accuracy within 0.2 degrees and 4 mm. The high accuracy\nand simplicity of the proposed system make it practical for large-scale\nindustrial applications such as automobile or robot manufacturing process that\ninspects the sensor attachment for the safety quality control.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 17:47:59 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Oh", "Seontake", ""], ["You", "Ji-Hwan", ""], ["Eskandarian", "Azim", ""], ["Kim", "Young-Keun", ""]]}, {"id": "2008.10588", "submitter": "Lucy Chai", "authors": "Lucy Chai, David Bau, Ser-Nam Lim, Phillip Isola", "title": "What makes fake images detectable? Understanding properties that\n  generalize", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of image generation and manipulation is reaching impressive\nlevels, making it increasingly difficult for a human to distinguish between\nwhat is real and what is fake. However, deep networks can still pick up on the\nsubtle artifacts in these doctored images. We seek to understand what\nproperties of fake images make them detectable and identify what generalizes\nacross different model architectures, datasets, and variations in training. We\nuse a patch-based classifier with limited receptive fields to visualize which\nregions of fake images are more easily detectable. We further show a technique\nto exaggerate these detectable properties and demonstrate that, even when the\nimage generator is adversarially finetuned against a fake image classifier, it\nis still imperfect and leaves detectable artifacts in certain image patches.\nCode is available at https://chail.github.io/patch-forensics/.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 17:50:28 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Chai", "Lucy", ""], ["Bau", "David", ""], ["Lim", "Ser-Nam", ""], ["Isola", "Phillip", ""]]}, {"id": "2008.10592", "submitter": "Benjamin Wilson", "authors": "Benjamin Wilson, Zsolt Kira, James Hays", "title": "3D for Free: Crossmodal Transfer Learning using HD Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection is a core perceptual challenge for robotics and\nautonomous driving. However, the class-taxonomies in modern autonomous driving\ndatasets are significantly smaller than many influential 2D detection datasets.\nIn this work, we address the long-tail problem by leveraging both the large\nclass-taxonomies of modern 2D datasets and the robustness of state-of-the-art\n2D detection methods. We proceed to mine a large, unlabeled dataset of images\nand LiDAR, and estimate 3D object bounding cuboids, seeded from an\noff-the-shelf 2D instance segmentation model. Critically, we constrain this\nill-posed 2D-to-3D mapping by using high-definition maps and object size\npriors. The result of the mining process is 3D cuboids with varying confidence.\nThis mining process is itself a 3D object detector, although not especially\naccurate when evaluated as such. However, we then train a 3D object detection\nmodel on these cuboids, consistent with other recent observations in the deep\nlearning literature, we find that the resulting model is fairly robust to the\nnoisy supervision that our mining process provides. We mine a collection of\n1151 unlabeled, multimodal driving logs from an autonomous vehicle and use the\ndiscovered objects to train a LiDAR-based object detector. We show that\ndetector performance increases as we mine more unlabeled data. With our full,\nunlabeled dataset, our method performs competitively with fully supervised\nmethods, even exceeding the performance for certain object categories, without\nany human 3D annotations.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 17:54:51 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Wilson", "Benjamin", ""], ["Kira", "Zsolt", ""], ["Hays", "James", ""]]}, {"id": "2008.10598", "submitter": "Jia-Bin Huang", "authors": "Hsin-Ping Huang, Hung-Yu Tseng, Hsin-Ying Lee, Jia-Bin Huang", "title": "Semantic View Synthesis", "comments": "ECCV 2020. Project: https://hhsinping.github.io/svs/index.html Colab:\n  https://colab.research.google.com/drive/1iT5PfK7zl1quAOwC227GfBjieFMVHjI5", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle a new problem of semantic view synthesis -- generating\nfree-viewpoint rendering of a synthesized scene using a semantic label map as\ninput. We build upon recent advances in semantic image synthesis and view\nsynthesis for handling photographic image content generation and view\nextrapolation. Direct application of existing image/view synthesis methods,\nhowever, results in severe ghosting/blurry artifacts. To address the drawbacks,\nwe propose a two-step approach. First, we focus on synthesizing the color and\ndepth of the visible surface of the 3D scene. We then use the synthesized color\nand depth to impose explicit constraints on the multiple-plane image (MPI)\nrepresentation prediction process. Our method produces sharp contents at the\noriginal view and geometrically consistent renderings across novel viewpoints.\nThe experiments on numerous indoor and outdoor images show favorable results\nagainst several strong baselines and validate the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 17:59:46 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Huang", "Hsin-Ping", ""], ["Tseng", "Hung-Yu", ""], ["Lee", "Hsin-Ying", ""], ["Huang", "Jia-Bin", ""]]}, {"id": "2008.10599", "submitter": "William Peebles", "authors": "William Peebles, John Peebles, Jun-Yan Zhu, Alexei Efros, Antonio\n  Torralba", "title": "The Hessian Penalty: A Weak Prior for Unsupervised Disentanglement", "comments": "ECCV 2020 (Spotlight). Code available at\n  https://github.com/wpeebles/hessian_penalty . Project page and videos\n  available at https://www.wpeebles.com/hessian-penalty", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing disentanglement methods for deep generative models rely on\nhand-picked priors and complex encoder-based architectures. In this paper, we\npropose the Hessian Penalty, a simple regularization term that encourages the\nHessian of a generative model with respect to its input to be diagonal. We\nintroduce a model-agnostic, unbiased stochastic approximation of this term\nbased on Hutchinson's estimator to compute it efficiently during training. Our\nmethod can be applied to a wide range of deep generators with just a few lines\nof code. We show that training with the Hessian Penalty often causes\naxis-aligned disentanglement to emerge in latent space when applied to ProGAN\non several datasets. Additionally, we use our regularization term to identify\ninterpretable directions in BigGAN's latent space in an unsupervised fashion.\nFinally, we provide empirical evidence that the Hessian Penalty encourages\nsubstantial shrinkage when applied to over-parameterized latent spaces.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 17:59:56 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Peebles", "William", ""], ["Peebles", "John", ""], ["Zhu", "Jun-Yan", ""], ["Efros", "Alexei", ""], ["Torralba", "Antonio", ""]]}, {"id": "2008.10631", "submitter": "Matthias M\\\"uller", "authors": "Matthias M\\\"uller, Vladlen Koltun", "title": "OpenBot: Turning Smartphones into Robots", "comments": "Accepted at ICRA'21. Documentation and code are available at\n  www.openbot.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current robots are either expensive or make significant compromises on\nsensory richness, computational power, and communication capabilities. We\npropose to leverage smartphones to equip robots with extensive sensor suites,\npowerful computational abilities, state-of-the-art communication channels, and\naccess to a thriving software ecosystem. We design a small electric vehicle\nthat costs $50 and serves as a robot body for standard Android smartphones. We\ndevelop a software stack that allows smartphones to use this body for mobile\noperation and demonstrate that the system is sufficiently powerful to support\nadvanced robotics workloads such as person following and real-time autonomous\nnavigation in unstructured environments. Controlled experiments demonstrate\nthat the presented approach is robust across different smartphones and robot\nbodies. A video of our work is available at\nhttps://www.youtube.com/watch?v=qc8hFLyWDOM\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 18:04:50 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 19:08:00 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["M\u00fcller", "Matthias", ""], ["Koltun", "Vladlen", ""]]}, {"id": "2008.10634", "submitter": "Michael Firman", "authors": "Michael Firman, Neill D. F. Campbell, Lourdes Agapito, Gabriel J.\n  Brostow", "title": "DiverseNet: When One Right Answer is not Enough", "comments": "Presented at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many structured prediction tasks in machine vision have a collection of\nacceptable answers, instead of one definitive ground truth answer. Segmentation\nof images, for example, is subject to human labeling bias. Similarly, there are\nmultiple possible pixel values that could plausibly complete occluded image\nregions. State-of-the art supervised learning methods are typically optimized\nto make a single test-time prediction for each query, failing to find other\nmodes in the output space. Existing methods that allow for sampling often\nsacrifice speed or accuracy.\n  We introduce a simple method for training a neural network, which enables\ndiverse structured predictions to be made for each test-time query. For a\nsingle input, we learn to predict a range of possible answers. We compare\nfavorably to methods that seek diversity through an ensemble of networks. Such\nstochastic multiple choice learning faces mode collapse, where one or more\nensemble members fail to receive any training signal. Our best performing\nsolution can be deployed for various tasks, and just involves small\nmodifications to the existing single-mode architecture, loss function, and\ntraining regime. We demonstrate that our method results in quantitative\nimprovements across three challenging tasks: 2D image completion, 3D volume\nestimation, and flow prediction.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 18:12:49 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Firman", "Michael", ""], ["Campbell", "Neill D. F.", ""], ["Agapito", "Lourdes", ""], ["Brostow", "Gabriel J.", ""]]}, {"id": "2008.10652", "submitter": "Ling Zhang", "authors": "Ling Zhang, Yu Shi, Jiawen Yao, Yun Bian, Kai Cao, Dakai Jin, Jing\n  Xiao, Le Lu", "title": "Robust Pancreatic Ductal Adenocarcinoma Segmentation with\n  Multi-Institutional Multi-Phase Partially-Annotated CT Scans", "comments": "10 pages, 2 figures; MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and automated tumor segmentation is highly desired since it has the\ngreat potential to increase the efficiency and reproducibility of computing\nmore complete tumor measurements and imaging biomarkers, comparing to (often\npartial) human measurements. This is probably the only viable means to enable\nthe large-scale clinical oncology patient studies that utilize medical imaging.\nDeep learning approaches have shown robust segmentation performances for\ncertain types of tumors, e.g., brain tumors in MRI imaging, when a training\ndataset with plenty of pixel-level fully-annotated tumor images is available.\nHowever, more than often, we are facing the challenge that only (very) limited\nannotations are feasible to acquire, especially for hard tumors. Pancreatic\nductal adenocarcinoma (PDAC) segmentation is one of the most challenging tumor\nsegmentation tasks, yet critically important for clinical needs. Previous work\non PDAC segmentation is limited to the moderate amounts of annotated patient\nimages (n<300) from venous or venous+arterial phase CT scans. Based on a new\nself-learning framework, we propose to train the PDAC segmentation model using\na much larger quantity of patients (n~=1,000), with a mix of annotated and\nun-annotated venous or multi-phase CT images. Pseudo annotations are generated\nby combining two teacher models with different PDAC segmentation specialties on\nunannotated images, and can be further refined by a teaching assistant model\nthat identifies associated vessels around the pancreas. A student model is\ntrained on both manual and pseudo annotated multi-phase images. Experiment\nresults show that our proposed method provides an absolute improvement of 6.3%\nDice score over the strong baseline of nnUNet trained on annotated images,\nachieving the performance (Dice = 0.71) similar to the inter-observer\nvariability between radiologists.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 18:50:30 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Zhang", "Ling", ""], ["Shi", "Yu", ""], ["Yao", "Jiawen", ""], ["Bian", "Yun", ""], ["Cao", "Kai", ""], ["Jin", "Dakai", ""], ["Xiao", "Jing", ""], ["Lu", "Le", ""]]}, {"id": "2008.10678", "submitter": "Josef Lorenz Rumberger", "authors": "Josef Lorenz Rumberger, Lisa Mais, Dagmar Kainmueller", "title": "Probabilistic Deep Learning for Instance Segmentation", "comments": "ECCV 2020 BioImage Computing Workshop", "journal-ref": null, "doi": "10.1007/978-3-030-66415-2_29", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic convolutional neural networks, which predict distributions of\npredictions instead of point estimates, led to recent advances in many areas of\ncomputer vision, from image reconstruction to semantic segmentation. Besides\nstate of the art benchmark results, these networks made it possible to quantify\nlocal uncertainties in the predictions. These were used in active learning\nframeworks to target the labeling efforts of specialist annotators or to assess\nthe quality of a prediction in a safety-critical environment. However, for\ninstance segmentation problems these methods are not frequently used so far. We\nseek to close this gap by proposing a generic method to obtain model-inherent\nuncertainty estimates within proposal-free instance segmentation models.\nFurthermore, we analyze the quality of the uncertainty estimates with a metric\nadapted from semantic segmentation. We evaluate our method on the BBBC010 C.\\\nelegans dataset, where it yields competitive performance while also predicting\nuncertainty estimates that carry information about object-level inaccuracies\nlike false splits and false merges. We perform a simulation to show the\npotential use of such uncertainty estimates in guided proofreading.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 19:51:48 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 11:38:42 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Rumberger", "Josef Lorenz", ""], ["Mais", "Lisa", ""], ["Kainmueller", "Dagmar", ""]]}, {"id": "2008.10680", "submitter": "Zhihao Shi", "authors": "Zhihao Shi, Xiaohong Liu, Kangdi Shi, Linhui Dai, Jun Chen", "title": "Video Frame Interpolation via Generalized Deformable Convolution", "comments": "13pages, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video frame interpolation aims at synthesizing intermediate frames from\nnearby source frames while maintaining spatial and temporal consistencies. The\nexisting deep-learning-based video frame interpolation methods can be roughly\ndivided into two categories: flow-based methods and kernel-based methods. The\nperformance of flow-based methods is often jeopardized by the inaccuracy of\nflow map estimation due to oversimplified motion models, while that of\nkernel-based methods tends to be constrained by the rigidity of kernel shape.\nTo address these performance-limiting issues, a novel mechanism named\ngeneralized deformable convolution is proposed, which can effectively learn\nmotion information in a data-driven manner and freely select sampling points in\nspace-time. We further develop a new video frame interpolation method based on\nthis mechanism. Our extensive experiments demonstrate that the new method\nperforms favorably against the state-of-the-art, especially when dealing with\ncomplex motions.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 20:00:39 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 20:27:05 GMT"}, {"version": "v3", "created": "Thu, 18 Mar 2021 16:09:35 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Shi", "Zhihao", ""], ["Liu", "Xiaohong", ""], ["Shi", "Kangdi", ""], ["Dai", "Linhui", ""], ["Chen", "Jun", ""]]}, {"id": "2008.10710", "submitter": "Xiaohong Liu", "authors": "Xiaohong Liu, Kangdi Shi, Zhe Wang, Jun Chen", "title": "Exploit Camera Raw Data for Video Super-Resolution via Hidden Markov\n  Model Inference", "comments": "13 pages, 14 figures, accepted in IEEE Transactions on Image\n  Processing", "journal-ref": null, "doi": "10.1109/TIP.2021.3049974", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To the best of our knowledge, the existing deep-learning-based Video\nSuper-Resolution (VSR) methods exclusively make use of videos produced by the\nImage Signal Processor (ISP) of the camera system as inputs. Such methods are\n1) inherently suboptimal due to information loss incurred by non-invertible\noperations in ISP, and 2) inconsistent with the real imaging pipeline where VSR\nin fact serves as a pre-processing unit of ISP. To address this issue, we\npropose a new VSR method that can directly exploit camera sensor data,\naccompanied by a carefully built Raw Video Dataset (RawVD) for training,\nvalidation, and testing. This method consists of a Successive Deep Inference\n(SDI) module and a reconstruction module, among others. The SDI module is\ndesigned according to the architectural principle suggested by a canonical\ndecomposition result for Hidden Markov Model (HMM) inference; it estimates the\ntarget high-resolution frame by repeatedly performing pairwise feature fusion\nusing deformable convolutions. The reconstruction module, built with\nelaborately designed Attention-based Residual Dense Blocks (ARDBs), serves the\npurpose of 1) refining the fused feature and 2) learning the color information\nneeded to generate a spatial-specific transformation for accurate color\ncorrection. Extensive experiments demonstrate that owing to the informativeness\nof the camera raw data, the effectiveness of the network architecture, and the\nseparation of super-resolution and color correction processes, the proposed\nmethod achieves superior VSR results compared to the state-of-the-art and can\nbe adapted to any specific camera-ISP. Code and dataset are available at\nhttps://github.com/proteus1991/RawVSR.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 21:14:13 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 04:03:55 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Liu", "Xiaohong", ""], ["Shi", "Kangdi", ""], ["Wang", "Zhe", ""], ["Chen", "Jun", ""]]}, {"id": "2008.10719", "submitter": "Tianchang Shen", "authors": "Tianchang Shen, Jun Gao, Amlan Kar, Sanja Fidler", "title": "Interactive Annotation of 3D Object Geometry using 2D Scribbles", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring detailed 3D geometry of the scene is crucial for robotics\napplications, simulation, and 3D content creation. However, such information is\nhard to obtain, and thus very few datasets support it. In this paper, we\npropose an interactive framework for annotating 3D object geometry from both\npoint cloud data and RGB imagery. The key idea behind our approach is to\nexploit strong priors that humans have about the 3D world in order to\ninteractively annotate complete 3D shapes. Our framework targets naive users\nwithout artistic or graphics expertise. We introduce two simple-to-use\ninteraction modules. First, we make an automatic guess of the 3D shape and\nallow the user to provide feedback about large errors by drawing scribbles in\ndesired 2D views. Next, we aim to correct minor errors, in which users drag and\ndrop mesh vertices, assisted by a neural interactive module implemented as a\nGraph Convolutional Network. Experimentally, we show that only a few user\ninteractions are needed to produce good quality 3D shapes on popular benchmarks\nsuch as ShapeNet, Pix3D and ScanNet. We implement our framework as a web\nservice and conduct a user study, where we show that user annotated data using\nour method effectively facilitates real-world learning tasks. Web service:\nhttp://www.cs.toronto.edu/~shenti11/scribble3d.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 21:51:29 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 02:43:19 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Shen", "Tianchang", ""], ["Gao", "Jun", ""], ["Kar", "Amlan", ""], ["Fidler", "Sanja", ""]]}, {"id": "2008.10736", "submitter": "Ovi Paul", "authors": "Abu Bakar Siddik Nayem, Anis Sarker, Ovi Paul, Amin Ali, Md. Ashraful\n  Amin and AKM Mahbubur Rahman", "title": "LULC Segmentation of RGB Satellite Image Using FCN-8", "comments": "Accepted paper at 3rd SLAAI-International Conference on Artificial\n  Intelligence; 13 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents use of Fully Convolutional Network (FCN-8) for semantic\nsegmentation of high-resolution RGB earth surface satel-lite images into land\nuse land cover (LULC) categories. Specically, we propose a non-overlapping\ngrid-based approach to train a Fully Convo-lutional Network (FCN-8) with vgg-16\nweights to segment satellite im-ages into four (forest, built-up, farmland and\nwater) classes. The FCN-8 semantically projects the discriminating features in\nlower resolution learned by the encoder onto the pixel space in higher\nresolution to get a dense classi cation. We experimented the proposed system\nwith Gaofen-2 image dataset, that contains 150 images of over 60 di erent\ncities in china. For comparison, we used available ground-truth along with\nimages segmented using a widely used commeriial GIS software called\neCogni-tion. With the proposed non-overlapping grid-based approach, FCN-8\nobtains signi cantly improved performance, than the eCognition soft-ware. Our\nmodel achieves average accuracy of 91.0% and average Inter-section over Union\n(IoU) of 0.84. In contrast, eCognitions average accu-racy is 74.0% and IoU is\n0.60. This paper also reports a detail analysis of errors occurred at the LULC\nboundary.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 22:32:30 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Nayem", "Abu Bakar Siddik", ""], ["Sarker", "Anis", ""], ["Paul", "Ovi", ""], ["Ali", "Amin", ""], ["Amin", "Md. Ashraful", ""], ["Rahman", "AKM Mahbubur", ""]]}, {"id": "2008.10766", "submitter": "Dong Lao", "authors": "Dong Lao, Peihao Zhu, Peter Wonka, Ganesh Sundaramoorthi", "title": "Channel-Directed Gradients for Optimization of Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce optimization methods for convolutional neural networks that can\nbe used to improve existing gradient-based optimization in terms of\ngeneralization error. The method requires only simple processing of existing\nstochastic gradients, can be used in conjunction with any optimizer, and has\nonly a linear overhead (in the number of parameters) compared to computation of\nthe stochastic gradient. The method works by computing the gradient of the loss\nfunction with respect to output-channel directed re-weighted L2 or Sobolev\nmetrics, which has the effect of smoothing components of the gradient across a\ncertain direction of the parameter tensor. We show that defining the gradients\nalong the output channel direction leads to a performance boost, while other\ndirections can be detrimental. We present the continuum theory of such\ngradients, its discretization, and application to deep networks. Experiments on\nbenchmark datasets, several networks and baseline optimizers show that\noptimizers can be improved in generalization error by simply computing the\nstochastic gradient with respect to output-channel directed metrics.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 00:44:09 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Lao", "Dong", ""], ["Zhu", "Peihao", ""], ["Wonka", "Peter", ""], ["Sundaramoorthi", "Ganesh", ""]]}, {"id": "2008.10774", "submitter": "Saeed Anwar", "authors": "Saeed Anwar, Muhammad Tahir, Chongyi Li, Ajmal Mian, Fahad Shahbaz\n  Khan, Abdul Wahab Muzaffar", "title": "Image Colorization: A Survey and Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image colorization is an essential image processing and computer vision\nbranch to colorize images and videos. Recently, deep learning techniques\nprogressed notably for image colorization. This article presents a\ncomprehensive survey of recent state-of-the-art colorization using deep\nlearning algorithms, describing their fundamental block architectures in terms\nof skip connections, input etc. as well as optimizers, loss functions, training\nprotocols, and training data etc. Generally, we can roughly categorize the\nexisting colorization techniques into seven classes. Besides, we also provide\nsome additional essential issues, such as benchmark datasets and evaluation\nmetrics. We also introduce a new dataset specific to colorization and perform\nan experimental evaluation of the publicly available methods. In the last\nsection, we discuss the limitations, possible solutions, and future research\ndirections of the rapidly evolving topic of deep image colorization that the\ncommunity should further address. Dataset and Codes for evaluation will be\npublicly available at https://github.com/saeed-anwar/ColorSurvey\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 01:22:52 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 11:44:35 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Anwar", "Saeed", ""], ["Tahir", "Muhammad", ""], ["Li", "Chongyi", ""], ["Mian", "Ajmal", ""], ["Khan", "Fahad Shahbaz", ""], ["Muzaffar", "Abdul Wahab", ""]]}, {"id": "2008.10785", "submitter": "Pengfei Ge", "authors": "Chuan-Xian Ren, Pengfei Ge, Peiyi Yang, Shuicheng Yan", "title": "Learning Target Domain Specific Classifier for Partial Domain Adaptation", "comments": null, "journal-ref": null, "doi": "10.1109/TNNLS.2020.2995648", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation~(UDA) aims at reducing the distribution\ndiscrepancy when transferring knowledge from a labeled source domain to an\nunlabeled target domain. Previous UDA methods assume that the source and target\ndomains share an identical label space, which is unrealistic in practice since\nthe label information of the target domain is agnostic. This paper focuses on a\nmore realistic UDA scenario, i.e. partial domain adaptation (PDA), where the\ntarget label space is subsumed to the source label space. In the PDA scenario,\nthe source outliers that are absent in the target domain may be wrongly matched\nto the target domain (technically named negative transfer), leading to\nperformance degradation of UDA methods. This paper proposes a novel Target\nDomain Specific Classifier Learning-based Domain Adaptation (TSCDA) method.\nTSCDA presents a soft-weighed maximum mean discrepancy criterion to partially\nalign feature distributions and alleviate negative transfer. Also, it learns a\ntarget-specific classifier for the target domain with pseudo-labels and\nmultiple auxiliary classifiers, to further address classifier shift. A module\nnamed Peers Assisted Learning is used to minimize the prediction difference\nbetween multiple target-specific classifiers, which makes the classifiers more\ndiscriminant for the target domain. Extensive experiments conducted on three\nPDA benchmark datasets show that TSCDA outperforms other state-of-the-art\nmethods with a large margin, e.g. $4\\%$ and $5.6\\%$ averagely on Office-31 and\nOffice-Home, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 02:28:24 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Ren", "Chuan-Xian", ""], ["Ge", "Pengfei", ""], ["Yang", "Peiyi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "2008.10786", "submitter": "Chiwoo Park", "authors": "Chiwoo Park, Sang Do Noh and Anuj Srivastava", "title": "Data Science for Motion and Time Analysis with Modern Motion Sensor Data", "comments": "Keywords: motion and time study, motion sensors, Riemannian manifold,\n  probability distribution on manifold, temporal evolution of probability\n  distributions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The motion-and-time analysis has been a popular research topic in operations\nresearch, especially for analyzing work performances in manufacturing and\nservice operations. It is regaining attention as continuous improvement tools\nfor lean manufacturing and smart factory. This paper develops a framework for\ndata-driven analysis of work motions and studies their correlations to work\nspeeds or execution rates, using data collected from modern motion sensors. The\npast analyses largely relied on manual steps involving time-consuming\nstop-watching and video-taping, followed by manual data analysis. While modern\nsensing devices have automated the collection of motion data, the motion\nanalytics that transform the new data into knowledge are largely\nunderdeveloped. Unsolved technical questions include: How the motion and time\ninformation can be extracted from the motion sensor data, how work motions and\nexecution rates are statistically modeled and compared, and what are the\nstatistical correlations of motions to the rates? In this paper, we develop a\nnovel mathematical framework for motion and time analysis with motion sensor\ndata, by defining new mathematical representation spaces of human motions and\nexecution rates and by developing statistical tools on these new spaces. This\nmethodological research is demonstrated using five use cases applied to\nmanufacturing motion data.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 02:33:33 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Park", "Chiwoo", ""], ["Noh", "Sang Do", ""], ["Srivastava", "Anuj", ""]]}, {"id": "2008.10796", "submitter": "Zongsheng Yue", "authors": "Zongsheng Yue, Hongwei Yong, Qian Zhao, Lei Zhang, Deyu Meng", "title": "Variational Image Restoration Network", "comments": "Extended Version of VDNet (NeurIPS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep neural networks (DNNs) have achieved significant success in image\nrestoration tasks by directly learning a powerful non-linear mapping from\ncorrupted images to their latent clean ones. However, there still exist two\nmajor limitations for these deep learning (DL)-based methods. Firstly, the\nnoises contained in real corrupted images are very complex, usually neglected\nand largely under-estimated in most current methods. Secondly, existing DL\nmethods are mostly trained on one pre-assumed degradation process for all of\nthe training image pairs, such as the widely used bicubic downsampling\nassumption in the image super-resolution task, inevitably leading to poor\ngeneralization performance when the true degradation does not match with such\nassumed one. To address these issues, we propose a unified generative model for\nthe image restoration, which elaborately configures the degradation process\nfrom the latent clean image to the observed corrupted one. Specifically,\ndifferent from most of current methods, the pixel-wisely non-i.i.d. Gaussian\ndistribution, being with more flexibility, is adopted in our method to fit the\ncomplex real noises. Furthermore, the method is built on the general image\ndegradation process, making it capable of adapting diverse degradations under\none single model. Besides, we design a variational inference algorithm to learn\nall parameters involved in the proposed model with explicit form of objective\nloss. Specifically, beyond traditional variational methodology, two DNNs are\nemployed to parameterize the posteriori distributions, one to infer the\ndistribution of the latent clean image, and another to infer the distribution\nof the image noise. Extensive experiments demonstrate the superiority of the\nproposed method on three classical image restoration tasks, including image\ndenoising, image super-resolution and JPEG image deblocking.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 03:30:53 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 08:38:05 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Yue", "Zongsheng", ""], ["Yong", "Hongwei", ""], ["Zhao", "Qian", ""], ["Zhang", "Lei", ""], ["Meng", "Deyu", ""]]}, {"id": "2008.10805", "submitter": "Kartikeya Bhardwaj", "authors": "Kartikeya Bhardwaj, Wei Chen, Radu Marculescu", "title": "New Directions in Distributed Deep Learning: Bringing the Network at\n  Forefront of IoT Design", "comments": "This preprint is for personal use only. The official article will\n  appear in proceedings of Design Automation Conference (DAC), 2020. This work\n  was presented at the DAC 2020 special session on Edge-to-Cloud Neural\n  Networks for Machine Learning Applications in Future IoT Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we first highlight three major challenges to large-scale\nadoption of deep learning at the edge: (i) Hardware-constrained IoT devices,\n(ii) Data security and privacy in the IoT era, and (iii) Lack of network-aware\ndeep learning algorithms for distributed inference across multiple IoT devices.\nWe then provide a unified view targeting three research directions that\nnaturally emerge from the above challenges: (1) Federated learning for training\ndeep networks, (2) Data-independent deployment of learning algorithms, and (3)\nCommunication-aware distributed inference. We believe that the above research\ndirections need a network-centric approach to enable the edge intelligence and,\ntherefore, fully exploit the true potential of IoT.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 04:08:10 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Bhardwaj", "Kartikeya", ""], ["Chen", "Wei", ""], ["Marculescu", "Radu", ""]]}, {"id": "2008.10824", "submitter": "Varuna De Silva D", "authors": "Varuna De Silva", "title": "A Critical Analysis of Patch Similarity Based Image Denoising Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image denoising is a classical signal processing problem that has received\nsignificant interest within the image processing community during the past two\ndecades. Most of the algorithms for image denoising has focused on the paradigm\nof non-local similarity, where image blocks in the neighborhood that are\nsimilar, are collected to build a basis for reconstruction. Through rigorous\nexperimentation, this paper reviews multiple aspects of image denoising\nalgorithm development based on non-local similarity. Firstly, the concept of\nnon-local similarity as a foundational quality that exists in natural images\nhas not received adequate attention. Secondly, the image denoising algorithms\nthat are developed are a combination of multiple building blocks, making\ncomparison among them a tedious task. Finally, most of the work surrounding\nimage denoising presents performance results based on Peak-Signal-to-Noise\nRatio (PSNR) between a denoised image and a reference image (which is perturbed\nwith Additive White Gaussian Noise). This paper starts with a statistical\nanalysis on non-local similarity and its effectiveness under various noise\nlevels, followed by a theoretical comparison of different state-of-the-art\nimage denoising algorithms. Finally, we argue for a methodological overhaul to\nincorporate no-reference image quality measures and unprocessed images (raw)\nduring performance evaluation of image denoising algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 05:30:37 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["De Silva", "Varuna", ""]]}, {"id": "2008.10831", "submitter": "Ajoy Mondal Dr.", "authors": "Madhav Agarwal and Ajoy Mondal and C. V. Jawahar", "title": "CDeC-Net: Composite Deformable Cascade Network for Table Detection in\n  Document Images", "comments": "12", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Localizing page elements/objects such as tables, figures, equations, etc. is\nthe primary step in extracting information from document images. We propose a\nnovel end-to-end trainable deep network, (CDeC-Net) for detecting tables\npresent in the documents. The proposed network consists of a multistage\nextension of Mask R-CNN with a dual backbone having deformable convolution for\ndetecting tables varying in scale with high detection accuracy at higher IoU\nthreshold. We empirically evaluate CDeC-Net on all the publicly available\nbenchmark datasets - ICDAR-2013, ICDAR-2017, ICDAR-2019,UNLV, Marmot,\nPubLayNet, and TableBank - with extensive experiments.\n  Our solution has three important properties: (i) a single trained model\nCDeC-Net{\\ddag} performs well across all the popular benchmark datasets; (ii)\nwe report excellent performances across multiple, including higher, thresholds\nof IoU; (iii) by following the same protocol of the recent papers for each of\nthe benchmarks, we consistently demonstrate the superior quantitative\nperformance. Our code and models will be publicly released for enabling the\nreproducibility of the results.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 05:53:59 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Agarwal", "Madhav", ""], ["Mondal", "Ajoy", ""], ["Jawahar", "C. V.", ""]]}, {"id": "2008.10833", "submitter": "Shanshan Zhao", "authors": "Shanshan Zhao, Mingming Gong, Huan Fu, and Dacheng Tao", "title": "Adaptive Context-Aware Multi-Modal Network for Depth Completion", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2021.3079821", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth completion aims to recover a dense depth map from the sparse depth data\nand the corresponding single RGB image. The observed pixels provide the\nsignificant guidance for the recovery of the unobserved pixels' depth. However,\ndue to the sparsity of the depth data, the standard convolution operation,\nexploited by most of existing methods, is not effective to model the observed\ncontexts with depth values. To address this issue, we propose to adopt the\ngraph propagation to capture the observed spatial contexts. Specifically, we\nfirst construct multiple graphs at different scales from observed pixels. Since\nthe graph structure varies from sample to sample, we then apply the attention\nmechanism on the propagation, which encourages the network to model the\ncontextual information adaptively. Furthermore, considering the mutli-modality\nof input data, we exploit the graph propagation on the two modalities\nrespectively to extract multi-modal representations. Finally, we introduce the\nsymmetric gated fusion strategy to exploit the extracted multi-modal features\neffectively. The proposed strategy preserves the original information for one\nmodality and also absorbs complementary information from the other through\nlearning the adaptive gating weights. Our model, named Adaptive Context-Aware\nMulti-Modal Network (ACMNet), achieves the state-of-the-art performance on two\nbenchmarks, {\\it i.e.}, KITTI and NYU-v2, and at the same time has fewer\nparameters than latest models. Our code is available at:\n\\url{https://github.com/sshan-zhao/ACMNet}.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 06:00:06 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Zhao", "Shanshan", ""], ["Gong", "Mingming", ""], ["Fu", "Huan", ""], ["Tao", "Dacheng", ""]]}, {"id": "2008.10843", "submitter": "Ajoy Mondal Dr.", "authors": "Ranajit Saha and Ajoy Mondal and C. V. Jawahar", "title": "Graphical Object Detection in Document Images", "comments": "8", "journal-ref": "ICDAR 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graphical elements: particularly tables and figures contain a visual summary\nof the most valuable information contained in a document. Therefore,\nlocalization of such graphical objects in the document images is the initial\nstep to understand the content of such graphical objects or document images. In\nthis paper, we present a novel end-to-end trainable deep learning based\nframework to localize graphical objects in the document images called as\nGraphical Object Detection (GOD). Our framework is data-driven and does not\nrequire any heuristics or meta-data to locate graphical objects in the document\nimages. The GOD explores the concept of transfer learning and domain adaptation\nto handle scarcity of labeled training images for graphical object detection\ntask in the document images. Performance analysis carried out on the various\npublic benchmark data sets: ICDAR-2013, ICDAR-POD2017,and UNLV shows that our\nmodel yields promising results as compared to state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 06:35:57 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Saha", "Ranajit", ""], ["Mondal", "Ajoy", ""], ["Jawahar", "C. V.", ""]]}, {"id": "2008.10850", "submitter": "Manyuan Zhang", "authors": "Manyuan Zhang, Guanglu Song, Hang Zhou, Yu Liu", "title": "Discriminability Distillation in Group Representation Learning", "comments": "To appear in Proceedings of the European Conference on Computer\n  Vision (ECCV), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning group representation is a commonly concerned issue in tasks where\nthe basic unit is a group, set, or sequence. Previously, the research community\ntries to tackle it by aggregating the elements in a group based on an indicator\neither defined by humans such as the quality and saliency, or generated by a\nblack box such as the attention score. This article provides a more essential\nand explicable view. We claim the most significant indicator to show whether\nthe group representation can be benefited from one of its element is not the\nquality or an inexplicable score, but the discriminability w.r.t. the model. We\nexplicitly design the discrimiability using embedded class centroids on a proxy\nset. We show the discrimiability knowledge has good properties that can be\ndistilled by a light-weight distillation network and can be generalized on the\nunseen target set. The whole procedure is denoted as discriminability\ndistillation learning (DDL). The proposed DDL can be flexibly plugged into many\ngroup-based recognition tasks without influencing the original training\nprocedures. Comprehensive experiments on various tasks have proven the\neffectiveness of DDL for both accuracy and efficiency. Moreover, it pushes\nforward the state-of-the-art results on these tasks by an impressive margin.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 07:15:51 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 07:33:34 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Zhang", "Manyuan", ""], ["Song", "Guanglu", ""], ["Zhou", "Hang", ""], ["Liu", "Yu", ""]]}, {"id": "2008.10869", "submitter": "David Fern\\'andez-Llorca", "authors": "David Fern\\'andez-Llorca, Mahdi Biparva, Rub\\'en Izquierdo-Gonzalo and\n  John K. Tsotsos", "title": "Two-Stream Networks for Lane-Change Prediction of Surrounding Vehicles", "comments": "This work has been accepted at the IEEE Intelligent Transportation\n  Systems Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In highway scenarios, an alert human driver will typically anticipate early\ncut-in and cut-out maneuvers of surrounding vehicles using only visual cues. An\nautomated system must anticipate these situations at an early stage too, to\nincrease the safety and the efficiency of its performance. To deal with\nlane-change recognition and prediction of surrounding vehicles, we pose the\nproblem as an action recognition/prediction problem by stacking visual cues\nfrom video cameras. Two video action recognition approaches are analyzed:\ntwo-stream convolutional networks and spatiotemporal multiplier networks.\nDifferent sizes of the regions around the vehicles are analyzed, evaluating the\nimportance of the interaction between vehicles and the context information in\nthe performance. In addition, different prediction horizons are evaluated. The\nobtained results demonstrate the potential of these methodologies to serve as\nrobust predictors of future lane-changes of surrounding vehicles in time\nhorizons between 1 and 2 seconds.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 07:59:15 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Fern\u00e1ndez-Llorca", "David", ""], ["Biparva", "Mahdi", ""], ["Izquierdo-Gonzalo", "Rub\u00e9n", ""], ["Tsotsos", "John K.", ""]]}, {"id": "2008.10902", "submitter": "Shuaiyi Huang", "authors": "Shuaiyi Huang, Qiuyue Wang, Xuming He", "title": "Confidence-aware Adversarial Learning for Self-supervised Semantic\n  Matching", "comments": "PRCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to address the challenging task of semantic matching\nwhere matching ambiguity is difficult to resolve even with learned deep\nfeatures. We tackle this problem by taking into account the confidence in\npredictions and develop a novel refinement strategy to correct partial matching\nerrors. Specifically, we introduce a Confidence-Aware Semantic Matching Network\n(CAMNet) which instantiates two key ideas of our approach. First, we propose to\nestimate a dense confidence map for a matching prediction through\nself-supervised learning. Second, based on the estimated confidence, we refine\ninitial predictions by propagating reliable matching to the rest of locations\non the image plane. In addition, we develop a new hybrid loss in which we\nintegrate a semantic alignment loss with a confidence loss, and an adversarial\nloss that measures the quality of semantic correspondence. We are the first\nthat exploit confidence during refinement to improve semantic matching accuracy\nand develop an end-to-end self-supervised adversarial learning procedure for\nthe entire matching network. We evaluate our method on two public benchmarks,\non which we achieve top performance over the prior state of the art. We will\nrelease our source code at https://github.com/ShuaiyiHuang/CAMNet.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 09:15:48 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Huang", "Shuaiyi", ""], ["Wang", "Qiuyue", ""], ["He", "Xuming", ""]]}, {"id": "2008.10913", "submitter": "Lorenzo Bertoni", "authors": "Lorenzo Bertoni, Sven Kreiss, Taylor Mordan, Alexandre Alahi", "title": "MonStereo: When Monocular and Stereo Meet at the Tail of 3D Human\n  Localization", "comments": "Accepted at the IEEE International Conference on Robotics and\n  Automation (ICRA) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular and stereo visions are cost-effective solutions for 3D human\nlocalization in the context of self-driving cars or social robots. However,\nthey are usually developed independently and have their respective strengths\nand limitations. We propose a novel unified learning framework that leverages\nthe strengths of both monocular and stereo cues for 3D human localization. Our\nmethod jointly (i) associates humans in left-right images, (ii) deals with\noccluded and distant cases in stereo settings by relying on the robustness of\nmonocular cues, and (iii) tackles the intrinsic ambiguity of monocular\nperspective projection by exploiting prior knowledge of the human height\ndistribution. We specifically evaluate outliers as well as challenging\ninstances, such as occluded and far-away pedestrians, by analyzing the entire\nerror distribution and by estimating calibrated confidence intervals. Finally,\nwe critically review the official KITTI 3D metrics and propose a practical 3D\nlocalization metric tailored for humans.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 09:47:58 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 16:59:49 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Bertoni", "Lorenzo", ""], ["Kreiss", "Sven", ""], ["Mordan", "Taylor", ""], ["Alahi", "Alexandre", ""]]}, {"id": "2008.10916", "submitter": "Shuxin Qin", "authors": "Shuxin Qin and Sijiang Liu", "title": "Towards End-to-end Car License Plate Location and Recognition in\n  Unconstrained Scenarios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benefiting from the rapid development of convolutional neural networks, the\nperformance of car license plate detection and recognition has been largely\nimproved. Nonetheless, challenges still exist especially for real-world\napplications. In this paper, we present an efficient and accurate framework to\nsolve the license plate detection and recognition tasks simultaneously. It is a\nlightweight and unified deep neural network, that can be optimized end-to-end\nand work in real-time. Specifically, for unconstrained scenarios, an\nanchor-free method is adopted to efficiently detect the bounding box and four\ncorners of a license plate, which are used to extract and rectify the target\nregion features. Then, a novel convolutional neural network branch is designed\nto further extract features of characters without segmentation. Finally,\nrecognition task is treated as sequence labelling problems, which are solved by\nConnectionist Temporal Classification (CTC) directly. Several public datasets\nincluding images collected from different scenarios under various conditions\nare chosen for evaluation. A large number of experiments indicate that the\nproposed method significantly outperforms the previous state-of-the-art methods\nin both speed and precision.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 09:51:33 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Qin", "Shuxin", ""], ["Liu", "Sijiang", ""]]}, {"id": "2008.10924", "submitter": "Jinheng Xie", "authors": "Jinheng Xie, Jun Wan, Linlin Shen, Zhihui Lai", "title": "Think about boundary: Fusing multi-level boundary information for\n  landmark heatmap regression", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although current face alignment algorithms have obtained pretty good\nperformances at predicting the location of facial landmarks, huge challenges\nremain for faces with severe occlusion and large pose variations, etc. On the\ncontrary, semantic location of facial boundary is more likely to be reserved\nand estimated on these scenes. Therefore, we study a two-stage but end-to-end\napproach for exploring the relationship between the facial boundary and\nlandmarks to get boundary-aware landmark predictions, which consists of two\nmodules: the self-calibrated boundary estimation (SCBE) module and the\nboundary-aware landmark transform (BALT) module. In the SCBE module, we modify\nthe stem layers and employ intermediate supervision to help generate\nhigh-quality facial boundary heatmaps. Boundary-aware features inherited from\nthe SCBE module are integrated into the BALT module in a multi-scale fusion\nframework to better model the transformation from boundary to landmark heatmap.\nExperimental results conducted on the challenging benchmark datasets\ndemonstrate that our approach outperforms state-of-the-art methods in the\nliterature.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 10:14:13 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Xie", "Jinheng", ""], ["Wan", "Jun", ""], ["Shen", "Linlin", ""], ["Lai", "Zhihui", ""]]}, {"id": "2008.10960", "submitter": "Julien Despois", "authors": "Julien Despois, Frederic Flament, Matthieu Perrot", "title": "AgingMapGAN (AMGAN): High-Resolution Controllable Face Aging with\n  Spatially-Aware Conditional GANs", "comments": "Project page: https://despoisj.github.io/AgingMapGAN/", "journal-ref": null, "doi": "10.1007/978-3-030-67070-2_37", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches and datasets for face aging produce results skewed\ntowards the mean, with individual variations and expression wrinkles often\ninvisible or overlooked in favor of global patterns such as the fattening of\nthe face. Moreover, they offer little to no control over the way the faces are\naged and can difficultly be scaled to large images, thus preventing their usage\nin many real-world applications. To address these limitations, we present an\napproach to change the appearance of a high-resolution image using\nethnicity-specific aging information and weak spatial supervision to guide the\naging process. We demonstrate the advantage of our proposed method in terms of\nquality, control, and how it can be used on high-definition images while\nlimiting the computational overhead.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 12:35:48 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 09:51:12 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Despois", "Julien", ""], ["Flament", "Frederic", ""], ["Perrot", "Matthieu", ""]]}, {"id": "2008.10966", "submitter": "Lijie Fan", "authors": "Lijie Fan, Tianhong Li, Yuan Yuan, Dina Katabi", "title": "In-Home Daily-Life Captioning Using Radio Signals", "comments": "ECCV 2020. The first two authors contributed equally to this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to caption daily life --i.e., to create a textual description\nof people's activities and interactions with objects in their homes. Addressing\nthis problem requires novel methods beyond traditional video captioning, as\nmost people would have privacy concerns about deploying cameras throughout\ntheir homes. We introduce RF-Diary, a new model for captioning daily life by\nanalyzing the privacy-preserving radio signal in the home with the home's\nfloormap. RF-Diary can further observe and caption people's life through walls\nand occlusions and in dark settings. In designing RF-Diary, we exploit the\nability of radio signals to capture people's 3D dynamics, and use the floormap\nto help the model learn people's interactions with objects. We also use a\nmulti-modal feature alignment training scheme that leverages existing\nvideo-based captioning datasets to improve the performance of our radio-based\ncaptioning model. Extensive experimental results demonstrate that RF-Diary\ngenerates accurate captions under visible conditions. It also sustains its good\nperformance in dark or occluded settings, where video-based captioning\napproaches fail to generate meaningful captions. For more information, please\nvisit our project webpage: http://rf-diary.csail.mit.edu\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 12:45:39 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Fan", "Lijie", ""], ["Li", "Tianhong", ""], ["Yuan", "Yuan", ""], ["Katabi", "Dina", ""]]}, {"id": "2008.10968", "submitter": "Eden Belouadah", "authors": "Eden Belouadah, Adrian Popescu, Umang Aggarwal, L\\'eo Saci", "title": "Active Class Incremental Learning for Imbalanced Datasets", "comments": "Accepted in IPCV workshop from ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental Learning (IL) allows AI systems to adapt to streamed data. Most\nexisting algorithms make two strong hypotheses which reduce the realism of the\nincremental scenario: (1) new data are assumed to be readily annotated when\nstreamed and (2) tests are run with balanced datasets while most real-life\ndatasets are actually imbalanced. These hypotheses are discarded and the\nresulting challenges are tackled with a combination of active and imbalanced\nlearning. We introduce sample acquisition functions which tackle imbalance and\nare compatible with IL constraints. We also consider IL as an imbalanced\nlearning problem instead of the established usage of knowledge distillation\nagainst catastrophic forgetting. Here, imbalance effects are reduced during\ninference through class prediction scaling. Evaluation is done with four visual\ndatasets and compares existing and proposed sample acquisition functions.\nResults indicate that the proposed contributions have a positive effect and\nreduce the gap between active and standard IL performance.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 12:47:09 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Belouadah", "Eden", ""], ["Popescu", "Adrian", ""], ["Aggarwal", "Umang", ""], ["Saci", "L\u00e9o", ""]]}, {"id": "2008.11009", "submitter": "Chee Seng Chan", "authors": "Jian Han Lim, Chee Seng Chan, Kam Woh Ng, Lixin Fan, Qiang Yang", "title": "Protect, Show, Attend and Tell: Image Captioning Model with Ownership\n  Protection", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By and large, existing Intellectual Property Right (IPR) protection on deep\nneural networks typically i) focus on image classification task only, and ii)\nfollow a standard digital watermarking framework that were conventionally used\nto protect the ownership of multimedia and video content. This paper\ndemonstrates that current digital watermarking framework is insufficient to\nprotect image captioning task that often regarded as one of the frontier A.I.\nproblems. As a remedy, this paper studies and proposes two different embedding\nschemes in the hidden memory state of a recurrent neural network to protect\nimage captioning model. From both theoretically and empirically points, we\nprove that a forged key will yield an unusable image captioning model,\ndefeating the purpose on infringement. To the best of our knowledge, this work\nis the first to propose ownership protection on image captioning task. Also,\nextensive experiments show that the proposed method does not compromise the\noriginal image captioning performance on all common captioning metrics on\nFlickr30k and MS-COCO datasets, and at the same time it is able to withstand\nboth removal and ambiguity attacks.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 13:48:35 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Lim", "Jian Han", ""], ["Chan", "Chee Seng", ""], ["Ng", "Kam Woh", ""], ["Fan", "Lixin", ""], ["Yang", "Qiang", ""]]}, {"id": "2008.11010", "submitter": "David Honz\\'atko", "authors": "David Honz\\'atko, Siavash A. Bigdeli, Engin T\\\"uretken, L. Andrea\n  Dunbar", "title": "Efficient Blind-Spot Neural Network Architecture for Image Denoising", "comments": null, "journal-ref": "2020 7th Swiss Conference on Data Science (SDS), Luzern,\n  Switzerland, 2020, pp. 59-60", "doi": "10.1109/SDS49233.2020.00022", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image denoising is an essential tool in computational photography. Standard\ndenoising techniques, which use deep neural networks at their core, require\npairs of clean and noisy images for its training. If we do not possess the\nclean samples, we can use blind-spot neural network architectures, which\nestimate the pixel value based on the neighbouring pixels only. These networks\nthus allow training on noisy images directly, as they by-design avoid trivial\nsolutions. Nowadays, the blind-spot is mostly achieved using shifted\nconvolutions or serialization. We propose a novel fully convolutional network\narchitecture that uses dilations to achieve the blind-spot property. Our\nnetwork improves the performance over the prior work and achieves\nstate-of-the-art results on established datasets.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 13:48:40 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Honz\u00e1tko", "David", ""], ["Bigdeli", "Siavash A.", ""], ["T\u00fcretken", "Engin", ""], ["Dunbar", "L. Andrea", ""]]}, {"id": "2008.11014", "submitter": "Xiangyong Cao Mr.", "authors": "Haixia Bi, Lin Xu, Xiangyong Cao, Yong Xue, Zongben Xu", "title": "Polarimetric SAR Image Semantic Segmentation with 3D Discrete Wavelet\n  Transform and Markov Random Field", "comments": null, "journal-ref": "IEEE Transactions on Image Processing (2020)", "doi": "10.1109/TIP.2020.2992177", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polarimetric synthetic aperture radar (PolSAR) image segmentation is\ncurrently of great importance in image processing for remote sensing\napplications. However, it is a challenging task due to two main reasons.\nFirstly, the label information is difficult to acquire due to high annotation\ncosts. Secondly, the speckle effect embedded in the PolSAR imaging process\nremarkably degrades the segmentation performance. To address these two issues,\nwe present a contextual PolSAR image semantic segmentation method in this\npaper.With a newly defined channelwise consistent feature set as input, the\nthree-dimensional discrete wavelet transform (3D-DWT) technique is employed to\nextract discriminative multi-scale features that are robust to speckle noise.\nThen Markov random field (MRF) is further applied to enforce label smoothness\nspatially during segmentation. By simultaneously utilizing 3D-DWT features and\nMRF priors for the first time, contextual information is fully integrated\nduring the segmentation to ensure accurate and smooth segmentation. To\ndemonstrate the effectiveness of the proposed method, we conduct extensive\nexperiments on three real benchmark PolSAR image data sets. Experimental\nresults indicate that the proposed method achieves promising segmentation\naccuracy and preferable spatial consistency using a minimal number of labeled\npixels.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 08:28:18 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Bi", "Haixia", ""], ["Xu", "Lin", ""], ["Cao", "Xiangyong", ""], ["Xue", "Yong", ""], ["Xu", "Zongben", ""]]}, {"id": "2008.11048", "submitter": "Jun Wei", "authors": "Jun Wei, Shuhui Wang, Zhe Wu, Chi Su, Qingming Huang, Qi Tian", "title": "Label Decoupling Framework for Salient Object Detection", "comments": "Accepted by CVPR2020, https://github.com/weijun88/LDF", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To get more accurate saliency maps, recent methods mainly focus on\naggregating multi-level features from fully convolutional network (FCN) and\nintroducing edge information as auxiliary supervision. Though remarkable\nprogress has been achieved, we observe that the closer the pixel is to the\nedge, the more difficult it is to be predicted, because edge pixels have a very\nimbalance distribution. To address this problem, we propose a label decoupling\nframework (LDF) which consists of a label decoupling (LD) procedure and a\nfeature interaction network (FIN). LD explicitly decomposes the original\nsaliency map into body map and detail map, where body map concentrates on\ncenter areas of objects and detail map focuses on regions around edges. Detail\nmap works better because it involves much more pixels than traditional edge\nsupervision. Different from saliency map, body map discards edge pixels and\nonly pays attention to center areas. This successfully avoids the distraction\nfrom edge pixels during training. Therefore, we employ two branches in FIN to\ndeal with body map and detail map respectively. Feature interaction (FI) is\ndesigned to fuse the two complementary branches to predict the saliency map,\nwhich is then used to refine the two branches again. This iterative refinement\nis helpful for learning better representations and more precise saliency maps.\nComprehensive experiments on six benchmark datasets demonstrate that LDF\noutperforms state-of-the-art approaches on different evaluation metrics.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 14:23:38 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Wei", "Jun", ""], ["Wang", "Shuhui", ""], ["Wu", "Zhe", ""], ["Su", "Chi", ""], ["Huang", "Qingming", ""], ["Tian", "Qi", ""]]}, {"id": "2008.11055", "submitter": "Luciano Oliveira", "authors": "Gabriel Lefundes, Luciano Oliveira", "title": "On estimating gaze by self-attention augmented convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimation of 3D gaze is highly relevant to multiple fields, including but\nnot limited to interactive systems, specialized human-computer interfaces, and\nbehavioral research. Although recently deep learning methods have boosted the\naccuracy of appearance-based gaze estimation, there is still room for\nimprovement in the network architectures for this particular task. Therefore we\npropose here a novel network architecture grounded on self-attention augmented\nconvolutions to improve the quality of the learned features during the training\nof a shallower residual network. The rationale is that self-attention mechanism\ncan help outperform deeper architectures by learning dependencies between\ndistant regions in full-face images. This mechanism can also create better and\nmore spatially-aware feature representations derived from the face and eye\nimages before gaze regression. We dubbed our framework ARes-gaze, which\nexplores our Attention-augmented ResNet (ARes-14) as twin convolutional\nbackbones. In our experiments, results showed a decrease of the average angular\nerror by 2.38% when compared to state-of-the-art methods on the MPIIFaceGaze\ndata set, and a second-place on the EyeDiap data set. It is noteworthy that our\nproposed framework was the only one to reach high accuracy simultaneously on\nboth data sets.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 14:29:05 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 13:49:19 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Lefundes", "Gabriel", ""], ["Oliveira", "Luciano", ""]]}, {"id": "2008.11062", "submitter": "Haotao Wang", "authors": "Haotao Wang, Shupeng Gui, Haichuan Yang, Ji Liu, Zhangyang Wang", "title": "GAN Slimming: All-in-One GAN Compression by A Unified Optimization\n  Framework", "comments": "ECCV 2020 spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have gained increasing popularity in\nvarious computer vision applications, and recently start to be deployed to\nresource-constrained mobile devices. Similar to other deep models,\nstate-of-the-art GANs suffer from high parameter complexities. That has\nrecently motivated the exploration of compressing GANs (usually generators).\nCompared to the vast literature and prevailing success in compressing deep\nclassifiers, the study of GAN compression remains in its infancy, so far\nleveraging individual compression techniques instead of more sophisticated\ncombinations. We observe that due to the notorious instability of training\nGANs, heuristically stacking different compression techniques will result in\nunsatisfactory results. To this end, we propose the first unified optimization\nframework combining multiple compression means for GAN compression, dubbed GAN\nSlimming (GS). GS seamlessly integrates three mainstream compression\ntechniques: model distillation, channel pruning and quantization, together with\nthe GAN minimax objective, into one unified optimization form, that can be\nefficiently optimized from end to end. Without bells and whistles, GS largely\noutperforms existing options in compressing image-to-image translation GANs.\nSpecifically, we apply GS to compress CartoonGAN, a state-of-the-art style\ntransfer network, by up to 47 times, with minimal visual quality degradation.\nCodes and pre-trained models can be found at\nhttps://github.com/TAMU-VITA/GAN-Slimming.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 14:39:42 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Wang", "Haotao", ""], ["Gui", "Shupeng", ""], ["Yang", "Haichuan", ""], ["Liu", "Ji", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2008.11073", "submitter": "Miriam Bellver Bueno", "authors": "Miriam Bellver, Amaia Salvador, Jordi Torres, Xavier Giro-i-Nieto", "title": "Mask-guided sample selection for Semi-Supervised Instance Segmentation", "comments": "Preprint submitted to Multimedia Tools and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation methods are usually trained with pixel-level annotations,\nwhich require significant human effort to collect. The most common solution to\naddress this constraint is to implement weakly-supervised pipelines trained\nwith lower forms of supervision, such as bounding boxes or scribbles. Another\noption are semi-supervised methods, which leverage a large amount of unlabeled\ndata and a limited number of strongly-labeled samples. In this second setup,\nsamples to be strongly-annotated can be selected randomly or with an active\nlearning mechanism that chooses the ones that will maximize the model\nperformance. In this work, we propose a sample selection approach to decide\nwhich samples to annotate for semi-supervised instance segmentation. Our method\nconsists in first predicting pseudo-masks for the unlabeled pool of samples,\ntogether with a score predicting the quality of the mask. This score is an\nestimate of the Intersection Over Union (IoU) of the segment with the ground\ntruth mask. We study which samples are better to annotate given the quality\nscore, and show how our approach outperforms a random selection, leading to\nimproved performance for semi-supervised instance segmentation with low\nannotation budgets.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 14:44:58 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Bellver", "Miriam", ""], ["Salvador", "Amaia", ""], ["Torres", "Jordi", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "2008.11083", "submitter": "William Diggin", "authors": "William Diggin and Michael Diggin", "title": "Using the discrete radon transformation for grayscale image moments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image moments are weighted sums over pixel values in a given image and are\nused in object detection and localization. Raw image moments are derived\ndirectly from the image and are fundamental in deriving moment invariants\nquantities. The current general algorithm for raw image moments is\ncomputationally expensive and the number of multiplications needed scales with\nthe number of pixels in the image. For an image of size (N,M), it has O(NM)\nmultiplications. In this paper we outline an algorithm using the Discrete Radon\nTransformation for computing the raw image moments of a grayscale image. It\nreduces two dimensional moment calculations to linear combinations of one\ndimensional moment calculations. We show that the number of multiplications\nneeded scales as O(N + M), making it faster then the most widely used algorithm\nof raw image moments.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 14:55:53 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Diggin", "William", ""], ["Diggin", "Michael", ""]]}, {"id": "2008.11098", "submitter": "Jialiang Wang", "authors": "Jialiang Wang, Varun Jampani, Deqing Sun, Charles Loop, Stan\n  Birchfield, Jan Kautz", "title": "Improving Deep Stereo Network Generalization with Geometric Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end deep learning methods have advanced stereo vision in recent years\nand obtained excellent results when the training and test data are similar.\nHowever, large datasets of diverse real-world scenes with dense ground truth\nare difficult to obtain and currently not publicly available to the research\ncommunity. As a result, many algorithms rely on small real-world datasets of\nsimilar scenes or synthetic datasets, but end-to-end algorithms trained on such\ndatasets often generalize poorly to different images that arise in real-world\napplications. As a step towards addressing this problem, we propose to\nincorporate prior knowledge of scene geometry into an end-to-end stereo network\nto help networks generalize better. For a given network, we explicitly add a\ngradient-domain smoothness prior and occlusion reasoning into the network\ntraining, while the architecture remains unchanged during inference.\nExperimentally, we show consistent improvements if we train on synthetic\ndatasets and test on the Middlebury (real images) dataset. Noticeably, we\nimprove PSM-Net accuracy on Middlebury from 5.37 MAE to 3.21 MAE without\nsacrificing speed.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 15:24:02 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Wang", "Jialiang", ""], ["Jampani", "Varun", ""], ["Sun", "Deqing", ""], ["Loop", "Charles", ""], ["Birchfield", "Stan", ""], ["Kautz", "Jan", ""]]}, {"id": "2008.11104", "submitter": "Malik Aqeel Anwar", "authors": "Aqeel Anwar, Arijit Raychowdhury", "title": "Masked Face Recognition for Secure Authentication", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent world-wide COVID-19 pandemic, using face masks have become an\nimportant part of our lives. People are encouraged to cover their faces when in\npublic area to avoid the spread of infection. The use of these face masks has\nraised a serious question on the accuracy of the facial recognition system used\nfor tracking school/office attendance and to unlock phones. Many organizations\nuse facial recognition as a means of authentication and have already developed\nthe necessary datasets in-house to be able to deploy such a system.\nUnfortunately, masked faces make it difficult to be detected and recognized,\nthereby threatening to make the in-house datasets invalid and making such\nfacial recognition systems inoperable. This paper addresses a methodology to\nuse the current facial datasets by augmenting it with tools that enable masked\nfaces to be recognized with low false-positive rates and high overall accuracy,\nwithout requiring the user dataset to be recreated by taking new pictures for\nauthentication. We present an open-source tool, MaskTheFace to mask faces\neffectively creating a large dataset of masked faces. The dataset generated\nwith this tool is then used towards training an effective facial recognition\nsystem with target accuracy for masked faces. We report an increase of 38% in\nthe true positive rate for the Facenet system. We also test the accuracy of\nre-trained system on a custom real-world dataset MFR2 and report similar\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 15:33:59 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Anwar", "Aqeel", ""], ["Raychowdhury", "Arijit", ""]]}, {"id": "2008.11109", "submitter": "Qiaoying Huang", "authors": "Qiaoying Huang, Eric Z. Chen, Hanchao Yu, Yimo Guo, Terrence Chen,\n  Dimitris Metaxas, Shanhui Sun", "title": "Measure Anatomical Thickness from Cardiac MRI with Deep Neural Networks", "comments": "Accepted by STACOM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate estimation of shape thickness from medical images is crucial in\nclinical applications. For example, the thickness of myocardium is one of the\nkey to cardiac disease diagnosis. While mathematical models are available to\nobtain accurate dense thickness estimation, they suffer from heavy\ncomputational overhead due to iterative solvers. To this end, we propose novel\nmethods for dense thickness estimation, including a fast solver that estimates\nthickness from binary annular shapes and an end-to-end network that estimates\nthickness directly from raw cardiac images.We test the proposed models on three\ncardiac datasets and one synthetic dataset, achieving impressive results and\ngeneralizability on all. Thickness estimation is performed without iterative\nsolvers or manual correction, which is 100 times faster than the mathematical\nmodel. We also analyze thickness patterns on different cardiac pathologies with\na standard clinical model and the results demonstrate the potential clinical\nvalue of our method for thickness based cardiac disease diagnosis.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 15:37:57 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Huang", "Qiaoying", ""], ["Chen", "Eric Z.", ""], ["Yu", "Hanchao", ""], ["Guo", "Yimo", ""], ["Chen", "Terrence", ""], ["Metaxas", "Dimitris", ""], ["Sun", "Shanhui", ""]]}, {"id": "2008.11149", "submitter": "Akshat Gupta", "authors": "Akshat Gupta, Milan Desai, Wusheng Liang, Magesh Kannan", "title": "Spatiotemporal Action Recognition in Restaurant Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatiotemporal action recognition is the task of locating and classifying\nactions in videos. Our project applies this task to analyzing video footage of\nrestaurant workers preparing food, for which potential applications include\nautomated checkout and inventory management. Such videos are quite different\nfrom the standardized datasets that researchers are used to, as they involve\nsmall objects, rapid actions, and notoriously unbalanced data classes. We\nexplore two approaches. The first approach involves the familiar object\ndetector You Only Look Once, and another applying a recently proposed analogue\nfor action recognition, You Only Watch Once. In the first, we design and\nimplement a novel, recurrent modification of YOLO using convolutional LSTMs and\nexplore the various subtleties in the training of such a network. In the\nsecond, we study the ability of YOWOs three dimensional convolutions to capture\nthe spatiotemporal features of our unique dataset\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 16:30:01 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Gupta", "Akshat", ""], ["Desai", "Milan", ""], ["Liang", "Wusheng", ""], ["Kannan", "Magesh", ""]]}, {"id": "2008.11151", "submitter": "Feiyan Hu", "authors": "Feiyan Hu and Kevin McGuinness", "title": "FastSal: a Computationally Efficient Network for Visual Saliency\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the problem of visual saliency prediction, predicting\nregions of an image that tend to attract human visual attention, under a\nconstrained computational budget. We modify and test various recent efficient\nconvolutional neural network architectures like EfficientNet and MobileNetV2\nand compare them with existing state-of-the-art saliency models such as SalGAN\nand DeepGaze II both in terms of standard accuracy metrics like AUC and NSS,\nand in terms of the computational complexity and model size. We find that\nMobileNetV2 makes an excellent backbone for a visual saliency model and can be\neffective even without a complex decoder. We also show that knowledge transfer\nfrom a more computationally expensive model like DeepGaze II can be achieved\nvia pseudo-labelling an unlabelled dataset, and that this approach gives result\non-par with many state-of-the-art algorithms with a fraction of the\ncomputational cost and model size. Source code is available at\nhttps://github.com/feiyanhu/FastSal.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 16:32:33 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Hu", "Feiyan", ""], ["McGuinness", "Kevin", ""]]}, {"id": "2008.11170", "submitter": "Tingting Xie", "authors": "Ting-Ting Xie, Christos Tzelepis, Ioannis Patras", "title": "Boundary Uncertainty in a Single-Stage Temporal Action Localization\n  Network", "comments": "Tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of temporal action localization with a\nsingle stage neural network. In the proposed architecture we model the boundary\npredictions as uni-variate Gaussian distributions in order to model their\nuncertainties, which is the first in this area to the best of our knowledge. We\nuse two uncertainty-aware boundary regression losses: first, the\nKullback-Leibler divergence between the ground truth location of the boundary\nand the Gaussian modeling the prediction of the boundary and second, the\nexpectation of the $\\ell_1$ loss under the same Gaussian. We show that with\nboth uncertainty modeling approaches improve the detection performance by more\nthan $1.5\\%$ in mAP@tIoU=0.5 and that the proposed simple one-stage network\nperforms closely to more complex one and two stage networks.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 17:04:39 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Xie", "Ting-Ting", ""], ["Tzelepis", "Christos", ""], ["Patras", "Ioannis", ""]]}, {"id": "2008.11174", "submitter": "Robin Strudel", "authors": "Robin Strudel, Ricardo Garcia, Justin Carpentier, Jean-Paul Laumond,\n  Ivan Laptev, Cordelia Schmid", "title": "Learning Obstacle Representations for Neural Motion Planning", "comments": "CoRL 2020. See the project webpage at\n  https://www.di.ens.fr/willow/research/nmp_repr/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motion planning and obstacle avoidance is a key challenge in robotics\napplications. While previous work succeeds to provide excellent solutions for\nknown environments, sensor-based motion planning in new and dynamic\nenvironments remains difficult. In this work we address sensor-based motion\nplanning from a learning perspective. Motivated by recent advances in visual\nrecognition, we argue the importance of learning appropriate representations\nfor motion planning. We propose a new obstacle representation based on the\nPointNet architecture and train it jointly with policies for obstacle\navoidance. We experimentally evaluate our approach for rigid body motion\nplanning in challenging environments and demonstrate significant improvements\nof the state of the art in terms of accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 17:12:32 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2020 16:51:26 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2020 15:58:40 GMT"}, {"version": "v4", "created": "Sat, 7 Nov 2020 11:30:09 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Strudel", "Robin", ""], ["Garcia", "Ricardo", ""], ["Carpentier", "Justin", ""], ["Laumond", "Jean-Paul", ""], ["Laptev", "Ivan", ""], ["Schmid", "Cordelia", ""]]}, {"id": "2008.11185", "submitter": "William Thong", "authors": "William Thong and Cees G.M. Snoek", "title": "Bias-Awareness for Zero-Shot Learning the Seen and Unseen", "comments": "Accepted at British Machine Vision Conference (BMVC) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized zero-shot learning recognizes inputs from both seen and unseen\nclasses. Yet, existing methods tend to be biased towards the classes seen\nduring training. In this paper, we strive to mitigate this bias. We propose a\nbias-aware learner to map inputs to a semantic embedding space for generalized\nzero-shot learning. During training, the model learns to regress to real-valued\nclass prototypes in the embedding space with temperature scaling, while a\nmargin-based bidirectional entropy term regularizes seen and unseen\nprobabilities. Relying on a real-valued semantic embedding space provides a\nversatile approach, as the model can operate on different types of semantic\ninformation for both seen and unseen classes. Experiments are carried out on\nfour benchmarks for generalized zero-shot learning and demonstrate the benefits\nof the proposed bias-aware classifier, both as a stand-alone method or in\ncombination with generated features.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 17:38:40 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Thong", "William", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "2008.11200", "submitter": "Omid Taheri", "authors": "Omid Taheri, Nima Ghorbani, Michael J. Black, and Dimitrios Tzionas", "title": "GRAB: A Dataset of Whole-Body Human Grasping of Objects", "comments": "ECCV 2020", "journal-ref": null, "doi": "10.1007/978-3-030-58548-8_34", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training computers to understand, model, and synthesize human grasping\nrequires a rich dataset containing complex 3D object shapes, detailed contact\ninformation, hand pose and shape, and the 3D body motion over time. While\n\"grasping\" is commonly thought of as a single hand stably lifting an object, we\ncapture the motion of the entire body and adopt the generalized notion of\n\"whole-body grasps\". Thus, we collect a new dataset, called GRAB (GRasping\nActions with Bodies), of whole-body grasps, containing full 3D shape and pose\nsequences of 10 subjects interacting with 51 everyday objects of varying shape\nand size. Given MoCap markers, we fit the full 3D body shape and pose,\nincluding the articulated face and hands, as well as the 3D object pose. This\ngives detailed 3D meshes over time, from which we compute contact between the\nbody and object. This is a unique dataset, that goes well beyond existing ones\nfor modeling and understanding how humans grasp and manipulate objects, how\ntheir full body is involved, and how interaction varies with the task. We\nillustrate the practical value of GRAB with an example application; we train\nGrabNet, a conditional generative network, to predict 3D hand grasps for unseen\n3D object shapes. The dataset and code are available for research purposes at\nhttps://grab.is.tue.mpg.de.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 17:57:55 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Taheri", "Omid", ""], ["Ghorbani", "Nima", ""], ["Black", "Michael J.", ""], ["Tzionas", "Dimitrios", ""]]}, {"id": "2008.11201", "submitter": "V\\'it R\\r{u}\\v{z}i\\v{c}ka", "authors": "V\\'it R\\r{u}\\v{z}i\\v{c}ka, Stefano D'Aronco, Jan Dirk Wegner, Konrad\n  Schindler", "title": "Deep Active Learning in Remote Sensing for data efficient Change\n  Detection", "comments": "10 pages, 5 figures, ECML/PKDD Workshop on Machine Learning for Earth\n  Observation, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate active learning in the context of deep neural network models\nfor change detection and map updating. Active learning is a natural choice for\na number of remote sensing tasks, including the detection of local surface\nchanges: changes are on the one hand rare and on the other hand their\nappearance is varied and diffuse, making it hard to collect a representative\ntraining set in advance. In the active learning setting, one starts from a\nminimal set of training examples and progressively chooses informative samples\nthat are annotated by a user and added to the training set. Hence, a core\ncomponent of an active learning system is a mechanism to estimate model\nuncertainty, which is then used to pick uncertain, informative samples. We\nstudy different mechanisms to capture and quantify this uncertainty when\nworking with deep networks, based on the variance or entropy across explicit or\nimplicit model ensembles. We show that active learning successfully finds\nhighly informative samples and automatically balances the training\ndistribution, and reaches the same performance as a model supervised with a\nlarge, pre-annotated training set, with $\\approx$99% fewer annotated samples.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 17:58:17 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["R\u016f\u017ei\u010dka", "V\u00edt", ""], ["D'Aronco", "Stefano", ""], ["Wegner", "Jan Dirk", ""], ["Schindler", "Konrad", ""]]}, {"id": "2008.11203", "submitter": "Yun-Chun Chen", "authors": "Yun-Chun Chen, Chao-Te Chou, Yu-Chiang Frank Wang", "title": "Learning to Learn in a Semi-Supervised Fashion", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address semi-supervised learning from both labeled and unlabeled data, we\npresent a novel meta-learning scheme. We particularly consider that labeled and\nunlabeled data share disjoint ground truth label sets, which can be seen tasks\nlike in person re-identification or image retrieval. Our learning scheme\nexploits the idea of leveraging information from labeled to unlabeled data.\nInstead of fitting the associated class-wise similarity scores as most\nmeta-learning algorithms do, we propose to derive semantics-oriented similarity\nrepresentations from labeled data, and transfer such representation to\nunlabeled ones. Thus, our strategy can be viewed as a self-supervised learning\nscheme, which can be applied to fully supervised learning tasks for improved\nperformance. Our experiments on various tasks and settings confirm the\neffectiveness of our proposed approach and its superiority over the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 17:59:53 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Chen", "Yun-Chun", ""], ["Chou", "Chao-Te", ""], ["Wang", "Yu-Chiang Frank", ""]]}, {"id": "2008.11230", "submitter": "Zhe Jiang", "authors": "Zhe Jiang, Arpan Man Sainju", "title": "Flood Extent Mapping based on High Resolution Aerial Imagery and DEM: A\n  Hidden Markov Tree Approach", "comments": null, "journal-ref": null, "doi": "10.1080/01431161.2020.1823514", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flood extent mapping plays a crucial role in disaster management and national\nwater forecasting. In recent years, high-resolution optical imagery becomes\nincreasingly available with the deployment of numerous small satellites and\ndrones. However, analyzing such imagery data to extract flood extent poses\nunique challenges due to the rich noise and shadows, obstacles (e.g., tree\ncanopies, clouds), and spectral confusion between pixel classes (flood, dry)\ndue to spatial heterogeneity. Existing machine learning techniques often focus\non spectral and spatial features from raster images without fully incorporating\nthe geographic terrain within classification models. In contrast, we recently\nproposed a novel machine learning model called geographical hidden Markov tree\nthat integrates spectral features of pixels and topographic constraints from\nDigital Elevation Model (DEM) data (i.e., water flow directions) in a holistic\nmanner. This paper evaluates the model through case studies on high-resolution\naerial imagery from the National Oceanic and Atmospheric Administration (NOAA)\nNational Geodetic Survey together with DEM. Three scenes are selected in\nheavily vegetated floodplains near the cities of Grimesland and Kinston in\nNorth Carolina during Hurricane Matthew floods in 2016. Results show that the\nproposed hidden Markov tree model outperforms several state of the art machine\nlearning algorithms (e.g., random forests, gradient boosted model) by an\nimprovement of F-score (the harmonic mean of the user's accuracy and producer's\naccuracy) from around 70% to 80% to over 95% on our datasets.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 18:35:28 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 22:40:58 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Jiang", "Zhe", ""], ["Sainju", "Arpan Man", ""]]}, {"id": "2008.11239", "submitter": "Federica Bogo", "authors": "Dorin Ungureanu, Federica Bogo, Silvano Galliani, Pooja Sama, Xin\n  Duan, Casey Meekhof, Jan St\\\"uhmer, Thomas J. Cashman, Bugra Tekin, Johannes\n  L. Sch\\\"onberger, Pawel Olszta, Marc Pollefeys", "title": "HoloLens 2 Research Mode as a Tool for Computer Vision Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed reality headsets, such as the Microsoft HoloLens 2, are powerful\nsensing devices with integrated compute capabilities, which makes it an ideal\nplatform for computer vision research. In this technical report, we present\nHoloLens 2 Research Mode, an API and a set of tools enabling access to the raw\nsensor streams. We provide an overview of the API and explain how it can be\nused to build mixed reality applications based on processing sensor data. We\nalso show how to combine the Research Mode sensor data with the built-in eye\nand hand tracking capabilities provided by HoloLens 2. By releasing the\nResearch Mode API and a set of open-source tools, we aim to foster further\nresearch in the fields of computer vision as well as robotics and encourage\ncontributions from the research community.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 19:05:38 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Ungureanu", "Dorin", ""], ["Bogo", "Federica", ""], ["Galliani", "Silvano", ""], ["Sama", "Pooja", ""], ["Duan", "Xin", ""], ["Meekhof", "Casey", ""], ["St\u00fchmer", "Jan", ""], ["Cashman", "Thomas J.", ""], ["Tekin", "Bugra", ""], ["Sch\u00f6nberger", "Johannes L.", ""], ["Olszta", "Pawel", ""], ["Pollefeys", "Marc", ""]]}, {"id": "2008.11254", "submitter": "Tingting Xie", "authors": "Ting-Ting Xie, Christos Tzelepis, Ioannis Patras", "title": "Temporal Action Localization with Variance-Aware Networks", "comments": "Journal paper; Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the problem of temporal action localization with\nVariance-Aware Networks (VAN), i.e., DNNs that use second-order statistics in\nthe input and/or the output of regression tasks. We first propose a network\n(VANp) that when presented with the second-order statistics of the input, i.e.,\neach sample has a mean and a variance, it propagates the mean and the variance\nthroughout the network to deliver outputs with second order statistics. In this\nframework, both the input and the output could be interpreted as Gaussians. To\ndo so, we derive differentiable analytic solutions, or reasonable\napproximations, to propagate across commonly used NN layers. To train the\nnetwork, we define a differentiable loss based on the KL-divergence between the\npredicted Gaussian and a Gaussian around the ground truth action borders, and\nuse standard back-propagation. Importantly, the variances propagation in VANp\ndoes not require any additional parameters, and during testing, does not\nrequire any additional computations either. In action localization, the means\nand the variances of the input are computed at pooling operations, that are\ntypically used to bring arbitrarily long videos to a vector with fixed\ndimensions. Second, we propose two alternative formulations that augment the\nfirst (respectively, the last) layer of a regression network with additional\nparameters so as to take in the input (respectively, predict in the output)\nboth means and variances. Results in the action localization problem show that\nthe incorporation of second order statistics improves over the baseline\nnetwork, and that VANp surpasses the accuracy of virtually all other two-stage\nnetworks without involving any additional parameters.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 20:12:59 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Xie", "Ting-Ting", ""], ["Tzelepis", "Christos", ""], ["Patras", "Ioannis", ""]]}, {"id": "2008.11269", "submitter": "Zhe Jiang", "authors": "Wenchong He, Arpan Man Sainju, Zhe Jiang and Da Yan", "title": "Deep Neural Network for 3D Surface Segmentation based on Contour Tree\n  Hierarchy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a 3D surface defined by an elevation function on a 2D grid as well as\nnon-spatial features observed at each pixel, the problem of surface\nsegmentation aims to classify pixels into contiguous classes based on both\nnon-spatial features and surface topology. The problem has important\napplications in hydrology, planetary science, and biochemistry but is uniquely\nchallenging for several reasons. First, the spatial extent of class segments\nfollows surface contours in the topological space, regardless of their spatial\nshapes and directions. Second, the topological structure exists in multiple\nspatial scales based on different surface resolutions. Existing widely\nsuccessful deep learning models for image segmentation are often not applicable\ndue to their reliance on convolution and pooling operations to learn regular\nstructural patterns on a grid. In contrast, we propose to represent surface\ntopological structure by a contour tree skeleton, which is a polytree capturing\nthe evolution of surface contours at different elevation levels. We further\ndesign a graph neural network based on the contour tree hierarchy to model\nsurface topological structure at different spatial scales. Experimental\nevaluations based on real-world hydrological datasets show that our model\noutperforms several baseline methods in classification accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 20:54:27 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["He", "Wenchong", ""], ["Sainju", "Arpan Man", ""], ["Jiang", "Zhe", ""], ["Yan", "Da", ""]]}, {"id": "2008.11289", "submitter": "Rajat Hebbar", "authors": "Krishna Somandepalli, Rajat Hebbar, Shrikanth Narayanan", "title": "Multi-Face: Self-supervised Multiview Adaptation for Robust Face\n  Clustering in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust face clustering is a key step towards computational understanding of\nvisual character portrayals in media. Face clustering for long-form content\nsuch as movies is challenging because of variations in appearance and lack of\nlarge-scale labeled video resources. However, local face tracking in videos can\nbe used to mine samples belonging to same/different persons by examining the\nfaces co-occurring in a video frame. In this work, we use this idea of\nself-supervision to harvest large amounts of weakly labeled face tracks in\nmovies. We propose a nearest-neighbor search in the embedding space to mine\nhard examples from the face tracks followed by domain adaptation using\nmultiview shared subspace learning. Our benchmarking on movie datasets\ndemonstrate the robustness of multiview adaptation for face verification and\nclustering. We hope that the large-scale data resources developed in this work\ncan further advance automatic character labeling in videos.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 22:07:41 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Somandepalli", "Krishna", ""], ["Hebbar", "Rajat", ""], ["Narayanan", "Shrikanth", ""]]}, {"id": "2008.11297", "submitter": "Malik Boudiaf", "authors": "Malik Boudiaf, Ziko Imtiaz Masud, J\\'er\\^ome Rony, Jos\\'e Dolz, Pablo\n  Piantanida, Ismail Ben Ayed", "title": "Transductive Information Maximization For Few-Shot Learning", "comments": "NeurIPS 2020. Code available at https://github.com/mboudiaf/TIM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Transductive Infomation Maximization (TIM) for few-shot\nlearning. Our method maximizes the mutual information between the query\nfeatures and their label predictions for a given few-shot task, in conjunction\nwith a supervision loss based on the support set. Furthermore, we propose a new\nalternating-direction solver for our mutual-information loss, which\nsubstantially speeds up transductive-inference convergence over gradient-based\noptimization, while yielding similar accuracy. TIM inference is modular: it can\nbe used on top of any base-training feature extractor. Following standard\ntransductive few-shot settings, our comprehensive experiments demonstrate that\nTIM outperforms state-of-the-art methods significantly across various datasets\nand networks, while used on top of a fixed feature extractor trained with\nsimple cross-entropy on the base classes, without resorting to complex\nmeta-learning schemes. It consistently brings between 2% and 5% improvement in\naccuracy over the best performing method, not only on all the well-established\nfew-shot benchmarks but also on more challenging scenarios,with domain shifts\nand larger numbers of classes.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 22:38:41 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 02:43:51 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 17:36:19 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Boudiaf", "Malik", ""], ["Masud", "Ziko Imtiaz", ""], ["Rony", "J\u00e9r\u00f4me", ""], ["Dolz", "Jos\u00e9", ""], ["Piantanida", "Pablo", ""], ["Ayed", "Ismail Ben", ""]]}, {"id": "2008.11300", "submitter": "Fu Lin", "authors": "Fu Lin, Rohit Mittapalli, Prithvijit Chattopadhyay, Daniel Bolya, Judy\n  Hoffman", "title": "Likelihood Landscapes: A Unifying Principle Behind Many Adversarial\n  Defenses", "comments": "ECCV 2020 Workshop on Adversarial Robustness in the Real World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks have been shown to be vulnerable to adversarial\nexamples, which are known to locate in subspaces close to where normal data\nlies but are not naturally occurring and of low probability. In this work, we\ninvestigate the potential effect defense techniques have on the geometry of the\nlikelihood landscape - likelihood of the input images under the trained model.\nWe first propose a way to visualize the likelihood landscape leveraging an\nenergy-based model interpretation of discriminative classifiers. Then we\nintroduce a measure to quantify the flatness of the likelihood landscape. We\nobserve that a subset of adversarial defense techniques results in a similar\neffect of flattening the likelihood landscape. We further explore directly\nregularizing towards a flat landscape for adversarial robustness.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 22:51:51 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Lin", "Fu", ""], ["Mittapalli", "Rohit", ""], ["Chattopadhyay", "Prithvijit", ""], ["Bolya", "Daniel", ""], ["Hoffman", "Judy", ""]]}, {"id": "2008.11331", "submitter": "Yuan Xue", "authors": "Jiarong Ye, Yuan Xue, L. Rodney Long, Sameer Antani, Zhiyun Xue, Keith\n  Cheng, Xiaolei Huang", "title": "Synthetic Sample Selection via Reinforcement Learning", "comments": "MICCAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing realistic medical images provides a feasible solution to the\nshortage of training data in deep learning based medical image recognition\nsystems. However, the quality control of synthetic images for data augmentation\npurposes is under-investigated, and some of the generated images are not\nrealistic and may contain misleading features that distort data distribution\nwhen mixed with real images. Thus, the effectiveness of those synthetic images\nin medical image recognition systems cannot be guaranteed when they are being\nadded randomly without quality assurance. In this work, we propose a\nreinforcement learning (RL) based synthetic sample selection method that learns\nto choose synthetic images containing reliable and informative features. A\ntransformer based controller is trained via proximal policy optimization (PPO)\nusing the validation classification accuracy as the reward. The selected images\nare mixed with the original training data for improved training of image\nrecognition systems. To validate our method, we take the pathology image\nrecognition as an example and conduct extensive experiments on two\nhistopathology image datasets. In experiments on a cervical dataset and a lymph\nnode dataset, the image classification performance is improved by 8.1% and\n2.3%, respectively, when utilizing high-quality synthetic images selected by\nour RL framework. Our proposed synthetic sample selection method is general and\nhas great potential to boost the performance of various medical image\nrecognition systems given limited annotation.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 01:34:19 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Ye", "Jiarong", ""], ["Xue", "Yuan", ""], ["Long", "L. Rodney", ""], ["Antani", "Sameer", ""], ["Xue", "Zhiyun", ""], ["Cheng", "Keith", ""], ["Huang", "Xiaolei", ""]]}, {"id": "2008.11351", "submitter": "Rui Fan", "authors": "Rui Fan, Hengli Wang, Peide Cai, Ming Liu", "title": "SNE-RoadSeg: Incorporating Surface Normal Information into Semantic\n  Segmentation for Accurate Freespace Detection", "comments": "ECCV 2020", "journal-ref": null, "doi": "10.1007/978-3-030-58577-8_21", "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Freespace detection is an essential component of visual perception for\nself-driving cars. The recent efforts made in data-fusion convolutional neural\nnetworks (CNNs) have significantly improved semantic driving scene\nsegmentation. Freespace can be hypothesized as a ground plane, on which the\npoints have similar surface normals. Hence, in this paper, we first introduce a\nnovel module, named surface normal estimator (SNE), which can infer surface\nnormal information from dense depth/disparity images with high accuracy and\nefficiency. Furthermore, we propose a data-fusion CNN architecture, referred to\nas RoadSeg, which can extract and fuse features from both RGB images and the\ninferred surface normal information for accurate freespace detection. For\nresearch purposes, we publish a large-scale synthetic freespace detection\ndataset, named Ready-to-Drive (R2D) road dataset, collected under different\nillumination and weather conditions. The experimental results demonstrate that\nour proposed SNE module can benefit all the state-of-the-art CNNs for freespace\ndetection, and our SNE-RoadSeg achieves the best overall performance among\ndifferent datasets.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 02:43:25 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Fan", "Rui", ""], ["Wang", "Hengli", ""], ["Cai", "Peide", ""], ["Liu", "Ming", ""]]}, {"id": "2008.11353", "submitter": "Shan Jia", "authors": "Shan Jia, Shuo Wang, Chuanbo Hu, Paula Webster, Xin Li", "title": "Detection of Genuine and Posed Facial Expressions of Emotion: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expressions of emotion play an important role in human social\ninteractions. However, posed acting is not always the same as genuine feeling.\nTherefore, the credibility assessment of facial expressions, namely, the\ndiscrimination of genuine (spontaneous) expressions from\nposed(deliberate/volitional/deceptive) ones, is a crucial yet challenging task\nin facial expression understanding. Rapid progress has been made in recent\nyears for automatic detection of genuine and posed facial expressions. This\npaper presents a general review of the relevant research, including several\nspontaneous vs. posed (SVP) facial expression databases and various computer\nvision based detection methods. In addition, a variety of factors that will\ninfluence the performance of SVP detection methods are discussed along with\nopen issues and technical challenges.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 02:49:32 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Jia", "Shan", ""], ["Wang", "Shuo", ""], ["Hu", "Chuanbo", ""], ["Webster", "Paula", ""], ["Li", "Xin", ""]]}, {"id": "2008.11354", "submitter": "James Tompkin", "authors": "Atsunobu Kotani, Stefanie Tellex, James Tompkin", "title": "Generating Handwriting via Decoupled Style Descriptors", "comments": "European Conference on Computer Vision (ECCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing a space of handwriting stroke styles includes the challenge of\nrepresenting both the style of each character and the overall style of the\nhuman writer. Existing VRNN approaches to representing handwriting often do not\ndistinguish between these different style components, which can reduce model\ncapability. Instead, we introduce the Decoupled Style Descriptor (DSD) model\nfor handwriting, which factors both character- and writer-level styles and\nallows our model to represent an overall greater space of styles. This approach\nalso increases flexibility: given a few examples, we can generate handwriting\nin new writer styles, and also now generate handwriting of new characters\nacross writer styles. In experiments, our generated results were preferred over\na state of the art baseline method 88% of the time, and in a writer\nidentification task on 20 held-out writers, our DSDs achieved 89.38% accuracy\nfrom a single sample word. Overall, DSDs allows us to improve both the quality\nand flexibility over existing handwriting stroke generation approaches.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 02:52:48 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 22:50:41 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Kotani", "Atsunobu", ""], ["Tellex", "Stefanie", ""], ["Tompkin", "James", ""]]}, {"id": "2008.11360", "submitter": "Taotao Jing", "authors": "Taotao Jing, Ming Shao, Zhengming Ding", "title": "Discriminative Cross-Domain Feature Learning for Partial Domain\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial domain adaptation aims to adapt knowledge from a larger and more\ndiverse source domain to a smaller target domain with less number of classes,\nwhich has attracted appealing attention. Recent practice on domain adaptation\nmanages to extract effective features by incorporating the pseudo labels for\nthe target domain to better fight off the cross-domain distribution\ndivergences. However, it is essential to align target data with only a small\nset of source data. In this paper, we develop a novel Discriminative\nCross-Domain Feature Learning (DCDF) framework to iteratively optimize target\nlabels with a cross-domain graph in a weighted scheme. Specifically, a weighted\ncross-domain center loss and weighted cross-domain graph propagation are\nproposed to couple unlabeled target data to related source samples for\ndiscriminative cross-domain feature learning, where irrelevant source centers\nwill be ignored, to alleviate the marginal and conditional disparities\nsimultaneously. Experimental evaluations on several popular benchmarks\ndemonstrate the effectiveness of our proposed approach on facilitating the\nrecognition for the unlabeled target domain, through comparing it to the\nstate-of-the-art partial domain adaptation approaches.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 03:18:53 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Jing", "Taotao", ""], ["Shao", "Ming", ""], ["Ding", "Zhengming", ""]]}, {"id": "2008.11363", "submitter": "Ilke Demir", "authors": "Umur Aybars Ciftci and Ilke Demir and Lijun Yin", "title": "How Do the Hearts of Deep Fakes Beat? Deep Fake Source Detection via\n  Interpreting Residuals with Biological Signals", "comments": "To be published in the proceedings of 2020 IEEE/IAPR International\n  Joint Conference on Biometrics (IJCB)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fake portrait video generation techniques have been posing a new threat to\nthe society with photorealistic deep fakes for political propaganda, celebrity\nimitation, forged evidences, and other identity related manipulations.\nFollowing these generation techniques, some detection approaches have also been\nproved useful due to their high classification accuracy. Nevertheless, almost\nno effort was spent to track down the source of deep fakes. We propose an\napproach not only to separate deep fakes from real videos, but also to discover\nthe specific generative model behind a deep fake. Some pure deep learning based\napproaches try to classify deep fakes using CNNs where they actually learn the\nresiduals of the generator. We believe that these residuals contain more\ninformation and we can reveal these manipulation artifacts by disentangling\nthem with biological signals. Our key observation yields that the\nspatiotemporal patterns in biological signals can be conceived as a\nrepresentative projection of residuals. To justify this observation, we extract\nPPG cells from real and fake videos and feed these to a state-of-the-art\nclassification network for detecting the generative model per video. Our\nresults indicate that our approach can detect fake videos with 97.29% accuracy,\nand the source model with 93.39% accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 03:35:47 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Ciftci", "Umur Aybars", ""], ["Demir", "Ilke", ""], ["Yin", "Lijun", ""]]}, {"id": "2008.11368", "submitter": "Olga Moskvyak", "authors": "Olga Moskvyak, Frederic Maire, Feras Dayoub and Mahsa Baktashmotlagh", "title": "Keypoint-Aligned Embeddings for Image Retrieval and Re-identification", "comments": "8 pages, 7 figures, accepted to WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning embeddings that are invariant to the pose of the object is crucial\nin visual image retrieval and re-identification. The existing approaches for\nperson, vehicle, or animal re-identification tasks suffer from high intra-class\nvariance due to deformable shapes and different camera viewpoints. To overcome\nthis limitation, we propose to align the image embedding with a predefined\norder of the keypoints. The proposed keypoint aligned embeddings model\n(KAE-Net) learns part-level features via multi-task learning which is guided by\nkeypoint locations. More specifically, KAE-Net extracts channels from a feature\nmap activated by a specific keypoint through learning the auxiliary task of\nheatmap reconstruction for this keypoint. The KAE-Net is compact, generic and\nconceptually simple. It achieves state of the art performance on the benchmark\ndatasets of CUB-200-2011, Cars196 and VeRi-776 for retrieval and\nre-identification tasks.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 03:56:37 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Moskvyak", "Olga", ""], ["Maire", "Frederic", ""], ["Dayoub", "Feras", ""], ["Baktashmotlagh", "Mahsa", ""]]}, {"id": "2008.11378", "submitter": "Yuecong Xu", "authors": "Haozhi Cao, Yuecong Xu, Jianfei Yang, Kezhi Mao, Jianxiong Yin and\n  Simon See", "title": "Effective Action Recognition with Embedded Key Point Shifts", "comments": "35 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal feature extraction is an essential technique in video-based action\nrecognition. Key points have been utilized in skeleton-based action recognition\nmethods but they require costly key point annotation. In this paper, we propose\na novel temporal feature extraction module, named Key Point Shifts Embedding\nModule ($KPSEM$), to adaptively extract channel-wise key point shifts across\nvideo frames without key point annotation for temporal feature extraction. Key\npoints are adaptively extracted as feature points with maximum feature values\nat split regions, while key point shifts are the spatial displacements of\ncorresponding key points. The key point shifts are encoded as the overall\ntemporal features via linear embedding layers in a multi-set manner. Our method\nachieves competitive performance through embedding key point shifts with\ntrivial computational cost, achieving the state-of-the-art performance of\n82.05% on Mini-Kinetics and competitive performance on UCF101,\nSomething-Something-v1, and HMDB51 datasets.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 05:19:04 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Cao", "Haozhi", ""], ["Xu", "Yuecong", ""], ["Yang", "Jianfei", ""], ["Mao", "Kezhi", ""], ["Yin", "Jianxiong", ""], ["See", "Simon", ""]]}, {"id": "2008.11383", "submitter": "Hengli Wang", "authors": "Hengli Wang, Rui Fan, Yuxiang Sun, Ming Liu", "title": "Applying Surface Normal Information in Drivable Area and Road Anomaly\n  Detection for Ground Mobile Robots", "comments": "6 pages, 6 figures and 1 table. This paper is accepted by IROS 2020", "journal-ref": null, "doi": "10.1109/IROS45743.2020.9341340", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The joint detection of drivable areas and road anomalies is a crucial task\nfor ground mobile robots. In recent years, many impressive semantic\nsegmentation networks, which can be used for pixel-level drivable area and road\nanomaly detection, have been developed. However, the detection accuracy still\nneeds improvement. Therefore, we develop a novel module named the Normal\nInference Module (NIM), which can generate surface normal information from\ndense depth images with high accuracy and efficiency. Our NIM can be deployed\nin existing convolutional neural networks (CNNs) to refine the segmentation\nperformance. To evaluate the effectiveness and robustness of our NIM, we embed\nit in twelve state-of-the-art CNNs. The experimental results illustrate that\nour NIM can greatly improve the performance of the CNNs for drivable area and\nroad anomaly detection. Furthermore, our proposed NIM-RTFNet ranks 8th on the\nKITTI road benchmark and exhibits a real-time inference speed.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 05:44:07 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Wang", "Hengli", ""], ["Fan", "Rui", ""], ["Sun", "Yuxiang", ""], ["Liu", "Ming", ""]]}, {"id": "2008.11401", "submitter": "Ping Liu", "authors": "Ping Liu, Yuewei Lin, Zibo Meng, Lu Lu, Weihong Deng, Joey Tianyi\n  Zhou, and Yi Yang", "title": "Point Adversarial Self Mining: A Simple Method for Facial Expression\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a simple yet effective approach, named Point\nAdversarial Self Mining (PASM), to improve the recognition accuracy in facial\nexpression recognition. Unlike previous works focusing on designing specific\narchitectures or loss functions to solve this problem, PASM boosts the network\ncapability by simulating human learning processes: providing updated learning\nmaterials and guidance from more capable teachers. Specifically, to generate\nnew learning materials, PASM leverages a point adversarial attack method and a\ntrained teacher network to locate the most informative position related to the\ntarget task, generating harder learning samples to refine the network. The\nsearched position is highly adaptive since it considers both the statistical\ninformation of each sample and the teacher network capability. Other than being\nprovided new learning materials, the student network also receives guidance\nfrom the teacher network. After the student network finishes training, the\nstudent network changes its role and acts as a teacher, generating new learning\nmaterials and providing stronger guidance to train a better student network.\nThe adaptive learning materials generation and teacher/student update can be\nconducted more than one time, improving the network capability iteratively.\nExtensive experimental results validate the efficacy of our method over the\nexisting state of the arts for facial expression recognition.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 06:39:24 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 10:26:16 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Liu", "Ping", ""], ["Lin", "Yuewei", ""], ["Meng", "Zibo", ""], ["Lu", "Lu", ""], ["Deng", "Weihong", ""], ["Zhou", "Joey Tianyi", ""], ["Yang", "Yi", ""]]}, {"id": "2008.11423", "submitter": "Tsai-Shien Chen", "authors": "Tsai-Shien Chen, Chih-Ting Liu, Chih-Wei Wu, Shao-Yi Chien", "title": "Orientation-aware Vehicle Re-identification with Semantics-guided Part\n  Attention Network", "comments": "ECCV 2020 (Oral). Paper Website:\n  http://media.ee.ntu.edu.tw/research/SPAN/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle re-identification (re-ID) focuses on matching images of the same\nvehicle across different cameras. It is fundamentally challenging because\ndifferences between vehicles are sometimes subtle. While several studies\nincorporate spatial-attention mechanisms to help vehicle re-ID, they often\nrequire expensive keypoint labels or suffer from noisy attention mask if not\ntrained with expensive labels. In this work, we propose a dedicated\nSemantics-guided Part Attention Network (SPAN) to robustly predict part\nattention masks for different views of vehicles given only image-level semantic\nlabels during training. With the help of part attention masks, we can extract\ndiscriminative features in each part separately. Then we introduce\nCo-occurrence Part-attentive Distance Metric (CPDM) which places greater\nemphasis on co-occurrence vehicle parts when evaluating the feature distance of\ntwo images. Extensive experiments validate the effectiveness of the proposed\nmethod and show that our framework outperforms the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 07:33:09 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 11:51:16 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Chen", "Tsai-Shien", ""], ["Liu", "Chih-Ting", ""], ["Wu", "Chih-Wei", ""], ["Chien", "Shao-Yi", ""]]}, {"id": "2008.11434", "submitter": "Yu Zhang", "authors": "Yu Zhang, Xiaoguang Di, Bin Zhang, Ruihang Ji, and Chunhui Wang", "title": "Better Than Reference In Low Light Image Enhancement: Conditional\n  Re-Enhancement Networks", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low light images suffer from severe noise, low brightness, low contrast, etc.\nIn previous researches, many image enhancement methods have been proposed, but\nfew methods can deal with these problems simultaneously. In this paper, to\nsolve these problems simultaneously, we propose a low light image enhancement\nmethod that can combined with supervised learning and previous HSV (Hue,\nSaturation, Value) or Retinex model based image enhancement methods. First, we\nanalyse the relationship between the HSV color space and the Retinex theory,\nand show that the V channel (V channel in HSV color space, equals the maximum\nchannel in RGB color space) of the enhanced image can well represent the\ncontrast and brightness enhancement process. Then, a data-driven conditional\nre-enhancement network (denoted as CRENet) is proposed. The network takes low\nlight images as input and the enhanced V channel as condition, then it can\nre-enhance the contrast and brightness of the low light image and at the same\ntime reduce noise and color distortion. It should be noted that during the\ntraining process, any paired images with different exposure time can be used\nfor training, and there is no need to carefully select the supervised images\nwhich will save a lot. In addition, it takes less than 20 ms to process a color\nimage with the resolution 400*600 on a 2080Ti GPU. Finally, some comparative\nexperiments are implemented to prove the effectiveness of the method. The\nresults show that the method proposed in this paper can significantly improve\nthe quality of the enhanced image, and by combining with other image contrast\nenhancement methods, the final enhancement result can even be better than the\nreference image in contrast and brightness. (Code will be available at\nhttps://github.com/hitzhangyu/image-enhancement-with-denoise)\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 08:10:48 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Zhang", "Yu", ""], ["Di", "Xiaoguang", ""], ["Zhang", "Bin", ""], ["Ji", "Ruihang", ""], ["Wang", "Chunhui", ""]]}, {"id": "2008.11440", "submitter": "Sungho Suh", "authors": "Sungho Suh, Paul Lukowicz and Yong Oh Lee", "title": "Fusion of Global-Local Features for Image Quality Inspection of Shipping\n  Label", "comments": "Accepted at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demands of automated shipping address recognition and verification have\nincreased to handle a large number of packages and to save costs associated\nwith misdelivery. A previous study proposed a deep learning system where the\nshipping address is recognized and verified based on a camera image capturing\nthe shipping address and barcode area. Because the system performance depends\non the input image quality, inspection of input image quality is necessary for\nimage preprocessing. In this paper, we propose an input image quality\nverification method combining global and local features. Object detection and\nscale-invariant feature transform in different feature spaces are developed to\nextract global and local features from several independent convolutional neural\nnetworks. The conditions of shipping label images are classified by fully\nconnected fusion layers with concatenated global and local features. The\nexperimental results regarding real captured and generated images show that the\nproposed method achieves better performance than other methods. These results\nare expected to improve the shipping address recognition and verification\nsystem by applying different image preprocessing steps based on the classified\nconditions.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 08:25:34 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Suh", "Sungho", ""], ["Lukowicz", "Paul", ""], ["Lee", "Yong Oh", ""]]}, {"id": "2008.11449", "submitter": "Qingyan Sun", "authors": "Qingyan Sun, Shuo Zhang, Song Chang, Lixi Zhu and Youfang Lin", "title": "Multi-Dimension Fusion Network for Light Field Spatial Super-Resolution\n  using Dynamic Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field cameras have been proved to be powerful tools for 3D\nreconstruction and virtual reality applications. However, the limited\nresolution of light field images brings a lot of difficulties for further\ninformation display and extraction. In this paper, we introduce a novel\nlearning-based framework to improve the spatial resolution of light fields.\nFirst, features from different dimensions are parallelly extracted and fused\ntogether in our multi-dimension fusion architecture. These features are then\nused to generate dynamic filters, which extract subpixel information from\nmicro-lens images and also implicitly consider the disparity information.\nFinally, more high-frequency details learned in the residual branch are added\nto the upsampled images and the final super-resolved light fields are obtained.\nExperimental results show that the proposed method uses fewer parameters but\nachieves better performances than other state-of-the-art methods in various\nkinds of datasets. Our reconstructed images also show sharp details and\ndistinct lines in both sub-aperture images and epipolar plane images.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 09:05:07 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Sun", "Qingyan", ""], ["Zhang", "Shuo", ""], ["Chang", "Song", ""], ["Zhu", "Lixi", ""], ["Lin", "Youfang", ""]]}, {"id": "2008.11451", "submitter": "Samik Some", "authors": "Samik Some, Mithun Das Gupta, Vinay P. Namboodiri", "title": "Determinantal Point Process as an alternative to NMS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a determinantal point process (DPP) inspired alternative to\nnon-maximum suppression (NMS) which has become an integral step in all\nstate-of-the-art object detection frameworks. DPPs have been shown to encourage\ndiversity in subset selection problems. We pose NMS as a subset selection\nproblem and posit that directly incorporating DPP like framework can improve\nthe overall performance of the object detection system. We propose an\noptimization problem which takes the same inputs as NMS, but introduces a novel\nsub-modularity based diverse subset selection functional. Our results strongly\nindicate that the modifications proposed in this paper can provide consistent\nimprovements to state-of-the-art object detection pipelines.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 09:06:11 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Some", "Samik", ""], ["Gupta", "Mithun Das", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "2008.11459", "submitter": "Xin Kong", "authors": "Xin Kong, Xuemeng Yang, Guangyao Zhai, Xiangrui Zhao, Xianfang Zeng,\n  Mengmeng Wang, Yong Liu, Wanlong Li, Feng Wen", "title": "Semantic Graph Based Place Recognition for 3D Point Clouds", "comments": "8 pages. Accpeted by IROS-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the difficulty in generating the effective descriptors which are\nrobust to occlusion and viewpoint changes, place recognition for 3D point cloud\nremains an open issue. Unlike most of the existing methods that focus on\nextracting local, global, and statistical features of raw point clouds, our\nmethod aims at the semantic level that can be superior in terms of robustness\nto environmental changes. Inspired by the perspective of humans, who recognize\nscenes through identifying semantic objects and capturing their relations, this\npaper presents a novel semantic graph based approach for place recognition.\nFirst, we propose a novel semantic graph representation for the point cloud\nscenes by reserving the semantic and topological information of the raw point\ncloud. Thus, place recognition is modeled as a graph matching problem. Then we\ndesign a fast and effective graph similarity network to compute the similarity.\nExhaustive evaluations on the KITTI dataset show that our approach is robust to\nthe occlusion as well as viewpoint changes and outperforms the state-of-the-art\nmethods with a large margin. Our code is available at:\n\\url{https://github.com/kxhit/SG_PR}.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 09:27:26 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Kong", "Xin", ""], ["Yang", "Xuemeng", ""], ["Zhai", "Guangyao", ""], ["Zhao", "Xiangrui", ""], ["Zeng", "Xianfang", ""], ["Wang", "Mengmeng", ""], ["Liu", "Yong", ""], ["Li", "Wanlong", ""], ["Wen", "Feng", ""]]}, {"id": "2008.11469", "submitter": "Qi Fang", "authors": "Jianan Zhen, Qi Fang, Jiaming Sun, Wentao Liu, Wei Jiang, Hujun Bao,\n  Xiaowei Zhou", "title": "SMAP: Single-Shot Multi-Person Absolute 3D Pose Estimation", "comments": "ECCV 2020, project page: https://zju3dv.github.io/SMAP/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering multi-person 3D poses with absolute scales from a single RGB image\nis a challenging problem due to the inherent depth and scale ambiguity from a\nsingle view. Addressing this ambiguity requires to aggregate various cues over\nthe entire image, such as body sizes, scene layouts, and inter-person\nrelationships. However, most previous methods adopt a top-down scheme that\nfirst performs 2D pose detection and then regresses the 3D pose and scale for\neach detected person individually, ignoring global contextual cues. In this\npaper, we propose a novel system that first regresses a set of 2.5D\nrepresentations of body parts and then reconstructs the 3D absolute poses based\non these 2.5D representations with a depth-aware part association algorithm.\nSuch a single-shot bottom-up scheme allows the system to better learn and\nreason about the inter-person depth relationship, improving both 3D and 2D pose\nestimation. The experiments demonstrate that the proposed approach achieves the\nstate-of-the-art performance on the CMU Panoptic and MuPoTS-3D datasets and is\napplicable to in-the-wild videos.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 09:56:07 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Zhen", "Jianan", ""], ["Fang", "Qi", ""], ["Sun", "Jiaming", ""], ["Liu", "Wentao", ""], ["Jiang", "Wei", ""], ["Bao", "Hujun", ""], ["Zhou", "Xiaowei", ""]]}, {"id": "2008.11476", "submitter": "M. Akif \\\"Ozkan", "authors": "M. Akif \\\"Ozkan, Burak Ok, Bo Qiao, J\\\"urgen Teich, Frank Hannig", "title": "HipaccVX: Wedding of OpenVX and DSL-based Code Generation", "comments": null, "journal-ref": "Journal of Real-Time Image Processing, 2020", "doi": "10.1007/s11554-020-01015-5", "report-no": null, "categories": "cs.CV cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Writing programs for heterogeneous platforms optimized for high performance\nis hard since this requires the code to be tuned at a low level with\narchitecture-specific optimizations that are most times based on fundamentally\ndiffering programming paradigms and languages. OpenVX promises to solve this\nissue for computer vision applications with a royalty-free industry standard\nthat is based on a graph-execution model. Yet, the OpenVX' algorithm space is\nconstrained to a small set of vision functions. This hinders accelerating\ncomputations that are not included in the standard.\n  In this paper, we analyze OpenVX vision functions to find an orthogonal set\nof computational abstractions. Based on these abstractions, we couple an\nexisting Domain-Specific Language (DSL) back end to the OpenVX environment and\nprovide language constructs to the programmer for the definition of\nuser-defined nodes. In this way, we enable optimizations that are not possible\nto detect with OpenVX graph implementations using the standard computer vision\nfunctions. These optimizations can double the throughput on an Nvidia GTX GPU\nand decrease the resource usage of a Xilinx Zynq FPGA by 50% for our\nbenchmarks. Finally, we show that our proposed compiler framework, called\nHipaccVX, can achieve better results than the state-of-the-art approaches\nNvidia VisionWorks and Halide-HLS.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 10:30:18 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["\u00d6zkan", "M. Akif", ""], ["Ok", "Burak", ""], ["Qiao", "Bo", ""], ["Teich", "J\u00fcrgen", ""], ["Hannig", "Frank", ""]]}, {"id": "2008.11478", "submitter": "Pengyi Zhang", "authors": "Pengyi Zhang, Yunxin Zhong, Yulin Deng, Xiaoying Tang, Xiaoqiong Li", "title": "DRR4Covid: Learning Automated COVID-19 Infection Segmentation from\n  Digitally Reconstructed Radiographs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automated infection measurement and COVID-19 diagnosis based on Chest X-ray\n(CXR) imaging is important for faster examination. We propose a novel approach,\ncalled DRR4Covid, to learn automated COVID-19 diagnosis and infection\nsegmentation on CXRs from digitally reconstructed radiographs (DRRs). DRR4Covid\ncomprises of an infection-aware DRR generator, a classification and/or\nsegmentation network, and a domain adaptation module. The infection-aware DRR\ngenerator is able to produce DRRs with adjustable strength of radiological\nsigns of COVID-19 infection, and generate pixel-level infection annotations\nthat match the DRRs precisely. The domain adaptation module is introduced to\nreduce the domain discrepancy between DRRs and CXRs by training networks on\nunlabeled real CXRs and labeled DRRs together.We provide a simple but effective\nimplementation of DRR4Covid by using a domain adaptation module based on\nMaximum Mean Discrepancy (MMD), and a FCN-based network with a classification\nheader and a segmentation header. Extensive experiment results have confirmed\nthe efficacy of our method; specifically, quantifying the performance by\naccuracy, AUC and F1-score, our network without using any annotations from CXRs\nhas achieved a classification score of (0.954, 0.989, 0.953) and a segmentation\nscore of (0.957, 0.981, 0.956) on a test set with 794 normal cases and 794\npositive cases. Besides, we estimate the sensitive of X-ray images in detecting\nCOVID-19 infection by adjusting the strength of radiological signs of COVID-19\ninfection in synthetic DRRs. The estimated detection limit of the proportion of\ninfected voxels in the lungs is 19.43%, and the estimated lower bound of the\ncontribution rate of infected voxels is 20.0% for significant radiological\nsigns of COVID-19 infection. Our codes will be made publicly available at\nhttps://github.com/PengyiZhang/DRR4Covid.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 10:34:45 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Zhang", "Pengyi", ""], ["Zhong", "Yunxin", ""], ["Deng", "Yulin", ""], ["Tang", "Xiaoying", ""], ["Li", "Xiaoqiong", ""]]}, {"id": "2008.11479", "submitter": "Marie Katsurai", "authors": "Koya Tango, Marie Katsurai, Hayato Maki, Ryosuke Goto", "title": "Anime-to-Real Clothing: Cosplay Costume Generation via Image-to-Image\n  Translation", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cosplay has grown from its origins at fan conventions into a billion-dollar\nglobal dress phenomenon. To facilitate imagination and reinterpretation from\nanimated images to real garments, this paper presents an automatic costume\nimage generation method based on image-to-image translation. Cosplay items can\nbe significantly diverse in their styles and shapes, and conventional methods\ncannot be directly applied to the wide variation in clothing images that are\nthe focus of this study. To solve this problem, our method starts by collecting\nand preprocessing web images to prepare a cleaned, paired dataset of the anime\nand real domains. Then, we present a novel architecture for generative\nadversarial networks (GANs) to facilitate high-quality cosplay image\ngeneration. Our GAN consists of several effective techniques to fill the gap\nbetween the two domains and improve both the global and local consistency of\ngenerated images. Experiments demonstrated that, with two types of evaluation\nmetrics, the proposed GAN achieves better performance than existing methods. We\nalso showed that the images generated by the proposed method are more realistic\nthan those generated by the conventional methods. Our codes and pretrained\nmodel are available on the web.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 10:34:46 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Tango", "Koya", ""], ["Katsurai", "Marie", ""], ["Maki", "Hayato", ""], ["Goto", "Ryosuke", ""]]}, {"id": "2008.11491", "submitter": "Sam Blakeman", "authors": "Sam Blakeman, Denis Mareschal", "title": "Selective Particle Attention: Visual Feature-Based Attention in Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human brain uses selective attention to filter perceptual input so that\nonly the components that are useful for behaviour are processed using its\nlimited computational resources. We focus on one particular form of visual\nattention known as feature-based attention, which is concerned with identifying\nfeatures of the visual input that are important for the current task regardless\nof their spatial location. Visual feature-based attention has been proposed to\nimprove the efficiency of Reinforcement Learning (RL) by reducing the\ndimensionality of state representations and guiding learning towards relevant\nfeatures. Despite achieving human level performance in complex perceptual-motor\ntasks, Deep RL algorithms have been consistently criticised for their poor\nefficiency and lack of flexibility. Visual feature-based attention therefore\nrepresents one option for addressing these criticisms. Nevertheless, it is\nstill an open question how the brain is able to learn which features to attend\nto during RL. To help answer this question we propose a novel algorithm, termed\nSelective Particle Attention (SPA), which imbues a Deep RL agent with the\nability to perform selective feature-based attention. SPA learns which\ncombinations of features to attend to based on their bottom-up saliency and how\naccurately they predict future reward. We evaluate SPA on a multiple choice\ntask and a 2D video game that both involve raw pixel input and dynamic changes\nto the task structure. We show various benefits of SPA over approaches that\nnaively attend to either all or random subsets of features. Our results\ndemonstrate (1) how visual feature-based attention in Deep RL models can\nimprove their learning efficiency and ability to deal with sudden changes in\ntask structure and (2) that particle filters may represent a viable\ncomputational account of how visual feature-based attention occurs in the\nbrain.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 11:07:50 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Blakeman", "Sam", ""], ["Mareschal", "Denis", ""]]}, {"id": "2008.11493", "submitter": "Ruben Izquierdo", "authors": "R. Izquierdo, A. Quintanar, I. Parra, D. Fernandez-Llorca, and M. A.\n  Sotelo", "title": "Vehicle Trajectory Prediction in Crowded Highway Scenarios Using Bird\n  Eye View Representations and CNNs", "comments": "This work has been accepted for publication at IEEE Intelligent\n  Transportation Systems Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel approach to perform vehicle trajectory\npredictions employing graphic representations. The vehicles are represented\nusing Gaussian distributions into a Bird Eye View. Then the U-net model is used\nto perform sequence to sequence predictions. This deep learning-based\nmethodology has been trained using the HighD dataset, which contains vehicles'\ndetection in a highway scenario from aerial imagery. The problem is faced as an\nimage to image regression problem training the network to learn the underlying\nrelations between the traffic participants. This approach generates an\nestimation of the future appearance of the input scene, not trajectories or\nnumeric positions. An extra step is conducted to extract the positions from the\npredicted representation with subpixel resolution. Different network\nconfigurations have been tested, and prediction error up to three seconds ahead\nis in the order of the representation resolution. The model has been tested in\nhighway scenarios with more than 30 vehicles simultaneously in two opposite\ntraffic flow streams showing good qualitative and quantitative results.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 11:15:49 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Izquierdo", "R.", ""], ["Quintanar", "A.", ""], ["Parra", "I.", ""], ["Fernandez-Llorca", "D.", ""], ["Sotelo", "M. A.", ""]]}, {"id": "2008.11497", "submitter": "Pedro Neto", "authors": "Andr\\'e Br\\'as, Miguel Sim\\~ao, Pedro Neto", "title": "Gesture Recognition from Skeleton Data for Intuitive Human-Machine\n  Interaction", "comments": "Transdisciplinary Engineering Methods for Social Innovation of\n  Industry 4.0", "journal-ref": null, "doi": "10.3233/978-1-61499-898-3-271", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human gesture recognition has assumed a capital role in industrial\napplications, such as Human-Machine Interaction. We propose an approach for\nsegmentation and classification of dynamic gestures based on a set of\nhandcrafted features, which are drawn from the skeleton data provided by the\nKinect sensor. The module for gesture detection relies on a feedforward neural\nnetwork which performs framewise binary classification. The method for gesture\nrecognition applies a sliding window, which extracts information from both the\nspatial and temporal dimensions. Then we combine windows of varying durations\nto get a multi-temporal scale approach and an additional gain in performance.\nEncouraged by the recent success of Recurrent Neural Networks for time series\ndomains, we also propose a method for simultaneous gesture segmentation and\nclassification based on the bidirectional Long Short-Term Memory cells, which\nhave shown ability for learning the temporal relationships on long temporal\nscales. We evaluate all the different approaches on the dataset published for\nthe ChaLearn Looking at People Challenge 2014. The most effective method\nachieves a Jaccard index of 0.75, which suggests a performance almost on pair\nwith that presented by the state-of-the-art techniques. At the end, the\nrecognized gestures are used to interact with a collaborative robot.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 11:28:50 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Br\u00e1s", "Andr\u00e9", ""], ["Sim\u00e3o", "Miguel", ""], ["Neto", "Pedro", ""]]}, {"id": "2008.11505", "submitter": "Juepeng Zheng", "authors": "Juepeng Zheng, Haohuan Fu, Weijia Li, Wenzhao Wu, Yi Zhao, Runmin Dong\n  and Le Yu", "title": "Cross-regional oil palm tree counting and detection via multi-level\n  attention domain adaptation network", "comments": "39 pages, 13 figures, accepted by ISPRS PG&RS", "journal-ref": "ISPRS Journal of Photogrammetry and Remote Sensing 2020", "doi": "10.1016/j.isprsjprs.2020.07.002", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing an accurate evaluation of palm tree plantation in a large region\ncan bring meaningful impacts in both economic and ecological aspects. However,\nthe enormous spatial scale and the variety of geological features across\nregions has made it a grand challenge with limited solutions based on manual\nhuman monitoring efforts. Although deep learning based algorithms have\ndemonstrated potential in forming an automated approach in recent years, the\nlabelling efforts needed for covering different features in different regions\nlargely constrain its effectiveness in large-scale problems. In this paper, we\npropose a novel domain adaptive oil palm tree detection method, i.e., a\nMulti-level Attention Domain Adaptation Network (MADAN) to reap cross-regional\noil palm tree counting and detection. MADAN consists of 4 procedures: First, we\nadopted a batch-instance normalization network (BIN) based feature extractor\nfor improving the generalization ability of the model, integrating batch\nnormalization and instance normalization. Second, we embedded a multi-level\nattention mechanism (MLA) into our architecture for enhancing the\ntransferability, including a feature level attention and an entropy level\nattention. Then we designed a minimum entropy regularization (MER) to increase\nthe confidence of the classifier predictions through assigning the entropy\nlevel attention value to the entropy penalty. Finally, we employed a sliding\nwindow-based prediction and an IOU based post-processing approach to attain the\nfinal detection results. We conducted comprehensive ablation experiments using\nthree different satellite images of large-scale oil palm plantation area with\nsix transfer tasks. MADAN improves the detection accuracy by 14.98% in terms of\naverage F1-score compared with the Baseline method (without DA), and performs\n3.55%-14.49% better than existing domain adaptation methods.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 12:02:44 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Zheng", "Juepeng", ""], ["Fu", "Haohuan", ""], ["Li", "Weijia", ""], ["Wu", "Wenzhao", ""], ["Zhao", "Yi", ""], ["Dong", "Runmin", ""], ["Yu", "Le", ""]]}, {"id": "2008.11508", "submitter": "Mohamed Waly", "authors": "Mohamed. I. Waly, Ahmed El-Hossiny", "title": "Detection of Retinal Blood Vessels by using Gabor filter with Entropic\n  threshold", "comments": null, "journal-ref": "Vol. 4, issue 2, November 2016 - Safar 1438", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic retinopathy is the basic reason for visual deficiency. This paper\nintroduces a programmed strategy to identify and dispense with the blood\nvessels. The location of the blood vessels is the fundamental stride in the\ndiscovery of diabetic retinopathy because the blood vessels are the typical\nelements of the retinal picture. The location of the blood vessels can help the\nophthalmologists to recognize the sicknesses prior and quicker. The blood\nvessels recognized and wiped out by utilizing Gobar filter on two freely\naccessible retinal databases which are STARE and DRIVE. The exactness of\nsegmentation calculation is assessed quantitatively by contrasting the\nphysically sectioned pictures and the comparing yield pictures, the Gabor\nfilter with Entropic threshold vessel pixel segmentation by Entropic\nthresholding is better vessels with less false positive portion rate.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 16:51:12 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Waly", "Mohamed. I.", ""], ["El-Hossiny", "Ahmed", ""]]}, {"id": "2008.11514", "submitter": "Xiao Liu", "authors": "Xiao Liu, Spyridon Thermos, Agisilaos Chartsias, Alison O'Neil and\n  Sotirios A. Tsaftaris", "title": "Disentangled Representations for Domain-generalized Cardiac Segmentation", "comments": "Accepted by STACOM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust cardiac image segmentation is still an open challenge due to the\ninability of the existing methods to achieve satisfactory performance on unseen\ndata of different domains. Since the acquisition and annotation of medical data\nare costly and time-consuming, recent work focuses on domain adaptation and\ngeneralization to bridge the gap between data from different populations and\nscanners. In this paper, we propose two data augmentation methods that focus on\nimproving the domain adaptation and generalization abilities of\nstate-to-the-art cardiac segmentation models. In particular, our \"Resolution\nAugmentation\" method generates more diverse data by rescaling images to\ndifferent resolutions within a range spanning different scanner protocols.\nSubsequently, our \"Factor-based Augmentation\" method generates more diverse\ndata by projecting the original samples onto disentangled latent spaces, and\ncombining the learned anatomy and modality factors from different domains. Our\nextensive experiments demonstrate the importance of efficient adaptation\nbetween seen and unseen domains, as well as model generalization ability, to\nrobust cardiac image segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 12:20:09 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Liu", "Xiao", ""], ["Thermos", "Spyridon", ""], ["Chartsias", "Agisilaos", ""], ["O'Neil", "Alison", ""], ["Tsaftaris", "Sotirios A.", ""]]}, {"id": "2008.11516", "submitter": "Sabarinath Mahadevan", "authors": "Sabarinath Mahadevan, Ali Athar, Aljo\\v{s}a O\\v{s}ep, Sebastian\n  Hennen, Laura Leal-Taix\\'e, Bastian Leibe", "title": "Making a Case for 3D Convolutions for Object Segmentation in Videos", "comments": "BMVC '20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of object segmentation in videos is usually accomplished by\nprocessing appearance and motion information separately using standard 2D\nconvolutional networks, followed by a learned fusion of the two sources of\ninformation. On the other hand, 3D convolutional networks have been\nsuccessfully applied for video classification tasks, but have not been\nleveraged as effectively to problems involving dense per-pixel interpretation\nof videos compared to their 2D convolutional counterparts and lag behind the\naforementioned networks in terms of performance. In this work, we show that 3D\nCNNs can be effectively applied to dense video prediction tasks such as salient\nobject segmentation. We propose a simple yet effective encoder-decoder network\narchitecture consisting entirely of 3D convolutions that can be trained\nend-to-end using a standard cross-entropy loss. To this end, we leverage an\nefficient 3D encoder, and propose a 3D decoder architecture, that comprises\nnovel 3D Global Convolution layers and 3D Refinement modules. Our approach\noutperforms existing state-of-the-arts by a large margin on the DAVIS'16\nUnsupervised, FBMS and ViSal dataset benchmarks in addition to being faster,\nthus showing that our architecture can efficiently learn expressive\nspatio-temporal features and produce high quality video segmentation masks. Our\ncode and models will be made publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 12:24:23 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Mahadevan", "Sabarinath", ""], ["Athar", "Ali", ""], ["O\u0161ep", "Aljo\u0161a", ""], ["Hennen", "Sebastian", ""], ["Leal-Taix\u00e9", "Laura", ""], ["Leibe", "Bastian", ""]]}, {"id": "2008.11546", "submitter": "Yu Li", "authors": "Yu Li", "title": "Towards Structured Prediction in Bioinformatics with Deep Learning", "comments": "PhD dissertatation", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using machine learning, especially deep learning, to facilitate biological\nresearch is a fascinating research direction. However, in addition to the\nstandard classification or regression problems, in bioinformatics, we often\nneed to predict more complex structured targets, such as 2D images and 3D\nmolecular structures. The above complex prediction tasks are referred to as\nstructured prediction. Structured prediction is more complicated than the\ntraditional classification but has much broader applications, considering that\nmost of the original bioinformatics problems have complex output objects. Due\nto the properties of those structured prediction problems, such as having\nproblem-specific constraints and dependency within the labeling space, the\nstraightforward application of existing deep learning models can lead to\nunsatisfactory results. Here, we argue that the following ideas can help\nresolve structured prediction problems in bioinformatics. Firstly, we can\ncombine deep learning with other classic algorithms, such as probabilistic\ngraphical models, which model the problem structure explicitly. Secondly, we\ncan design the problem-specific deep learning architectures or methods by\nconsidering the structured labeling space and problem constraints, either\nexplicitly or implicitly. We demonstrate our ideas with six projects from four\nbioinformatics subfields, including sequencing analysis, structure prediction,\nfunction annotation, and network analysis. The structured outputs cover 1D\nsignals, 2D images, 3D structures, hierarchical labeling, and heterogeneous\nnetworks. With the help of the above ideas, all of our methods can achieve SOTA\nperformance on the corresponding problems. The success of these projects\nmotivates us to extend our work towards other more challenging but important\nproblems, such as health-care problems, which can directly benefit people's\nhealth and wellness.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 02:52:18 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Li", "Yu", ""]]}, {"id": "2008.11560", "submitter": "Weiming Zhuang", "authors": "Weiming Zhuang, Yonggang Wen, Xuesen Zhang, Xin Gan, Daiying Yin,\n  Dongzhan Zhou, Shuai Zhang, Shuai Yi", "title": "Performance Optimization for Federated Person Re-identification via\n  Benchmark Analysis", "comments": "ACMMM'20", "journal-ref": null, "doi": "10.1145/3394171.3413814", "report-no": null, "categories": "cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is a privacy-preserving machine learning technique that\nlearns a shared model across decentralized clients. It can alleviate privacy\nconcerns of personal re-identification, an important computer vision task. In\nthis work, we implement federated learning to person re-identification\n(FedReID) and optimize its performance affected by statistical heterogeneity in\nthe real-world scenario. We first construct a new benchmark to investigate the\nperformance of FedReID. This benchmark consists of (1) nine datasets with\ndifferent volumes sourced from different domains to simulate the heterogeneous\nsituation in reality, (2) two federated scenarios, and (3) an enhanced\nfederated algorithm for FedReID. The benchmark analysis shows that the\nclient-edge-cloud architecture, represented by the federated-by-dataset\nscenario, has better performance than client-server architecture in FedReID. It\nalso reveals the bottlenecks of FedReID under the real-world scenario,\nincluding poor performance of large datasets caused by unbalanced weights in\nmodel aggregation and challenges in convergence. Then we propose two\noptimization methods: (1) To address the unbalanced weight problem, we propose\na new method to dynamically change the weights according to the scale of model\nchanges in clients in each training round; (2) To facilitate convergence, we\nadopt knowledge distillation to refine the server model with knowledge\ngenerated from client models on a public dataset. Experiment results\ndemonstrate that our strategies can achieve much better convergence with\nsuperior performance on all datasets. We believe that our work will inspire the\ncommunity to further explore the implementation of federated learning on more\ncomputer vision tasks in real-world scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 13:41:20 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 17:57:52 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Zhuang", "Weiming", ""], ["Wen", "Yonggang", ""], ["Zhang", "Xuesen", ""], ["Gan", "Xin", ""], ["Yin", "Daiying", ""], ["Zhou", "Dongzhan", ""], ["Zhang", "Shuai", ""], ["Yi", "Shuai", ""]]}, {"id": "2008.11572", "submitter": "Beatriz Garcia Santa Cruz", "authors": "Beatriz Garcia Santa Cruz, Jan S\\\"olter, Matias Nicolas Bossa and\n  Andreas Dominik Husch", "title": "On the Composition and Limitations of Publicly Available COVID-19 X-Ray\n  Imaging Datasets", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning based methods for diagnosis and progression prediction of\nCOVID-19 from imaging data have gained significant attention in the last\nmonths, in particular by the use of deep learning models. In this context\nhundreds of models where proposed with the majority of them trained on public\ndatasets. Data scarcity, mismatch between training and target population, group\nimbalance, and lack of documentation are important sources of bias, hindering\nthe applicability of these models to real-world clinical practice. Considering\nthat datasets are an essential part of model building and evaluation, a deeper\nunderstanding of the current landscape is needed. This paper presents an\noverview of the currently public available COVID-19 chest X-ray datasets. Each\ndataset is briefly described and potential strength, limitations and\ninteractions between datasets are identified. In particular, some key\nproperties of current datasets that could be potential sources of bias,\nimpairing models trained on them are pointed out. These descriptions are useful\nfor model building on those datasets, to choose the best dataset according the\nmodel goal, to take into account the specific limitations to avoid reporting\noverconfident benchmark results, and to discuss their impact on the\ngeneralisation capabilities in a specific clinical setting\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 14:16:01 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Cruz", "Beatriz Garcia Santa", ""], ["S\u00f6lter", "Jan", ""], ["Bossa", "Matias Nicolas", ""], ["Husch", "Andreas Dominik", ""]]}, {"id": "2008.11576", "submitter": "Mehul S. Raval", "authors": "Rupal Agravat, Mehul S Raval", "title": "3D Semantic Segmentation of Brain Tumor for Overall Survival Prediction", "comments": "11 pages, 3 figures, BRaTS 2020. arXiv admin note: text overlap with\n  arXiv:1909.09399", "journal-ref": "LNCS, Springer, 2021", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Glioma, the malignant brain tumor, requires immediate treatment to improve\nthe survival of patients. Gliomas heterogeneous nature makes the segmentation\ndifficult, especially for sub-regions like necrosis, enhancing tumor,\nnon-enhancing tumor, and Edema. Deep neural networks like full convolution\nneural networks and ensemble of fully convolution neural networks are\nsuccessful for Glioma segmentation. The paper demonstrates the use of a 3D\nfully convolution neural network with a three layer encoder decoder approach\nfor layer arrangement. The encoder blocks include the dense modules, and\ndecoder blocks include convolution modules. The input to the network is 3D\npatches. The loss function combines dice loss and focal loss functions. The\nvalidation set dice score of the network is 0.74, 0.88, and 0.73 for enhancing\ntumor, whole tumor, and tumor core, respectively. The Random Forest Regressor\nuses shape, volumetric, and age features extracted from ground truth for\noverall survival prediction. The regressor achieves an accuracy of 44.8% on the\nvalidation set.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 04:32:29 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 04:59:08 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Agravat", "Rupal", ""], ["Raval", "Mehul S", ""]]}, {"id": "2008.11586", "submitter": "Lele Cheng", "authors": "Lele Cheng, Xiangzeng Zhou, Liming Zhao, Dangwei Li, Hong Shang, Yun\n  Zheng, Pan Pan, Yinghui Xu", "title": "Weakly Supervised Learning with Side Information for Noisy Labeled\n  Images", "comments": "Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world datasets, like WebVision, the performance of DNN based\nclassifier is often limited by the noisy labeled data. To tackle this problem,\nsome image related side information, such as captions and tags, often reveal\nunderlying relationships across images. In this paper, we present an efficient\nweakly supervised learning by using a Side Information Network (SINet), which\naims to effectively carry out a large scale classification with severely noisy\nlabels. The proposed SINet consists of a visual prototype module and a noise\nweighting module. The visual prototype module is designed to generate a compact\nrepresentation for each category by introducing the side information. The noise\nweighting module aims to estimate the correctness of each noisy image and\nproduce a confidence score for image ranking during the training procedure. The\npropsed SINet can largely alleviate the negative impact of noisy image labels,\nand is beneficial to train a high performance CNN based classifier. Besides, we\nreleased a fine-grained product dataset called AliProducts, which contains more\nthan 2.5 million noisy web images crawled from the internet by using queries\ngenerated from 50,000 fine-grained semantic classes. Extensive experiments on\nseveral popular benchmarks (i.e. Webvision, ImageNet and Clothing-1M) and our\nproposed AliProducts achieve state-of-the-art performance. The SINet has won\nthe first place in the classification task on WebVision Challenge 2019, and\noutperformed other competitors by a large margin.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 05:43:48 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 08:02:28 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Cheng", "Lele", ""], ["Zhou", "Xiangzeng", ""], ["Zhao", "Liming", ""], ["Li", "Dangwei", ""], ["Shang", "Hong", ""], ["Zheng", "Yun", ""], ["Pan", "Pan", ""], ["Xu", "Yinghui", ""]]}, {"id": "2008.11588", "submitter": "Alejandro Lopez-Cifuentes", "authors": "Alejandro L\\'opez-Cifuentes, Marcos Escudero-Vi\\~nolo, Jes\\'us\n  Besc\\'os", "title": "A Prospective Study on Sequence-Driven Temporal Sampling and Ego-Motion\n  Compensation for Action Recognition in the EPIC-Kitchens Dataset", "comments": "Paper accepted at CVPR 2020 EPIC Kitchens Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition is currently one of the top-challenging research fields in\ncomputer vision. Convolutional Neural Networks (CNNs) have significantly\nboosted its performance but rely on fixed-size spatio-temporal windows of\nanalysis, reducing CNNs temporal receptive fields. Among action recognition\ndatasets, egocentric recorded sequences have become of important relevance\nwhile entailing an additional challenge: ego-motion is unavoidably transferred\nto these sequences. The proposed method aims to cope with it by estimating this\nego-motion or camera motion. The estimation is used to temporally partition\nvideo sequences into motion-compensated temporal \\textit{chunks} showing the\naction under stable backgrounds and allowing for a content-driven temporal\nsampling. A CNN trained in an end-to-end fashion is used to extract temporal\nfeatures from each \\textit{chunk}, which are late fused. This process leads to\nthe extraction of features from the whole temporal range of an action,\nincreasing the temporal receptive field of the network.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 14:44:45 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["L\u00f3pez-Cifuentes", "Alejandro", ""], ["Escudero-Vi\u00f1olo", "Marcos", ""], ["Besc\u00f3s", "Jes\u00fas", ""]]}, {"id": "2008.11598", "submitter": "Xinshuo Weng", "authors": "Xinshuo Weng, Ye Yuan, Kris Kitani", "title": "End-to-End 3D Multi-Object Tracking and Trajectory Forecasting", "comments": "Extended abstract. The first two authors contributed equally. Project\n  website: http://www.xinshuoweng.com/projects/GNNTrkForecast. arXiv admin\n  note: substantial text overlap with arXiv:2003.07847", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D multi-object tracking (MOT) and trajectory forecasting are two critical\ncomponents in modern 3D perception systems. We hypothesize that it is\nbeneficial to unify both tasks under one framework to learn a shared feature\nrepresentation of agent interaction. To evaluate this hypothesis, we propose a\nunified solution for 3D MOT and trajectory forecasting which also incorporates\ntwo additional novel computational units. First, we employ a feature\ninteraction technique by introducing Graph Neural Networks (GNNs) to capture\nthe way in which multiple agents interact with one another. The GNN is able to\nmodel complex hierarchical interactions, improve the discriminative feature\nlearning for MOT association, and provide socially-aware context for trajectory\nforecasting. Second, we use a diversity sampling function to improve the\nquality and diversity of our forecasted trajectories. The learned sampling\nfunction is trained to efficiently extract a variety of outcomes from a\ngenerative trajectory distribution and helps avoid the problem of generating\nmany duplicate trajectory samples. We show that our method achieves\nstate-of-the-art performance on the KITTI dataset. Our project website is at\nhttp://www.xinshuoweng.com/projects/GNNTrkForecast.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 16:54:46 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Weng", "Xinshuo", ""], ["Yuan", "Ye", ""], ["Kitani", "Kris", ""]]}, {"id": "2008.11600", "submitter": "Chirag Agarwal", "authors": "Chirag Agarwal, Sara Hooker", "title": "Estimating Example Difficulty Using Variance of Gradients", "comments": "Preliminary results accepted to Workshop on Human Interpretability in\n  Machine Learning (WHI), ICML, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, a question of great interest is understanding what\nexamples are challenging for a model to classify. Identifying atypical examples\nhelps inform safe deployment of models, isolates examples that require further\nhuman inspection, and provides interpretability into model behavior. In this\nwork, we propose Variance of Gradients ($VOG$) as a valuable and efficient\nproxy metric for detecting outliers in the data distribution. We provide\nquantitative and qualitative support that $VOG$ is a meaningful way to rank\ndata by difficulty and to surface a tractable subset of the most challenging\nexamples for human-in-the-loop auditing. Data points with high $VOG$ scores are\nfar more difficult for the model to learn and over-index on corrupted or\nmemorized examples.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 14:53:24 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 01:48:27 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Agarwal", "Chirag", ""], ["Hooker", "Sara", ""]]}, {"id": "2008.11603", "submitter": "Chunhui Li", "authors": "Chunhui Li, Xingshu Chen, Haizhou Wang, Yu Zhang, Peiming Wang", "title": "An End-to-End Attack on Text-based CAPTCHAs Based on Cycle-Consistent\n  Generative Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a widely deployed security scheme, text-based CAPTCHAs have become more\nand more difficult to resist machine learning-based attacks. So far, many\nresearchers have conducted attacking research on text-based CAPTCHAs deployed\nby different companies (such as Microsoft, Amazon, and Apple) and achieved\ncertain results.However, most of these attacks have some shortcomings, such as\npoor portability of attack methods, requiring a series of data preprocessing\nsteps, and relying on large amounts of labeled CAPTCHAs. In this paper, we\npropose an efficient and simple end-to-end attack method based on\ncycle-consistent generative adversarial networks. Compared with previous\nstudies, our method greatly reduces the cost of data labeling. In addition,\nthis method has high portability. It can attack common text-based CAPTCHA\nschemes only by modifying a few configuration parameters, which makes the\nattack easier. Firstly, we train CAPTCHA synthesizers based on the cycle-GAN to\ngenerate some fake samples. Basic recognizers based on the convolutional\nrecurrent neural network are trained with the fake data. Subsequently, an\nactive transfer learning method is employed to optimize the basic recognizer\nutilizing tiny amounts of labeled real-world CAPTCHA samples. Our approach\nefficiently cracked the CAPTCHA schemes deployed by 10 popular websites,\nindicating that our attack is likely very general. Additionally, we analyzed\nthe current most popular anti-recognition mechanisms. The results show that the\ncombination of more anti-recognition mechanisms can improve the security of\nCAPTCHA, but the improvement is limited. Conversely, generating more complex\nCAPTCHAs may cost more resources and reduce the availability of CAPTCHAs.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 14:57:47 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Li", "Chunhui", ""], ["Chen", "Xingshu", ""], ["Wang", "Haizhou", ""], ["Zhang", "Yu", ""], ["Wang", "Peiming", ""]]}, {"id": "2008.11604", "submitter": "Fernando Alonso-Fernandez", "authors": "Kevin Hernandez-Diaz, Fernando Alonso-Fernandez, Josef Bigun", "title": "Cross-Spectral Periocular Recognition with Conditional Adversarial\n  Networks", "comments": "Accepted for publication at 2020 International Joint Conference on\n  Biometrics (IJCB 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the challenge of comparing periocular images captured in\ndifferent spectra, which is known to produce significant drops in performance\nin comparison to operating in the same spectrum. We propose the use of\nConditional Generative Adversarial Networks, trained to con-vert periocular\nimages between visible and near-infrared spectra, so that biometric\nverification is carried out in the same spectrum. The proposed setup allows the\nuse of existing feature methods typically optimized to operate in a single\nspectrum. Recognition experiments are done using a number of off-the-shelf\nperiocular comparators based both on hand-crafted features and CNN descriptors.\nUsing the Hong Kong Polytechnic University Cross-Spectral Iris Images Database\n(PolyU) as benchmark dataset, our experiments show that cross-spectral\nperformance is substantially improved if both images are converted to the same\nspectrum, in comparison to matching features extracted from images in different\nspectra. In addition to this, we fine-tune a CNN based on the ResNet50\narchitecture, obtaining a cross-spectral periocular performance of EER=1%, and\nGAR>99% @ FAR=1%, which is comparable to the state-of-the-art with the PolyU\ndatabase.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 15:02:04 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Hernandez-Diaz", "Kevin", ""], ["Alonso-Fernandez", "Fernando", ""], ["Bigun", "Josef", ""]]}, {"id": "2008.11638", "submitter": "Ujjal Kr Dutta", "authors": "Abhinav Ravi, Sandeep Repakula, Ujjal Kr Dutta, Maulik Parmar", "title": "Buy Me That Look: An Approach for Recommending Similar Fashion Products", "comments": "Accepted at the IEEE International Conference on Multimedia\n  Information Processing and Retrieval (MIPR) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Have you ever looked at an Instagram model, or a model in a fashion\ne-commerce web-page, and thought \\textit{\"Wish I could get a list of fashion\nitems similar to the ones worn by the model!\"}. This is what we address in this\npaper, where we propose a novel computer vision based technique called\n\\textbf{ShopLook} to address the challenging problem of recommending similar\nfashion products. The proposed method has been evaluated at Myntra\n(www.myntra.com), a leading online fashion e-commerce platform. In particular,\ngiven a user query and the corresponding Product Display Page (PDP) against the\nquery, the goal of our method is to recommend similar fashion products\ncorresponding to the entire set of fashion articles worn by a model in the PDP\nfull-shot image (the one showing the entire model from head to toe). The\nnovelty and strength of our method lies in its capability to recommend similar\narticles for all the fashion items worn by the model, in addition to the\nprimary article corresponding to the query. This is not only important to\npromote cross-sells for boosting revenue, but also for improving customer\nexperience and engagement. In addition, our approach is also capable of\nrecommending similar products for User Generated Content (UGC), eg., fashion\narticle images uploaded by users. Formally, our proposed method consists of the\nfollowing components (in the same order): i) Human keypoint detection, ii) Pose\nclassification, iii) Article localisation and object detection, along with\nactive learning feedback, and iv) Triplet network based image embedding model.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 16:01:00 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 13:05:30 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Ravi", "Abhinav", ""], ["Repakula", "Sandeep", ""], ["Dutta", "Ujjal Kr", ""], ["Parmar", "Maulik", ""]]}, {"id": "2008.11639", "submitter": "Samir Yadav S", "authors": "Samir S. Yadav, Jasminder Kaur Sandhu, Mininath R. Bendre, Pratap S.\n  Vikhe, Amandeep Kaur", "title": "A comparison of deep machine learning algorithms in COVID-19 disease\n  diagnosis", "comments": "16 pages, 10 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of the work is to use deep neural network models for solving the\nproblem of image recognition. These days, every human being is threatened by a\nharmful coronavirus disease, also called COVID-19 disease. The spread of\ncoronavirus affects the economy of many countries in the world. To find\nCOVID-19 patients early is very essential to avoid the spread and harm to\nsociety. Pathological tests and Chromatography(CT) scans are helpful for the\ndiagnosis of COVID-19. However, these tests are having drawbacks such as a\nlarge number of false positives, and cost of these tests are so expensive.\nHence, it requires finding an easy, accurate, and less expensive way for the\ndetection of the harmful COVID-19 disease. Chest-x-ray can be useful for the\ndetection of this disease. Therefore, in this work chest, x-ray images are used\nfor the diagnosis of suspected COVID-19 patients using modern machine learning\ntechniques. The analysis of the results is carried out and conclusions are made\nabout the effectiveness of deep machine learning algorithms in image\nrecognition problems.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 10:51:54 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 07:25:26 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Yadav", "Samir S.", ""], ["Sandhu", "Jasminder Kaur", ""], ["Bendre", "Mininath R.", ""], ["Vikhe", "Pratap S.", ""], ["Kaur", "Amandeep", ""]]}, {"id": "2008.11646", "submitter": "Tingyu Wang", "authors": "Tingyu Wang, Zhedong Zheng, Chenggang Yan, Jiyong Zhang, Yaoqi Sun,\n  Bolun Zheng, and Yi Yang", "title": "Each Part Matters: Local Patterns Facilitate Cross-view Geo-localization", "comments": "accepted by TCSVT", "journal-ref": null, "doi": "10.1109/TCSVT.2021.3061265", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-view geo-localization is to spot images of the same geographic target\nfrom different platforms, e.g., drone-view cameras and satellites. It is\nchallenging in the large visual appearance changes caused by extreme viewpoint\nvariations. Existing methods usually concentrate on mining the fine-grained\nfeature of the geographic target in the image center, but underestimate the\ncontextual information in neighbor areas. In this work, we argue that neighbor\nareas can be leveraged as auxiliary information, enriching discriminative clues\nfor geolocalization. Specifically, we introduce a simple and effective deep\nneural network, called Local Pattern Network (LPN), to take advantage of\ncontextual information in an end-to-end manner. Without using extra part\nestimators, LPN adopts a square-ring feature partition strategy, which provides\nthe attention according to the distance to the image center. It eases the part\nmatching and enables the part-wise representation learning. Owing to the\nsquare-ring partition design, the proposed LPN has good scalability to rotation\nvariations and achieves competitive results on three prevailing benchmarks,\ni.e., University-1652, CVUSA and CVACT. Besides, we also show the proposed LPN\ncan be easily embedded into other frameworks to further boost performance.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 16:06:11 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 06:49:51 GMT"}, {"version": "v3", "created": "Sun, 6 Jun 2021 02:46:25 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wang", "Tingyu", ""], ["Zheng", "Zhedong", ""], ["Yan", "Chenggang", ""], ["Zhang", "Jiyong", ""], ["Sun", "Yaoqi", ""], ["Zheng", "Bolun", ""], ["Yang", "Yi", ""]]}, {"id": "2008.11647", "submitter": "Javier Lorenzo D\\'iaz", "authors": "Javier Lorenzo, Ignacio Parra, Florian Wirth, Christoph Stiller, David\n  Fernandez Llorca and Miguel Angel Sotelo", "title": "RNN-based Pedestrian Crossing Prediction using Activity and Pose-related\n  Features", "comments": "6 pages, 5 figures. This work has been accepted for publication at\n  IEEE Intelligent Vehicle Symposium 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian crossing prediction is a crucial task for autonomous driving.\nNumerous studies show that an early estimation of the pedestrian's intention\ncan decrease or even avoid a high percentage of accidents. In this paper,\ndifferent variations of a deep learning system are proposed to attempt to solve\nthis problem. The proposed models are composed of two parts: a CNN-based\nfeature extractor and an RNN module. All the models were trained and tested on\nthe JAAD dataset. The results obtained indicate that the choice of the features\nextraction method, the inclusion of additional variables such as pedestrian\ngaze direction and discrete orientation, and the chosen RNN type have a\nsignificant impact on the final performance.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 16:06:24 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Lorenzo", "Javier", ""], ["Parra", "Ignacio", ""], ["Wirth", "Florian", ""], ["Stiller", "Christoph", ""], ["Llorca", "David Fernandez", ""], ["Sotelo", "Miguel Angel", ""]]}, {"id": "2008.11662", "submitter": "Ujjal Kr Dutta", "authors": "Rajdeep Hazra Banerjee, Abhinav Ravi, Ujjal Kr Dutta", "title": "Attr2Style: A Transfer Learning Approach for Inferring Fashion Styles\n  via Apparel Attributes", "comments": "In Annual Conference on Innovative Applications of Artificial\n  Intelligence (IAAI), colocated with AAAI Conference on Artificial\n  Intelligence (AAAI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular fashion e-commerce platforms mostly provide details about low-level\nattributes of an apparel (eg, neck type, dress length, collar type) on their\nproduct detail pages. However, customers usually prefer to buy apparel based on\ntheir style information, or simply put, occasion (eg, party/ sports/ casual\nwear). Application of a supervised image-captioning model to generate\nstyle-based image captions is limited because obtaining ground-truth\nannotations in the form of style-based captions is difficult. This is because\nannotating style-based captions requires a certain amount of fashion domain\nexpertise, and also adds to the costs and manual effort. On the contrary,\nlow-level attribute based annotations are much more easily available. To\naddress this issue, we propose a transfer-learning based image captioning model\nthat is trained on a source dataset with sufficient attribute-based\nground-truth captions, and used to predict style-based captions on a target\ndataset. The target dataset has only a limited amount of images with\nstyle-based ground-truth captions. The main motivation of our approach comes\nfrom the fact that most often there are correlations among the low-level\nattributes and the higher-level styles for an apparel. We leverage this fact\nand train our model in an encoder-decoder based framework using attention\nmechanism. In particular, the encoder of the model is first trained on the\nsource dataset to obtain latent representations capturing the low-level\nattributes. The trained model is fine-tuned to generate style-based captions\nfor the target dataset. To highlight the effectiveness of our method, we\nqualitatively and quantitatively demonstrate that the captions generated by our\napproach are close to the actual style information for the evaluated apparel. A\nProof Of Concept for our model is under pilot at Myntra where it is exposed to\nsome internal users for feedback.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 16:42:21 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 12:03:48 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Banerjee", "Rajdeep Hazra", ""], ["Ravi", "Abhinav", ""], ["Dutta", "Ujjal Kr", ""]]}, {"id": "2008.11672", "submitter": "Mahdi Rezaei", "authors": "Mahdi Rezaei, Mohsen Azarmi", "title": "DeepSOCIAL: Social Distancing Monitoring and Infection Risk Assessment\n  in COVID-19 Pandemic", "comments": null, "journal-ref": "Applied Sciences. 2020, 10, 7514", "doi": "10.3390/app10217514", "report-no": null, "categories": "cs.CV cs.LG eess.IV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social distancing is a recommended solution by the World Health Organisation\n(WHO) to minimise the spread of COVID-19 in public places. The majority of\ngovernments and national health authorities have set the 2-meter physical\ndistancing as a mandatory safety measure in shopping centres, schools and other\ncovered areas. In this research, we develop a hybrid Computer Vision and\nYOLOv4-based Deep Neural Network model for automated people detection in the\ncrowd in indoor and outdoor environments using common CCTV security cameras.\nThe proposed DNN model in combination with an adapted inverse perspective\nmapping (IPM) technique and SORT tracking algorithm leads to a robust people\ndetection and social distancing monitoring. The model has been trained against\ntwo most comprehensive datasets by the time of the research the Microsoft\nCommon Objects in Context (MS COCO) and Google Open Image datasets. The system\nhas been evaluated against the Oxford Town Centre dataset with superior\nperformance compared to three state-of-the-art methods. The evaluation has been\nconducted in challenging conditions, including occlusion, partial visibility,\nand under lighting variations with the mean average precision of 99.8% and the\nreal-time speed of 24.1 fps. We also provide an online infection risk\nassessment scheme by statistical analysis of the Spatio-temporal data from\npeople's moving trajectories and the rate of social distancing violations. The\ndeveloped model is a generic and accurate people detection and tracking\nsolution that can be applied in many other fields such as autonomous vehicles,\nhuman action recognition, anomaly detection, sports, crowd analysis, or any\nother research areas where the human detection is in the centre of attention.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 16:56:57 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 22:05:27 GMT"}, {"version": "v3", "created": "Sat, 28 Nov 2020 13:46:27 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Rezaei", "Mahdi", ""], ["Azarmi", "Mohsen", ""]]}, {"id": "2008.11673", "submitter": "Maxime Lafarge", "authors": "Maxime W. Lafarge, Josien P.W. Pluim and Mitko Veta", "title": "Orientation-Disentangled Unsupervised Representation Learning for\n  Computational Pathology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning enables modeling complex images without the need for\nannotations. The representation learned by such models can facilitate any\nsubsequent analysis of large image datasets.\n  However, some generative factors that cause irrelevant variations in images\ncan potentially get entangled in such a learned representation causing the risk\nof negatively affecting any subsequent use. The orientation of imaged objects,\nfor instance, is often arbitrary/irrelevant, thus it can be desired to learn a\nrepresentation in which the orientation information is disentangled from all\nother factors.\n  Here, we propose to extend the Variational Auto-Encoder framework by\nleveraging the group structure of rotation-equivariant convolutional networks\nto learn orientation-wise disentangled generative factors of histopathology\nimages. This way, we enforce a novel partitioning of the latent space, such\nthat oriented and isotropic components get separated.\n  We evaluated this structured representation on a dataset that consists of\ntissue regions for which nuclear pleomorphism and mitotic activity was assessed\nby expert pathologists. We show that the trained models efficiently disentangle\nthe inherent orientation information of single-cell images. In comparison to\nclassical approaches, the resulting aggregated representation of\nsub-populations of cells produces higher performances in subsequent tasks.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 16:57:45 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Lafarge", "Maxime W.", ""], ["Pluim", "Josien P. W.", ""], ["Veta", "Mitko", ""]]}, {"id": "2008.11689", "submitter": "Yanyu Zhang", "authors": "Yanyu Zhang, Osama Alshaykh", "title": "5G Utility Pole Planner Using Google Street View and Mask R-CNN", "comments": "4 pages, 7 figures", "journal-ref": "2020 IEEE International Conference on Electro Information\n  Technology (EIT)", "doi": "10.1109/EIT48999.2020.9208333", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advances of fifth-generation (5G) cellular networks technology, many\nstudies and work have been carried out on how to build 5G networks for smart\ncities. In the previous research, street lighting poles and smart light poles\nare capable of being a 5G access point. In order to determine the position of\nthe points, this paper discusses a new way to identify poles based on Mask\nR-CNN, which extends Fast R-CNNs by making it employ recursive Bayesian\nfiltering and perform proposal propagation and reuse. The dataset contains\n3,000 high-resolution images from google map. To make training faster, we used\na very efficient GPU implementation of the convolution operation. We achieved a\ntrain error rate of 7.86% and a test error rate of 32.03%. At last, we used the\nimmune algorithm to set 5G poles in the smart cities.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 17:27:52 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Zhang", "Yanyu", ""], ["Alshaykh", "Osama", ""]]}, {"id": "2008.11702", "submitter": "Jiahao Xie", "authors": "Jiahao Xie, Xiaohang Zhan, Ziwei Liu, Yew Soon Ong, Chen Change Loy", "title": "Delving into Inter-Image Invariance for Unsupervised Visual\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive learning has recently shown immense potential in unsupervised\nvisual representation learning. Existing studies in this track mainly focus on\nintra-image invariance learning. The learning typically uses rich intra-image\ntransformations to construct positive pairs and then maximizes agreement using\na contrastive loss. The merits of inter-image invariance, conversely, remain\nmuch less explored. One major obstacle to exploit inter-image invariance is\nthat it is unclear how to reliably construct inter-image positive pairs, and\nfurther derive effective supervision from them since there are no pair\nannotations available. In this work, we present a rigorous and comprehensive\nstudy on inter-image invariance learning from three main constituting\ncomponents: pseudo-label maintenance, sampling strategy, and decision boundary\ndesign. Through carefully-designed comparisons and analysis, we propose a\nunified and generic framework that supports the integration of unsupervised\nintra- and inter-image invariance learning. With all the obtained recipes, our\nfinal model, namely InterCLR, shows consistent improvements over\nstate-of-the-art intra-image invariance learning methods on multiple standard\nbenchmarks. Codes will be released at\nhttps://github.com/open-mmlab/OpenSelfSup.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 17:44:23 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 17:03:15 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Xie", "Jiahao", ""], ["Zhan", "Xiaohang", ""], ["Liu", "Ziwei", ""], ["Ong", "Yew Soon", ""], ["Loy", "Chen Change", ""]]}, {"id": "2008.11713", "submitter": "Jia-Bin Huang", "authors": "Yun-Chun Chen, Chen Gao, Esther Robb, Jia-Bin Huang", "title": "NAS-DIP: Learning Deep Image Prior with Neural Architecture Search", "comments": "ECCV 2020. Project: https://yunchunchen.github.io/NAS-DIP/ Code:\n  https://github.com/YunChunChen/NAS-DIP-pytorch The first two authors\n  contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that the structure of deep convolutional neural\nnetworks can be used as a structured image prior for solving various inverse\nimage restoration tasks. Instead of using hand-designed architectures, we\npropose to search for neural architectures that capture stronger image priors.\nBuilding upon a generic U-Net architecture, our core contribution lies in\ndesigning new search spaces for (1) an upsampling cell and (2) a pattern of\ncross-scale residual connections. We search for an improved network by\nleveraging an existing neural architecture search algorithm (using\nreinforcement learning with a recurrent neural network controller). We validate\nthe effectiveness of our method via a wide variety of applications, including\nimage restoration, dehazing, image-to-image translation, and matrix\nfactorization. Extensive experimental results show that our algorithm performs\nfavorably against state-of-the-art learning-free approaches and reaches\ncompetitive performance with existing learning-based methods in some cases.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 17:59:36 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Chen", "Yun-Chun", ""], ["Gao", "Chen", ""], ["Robb", "Esther", ""], ["Huang", "Jia-Bin", ""]]}, {"id": "2008.11714", "submitter": "Jia-Bin Huang", "authors": "Chen Gao, Jiarui Xu, Yuliang Zou, Jia-Bin Huang", "title": "DRG: Dual Relation Graph for Human-Object Interaction Detection", "comments": "ECCV 2020. Project: http://chengao.vision/DRG/ Code:\n  https://github.com/vt-vl-lab/DRG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the challenging problem of human-object interaction (HOI)\ndetection. Existing methods either recognize the interaction of each\nhuman-object pair in isolation or perform joint inference based on complex\nappearance-based features. In this paper, we leverage an abstract\nspatial-semantic representation to describe each human-object pair and\naggregate the contextual information of the scene via a dual relation graph\n(one human-centric and one object-centric). Our proposed dual relation graph\neffectively captures discriminative cues from the scene to resolve ambiguity\nfrom local predictions. Our model is conceptually simple and leads to favorable\nresults compared to the state-of-the-art HOI detection algorithms on two\nlarge-scale benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 17:59:40 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Gao", "Chen", ""], ["Xu", "Jiarui", ""], ["Zou", "Yuliang", ""], ["Huang", "Jia-Bin", ""]]}, {"id": "2008.11752", "submitter": "Shounak Datta", "authors": "Sankha Subhra Mullick and Shounak Datta and Sourish Gunesh Dhekane and\n  Swagatam Das", "title": "Appropriateness of Performance Indices for Imbalanced Data\n  Classification: An Analysis", "comments": "Published in Pattern Recognition (Elsevier)", "journal-ref": "Pattern Recognition, 102, p.107197 (2020)", "doi": "10.1016/j.patcog.2020.107197", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indices quantifying the performance of classifiers under class-imbalance,\noften suffer from distortions depending on the constitution of the test set or\nthe class-specific classification accuracy, creating difficulties in assessing\nthe merit of the classifier. We identify two fundamental conditions that a\nperformance index must satisfy to be respectively resilient to altering number\nof testing instances from each class and the number of classes in the test set.\nIn light of these conditions, under the effect of class imbalance, we\ntheoretically analyze four indices commonly used for evaluating binary\nclassifiers and five popular indices for multi-class classifiers. For indices\nviolating any of the conditions, we also suggest remedial modification and\nnormalization. We further investigate the capability of the indices to retain\ninformation about the classification performance over all the classes, even\nwhen the classifier exhibits extreme performance on some classes. Simulation\nstudies are performed on high dimensional deep representations of subset of the\nImageNet dataset using four state-of-the-art classifiers tailored for handling\nclass imbalance. Finally, based on our theoretical findings and empirical\nevidence, we recommend the appropriate indices that should be used to evaluate\nthe performance of classifiers in presence of class-imbalance.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 18:23:36 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Mullick", "Sankha Subhra", ""], ["Datta", "Shounak", ""], ["Dhekane", "Sourish Gunesh", ""], ["Das", "Swagatam", ""]]}, {"id": "2008.11755", "submitter": "Mohammad Zaki Zadeh", "authors": "Mohammad Zaki Zadeh, Ashwin Ramesh Babu, Ashish Jaiswal, Fillia\n  Makedon", "title": "Self-Supervised Human Activity Recognition by Augmenting Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": "10.1145/3453892.3453893", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a novel approach for augmenting generative adversarial\nnetwork (GAN) with a self-supervised task in order to improve its ability for\nencoding video representations that are useful in downstream tasks such as\nhuman activity recognition. In the proposed method, input video frames are\nrandomly transformed by different spatial transformations, such as rotation,\ntranslation and shearing or temporal transformations such as shuffling temporal\norder of frames. Then discriminator is encouraged to predict the applied\ntransformation by introducing an auxiliary loss. Subsequently, results prove\nsuperiority of the proposed method over baseline methods for providing a useful\nrepresentation of videos used in human activity recognition performed on\ndatasets such as KTH, UCF101 and Ball-Drop. Ball-Drop dataset is a specifically\ndesigned dataset for measuring executive functions in children through\nphysically and cognitively demanding tasks. Using features from proposed method\ninstead of baseline methods caused the top-1 classification accuracy to\nincrease by more then 4%. Moreover, ablation study was performed to investigate\nthe contribution of different transformations on downstream task.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 18:28:17 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 18:45:50 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Zadeh", "Mohammad Zaki", ""], ["Babu", "Ashwin Ramesh", ""], ["Jaiswal", "Ashish", ""], ["Makedon", "Fillia", ""]]}, {"id": "2008.11762", "submitter": "Oliver Woodford", "authors": "Oliver J. Woodford, Edward Rosten", "title": "Large Scale Photometric Bundle Adjustment", "comments": "Presented at BMVC 2020. Fixed errors: intrinsic regularization\n  corrected, and added to the cost", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct methods have shown promise on visual odometry and SLAM, leading to\ngreater accuracy and robustness over feature-based methods. However, offline\n3-d reconstruction from internet images has not yet benefited from a joint,\nphotometric optimization over dense geometry and camera parameters. Issues such\nas the lack of brightness constancy, and the sheer volume of data, make this a\nmore challenging task. This work presents a framework for jointly optimizing\nmillions of scene points and hundreds of camera poses and intrinsics, using a\nphotometric cost that is invariant to local lighting changes. The improvement\nin metric reconstruction accuracy that it confers over feature-based bundle\nadjustment is demonstrated on the large-scale Tanks & Temples benchmark. We\nfurther demonstrate qualitative reconstruction improvements on an internet\nphoto collection, with challenging diversity in lighting and camera intrinsics.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 18:49:30 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 20:31:47 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Woodford", "Oliver J.", ""], ["Rosten", "Edward", ""]]}, {"id": "2008.11769", "submitter": "Bi Li", "authors": "Bi Li, Chengquan Zhang, Zhibin Hong, Xu Tang, Jingtuo Liu, Junyu Han,\n  Errui Ding, Wenyu Liu", "title": "Learning Global Structure Consistency for Robust Object Tracking", "comments": "Accepted by ACM MM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast appearance variations and the distractions of similar objects are two of\nthe most challenging problems in visual object tracking. Unlike many existing\ntrackers that focus on modeling only the target, in this work, we consider the\n\\emph{transient variations of the whole scene}. The key insight is that the\nobject correspondence and spatial layout of the whole scene are consistent\n(i.e., global structure consistency) in consecutive frames which helps to\ndisambiguate the target from distractors. Moreover, modeling transient\nvariations enables to localize the target under fast variations. Specifically,\nwe propose an effective and efficient short-term model that learns to exploit\nthe global structure consistency in a short time and thus can handle fast\nvariations and distractors. Since short-term modeling falls short of handling\nocclusion and out of the views, we adopt the long-short term paradigm and use a\nlong-term model that corrects the short-term model when it drifts away from the\ntarget or the target is not present. These two components are carefully\ncombined to achieve the balance of stability and plasticity during tracking. We\nempirically verify that the proposed tracker can tackle the two challenging\nscenarios and validate it on large scale benchmarks. Remarkably, our tracker\nimproves state-of-the-art-performance on VOT2018 from 0.440 to 0.460, GOT-10k\nfrom 0.611 to 0.640, and NFS from 0.619 to 0.629.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 19:12:53 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Li", "Bi", ""], ["Zhang", "Chengquan", ""], ["Hong", "Zhibin", ""], ["Tang", "Xu", ""], ["Liu", "Jingtuo", ""], ["Han", "Junyu", ""], ["Ding", "Errui", ""], ["Liu", "Wenyu", ""]]}, {"id": "2008.11772", "submitter": "Shasha Li", "authors": "Shasha Li, Karim Khalil, Rameswar Panda, Chengyu Song, Srikanth V.\n  Krishnamurthy, Amit K. Roy-Chowdhury, Ananthram Swami", "title": "Measurement-driven Security Analysis of Imperceptible Impersonation\n  Attacks", "comments": "accepted and appears in ICCCN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of Internet of Things (IoT) brings about new security\nchallenges at the intersection of cyber and physical spaces. One prime example\nis the vulnerability of Face Recognition (FR) based access control in IoT\nsystems. While previous research has shown that Deep Neural Network(DNN)-based\nFR systems (FRS) are potentially susceptible to imperceptible impersonation\nattacks, the potency of such attacks in a wide set of scenarios has not been\nthoroughly investigated. In this paper, we present the first systematic,\nwide-ranging measurement study of the exploitability of DNN-based FR systems\nusing a large scale dataset. We find that arbitrary impersonation attacks,\nwherein an arbitrary attacker impersonates an arbitrary target, are hard if\nimperceptibility is an auxiliary goal. Specifically, we show that factors such\nas skin color, gender, and age, impact the ability to carry out an attack on a\nspecific target victim, to different extents. We also study the feasibility of\nconstructing universal attacks that are robust to different poses or views of\nthe attacker's face. Our results show that finding a universal perturbation is\na much harder problem from the attacker's perspective. Finally, we find that\nthe perturbed images do not generalize well across different DNN models. This\nsuggests security countermeasures that can dramatically reduce the\nexploitability of DNN-based FR systems.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 19:27:27 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Li", "Shasha", ""], ["Khalil", "Karim", ""], ["Panda", "Rameswar", ""], ["Song", "Chengyu", ""], ["Krishnamurthy", "Srikanth V.", ""], ["Roy-Chowdhury", "Amit K.", ""], ["Swami", "Ananthram", ""]]}, {"id": "2008.11776", "submitter": "Cian M. Scannell", "authors": "Cian M. Scannell and Amedeo Chiribiri and Mitko Veta", "title": "Domain-Adversarial Learning for Multi-Centre, Multi-Vendor, and\n  Multi-Disease Cardiac MR Image Segmentation", "comments": "Accepted at the STACOM workshop at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cine cardiac magnetic resonance (CMR) has become the gold standard for the\nnon-invasive evaluation of cardiac function. In particular, it allows the\naccurate quantification of functional parameters including the chamber volumes\nand ejection fraction. Deep learning has shown the potential to automate the\nrequisite cardiac structure segmentation. However, the lack of robustness of\ndeep learning models has hindered their widespread clinical adoption. Due to\ndifferences in the data characteristics, neural networks trained on data from a\nspecific scanner are not guaranteed to generalise well to data acquired at a\ndifferent centre or with a different scanner. In this work, we propose a\nprincipled solution to the problem of this domain shift. Domain-adversarial\nlearning is used to train a domain-invariant 2D U-Net using labelled and\nunlabelled data. This approach is evaluated on both seen and unseen domains\nfrom the M\\&Ms challenge dataset and the domain-adversarial approach shows\nimproved performance as compared to standard training. Additionally, we show\nthat the domain information cannot be recovered from the learned features.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 19:40:55 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Scannell", "Cian M.", ""], ["Chiribiri", "Amedeo", ""], ["Veta", "Mitko", ""]]}, {"id": "2008.11783", "submitter": "Taesup Kim", "authors": "Taesup Kim, Sungwoong Kim, Yoshua Bengio", "title": "Visual Concept Reasoning Networks", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A split-transform-merge strategy has been broadly used as an architectural\nconstraint in convolutional neural networks for visual recognition tasks. It\napproximates sparsely connected networks by explicitly defining multiple\nbranches to simultaneously learn representations with different visual concepts\nor properties. Dependencies or interactions between these representations are\ntypically defined by dense and local operations, however, without any\nadaptiveness or high-level reasoning. In this work, we propose to exploit this\nstrategy and combine it with our Visual Concept Reasoning Networks (VCRNet) to\nenable reasoning between high-level visual concepts. We associate each branch\nwith a visual concept and derive a compact concept state by selecting a few\nlocal descriptors through an attention module. These concept states are then\nupdated by graph-based interaction and used to adaptively modulate the local\ndescriptors. We describe our proposed model by\nsplit-transform-attend-interact-modulate-merge stages, which are implemented by\nopting for a highly modularized architecture. Extensive experiments on visual\nrecognition tasks such as image classification, semantic segmentation, object\ndetection, scene recognition, and action recognition show that our proposed\nmodel, VCRNet, consistently improves the performance by increasing the number\nof parameters by less than 1%.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 20:02:40 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Kim", "Taesup", ""], ["Kim", "Sungwoong", ""], ["Bengio", "Yoshua", ""]]}, {"id": "2008.11789", "submitter": "Hang Chu", "authors": "Hang Chu, Shugao Ma, Fernando De la Torre, Sanja Fidler, Yaser Sheikh", "title": "Expressive Telepresence via Modular Codec Avatars", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  VR telepresence consists of interacting with another human in a virtual space\nrepresented by an avatar. Today most avatars are cartoon-like, but soon the\ntechnology will allow video-realistic ones. This paper aims in this direction\nand presents Modular Codec Avatars (MCA), a method to generate hyper-realistic\nfaces driven by the cameras in the VR headset. MCA extends traditional Codec\nAvatars (CA) by replacing the holistic models with a learned modular\nrepresentation. It is important to note that traditional person-specific CAs\nare learned from few training samples, and typically lack robustness as well as\nlimited expressiveness when transferring facial expressions. MCAs solve these\nissues by learning a modulated adaptive blending of different facial components\nas well as an exemplar-based latent alignment. We demonstrate that MCA achieves\nimproved expressiveness and robustness w.r.t to CA in a variety of real-world\ndatasets and practical scenarios. Finally, we showcase new applications in VR\ntelepresence enabled by the proposed model.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 20:16:43 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Chu", "Hang", ""], ["Ma", "Shugao", ""], ["De la Torre", "Fernando", ""], ["Fidler", "Sanja", ""], ["Sheikh", "Yaser", ""]]}, {"id": "2008.11833", "submitter": "Francisco Luongo", "authors": "Francisco Luongo (1), Ryan Hakim (2), Jessica H. Nguyen (2),\n  Animashree Anandkumar (3), Andrew J Hung (2) ((1) Department of Biology and\n  Biological Engineering, Caltech (2) Center for Robotic Simulation &\n  Education, Catherine & Joseph Aresty Department of Urology, USC Institute of\n  Urology, University of Southern California (3) Department of Computing &\n  Mathematical Sciences, Caltech)", "title": "Deep learning-based computer vision to recognize and classify suturing\n  gestures in robot-assisted surgery", "comments": "5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our previous work classified a taxonomy of suturing gestures during a\nvesicourethral anastomosis of robotic radical prostatectomy in association with\ntissue tears and patient outcomes. Herein, we train deep-learning based\ncomputer vision (CV) to automate the identification and classification of\nsuturing gestures for needle driving attempts. Using two independent raters, we\nmanually annotated live suturing video clips to label timepoints and gestures.\nIdentification (2395 videos) and classification (511 videos) datasets were\ncompiled to train CV models to produce two- and five-class label predictions,\nrespectively. Networks were trained on inputs of raw RGB pixels as well as\noptical flow for each frame. Each model was trained on 80/20 train/test splits.\nIn this study, all models were able to reliably predict either the presence of\na gesture (identification, AUC: 0.88) as well as the type of gesture\n(classification, AUC: 0.87) at significantly above chance levels. For both\ngesture identification and classification datasets, we observed no effect of\nrecurrent classification model choice (LSTM vs. convLSTM) on performance. Our\nresults demonstrate CV's ability to recognize features that not only can\nidentify the action of suturing but also distinguish between different\nclassifications of suturing gestures. This demonstrates the potential to\nutilize deep learning CV towards future automation of surgical skill\nassessment.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 21:45:04 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Luongo", "Francisco", ""], ["Hakim", "Ryan", ""], ["Nguyen", "Jessica H.", ""], ["Anandkumar", "Animashree", ""], ["Hung", "Andrew J", ""]]}, {"id": "2008.11842", "submitter": "Soumyadeep Dey", "authors": "Soumyadeep Dey, Jayanta Mukhopadhyay, Shamik Sural", "title": "Tabular Structure Detection from Document Images for Resource\n  Constrained Devices Using A Row Based Similarity Measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tabular structures are used to present crucial information in a structured\nand crisp manner. Detection of such regions is of great importance for proper\nunderstanding of a document. Tabular structures can be of various layouts and\ntypes. Therefore, detection of these regions is a hard problem. Most of the\nexisting techniques detect tables from a document image by using prior\nknowledge of the structures of the tables. However, these methods are not\napplicable for generalized tabular structures. In this work, we propose a\nsimilarity measure to find similarities between pairs of rows in a tabular\nstructure. This similarity measure is utilized to identify a tabular region.\nSince the tabular regions are detected exploiting the similarities among all\nrows, the method is inherently independent of layouts of the tabular regions\npresent in the training data. Moreover, the proposed similarity measure can be\nused to identify tabular regions without using large sets of parameters\nassociated with recent deep learning based methods. Thus, the proposed method\ncan easily be used with resource constrained devices such as mobile devices\nwithout much of an overhead.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 21:59:27 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Dey", "Soumyadeep", ""], ["Mukhopadhyay", "Jayanta", ""], ["Sural", "Shamik", ""]]}, {"id": "2008.11853", "submitter": "Jiawen Yao", "authors": "Jiawen Yao, Yu Shi, Le Lu, Jing Xiao, Ling Zhang", "title": "DeepPrognosis: Preoperative Prediction of Pancreatic Cancer Survival and\n  Surgical Margin via Contrast-Enhanced CT Imaging", "comments": "11 pages, 3 figures, Early accepted to Medical Image Computing and\n  Computer Assisted Interventions Conference (MICCAI) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pancreatic ductal adenocarcinoma (PDAC) is one of the most lethal cancers and\ncarries a dismal prognosis. Surgery remains the best chance of a potential cure\nfor patients who are eligible for initial resection of PDAC. However, outcomes\nvary significantly even among the resected patients of the same stage and\nreceived similar treatments. Accurate preoperative prognosis of resectable\nPDACs for personalized treatment is thus highly desired. Nevertheless, there\nare no automated methods yet to fully exploit the contrast-enhanced computed\ntomography (CE-CT) imaging for PDAC. Tumor attenuation changes across different\nCT phases can reflect the tumor internal stromal fractions and vascularization\nof individual tumors that may impact the clinical outcomes. In this work, we\npropose a novel deep neural network for the survival prediction of resectable\nPDAC patients, named as 3D Contrast-Enhanced Convolutional Long Short-Term\nMemory network(CE-ConvLSTM), which can derive the tumor attenuation signatures\nor patterns from CE-CT imaging studies. We present a multi-task CNN to\naccomplish both tasks of outcome and margin prediction where the network\nbenefits from learning the tumor resection margin related features to improve\nsurvival prediction. The proposed framework can improve the prediction\nperformances compared with existing state-of-the-art survival analysis\napproaches. The tumor signature built from our model has evidently added values\nto be combined with the existing clinical staging system.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 22:51:24 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Yao", "Jiawen", ""], ["Shi", "Yu", ""], ["Lu", "Le", ""], ["Xiao", "Jing", ""], ["Zhang", "Ling", ""]]}, {"id": "2008.11865", "submitter": "Vardan Papyan", "authors": "Vardan Papyan", "title": "Traces of Class/Cross-Class Structure Pervade Deep Learning Spectra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous researchers recently applied empirical spectral analysis to the\nstudy of modern deep learning classifiers. We identify and discuss an important\nformal class/cross-class structure and show how it lies at the origin of the\nmany visually striking features observed in deepnet spectra, some of which were\nreported in recent articles, others are unveiled here for the first time. These\ninclude spectral outliers, \"spikes\", and small but distinct continuous\ndistributions, \"bumps\", often seen beyond the edge of a \"main bulk\".\n  The significance of the cross-class structure is illustrated in three ways:\n(i) we prove the ratio of outliers to bulk in the spectrum of the Fisher\ninformation matrix is predictive of misclassification, in the context of\nmultinomial logistic regression; (ii) we demonstrate how, gradually with depth,\na network is able to separate class-distinctive information from class\nvariability, all while orthogonalizing the class-distinctive information; and\n(iii) we propose a correction to KFAC, a well-known second-order optimization\nalgorithm for training deepnets.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 00:08:49 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Papyan", "Vardan", ""]]}, {"id": "2008.11870", "submitter": "Zhuotun Zhu", "authors": "Zhuotun Zhu, Dakai Jin, Ke Yan, Tsung-Ying Ho, Xianghua Ye, Dazhou\n  Guo, Chun-Hung Chao, Jing Xiao, Alan Yuille, and Le Lu", "title": "Lymph Node Gross Tumor Volume Detection and Segmentation via\n  Distance-based Gating using 3D CT/PET Imaging in Radiotherapy", "comments": "MICCAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding, identifying and segmenting suspicious cancer metastasized lymph\nnodes from 3D multi-modality imaging is a clinical task of paramount\nimportance. In radiotherapy, they are referred to as Lymph Node Gross Tumor\nVolume (GTVLN). Determining and delineating the spread of GTVLN is essential in\ndefining the corresponding resection and irradiating regions for the downstream\nworkflows of surgical resection and radiotherapy of various cancers. In this\nwork, we propose an effective distance-based gating approach to simulate and\nsimplify the high-level reasoning protocols conducted by radiation oncologists,\nin a divide-and-conquer manner. GTVLN is divided into two subgroups of\ntumor-proximal and tumor-distal, respectively, by means of binary or soft\ndistance gating. This is motivated by the observation that each category can\nhave distinct though overlapping distributions of appearance, size and other LN\ncharacteristics. A novel multi-branch detection-by-segmentation network is\ntrained with each branch specializing on learning one GTVLN category features,\nand outputs from multi-branch are fused in inference. The proposed method is\nevaluated on an in-house dataset of $141$ esophageal cancer patients with both\nPET and CT imaging modalities. Our results validate significant improvements on\nthe mean recall from $72.5\\%$ to $78.2\\%$, as compared to previous\nstate-of-the-art work. The highest achieved GTVLN recall of $82.5\\%$ at $20\\%$\nprecision is clinically relevant and valuable since human observers tend to\nhave low sensitivity (around $80\\%$ for the most experienced radiation\noncologists, as reported by literature).\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 00:37:50 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Zhu", "Zhuotun", ""], ["Jin", "Dakai", ""], ["Yan", "Ke", ""], ["Ho", "Tsung-Ying", ""], ["Ye", "Xianghua", ""], ["Guo", "Dazhou", ""], ["Chao", "Chun-Hung", ""], ["Xiao", "Jing", ""], ["Yuille", "Alan", ""], ["Lu", "Le", ""]]}, {"id": "2008.11872", "submitter": "Wenceslao Villegas Marset", "authors": "Wenceslao Villegas Marset, Diego Sebasti\\'an P\\'erez, Carlos Ariel\n  D\\'iaz and Facundo Bromberg", "title": "Towards Practical 2D Grapevine Bud Detection with Fully Convolutional\n  Networks", "comments": "Revised version submitted to Journal of Computers and Electronics in\n  Agriculture November 13, 2020", "journal-ref": "Computers and Electronics in Agriculture, Volume 182, 2021,105947,\n  ISSN 0168-1699", "doi": "10.1016/j.compag.2020.105947", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Viticulture, visual inspection of the plant is a necessary task for\nmeasuring relevant variables. In many cases, these visual inspections are\nsusceptible to automation through computer vision methods. Bud detection is one\nsuch visual task, central for the measurement of important variables such as:\nmeasurement of bud sunlight exposure, autonomous pruning, bud counting,\ntype-of-bud classification, bud geometric characterization, internode length,\nbud area, and bud development stage, among others. This paper presents a\ncomputer method for grapevine bud detection based on a Fully Convolutional\nNetworks MobileNet architecture (FCN-MN). To validate its performance, this\narchitecture was compared in the detection task with a strong method for bud\ndetection, Scanning Windows (SW) based on a patch classifier, showing\nimprovements over three aspects of detection: segmentation, correspondence\nidentification and localization. The best version of FCN-MN showed a detection\nF1-measure of $88.6\\%$ (for true positives defined as detected components whose\nintersection-over-union with the true bud is above $0.5$), and false positives\nthat are small and near the true bud. Splits -- false positives overlapping the\ntrue bud -- showed a mean segmentation precision of $89.3\\% (21.7)$, while\nfalse alarms -- false positives not overlapping the true bud -- showed a mean\npixel area of only $8\\%$ the area of a true bud, and a distance (between mass\ncenters) of $1.1$ true bud diameters. The paper concludes by discussing how\nthese results for FCN-MN would produce sufficiently accurate measurements of\nbud variables such as bud number, bud area, and internode length, suggesting a\ngood performance in a practical setup.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 00:46:03 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 03:26:16 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Marset", "Wenceslao Villegas", ""], ["P\u00e9rez", "Diego Sebasti\u00e1n", ""], ["D\u00edaz", "Carlos Ariel", ""], ["Bromberg", "Facundo", ""]]}, {"id": "2008.11873", "submitter": "Taotao Jing", "authors": "Taotao Jing, Haifeng Xia, Zhengming Ding", "title": "Adaptively-Accumulated Knowledge Transfer for Partial Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial domain adaptation (PDA) attracts appealing attention as it deals with\na realistic and challenging problem when the source domain label space\nsubstitutes the target domain. Most conventional domain adaptation (DA) efforts\nconcentrate on learning domain-invariant features to mitigate the distribution\ndisparity across domains. However, it is crucial to alleviate the negative\ninfluence caused by the irrelevant source domain categories explicitly for PDA.\nIn this work, we propose an Adaptively-Accumulated Knowledge Transfer framework\n(A$^2$KT) to align the relevant categories across two domains for effective\ndomain adaptation. Specifically, an adaptively-accumulated mechanism is\nexplored to gradually filter out the most confident target samples and their\ncorresponding source categories, promoting positive transfer with more\nknowledge across two domains. Moreover, a dual distinct classifier architecture\nconsisting of a prototype classifier and a multilayer perceptron classifier is\nbuilt to capture intrinsic data distribution knowledge across domains from\nvarious perspectives. By maximizing the inter-class center-wise discrepancy and\nminimizing the intra-class sample-wise compactness, the proposed model is able\nto obtain more domain-invariant and task-specific discriminative\nrepresentations of the shared categories data. Comprehensive experiments on\nseveral partial domain adaptation benchmarks demonstrate the effectiveness of\nour proposed model, compared with the state-of-the-art PDA methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 00:53:43 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Jing", "Taotao", ""], ["Xia", "Haifeng", ""], ["Ding", "Zhengming", ""]]}, {"id": "2008.11878", "submitter": "Taotao Jing", "authors": "Taotao Jing, Zhengming Ding", "title": "Adversarial Dual Distinct Classifiers for Unsupervised Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised Domain adaptation (UDA) attempts to recognize the unlabeled\ntarget samples by building a learning model from a differently-distributed\nlabeled source domain. Conventional UDA concentrates on extracting\ndomain-invariant features through deep adversarial networks. However, most of\nthem seek to match the different domain feature distributions, without\nconsidering the task-specific decision boundaries across various classes. In\nthis paper, we propose a novel Adversarial Dual Distinct Classifiers Network\n(AD$^2$CN) to align the source and target domain data distribution\nsimultaneously with matching task-specific category boundaries. To be specific,\na domain-invariant feature generator is exploited to embed the source and\ntarget data into a latent common space with the guidance of discriminative\ncross-domain alignment. Moreover, we naturally design two different structure\nclassifiers to identify the unlabeled target samples over the supervision of\nthe labeled source domain data. Such dual distinct classifiers with various\narchitectures can capture diverse knowledge of the target data structure from\ndifferent perspectives. Extensive experimental results on several cross-domain\nvisual benchmarks prove the model's effectiveness by comparing it with other\nstate-of-the-art UDA.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 01:29:10 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Jing", "Taotao", ""], ["Ding", "Zhengming", ""]]}, {"id": "2008.11882", "submitter": "Xuewen Yang", "authors": "Xuewen Yang, Dongliang Xie, Xin Wang", "title": "Crossing-Domain Generative Adversarial Networks for Unsupervised\n  Multi-Domain Image-to-Image Translation", "comments": "accepted in proceedings of ACM Multimedia 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art techniques in Generative Adversarial Networks (GANs) have\nshown remarkable success in image-to-image translation from peer domain X to\ndomain Y using paired image data. However, obtaining abundant paired data is a\nnon-trivial and expensive process in the majority of applications. When there\nis a need to translate images across n domains, if the training is performed\nbetween every two domains, the complexity of the training will increase\nquadratically. Moreover, training with data from two domains only at a time\ncannot benefit from data of other domains, which prevents the extraction of\nmore useful features and hinders the progress of this research area. In this\nwork, we propose a general framework for unsupervised image-to-image\ntranslation across multiple domains, which can translate images from domain X\nto any a domain without requiring direct training between the two domains\ninvolved in image translation. A byproduct of the framework is the reduction of\ncomputing time and computing resources, since it needs less time than training\nthe domains in pairs as is done in state-of-the-art works. Our proposed\nframework consists of a pair of encoders along with a pair of GANs which learns\nhigh-level features across different domains to generate diverse and realistic\nsamples from. Our framework shows competing results on many image-to-image\ntasks compared with state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 01:54:07 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Yang", "Xuewen", ""], ["Xie", "Dongliang", ""], ["Wang", "Xin", ""]]}, {"id": "2008.11887", "submitter": "Muhammad Zaigham Zaheer", "authors": "Muhammad Zaigham Zaheer, Arif Mahmood, Hochul Shin, Seung-Ik Lee", "title": "A Self-Reasoning Framework for Anomaly Detection Using Video-Level\n  Labels", "comments": "Accepted to the IEEE Signal Processing Letters Journal", "journal-ref": null, "doi": "10.1109/LSP.2020.3025688", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomalous event detection in surveillance videos is a challenging and\npractical research problem among image and video processing community. Compared\nto the frame-level annotations of anomalous events, obtaining video-level\nannotations is quite fast and cheap though such high-level labels may contain\nsignificant noise. More specifically, an anomalous labeled video may actually\ncontain anomaly only in a short duration while the rest of the video frames may\nbe normal. In the current work, we propose a weakly supervised anomaly\ndetection framework based on deep neural networks which is trained in a\nself-reasoning fashion using only video-level labels. To carry out the\nself-reasoning based training, we generate pseudo labels by using binary\nclustering of spatio-temporal video features which helps in mitigating the\nnoise present in the labels of anomalous videos. Our proposed formulation\nencourages both the main network and the clustering to complement each other in\nachieving the goal of more accurate anomaly detection. The proposed framework\nhas been evaluated on publicly available real-world anomaly detection datasets\nincluding UCF-crime, ShanghaiTech and UCSD Ped2. The experiments demonstrate\nsuperiority of our proposed framework over the current state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 02:14:15 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Zaheer", "Muhammad Zaigham", ""], ["Mahmood", "Arif", ""], ["Shin", "Hochul", ""], ["Lee", "Seung-Ik", ""]]}, {"id": "2008.11894", "submitter": "Jingkang Yang", "authors": "Jingkang Yang, Litong Feng, Weirong Chen, Xiaopeng Yan, Huabin Zheng,\n  Ping Luo, Wayne Zhang", "title": "Webly Supervised Image Classification with Self-Contained Confidence", "comments": "16 pages, 4 figures, Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on webly supervised learning (WSL), where datasets are\nbuilt by crawling samples from the Internet and directly using search queries\nas web labels. Although WSL benefits from fast and low-cost data collection,\nnoises in web labels hinder better performance of the image classification\nmodel. To alleviate this problem, in recent works, self-label supervised loss\n$\\mathcal{L}_s$ is utilized together with webly supervised loss\n$\\mathcal{L}_w$. $\\mathcal{L}_s$ relies on pseudo labels predicted by the model\nitself. Since the correctness of the web label or pseudo label is usually on a\ncase-by-case basis for each web sample, it is desirable to adjust the balance\nbetween $\\mathcal{L}_s$ and $\\mathcal{L}_w$ on sample level. Inspired by the\nability of Deep Neural Networks (DNNs) in confidence prediction, we introduce\nSelf-Contained Confidence (SCC) by adapting model uncertainty for WSL setting,\nand use it to sample-wisely balance $\\mathcal{L}_s$ and $\\mathcal{L}_w$.\nTherefore, a simple yet effective WSL framework is proposed. A series of\nSCC-friendly regularization approaches are investigated, among which the\nproposed graph-enhanced mixup is the most effective method to provide\nhigh-quality confidence to enhance our framework. The proposed WSL framework\nhas achieved the state-of-the-art results on two large-scale WSL datasets,\nWebVision-1000 and Food101-N. Code is available at\nhttps://github.com/bigvideoresearch/SCC.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 02:49:51 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Yang", "Jingkang", ""], ["Feng", "Litong", ""], ["Chen", "Weirong", ""], ["Yan", "Xiaopeng", ""], ["Zheng", "Huabin", ""], ["Luo", "Ping", ""], ["Zhang", "Wayne", ""]]}, {"id": "2008.11898", "submitter": "Heshan Liu", "authors": "Ji Liu, Heshan Liu, Mang-Tik Chiu, Yu-Wing Tai, Chi-Keung Tang", "title": "Pose-Guided High-Resolution Appearance Transfer via Progressive Training", "comments": "10 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel pose-guided appearance transfer network for transferring a\ngiven reference appearance to a target pose in unprecedented image resolution\n(1024 * 1024), given respectively an image of the reference and target person.\nNo 3D model is used. Instead, our network utilizes dense local descriptors\nincluding local perceptual loss and local discriminators to refine details,\nwhich is trained progressively in a coarse-to-fine manner to produce the\nhigh-resolution output to faithfully preserve complex appearance of garment\ntextures and geometry, while hallucinating seamlessly the transferred\nappearances including those with dis-occlusion. Our progressive encoder-decoder\narchitecture can learn the reference appearance inherent in the input image at\nmultiple scales. Extensive experimental results on the Human3.6M dataset, the\nDeepFashion dataset, and our dataset collected from YouTube show that our model\nproduces high-quality images, which can be further utilized in useful\napplications such as garment transfer between people and pose-guided human\nvideo generation.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 03:18:44 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Liu", "Ji", ""], ["Liu", "Heshan", ""], ["Chiu", "Mang-Tik", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "2008.11901", "submitter": "Fang-Chieh Chou", "authors": "Sudeep Fadadu, Shreyash Pandey, Darshan Hegde, Yi Shi, Fang-Chieh\n  Chou, Nemanja Djuric, Carlos Vallespi-Gonzalez", "title": "Multi-View Fusion of Sensor Data for Improved Perception and Prediction\n  in Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end method for object detection and trajectory\nprediction utilizing multi-view representations of LiDAR returns. Our method\nbuilds on a state-of-the-art Bird's-Eye View (BEV) network that fuses voxelized\nfeatures from a sequence of historical LiDAR data as well as rasterized\nhigh-definition map to perform detection and prediction tasks. We extend the\nBEV network with additional LiDAR Range-View (RV) features that use the raw\nLiDAR information in its native, non-quantized representation. The RV feature\nmap is projected into BEV and fused with the BEV features computed from LiDAR\nand high-definition map. The fused features are then further processed to\noutput the final detections and trajectories, within a single end-to-end\ntrainable network. In addition, using this framework the RV fusion of LiDAR and\ncamera is performed in a straightforward and computational efficient manner.\nThe proposed approach improves the state-of-the-art on proprietary large-scale\nreal-world data collected by a fleet of self-driving vehicles, as well as on\nthe public nuScenes data set.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 03:32:25 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Fadadu", "Sudeep", ""], ["Pandey", "Shreyash", ""], ["Hegde", "Darshan", ""], ["Shi", "Yi", ""], ["Chou", "Fang-Chieh", ""], ["Djuric", "Nemanja", ""], ["Vallespi-Gonzalez", "Carlos", ""]]}, {"id": "2008.11911", "submitter": "Nimit Kalra", "authors": "Brady Zhou, Nimit Kalra, Philipp Kr\\\"ahenb\\\"uhl", "title": "Domain Adaptation Through Task Distillation", "comments": "Published in European Conference on Computer Vision (ECCV 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks devour millions of precisely annotated images to build their\ncomplex and powerful representations. Unfortunately, tasks like autonomous\ndriving have virtually no real-world training data. Repeatedly crashing a car\ninto a tree is simply too expensive. The commonly prescribed solution is\nsimple: learn a representation in simulation and transfer it to the real world.\nHowever, this transfer is challenging since simulated and real-world visual\nexperiences vary dramatically. Our core observation is that for certain tasks,\nsuch as image recognition, datasets are plentiful. They exist in any\ninteresting domain, simulated or real, and are easy to label and extend. We use\nthese recognition datasets to link up a source and target domain to transfer\nmodels between them in a task distillation framework. Our method can\nsuccessfully transfer navigation policies between drastically different\nsimulators: ViZDoom, SuperTuxKart, and CARLA. Furthermore, it shows promising\nresults on standard domain adaptation benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 04:44:49 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Zhou", "Brady", ""], ["Kalra", "Nimit", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""]]}, {"id": "2008.11917", "submitter": "Ai Takahashi", "authors": "Ai Takahashi, Yoshinori Koda, Koichi Ito, Takafumi Aoki", "title": "Fingerprint Feature Extraction by Combining Texture, Minutiae, and\n  Frequency Spectrum Using Multi-Task CNN", "comments": "IJCB2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although most fingerprint matching methods utilize minutia points and/or\ntexture of fingerprint images as fingerprint features, the frequency spectrum\nis also a useful feature since a fingerprint is composed of ridge patterns with\nits inherent frequency band. We propose a novel CNN-based method for extracting\nfingerprint features from texture, minutiae, and frequency spectrum. In order\nto extract effective texture features from local regions around the minutiae,\nthe minutia attention module is introduced to the proposed method. We also\npropose new data augmentation methods, which takes into account the\ncharacteristics of fingerprint images to increase the number of images during\ntraining since we use only a public dataset in training, which includes a few\nfingerprint classes. Through a set of experiments using FVC2004 DB1 and DB2, we\ndemonstrated that the proposed method exhibits the efficient performance on\nfingerprint verification compared with a commercial fingerprint matching\nsoftware and the conventional method.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 05:15:39 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Takahashi", "Ai", ""], ["Koda", "Yoshinori", ""], ["Ito", "Koichi", ""], ["Aoki", "Takafumi", ""]]}, {"id": "2008.11921", "submitter": "Yutaro Iwamoto", "authors": "Yutaro Iwamoto, Kyohei Takeda, Yinhao Li, Akihiko Shiino, Yen-Wei Chen", "title": "Unsupervised MRI Super-Resolution Using Deep External Learning and\n  Guided Residual Dense Network with Multimodal Image Priors", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques have led to state-of-the-art single image\nsuper-resolution (SISR) with natural images. Pairs of high-resolution (HR) and\nlow-resolution (LR) images are used to train the deep learning model (mapping\nfunction). These techniques have also been applied to medical image\nsuper-resolution (SR). Compared with natural images, medical images have\nseveral unique characteristics. First, there are no HR images for training in\nreal clinical applications because of the limitations of imaging systems and\nclinical requirements. Second, other modal HR images are available (e.g., HR\nT1-weighted images are available for enhancing LR T2-weighted images). In this\npaper, we propose an unsupervised SISR technique based on simple prior\nknowledge of the human anatomy; this technique does not require HR images for\ntraining. Furthermore, we present a guided residual dense network, which\nincorporates a residual dense network with a guided deep convolutional neural\nnetwork for enhancing the resolution of LR images by referring to different HR\nimages of the same subject. Experiments on a publicly available brain MRI\ndatabase showed that our proposed method achieves better performance than the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 05:46:31 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 08:44:30 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Iwamoto", "Yutaro", ""], ["Takeda", "Kyohei", ""], ["Li", "Yinhao", ""], ["Shiino", "Akihiko", ""], ["Chen", "Yen-Wei", ""]]}, {"id": "2008.11932", "submitter": "Ke Ma", "authors": "Ke Ma, Bo Zhao, Leonid Sigal", "title": "Attribute-guided image generation from layout", "comments": null, "journal-ref": "BMVC 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent approaches have achieved great success in image generation from\nstructured inputs, e.g., semantic segmentation, scene graph or layout. Although\nthese methods allow specification of objects and their locations at\nimage-level, they lack the fidelity and semantic control to specify visual\nappearance of these objects at an instance-level. To address this limitation,\nwe propose a new image generation method that enables instance-level attribute\ncontrol. Specifically, the input to our attribute-guided generative model is a\ntuple that contains: (1) object bounding boxes, (2) object categories and (3)\nan (optional) set of attributes for each object. The output is a generated\nimage where the requested objects are in the desired locations and have\nprescribed attributes. Several losses work collaboratively to encourage\naccurate, consistent and diverse image generation. Experiments on Visual Genome\ndataset demonstrate our model's capacity to control object-level attributes in\ngenerated images, and validate plausibility of disentangled object-attribute\nrepresentation in the image generation from layout task. Also, the generated\nimages from our model have higher resolution, object classification accuracy\nand consistency, as compared to the previous state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 06:22:14 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Ma", "Ke", ""], ["Zhao", "Bo", ""], ["Sigal", "Leonid", ""]]}, {"id": "2008.11935", "submitter": "Zhou Liu", "authors": "Zhou Liu, Lei Yu, Gui-Song Xia, Hong Sun", "title": "Mixed Noise Removal with Pareto Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Denoising images contaminated by the mixture of additive white Gaussian noise\n(AWGN) and impulse noise (IN) is an essential but challenging problem. The\npresence of impulsive disturbances inevitably affects the distribution of\nnoises and thus largely degrades the performance of traditional AWGN denoisers.\nExisting methods target to compensate the effects of IN by introducing a\nweighting matrix, which, however, is lack of proper priori and thus hard to be\naccurately estimated. To address this problem, we exploit the Pareto\ndistribution as the priori of the weighting matrix, based on which an accurate\nand robust weight estimator is proposed for mixed noise removal. Particularly,\na relatively small portion of pixels are assumed to be contaminated with IN,\nwhich should have weights with small values and then be penalized out. This\nphenomenon can be properly described by the Pareto distribution of type 1.\nTherefore, armed with the Pareto distribution, we formulate the problem of\nmixed noise removal in the Bayesian framework, where nonlocal self-similarity\npriori is further exploited by adopting nonlocal low rank approximation.\nCompared to existing methods, the proposed method can estimate the weighting\nmatrix adaptively, accurately, and robust for different level of noises, thus\ncan boost the denoising performance. Experimental results on widely used image\ndatasets demonstrate the superiority of our proposed method to the\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 06:35:15 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Liu", "Zhou", ""], ["Yu", "Lei", ""], ["Xia", "Gui-Song", ""], ["Sun", "Hong", ""]]}, {"id": "2008.11945", "submitter": "Yongquan Yang", "authors": "Yongquan Yang and Zhongxi Zheng", "title": "Moderately supervised learning: definition and framework", "comments": "7pages,2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning (SL) has achieved remarkable success in numerous\nartificial intelligence applications. In the current literature, by referring\nto the properties of the ground-truth labels prepared for a training data set,\nSL is roughly categorized as fully supervised learning (FSL) and weakly\nsupervised learning (WSL). However, solutions for various FSL tasks have shown\nthat the given ground-truth labels are not always learnable, and the target\ntransformation from the given ground-truth labels to learnable targets can\nsignificantly affect the performance of the final FSL solutions. Without\nconsidering the properties of the target transformation from the given\nground-truth labels to learnable targets, the roughness of the FSL category\nconceals some details that can be critical to building the optimal solutions\nfor some specific FSL tasks. Thus, it is desirable to reveal these details.\nThis article attempts to achieve this goal by expanding the categorization of\nFSL and investigating the subtype that plays the central role in FSL. Taking\ninto consideration the properties of the target transformation from the given\nground-truth labels to learnable targets, we first categorize FSL into three\nnarrower subtypes. Then, we focus on the subtype moderately supervised learning\n(MSL). MSL concerns the situation where the given ground-truth labels are\nideal, but due to the simplicity in annotation of the given ground-truth\nlabels, careful designs are required to transform the given ground-truth labels\ninto learnable targets. From the perspectives of definition and framework, we\ncomprehensively illustrate MSL to reveal what details are concealed by the\nroughness of the FSL category. Finally, discussions on the revealed details\nsuggest that MSL should be given more attention.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 06:53:53 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Yang", "Yongquan", ""], ["Zheng", "Zhongxi", ""]]}, {"id": "2008.11946", "submitter": "Daochang Liu", "authors": "Daochang Liu, Yuhui Wei, Tingting Jiang, Yizhou Wang, Rulin Miao, Fei\n  Shan, Ziyu Li", "title": "Unsupervised Surgical Instrument Segmentation via Anchor Generation and\n  Semantic Diffusion", "comments": "MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surgical instrument segmentation is a key component in developing\ncontext-aware operating rooms. Existing works on this task heavily rely on the\nsupervision of a large amount of labeled data, which involve laborious and\nexpensive human efforts. In contrast, a more affordable unsupervised approach\nis developed in this paper. To train our model, we first generate anchors as\npseudo labels for instruments and background tissues respectively by fusing\ncoarse handcrafted cues. Then a semantic diffusion loss is proposed to resolve\nthe ambiguity in the generated anchors via the feature correlation between\nadjacent video frames. In the experiments on the binary instrument segmentation\ntask of the 2017 MICCAI EndoVis Robotic Instrument Segmentation Challenge\ndataset, the proposed method achieves 0.71 IoU and 0.81 Dice score without\nusing a single manual annotation, which is promising to show the potential of\nunsupervised learning for surgical tool segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 06:54:27 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Liu", "Daochang", ""], ["Wei", "Yuhui", ""], ["Jiang", "Tingting", ""], ["Wang", "Yizhou", ""], ["Miao", "Rulin", ""], ["Shan", "Fei", ""], ["Li", "Ziyu", ""]]}, {"id": "2008.11954", "submitter": "Daochang Liu", "authors": "Daochang Liu, Tingting Jiang, Yizhou Wang, Rulin Miao, Fei Shan, Ziyu\n  Li", "title": "Surgical Skill Assessment on In-Vivo Clinical Data via the Clearness of\n  Operating Field", "comments": "MICCAI 2019", "journal-ref": null, "doi": "10.1007/978-3-030-32254-0_53", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surgical skill assessment is important for surgery training and quality\ncontrol. Prior works on this task largely focus on basic surgical tasks such as\nsuturing and knot tying performed in simulation settings. In contrast, surgical\nskill assessment is studied in this paper on a real clinical dataset, which\nconsists of fifty-seven in-vivo laparoscopic surgeries and corresponding skill\nscores annotated by six surgeons. From analyses on this dataset, the clearness\nof operating field (COF) is identified as a good proxy for overall surgical\nskills, given its strong correlation with overall skills and high\ninter-annotator consistency. Then an objective and automated framework based on\nneural network is proposed to predict surgical skills through the proxy of COF.\nThe neural network is jointly trained with a supervised regression loss and an\nunsupervised rank loss. In experiments, the proposed method achieves 0.55\nSpearman's correlation with the ground truth of overall technical skill, which\nis even comparable with the human performance of junior surgeons.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 07:12:16 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Liu", "Daochang", ""], ["Jiang", "Tingting", ""], ["Wang", "Yizhou", ""], ["Miao", "Rulin", ""], ["Shan", "Fei", ""], ["Li", "Ziyu", ""]]}, {"id": "2008.11961", "submitter": "Chen-Hsiu Huang", "authors": "Chen-Hsiu Huang, Ja-Ling Wu", "title": "Multi-task deep CNN model for no-reference image quality assessment on\n  smartphone camera photos", "comments": "Proceedings of Computer Vision & Graphic Image Processing (CVGIP),\n  Hsinchu, Taiwan, Aug. 16-18, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smartphone is the most successful consumer electronic product in today's\nmobile social network era. The smartphone camera quality and its image\npost-processing capability is the dominant factor that impacts consumer's\nbuying decision. However, the quality evaluation of photos taken from\nsmartphones remains a labor-intensive work and relies on professional\nphotographers and experts. As an extension of the prior CNN-based NR-IQA\napproach, we propose a multi-task deep CNN model with scene type detection as\nan auxiliary task. With the shared model parameters in the convolution layer,\nthe learned feature maps could become more scene-relevant and enhance the\nperformance. The evaluation result shows improved SROCC performance compared to\ntraditional NR-IQA methods and single task CNN-based models.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 07:33:05 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Huang", "Chen-Hsiu", ""], ["Wu", "Ja-Ling", ""]]}, {"id": "2008.11976", "submitter": "Ankan Bansal", "authors": "Ankan Bansal, Yuting Zhang, Rama Chellappa", "title": "Visual Question Answering on Image Sets", "comments": "Conference paper at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the task of Image-Set Visual Question Answering (ISVQA), which\ngeneralizes the commonly studied single-image VQA problem to multi-image\nsettings. Taking a natural language question and a set of images as input, it\naims to answer the question based on the content of the images. The questions\ncan be about objects and relationships in one or more images or about the\nentire scene depicted by the image set. To enable research in this new topic,\nwe introduce two ISVQA datasets - indoor and outdoor scenes. They simulate the\nreal-world scenarios of indoor image collections and multiple car-mounted\ncameras, respectively. The indoor-scene dataset contains 91,479 human annotated\nquestions for 48,138 image sets, and the outdoor-scene dataset has 49,617\nquestions for 12,746 image sets. We analyze the properties of the two datasets,\nincluding question-and-answer distributions, types of questions, biases in\ndataset, and question-image dependencies. We also build new baseline models to\ninvestigate new research challenges in ISVQA.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 08:03:32 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Bansal", "Ankan", ""], ["Zhang", "Yuting", ""], ["Chellappa", "Rama", ""]]}, {"id": "2008.11977", "submitter": "Jonghyun Kim", "authors": "Jonghyun Kim, Gen Li, Inyong Yun, Cheolkon Jung, and Joongkyu Kim", "title": "Edge and Identity Preserving Network for Face Super-Resolution", "comments": "Neurocomputing'2021", "journal-ref": null, "doi": "10.1016/j.neucom.2021.03.048", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Face super-resolution (SR) has become an indispensable function in security\nsolutions such as video surveillance and identification system, but the\ndistortion in facial components is a great challenge in it. Most\nstate-of-the-art methods have utilized facial priors with deep neural networks.\nThese methods require extra labels, longer training time, and larger\ncomputation memory. In this paper, we propose a novel Edge and Identity\nPreserving Network for Face SR Network, named as EIPNet, to minimize the\ndistortion by utilizing a lightweight edge block and identity information. We\npresent an edge block to extract perceptual edge information, and concatenate\nit to the original feature maps in multiple scales. This structure\nprogressively provides edge information in reconstruction to aggregate local\nand global structural information. Moreover, we define an identity loss\nfunction to preserve identification of SR images. The identity loss function\ncompares feature distributions between SR images and their ground truth to\nrecover identities in SR images. In addition, we provide a\nluminance-chrominance error (LCE) to separately infer brightness and color\ninformation in SR images. The LCE method not only reduces the dependency of\ncolor information by dividing brightness and color components but also enables\nour network to reflect differences between SR images and their ground truth in\ntwo color spaces of RGB and YUV. The proposed method facilitates the proposed\nSR network to elaborately restore facial components and generate high quality\n8x scaled SR images with a lightweight network structure. Furthermore, our\nnetwork is able to reconstruct an 128x128 SR image with 215 fps on a GTX 1080Ti\nGPU. Extensive experiments demonstrate that our network qualitatively and\nquantitatively outperforms state-of-the-art methods on two challenging\ndatasets: CelebA and VGGFace2.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 08:04:57 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 02:17:03 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Kim", "Jonghyun", ""], ["Li", "Gen", ""], ["Yun", "Inyong", ""], ["Jung", "Cheolkon", ""], ["Kim", "Joongkyu", ""]]}, {"id": "2008.11988", "submitter": "Guang Yu", "authors": "Guang Yu, Siqi Wang, Zhiping Cai, En Zhu, Chuanfu Xu, Jianping Yin,\n  Marius Kloft", "title": "Cloze Test Helps: Effective Video Anomaly Detection via Learning to\n  Complete Video Events", "comments": "To be published as an oral paper in Proceedings of the 28th ACM\n  International Conference on Multimedia (ACM MM '20). 9 pages, 7 figures", "journal-ref": null, "doi": "10.1145/3394171.3413973", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a vital topic in media content interpretation, video anomaly detection\n(VAD) has made fruitful progress via deep neural network (DNN). However,\nexisting methods usually follow a reconstruction or frame prediction routine.\nThey suffer from two gaps: (1) They cannot localize video activities in a both\nprecise and comprehensive manner. (2) They lack sufficient abilities to utilize\nhigh-level semantics and temporal context information. Inspired by\nfrequently-used cloze test in language study, we propose a brand-new VAD\nsolution named Video Event Completion (VEC) to bridge gaps above: First, we\npropose a novel pipeline to achieve both precise and comprehensive enclosure of\nvideo activities. Appearance and motion are exploited as mutually complimentary\ncues to localize regions of interest (RoIs). A normalized spatio-temporal cube\n(STC) is built from each RoI as a video event, which lays the foundation of VEC\nand serves as a basic processing unit. Second, we encourage DNN to capture\nhigh-level semantics by solving a visual cloze test. To build such a visual\ncloze test, a certain patch of STC is erased to yield an incomplete event (IE).\nThe DNN learns to restore the original video event from the IE by inferring the\nmissing patch. Third, to incorporate richer motion dynamics, another DNN is\ntrained to infer erased patches' optical flow. Finally, two ensemble strategies\nusing different types of IE and modalities are proposed to boost VAD\nperformance, so as to fully exploit the temporal context and modality\ninformation for VAD. VEC can consistently outperform state-of-the-art methods\nby a notable margin (typically 1.5%-5% AUROC) on commonly-used VAD benchmarks.\nOur codes and results can be verified at github.com/yuguangnudt/VEC_VAD.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 08:32:51 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Yu", "Guang", ""], ["Wang", "Siqi", ""], ["Cai", "Zhiping", ""], ["Zhu", "En", ""], ["Xu", "Chuanfu", ""], ["Yin", "Jianping", ""], ["Kloft", "Marius", ""]]}, {"id": "2008.11995", "submitter": "Amelie Royer", "authors": "Amelie Royer and Christoph H. Lampert", "title": "A Flexible Selection Scheme for Minimum-Effort Transfer Learning", "comments": "WACV 2020", "journal-ref": null, "doi": "10.1109/WACV45572.2020.9093635", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-tuning is a popular way of exploiting knowledge contained in a\npre-trained convolutional network for a new visual recognition task. However,\nthe orthogonal setting of transferring knowledge from a pretrained network to a\nvisually different yet semantically close source is rarely considered: This\ncommonly happens with real-life data, which is not necessarily as clean as the\ntraining source (noise, geometric transformations, different modalities, etc.).\nTo tackle such scenarios, we introduce a new, generalized form of fine-tuning,\ncalled flex-tuning, in which any individual unit (e.g. layer) of a network can\nbe tuned, and the most promising one is chosen automatically. In order to make\nthe method appealing for practical use, we propose two lightweight and faster\nselection procedures that prove to be good approximations in practice. We study\nthese selection criteria empirically across a variety of domain shifts and data\nscarcity scenarios, and show that fine-tuning individual units, despite its\nsimplicity, yields very good results as an adaptation technique. As it turns\nout, in contrast to common practice, rather than the last fully-connected unit\nit is best to tune an intermediate or early one in many domain-shift scenarios,\nwhich is accurately detected by flex-tuning.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 08:57:30 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Royer", "Amelie", ""], ["Lampert", "Christoph H.", ""]]}, {"id": "2008.12002", "submitter": "Victor Joos De Ter Beerst", "authors": "Antoine Vanderschueren, Victor Joos, Christophe De Vleeschouwer", "title": "How semantic and geometric information mutually reinforce each other in\n  ToF object localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to localize a 3D object from the intensity and\ndepth information images provided by a Time-of-Flight (ToF) sensor. Our method\nuses two CNNs. The first one uses raw depth and intensity images as input, to\nsegment the floor pixels, from which the extrinsic parameters of the camera are\nestimated. The second CNN is in charge of segmenting the object-of-interest. As\na main innovation, it exploits the calibration estimated from the prediction of\nthe first CNN to represent the geometric depth information in a coordinate\nsystem that is attached to the ground, and is thus independent of the camera\nelevation. In practice, both the height of pixels with respect to the ground,\nand the orientation of normals to the point cloud are provided as input to the\nsecond CNN. Given the segmentation predicted by the second CNN, the object is\nlocalized based on point cloud alignment with a reference model. Our\nexperiments demonstrate that our proposed two-step approach improves\nsegmentation and localization accuracy by a significant margin compared to a\nconventional CNN architecture, ignoring calibration and height maps, but also\ncompared to PointNet++.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 09:13:26 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Vanderschueren", "Antoine", ""], ["Joos", "Victor", ""], ["De Vleeschouwer", "Christophe", ""]]}, {"id": "2008.12024", "submitter": "Alexander Sch\\\"afer", "authors": "Jason Rambach, Gergana Lilligreen, Alexander Sch\\\"afer, Ramya\n  Bankanal, Alexander Wiebel, Didier Stricker", "title": "A survey on applications of augmented, mixed and virtual reality for\n  nature and environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.CY cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented reality (AR), virtual reality (VR) and mixed reality (MR) are\ntechnologies of great potential due to the engaging and enriching experiences\nthey are capable of providing. Their use is rapidly increasing in diverse\nfields such as medicine, manufacturing or entertainment. However, the\npossibilities that AR, VR and MR offer in the area of environmental\napplications are not yet widely explored. In this paper we present the outcome\nof a survey meant to discover and classify existing AR/VR/MR applications that\ncan benefit the environment or increase awareness on environmental issues. We\nperformed an exhaustive search over several online publication access platforms\nand past proceedings of major conferences in the fields of AR/VR/MR. Identified\nrelevant papers were filtered based on novelty, technical soundness, impact and\ntopic relevance, and classified into different categories. Referring to the\nselected papers, we discuss how the applications of each category are\ncontributing to environmental protection, preservation and sensitization\npurposes. We further analyse these approaches as well as possible future\ndirections in the scope of existing and upcoming AR/VR/MR enabling\ntechnologies.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 09:59:27 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 08:47:26 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Rambach", "Jason", ""], ["Lilligreen", "Gergana", ""], ["Sch\u00e4fer", "Alexander", ""], ["Bankanal", "Ramya", ""], ["Wiebel", "Alexander", ""], ["Stricker", "Didier", ""]]}, {"id": "2008.12037", "submitter": "Ekaterina Iakovleva", "authors": "Ekaterina Iakovleva, Jakob Verbeek, Karteek Alahari", "title": "Meta-Learning with Shared Amortized Variational Inference", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel amortized variational inference scheme for an empirical\nBayes meta-learning model, where model parameters are treated as latent\nvariables. We learn the prior distribution over model parameters conditioned on\nlimited training data using a variational autoencoder approach. Our framework\nproposes sharing the same amortized inference network between the conditional\nprior and variational posterior distributions over the model parameters. While\nthe posterior leverages both the labeled support and query data, the\nconditional prior is based only on the labeled support data. We show that in\nearlier work, relying on Monte-Carlo approximation, the conditional prior\ncollapses to a Dirac delta function. In contrast, our variational approach\nprevents this collapse and preserves uncertainty over the model parameters. We\nevaluate our approach on the miniImageNet, CIFAR-FS and FC100 datasets, and\npresent results demonstrating its advantages over previous work.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 10:28:13 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Iakovleva", "Ekaterina", ""], ["Verbeek", "Jakob", ""], ["Alahari", "Karteek", ""]]}, {"id": "2008.12046", "submitter": "Claudio Ferrari", "authors": "Claudio Ferrari, Lorenzo Berlincioni, Marco Bertini, Alberto Del Bimbo", "title": "Inner Eye Canthus Localization for Human Body Temperature Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an automatic approach for localizing the inner eye\ncanthus in thermal face images. We first coarsely detect 5 facial keypoints\ncorresponding to the center of the eyes, the nosetip and the ears. Then we\ncompute a sparse 2D-3D points correspondence using a 3D Morphable Face Model\n(3DMM). This correspondence is used to project the entire 3D face onto the\nimage, and subsequently locate the inner eye canthus. Detecting this location\nallows to obtain the most precise body temperature measurement for a person\nusing a thermal camera. We evaluated the approach on a thermal face dataset\nprovided with manually annotated landmarks. However, such manual annotations\nare normally conceived to identify facial parts such as eyes, nose and mouth,\nand are not specifically tailored for localizing the eye canthus region. As\nadditional contribution, we enrich the original dataset by using the annotated\nlandmarks to deform and project the 3DMM onto the images. Then, by manually\nselecting a small region corresponding to the eye canthus, we enrich the\ndataset with additional annotations. By using the manual landmarks, we ensure\nthe correctness of the 3DMM projection, which can be used as ground-truth for\nfuture evaluations. Moreover, we supply the dataset with the 3D head poses and\nper-point visibility masks for detecting self-occlusions. The data will be\npublicly released.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 10:47:57 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Ferrari", "Claudio", ""], ["Berlincioni", "Lorenzo", ""], ["Bertini", "Marco", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "2008.12052", "submitter": "Junjie Huang", "authors": "Zhibo Zou, Junjie Huang, Ping Luo", "title": "Compensation Tracker: Reprocessing for Lost Object", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At present, the main research direction of multi-object tracking framework is\ntracking by detection. Although the detection-based tracking framework can\nachieve good results, it is very dependent on the performance of the detector.\nThe tracking results will be affected to a certain extent when the detector has\nthe behaviors of omission and error detection. Therefore, in order to solve the\nproblem of missing detection, we designs a compensation tracker based on motion\ncompensation and objects selection. Besides the tracker can be embedded into\nother non-end-to-end tracking frameworks. Experiments show that after using the\ncompensation tracker designed in this paper, evaluation indicators have\nimproved in varying degrees on MOT Challenge datasets. With limit cost, the\ncompensation tracker haves reached 66% MOTA and 67% IDF1 in the 2020 datasets\nof dense scenarios. This shows that the proposed method can effectively improve\nthe tracking performance of the model.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 10:59:54 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 04:48:44 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2021 13:29:01 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Zou", "Zhibo", ""], ["Huang", "Junjie", ""], ["Luo", "Ping", ""]]}, {"id": "2008.12065", "submitter": "Md Abul Bashar", "authors": "Md Abul Bashar, Astin-Walmsley Kieren, Heath Kerina, Richi Nayak", "title": "Propensity-to-Pay: Machine Learning for Estimating Prediction\n  Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting a customer's propensity-to-pay at an early point in the revenue\ncycle can provide organisations many opportunities to improve the customer\nexperience, reduce hardship and reduce the risk of impaired cash flow and\noccurrence of bad debt. With the advancements in data science; machine learning\ntechniques can be used to build models to accurately predict a customer's\npropensity-to-pay. Creating effective machine learning models without access to\nlarge and detailed datasets presents some significant challenges. This paper\npresents a case-study, conducted on a dataset from an energy organisation, to\nexplore the uncertainty around the creation of machine learning models that are\nable to predict residential customers entering financial hardship which then\nreduces their ability to pay energy bills. Incorrect predictions can result in\ninefficient resource allocation and vulnerable customers not being proactively\nidentified. This study investigates machine learning models' ability to\nconsider different contexts and estimate the uncertainty in the prediction.\nSeven models from four families of machine learning algorithms are investigated\nfor their novel utilisation. A novel concept of utilising a Baysian Neural\nNetwork to the binary classification problem of propensity-to-pay energy bills\nis proposed and explored for deployment.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 11:49:25 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Bashar", "Md Abul", ""], ["Kieren", "Astin-Walmsley", ""], ["Kerina", "Heath", ""], ["Nayak", "Richi", ""]]}, {"id": "2008.12066", "submitter": "Jaeyeon Kim", "authors": "Jaeyeon Kim, Binh-Son Hua, Duc Thanh Nguyen, Sai-Kit Yeung", "title": "Minimal Adversarial Examples for Deep Learning on 3D Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With recent developments of convolutional neural networks, deep learning for\n3D point clouds has shown significant progress in various 3D scene\nunderstanding tasks, e.g., object recognition, object detection. In a\nsafety-critical environment, it is however not well understood how such deep\nlearning models are vulnerable to adversarial examples. In this work, we\nexplore adversarial attacks for point cloud-based neural networks. We propose a\ngeneral formulation for adversarial point cloud generation via $\\ell_0$-norm\noptimisation. Our method generates adversarial examples by attacking the\nclassification ability of the point cloud-based networks while considering the\nperceptibility of the examples and ensuring the minimum level of point\nmanipulations. The proposed method is general and can be realised in different\nattack strategies. Experimental results show that our method achieves the\nstate-of-the-art performance with higher than 89% and 90% of attack success on\nsynthetic and real-world data respectively, while manipulating only about 4% of\nthe total points.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 11:50:45 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 14:14:58 GMT"}, {"version": "v3", "created": "Sat, 27 Mar 2021 11:06:10 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Kim", "Jaeyeon", ""], ["Hua", "Binh-Son", ""], ["Nguyen", "Duc Thanh", ""], ["Yeung", "Sai-Kit", ""]]}, {"id": "2008.12085", "submitter": "Juan Diego Ortega", "authors": "Juan Diego Ortega, Neslihan Kose, Paola Ca\\~nas, Min-An Chao,\n  Alexander Unnervik, Marcos Nieto, Oihana Otaegui, Luis Salgado", "title": "DMD: A Large-Scale Multi-Modal Driver Monitoring Dataset for Attention\n  and Alertness Analysis", "comments": "Accepted to ECCV 2020 workshop - Assistive Computer Vision and\n  Robotics", "journal-ref": null, "doi": "10.1007/978-3-030-66823-5_23", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Vision is the richest and most cost-effective technology for Driver\nMonitoring Systems (DMS), especially after the recent success of Deep Learning\n(DL) methods. The lack of sufficiently large and comprehensive datasets is\ncurrently a bottleneck for the progress of DMS development, crucial for the\ntransition of automated driving from SAE Level-2 to SAE Level-3. In this paper,\nwe introduce the Driver Monitoring Dataset (DMD), an extensive dataset which\nincludes real and simulated driving scenarios: distraction, gaze allocation,\ndrowsiness, hands-wheel interaction and context data, in 41 hours of RGB, depth\nand IR videos from 3 cameras capturing face, body and hands of 37 drivers. A\ncomparison with existing similar datasets is included, which shows the DMD is\nmore extensive, diverse, and multi-purpose. The usage of the DMD is illustrated\nby extracting a subset of it, the dBehaviourMD dataset, containing 13\ndistraction activities, prepared to be used in DL training processes.\nFurthermore, we propose a robust and real-time driver behaviour recognition\nsystem targeting a real-world application that can run on cost-efficient\nCPU-only platforms, based on the dBehaviourMD. Its performance is evaluated\nwith different types of fusion strategies, which all reach enhanced accuracy\nstill providing real-time response.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 12:33:54 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Ortega", "Juan Diego", ""], ["Kose", "Neslihan", ""], ["Ca\u00f1as", "Paola", ""], ["Chao", "Min-An", ""], ["Unnervik", "Alexander", ""], ["Nieto", "Marcos", ""], ["Otaegui", "Oihana", ""], ["Salgado", "Luis", ""]]}, {"id": "2008.12094", "submitter": "Benlin Liu", "authors": "Benlin Liu, Yongming Rao, Jiwen Lu, Jie Zhou, Cho-jui Hsieh", "title": "MetaDistiller: Network Self-Boosting via Meta-Learned Top-Down\n  Distillation", "comments": "Accepted by ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Distillation (KD) has been one of the most popu-lar methods to\nlearn a compact model. However, it still suffers from highdemand in time and\ncomputational resources caused by sequential train-ing pipeline. Furthermore,\nthe soft targets from deeper models do notoften serve as good cues for the\nshallower models due to the gap of com-patibility. In this work, we consider\nthese two problems at the same time.Specifically, we propose that better soft\ntargets with higher compatibil-ity can be generated by using a label generator\nto fuse the feature mapsfrom deeper stages in a top-down manner, and we can\nemploy the meta-learning technique to optimize this label generator. Utilizing\nthe softtargets learned from the intermediate feature maps of the model, we\ncanachieve better self-boosting of the network in comparison with the\nstate-of-the-art. The experiments are conducted on two standard\nclassificationbenchmarks, namely CIFAR-100 and ILSVRC2012. We test various\nnet-work architectures to show the generalizability of our MetaDistiller.\nTheexperiments results on two datasets strongly demonstrate the effective-ness\nof our method.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 13:04:27 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Liu", "Benlin", ""], ["Rao", "Yongming", ""], ["Lu", "Jiwen", ""], ["Zhou", "Jie", ""], ["Hsieh", "Cho-jui", ""]]}, {"id": "2008.12103", "submitter": "Syed Ali Hassan", "authors": "Farooque Hassan Kumbhar, Syed Ali Hassan, Soo Young Shin", "title": "New Normal: Cooperative Paradigm for Covid-19 Timely Detection and\n  Containment using Internet of Things and Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spread of the novel coronavirus (COVID-19) has caused trillions of\ndollars in damages to the governments and health authorities by affecting the\nglobal economies. The purpose of this study is to introduce a connected smart\nparadigm that not only detects the possible spread of viruses but also helps to\nrestart businesses/economies, and resume social life. We are proposing a\nconnected Internet of Things ( IoT) based paradigm that makes use of object\ndetection based on convolution neural networks (CNN), smart wearable and\nconnected e-health to avoid current and future outbreaks. First, connected\nsurveillance cameras feed continuous video stream to the server where we detect\nthe inter-object distance to identify any social distancing violations. A\nviolation activates area-based monitoring of active smartphone users and their\ncurrent state of illness. In case a confirmed patient or a person with high\nsymptoms is present, the system tracks exposed and infected people and\nappropriate measures are put into actions. We evaluated the proposed scheme for\nsocial distancing violation detection using YOLO (you only look once) v2 and\nv3, and for infection spread tracing using Python simulation.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 14:33:53 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Kumbhar", "Farooque Hassan", ""], ["Hassan", "Syed Ali", ""], ["Shin", "Soo Young", ""]]}, {"id": "2008.12134", "submitter": "Keren Fu", "authors": "Keren Fu, Deng-Ping Fan, Ge-Peng Ji, Qijun Zhao, Jianbing Shen, Ce Zhu", "title": "Siamese Network for RGB-D Salient Object Detection and Beyond", "comments": "arXiv admin note: substantial text overlap with arXiv:2004.08515", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing RGB-D salient object detection (SOD) models usually treat RGB and\ndepth as independent information and design separate networks for feature\nextraction from each. Such schemes can easily be constrained by a limited\namount of training data or over-reliance on an elaborately designed training\nprocess. Inspired by the observation that RGB and depth modalities actually\npresent certain commonality in distinguishing salient objects, a novel joint\nlearning and densely cooperative fusion (JL-DCF) architecture is designed to\nlearn from both RGB and depth inputs through a shared network backbone, known\nas the Siamese architecture. In this paper, we propose two effective\ncomponents: joint learning (JL), and densely cooperative fusion (DCF). The JL\nmodule provides robust saliency feature learning by exploiting cross-modal\ncommonality via a Siamese network, while the DCF module is introduced for\ncomplementary feature discovery. Comprehensive experiments using five popular\nmetrics show that the designed framework yields a robust RGB-D saliency\ndetector with good generalization. As a result, JL-DCF significantly advances\nthe state-of-the-art models by an average of ~2.0% (max F-measure) across seven\nchallenging datasets. In addition, we show that JL-DCF is readily applicable to\nother related multi-modal detection tasks, including RGB-T (thermal infrared)\nSOD and video SOD, achieving comparable or even better performance against\nstate-of-the-art methods. We also link JL-DCF to the RGB-D semantic\nsegmentation field, showing its capability of outperforming several semantic\nsegmentation models on the task of RGB-D SOD. These facts further confirm that\nthe proposed framework could offer a potential solution for various\napplications and provide more insight into the cross-modal complementarity\ntask.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 06:01:05 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 05:52:03 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Fu", "Keren", ""], ["Fan", "Deng-Ping", ""], ["Ji", "Ge-Peng", ""], ["Zhao", "Qijun", ""], ["Shen", "Jianbing", ""], ["Zhu", "Ce", ""]]}, {"id": "2008.12141", "submitter": "Sherin Muckatira", "authors": "Sherin Muckatira", "title": "Properties Of Winning Tickets On Skin Lesion Classification", "comments": "11 pages, 11 figures, presented at WiCV workshop at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin cancer affects a large population every year -- automated skin cancer\ndetection algorithms can thus greatly help clinicians. Prior efforts involving\ndeep learning models have high detection accuracy. However, most of the models\nhave a large number of parameters, with some works even using an ensemble of\nmodels to achieve good accuracy. In this paper, we investigate a recently\nproposed pruning technique called Lottery Ticket Hypothesis. We find that\niterative pruning of the network resulted in improved accuracy, compared to\nthat of the unpruned network, implying that -- the lottery ticket hypothesis\ncan be applied to the problem of skin cancer detection and this hypothesis can\nresult in a smaller network for inference. We also examine the accuracy across\nsub-groups -- created by gender and age -- and it was found that some\nsub-groups show a larger increase in accuracy than others.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 21:36:56 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Muckatira", "Sherin", ""]]}, {"id": "2008.12165", "submitter": "Janine Thoma", "authors": "Janine Thoma, Danda Pani Paudel, Ajad Chhatkuli, Luc Van Gool", "title": "Learning Condition Invariant Features for Retrieval-Based Localization\n  from 1M Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image features for retrieval-based localization must be invariant to dynamic\nobjects (e.g. cars) as well as seasonal and daytime changes. Such invariances\nare, up to some extent, learnable with existing methods using triplet-like\nlosses, given a large number of diverse training images. However, due to the\nhigh algorithmic training complexity, there exists insufficient comparison\nbetween different loss functions on large datasets. In this paper, we train and\nevaluate several localization methods on three different benchmark datasets,\nincluding Oxford RobotCar with over one million images. This large scale\nevaluation yields valuable insights into the generalizability and performance\nof retrieval-based localization. Based on our findings, we develop a novel\nmethod for learning more accurate and better generalizing localization\nfeatures. It consists of two main contributions: (i) a feature volume-based\nloss function, and (ii) hard positive and pairwise negative mining. On the\nchallenging Oxford RobotCar night condition, our method outperforms the\nwell-known triplet loss by 24.4% in localization accuracy within 5m.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 14:46:22 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 23:43:12 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Thoma", "Janine", ""], ["Paudel", "Danda Pani", ""], ["Chhatkuli", "Ajad", ""], ["Van Gool", "Luc", ""]]}, {"id": "2008.12197", "submitter": "Ke Mei", "authors": "Ke Mei, Chuang Zhu, Jiaqi Zou, Shanghang Zhang", "title": "Instance Adaptive Self-Training for Unsupervised Domain Adaptation", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The divergence between labeled training data and unlabeled testing data is a\nsignificant challenge for recent deep learning models. Unsupervised domain\nadaptation (UDA) attempts to solve such a problem. Recent works show that\nself-training is a powerful approach to UDA. However, existing methods have\ndifficulty in balancing scalability and performance. In this paper, we propose\nan instance adaptive self-training framework for UDA on the task of semantic\nsegmentation. To effectively improve the quality of pseudo-labels, we develop a\nnovel pseudo-label generation strategy with an instance adaptive selector.\nBesides, we propose the region-guided regularization to smooth the pseudo-label\nregion and sharpen the non-pseudo-label region. Our method is so concise and\nefficient that it is easy to be generalized to other unsupervised domain\nadaptation methods. Experiments on 'GTA5 to Cityscapes' and 'SYNTHIA to\nCityscapes' demonstrate the superior performance of our approach compared with\nthe state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 15:50:27 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Mei", "Ke", ""], ["Zhu", "Chuang", ""], ["Zou", "Jiaqi", ""], ["Zhang", "Shanghang", ""]]}, {"id": "2008.12205", "submitter": "Lei Li", "authors": "Lei Li, Veronika A. Zimmer, Wangbin Ding, Fuping Wu, Liqin Huang,\n  Julia A. Schnabel, Xiahai Zhuang", "title": "Random Style Transfer based Domain Generalization Networks Integrating\n  Shape and Spatial Information", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL)-based models have demonstrated good performance in medical\nimage segmentation. However, the models trained on a known dataset often fail\nwhen performed on an unseen dataset collected from different centers, vendors\nand disease populations. In this work, we present a random style transfer\nnetwork to tackle the domain generalization problem for multi-vendor and center\ncardiac image segmentation. Style transfer is used to generate training data\nwith a wider distribution/ heterogeneity, namely domain augmentation. As the\ntarget domain could be unknown, we randomly generate a modality vector for the\ntarget modality in the style transfer stage, to simulate the domain shift for\nunknown domains. The model can be trained in a semi-supervised manner by\nsimultaneously optimizing a supervised segmentation and an unsupervised style\ntranslation objective. Besides, the framework incorporates the spatial\ninformation and shape prior of the target by introducing two regularization\nterms. We evaluated the proposed framework on 40 subjects from the M\\&Ms\nchallenge2020, and obtained promising performance in the segmentation for data\nfrom unknown vendors and centers.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 16:00:21 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 11:18:42 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Li", "Lei", ""], ["Zimmer", "Veronika A.", ""], ["Ding", "Wangbin", ""], ["Wu", "Fuping", ""], ["Huang", "Liqin", ""], ["Schnabel", "Julia A.", ""], ["Zhuang", "Xiahai", ""]]}, {"id": "2008.12262", "submitter": "Yuval Nirkin", "authors": "Yuval Nirkin, Lior Wolf, Yosi Keller and Tal Hassner", "title": "DeepFake Detection Based on the Discrepancy Between the Face and its\n  Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for detecting face swapping and other identity\nmanipulations in single images. Face swapping methods, such as DeepFake,\nmanipulate the face region, aiming to adjust the face to the appearance of its\ncontext, while leaving the context unchanged. We show that this modus operandi\nproduces discrepancies between the two regions. These discrepancies offer\nexploitable telltale signs of manipulation. Our approach involves two networks:\n(i) a face identification network that considers the face region bounded by a\ntight semantic segmentation, and (ii) a context recognition network that\nconsiders the face context (e.g., hair, ears, neck). We describe a method which\nuses the recognition signals from our two networks to detect such\ndiscrepancies, providing a complementary detection signal that improves\nconventional real vs. fake classifiers commonly used for detecting fake images.\nOur method achieves state of the art results on the FaceForensics++,\nCeleb-DF-v2, and DFDC benchmarks for face manipulation detection, and even\ngeneralizes to detect fakes produced by unseen methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 17:04:46 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Nirkin", "Yuval", ""], ["Wolf", "Lior", ""], ["Keller", "Yosi", ""], ["Hassner", "Tal", ""]]}, {"id": "2008.12272", "submitter": "Yu Sun", "authors": "Yu Sun, Qian Bao, Wu Liu, Yili Fu, Michael J. Black, Tao Mei", "title": "Monocular, One-stage, Regression of Multiple 3D People", "comments": "Code https://github.com/Arthur151/ROMP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the regression of multiple 3D people from a single RGB\nimage. Existing approaches predominantly follow a multi-stage pipeline, which\nfirst detects people with the bounding boxes and then regresses their 3D body\nmeshes. In contrast, we propose to Regress all meshes in a One-stage fashion\nfor Multiple 3D People (termed ROMP), which is conceptually simple, bounding\nbox-free, and able to learn per-pixel representation in an end-to-end manner.\nOur method simultaneously predicts a Body Center heatmap and a Mesh Parameter\nmap, which can jointly describe the 3D body mesh on the pixel level. Through a\nbody-center-guided sampling process, the body mesh parameters of all people in\nthe image can be easily extracted from the Mesh Parameter map. Equipped with\nsuch a fine-grained representation, our one-stage framework is free of the\ncomplex multi-stage process and more robust to occlusion. Compared with the\nstate-of-the-art methods, ROMP achieves superior performance on the challenging\nmulti-person/occlusion benchmarks, including 3DPW, CMU Panoptic, and 3DOH50K.\nExperiments on crowded/occluded datasets demonstrate the robustness under\nvarious types of occlusion. It is also worth noting that our released demo code\n( https://github.com/Arthur151/ROMP ) is the first real-time (over 30 FPS)\nimplementation of monocular multi-person 3D mesh regression to date.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 17:21:47 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 19:10:44 GMT"}, {"version": "v3", "created": "Fri, 2 Apr 2021 09:12:06 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Sun", "Yu", ""], ["Bao", "Qian", ""], ["Liu", "Wu", ""], ["Fu", "Yili", ""], ["Black", "Michael J.", ""], ["Mei", "Tao", ""]]}, {"id": "2008.12284", "submitter": "S\\'ebastien Arnold", "authors": "S\\'ebastien M. R. Arnold, Praateek Mahajan, Debajyoti Datta, Ian\n  Bunner, Konstantinos Saitas Zarkias", "title": "learn2learn: A Library for Meta-Learning Research", "comments": "Software available at: https://github.com/learnables/learn2learn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Meta-learning researchers face two fundamental issues in their empirical\nwork: prototyping and reproducibility. Researchers are prone to make mistakes\nwhen prototyping new algorithms and tasks because modern meta-learning methods\nrely on unconventional functionalities of machine learning frameworks. In turn,\nreproducing existing results becomes a tedious endeavour -- a situation\nexacerbated by the lack of standardized implementations and benchmarks. As a\nresult, researchers spend inordinate amounts of time on implementing software\nrather than understanding and developing new ideas.\n  This manuscript introduces learn2learn, a library for meta-learning research\nfocused on solving those prototyping and reproducibility issues. learn2learn\nprovides low-level routines common across a wide-range of meta-learning\ntechniques (e.g. meta-descent, meta-reinforcement learning, few-shot learning),\nand builds standardized interfaces to algorithms and benchmarks on top of them.\nIn releasing learn2learn under a free and open source license, we hope to\nfoster a community around standardized software for meta-learning research.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 17:41:34 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 03:48:50 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Arnold", "S\u00e9bastien M. R.", ""], ["Mahajan", "Praateek", ""], ["Datta", "Debajyoti", ""], ["Bunner", "Ian", ""], ["Zarkias", "Konstantinos Saitas", ""]]}, {"id": "2008.12295", "submitter": "Aleksander Holynski", "authors": "Aleksander Holynski, David Geraghty, Jan-Michael Frahm, Chris Sweeney,\n  Richard Szeliski", "title": "Reducing Drift in Structure From Motion Using Extended Features", "comments": "3DV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-frequency long-range errors (drift) are an endemic problem in 3D\nstructure from motion, and can often hamper reasonable reconstructions of the\nscene. In this paper, we present a method to dramatically reduce scale and\npositional drift by using extended structural features such as planes and\nvanishing points. Unlike traditional feature matches, our extended features are\nable to span non-overlapping input images, and hence provide long-range\nconstraints on the scale and shape of the reconstruction. We add these features\nas additional constraints to a state-of-the-art global structure from motion\nalgorithm and demonstrate that the added constraints enable the reconstruction\nof particularly drift-prone sequences such as long, low field-of-view videos\nwithout inertial measurements. Additionally, we provide an analysis of the\ndrift-reducing capabilities of these constraints by evaluating on a synthetic\ndataset. Our structural features are able to significantly reduce drift for\nscenes that contain long-spanning man-made structures, such as aligned rows of\nwindows or planar building facades.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 17:59:04 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 09:27:22 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 01:59:53 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Holynski", "Aleksander", ""], ["Geraghty", "David", ""], ["Frahm", "Jan-Michael", ""], ["Sweeney", "Chris", ""], ["Szeliski", "Richard", ""]]}, {"id": "2008.12298", "submitter": "Johannes Kopf", "authors": "Johannes Kopf, Kevin Matzen, Suhib Alsisan, Ocean Quigley, Francis Ge,\n  Yangming Chong, Josh Patterson, Jan-Michael Frahm, Shu Wu, Matthew Yu,\n  Peizhao Zhang, Zijian He, Peter Vajda, Ayush Saraf, Michael Cohen", "title": "One Shot 3D Photography", "comments": "Project page:\n  https://facebookresearch.github.io/one_shot_3d_photography/ Code:\n  https://github.com/facebookresearch/one_shot_3d_photography", "journal-ref": "ACM Transactions on Graphics (Proceedings of SIGGRAPH 2020),\n  Volume 39, Number 4, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D photography is a new medium that allows viewers to more fully experience a\ncaptured moment. In this work, we refer to a 3D photo as one that displays\nparallax induced by moving the viewpoint (as opposed to a stereo pair with a\nfixed viewpoint). 3D photos are static in time, like traditional photos, but\nare displayed with interactive parallax on mobile or desktop screens, as well\nas on Virtual Reality devices, where viewing it also includes stereo. We\npresent an end-to-end system for creating and viewing 3D photos, and the\nalgorithmic and design choices therein. Our 3D photos are captured in a single\nshot and processed directly on a mobile device. The method starts by estimating\ndepth from the 2D input image using a new monocular depth estimation network\nthat is optimized for mobile devices. It performs competitively to the\nstate-of-the-art, but has lower latency and peak memory consumption and uses an\norder of magnitude fewer parameters. The resulting depth is lifted to a layered\ndepth image, and new geometry is synthesized in parallax regions. We synthesize\ncolor texture and structures in the parallax regions as well, using an\ninpainting network, also optimized for mobile devices, on the LDI directly.\nFinally, we convert the result into a mesh-based representation that can be\nefficiently transmitted and rendered even on low-end devices and over poor\nnetwork connections. Altogether, the processing takes just a few seconds on a\nmobile device, and the result can be instantly viewed and shared. We perform\nextensive quantitative evaluation to validate our system and compare its new\ncomponents against the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 17:59:31 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 14:52:55 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Kopf", "Johannes", ""], ["Matzen", "Kevin", ""], ["Alsisan", "Suhib", ""], ["Quigley", "Ocean", ""], ["Ge", "Francis", ""], ["Chong", "Yangming", ""], ["Patterson", "Josh", ""], ["Frahm", "Jan-Michael", ""], ["Wu", "Shu", ""], ["Yu", "Matthew", ""], ["Zhang", "Peizhao", ""], ["He", "Zijian", ""], ["Vajda", "Peter", ""], ["Saraf", "Ayush", ""], ["Cohen", "Michael", ""]]}, {"id": "2008.12321", "submitter": "David Li", "authors": "David Z. Li, Masaru Ishii, Russell H. Taylor, Gregory D. Hager, Ayushi\n  Sinha", "title": "Learning Representations of Endoscopic Videos to Detect Tool Presence\n  Without Supervision", "comments": "10 pages, 4 figures, CLIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we explore whether it is possible to learn representations of\nendoscopic video frames to perform tasks such as identifying surgical tool\npresence without supervision. We use a maximum mean discrepancy (MMD)\nvariational autoencoder (VAE) to learn low-dimensional latent representations\nof endoscopic videos and manipulate these representations to distinguish frames\ncontaining tools from those without tools. We use three different methods to\nmanipulate these latent representations in order to predict tool presence in\neach frame. Our fully unsupervised methods can identify whether endoscopic\nvideo frames contain tools with average precision of 71.56, 73.93, and 76.18,\nrespectively, comparable to supervised methods. Our code is available at\nhttps://github.com/zdavidli/tool-presence/\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 18:23:05 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Li", "David Z.", ""], ["Ishii", "Masaru", ""], ["Taylor", "Russell H.", ""], ["Hager", "Gregory D.", ""], ["Sinha", "Ayushi", ""]]}, {"id": "2008.12328", "submitter": "Radu Tudor Ionescu", "authors": "Mariana-Iuliana Georgescu, Radu Tudor Ionescu, Fahad Shahbaz Khan,\n  Marius Popescu and Mubarak Shah", "title": "A Background-Agnostic Framework with Adversarial Training for Abnormal\n  Event Detection in Video", "comments": "Accepted in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3074805", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abnormal event detection in video is a complex computer vision problem that\nhas attracted significant attention in recent years. The complexity of the task\narises from the commonly-adopted definition of an abnormal event, that is, a\nrarely occurring event that typically depends on the surrounding context.\nFollowing the standard formulation of abnormal event detection as outlier\ndetection, we propose a background-agnostic framework that learns from training\nvideos containing only normal events. Our framework is composed of an object\ndetector, a set of appearance and motion auto-encoders, and a set of\nclassifiers. Since our framework only looks at object detections, it can be\napplied to different scenes, provided that normal events are defined\nidentically across scenes and that the single main factor of variation is the\nbackground. To overcome the lack of abnormal data during training, we propose\nan adversarial learning strategy for the auto-encoders. We create a\nscene-agnostic set of out-of-domain pseudo-abnormal examples, which are\ncorrectly reconstructed by the auto-encoders before applying gradient ascent on\nthe pseudo-abnormal examples. We further utilize the pseudo-abnormal examples\nto serve as abnormal examples when training appearance-based and motion-based\nbinary classifiers to discriminate between normal and abnormal latent features\nand reconstructions. We compare our framework with the state-of-the-art methods\non four benchmark data sets, using various evaluation metrics. Compared to\nexisting methods, the empirical results indicate that our approach achieves\nfavorable performance on all data sets. In addition, we provide region-based\nand track-based annotations for two large-scale abnormal event detection data\nsets from the literature, namely ShanghaiTech and Subway.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 18:39:24 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 14:38:12 GMT"}, {"version": "v3", "created": "Mon, 19 Apr 2021 20:40:37 GMT"}, {"version": "v4", "created": "Mon, 10 May 2021 14:47:29 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Georgescu", "Mariana-Iuliana", ""], ["Ionescu", "Radu Tudor", ""], ["Khan", "Fahad Shahbaz", ""], ["Popescu", "Marius", ""], ["Shah", "Mubarak", ""]]}, {"id": "2008.12338", "submitter": "Gauri Jagatap", "authors": "Gauri Jagatap, Ameya Joshi, Animesh Basak Chowdhury, Siddharth Garg,\n  Chinmay Hegde", "title": "Adversarially Robust Learning via Entropic Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new family of algorithms, ATENT, for training\nadversarially robust deep neural networks. We formulate a new loss function\nthat is equipped with an additional entropic regularization. Our loss function\nconsiders the contribution of adversarial samples that are drawn from a\nspecially designed distribution in the data space that assigns high probability\nto points with high loss and in the immediate neighborhood of training samples.\nOur proposed algorithms optimize this loss to seek adversarially robust valleys\nof the loss landscape. Our approach achieves competitive (or better)\nperformance in terms of robust classification accuracy as compared to several\nstate-of-the-art robust learning approaches on benchmark datasets such as MNIST\nand CIFAR-10.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 18:54:43 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 15:39:02 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Jagatap", "Gauri", ""], ["Joshi", "Ameya", ""], ["Chowdhury", "Animesh Basak", ""], ["Garg", "Siddharth", ""], ["Hegde", "Chinmay", ""]]}, {"id": "2008.12350", "submitter": "Junggab Son", "authors": "Tejaswini Mallavarapu, Luke Cranfill, Junggab Son, Eun Hye Kim, Reza\n  M. Parizi, and John Morris", "title": "A Federated Approach for Fine-Grained Classification of Fashion Apparel", "comments": "11 pages, 4 figures, 5 tables, submitted to IEEE ACCESS (under\n  review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As online retail services proliferate and are pervasive in modern lives,\napplications for classifying fashion apparel features from image data are\nbecoming more indispensable. Online retailers, from leading companies to\nstart-ups, can leverage such applications in order to increase profit margin\nand enhance the consumer experience. Many notable schemes have been proposed to\nclassify fashion items, however, the majority of which focused upon classifying\nbasic-level categories, such as T-shirts, pants, skirts, shoes, bags, and so\nforth. In contrast to most prior efforts, this paper aims to enable an in-depth\nclassification of fashion item attributes within the same category. Beginning\nwith a single dress, we seek to classify the type of dress hem, the hem length,\nand the sleeve length. The proposed scheme is comprised of three major stages:\n(a) localization of a target item from an input image using semantic\nsegmentation, (b) detection of human key points (e.g., point of shoulder) using\na pre-trained CNN and a bounding box, and (c) three phases to classify the\nattributes using a combination of algorithmic approaches and deep neural\nnetworks. The experimental results demonstrate that the proposed scheme is\nhighly effective, with all categories having average precision of above 93.02%,\nand outperforms existing Convolutional Neural Networks (CNNs)-based schemes.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 19:44:43 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Mallavarapu", "Tejaswini", ""], ["Cranfill", "Luke", ""], ["Son", "Junggab", ""], ["Kim", "Eun Hye", ""], ["Parizi", "Reza M.", ""], ["Morris", "John", ""]]}, {"id": "2008.12363", "submitter": "Isha Ghodgaonkar", "authors": "Isha Ghodgaonkar, Subhankar Chakraborty, Vishnu Banna, Shane Allcroft,\n  Mohammed Metwaly, Fischer Bordwell, Kohsuke Kimura, Xinxin Zhao, Abhinav\n  Goel, Caleb Tung, Akhil Chinnakotla, Minghao Xue, Yung-Hsiang Lu, Mark Daniel\n  Ward, Wei Zakharov, David S. Ebert, David M. Barbarash, George K.\n  Thiruvathukal", "title": "Analyzing Worldwide Social Distancing through Large-Scale Computer\n  Vision", "comments": "10 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to contain the COVID-19 pandemic, countries around the world have\nintroduced social distancing guidelines as public health interventions to\nreduce the spread of the disease. However, monitoring the efficacy of these\nguidelines at a large scale (nationwide or worldwide) is difficult. To make\nmatters worse, traditional observational methods such as in-person reporting is\ndangerous because observers may risk infection. A better solution is to observe\nactivities through network cameras; this approach is scalable and observers can\nstay in safe locations. This research team has created methods that can\ndiscover thousands of network cameras worldwide, retrieve data from the\ncameras, analyze the data, and report the sizes of crowds as different\ncountries issued and lifted restrictions (also called ''lockdown''). We\ndiscover 11,140 network cameras that provide real-time data and we present the\nresults across 15 countries. We collect data from these cameras beginning April\n2020 at approximately 0.5TB per week. After analyzing 10,424,459 images from\nstill image cameras and frames extracted periodically from video, the data\nreveals that the residents in some countries exhibited more activity (judged by\nnumbers of people and vehicles) after the restrictions were lifted. In other\ncountries, the amounts of activities showed no obvious changes during the\nrestrictions and after the restrictions were lifted. The data further reveals\nwhether people stay ''social distancing'', at least 6 feet apart. This study\ndiscerns whether social distancing is being followed in several types of\nlocations and geographical locations worldwide and serve as an early indicator\nwhether another wave of infections is likely to occur soon.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 20:20:11 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Ghodgaonkar", "Isha", ""], ["Chakraborty", "Subhankar", ""], ["Banna", "Vishnu", ""], ["Allcroft", "Shane", ""], ["Metwaly", "Mohammed", ""], ["Bordwell", "Fischer", ""], ["Kimura", "Kohsuke", ""], ["Zhao", "Xinxin", ""], ["Goel", "Abhinav", ""], ["Tung", "Caleb", ""], ["Chinnakotla", "Akhil", ""], ["Xue", "Minghao", ""], ["Lu", "Yung-Hsiang", ""], ["Ward", "Mark Daniel", ""], ["Zakharov", "Wei", ""], ["Ebert", "David S.", ""], ["Barbarash", "David M.", ""], ["Thiruvathukal", "George K.", ""]]}, {"id": "2008.12371", "submitter": "Steff Farley", "authors": "Steff Farley, Jo E.A. Hodgkinson, Oliver M. Gordon, Joanna Turner,\n  Andrea Soltoggio, Philip J. Moriarty, Eugenie Hunsicker", "title": "Improving the Segmentation of Scanning Probe Microscope Images using\n  Convolutional Neural Networks", "comments": "21 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide range of techniques can be considered for segmentation of images of\nnanostructured surfaces. Manually segmenting these images is time-consuming and\nresults in a user-dependent segmentation bias, while there is currently no\nconsensus on the best automated segmentation methods for particular techniques,\nimage classes, and samples. Any image segmentation approach must minimise the\nnoise in the images to ensure accurate and meaningful statistical analysis can\nbe carried out. Here we develop protocols for the segmentation of images of 2D\nassemblies of gold nanoparticles formed on silicon surfaces via deposition from\nan organic solvent. The evaporation of the solvent drives far-from-equilibrium\nself-organisation of the particles, producing a wide variety of nano- and\nmicro-structured patterns. We show that a segmentation strategy using the U-Net\nconvolutional neural network outperforms traditional automated approaches and\nhas particular potential in the processing of images of nanostructured systems.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 20:49:59 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Farley", "Steff", ""], ["Hodgkinson", "Jo E. A.", ""], ["Gordon", "Oliver M.", ""], ["Turner", "Joanna", ""], ["Soltoggio", "Andrea", ""], ["Moriarty", "Philip J.", ""], ["Hunsicker", "Eugenie", ""]]}, {"id": "2008.12378", "submitter": "Spyridon Thermos", "authors": "Xiao Liu, Spyridon Thermos, Gabriele Valvano, Agisilaos Chartsias,\n  Alison O'Neil and Sotirios A. Tsaftaris", "title": "Metrics for Exposing the Biases of Content-Style Disentanglement", "comments": "17 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent spate of state-of-the-art semi- and unsupervised solutions for\nchallenging computer vision tasks encode image \"content\" into a spatial tensor\nand image appearance or \"style\" into a vector. Most of these solutions use the\nterm disentangled for their representations and employ different \"biases\" such\nas model design, learning objectives, and data, to achieve good performance in\nspatially equivariant tasks (e.g. image-to-image translation). While\nconsiderable effort has been made to measure disentanglement in vector\nrepresentations, we have lacked metrics for spatial content and vector style\nrepresentations. In this paper, we propose such metrics to characterize the\ndegree of disentanglement in terms of how (un)correlated and informative the\ncontent and style representations are, and we further examine its relation to\ntask performance. In particular, we first identify key design choices and\nlearning constraints on three popular models that employ content-style\ndisentanglement and derive ablated versions. Secondly, we use our metrics to\nascertain the role of each bias. Our experiments reveal a \"sweet spot\" between\ndisentanglement, task performance and latent space interpretability. Our\nmetrics are not task-dependent; thus, they can help guide either the design of\nnew future models or the selection of viable models such that this ideal \"sweet\nspot\" is achieved in any task where content-style representations are useful.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 21:41:37 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 21:34:28 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2020 17:56:33 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Liu", "Xiao", ""], ["Thermos", "Spyridon", ""], ["Valvano", "Gabriele", ""], ["Chartsias", "Agisilaos", ""], ["O'Neil", "Alison", ""], ["Tsaftaris", "Sotirios A.", ""]]}, {"id": "2008.12380", "submitter": "Alvaro Gomariz", "authors": "Alvaro Gomariz, Tiziano Portenier, Patrick M. Helbling, Stephan\n  Isringhausen, Ute Suessbier, C\\'esar Nombela-Arrieta, Orcun Goksel", "title": "Modality Attention and Sampling Enables Deep Learning with Heterogeneous\n  Marker Combinations in Fluorescence Microscopy", "comments": "Main: 21 pages, 6 figures, 1 table. Supplementary: 5 pages, 7\n  figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fluorescence microscopy allows for a detailed inspection of cells, cellular\nnetworks, and anatomical landmarks by staining with a variety of\ncarefully-selected markers visualized as color channels. Quantitative\ncharacterization of structures in acquired images often relies on automatic\nimage analysis methods. Despite the success of deep learning methods in other\nvision applications, their potential for fluorescence image analysis remains\nunderexploited. One reason lies in the considerable workload required to train\naccurate models, which are normally specific for a given combination of\nmarkers, and therefore applicable to a very restricted number of experimental\nsettings. We herein propose Marker Sampling and Excite, a neural network\napproach with a modality sampling strategy and a novel attention module that\ntogether enable (i) flexible training with heterogeneous datasets with\ncombinations of markers and (ii) successful utility of learned models on\narbitrary subsets of markers prospectively. We show that our single neural\nnetwork solution performs comparably to an upper bound scenario where an\nensemble of many networks is na\\\"ively trained for each possible marker\ncombination separately. In addition, we demonstrate the feasibility of this\nframework in high-throughput biological analysis by revising a recent\nquantitative characterization of bone marrow vasculature in 3D confocal\nmicroscopy datasets and further confirm the validity of our approach on an\nadditional, significantly different dataset of microvessels in fetal liver\ntissues. Not only can our work substantially ameliorate the use of deep\nlearning in fluorescence microscopy analysis, but it can also be utilized in\nother fields with incomplete data acquisitions and missing modalities.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 21:57:07 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 19:37:38 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Gomariz", "Alvaro", ""], ["Portenier", "Tiziano", ""], ["Helbling", "Patrick M.", ""], ["Isringhausen", "Stephan", ""], ["Suessbier", "Ute", ""], ["Nombela-Arrieta", "C\u00e9sar", ""], ["Goksel", "Orcun", ""]]}, {"id": "2008.12405", "submitter": "Ben Saunders", "authors": "Ben Saunders, Necati Cihan Camgoz, Richard Bowden", "title": "Adversarial Training for Multi-Channel Sign Language Production", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign Languages are rich multi-channel languages, requiring articulation of\nboth manual (hands) and non-manual (face and body) features in a precise,\nintricate manner. Sign Language Production (SLP), the automatic translation\nfrom spoken to sign languages, must embody this full sign morphology to be\ntruly understandable by the Deaf community. Previous work has mainly focused on\nmanual feature production, with an under-articulated output caused by\nregression to the mean.\n  In this paper, we propose an Adversarial Multi-Channel approach to SLP. We\nframe sign production as a minimax game between a transformer-based Generator\nand a conditional Discriminator. Our adversarial discriminator evaluates the\nrealism of sign production conditioned on the source text, pushing the\ngenerator towards a realistic and articulate output. Additionally, we fully\nencapsulate sign articulators with the inclusion of non-manual features,\nproducing facial features and mouthing patterns.\n  We evaluate on the challenging RWTH-PHOENIX-Weather-2014T (PHOENIX14T)\ndataset, and report state-of-the art SLP back-translation performance for\nmanual production. We set new benchmarks for the production of multi-channel\nsign to underpin future research into realistic SLP.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 23:05:54 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Saunders", "Ben", ""], ["Camgoz", "Necati Cihan", ""], ["Bowden", "Richard", ""]]}, {"id": "2008.12413", "submitter": "Gautam Gare", "authors": "Gautam Rajendrakumar Gare, Jiayuan Li, Rohan Joshi, Mrunal Prashant\n  Vaze, Rishikesh Magar, Michael Yousefpour, Ricardo Luis Rodriguez and John\n  Micheal Galeotti", "title": "W-Net: Dense Semantic Segmentation of Subcutaneous Tissue in Ultrasound\n  Images by Expanding U-Net to Incorporate Ultrasound RF Waveform Data", "comments": "The paper is currently under review for publication in a\n  peer-reviewed journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present W-Net, a novel Convolution Neural Network (CNN) framework that\nemploys raw ultrasound waveforms from each A-scan, typically referred to as\nultrasound Radio Frequency (RF) data, in addition to the gray ultrasound image\nto semantically segment and label tissues. Unlike prior work, we seek to label\nevery pixel in the image, without the use of a background class. To the best of\nour knowledge, this is also the first deep-learning or CNN approach for\nsegmentation that analyses ultrasound raw RF data along with the gray image.\nInternational patent(s) pending [PCT/US20/37519]. We chose subcutaneous tissue\n(SubQ) segmentation as our initial clinical goal since it has diverse\nintermixed tissues, is challenging to segment, and is an underrepresented\nresearch area. SubQ potential applications include plastic surgery, adipose\nstem-cell harvesting, lymphatic monitoring, and possibly detection/treatment of\ncertain types of tumors. A custom dataset consisting of hand-labeled images by\nan expert clinician and trainees are used for the experimentation, currently\nlabeled into the following categories: skin, fat, fat fascia/stroma, muscle and\nmuscle fascia. We compared our results with U-Net and Attention U-Net. Our\nnovel \\emph{W-Net}'s RF-Waveform input and architecture increased mIoU accuracy\n(averaged across all tissue classes) by 4.5\\% and 4.9\\% compared to regular\nU-Net and Attention U-Net, respectively. We present analysis as to why the\nMuscle fascia and Fat fascia/stroma are the most difficult tissues to label.\nMuscle fascia in particular, the most difficult anatomic class to recognize for\nboth humans and AI algorithms, saw mIoU improvements of 13\\% and 16\\% from our\nW-Net vs U-Net and Attention U-Net respectively.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 23:53:20 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 09:14:27 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Gare", "Gautam Rajendrakumar", ""], ["Li", "Jiayuan", ""], ["Joshi", "Rohan", ""], ["Vaze", "Mrunal Prashant", ""], ["Magar", "Rishikesh", ""], ["Yousefpour", "Michael", ""], ["Rodriguez", "Ricardo Luis", ""], ["Galeotti", "John Micheal", ""]]}, {"id": "2008.12416", "submitter": "Yu-Huan Wu", "authors": "Yu-Huan Wu, Yun Liu, Le Zhang, Wang Gao, and Ming-Ming Cheng", "title": "Regularized Densely-connected Pyramid Network for Salient Instance\n  Segmentation", "comments": "Accepted in IEEE Transactions on Image Processing. Code:\n  https://github.com/yuhuan-wu/RDPNet", "journal-ref": null, "doi": "10.1109/TIP.2021.3065822", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Much of the recent efforts on salient object detection (SOD) have been\ndevoted to producing accurate saliency maps without being aware of their\ninstance labels. To this end, we propose a new pipeline for end-to-end salient\ninstance segmentation (SIS) that predicts a class-agnostic mask for each\ndetected salient instance. To better use the rich feature hierarchies in deep\nnetworks and enhance the side predictions, we propose the regularized dense\nconnections, which attentively promote informative features and suppress\nnon-informative ones from all feature pyramids. A novel multi-level RoIAlign\nbased decoder is introduced to adaptively aggregate multi-level features for\nbetter mask predictions. Such strategies can be well-encapsulated into the Mask\nR-CNN pipeline. Extensive experiments on popular benchmarks demonstrate that\nour design significantly outperforms existing \\sArt competitors by 6.3\\%\n(58.6\\% vs. 52.3\\%) in terms of the AP metric.The code is available at\nhttps://github.com/yuhuan-wu/RDPNet.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 00:13:30 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 03:21:50 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Wu", "Yu-Huan", ""], ["Liu", "Yun", ""], ["Zhang", "Le", ""], ["Gao", "Wang", ""], ["Cheng", "Ming-Ming", ""]]}, {"id": "2008.12432", "submitter": "Pallabi Ghosh", "authors": "Pallabi Ghosh, Nirat Saini, Larry S. Davis, Abhinav Shrivastava", "title": "All About Knowledge Graphs for Actions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current action recognition systems require large amounts of training data for\nrecognizing an action. Recent works have explored the paradigm of zero-shot and\nfew-shot learning to learn classifiers for unseen categories or categories with\nfew labels. Following similar paradigms in object recognition, these approaches\nutilize external sources of knowledge (eg. knowledge graphs from language\ndomains). However, unlike objects, it is unclear what is the best knowledge\nrepresentation for actions. In this paper, we intend to gain a better\nunderstanding of knowledge graphs (KGs) that can be utilized for zero-shot and\nfew-shot action recognition. In particular, we study three different\nconstruction mechanisms for KGs: action embeddings, action-object embeddings,\nvisual embeddings. We present extensive analysis of the impact of different KGs\nin different experimental setups. Finally, to enable a systematic study of\nzero-shot and few-shot approaches, we propose an improved evaluation paradigm\nbased on UCF101, HMDB51, and Charades datasets for knowledge transfer from\nmodels trained on Kinetics.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 01:44:01 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Ghosh", "Pallabi", ""], ["Saini", "Nirat", ""], ["Davis", "Larry S.", ""], ["Shrivastava", "Abhinav", ""]]}, {"id": "2008.12444", "submitter": "Yu Rong", "authors": "Jiangjing Lyu, Xiaobo Li, Xiangyu Zhu, Cheng Cheng", "title": "Pixel-Face: A Large-Scale, High-Resolution Benchmark for 3D Face\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D face reconstruction is a fundamental task that can facilitate numerous\napplications such as robust facial analysis and augmented reality. It is also a\nchallenging task due to the lack of high-quality datasets that can fuel current\ndeep learning-based methods. However, existing datasets are limited in\nquantity, realisticity and diversity. To circumvent these hurdles, we introduce\nPixel-Face, a large-scale, high-resolution and diverse 3D face dataset with\nmassive annotations. Specifically, Pixel-Face contains 855 subjects aging from\n18 to 80. Each subject has more than 20 samples with various expressions. Each\nsample is composed of high-resolution multi-view RGB images and 3D meshes with\nvarious expressions. Moreover, we collect precise landmarks annotation and 3D\nregistration result for each data. To demonstrate the advantages of Pixel-Face,\nwe re-parameterize the 3D Morphable Model (3DMM) into Pixel-3DM using the\ncollected data. We show that the obtained Pixel-3DM is better in modeling a\nwide range of face shapes and expressions. We also carefully benchmark existing\n3D face reconstruction methods on our dataset. Moreover, Pixel-Face serves as\nan effective training source. We observe that the performance of current face\nreconstruction models significantly improves both on existing benchmarks and\nPixel-Face after being fine-tuned using our newly collected data. Extensive\nexperiments demonstrate the effectiveness of Pixel-3DM and the usefulness of\nPixel-Face.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 02:22:07 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 03:25:24 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2020 02:33:25 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Lyu", "Jiangjing", ""], ["Li", "Xiaobo", ""], ["Zhu", "Xiangyu", ""], ["Cheng", "Cheng", ""]]}, {"id": "2008.12447", "submitter": "Zhenhang Huang", "authors": "Zhenhang Huang, Shihao Sun, Ruirui Li", "title": "Fast Single-shot Ship Instance Segmentation Based on Polar Template Mask\n  in Remote Sensing Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection and instance segmentation in remote sensing images is a\nfundamental and challenging task, due to the complexity of scenes and targets.\nThe latest methods tried to take into account both the efficiency and the\naccuracy of instance segmentation. In order to improve both of them, in this\npaper, we propose a single-shot convolutional neural network structure, which\nis conceptually simple and straightforward, and meanwhile makes up for the\nproblem of low accuracy of single-shot networks. Our method, termed with\nSSS-Net, detects targets based on the location of the object's center and the\ndistances between the center and the points on the silhouette sampling with\nnon-uniform angle intervals, thereby achieving abalanced sampling of lines in\nmask generation. In addition, we propose a non-uniform polar template IoU based\non the contour template in polar coordinates. Experiments on both the Airbus\nShip Detection Challenge dataset and the ISAIDships dataset show that SSS-Net\nhas strong competitiveness in precision and speed for ship instance\nsegmentation.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 02:38:04 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Huang", "Zhenhang", ""], ["Sun", "Shihao", ""], ["Li", "Ruirui", ""]]}, {"id": "2008.12454", "submitter": "Robert Bassett", "authors": "Robert Bassett, Mitchell Graves, Patrick Reilly", "title": "Color and Edge-Aware Adversarial Image Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial perturbation of images, in which a source image is deliberately\nmodified with the intent of causing a classifier to misclassify the image,\nprovides important insight into the robustness of image classifiers. In this\nwork we develop two new methods for constructing adversarial perturbations,\nboth of which are motivated by minimizing human ability to detect changes\nbetween the perturbed and source image. The first of these, the Edge-Aware\nmethod, reduces the magnitude of perturbations permitted in smooth regions of\nan image where changes are more easily detected. Our second method, the\nColor-Aware method, performs the perturbation in a color space which accurately\ncaptures human ability to distinguish differences in colors, thus reducing the\nperceived change. The Color-Aware and Edge-Aware methods can also be\nimplemented simultaneously, resulting in image perturbations which account for\nboth human color perception and sensitivity to changes in homogeneous regions.\nBecause Edge-Aware and Color-Aware modifications exist for many image\nperturbations techniques, we also focus on computation to demonstrate their\npotential for use within more complex perturbation schemes. We empirically\ndemonstrate that the Color-Aware and Edge-Aware perturbations we consider\neffectively cause misclassification, are less distinguishable to human\nperception, and are as easy to compute as the most efficient image perturbation\ntechniques. Code and demo available at\nhttps://github.com/rbassett3/Color-and-Edge-Aware-Perturbations\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 03:02:20 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 19:25:48 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Bassett", "Robert", ""], ["Graves", "Mitchell", ""], ["Reilly", "Patrick", ""]]}, {"id": "2008.12463", "submitter": "Xu Ouyang", "authors": "Xu Ouyang, Gady Agam", "title": "Accelerated WGAN update strategy with loss change rate balancing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing the discriminator in Generative Adversarial Networks (GANs) to\ncompletion in the inner training loop is computationally prohibitive, and on\nfinite datasets would result in overfitting. To address this, a common update\nstrategy is to alternate between k optimization steps for the discriminator D\nand one optimization step for the generator G. This strategy is repeated in\nvarious GAN algorithms where k is selected empirically. In this paper, we show\nthat this update strategy is not optimal in terms of accuracy and convergence\nspeed, and propose a new update strategy for Wasserstein GANs (WGAN) and other\nGANs using the WGAN loss(e.g. WGAN-GP, Deblur GAN, and Super-resolution GAN).\nThe proposed update strategy is based on a loss change ratio comparison of G\nand D. We demonstrate that the proposed strategy improves both convergence\nspeed and accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 03:29:09 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 01:45:11 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Ouyang", "Xu", ""], ["Agam", "Gady", ""]]}, {"id": "2008.12470", "submitter": "Guangshuai Gao", "authors": "Guangshuai Gao and Qingjie Liu and Yunhong Wang", "title": "Counting from Sky: A Large-scale Dataset for Remote Sensing Object\n  Counting and A Benchmark Method", "comments": "To be prepared in TGRS", "journal-ref": null, "doi": "10.1109/TGRS.2020.3020555", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object counting, whose aim is to estimate the number of objects from a given\nimage, is an important and challenging computation task. Significant efforts\nhave been devoted to addressing this problem and achieved great progress, yet\ncounting the number of ground objects from remote sensing images is barely\nstudied. In this paper, we are interested in counting dense objects from remote\nsensing images. Compared with object counting in a natural scene, this task is\nchallenging in the following factors: large scale variation, complex cluttered\nbackground, and orientation arbitrariness. More importantly, the scarcity of\ndata severely limits the development of research in this field. To address\nthese issues, we first construct a large-scale object counting dataset with\nremote sensing images, which contains four important geographic objects:\nbuildings, crowded ships in harbors, large-vehicles and small-vehicles in\nparking lots. We then benchmark the dataset by designing a novel neural network\nthat can generate a density map of an input image. The proposed network\nconsists of three parts namely attention module, scale pyramid module and\ndeformable convolution module to attack the aforementioned challenging factors.\nExtensive experiments are performed on the proposed dataset and one crowd\ncounting datset, which demonstrate the challenges of the proposed dataset and\nthe superiority and effectiveness of our method compared with state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 03:47:49 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Gao", "Guangshuai", ""], ["Liu", "Qingjie", ""], ["Wang", "Yunhong", ""]]}, {"id": "2008.12480", "submitter": "Ali Septiandri", "authors": "Ali Akbar Septiandri, Ade Jamal, Pritta Ameilia Iffanolida, Oki\n  Riayati, Budi Wiweko", "title": "Human Blastocyst Classification after In Vitro Fertilization Using Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embryo quality assessment after in vitro fertilization (IVF) is primarily\ndone visually by embryologists. Variability among assessors, however, remains\none of the main causes of the low success rate of IVF. This study aims to\ndevelop an automated embryo assessment based on a deep learning model. This\nstudy includes a total of 1084 images from 1226 embryos. The images were\ncaptured by an inverted microscope at day 3 after fertilization. The images\nwere labelled based on Veeck criteria that differentiate embryos to grade 1 to\n5 based on the size of the blastomere and the grade of fragmentation. Our deep\nlearning grading results were compared to the grading results from trained\nembryologists to evaluate the model performance. Our best model from\nfine-tuning a pre-trained ResNet50 on the dataset results in 91.79% accuracy.\nThe model presented could be developed into an automated embryo assessment\nmethod in point-of-care settings.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 04:40:55 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Septiandri", "Ali Akbar", ""], ["Jamal", "Ade", ""], ["Iffanolida", "Pritta Ameilia", ""], ["Riayati", "Oki", ""], ["Wiweko", "Budi", ""]]}, {"id": "2008.12493", "submitter": "Dokyeong Kwon", "authors": "Dokyeong Kwon, Guisik Kim, Junseok Kwon", "title": "DALE : Dark Region-Aware Low-light Image Enhancement", "comments": "12 pages, 7 figures, The 31st British Machine Vision Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel low-light image enhancement method called\ndark region-aware low-light image enhancement (DALE), where dark regions are\naccurately recognized by the proposed visual attention module and their\nbrightness are intensively enhanced. Our method can estimate the visual\nattention in an efficient manner using super-pixels without any complicated\nprocess. Thus, the method can preserve the color, tone, and brightness of\noriginal images and prevents normally illuminated areas of the images from\nbeing saturated and distorted. Experimental results show that our method\naccurately identifies dark regions via the proposed visual attention, and\nqualitatively and quantitatively outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 06:14:21 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Kwon", "Dokyeong", ""], ["Kim", "Guisik", ""], ["Kwon", "Junseok", ""]]}, {"id": "2008.12496", "submitter": "Geonuk Kim", "authors": "Geonuk Kim, Hong-Gyu Jung, Seong-Whan Lee", "title": "Few-Shot Object Detection via Knowledge Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional methods for object detection usually require substantial amounts\nof training data and annotated bounding boxes. If there are only a few training\ndata and annotations, the object detectors easily overfit and fail to\ngeneralize. It exposes the practical weakness of the object detectors. On the\nother hand, human can easily master new reasoning rules with only a few\ndemonstrations using previously learned knowledge. In this paper, we introduce\na few-shot object detection via knowledge transfer, which aims to detect\nobjects from a few training examples. Central to our method is prototypical\nknowledge transfer with an attached meta-learner. The meta-learner takes\nsupport set images that include the few examples of the novel categories and\nbase categories, and predicts prototypes that represent each category as a\nvector. Then, the prototypes reweight each RoI (Region-of-Interest) feature\nvector from a query image to remodels R-CNN predictor heads. To facilitate the\nremodeling process, we predict the prototypes under a graph structure, which\npropagates information of the correlated base categories to the novel\ncategories with explicit guidance of prior knowledge that represents\ncorrelations among categories. Extensive experiments on the PASCAL VOC dataset\nverifies the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 06:35:27 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Kim", "Geonuk", ""], ["Jung", "Hong-Gyu", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2008.12505", "submitter": "Ezgi Demircan-Tureyen", "authors": "Ezgi Demircan-Tureyen, Mustafa E. Kamasak", "title": "Nonlocal Adaptive Direction-Guided Structure Tensor Total Variation For\n  Image Recovery", "comments": "9 pages, 4 figures, article", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common strategy in variational image recovery is utilizing the nonlocal\nself-similarity (NSS) property, when designing energy functionals. One such\ncontribution is nonlocal structure tensor total variation (NLSTV), which lies\nat the core of this study. This paper is concerned with boosting the NLSTV\nregularization term through the use of directional priors. More specifically,\nNLSTV is leveraged so that, at each image point, it gains more sensitivity in\nthe direction that is presumed to have the minimum local variation. The actual\ndifficulty here is capturing this directional information from the corrupted\nimage. In this regard, we propose a method that employs anisotropic Gaussian\nkernels to estimate directional features to be later used by our proposed\nmodel. The experiments validate that our entire two-stage framework achieves\nbetter results than the NLSTV model and two other competing local models, in\nterms of visual and quantitative evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 06:58:35 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Demircan-Tureyen", "Ezgi", ""], ["Kamasak", "Mustafa E.", ""]]}, {"id": "2008.12511", "submitter": "Yuzuko Utsumi", "authors": "Ryota Akai, Yuzuko Utsumi, Yuka Miwa, Masakazu Iwamura, Koichi Kise", "title": "Distortion-Adaptive Grape Bunch Counting for Omnidirectional Images", "comments": "Accepted 25th International Conference on Pattern Recognition (ICPR\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the first object counting method for omnidirectional\nimages. Because conventional object counting methods cannot handle the\ndistortion of omnidirectional images, we propose to process them using\nstereographic projection, which enables conventional methods to obtain a good\napproximation of the density function. However, the images obtained by\nstereographic projection are still distorted. Hence, to manage this distortion,\nwe propose two methods. One is a new data augmentation method designed for the\nstereographic projection of omnidirectional images. The other is a\ndistortion-adaptive Gaussian kernel that generates a density map ground truth\nwhile taking into account the distortion of stereographic projection. Using the\ncounting of grape bunches as a case study, we constructed an original\ngrape-bunch image dataset consisting of omnidirectional images and conducted\nexperiments to evaluate the proposed method. The results show that the proposed\nmethod performs better than a direct application of the conventional method,\nimproving mean absolute error by 14.7% and mean squared error by 10.5%.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 07:17:34 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Akai", "Ryota", ""], ["Utsumi", "Yuzuko", ""], ["Miwa", "Yuka", ""], ["Iwamura", "Masakazu", ""], ["Kise", "Koichi", ""]]}, {"id": "2008.12520", "submitter": "Noa Garcia", "authors": "Noa Garcia, Chentao Ye, Zihua Liu, Qingtao Hu, Mayu Otani, Chenhui\n  Chu, Yuta Nakashima, Teruko Mitamura", "title": "A Dataset and Baselines for Visual Question Answering on Art", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answering questions related to art pieces (paintings) is a difficult task, as\nit implies the understanding of not only the visual information that is shown\nin the picture, but also the contextual knowledge that is acquired through the\nstudy of the history of art. In this work, we introduce our first attempt\ntowards building a new dataset, coined AQUA (Art QUestion Answering). The\nquestion-answer (QA) pairs are automatically generated using state-of-the-art\nquestion generation methods based on paintings and comments provided in an\nexisting art understanding dataset. The QA pairs are cleansed by crowdsourcing\nworkers with respect to their grammatical correctness, answerability, and\nanswers' correctness. Our dataset inherently consists of visual\n(painting-based) and knowledge (comment-based) questions. We also present a\ntwo-branch model as baseline, where the visual and knowledge questions are\nhandled independently. We extensively compare our baseline model against the\nstate-of-the-art models for question answering, and we provide a comprehensive\nstudy about the challenges and potential future directions for visual question\nanswering on art.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 07:33:30 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Garcia", "Noa", ""], ["Ye", "Chentao", ""], ["Liu", "Zihua", ""], ["Hu", "Qingtao", ""], ["Otani", "Mayu", ""], ["Chu", "Chenhui", ""], ["Nakashima", "Yuta", ""], ["Mitamura", "Teruko", ""]]}, {"id": "2008.12544", "submitter": "Maria Wimmer", "authors": "Theresa Neubauer, Maria Wimmer, Astrid Berg, David Major, Dimitrios\n  Lenis, Thomas Beyer, Jelena Saponjski, Katja B\\\"uhler", "title": "Soft Tissue Sarcoma Co-Segmentation in Combined MRI and PET/CT Data", "comments": "Accepted for publication at Multimodal Learning for Clinical Decision\n  Support Workshop at MICCAI 2020 (edit: corrected typos and model name in Fig.\n  3, added missing circles in Table 1)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tumor segmentation in multimodal medical images has seen a growing trend\ntowards deep learning based methods. Typically, studies dealing with this topic\nfuse multimodal image data to improve the tumor segmentation contour for a\nsingle imaging modality. However, they do not take into account that tumor\ncharacteristics are emphasized differently by each modality, which affects the\ntumor delineation. Thus, the tumor segmentation is modality- and\ntask-dependent. This is especially the case for soft tissue sarcomas, where,\ndue to necrotic tumor tissue, the segmentation differs vastly. Closing this\ngap, we develop a modalityspecific sarcoma segmentation model that utilizes\nmultimodal image data to improve the tumor delineation on each individual\nmodality. We propose a simultaneous co-segmentation method, which enables\nmultimodal feature learning through modality-specific encoder and decoder\nbranches, and the use of resource-effcient densely connected convolutional\nlayers. We further conduct experiments to analyze how different input\nmodalities and encoder-decoder fusion strategies affect the segmentation\nresult. We demonstrate the effectiveness of our approach on public soft tissue\nsarcoma data, which comprises MRI (T1 and T2 sequence) and PET/CT scans. The\nresults show that our multimodal co-segmentation model provides better\nmodality-specific tumor segmentation than models using only the PET or MRI (T1\nand T2) scan as input.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 09:15:42 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 09:10:17 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Neubauer", "Theresa", ""], ["Wimmer", "Maria", ""], ["Berg", "Astrid", ""], ["Major", "David", ""], ["Lenis", "Dimitrios", ""], ["Beyer", "Thomas", ""], ["Saponjski", "Jelena", ""], ["B\u00fchler", "Katja", ""]]}, {"id": "2008.12571", "submitter": "Waheeda Saib", "authors": "Waheeda Saib, David Sengeh, Gcininwe Dlamini, Elvira Singh", "title": "Hierarchical Deep Learning Ensemble to Automate the Classification of\n  Breast Cancer Pathology Reports by ICD-O Topography", "comments": "Accepted to KDD workshop on Machine Learning for Medicine and\n  Healthcare, August 2018, London UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Like most global cancer registries, the National Cancer Registry in South\nAfrica employs expert human coders to label pathology reports using appropriate\nInternational Classification of Disease for Oncology (ICD-O) codes spanning 42\ndifferent cancer types. The annotation is extensive for the large volume of\ncancer pathology reports the registry receives annually from public and private\nsector institutions. This manual process, coupled with other challenges results\nin a significant 4-year lag in reporting of annual cancer statistics in South\nAfrica. We present a hierarchical deep learning ensemble method incorporating\nstate of the art convolutional neural network models for the automatic\nlabelling of 2201 de-identified, free text pathology reports, with appropriate\nICD-O breast cancer topography codes across 8 classes. Our results show an\nimprovement in primary site classification over the state of the art CNN model\nby greater than 14% for F1 micro and 55% for F1 macro scores. We demonstrate\nthat the hierarchical deep learning ensemble improves on state-of-the-art\nmodels for ICD-O topography classification in comparison to a flat multiclass\nmodel for predicting ICD-O topography codes for pathology reports.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 10:29:56 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Saib", "Waheeda", ""], ["Sengeh", "David", ""], ["Dlamini", "Gcininwe", ""], ["Singh", "Elvira", ""]]}, {"id": "2008.12577", "submitter": "Marco Rudolph", "authors": "Marco Rudolph and Bastian Wandt and Bodo Rosenhahn", "title": "Same Same But DifferNet: Semi-Supervised Defect Detection with\n  Normalizing Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of manufacturing errors is crucial in fabrication processes to\nensure product quality and safety standards. Since many defects occur very\nrarely and their characteristics are mostly unknown a priori, their detection\nis still an open research question. To this end, we propose DifferNet: It\nleverages the descriptiveness of features extracted by convolutional neural\nnetworks to estimate their density using normalizing flows. Normalizing flows\nare well-suited to deal with low dimensional data distributions. However, they\nstruggle with the high dimensionality of images. Therefore, we employ a\nmulti-scale feature extractor which enables the normalizing flow to assign\nmeaningful likelihoods to the images. Based on these likelihoods we develop a\nscoring function that indicates defects. Moreover, propagating the score back\nto the image enables pixel-wise localization. To achieve a high robustness and\nperformance we exploit multiple transformations in training and evaluation. In\ncontrast to most other methods, ours does not require a large number of\ntraining samples and performs well with as low as 16 images. We demonstrate the\nsuperior performance over existing approaches on the challenging and newly\nproposed MVTec AD and Magnetic Tile Defects datasets.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 10:49:28 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Rudolph", "Marco", ""], ["Wandt", "Bastian", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "2008.12599", "submitter": "Shaoshuai Shi", "authors": "Shaoshuai Shi, Chaoxu Guo, Jihan Yang, Hongsheng Li", "title": "PV-RCNN: The Top-Performing LiDAR-only Solutions for 3D Detection / 3D\n  Tracking / Domain Adaptation of Waymo Open Dataset Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report, we present the top-performing LiDAR-only solutions\nfor 3D detection, 3D tracking and domain adaptation three tracks in Waymo Open\nDataset Challenges 2020. Our solutions for the competition are built upon our\nrecent proposed PV-RCNN 3D object detection framework. Several variants of our\nPV-RCNN are explored, including temporal information incorporation, dynamic\nvoxelization, adaptive training sample selection, classification with RoI\nfeatures, etc. A simple model ensemble strategy with non-maximum-suppression\nand box voting is adopted to generate the final results. By using only LiDAR\npoint cloud data, our models finally achieve the 1st place among all LiDAR-only\nmethods, and the 2nd place among all multi-modal methods, on the 3D Detection,\n3D Tracking and Domain Adaptation three tracks of Waymo Open Dataset\nChallenges. Our solutions will be available at\nhttps://github.com/open-mmlab/OpenPCDet\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 12:05:26 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Shi", "Shaoshuai", ""], ["Guo", "Chaoxu", ""], ["Yang", "Jihan", ""], ["Li", "Hongsheng", ""]]}, {"id": "2008.12602", "submitter": "Amirreza Mahbod", "authors": "Amirreza Mahbod, Philipp Tschandl, Georg Langs, Rupert Ecker, Isabella\n  Ellinger", "title": "The Effects of Skin Lesion Segmentation on the Performance of\n  Dermatoscopic Image Classification", "comments": "Accepted to the Computer Methods and Programs in Biomedicine journal", "journal-ref": null, "doi": "10.1016/j.cmpb.2020.105725", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malignant melanoma (MM) is one of the deadliest types of skin cancer.\nAnalysing dermatoscopic images plays an important role in the early detection\nof MM and other pigmented skin lesions. Among different computer-based methods,\ndeep learning-based approaches and in particular convolutional neural networks\nhave shown excellent classification and segmentation performances for\ndermatoscopic skin lesion images. These models can be trained end-to-end\nwithout requiring any hand-crafted features. However, the effect of using\nlesion segmentation information on classification performance has remained an\nopen question. In this study, we explicitly investigated the impact of using\nskin lesion segmentation masks on the performance of dermatoscopic image\nclassification. To do this, first, we developed a baseline classifier as the\nreference model without using any segmentation masks. Then, we used either\nmanually or automatically created segmentation masks in both training and test\nphases in different scenarios and investigated the classification performances.\nEvaluated on the ISIC 2017 challenge dataset which contained two binary\nclassification tasks (i.e. MM vs. all and seborrheic keratosis (SK) vs. all)\nand based on the derived area under the receiver operating characteristic curve\nscores, we observed four main outcomes. Our results show that 1) using\nsegmentation masks did not significantly improve the MM classification\nperformance in any scenario, 2) in one of the scenarios (using segmentation\nmasks for dilated cropping), SK classification performance was significantly\nimproved, 3) removing all background information by the segmentation masks\nsignificantly degraded the overall classification performance, and 4) in case\nof using the appropriate scenario (using segmentation for dilated cropping),\nthere is no significant difference of using manually or automatically created\nsegmentation masks.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 12:17:25 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Mahbod", "Amirreza", ""], ["Tschandl", "Philipp", ""], ["Langs", "Georg", ""], ["Ecker", "Rupert", ""], ["Ellinger", "Isabella", ""]]}, {"id": "2008.12603", "submitter": "Alzayat Saleh", "authors": "Alzayat Saleh, Issam H. Laradji, Dmitry A. Konovalov, Michael Bradley,\n  David Vazquez, and Marcus Sheaves", "title": "A Realistic Fish-Habitat Dataset to Evaluate Algorithms for Underwater\n  Visual Analysis", "comments": "10 pages, 5 figures, 3 tables, Accepted for Publication in Scientific\n  Reports (Nature) 14 August 2020", "journal-ref": null, "doi": "10.1038/s41598-020-71639-x", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual analysis of complex fish habitats is an important step towards\nsustainable fisheries for human consumption and environmental protection. Deep\nLearning methods have shown great promise for scene analysis when trained on\nlarge-scale datasets. However, current datasets for fish analysis tend to focus\non the classification task within constrained, plain environments which do not\ncapture the complexity of underwater fish habitats. To address this limitation,\nwe present DeepFish as a benchmark suite with a large-scale dataset to train\nand test methods for several computer vision tasks. The dataset consists of\napproximately 40 thousand images collected underwater from 20 \\green{habitats\nin the} marine-environments of tropical Australia. The dataset originally\ncontained only classification labels. Thus, we collected point-level and\nsegmentation labels to have a more comprehensive fish analysis benchmark. These\nlabels enable models to learn to automatically monitor fish count, identify\ntheir locations, and estimate their sizes. Our experiments provide an in-depth\nanalysis of the dataset characteristics, and the performance evaluation of\nseveral state-of-the-art approaches based on our benchmark. Although models\npre-trained on ImageNet have successfully performed on this benchmark, there is\nstill room for improvement. Therefore, this benchmark serves as a testbed to\nmotivate further development in this challenging domain of underwater computer\nvision. Code is available at: https://github.com/alzayats/DeepFish\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 12:20:59 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Saleh", "Alzayat", ""], ["Laradji", "Issam H.", ""], ["Konovalov", "Dmitry A.", ""], ["Bradley", "Michael", ""], ["Vazquez", "David", ""], ["Sheaves", "Marcus", ""]]}, {"id": "2008.12606", "submitter": "Yurui Ren", "authors": "Yurui Ren and Ge Li and Shan Liu and Thomas H. Li", "title": "Deep Spatial Transformation for Pose-Guided Person Image Generation and\n  Animation", "comments": "arXiv admin note: text overlap with arXiv:2003.00696", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pose-guided person image generation and animation aim to transform a source\nperson image to target poses. These tasks require spatial manipulation of\nsource data. However, Convolutional Neural Networks are limited by the lack of\nability to spatially transform the inputs. In this paper, we propose a\ndifferentiable global-flow local-attention framework to reassemble the inputs\nat the feature level. This framework first estimates global flow fields between\nsources and targets. Then, corresponding local source feature patches are\nsampled with content-aware local attention coefficients. We show that our\nframework can spatially transform the inputs in an efficient manner. Meanwhile,\nwe further model the temporal consistency for the person image animation task\nto generate coherent videos. The experiment results of both image generation\nand animation tasks demonstrate the superiority of our model. Besides,\nadditional results of novel view synthesis and face image animation show that\nour model is applicable to other tasks requiring spatial transformation. The\nsource code of our project is available at\nhttps://github.com/RenYurui/Global-Flow-Local-Attention.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 08:59:44 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Ren", "Yurui", ""], ["Li", "Ge", ""], ["Liu", "Shan", ""], ["Li", "Thomas H.", ""]]}, {"id": "2008.12617", "submitter": "Dilip K. Prasad", "authors": "Arif Ahmed Sekh and Ida S. Opstad and Rohit Agarwal and Asa Birna\n  Birgisdottir and Truls Myrmel and Balpreet Singh Ahluwalia and Krishna\n  Agarwal and Dilip K. Prasad", "title": "Simulation-supervised deep learning for analysing organelles states and\n  behaviour in living cells", "comments": "under review at NIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world scientific problems, generating ground truth (GT) for\nsupervised learning is almost impossible. The causes include limitations\nimposed by scientific instrument, physical phenomenon itself, or the complexity\nof modeling. Performing artificial intelligence (AI) tasks such as\nsegmentation, tracking, and analytics of small sub-cellular structures such as\nmitochondria in microscopy videos of living cells is a prime example. The 3D\nblurring function of microscope, digital resolution from pixel size, optical\nresolution due to the character of light, noise characteristics, and complex 3D\ndeformable shapes of mitochondria, all contribute to making this problem GT\nhard. Manual segmentation of 100s of mitochondria across 1000s of frames and\nthen across many such videos is not only herculean but also physically\ninaccurate because of the instrument and phenomena imposed limitations.\nUnsupervised learning produces less than optimal results and accuracy is\nimportant if inferences relevant to therapy are to be derived. In order to\nsolve this unsurmountable problem, we bring modeling and deep learning to a\nnexus. We show that accurate physics based modeling of microscopy data\nincluding all its limitations can be the solution for generating simulated\ntraining datasets for supervised learning. We show here that our\nsimulation-supervised segmentation approach is a great enabler for studying\nmitochondrial states and behaviour in heart muscle cells, where mitochondria\nhave a significant role to play in the health of the cells. We report\nunprecedented mean IoU score of 91% for binary segmentation (19% better than\nthe best performing unsupervised approach) of mitochondria in actual microscopy\nvideos of living cells. We further demonstrate the possibility of performing\nmulti-class classification, tracking, and morphology associated analytics at\nthe scale of individual mitochondrion.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 19:53:46 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Sekh", "Arif Ahmed", ""], ["Opstad", "Ida S.", ""], ["Agarwal", "Rohit", ""], ["Birgisdottir", "Asa Birna", ""], ["Myrmel", "Truls", ""], ["Ahluwalia", "Balpreet Singh", ""], ["Agarwal", "Krishna", ""], ["Prasad", "Dilip K.", ""]]}, {"id": "2008.12650", "submitter": "Peter Meer", "authors": "Peter Meer", "title": "Are Deep Neural Networks \"Robust\"?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Separating outliers from inliers is the definition of robustness in computer\nvision. This essay delineates how deep neural networks are different than\ntypical robust estimators. Deep neural networks not robust by this traditional\ndefinition.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 16:57:19 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Meer", "Peter", ""]]}, {"id": "2008.12664", "submitter": "Daryl Peralta", "authors": "Daryl Peralta, Joel Casimiro, Aldrin Michael Nilles, Justine Aletta\n  Aguilar, Rowel Atienza and Rhandley Cajote", "title": "Next-Best View Policy for 3D Reconstruction", "comments": "To be published in ECCV 2020 Workshops; typos in abstract corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manually selecting viewpoints or using commonly available flight planners\nlike circular path for large-scale 3D reconstruction using drones often results\nin incomplete 3D models. Recent works have relied on hand-engineered heuristics\nsuch as information gain to select the Next-Best Views. In this work, we\npresent a learning-based algorithm called Scan-RL to learn a Next-Best View\n(NBV) Policy. To train and evaluate the agent, we created Houses3K, a dataset\nof 3D house models. Our experiments show that using Scan-RL, the agent can scan\nhouses with fewer number of steps and a shorter distance compared to our\nbaseline circular path. Experimental results also demonstrate that a single NBV\npolicy can be used to scan multiple houses including those that were not seen\nduring training. The link to Scan-RL is available at\nhttps://github.com/darylperalta/ScanRL and Houses3K dataset can be found at\nhttps://github.com/darylperalta/Houses3K.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 14:03:59 GMT"}, {"version": "v2", "created": "Sun, 6 Sep 2020 15:30:45 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Peralta", "Daryl", ""], ["Casimiro", "Joel", ""], ["Nilles", "Aldrin Michael", ""], ["Aguilar", "Justine Aletta", ""], ["Atienza", "Rowel", ""], ["Cajote", "Rhandley", ""]]}, {"id": "2008.12679", "submitter": "Weidong Yin", "authors": "Weidong Yin, Ziwei Liu, Leonid Sigal", "title": "Person-in-Context Synthesiswith Compositional Structural Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant progress, controlled generation of complex images with\ninteracting people remains difficult. Existing layout generation methods fall\nshort of synthesizing realistic person instances; while pose-guided generation\napproaches focus on a single person and assume simple or known backgrounds. To\ntackle these limitations, we propose a new problem, \\textbf{Persons in Context\nSynthesis}, which aims to synthesize diverse person instance(s) in consistent\ncontexts, with user control over both. The context is specified by the bounding\nbox object layout which lacks shape information, while pose of the person(s) by\nkeypoints which are sparsely annotated. To handle the stark difference in input\nstructures, we proposed two separate neural branches to attentively composite\nthe respective (context/person) inputs into shared ``compositional structural\nspace'', which encodes shape, location and appearance information for both\ncontext and person structures in a disentangled manner. This structural space\nis then decoded to the image space using multi-level feature modulation\nstrategy, and learned in a self supervised manner from image collections and\ntheir corresponding inputs. Extensive experiments on two large-scale datasets\n(COCO-Stuff \\cite{caesar2018cvpr} and Visual Genome \\cite{krishna2017visual})\ndemonstrate that our framework outperforms state-of-the-art methods w.r.t.\nsynthesis quality.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 14:33:28 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Yin", "Weidong", ""], ["Liu", "Ziwei", ""], ["Sigal", "Leonid", ""]]}, {"id": "2008.12680", "submitter": "Jyotirmay Senapati", "authors": "J. Senapati, A. Guha Roy, S. P\\\"olsterl, D. Gutmann, S. Gatidis, C.\n  Schlett, A. Peters, F. Bamberg, C. Wachinger", "title": "Bayesian Neural Networks for Uncertainty Estimation of Imaging\n  Biomarkers", "comments": "MICCAI-MLMI 2020 Workshop Paper (Accepted)", "journal-ref": null, "doi": "10.1007/978-3-030-59861-7_28", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation enables to extract quantitative measures from scans that\ncan serve as imaging biomarkers for diseases. However, segmentation quality can\nvary substantially across scans, and therefore yield unfaithful estimates in\nthe follow-up statistical analysis of biomarkers. The core problem is that\nsegmentation and biomarker analysis are performed independently. We propose to\npropagate segmentation uncertainty to the statistical analysis to account for\nvariations in segmentation confidence. To this end, we evaluate four Bayesian\nneural networks to sample from the posterior distribution and estimate the\nuncertainty. We then assign confidence measures to the biomarker and propose\nstatistical models for its integration in group analysis and disease\nclassification. Our results for segmenting the liver in patients with diabetes\nmellitus clearly demonstrate the improvement of integrating biomarker\nuncertainty in the statistical inference.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 14:34:12 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 23:22:54 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Senapati", "J.", ""], ["Roy", "A. Guha", ""], ["P\u00f6lsterl", "S.", ""], ["Gutmann", "D.", ""], ["Gatidis", "S.", ""], ["Schlett", "C.", ""], ["Peters", "A.", ""], ["Bamberg", "F.", ""], ["Wachinger", "C.", ""]]}, {"id": "2008.12700", "submitter": "Giuseppe Cattaneo", "authors": "Andrea Bruno, Giuseppe Cattaneo and Paola Capasso", "title": "On the Reliability of the PNU for Source Camera Identification Tasks", "comments": "14 pages, 7 figures, to be presented to IWBDAF on 2021, 11 Jan", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The PNU is an essential and reliable tool to perform SCI and, during the\nyears, became a standard de-facto for this task in the forensic field. In this\npaper, we show that, although strategies exist that aim to cancel, modify,\nreplace the PNU traces in a digital camera image, it is still possible, through\nour experimental method, to find residual traces of the noise produced by the\nsensor used to shoot the photo. Furthermore, we show that is possible to inject\nthe PNU of a different camera in a target image and trace it back to the source\ncamera, but only under the condition that the new camera is of the same model\nof the original one used to take the target image. Both cameras must fall\nwithin our availability.\n  For completeness, we carried out 2 experiments and, rather than using the\npopular public reference dataset, CASIA TIDE, we preferred to introduce a\ndataset that does not present any kind of statistical artifacts.\n  A preliminary experiment on a small dataset of smartphones showed that the\ninjection of PNU from a different device makes it impossible to identify the\nsource camera correctly.\n  For a second experiment, we built a large dataset of images taken with the\nsame model DSLR. We extracted a denoised version of each image, injected each\none with the RN of all the cameras in the dataset and compared all with a RP\nfrom each camera. The results of the experiments, clearly, show that either in\nthe denoised images and the injected ones is possible to find residual traces\nof the original camera PNU.\n  The combined results of the experiments show that, even in theory is possible\nto remove or replace the \\ac{PNU} from an image, this process can be, easily,\ndetected and is possible, under some hard conditions, confirming the robustness\nof the \\ac{PNU} under this type of attacks.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 15:15:31 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Bruno", "Andrea", ""], ["Cattaneo", "Giuseppe", ""], ["Capasso", "Paola", ""]]}, {"id": "2008.12709", "submitter": "Roman Shapovalov", "authors": "David Novotny, Roman Shapovalov, Andrea Vedaldi", "title": "Canonical 3D Deformer Maps: Unifying parametric and non-parametric\n  methods for dense weakly-supervised category reconstruction", "comments": "Published at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Canonical 3D Deformer Map, a new representation of the 3D\nshape of common object categories that can be learned from a collection of 2D\nimages of independent objects. Our method builds in a novel way on concepts\nfrom parametric deformation models, non-parametric 3D reconstruction, and\ncanonical embeddings, combining their individual advantages. In particular, it\nlearns to associate each image pixel with a deformation model of the\ncorresponding 3D object point which is canonical, i.e. intrinsic to the\nidentity of the point and shared across objects of the category. The result is\na method that, given only sparse 2D supervision at training time, can, at test\ntime, reconstruct the 3D shape and texture of objects from single views, while\nestablishing meaningful dense correspondences between object instances. It also\nachieves state-of-the-art results in dense 3D reconstruction on public\nin-the-wild datasets of faces, cars, and birds.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 15:44:05 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2020 11:59:06 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Novotny", "David", ""], ["Shapovalov", "Roman", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "2008.12750", "submitter": "Dimitris Perdios", "authors": "Dimitris Perdios, Manuel Vonlanthen, Florian Martinez, Marcel Arditi,\n  Jean-Philippe Thiran", "title": "CNN-Based Image Reconstruction Method for Ultrafast Ultrasound Imaging", "comments": "Main: 15 pages (6 figures). Supplement: 9 pages (10 figures). Video\n  provided as ancillary file. This work has been submitted to the IEEE\n  Transactions on Ultrasonics, Ferroelectrics, and Frequency Control for\n  possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrafast ultrasound (US) revolutionized biomedical imaging with its\ncapability of acquiring full-view frames at over 1 kHz, unlocking breakthrough\nmodalities such as shear-wave elastography and functional US neuroimaging. Yet,\nit suffers from strong diffraction artifacts, mainly caused by grating lobes,\nside lobes, or edge waves. Multiple acquisitions are typically required to\nobtain a sufficient image quality, at the cost of a reduced frame rate. To\nanswer the increasing demand for high-quality imaging from single-shot\nacquisitions, we propose a two-step convolutional neural network (CNN)-based\nimage reconstruction method, compatible with real-time imaging. A low-quality\nestimate is obtained by means of a backprojection-based operation, akin to\nconventional delay-and-sum beamforming, from which a high-quality image is\nrestored using a residual CNN with multi-scale and multi-channel filtering\nproperties, trained specifically to remove the diffraction artifacts inherent\nto ultrafast US imaging. To account for both the high dynamic range and the\nradio frequency property of US images, we introduce the mean signed logarithmic\nabsolute error (MSLAE) as training loss function. Experiments were conducted\nwith a linear transducer array, in single plane wave (PW) imaging. Trainings\nwere performed on a simulated dataset, crafted to contain a wide diversity of\nstructures and echogenicities. Extensive numerical evaluations demonstrate that\nthe proposed approach can reconstruct images from single PWs with a quality\nsimilar to that of gold-standard synthetic aperture imaging, on a dynamic range\nin excess of 60 dB. In vitro and in vivo experiments show that trainings\nperformed on simulated data translate well to experimental settings.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 17:15:37 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Perdios", "Dimitris", ""], ["Vonlanthen", "Manuel", ""], ["Martinez", "Florian", ""], ["Arditi", "Marcel", ""], ["Thiran", "Jean-Philippe", ""]]}, {"id": "2008.12760", "submitter": "Roozbeh Mottaghi", "authors": "Luca Weihs, Jordi Salvador, Klemen Kotar, Unnat Jain, Kuo-Hao Zeng,\n  Roozbeh Mottaghi, Aniruddha Kembhavi", "title": "AllenAct: A Framework for Embodied AI Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The domain of Embodied AI, in which agents learn to complete tasks through\ninteraction with their environment from egocentric observations, has\nexperienced substantial growth with the advent of deep reinforcement learning\nand increased interest from the computer vision, NLP, and robotics communities.\nThis growth has been facilitated by the creation of a large number of simulated\nenvironments (such as AI2-THOR, Habitat and CARLA), tasks (like point\nnavigation, instruction following, and embodied question answering), and\nassociated leaderboards. While this diversity has been beneficial and organic,\nit has also fragmented the community: a huge amount of effort is required to do\nsomething as simple as taking a model trained in one environment and testing it\nin another. This discourages good science. We introduce AllenAct, a modular and\nflexible learning framework designed with a focus on the unique requirements of\nEmbodied AI research. AllenAct provides first-class support for a growing\ncollection of embodied environments, tasks and algorithms, provides\nreproductions of state-of-the-art models and includes extensive documentation,\ntutorials, start-up code, and pre-trained models. We hope that our framework\nmakes Embodied AI more accessible and encourages new researchers to join this\nexciting area. The framework can be accessed at: https://allenact.org/\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 17:35:22 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Weihs", "Luca", ""], ["Salvador", "Jordi", ""], ["Kotar", "Klemen", ""], ["Jain", "Unnat", ""], ["Zeng", "Kuo-Hao", ""], ["Mottaghi", "Roozbeh", ""], ["Kembhavi", "Aniruddha", ""]]}, {"id": "2008.12818", "submitter": "Niki Efthymiou", "authors": "Niki Efthymiou, Panagiotis P. Filntisis, Petros Koutras, Antigoni\n  Tsiami, Jack Hadfield, Gerasimos Potamianos, Petros Maragos", "title": "ChildBot: Multi-Robot Perception and Interaction with Children", "comments": "19 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an integrated robotic system capable of\nparticipating in and performing a wide range of educational and entertainment\ntasks, in collaboration with one or more children. The system, called ChildBot,\nfeatures multimodal perception modules and multiple robotic agents that monitor\nthe interaction environment, and can robustly coordinate complex Child-Robot\nInteraction use-cases. In order to validate the effectiveness of the system and\nits integrated modules, we have conducted multiple experiments with a total of\n52 children. Our results show improved perception capabilities in comparison to\nour earlier works that ChildBot was based on. In addition, we have conducted a\npreliminary user experience study, employing some educational/entertainment\ntasks, that yields encouraging results regarding the technical validity of our\nsystem and initial insights on the user experience with it.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 19:07:28 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Efthymiou", "Niki", ""], ["Filntisis", "Panagiotis P.", ""], ["Koutras", "Petros", ""], ["Tsiami", "Antigoni", ""], ["Hadfield", "Jack", ""], ["Potamianos", "Gerasimos", ""], ["Maragos", "Petros", ""]]}, {"id": "2008.12839", "submitter": "Prithvijit Chattopadhyay Chattopadhyay", "authors": "Prithvijit Chattopadhyay, Yogesh Balaji, Judy Hoffman", "title": "Learning to Balance Specificity and Invariance for In and Out of Domain\n  Generalization", "comments": "Published at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Domain-specific Masks for Generalization, a model for improving\nboth in-domain and out-of-domain generalization performance. For domain\ngeneralization, the goal is to learn from a set of source domains to produce a\nsingle model that will best generalize to an unseen target domain. As such,\nmany prior approaches focus on learning representations which persist across\nall source domains with the assumption that these domain agnostic\nrepresentations will generalize well. However, often individual domains contain\ncharacteristics which are unique and when leveraged can significantly aid\nin-domain recognition performance. To produce a model which best generalizes to\nboth seen and unseen domains, we propose learning domain specific masks. The\nmasks are encouraged to learn a balance of domain-invariant and domain-specific\nfeatures, thus enabling a model which can benefit from the predictive power of\nspecialized features while retaining the universal applicability of\ndomain-invariant features. We demonstrate competitive performance compared to\nnaive baselines and state-of-the-art methods on both PACS and DomainNet.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 20:39:51 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Chattopadhyay", "Prithvijit", ""], ["Balaji", "Yogesh", ""], ["Hoffman", "Judy", ""]]}, {"id": "2008.12860", "submitter": "Gagik Gavalian", "authors": "Gagik Gavalian, Polykarpos Thomadakis, Angelos Angelopoulos, Veronique\n  Ziegler, Nikos Chrisochoides", "title": "Using Artificial Intelligence for Particle Track Identification in\n  CLAS12 Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we describe the development of machine learning models to\nassist the CLAS12 tracking algorithm by identifying the best track candidates\nfrom combinatorial track candidates from the hits in drift chambers. Several\ntypes of machine learning models were tested, including: Convolutional Neural\nNetworks (CNN), Multi-Layer Perceptron (MLP) and Extremely Randomized Trees\n(ERT). The final implementation was based on an MLP network and provided an\naccuracy $>99\\%$. The implementation of AI assisted tracking into the CLAS12\nreconstruction workflow and provided a 6 times code speedup.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 21:57:38 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Gavalian", "Gagik", ""], ["Thomadakis", "Polykarpos", ""], ["Angelopoulos", "Angelos", ""], ["Ziegler", "Veronique", ""], ["Chrisochoides", "Nikos", ""]]}, {"id": "2008.12873", "submitter": "Fait Poms", "authors": "Ravi Teja Mullapudi, Fait Poms, William R. Mark, Deva Ramanan, Kayvon\n  Fatahalian", "title": "Background Splitting: Finding Rare Classes in a Sea of Background", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the real-world problem of training accurate deep models for image\nclassification of a small number of rare categories. In these scenarios, almost\nall images belong to the background category in the dataset (>95% of the\ndataset is background). We demonstrate that both standard fine-tuning\napproaches and state-of-the-art approaches for training on imbalanced datasets\ndo not produce accurate deep models in the presence of this extreme imbalance.\nOur key observation is that the extreme imbalance due to the background\ncategory can be drastically reduced by leveraging visual knowledge from an\nexisting pre-trained model. Specifically, the background category is \"split\"\ninto smaller and more coherent pseudo-categories during training using a\npre-trained model. We incorporate background splitting into an image\nclassification model by adding an auxiliary loss that learns to mimic the\npredictions of the existing, pre-trained image classification model. Note that\nthis process is automatic and requires no additional manual labels. The\nauxiliary loss regularizes the feature representation of the shared network\ntrunk by requiring it to discriminate between previously homogeneous background\ninstances and reduces overfitting to the small number of rare category\npositives. We also show that BG splitting can be combined with other background\nimbalance methods to further improve performance. We evaluate our method on a\nmodified version of the iNaturalist dataset where only a small subset of rare\ncategory labels are available during training (all other images are labeled as\nbackground). By jointly learning to recognize ImageNet categories and selected\niNaturalist categories, our approach yields performance that is 42.3 mAP points\nhigher than a fine-tuning baseline when 99.98% of the data is background, and\n8.3 mAP points higher than SotA baselines when 98.30% of the data is\nbackground.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 23:05:15 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Mullapudi", "Ravi Teja", ""], ["Poms", "Fait", ""], ["Mark", "William R.", ""], ["Ramanan", "Deva", ""], ["Fatahalian", "Kayvon", ""]]}, {"id": "2008.12894", "submitter": "Junaid Malik", "authors": "Junaid Malik, Serkan Kiranyaz, Moncef Gabbouj", "title": "Self-Organized Operational Neural Networks for Severe Image Restoration\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Discriminative learning based on convolutional neural networks (CNNs) aims to\nperform image restoration by learning from training examples of noisy-clean\nimage pairs. It has become the go-to methodology for tackling image restoration\nand has outperformed the traditional non-local class of methods. However, the\ntop-performing networks are generally composed of many convolutional layers and\nhundreds of neurons, with trainable parameters in excess of several millions.\nWe claim that this is due to the inherent linear nature of convolution-based\ntransformation, which is inadequate for handling severe restoration problems.\nRecently, a non-linear generalization of CNNs, called the operational neural\nnetworks (ONN), has been shown to outperform CNN on AWGN denoising. However,\nits formulation is burdened by a fixed collection of well-known nonlinear\noperators and an exhaustive search to find the best possible configuration for\na given architecture, whose efficacy is further limited by a fixed output layer\noperator assignment. In this study, we leverage the Taylor series-based\nfunction approximation to propose a self-organizing variant of ONNs, Self-ONNs,\nfor image restoration, which synthesizes novel nodal transformations onthe-fly\nas part of the learning process, thus eliminating the need for redundant\ntraining runs for operator search. In addition, it enables a finer level of\noperator heterogeneity by diversifying individual connections of the receptive\nfields and weights. We perform a series of extensive ablation experiments\nacross three severe image restoration tasks. Even when a strict equivalence of\nlearnable parameters is imposed, Self-ONNs surpass CNNs by a considerable\nmargin across all problems, improving the generalization performance by up to 3\ndB in terms of PSNR.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 02:19:41 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Malik", "Junaid", ""], ["Kiranyaz", "Serkan", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "2008.12904", "submitter": "Hossein Soleimani", "authors": "Hossein Soleimani and Oleg V.Michailovich", "title": "On segmentation of pectoralis muscle in digital mammograms by means of\n  deep learning", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-aided diagnosis (CAD) has long become an integral part of\nradiological management of breast disease, facilitating a number of important\nclinical applications, including quantitative assessment of breast density and\nearly detection of malignancies based on X-ray mammography. Common to such\napplications is the need to automatically discriminate between breast tissue\nand adjacent anatomy, with the latter being predominantly represented by\npectoralis major (or pectoral muscle). Especially in the case of mammograms\nacquired in the mediolateral oblique (MLO) view, the muscle is easily\nconfusable with some elements of breast anatomy due to their morphological and\nphotometric similarity. As a result, the problem of automatic detection and\nsegmentation of pectoral muscle in MLO mammograms remains a challenging task,\ninnovative approaches to which are still required and constantly searched for.\nTo address this problem, the present paper introduces a two-step segmentation\nstrategy based on a combined use of data-driven prediction (deep learning) and\ngraph-based image processing. In particular, the proposed method employs a\nconvolutional neural network (CNN) which is designed to predict the location of\nbreast-pectoral boundary at different levels of spatial resolution.\nSubsequently, the predictions are used by the second stage of the algorithm, in\nwhich the desired boundary is recovered as a solution to the shortest path\nproblem on a specially designed graph. The proposed algorithm has been tested\non three different datasets (i.e., MIAS, CBIS-DDSm and InBreast) using a range\nof quantitative metrics. The results of comparative analysis show considerable\nimprovement over state-of-the-art, while offering the possibility of model-free\nand fully automatic processing.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 03:38:11 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Soleimani", "Hossein", ""], ["Michailovich", "Oleg V.", ""]]}, {"id": "2008.12912", "submitter": "Abdul Muqeet", "authors": "Abdul Muqeet, Jiwon Hwang, Subin Yang, Jung Heum Kang, Yongwoo Kim,\n  Sung-Ho Bae", "title": "Multi-Attention Based Ultra Lightweight Image Super-Resolution", "comments": "ECCVW AIM2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lightweight image super-resolution (SR) networks have the utmost significance\nfor real-world applications. There are several deep learning based SR methods\nwith remarkable performance, but their memory and computational cost are\nhindrances in practical usage. To tackle this problem, we propose a\nMulti-Attentive Feature Fusion Super-Resolution Network (MAFFSRN). MAFFSRN\nconsists of proposed feature fusion groups (FFGs) that serve as a feature\nextraction block. Each FFG contains a stack of proposed multi-attention blocks\n(MAB) that are combined in a novel feature fusion structure. Further, the MAB\nwith a cost-efficient attention mechanism (CEA) helps us to refine and extract\nthe features using multiple attention mechanisms. The comprehensive experiments\nshow the superiority of our model over the existing state-of-the-art. We\nparticipated in AIM 2020 efficient SR challenge with our MAFFSRN model and won\n1st, 3rd, and 4th places in memory usage, floating-point operations (FLOPs) and\nnumber of parameters, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 05:19:32 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 06:07:14 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Muqeet", "Abdul", ""], ["Hwang", "Jiwon", ""], ["Yang", "Subin", ""], ["Kang", "Jung Heum", ""], ["Kim", "Yongwoo", ""], ["Bae", "Sung-Ho", ""]]}, {"id": "2008.12949", "submitter": "Kagan Incetan", "authors": "Kagan Incetan, Ibrahim Omer Celik, Abdulhamid Obeid, Guliz Irem\n  Gokceler, Kutsev Bengisu Ozyoruk, Yasin Almalioglu, Richard J. Chen, Faisal\n  Mahmood, Hunter Gilbert, Nicholas J. Durr, Mehmet Turan", "title": "VR-Caps: A Virtual Environment for Capsule Endoscopy", "comments": "18 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current capsule endoscopes and next-generation robotic capsules for diagnosis\nand treatment of gastrointestinal diseases are complex cyber-physical platforms\nthat must orchestrate complex software and hardware functions. The desired\ntasks for these systems include visual localization, depth estimation, 3D\nmapping, disease detection and segmentation, automated navigation, active\ncontrol, path realization and optional therapeutic modules such as targeted\ndrug delivery and biopsy sampling. Data-driven algorithms promise to enable\nmany advanced functionalities for capsule endoscopes, but real-world data is\nchallenging to obtain. Physically-realistic simulations providing synthetic\ndata have emerged as a solution to the development of data-driven algorithms.\nIn this work, we present a comprehensive simulation platform for capsule\nendoscopy operations and introduce VR-Caps, a virtual active capsule\nenvironment that simulates a range of normal and abnormal tissue conditions\n(e.g., inflated, dry, wet etc.) and varied organ types, capsule endoscope\ndesigns (e.g., mono, stereo, dual and 360{\\deg}camera), and the type, number,\nstrength, and placement of internal and external magnetic sources that enable\nactive locomotion. VR-Caps makes it possible to both independently or jointly\ndevelop, optimize, and test medical imaging and analysis software for the\ncurrent and next-generation endoscopic capsule systems. To validate this\napproach, we train state-of-the-art deep neural networks to accomplish various\nmedical image analysis tasks using simulated data from VR-Caps and evaluate the\nperformance of these models on real medical data. Results demonstrate the\nusefulness and effectiveness of the proposed virtual platform in developing\nalgorithms that quantify fractional coverage, camera trajectory, 3D map\nreconstruction, and disease classification.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 09:54:05 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 12:55:11 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Incetan", "Kagan", ""], ["Celik", "Ibrahim Omer", ""], ["Obeid", "Abdulhamid", ""], ["Gokceler", "Guliz Irem", ""], ["Ozyoruk", "Kutsev Bengisu", ""], ["Almalioglu", "Yasin", ""], ["Chen", "Richard J.", ""], ["Mahmood", "Faisal", ""], ["Gilbert", "Hunter", ""], ["Durr", "Nicholas J.", ""], ["Turan", "Mehmet", ""]]}, {"id": "2008.12950", "submitter": "Geesara Prathap Kulathunga", "authors": "Geesara Kulathunga, Dmitry Devitt, Roman Fedorenko, Sergei Savin and\n  Alexandr Klimchik", "title": "Path Planning Followed by Kinodynamic Smoothing for Multirotor Aerial\n  Vehicles (MAVs)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore path planning followed by kinodynamic smoothing while ensuring the\nvehicle dynamics feasibility for MAVs. We have chosen a geometrically based\nmotion planning technique \\textquotedblleft RRT*\\textquotedblright\\; for this\npurpose. In the proposed technique, we modified original RRT* introducing an\nadaptive search space and a steering function which help to increase the\nconsistency of the planner. Moreover, we propose multiple RRT* which generates\na set of desired paths, provided that the optimal path is selected among them.\nThen, apply kinodynamic smoothing, which will result in dynamically feasible as\nwell as obstacle-free path. Thereafter, a b spline-based trajectory is\ngenerated to maneuver vehicle autonomously in unknown environments. Finally, we\nhave tested the proposed technique in various simulated environments.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 09:55:49 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Kulathunga", "Geesara", ""], ["Devitt", "Dmitry", ""], ["Fedorenko", "Roman", ""], ["Savin", "Sergei", ""], ["Klimchik", "Alexandr", ""]]}, {"id": "2008.12958", "submitter": "YuLi Sun", "authors": "Lin Lei, Yuli Sun, Gangyao Kuang", "title": "Adaptive Local Structure Consistency based Heterogeneous Remote Sensing\n  Change Detection", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection of heterogeneous remote sensing images is an important and\nchallenging topic in remote sensing for emergency situation resulting from\nnature disaster. Due to the different imaging mechanisms of heterogeneous\nsensors, it is difficult to directly compare the images. To address this\nchallenge, we explore an unsupervised change detection method based on adaptive\nlocal structure consistency (ALSC) between heterogeneous images in this letter,\nwhich constructs an adaptive graph representing the local structure for each\npatch in one image domain and then projects this graph to the other image\ndomain to measure the change level. This local structure consistency exploits\nthe fact that the heterogeneous images share the same structure information for\nthe same ground object, which is imaging modality-invariant. To avoid the\nleakage of heterogeneous data, the pixelwise change image is calculated in the\nsame image domain by graph projection. Experiment results demonstrate the\neffectiveness of the proposed ALSC based change detection method by comparing\nwith some state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 10:33:43 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 02:59:30 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Lei", "Lin", ""], ["Sun", "Yuli", ""], ["Kuang", "Gangyao", ""]]}, {"id": "2008.12959", "submitter": "Mohammadreza Salehi Dehnavi", "authors": "Mohammadreza Salehi, Ainaz Eftekhar, Niousha Sadjadi, Mohammad Hossein\n  Rohban, Hamid R. Rabiee", "title": "Puzzle-AE: Novelty Detection in Images through Solving Puzzles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoencoder, as an essential part of many anomaly detection methods, is\nlacking flexibility on normal data in complex datasets. U-Net is proved to be\neffective for this purpose but overfits on the training data if trained by just\nusing reconstruction error similar to other AE-based frameworks.\nPuzzle-solving, as a pretext task of self-supervised learning (SSL) methods,\nhas earlier proved its ability in learning semantically meaningful features. We\nshow that training U-Nets based on this task is an effective remedy that\nprevents overfitting and facilitates learning beyond pixel-level features.\nShortcut solutions, however, are a big challenge in SSL tasks, including jigsaw\npuzzles. We propose adversarial robust training as an effective automatic\nshortcut removal. We achieve competitive or superior results compared to the\nState of the Art (SOTA) anomaly detection methods on various toy and real-world\ndatasets. Unlike many competitors, the proposed framework is stable, fast,\ndata-efficient, and does not require unprincipled early stopping.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 10:53:55 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 16:48:00 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2020 06:48:14 GMT"}, {"version": "v4", "created": "Mon, 23 Nov 2020 07:10:23 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Salehi", "Mohammadreza", ""], ["Eftekhar", "Ainaz", ""], ["Sadjadi", "Niousha", ""], ["Rohban", "Mohammad Hossein", ""], ["Rabiee", "Hamid R.", ""]]}, {"id": "2008.12962", "submitter": "Bo Liu", "authors": "Bo Liu, Qiulei Dong, Zhanyi Hu", "title": "Zero-Shot Learning from Adversarial Feature Residual to Compact Visual\n  Feature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many zero-shot learning (ZSL) methods focused on learning\ndiscriminative object features in an embedding feature space, however, the\ndistributions of the unseen-class features learned by these methods are prone\nto be partly overlapped, resulting in inaccurate object recognition. Addressing\nthis problem, we propose a novel adversarial network to synthesize compact\nsemantic visual features for ZSL, consisting of a residual generator, a\nprototype predictor, and a discriminator. The residual generator is to generate\nthe visual feature residual, which is integrated with a visual prototype\npredicted via the prototype predictor for synthesizing the visual feature. The\ndiscriminator is to distinguish the synthetic visual features from the real\nones extracted from an existing categorization CNN. Since the generated\nresiduals are generally numerically much smaller than the distances among all\nthe prototypes, the distributions of the unseen-class features synthesized by\nthe proposed network are less overlapped. In addition, considering that the\nvisual features from categorization CNNs are generally inconsistent with their\nsemantic features, a simple feature selection strategy is introduced for\nextracting more compact semantic visual features. Extensive experimental\nresults on six benchmark datasets demonstrate that our method could achieve a\nsignificantly better performance than existing state-of-the-art methods by\n1.2-13.2% in most cases.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 11:16:11 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Liu", "Bo", ""], ["Dong", "Qiulei", ""], ["Hu", "Zhanyi", ""]]}, {"id": "2008.12965", "submitter": "Kyriaki-Margarita Bintsi", "authors": "Kyriaki-Margarita Bintsi, Vasileios Baltatzis, Arinbj\\\"orn\n  Kolbeinsson, Alexander Hammers, Daniel Rueckert", "title": "Patch-based Brain Age Estimation from MR Images", "comments": "Accepted (oral) at the MLCN workshop, MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain age estimation from Magnetic Resonance Images (MRI) derives the\ndifference between a subject's biological brain age and their chronological\nage. This is a potential biomarker for neurodegeneration, e.g. as part of\nAlzheimer's disease. Early detection of neurodegeneration manifesting as a\nhigher brain age can potentially facilitate better medical care and planning\nfor affected individuals. Many studies have been proposed for the prediction of\nchronological age from brain MRI using machine learning and specifically deep\nlearning techniques. Contrary to most studies, which use the whole brain\nvolume, in this study, we develop a new deep learning approach that uses 3D\npatches of the brain as well as convolutional neural networks (CNNs) to develop\na localised brain age estimator. In this way, we can obtain a visualization of\nthe regions that play the most important role for estimating brain age, leading\nto more anatomically driven and interpretable results, and thus confirming\nrelevant literature which suggests that the ventricles and the hippocampus are\nthe areas that are most informative. In addition, we leverage this knowledge in\norder to improve the overall performance on the task of age estimation by\ncombining the results of different patches using an ensemble method, such as\naveraging or linear regression. The network is trained on the UK Biobank\ndataset and the method achieves state-of-the-art results with a Mean Absolute\nError of 2.46 years for purely regional estimates, and 2.13 years for an\nensemble of patches before bias correction, while 1.96 years after bias\ncorrection.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 11:50:37 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 13:26:02 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Bintsi", "Kyriaki-Margarita", ""], ["Baltatzis", "Vasileios", ""], ["Kolbeinsson", "Arinbj\u00f6rn", ""], ["Hammers", "Alexander", ""], ["Rueckert", "Daniel", ""]]}, {"id": "2008.12967", "submitter": "Jong Chul Ye", "authors": "Gyutaek Oh, Byeongsu Sim, Hyungjin Chung, Leonard Sunwoo, and Jong\n  Chul Ye", "title": "Unpaired Deep Learning for Accelerated MRI using Optimal Transport\n  Driven CycleGAN", "comments": "Accepted for IEEE Transactions on Computational Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning approaches for accelerated MRI have been extensively\nstudied thanks to their high performance reconstruction in spite of\nsignificantly reduced runtime complexity. These neural networks are usually\ntrained in a supervised manner, so matched pairs of subsampled and fully\nsampled k-space data are required. Unfortunately, it is often difficult to\nacquire matched fully sampled k-space data, since the acquisition of fully\nsampled k-space data requires long scan time and often leads to the change of\nthe acquisition protocol. Therefore, unpaired deep learning without matched\nlabel data has become a very important research topic. In this paper, we\npropose an unpaired deep learning approach using a optimal transport driven\ncycle-consistent generative adversarial network (OT-cycleGAN) that employs a\nsingle pair of generator and discriminator. The proposed OT-cycleGAN\narchitecture is rigorously derived from a dual formulation of the optimal\ntransport formulation using a specially designed penalized least squares cost.\nThe experimental results show that our method can reconstruct high resolution\nMR images from accelerated k- space data from both single and multiple coil\nacquisition, without requiring matched reference data.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 12:02:49 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Oh", "Gyutaek", ""], ["Sim", "Byeongsu", ""], ["Chung", "Hyungjin", ""], ["Sunwoo", "Leonard", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2008.12969", "submitter": "Andreas B\\\"uhler", "authors": "Andreas B\\\"uhler, Adrien Gaidon, Andrei Cramariuc, Rares Ambrus, Guy\n  Rosman, Wolfram Burgard", "title": "Driving Through Ghosts: Behavioral Cloning with False Positives", "comments": "7 pages, 5 figures, 4 tables, accepted at 2020 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safe autonomous driving requires robust detection of other traffic\nparticipants. However, robust does not mean perfect, and safe systems typically\nminimize missed detections at the expense of a higher false positive rate. This\nresults in conservative and yet potentially dangerous behavior such as avoiding\nimaginary obstacles. In the context of behavioral cloning, perceptual errors at\ntraining time can lead to learning difficulties or wrong policies, as expert\ndemonstrations might be inconsistent with the perceived world state. In this\nwork, we propose a behavioral cloning approach that can safely leverage\nimperfect perception without being conservative. Our core contribution is a\nnovel representation of perceptual uncertainty for learning to plan. We propose\na new probabilistic birds-eye-view semantic grid to encode the noisy output of\nobject perception systems. We then leverage expert demonstrations to learn an\nimitative driving policy using this probabilistic representation. Using the\nCARLA simulator, we show that our approach can safely overcome critical false\npositives that would otherwise lead to catastrophic failures or conservative\nbehavior.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 12:10:23 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["B\u00fchler", "Andreas", ""], ["Gaidon", "Adrien", ""], ["Cramariuc", "Andrei", ""], ["Ambrus", "Rares", ""], ["Rosman", "Guy", ""], ["Burgard", "Wolfram", ""]]}, {"id": "2008.12977", "submitter": "Anne-Sophie Collin", "authors": "Anne-Sophie Collin and Christophe De Vleeschouwer", "title": "Improved anomaly detection by training an autoencoder with skip\n  connections on images corrupted with Stain-shaped noise", "comments": "8 pages, 7 figures, accepted at ICPR2020 -- Supplementary explanation\n  about the stain noise model in section III (point B)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In industrial vision, the anomaly detection problem can be addressed with an\nautoencoder trained to map an arbitrary image, i.e. with or without any defect,\nto a clean image, i.e. without any defect. In this approach, anomaly detection\nrelies conventionally on the reconstruction residual or, alternatively, on the\nreconstruction uncertainty. To improve the sharpness of the reconstruction, we\nconsider an autoencoder architecture with skip connections. In the common\nscenario where only clean images are available for training, we propose to\ncorrupt them with a synthetic noise model to prevent the convergence of the\nnetwork towards the identity mapping, and introduce an original Stain noise\nmodel for that purpose. We show that this model favors the reconstruction of\nclean images from arbitrary real-world images, regardless of the actual defects\nappearance. In addition to demonstrating the relevance of our approach, our\nvalidation provides the first consistent assessment of reconstruction-based\nmethods, by comparing their performance over the MVTec AD dataset, both for\npixel- and image-wise anomaly detection.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 13:50:49 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 17:09:53 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Collin", "Anne-Sophie", ""], ["De Vleeschouwer", "Christophe", ""]]}, {"id": "2008.12995", "submitter": "Akash Roy", "authors": "Akash Roy", "title": "AKHCRNet: Bengali Handwritten Character Recognition Using Deep Learning", "comments": "language ambiguity cleared, typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  I propose a state of the art deep neural architectural solution for\nhandwritten character recognition for Bengali alphabets, compound characters as\nwell as numerical digits that achieves state-of-the-art accuracy 96.8% in just\n11 epochs. Similar work has been done before by Chatterjee, Swagato, et al. but\nthey achieved 96.12% accuracy in about 47 epochs. The deep neural architecture\nused in that paper was fairly large considering the inclusion of the weights of\nthe ResNet 50 model which is a 50 layer Residual Network. This proposed model\nachieves higher accuracy as compared to any previous work & in a little number\nof epochs. ResNet50 is a good model trained on the ImageNet dataset, but I\npropose an HCR network that is trained from the scratch on Bengali characters\nwithout the \"Ensemble Learning\" that can outperform previous architectures.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 15:22:00 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 16:12:54 GMT"}, {"version": "v3", "created": "Sun, 20 Sep 2020 22:48:57 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Roy", "Akash", ""]]}, {"id": "2008.13002", "submitter": "Qianye Yang", "authors": "Qianye Yang, Yunguan Fu, Francesco Giganti, Nooshin Ghavami, Qingchao\n  Chen, J. Alison Noble, Tom Vercauteren, Dean Barratt, and Yipeng Hu", "title": "Longitudinal Image Registration with Temporal-order and\n  Subject-specificity Discrimination", "comments": "Accepted at MICCAI 2020", "journal-ref": null, "doi": "10.1007/978-3-030-59716-0_24", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphological analysis of longitudinal MR images plays a key role in\nmonitoring disease progression for prostate cancer patients, who are placed\nunder an active surveillance program. In this paper, we describe a\nlearning-based image registration algorithm to quantify changes on regions of\ninterest between a pair of images from the same patient, acquired at two\ndifferent time points. Combining intensity-based similarity and gland\nsegmentation as weak supervision, the population-data-trained registration\nnetworks significantly lowered the target registration errors (TREs) on holdout\npatient data, compared with those before registration and those from an\niterative registration algorithm. Furthermore, this work provides a\nquantitative analysis on several longitudinal-data-sampling strategies and, in\nturn, we propose a novel regularisation method based on maximum mean\ndiscrepancy, between differently-sampled training image pairs. Based on 216 3D\nMR images from 86 patients, we report a mean TRE of 5.6 mm and show\nstatistically significant differences between the different training data\nsampling strategies.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 16:06:58 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Yang", "Qianye", ""], ["Fu", "Yunguan", ""], ["Giganti", "Francesco", ""], ["Ghavami", "Nooshin", ""], ["Chen", "Qingchao", ""], ["Noble", "J. Alison", ""], ["Vercauteren", "Tom", ""], ["Barratt", "Dean", ""], ["Hu", "Yipeng", ""]]}, {"id": "2008.13013", "submitter": "Chun-Hung Chao", "authors": "Chun-Hung Chao, Zhuotun Zhu, Dazhou Guo, Ke Yan, Tsung-Ying Ho,\n  Jinzheng Cai, Adam P. Harrison, Xianghua Ye, Jing Xiao, Alan Yuille, Min Sun,\n  Le Lu, Dakai Jin", "title": "Lymph Node Gross Tumor Volume Detection in Oncology Imaging via\n  Relationship Learning Using Graph Neural Network", "comments": "Accepted to MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Determining the spread of GTV$_{LN}$ is essential in defining the respective\nresection or irradiating regions for the downstream workflows of surgical\nresection and radiotherapy for many cancers. Different from the more common\nenlarged lymph node (LN), GTV$_{LN}$ also includes smaller ones if associated\nwith high positron emission tomography signals and/or any metastasis signs in\nCT. This is a daunting task. In this work, we propose a unified LN appearance\nand inter-LN relationship learning framework to detect the true GTV$_{LN}$.\nThis is motivated by the prior clinical knowledge that LNs form a connected\nlymphatic system, and the spread of cancer cells among LNs often follows\ncertain pathways. Specifically, we first utilize a 3D convolutional neural\nnetwork with ROI-pooling to extract the GTV$_{LN}$'s instance-wise appearance\nfeatures. Next, we introduce a graph neural network to further model the\ninter-LN relationships where the global LN-tumor spatial priors are included in\nthe learning process. This leads to an end-to-end trainable network to detect\nby classifying GTV$_{LN}$. We operate our model on a set of GTV$_{LN}$\ncandidates generated by a preliminary 1st-stage method, which has a sensitivity\nof $>85\\%$ at the cost of high false positive (FP) ($>15$ FPs per patient). We\nvalidate our approach on a radiotherapy dataset with 142 paired PET/RTCT scans\ncontaining the chest and upper abdominal body parts. The proposed method\nsignificantly improves over the state-of-the-art (SOTA) LN classification\nmethod by $5.5\\%$ and $13.1\\%$ in F1 score and the averaged sensitivity value\nat $2, 3, 4, 6$ FPs per patient, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 16:59:23 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Chao", "Chun-Hung", ""], ["Zhu", "Zhuotun", ""], ["Guo", "Dazhou", ""], ["Yan", "Ke", ""], ["Ho", "Tsung-Ying", ""], ["Cai", "Jinzheng", ""], ["Harrison", "Adam P.", ""], ["Ye", "Xianghua", ""], ["Xiao", "Jing", ""], ["Yuille", "Alan", ""], ["Sun", "Min", ""], ["Lu", "Le", ""], ["Jin", "Dakai", ""]]}, {"id": "2008.13015", "submitter": "Seyed Mojtaba Marvasti-Zadeh", "authors": "Seyed Mojtaba Marvasti-Zadeh, Hossein Ghanei-Yakhdan, and Shohreh\n  Kasaei", "title": "Adaptive Exploitation of Pre-trained Deep Convolutional Neural Networks\n  for Robust Visual Tracking", "comments": "Accepted Manuscript in Multimedia Tools and Applications (MTAP),\n  Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the automatic feature extraction procedure via multi-layer nonlinear\ntransformations, the deep learning-based visual trackers have recently achieved\ngreat success in challenging scenarios for visual tracking purposes. Although\nmany of those trackers utilize the feature maps from pre-trained convolutional\nneural networks (CNNs), the effects of selecting different models and\nexploiting various combinations of their feature maps are still not compared\ncompletely. To the best of our knowledge, all those methods use a fixed number\nof convolutional feature maps without considering the scene attributes (e.g.,\nocclusion, deformation, and fast motion) that might occur during tracking. As a\npre-requisition, this paper proposes adaptive discriminative correlation\nfilters (DCF) based on the methods that can exploit CNN models with different\ntopologies. First, the paper provides a comprehensive analysis of four commonly\nused CNN models to determine the best feature maps of each model. Second, with\nthe aid of analysis results as attribute dictionaries, adaptive exploitation of\ndeep features is proposed to improve the accuracy and robustness of visual\ntrackers regarding video characteristics. Third, the generalization of the\nproposed method is validated on various tracking datasets as well as CNN models\nwith similar architectures. Finally, extensive experimental results demonstrate\nthe effectiveness of the proposed adaptive method compared with\nstate-of-the-art visual tracking methods.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 17:09:43 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 06:57:23 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Marvasti-Zadeh", "Seyed Mojtaba", ""], ["Ghanei-Yakhdan", "Hossein", ""], ["Kasaei", "Shohreh", ""]]}, {"id": "2008.13024", "submitter": "Hao Tang", "authors": "Hao Tang, Song Bai, Nicu Sebe", "title": "Dual Attention GANs for Semantic Image Synthesis", "comments": "Accepted to ACM MM 2020, camera ready (9 pages) + supplementary (10\n  pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the semantic image synthesis task that aims at\ntransferring semantic label maps to photo-realistic images. Existing methods\nlack effective semantic constraints to preserve the semantic information and\nignore the structural correlations in both spatial and channel dimensions,\nleading to unsatisfactory blurry and artifact-prone results. To address these\nlimitations, we propose a novel Dual Attention GAN (DAGAN) to synthesize\nphoto-realistic and semantically-consistent images with fine details from the\ninput layouts without imposing extra training overhead or modifying the network\narchitectures of existing methods. We also propose two novel modules, i.e.,\nposition-wise Spatial Attention Module (SAM) and scale-wise Channel Attention\nModule (CAM), to capture semantic structure attention in spatial and channel\ndimensions, respectively. Specifically, SAM selectively correlates the pixels\nat each position by a spatial attention map, leading to pixels with the same\nsemantic label being related to each other regardless of their spatial\ndistances. Meanwhile, CAM selectively emphasizes the scale-wise features at\neach channel by a channel attention map, which integrates associated features\namong all channel maps regardless of their scales. We finally sum the outputs\nof SAM and CAM to further improve feature representation. Extensive experiments\non four challenging datasets show that DAGAN achieves remarkably better results\nthan state-of-the-art methods, while using fewer model parameters. The source\ncode and trained models are available at https://github.com/Ha0Tang/DAGAN.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 17:49:01 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Tang", "Hao", ""], ["Bai", "Song", ""], ["Sebe", "Nicu", ""]]}, {"id": "2008.13050", "submitter": "Thomas Lampert", "authors": "Odyssee Merveille, Thomas Lampert, Jessica Schmitz, Germain Forestier,\n  Friedrich Feuerhake, C\\'edric Wemmert", "title": "An automatic framework for fusing information from differently stained\n  consecutive digital whole slide images: A case study in renal histology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: This article presents an automatic image processing framework to\nextract quantitative high-level information describing the micro-environment of\nglomeruli in consecutive whole slide images (WSIs) processed with different\nstaining modalities of patients with chronic kidney rejection after kidney\ntransplantation. Methods: This four-step framework consists of: 1) approximate\nrigid registration, 2) cell and anatomical structure segmentation 3) fusion of\ninformation from different stainings using a newly developed registration\nalgorithm 4) feature extraction. Results: Each step of the framework is\nvalidated independently both quantitatively and qualitatively by pathologists.\nAn illustration of the different types of features that can be extracted is\npresented. Conclusion: The proposed generic framework allows for the analysis\nof the micro-environment surrounding large structures that can be segmented\n(either manually or automatically). It is independent of the segmentation\napproach and is therefore applicable to a variety of biomedical research\nquestions. Significance: Chronic tissue remodelling processes after kidney\ntransplantation can result in interstitial fibrosis and tubular atrophy (IFTA)\nand glomerulosclerosis. This pipeline provides tools to quantitatively analyse,\nin the same spatial context, information from different consecutive WSIs and\nhelp researchers understand the complex underlying mechanisms leading to IFTA\nand glomerulosclerosis.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 20:30:46 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 19:12:07 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Merveille", "Odyssee", ""], ["Lampert", "Thomas", ""], ["Schmitz", "Jessica", ""], ["Forestier", "Germain", ""], ["Feuerhake", "Friedrich", ""], ["Wemmert", "C\u00e9dric", ""]]}, {"id": "2008.13084", "submitter": "Kangfu Mei", "authors": "Juncheng Li, Faming Fang, Jiaqian Li, Kangfu Mei, Guixu Zhang", "title": "MDCN: Multi-scale Dense Cross Network for Image Super-Resolution", "comments": "15 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have been proven to be of great benefit for\nsingle-image super-resolution (SISR). However, previous works do not make full\nuse of multi-scale features and ignore the inter-scale correlation between\ndifferent upsampling factors, resulting in sub-optimal performance. Instead of\nblindly increasing the depth of the network, we are committed to mining image\nfeatures and learning the inter-scale correlation between different upsampling\nfactors. To achieve this, we propose a Multi-scale Dense Cross Network (MDCN),\nwhich achieves great performance with fewer parameters and less execution time.\nMDCN consists of multi-scale dense cross blocks (MDCBs), hierarchical feature\ndistillation block (HFDB), and dynamic reconstruction block (DRB). Among them,\nMDCB aims to detect multi-scale features and maximize the use of image features\nflow at different scales, HFDB focuses on adaptively recalibrate channel-wise\nfeature responses to achieve feature distillation, and DRB attempts to\nreconstruct SR images with different upsampling factors in a single model. It\nis worth noting that all these modules can run independently. It means that\nthese modules can be selectively plugged into any CNN model to improve model\nperformance. Extensive experiments show that MDCN achieves competitive results\nin SISR, especially in the reconstruction task with multiple upsampling\nfactors. The code will be provided at https://github.com/MIVRC/MDCN-PyTorch.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 03:50:19 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Li", "Juncheng", ""], ["Fang", "Faming", ""], ["Li", "Jiaqian", ""], ["Mei", "Kangfu", ""], ["Zhang", "Guixu", ""]]}, {"id": "2008.13101", "submitter": "Deeksha Arya", "authors": "Deeksha Arya (1, 2), Hiroya Maeda (2), Sanjay Kumar Ghosh (1), Durga\n  Toshniwal (1), Alexander Mraz (2, 3), Takehiro Kashiyama (2), and Yoshihide\n  Sekimoto (2) ((1) Indian Institute of Technology Roorkee, India, (2) The\n  University of Tokyo, Japan, (3) Amazon EU, Luxembourg)", "title": "Transfer Learning-based Road Damage Detection for Multiple Countries", "comments": "16 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many municipalities and road authorities seek to implement automated\nevaluation of road damage. However, they often lack technology, know-how, and\nfunds to afford state-of-the-art equipment for data collection and analysis of\nroad damages. Although some countries, like Japan, have developed less\nexpensive and readily available Smartphone-based methods for automatic road\ncondition monitoring, other countries still struggle to find efficient\nsolutions. This work makes the following contributions in this context.\nFirstly, it assesses the usability of the Japanese model for other countries.\nSecondly, it proposes a large-scale heterogeneous road damage dataset\ncomprising 26620 images collected from multiple countries using smartphones.\nThirdly, we propose generalized models capable of detecting and classifying\nroad damages in more than one country. Lastly, we provide recommendations for\nreaders, local agencies, and municipalities of other countries when one other\ncountry publishes its data and model for automatic road damage detection and\nclassification. Our dataset is available at\n(https://github.com/sekilab/RoadDamageDetector/).\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 06:48:00 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Arya", "Deeksha", ""], ["Maeda", "Hiroya", ""], ["Ghosh", "Sanjay Kumar", ""], ["Toshniwal", "Durga", ""], ["Mraz", "Alexander", ""], ["Kashiyama", "Takehiro", ""], ["Sekimoto", "Yoshihide", ""]]}, {"id": "2008.13118", "submitter": "Islem Rekik", "authors": "Mert Lostar and Islem Rekik", "title": "Deep Hypergraph U-Net for Brain Graph Embedding and Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  -Background. Network neuroscience examines the brain as a complex system\nrepresented by a network (or connectome), providing deeper insights into the\nbrain morphology and function, allowing the identification of atypical brain\nconnectivity alterations, which can be used as diagnostic markers of\nneurological disorders. -Existing Methods. Graph embedding methods which map\ndata samples (e.g., brain networks) into a low dimensional space have been\nwidely used to explore the relationship between samples for classification or\nprediction tasks. However, the majority of these works are based on modeling\nthe pair-wise relationships between samples, failing to capture their\nhigher-order relationships. -New Method. In this paper, inspired by the nascent\nfield of geometric deep learning, we propose Hypergraph U-Net (HUNet), a novel\ndata embedding framework leveraging the hypergraph structure to learn\nlow-dimensional embeddings of data samples while capturing their high-order\nrelationships. Specifically, we generalize the U-Net architecture, naturally\noperating on graphs, to hypergraphs by improving local feature aggregation and\npreserving the high-order relationships present in the data. -Results. We\ntested our method on small-scale and large-scale heterogeneous brain\nconnectomic datasets including morphological and functional brain networks of\nautistic and demented patients, respectively. -Conclusion. Our HUNet\noutperformed state-of-the-art geometric graph and hypergraph data embedding\ntechniques with a gain of 4-14% in classification accuracy, demonstrating both\nscalability and generalizability. HUNet code is available at\nhttps://github.com/basiralab/HUNet.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 08:15:18 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Lostar", "Mert", ""], ["Rekik", "Islem", ""]]}, {"id": "2008.13193", "submitter": "Wei Zhang", "authors": "Yue Fan, Shilei Chu, Wei Zhang, Ran Song, and Yibin Li", "title": "Learn by Observation: Imitation Learning for Drone Patrolling from\n  Videos of A Human Navigator", "comments": "Accepted by IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an imitation learning method for autonomous drone patrolling based\nonly on raw videos. Different from previous methods, we propose to let the\ndrone learn patrolling in the air by observing and imitating how a human\nnavigator does it on the ground. The observation process enables the automatic\ncollection and annotation of data using inter-frame geometric consistency,\nresulting in less manual effort and high accuracy. Then a newly designed neural\nnetwork is trained based on the annotated data to predict appropriate\ndirections and translations for the drone to patrol in a lane-keeping manner as\nhumans. Our method allows the drone to fly at a high altitude with a broad view\nand low risk. It can also detect all accessible directions at crossroads and\nfurther carry out the integration of available user instructions and autonomous\npatrolling control commands. Extensive experiments are conducted to demonstrate\nthe accuracy of the proposed imitating learning process as well as the\nreliability of the holistic system for autonomous drone navigation. The codes,\ndatasets as well as video demonstrations are available at\nhttps://vsislab.github.io/uavpatrol\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 15:20:40 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Fan", "Yue", ""], ["Chu", "Shilei", ""], ["Zhang", "Wei", ""], ["Song", "Ran", ""], ["Li", "Yibin", ""]]}, {"id": "2008.13196", "submitter": "Yuxi Li", "authors": "Yuxi Li, Weiyao Lin, Tao Wang, John See, Rui Qian, Ning Xu, Limin\n  Wang, Shugong Xu", "title": "Finding Action Tubes with a Sparse-to-Dense Framework", "comments": "5 figures; AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The task of spatial-temporal action detection has attracted increasing\nattention among researchers. Existing dominant methods solve this problem by\nrelying on short-term information and dense serial-wise detection on each\nindividual frames or clips. Despite their effectiveness, these methods showed\ninadequate use of long-term information and are prone to inefficiency. In this\npaper, we propose for the first time, an efficient framework that generates\naction tube proposals from video streams with a single forward pass in a\nsparse-to-dense manner. There are two key characteristics in this framework:\n(1) Both long-term and short-term sampled information are explicitly utilized\nin our spatiotemporal network, (2) A new dynamic feature sampling module (DTS)\nis designed to effectively approximate the tube output while keeping the system\ntractable. We evaluate the efficacy of our model on the UCF101-24, JHMDB-21 and\nUCFSports benchmark datasets, achieving promising results that are competitive\nto state-of-the-art methods. The proposed sparse-to-dense strategy rendered our\nframework about 7.6 times more efficient than the nearest competitor.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 15:38:44 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Li", "Yuxi", ""], ["Lin", "Weiyao", ""], ["Wang", "Tao", ""], ["See", "John", ""], ["Qian", "Rui", ""], ["Xu", "Ning", ""], ["Wang", "Limin", ""], ["Xu", "Shugong", ""]]}, {"id": "2008.13227", "submitter": "Samad Zabihi", "authors": "Samad Zabihi, Hamed Rezazadegan Tavakoli, Ali Borji", "title": "A Compact Deep Architecture for Real-time Saliency Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency computation models aim to imitate the attention mechanism in the\nhuman visual system. The application of deep neural networks for saliency\nprediction has led to a drastic improvement over the last few years. However,\ndeep models have a high number of parameters which makes them less suitable for\nreal-time applications. Here we propose a compact yet fast model for real-time\nsaliency prediction. Our proposed model consists of a modified U-net\narchitecture, a novel fully connected layer, and central difference\nconvolutional layers. The modified U-Net architecture promotes compactness and\nefficiency. The novel fully-connected layer facilitates the implicit capturing\nof the location-dependent information. Using the central difference\nconvolutional layers at different scales enables capturing more robust and\nbiologically motivated features. We compare our model with state of the art\nsaliency models using traditional saliency scores as well as our newly devised\nscheme. Experimental results over four challenging saliency benchmark datasets\ndemonstrate the effectiveness of our approach in striking a balance between\naccuracy and speed. Our model can be run in real-time which makes it appealing\nfor edge devices and video processing.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 17:47:16 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Zabihi", "Samad", ""], ["Tavakoli", "Hamed Rezazadegan", ""], ["Borji", "Ali", ""]]}, {"id": "2008.13229", "submitter": "Ernest Greene", "authors": "Ernest Greene", "title": "An evolutionary perspective on the design of neuromorphic shape filters", "comments": null, "journal-ref": "IEEE Access, 2020, 8, 114228-114238", "doi": "10.1109/ACCESS.2020_3004412", "report-no": null, "categories": "q-bio.NC cs.AI cs.CV cs.NE eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A substantial amount of time and energy has been invested to develop machine\nvision using connectionist (neural network) principles. Most of that work has\nbeen inspired by theories advanced by neuroscientists and behaviorists for how\ncortical systems store stimulus information. Those theories call for\ninformation flow through connections among several neuron populations, with the\ninitial connections being random (or at least non-functional). Then the\nstrength or location of connections are modified through training trials to\nachieve an effective output, such as the ability to identify an object. Those\ntheories ignored the fact that animals that have no cortex, e.g., fish, can\ndemonstrate visual skills that outpace the best neural network models. Neural\ncircuits that allow for immediate effective vision and quick learning have been\npreprogrammed by hundreds of millions of years of evolution and the visual\nskills are available shortly after hatching. Cortical systems may be providing\nadvanced image processing, but most likely are using design principles that had\nbeen proven effective in simpler systems. The present article provides a brief\noverview of retinal and cortical mechanisms for registering shape information,\nwith the hope that it might contribute to the design of shape-encoding circuits\nthat more closely match the mechanisms of biological vision.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 17:53:44 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Greene", "Ernest", ""]]}, {"id": "2008.13254", "submitter": "Jinzheng Cai", "authors": "Jinzheng Cai, Ke Yan, Chi-Tung Cheng, Jing Xiao, Chien-Hung Liao, Le\n  Lu, Adam P. Harrison", "title": "Deep Volumetric Universal Lesion Detection using Light-Weight Pseudo 3D\n  Convolution and Surface Point Regression", "comments": "Accepted by MICCAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying, measuring and reporting lesions accurately and comprehensively\nfrom patient CT scans are important yet time-consuming procedures for\nphysicians. Computer-aided lesion/significant-findings detection techniques are\nat the core of medical imaging, which remain very challenging due to the\ntremendously large variability of lesion appearance, location and size\ndistributions in 3D imaging. In this work, we propose a novel deep anchor-free\none-stage VULD framework that incorporates (1) P3DC operators to recycle the\narchitectural configurations and pre-trained weights from the off-the-shelf 2D\nnetworks, especially ones with large capacities to cope with data variance, and\n(2) a new SPR method to effectively regress the 3D lesion spatial extents by\npinpointing their representative key points on lesion surfaces. Experimental\nvalidations are first conducted on the public large-scale NIH DeepLesion\ndataset where our proposed method delivers new state-of-the-art quantitative\nperformance. We also test VULD on our in-house dataset for liver tumor\ndetection. VULD generalizes well in both large-scale and small-sized tumor\ndatasets in CT imaging.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 19:42:06 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Cai", "Jinzheng", ""], ["Yan", "Ke", ""], ["Cheng", "Chi-Tung", ""], ["Xiao", "Jing", ""], ["Liao", "Chien-Hung", ""], ["Lu", "Le", ""], ["Harrison", "Adam P.", ""]]}, {"id": "2008.13305", "submitter": "Zhijian Li", "authors": "Zhijian Li, Bao Wang, and Jack Xin", "title": "An Integrated Approach to Produce Robust Models with High Efficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) needs to be both efficient and robust for\npractical uses. Quantization and structure simplification are promising ways to\nadapt DNNs to mobile devices, and adversarial training is the most popular\nmethod to make DNNs robust. In this work, we try to obtain both features by\napplying a convergent relaxation quantization algorithm, Binary-Relax (BR), to\na robust adversarial-trained model, ResNets Ensemble via Feynman-Kac Formalism\n(EnResNet). We also discover that high precision, such as ternary (tnn) and\n4-bit, quantization will produce sparse DNNs. However, this sparsity is\nunstructured under advarsarial training. To solve the problems that adversarial\ntraining jeopardizes DNNs' accuracy on clean images and the struture of\nsparsity, we design a trade-off loss function that helps DNNs preserve their\nnatural accuracy and improve the channel sparsity. With our trade-off loss\nfunction, we achieve both goals with no reduction of resistance under weak\nattacks and very minor reduction of resistance under strong attcks. Together\nwith quantized EnResNet with trade-off loss function, we provide robust models\nthat have high efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 00:44:59 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 05:01:28 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Li", "Zhijian", ""], ["Wang", "Bao", ""], ["Xin", "Jack", ""]]}, {"id": "2008.13336", "submitter": "Ali Borji", "authors": "Ali Borji", "title": "Shape Defense", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans rely heavily on shape information to recognize objects. Conversely,\nconvolutional neural networks (CNNs) are biased more towards texture. This is\nperhaps the main reason why CNNs are vulnerable to adversarial examples. Here,\nwe explore how shape bias can be incorporated into CNNs to improve their\nrobustness. Two algorithms are proposed, based on the observation that edges\nare invariant to moderate imperceptible perturbations. In the first one, a\nclassifier is adversarially trained on images with the edge map as an\nadditional channel. At inference time, the edge map is recomputed and\nconcatenated to the image. In the second algorithm, a conditional GAN is\ntrained to translate the edge maps, from clean and/or perturbed images, into\nclean images. Inference is done over the generated image corresponding to the\ninput's edge map. Extensive experiments over 10 datasets demonstrate the\neffectiveness of the proposed algorithms against FGSM and $\\ell_\\infty$ PGD-40\nattacks. Further, we show that a) edge information can also benefit other\nadversarial training methods, and b) CNNs trained on edge-augmented inputs are\nmore robust against natural image corruptions such as motion blur, impulse\nnoise and JPEG compression, than CNNs trained solely on RGB images. From a\nbroader perspective, our study suggests that CNNs do not adequately account for\nimage structures that are crucial for robustness. Code is available\nat:~\\url{https://github.com/aliborji/Shapedefence.git}.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 03:23:59 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Borji", "Ali", ""]]}, {"id": "2008.13343", "submitter": "Yuhang Li", "authors": "Yuhang Li and Xuejin Chen and Binxin Yang and Zihan Chen and Zhihua\n  Cheng and Zheng-Jun Zha", "title": "DeepFacePencil: Creating Face Images from Freehand Sketches", "comments": "ACM MM 2020 (oral)", "journal-ref": null, "doi": "10.1145/3394171.3413684", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the task of generating photo-realistic face images\nfrom hand-drawn sketches. Existing image-to-image translation methods require a\nlarge-scale dataset of paired sketches and images for supervision. They\ntypically utilize synthesized edge maps of face images as training data.\nHowever, these synthesized edge maps strictly align with the edges of the\ncorresponding face images, which limit their generalization ability to real\nhand-drawn sketches with vast stroke diversity. To address this problem, we\npropose DeepFacePencil, an effective tool that is able to generate\nphoto-realistic face images from hand-drawn sketches, based on a novel dual\ngenerator image translation network during training. A novel spatial attention\npooling (SAP) is designed to adaptively handle stroke distortions which are\nspatially varying to support various stroke styles and different levels of\ndetails. We conduct extensive experiments and the results demonstrate the\nsuperiority of our model over existing methods on both image quality and model\ngeneralization to hand-drawn sketches.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 03:35:21 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Li", "Yuhang", ""], ["Chen", "Xuejin", ""], ["Yang", "Binxin", ""], ["Chen", "Zihan", ""], ["Cheng", "Zhihua", ""], ["Zha", "Zheng-Jun", ""]]}, {"id": "2008.13362", "submitter": "Mrigank Rochan", "authors": "Mrigank Rochan, Mahesh Kumar Krishna Reddy, Yang Wang", "title": "Sentence Guided Temporal Modulation for Dynamic Video Thumbnail\n  Generation", "comments": "Accepted to BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sentence specified dynamic video thumbnail\ngeneration. Given an input video and a user query sentence, the goal is to\ngenerate a video thumbnail that not only provides the preview of the video\ncontent, but also semantically corresponds to the sentence. In this paper, we\npropose a sentence guided temporal modulation (SGTM) mechanism that utilizes\nthe sentence embedding to modulate the normalized temporal activations of the\nvideo thumbnail generation network. Unlike the existing state-of-the-art method\nthat uses recurrent architectures, we propose a non-recurrent framework that is\nsimple and allows much more parallelization. Extensive experiments and analysis\non a large-scale dataset demonstrate the effectiveness of our framework.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 04:51:15 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Rochan", "Mrigank", ""], ["Reddy", "Mahesh Kumar Krishna", ""], ["Wang", "Yang", ""]]}, {"id": "2008.13363", "submitter": "Harsh Mehta", "authors": "Harsh Mehta, Ashok Cutkosky, Behnam Neyshabur", "title": "Extreme Memorization via Scale of Initialization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct an experimental setup in which changing the scale of\ninitialization strongly impacts the implicit regularization induced by SGD,\ninterpolating from good generalization performance to completely memorizing the\ntraining set while making little progress on the test set. Moreover, we find\nthat the extent and manner in which generalization ability is affected depends\non the activation and loss function used, with $\\sin$ activation demonstrating\nextreme memorization. In the case of the homogeneous ReLU activation, we show\nthat this behavior can be attributed to the loss function. Our empirical\ninvestigation reveals that increasing the scale of initialization correlates\nwith misalignment of representations and gradients across examples in the same\nclass. This insight allows us to devise an alignment measure over gradients and\nrepresentations which can capture this phenomenon. We demonstrate that our\nalignment measure correlates with generalization of deep models trained on\nimage classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 04:53:11 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 22:09:15 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Mehta", "Harsh", ""], ["Cutkosky", "Ashok", ""], ["Neyshabur", "Behnam", ""]]}, {"id": "2008.13367", "submitter": "Haoyang Zhang", "authors": "Haoyang Zhang, Ying Wang, Feras Dayoub and Niko S\\\"underhauf", "title": "VarifocalNet: An IoU-aware Dense Object Detector", "comments": "Accepted to CVPR 2021 as an oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately ranking the vast number of candidate detections is crucial for\ndense object detectors to achieve high performance. Prior work uses the\nclassification score or a combination of classification and predicted\nlocalization scores to rank candidates. However, neither option results in a\nreliable ranking, thus degrading detection performance. In this paper, we\npropose to learn an Iou-aware Classification Score (IACS) as a joint\nrepresentation of object presence confidence and localization accuracy. We show\nthat dense object detectors can achieve a more accurate ranking of candidate\ndetections based on the IACS. We design a new loss function, named Varifocal\nLoss, to train a dense object detector to predict the IACS, and propose a new\nstar-shaped bounding box feature representation for IACS prediction and\nbounding box refinement. Combining these two new components and a bounding box\nrefinement branch, we build an IoU-aware dense object detector based on the\nFCOS+ATSS architecture, that we call VarifocalNet or VFNet for short. Extensive\nexperiments on MS COCO show that our VFNet consistently surpasses the strong\nbaseline by $\\sim$2.0 AP with different backbones. Our best model VFNet-X-1200\nwith Res2Net-101-DCN achieves a single-model single-scale AP of 55.1 on COCO\ntest-dev, which is state-of-the-art among various object detectors.Code is\navailable at https://github.com/hyz-xmaster/VarifocalNet .\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 05:12:21 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 15:32:38 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Zhang", "Haoyang", ""], ["Wang", "Ying", ""], ["Dayoub", "Feras", ""], ["S\u00fcnderhauf", "Niko", ""]]}, {"id": "2008.13369", "submitter": "Leena Mathur", "authors": "Leena Mathur and Maja J Matari\\'c", "title": "Introducing Representations of Facial Affect in Automated Multimodal\n  Deception Detection", "comments": "10 pages, Accepted at ACM International Conference on Multimodal\n  Interaction (ICMI), October 2020", "journal-ref": "Proceedings of the 2020 International Conference on Multimodal\n  Interaction (ICMI)", "doi": "10.1145/3382507.3418864", "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated deception detection systems can enhance health, justice, and\nsecurity in society by helping humans detect deceivers in high-stakes\nsituations across medical and legal domains, among others. This paper presents\na novel analysis of the discriminative power of dimensional representations of\nfacial affect for automated deception detection, along with interpretable\nfeatures from visual, vocal, and verbal modalities. We used a video dataset of\npeople communicating truthfully or deceptively in real-world, high-stakes\ncourtroom situations. We leveraged recent advances in automated emotion\nrecognition in-the-wild by implementing a state-of-the-art deep neural network\ntrained on the Aff-Wild database to extract continuous representations of\nfacial valence and facial arousal from speakers. We experimented with unimodal\nSupport Vector Machines (SVM) and SVM-based multimodal fusion methods to\nidentify effective features, modalities, and modeling approaches for detecting\ndeception. Unimodal models trained on facial affect achieved an AUC of 80%, and\nfacial affect contributed towards the highest-performing multimodal approach\n(adaptive boosting) that achieved an AUC of 91% when tested on speakers who\nwere not part of training sets. This approach achieved a higher AUC than\nexisting automated machine learning approaches that used interpretable visual,\nvocal, and verbal features to detect deception in this dataset, but did not use\nfacial affect. Across all videos, deceptive and truthful speakers exhibited\nsignificant differences in facial valence and facial arousal, contributing\ncomputational support to existing psychological theories on affect and\ndeception. The demonstrated importance of facial affect in our models informs\nand motivates the future development of automated, affect-aware machine\nlearning approaches for modeling and detecting deception and other social\nbehaviors in-the-wild.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 05:12:57 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Mathur", "Leena", ""], ["Matari\u0107", "Maja J", ""]]}, {"id": "2008.13377", "submitter": "Zahra Anvari", "authors": "Zahra Anvari, Vassilis Athitsos", "title": "Evaluating Single Image Dehazing Methods Under Realistic Sunlight Haze", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Haze can degrade the visibility and the image quality drastically, thus\ndegrading the performance of computer vision tasks such as object detection.\nSingle image dehazing is a challenging and ill-posed problem, despite being\nwidely studied. Most existing methods assume that haze has a\nuniform/homogeneous distribution and haze can have a single color, i.e. grayish\nwhite color similar to smoke, while in reality haze can be distributed\nnon-uniformly with different patterns and colors. In this paper, we focus on\nhaze created by sunlight as it is one of the most prevalent type of haze in the\nwild. Sunlight can generate non-uniformly distributed haze with drastic density\nchanges due to sun rays and also a spectrum of haze color due to sunlight color\nchanges during the day. This presents a new challenge to image dehazing\nmethods. For these methods to be practical, this problem needs to be addressed.\nTo quantify the challenges and assess the performance of these methods, we\npresent a sunlight haze benchmark dataset, Sun-Haze, containing 107 hazy images\nwith different types of haze created by sunlight having a variety of intensity\nand color. We evaluate a representative set of state-of-the-art image dehazing\nmethods on this benchmark dataset in terms of standard metrics such as PSNR,\nSSIM, CIEDE2000, PI and NIQE. This uncovers the limitation of the current\nmethods, and questions their underlying assumptions as well as their\npracticality.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 05:53:26 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Anvari", "Zahra", ""], ["Athitsos", "Vassilis", ""]]}, {"id": "2008.13419", "submitter": "Linh K\\\"astner", "authors": "Linh K\\\"astner, Leon Eversberg, Marina Mursa, Jens Lambrecht", "title": "Integrative Object and Pose to Task Detection for an\n  Augmented-Reality-based Human Assistance System using Neural Networks", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a result of an increasingly automatized and digitized industry, processes\nare becoming more complex. Augmented Reality has shown considerable potential\nin assisting workers with complex tasks by enhancing user understanding and\nexperience with spatial information. However, the acceptance and integration of\nAR into industrial processes is still limited due to the lack of established\nmethods and tedious integration efforts. Meanwhile, deep neural networks have\nachieved remarkable results in computer vision tasks and bear great prospects\nto enrich Augmented Reality applications . In this paper, we propose an\nAugmented-Reality-based human assistance system to assist workers in complex\nmanual tasks where we incorporate deep neural networks for computer vision\ntasks. More specifically, we combine Augmented Reality with object and action\ndetectors to make workflows more intuitive and flexible. To evaluate our system\nin terms of user acceptance and efficiency, we conducted several user studies.\nWe found a significant reduction in time to task completion in untrained\nworkers and a decrease in error rate. Furthermore, we investigated the users\nlearning curve with our assistance system.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 08:24:06 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["K\u00e4stner", "Linh", ""], ["Eversberg", "Leon", ""], ["Mursa", "Marina", ""], ["Lambrecht", "Jens", ""]]}, {"id": "2008.13426", "submitter": "Jiangliu Wang", "authors": "Jiangliu Wang, Jianbo Jiao, Linchao Bao, Shengfeng He, Wei Liu, and\n  Yun-hui Liu", "title": "Self-supervised Video Representation Learning by Uncovering\n  Spatio-temporal Statistics", "comments": "Accepted by TPAMI. An extension of our previous work at\n  arXiv:1904.03597", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel pretext task to address the self-supervised video\nrepresentation learning problem. Specifically, given an unlabeled video clip,\nwe compute a series of spatio-temporal statistical summaries, such as the\nspatial location and dominant direction of the largest motion, the spatial\nlocation and dominant color of the largest color diversity along the temporal\naxis, etc. Then a neural network is built and trained to yield the statistical\nsummaries given the video frames as inputs. In order to alleviate the learning\ndifficulty, we employ several spatial partitioning patterns to encode rough\nspatial locations instead of exact spatial Cartesian coordinates. Our approach\nis inspired by the observation that human visual system is sensitive to rapidly\nchanging contents in the visual field, and only needs impressions about rough\nspatial locations to understand the visual contents. To validate the\neffectiveness of the proposed approach, we conduct extensive experiments with\nfour 3D backbone networks, i.e., C3D, 3D-ResNet, R(2+1)D and S3D-G. The results\nshow that our approach outperforms the existing approaches across these\nbackbone networks on four downstream video analysis tasks including action\nrecognition, video retrieval, dynamic scene recognition, and action similarity\nlabeling. The source code is publicly available at:\nhttps://github.com/laura-wang/video_repres_sts.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 08:31:56 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 02:41:22 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Wang", "Jiangliu", ""], ["Jiao", "Jianbo", ""], ["Bao", "Linchao", ""], ["He", "Shengfeng", ""], ["Liu", "Wei", ""], ["Liu", "Yun-hui", ""]]}, {"id": "2008.13429", "submitter": "Zhao Kang", "authors": "Zhao Kang and Chong Peng and Qiang Cheng and Xinwang Liu and Xi Peng\n  and Zenglin Xu and Ling Tian", "title": "Structured Graph Learning for Clustering and Semi-supervised\n  Classification", "comments": "Appear in Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs have become increasingly popular in modeling structures and\ninteractions in a wide variety of problems during the last decade. Graph-based\nclustering and semi-supervised classification techniques have shown impressive\nperformance. This paper proposes a graph learning framework to preserve both\nthe local and global structure of data. Specifically, our method uses the\nself-expressiveness of samples to capture the global structure and adaptive\nneighbor approach to respect the local structure. Furthermore, most existing\ngraph-based methods conduct clustering and semi-supervised classification on\nthe graph learned from the original data matrix, which doesn't have explicit\ncluster structure, thus they might not achieve the optimal performance. By\nconsidering rank constraint, the achieved graph will have exactly $c$ connected\ncomponents if there are $c$ clusters or classes. As a byproduct of this, graph\nlearning and label inference are jointly and iteratively implemented in a\nprincipled way. Theoretically, we show that our model is equivalent to a\ncombination of kernel k-means and k-means methods under certain condition.\nExtensive experiments on clustering and semi-supervised classification\ndemonstrate that the proposed method outperforms other state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 08:41:20 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Kang", "Zhao", ""], ["Peng", "Chong", ""], ["Cheng", "Qiang", ""], ["Liu", "Xinwang", ""], ["Peng", "Xi", ""], ["Xu", "Zenglin", ""], ["Tian", "Ling", ""]]}, {"id": "2008.13450", "submitter": "Guanshuo Wang", "authors": "Guanshuo Wang, Yufeng Yuan, Jiwei Li, Shiming Ge, Xi Zhou", "title": "Receptive Multi-granularity Representation for Person Re-Identification", "comments": "14 pages, 9 figures. Championship solution of NAIC 2019 Person re-ID\n  Track", "journal-ref": "IEEE Transactions on Image Processing 29, 6096-6109, 2020", "doi": "10.1109/TIP.2020.2986878", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key for person re-identification is achieving consistent local details for\ndiscriminative representation across variable environments. Current\nstripe-based feature learning approaches have delivered impressive accuracy,\nbut do not make a proper trade-off between diversity, locality, and robustness,\nwhich easily suffers from part semantic inconsistency for the conflict between\nrigid partition and misalignment. This paper proposes a receptive\nmulti-granularity learning approach to facilitate stripe-based feature\nlearning. This approach performs local partition on the intermediate\nrepresentations to operate receptive region ranges, rather than current\napproaches on input images or output features, thus can enhance the\nrepresentation of locality while remaining proper local association. Toward\nthis end, the local partitions are adaptively pooled by using\nsignificance-balanced activations for uniform stripes. Random shifting\naugmentation is further introduced for a higher variance of person appearing\nregions within bounding boxes to ease misalignment. By two-branch network\narchitecture, different scales of discriminative identity representation can be\nlearned. In this way, our model can provide a more comprehensive and efficient\nfeature representation without larger model storage costs. Extensive\nexperiments on intra-dataset and cross-dataset evaluations demonstrate the\neffectiveness of the proposed approach. Especially, our approach achieves a\nstate-of-the-art accuracy of 96.2%@Rank-1 or 90.0%@mAP on the challenging\nMarket-1501 benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 09:26:08 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Wang", "Guanshuo", ""], ["Yuan", "Yufeng", ""], ["Li", "Jiwei", ""], ["Ge", "Shiming", ""], ["Zhou", "Xi", ""]]}, {"id": "2008.13504", "submitter": "Binbin Xu", "authors": "Binbin Xu, Andrew J. Davison, and Stefan Leutenegger", "title": "Deep Probabilistic Feature-metric Tracking", "comments": "RAL 2020. 8 pages, 9 figures, video link:\n  https://youtu.be/6pMosl6ZAPE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense image alignment from RGB-D images remains a critical issue for\nreal-world applications, especially under challenging lighting conditions and\nin a wide baseline setting. In this paper, we propose a new framework to learn\na pixel-wise deep feature map and a deep feature-metric uncertainty map\npredicted by a Convolutional Neural Network (CNN), which together formulate a\ndeep probabilistic feature-metric residual of the two-view constraint that can\nbe minimised using Gauss-Newton in a coarse-to-fine optimisation framework.\nFurthermore, our network predicts a deep initial pose for faster and more\nreliable convergence. The optimisation steps are differentiable and unrolled to\ntrain in an end-to-end fashion. Due to its probabilistic essence, our approach\ncan easily couple with other residuals, where we show a combination with ICP.\nExperimental results demonstrate state-of-the-art performances on the TUM RGB-D\ndataset and the 3D rigid object tracking dataset. We further demonstrate our\nmethod's robustness and convergence qualitatively.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 11:47:59 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 23:47:16 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Xu", "Binbin", ""], ["Davison", "Andrew J.", ""], ["Leutenegger", "Stefan", ""]]}, {"id": "2008.13507", "submitter": "Manuel Marin-Jimenez", "authors": "Zihao Mu and Francisco M. Castro and Manuel J. Marin-Jimenez and\n  Nicolas Guil and Yan-ran Li and Shiqi Yu", "title": "iLGaCo: Incremental Learning of Gait Covariate Factors", "comments": "Accepted for presentation at IJCB'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gait is a popular biometric pattern used for identifying people based on\ntheir way of walking. Traditionally, gait recognition approaches based on deep\nlearning are trained using the whole training dataset. In fact, if new data\n(classes, view-points, walking conditions, etc.) need to be included, it is\nnecessary to re-train again the model with old and new data samples.\n  In this paper, we propose iLGaCo, the first incremental learning approach of\ncovariate factors for gait recognition, where the deep model can be updated\nwith new information without re-training it from scratch by using the whole\ndataset. Instead, our approach performs a shorter training process with the new\ndata and a small subset of previous samples. This way, our model learns new\ninformation while retaining previous knowledge.\n  We evaluate iLGaCo on CASIA-B dataset in two incremental ways: adding new\nview-points and adding new walking conditions. In both cases, our results are\nclose to the classical `training-from-scratch' approach, obtaining a marginal\ndrop in accuracy ranging from 0.2% to 1.2%, what shows the efficacy of our\napproach. In addition, the comparison of iLGaCo with other incremental learning\nmethods, such as LwF and iCarl, shows a significant improvement in accuracy,\nbetween 6% and 15% depending on the experiment.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 11:52:36 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Mu", "Zihao", ""], ["Castro", "Francisco M.", ""], ["Marin-Jimenez", "Manuel J.", ""], ["Guil", "Nicolas", ""], ["Li", "Yan-ran", ""], ["Yu", "Shiqi", ""]]}, {"id": "2008.13574", "submitter": "Sina Akbarian", "authors": "Sina Akbarian, Laleh Seyyed-Kalantari, Farzad Khalvati, and Elham\n  Dolatabadi", "title": "Evaluating Knowledge Transfer in Neural Network for Medical Images", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning and knowledge transfer techniques have permeated the field of\nmedical imaging and are considered as key approaches for revolutionizing\ndiagnostic imaging practices. However, there are still challenges for the\nsuccessful integration of deep learning into medical imaging tasks due to a\nlack of large annotated imaging data. To address this issue, we propose a\nteacher-student learning framework to transfer knowledge from a carefully\npre-trained convolutional neural network (CNN) teacher to a student CNN. In\nthis study, we explore the performance of knowledge transfer in the medical\nimaging setting. We investigate the proposed network's performance when the\nstudent network is trained on a small dataset (target dataset) as well as when\nteacher's and student's domains are distinct. The performances of the CNN\nmodels are evaluated on three medical imaging datasets including Diabetic\nRetinopathy, CheXpert, and ChestX-ray8. Our results indicate that the\nteacher-student learning framework outperforms transfer learning for small\nimaging datasets. Particularly, the teacher-student learning framework improves\nthe area under the ROC Curve (AUC) of the CNN model on a small sample of\nCheXpert (n=5k) by 4% and on ChestX-ray8 (n=5.6k) by 9%. In addition to small\ntraining data size, we also demonstrate a clear advantage of the\nteacher-student learning framework in the medical imaging setting compared to\ntransfer learning. We observe that the teacher-student network holds a great\npromise not only to improve the performance of diagnosis but also to reduce\noverfitting when the dataset is small.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 12:58:53 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 21:33:31 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Akbarian", "Sina", ""], ["Seyyed-Kalantari", "Laleh", ""], ["Khalvati", "Farzad", ""], ["Dolatabadi", "Elham", ""]]}, {"id": "2008.13611", "submitter": "Shreyas Kalvankar", "authors": "Shreyas Kalvankar, Hrushikesh Pandit, Pranav Parwate", "title": "Galaxy Morphology Classification using EfficientNet Architectures", "comments": "13 pages, 7 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV astro-ph.GA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the usage of EfficientNets and their applications to Galaxy\nMorphology Classification. We explore the usage of EfficientNets into\npredicting the vote fractions of the 79,975 testing images from the Galaxy Zoo\n2 challenge on Kaggle. We evaluate this model using the standard competition\nmetric i.e. rmse score and rank among the top 3 on the public leaderboard with\na public score of 0.07765. We propose a fine-tuned architecture using\nEfficientNetB5 to classify galaxies into seven classes - completely round\nsmooth, in-between smooth, cigarshaped smooth, lenticular, barred spiral,\nunbarred spiral and irregular. The network along with other popular\nconvolutional networks are used to classify 29,941 galaxy images. Different\nmetrics such as accuracy, recall, precision, F1 score are used to evaluate the\nperformance of the model along with a comparative study of other state of the\nart convolutional models to determine which one performs the best. We obtain an\naccuracy of 93.7% on our classification model with an F1 score of 0.8857.\nEfficientNets can be applied to large scale galaxy classification in future\noptical space surveys which will provide a large amount of data such as the\nLarge Synoptic Space Telescope.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 14:00:42 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 04:53:52 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Kalvankar", "Shreyas", ""], ["Pandit", "Hrushikesh", ""], ["Parwate", "Pranav", ""]]}, {"id": "2008.13626", "submitter": "Chao Zhang", "authors": "Chunzhi Gu, Xuequan Lu, Chao Zhang", "title": "Continuous Color Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color transfer, which plays a key role in image editing, has attracted\nnoticeable attention recently. It has remained a challenge to date due to\nvarious issues such as time-consuming manual adjustments and prior segmentation\nissues. In this paper, we propose to model color transfer under a probability\nframework and cast it as a parameter estimation problem. In particular, we\nrelate the transferred image with the example image under the Gaussian Mixture\nModel (GMM) and regard the transferred image color as the GMM centroids. We\nemploy the Expectation-Maximization (EM) algorithm (E-step and M-step) for\noptimization. To better preserve gradient information, we introduce a Laplacian\nbased regularization term to the objective function at the M-step which is\nsolved by deriving a gradient descent algorithm. Given the input of a source\nimage and an example image, our method is able to generate continuous color\ntransfer results with increasing EM iterations. Various experiments show that\nour approach generally outperforms other competitive color transfer methods,\nboth visually and quantitatively.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 14:07:54 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Gu", "Chunzhi", ""], ["Lu", "Xuequan", ""], ["Zhang", "Chao", ""]]}, {"id": "2008.13642", "submitter": "Ritu Yadav", "authors": "Ritu Yadav, Axel Vierling, Karsten Berns", "title": "Radar+RGB Attentive Fusion for Robust Object Detection in Autonomous\n  Vehicles", "comments": "This work is submitted to ICIP 2020 in Jan 2020 and the latest\n  version of this paper will be published on IEEE official website in Oct 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents two variations of architecture referred to as RANet and\nBIRANet. The proposed architecture aims to use radar signal data along with RGB\ncamera images to form a robust detection network that works efficiently, even\nin variable lighting and weather conditions such as rain, dust, fog, and\nothers. First, radar information is fused in the feature extractor network.\nSecond, radar points are used to generate guided anchors. Third, a method is\nproposed to improve region proposal network targets. BIRANet yields 72.3/75.3%\naverage AP/AR on the NuScenes dataset, which is better than the performance of\nour base network Faster-RCNN with Feature pyramid network(FFPN). RANet gives\n69.6/71.9% average AP/AR on the same dataset, which is reasonably acceptable\nperformance. Also, both BIRANet and RANet are evaluated to be robust towards\nthe noise.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 14:27:02 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Yadav", "Ritu", ""], ["Vierling", "Axel", ""], ["Berns", "Karsten", ""]]}, {"id": "2008.13646", "submitter": "Jong Chul Ye", "authors": "Shujaat Khan, Jaeyoung Huh and Jong Chul Ye", "title": "Switchable Deep Beamformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent proposals of deep beamformers using deep neural networks have\nattracted significant attention as computational efficient alternatives to\nadaptive and compressive beamformers. Moreover, deep beamformers are versatile\nin that image post-processing algorithms can be combined with the beamforming.\nUnfortunately, in the current technology, a separate beamformer should be\ntrained and stored for each application, demanding significant scanner\nresources. To address this problem, here we propose a {\\em switchable} deep\nbeamformer that can produce various types of output such as DAS, speckle\nremoval, deconvolution, etc., using a single network with a simple switch. In\nparticular, the switch is implemented through Adaptive Instance Normalization\n(AdaIN) layers, so that various output can be generated by merely changing the\nAdaIN code. Experimental results using B-mode focused ultrasound confirm the\nflexibility and efficacy of the proposed methods for various applications.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 14:31:03 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 11:48:41 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Khan", "Shujaat", ""], ["Huh", "Jaeyoung", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2008.13671", "submitter": "Ajaya Adhikari", "authors": "Ajaya Adhikari, Richard den Hollander, Ioannis Tolios, Michael van\n  Bekkum, Anneloes Bal, Stijn Hendriks, Maarten Kruithof, Dennis Gross, Nils\n  Jansen, Guillermo P\\'erez, Kit Buurman, Stephan Raaijmakers", "title": "Adversarial Patch Camouflage against Aerial Detection", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of military assets on the ground can be performed by applying deep\nlearning-based object detectors on drone surveillance footage. The traditional\nway of hiding military assets from sight is camouflage, for example by using\ncamouflage nets. However, large assets like planes or vessels are difficult to\nconceal by means of traditional camouflage nets. An alternative type of\ncamouflage is the direct misleading of automatic object detectors. Recently, it\nhas been observed that small adversarial changes applied to images of the\nobject can produce erroneous output by deep learning-based detectors. In\nparticular, adversarial attacks have been successfully demonstrated to prohibit\nperson detections in images, requiring a patch with a specific pattern held up\nin front of the person, thereby essentially camouflaging the person for the\ndetector. Research into this type of patch attacks is still limited and several\nquestions related to the optimal patch configuration remain open.\n  This work makes two contributions. First, we apply patch-based adversarial\nattacks for the use case of unmanned aerial surveillance, where the patch is\nlaid on top of large military assets, camouflaging them from automatic\ndetectors running over the imagery. The patch can prevent automatic detection\nof the whole object while only covering a small part of it. Second, we perform\nseveral experiments with different patch configurations, varying their size,\nposition, number and saliency. Our results show that adversarial patch attacks\nform a realistic alternative to traditional camouflage activities, and should\ntherefore be considered in the automated analysis of aerial surveillance\nimagery.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 15:21:50 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Adhikari", "Ajaya", ""], ["Hollander", "Richard den", ""], ["Tolios", "Ioannis", ""], ["van Bekkum", "Michael", ""], ["Bal", "Anneloes", ""], ["Hendriks", "Stijn", ""], ["Kruithof", "Maarten", ""], ["Gross", "Dennis", ""], ["Jansen", "Nils", ""], ["P\u00e9rez", "Guillermo", ""], ["Buurman", "Kit", ""], ["Raaijmakers", "Stephan", ""]]}, {"id": "2008.13705", "submitter": "Ting Yao", "authors": "Fuchen Long and Ting Yao and Zhaofan Qiu and Xinmei Tian and Jiebo Luo\n  and Tao Mei", "title": "Learning to Localize Actions from Moments", "comments": "ECCV 2020 Oral; The source code and data are available at:\n  https://github.com/FuchenUSTC/AherNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the knowledge of action moments (i.e., trimmed video clips that each\ncontains an action instance), humans could routinely localize an action\ntemporally in an untrimmed video. Nevertheless, most practical methods still\nrequire all training videos to be labeled with temporal annotations (action\ncategory and temporal boundary) and develop the models in a fully-supervised\nmanner, despite expensive labeling efforts and inapplicable to new categories.\nIn this paper, we introduce a new design of transfer learning type to learn\naction localization for a large set of action categories, but only on action\nmoments from the categories of interest and temporal annotations of untrimmed\nvideos from a small set of action classes. Specifically, we present Action\nHerald Networks (AherNet) that integrate such design into an one-stage action\nlocalization framework. Technically, a weight transfer function is uniquely\ndevised to build the transformation between classification of action moments or\nforeground video segments and action localization in synthetic contextual\nmoments or untrimmed videos. The context of each moment is learnt through the\nadversarial mechanism to differentiate the generated features from those of\nbackground in untrimmed videos. Extensive experiments are conducted on the\nlearning both across the splits of ActivityNet v1.3 and from THUMOS14 to\nActivityNet v1.3. Our AherNet demonstrates the superiority even comparing to\nmost fully-supervised action localization methods. More remarkably, we train\nAherNet to localize actions from 600 categories on the leverage of action\nmoments in Kinetics-600 and temporal annotations from 200 classes in\nActivityNet v1.3. Source code and data are available at\n\\url{https://github.com/FuchenUSTC/AherNet}.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 16:03:47 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Long", "Fuchen", ""], ["Yao", "Ting", ""], ["Qiu", "Zhaofan", ""], ["Tian", "Xinmei", ""], ["Luo", "Jiebo", ""], ["Mei", "Tao", ""]]}, {"id": "2008.13710", "submitter": "Eden Belouadah", "authors": "Eden Belouadah, Adrian Popescu, Ioannis Kanellos", "title": "Initial Classifier Weights Replay for Memoryless Class Incremental\n  Learning", "comments": "Accepted in BMVC2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental Learning (IL) is useful when artificial systems need to deal with\nstreams of data and do not have access to all data at all times. The most\nchallenging setting requires a constant complexity of the deep model and an\nincremental model update without access to a bounded memory of past data. Then,\nthe representations of past classes are strongly affected by catastrophic\nforgetting. To mitigate its negative effect, an adapted fine tuning which\nincludes knowledge distillation is usually deployed. We propose a different\napproach based on a vanilla fine tuning backbone. It leverages initial\nclassifier weights which provide a strong representation of past classes\nbecause they are trained with all class data. However, the magnitude of\nclassifiers learned in different states varies and normalization is needed for\na fair handling of all classes. Normalization is performed by standardizing the\ninitial classifier weights, which are assumed to be normally distributed. In\naddition, a calibration of prediction scores is done by using state level\nstatistics to further improve classification fairness. We conduct a thorough\nevaluation with four public datasets in a memoryless incremental learning\nsetting. Results show that our method outperforms existing techniques by a\nlarge margin for large-scale datasets.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 16:18:12 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Belouadah", "Eden", ""], ["Popescu", "Adrian", ""], ["Kanellos", "Ioannis", ""]]}, {"id": "2008.13711", "submitter": "Xiaohe Wu", "authors": "Xiaohe Wu, Ming Liu, Yue Cao, Dongwei Ren, Wangmeng Zuo", "title": "Unpaired Learning of Deep Image Denoising", "comments": "20 pages, 6 figures, ECCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the task of learning blind image denoising networks from an\nunpaired set of clean and noisy images. Such problem setting generally is\npractical and valuable considering that it is feasible to collect unpaired\nnoisy and clean images in most real-world applications. And we further assume\nthat the noise can be signal dependent but is spatially uncorrelated. In order\nto facilitate unpaired learning of denoising network, this paper presents a\ntwo-stage scheme by incorporating self-supervised learning and knowledge\ndistillation. For self-supervised learning, we suggest a dilated blind-spot\nnetwork (D-BSN) to learn denoising solely from real noisy images. Due to the\nspatial independence of noise, we adopt a network by stacking 1x1 convolution\nlayers to estimate the noise level map for each image. Both the D-BSN and\nimage-specific noise model (CNN\\_est) can be jointly trained via maximizing the\nconstrained log-likelihood. Given the output of D-BSN and estimated noise level\nmap, improved denoising performance can be further obtained based on the Bayes'\nrule. As for knowledge distillation, we first apply the learned noise models to\nclean images to synthesize a paired set of training images, and use the real\nnoisy images and the corresponding denoising results in the first stage to form\nanother paired set. Then, the ultimate denoising model can be distilled by\ntraining an existing denoising network using these two paired sets. Experiments\nshow that our unpaired learning method performs favorably on both synthetic\nnoisy images and real-world noisy photographs in terms of quantitative and\nqualitative evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 16:22:40 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Wu", "Xiaohe", ""], ["Liu", "Ming", ""], ["Cao", "Yue", ""], ["Ren", "Dongwei", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "2008.13715", "submitter": "Hao Sun", "authors": "Lele Luan and Jingwei Zheng and Yongchao Yang and Ming L. Wang and Hao\n  Sun", "title": "Extracting full-field subpixel structural displacements from videos via\n  deep learning", "comments": "22 figures; 24 figures", "journal-ref": null, "doi": "10.1016/j.jsv.2021.116142", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a deep learning framework based on convolutional neural\nnetworks (CNNs) that enable real-time extraction of full-field subpixel\nstructural displacements from videos. In particular, two new CNN architectures\nare designed and trained on a dataset generated by the phase-based motion\nextraction method from a single lab-recorded high-speed video of a dynamic\nstructure. As displacement is only reliable in the regions with sufficient\ntexture contrast, the sparsity of motion field induced by the texture mask is\nconsidered via the network architecture design and loss function definition.\nResults show that, with the supervision of full and sparse motion field, the\ntrained network is capable of identifying the pixels with sufficient texture\ncontrast as well as their subpixel motions. The performance of the trained\nnetworks is tested on various videos of other structures to extract the\nfull-field motion (e.g., displacement time histories), which indicates that the\ntrained networks have generalizability to accurately extract full-field subtle\ndisplacements for pixels with sufficient texture contrast.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 16:30:07 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 21:45:59 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Luan", "Lele", ""], ["Zheng", "Jingwei", ""], ["Yang", "Yongchao", ""], ["Wang", "Ming L.", ""], ["Sun", "Hao", ""]]}, {"id": "2008.13719", "submitter": "Tu Zheng", "authors": "Tu Zheng, Hao Fang, Yi Zhang, Wenjian Tang, Zheng Yang, Haifeng Liu,\n  Deng Cai", "title": "RESA: Recurrent Feature-Shift Aggregator for Lane Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lane detection is one of the most important tasks in self-driving. Due to\nvarious complex scenarios (e.g., severe occlusion, ambiguous lanes, etc.) and\nthe sparse supervisory signals inherent in lane annotations, lane detection\ntask is still challenging. Thus, it is difficult for the ordinary convolutional\nneural network (CNN) to train in general scenes to catch subtle lane feature\nfrom the raw image. In this paper, we present a novel module named REcurrent\nFeature-Shift Aggregator (RESA) to enrich lane feature after preliminary\nfeature extraction with an ordinary CNN. RESA takes advantage of strong shape\npriors of lanes and captures spatial relationships of pixels across rows and\ncolumns. It shifts sliced feature map recurrently in vertical and horizontal\ndirections and enables each pixel to gather global information. RESA can\nconjecture lanes accurately in challenging scenarios with weak appearance clues\nby aggregating sliced feature map. Moreover, we propose a Bilateral Up-Sampling\nDecoder that combines coarse-grained and fine-detailed features in the\nup-sampling stage. It can recover the low-resolution feature map into\npixel-wise prediction meticulously. Our method achieves state-of-the-art\nresults on two popular lane detection benchmarks (CULane and Tusimple). Code\nhas been made available at: https://github.com/ZJULearning/resa.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 16:37:30 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 17:14:56 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Zheng", "Tu", ""], ["Fang", "Hao", ""], ["Zhang", "Yi", ""], ["Tang", "Wenjian", ""], ["Yang", "Zheng", ""], ["Liu", "Haifeng", ""], ["Cai", "Deng", ""]]}, {"id": "2008.13745", "submitter": "Oindrila Saha", "authors": "Sandeep Mishra, Oindrila Saha", "title": "RecSal : Deep Recursive Supervision for Visual Saliency Prediction", "comments": "to appear in BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art saliency prediction methods develop upon model architectures\nor loss functions; while training to generate one target saliency map. However,\npublicly available saliency prediction datasets can be utilized to create more\ninformation for each stimulus than just a final aggregate saliency map. This\ninformation when utilized in a biologically inspired fashion can contribute in\nbetter prediction performance without the use of models with huge number of\nparameters. In this light, we propose to extract and use the statistics of (a)\nregion specific saliency and (b) temporal order of fixations, to provide\nadditional context to our network. We show that extra supervision using\nspatially or temporally sequenced fixations results in achieving better\nperformance in saliency prediction. Further, we also design novel architectures\nfor utilizing this extra information and show that it achieves superior\nperformance over a base model which is devoid of extra supervision. We show\nthat our best method outperforms previous state-of-the-art methods with 50-80%\nfewer parameters. We also show that our models perform consistently well across\nall evaluation metrics unlike prior methods.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 17:08:34 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Mishra", "Sandeep", ""], ["Saha", "Oindrila", ""]]}, {"id": "2008.13748", "submitter": "Lijie Liu", "authors": "Lijie Liu, Chufan Wu, Jiwen Lu, Lingxi Xie, Jie Zhou and Qi Tian", "title": "Reinforced Axial Refinement Network for Monocular 3D Object Detection", "comments": "Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular 3D object detection aims to extract the 3D position and properties\nof objects from a 2D input image. This is an ill-posed problem with a major\ndifficulty lying in the information loss by depth-agnostic cameras.\nConventional approaches sample 3D bounding boxes from the space and infer the\nrelationship between the target object and each of them, however, the\nprobability of effective samples is relatively small in the 3D space. To\nimprove the efficiency of sampling, we propose to start with an initial\nprediction and refine it gradually towards the ground truth, with only one 3d\nparameter changed in each step. This requires designing a policy which gets a\nreward after several steps, and thus we adopt reinforcement learning to\noptimize it. The proposed framework, Reinforced Axial Refinement Network\n(RAR-Net), serves as a post-processing stage which can be freely integrated\ninto existing monocular 3D detection methods, and improve the performance on\nthe KITTI dataset with small extra computational costs.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 17:10:48 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Liu", "Lijie", ""], ["Wu", "Chufan", ""], ["Lu", "Jiwen", ""], ["Xie", "Lingxi", ""], ["Zhou", "Jie", ""], ["Tian", "Qi", ""]]}, {"id": "2008.13751", "submitter": "Kai Zhang", "authors": "Kai Zhang, Yawei Li, Wangmeng Zuo, Lei Zhang, Luc Van Gool, Radu\n  Timofte", "title": "Plug-and-Play Image Restoration with Deep Denoiser Prior", "comments": "An extended version of IRCNN (CVPR17). Project page:\n  https://github.com/cszn/DPIR", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works on plug-and-play image restoration have shown that a denoiser\ncan implicitly serve as the image prior for model-based methods to solve many\ninverse problems. Such a property induces considerable advantages for\nplug-and-play image restoration (e.g., integrating the flexibility of\nmodel-based method and effectiveness of learning-based methods) when the\ndenoiser is discriminatively learned via deep convolutional neural network\n(CNN) with large modeling capacity. However, while deeper and larger CNN models\nare rapidly gaining popularity, existing plug-and-play image restoration\nhinders its performance due to the lack of suitable denoiser prior. In order to\npush the limits of plug-and-play image restoration, we set up a benchmark deep\ndenoiser prior by training a highly flexible and effective CNN denoiser. We\nthen plug the deep denoiser prior as a modular part into a half quadratic\nsplitting based iterative algorithm to solve various image restoration\nproblems. We, meanwhile, provide a thorough analysis of parameter setting,\nintermediate results and empirical convergence to better understand the working\nmechanism. Experimental results on three representative image restoration\ntasks, including deblurring, super-resolution and demosaicing, demonstrate that\nthe proposed plug-and-play image restoration with deep denoiser prior not only\nsignificantly outperforms other state-of-the-art model-based methods but also\nachieves competitive or even superior performance against state-of-the-art\nlearning-based methods. The source code is available at\nhttps://github.com/cszn/DPIR.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 17:18:58 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 20:28:19 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Zhang", "Kai", ""], ["Li", "Yawei", ""], ["Zuo", "Wangmeng", ""], ["Zhang", "Lei", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2008.13759", "submitter": "Gurkirt Singh", "authors": "Gurkirt Singh", "title": "Online Spatiotemporal Action Detection and Prediction via Causal\n  Representations", "comments": "PhD thesis, Oxford Brookes University, Examiners: Dr. Andrea Vedaldi\n  and Dr. Fridolin Wild, 172 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis, we focus on video action understanding problems from an\nonline and real-time processing point of view. We start with the conversion of\nthe traditional offline spatiotemporal action detection pipeline into an online\nspatiotemporal action tube detection system. An action tube is a set of\nbounding connected over time, which bounds an action instance in space and\ntime. Next, we explore the future prediction capabilities of such detection\nmethods by extending an existing action tube into the future by regression.\nLater, we seek to establish that online/causal representations can achieve\nsimilar performance to that of offline three dimensional (3D) convolutional\nneural networks (CNNs) on various tasks, including action recognition, temporal\naction segmentation and early prediction.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 17:28:51 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Singh", "Gurkirt", ""]]}, {"id": "2008.13781", "submitter": "Alex Aisen", "authors": "Menashe Benjamin, Guy Engelhard, Alex Aisen, Yinon Aradi, Elad\n  Benjamin", "title": "A Multisite, Report-Based, Centralized Infrastructure for Feedback and\n  Monitoring of Radiology AI/ML Development and Clinical Deployment", "comments": "21 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An infrastructure for multisite, geographically-distributed creation and\ncollection of diverse, high-quality, curated and labeled radiology image data\nis crucial for the successful automated development, deployment, monitoring and\ncontinuous improvement of Artificial Intelligence (AI)/Machine Learning (ML)\nsolutions in the real world. An interactive radiology reporting approach that\nintegrates image viewing, dictation, natural language processing (NLP) and\ncreation of hyperlinks between image findings and the report, provides\nlocalized labels during routine interpretation. These images and labels can be\ncaptured and centralized in a cloud-based system. This method provides a\npractical and efficient mechanism with which to monitor algorithm performance.\nIt also supplies feedback for iterative development and quality improvement of\nnew and existing algorithmic models. Both feedback and monitoring are achieved\nwithout burdening the radiologist. The method addresses proposed regulatory\nrequirements for post-marketing surveillance and external data. Comprehensive\nmulti-site data collection assists in reducing bias. Resource requirements are\ngreatly reduced compared to dedicated retrospective expert labeling.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 17:59:04 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Benjamin", "Menashe", ""], ["Engelhard", "Guy", ""], ["Aisen", "Alex", ""], ["Aradi", "Yinon", ""], ["Benjamin", "Elad", ""]]}]