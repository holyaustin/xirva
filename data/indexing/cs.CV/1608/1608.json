[{"id": "1608.00059", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, Amirali Abdolrashidi and Yao Wang", "title": "Face Recognition Using Scattering Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition has been an active research area in the past few decades. In\ngeneral, face recognition can be very challenging due to variations in\nviewpoint, illumination, facial expression, etc. Therefore it is essential to\nextract features which are invariant to some or all of these variations. Here a\nnew image representation, called scattering transform/network, has been used to\nextract features from faces. The scattering transform is a kind of\nconvolutional network which provides a powerful multi-layer representation for\nsignals. After extraction of scattering features, PCA is applied to reduce the\ndimensionality of the data and then a multi-class support vector machine is\nused to perform recognition. The proposed algorithm has been tested on three\nface datasets and achieved a very high recognition rate.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 01:39:04 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 22:38:09 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Minaee", "Shervin", ""], ["Abdolrashidi", "Amirali", ""], ["Wang", "Yao", ""]]}, {"id": "1608.00116", "submitter": "Muhammad Moazzam Jawaid", "authors": "Muhammad Moazzam Jawaid", "title": "Segmentation of Soft atherosclerotic plaques using active contour models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Detection of non-calcified plaques in the coronary tree is a challenging\nproblem due to the nature of comprising substances. Hard plaques are easily\ndiscernible in CTA data cloud due to apparent bright behaviour, therefore many\napproaches have been proposed for automatic segmentation of calcified plaques.\nIn contrast soft plaques show very small difference in intensity with respect\nto surrounding heart tissues & blood voxels. This similarity in intensity makes\nthe isolation and detection of soft plaques very difficult. This work aims to\ndevelop framework for segmentation of vulnerable plaques with minimal user\ndependency. In first step automatic seed point has been established based on\nthe fact that coronary artery behaves as tubular structure through axial\nslices. In the following step the behaviour of contrast agent has been modelled\nmathematically to reflect the dye diffusion in respective CTA volume.\nConsequently based on detected seed point & intensity behaviour, localized\nactive contour segmentation has been applied to extract complete coronary tree.\nBidirectional segmentation has been applied to avoid loss of coronary\ninformation due to the seed point location whereas auto adjustment feature of\ncontour grabs new emerging branches. Medial axis for extracted coronary tree is\ngenerated using fast marching method for obtaining curve planar reformation for\nvalidation of contrast agent behaviour. Obtained coronary tree is to be\nevaluated for soft plaques in second phase of this research.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 12:51:07 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Jawaid", "Muhammad Moazzam", ""]]}, {"id": "1608.00148", "submitter": "Bilal Ahmed", "authors": "Bilal Ahmed and Thomas Thesen and Karen E. Blackmon and Ruben\n  Kuzniecky and Orrin Devinsky and Jennifer G. Dy and Carla E. Brodley", "title": "Multi-task Learning with Weak Class Labels: Leveraging iEEG to Detect\n  Cortical Lesions in Cryptogenic Epilepsy", "comments": "Presented at 2016 Machine Learning and Healthcare Conference (MLHC\n  2016), Los Angeles, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) is useful for domains in which data originates from\nmultiple sources that are individually under-sampled. MTL methods are able to\nlearn classification models that have higher performance as compared to\nlearning a single model by aggregating all the data together or learning a\nseparate model for each data source. The performance of these methods relies on\nlabel accuracy. We address the problem of simultaneously learning multiple\nclassifiers in the MTL framework when the training data has imprecise labels.\nWe assume that there is an additional source of information that provides a\nscore for each instance which reflects the certainty about its label. Modeling\nthis score as being generated by an underlying ranking function, we augment the\nMTL framework with an added layer of supervision. This results in new MTL\nmethods that are able to learn accurate classifiers while preserving the domain\nstructure provided through the rank information. We apply these methods to the\ntask of detecting abnormal cortical regions in the MRIs of patients suffering\nfrom focal epilepsy whose MRI were read as normal by expert neuroradiologists.\nIn addition to the noisy labels provided by the results of surgical resection,\nwe employ the results of an invasive intracranial-EEG exam as an additional\nsource of label information. Our proposed methods are able to successfully\ndetect abnormal regions for all patients in our dataset and achieve a higher\nperformance as compared to baseline methods.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 17:04:47 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Ahmed", "Bilal", ""], ["Thesen", "Thomas", ""], ["Blackmon", "Karen E.", ""], ["Kuzniecky", "Ruben", ""], ["Devinsky", "Orrin", ""], ["Dy", "Jennifer G.", ""], ["Brodley", "Carla E.", ""]]}, {"id": "1608.00161", "submitter": "Nam Vo", "authors": "Nam Vo and James Hays", "title": "Localizing and Orienting Street Views Using Overhead Imagery", "comments": "ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we aim to determine the location and orientation of a\nground-level query image by matching to a reference database of overhead (e.g.\nsatellite) images. For this task we collect a new dataset with one million\npairs of street view and overhead images sampled from eleven U.S. cities. We\nexplore several deep CNN architectures for cross-domain matching --\nClassification, Hybrid, Siamese, and Triplet networks. Classification and\nHybrid architectures are accurate but slow since they allow only partial\nfeature precomputation. We propose a new loss function which significantly\nimproves the accuracy of Siamese and Triplet embedding networks while\nmaintaining their applicability to large-scale retrieval tasks like image\ngeolocalization. This image matching task is challenging not just because of\nthe dramatic viewpoint difference between ground-level and overhead imagery but\nbecause the orientation (i.e. azimuth) of the street views is unknown making\ncorrespondence even more difficult. We examine several mechanisms to match in\nspite of this -- training for rotation invariance, sampling possible rotations\nat query time, and explicitly predicting relative rotation of ground and\noverhead images with our deep networks. It turns out that explicit orientation\nsupervision also improves location prediction accuracy. Our best performing\narchitectures are roughly 2.5 times as accurate as the commonly used Siamese\nnetwork baseline.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 20:48:14 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 23:49:57 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Vo", "Nam", ""], ["Hays", "James", ""]]}, {"id": "1608.00168", "submitter": "Yashar Deldjoo", "authors": "Yashar Deldjoo, Shengping Zhang, Bahman Zanj, Paolo Cremonesi, Matteo\n  Matteucci", "title": "Sparse vs. Non-sparse: Which One Is Better for Practical Visual\n  Tracking?", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, sparse representation based visual tracking methods have attracted\nincreasing attention in the computer vision community. Although achieve\nsuperior performance to traditional tracking methods, however, a basic problem\nhas not been answered yet --- that whether the sparsity constrain is really\nneeded for visual tracking? To answer this question, in this paper, we first\npropose a robust non-sparse representation based tracker and then conduct\nextensive experiments to compare it against several state-of-the-art sparse\nrepresentation based trackers. Our experiment results and analysis indicate\nthat the proposed non-sparse tracker achieved competitive tracking accuracy\nwith sparse trackers while having faster running speed, which support our\nnon-sparse tracker to be used in practical applications.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 23:03:21 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Deldjoo", "Yashar", ""], ["Zhang", "Shengping", ""], ["Zanj", "Bahman", ""], ["Cremonesi", "Paolo", ""], ["Matteucci", "Matteo", ""]]}, {"id": "1608.00182", "submitter": "Peng Tang", "authors": "Peng Tang, Xinggang Wang, Baoguang Shi, Xiang Bai, Wenyu Liu, Zhuowen\n  Tu", "title": "Deep FisherNet for Object Classification", "comments": "submitted to NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the great success of convolutional neural networks (CNN) for the\nimage classification task on datasets like Cifar and ImageNet, CNN's\nrepresentation power is still somewhat limited in dealing with object images\nthat have large variation in size and clutter, where Fisher Vector (FV) has\nshown to be an effective encoding strategy. FV encodes an image by aggregating\nlocal descriptors with a universal generative Gaussian Mixture Model (GMM). FV\nhowever has limited learning capability and its parameters are mostly fixed\nafter constructing the codebook. To combine together the best of the two\nworlds, we propose in this paper a neural network structure with FV layer being\npart of an end-to-end trainable system that is differentiable; we name our\nnetwork FisherNet that is learnable using backpropagation. Our proposed\nFisherNet combines convolutional neural network training and Fisher Vector\nencoding in a single end-to-end structure. We observe a clear advantage of\nFisherNet over plain CNN and standard FV in terms of both classification\naccuracy and computational efficiency on the challenging PASCAL VOC object\nclassification task.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 03:56:30 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Tang", "Peng", ""], ["Wang", "Xinggang", ""], ["Shi", "Baoguang", ""], ["Bai", "Xiang", ""], ["Liu", "Wenyu", ""], ["Tu", "Zhuowen", ""]]}, {"id": "1608.00187", "submitter": "Ranjay Krishna", "authors": "Cewu Lu, Ranjay Krishna, Michael Bernstein, Li Fei-Fei", "title": "Visual Relationship Detection with Language Priors", "comments": "ECCV 2016 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual relationships capture a wide variety of interactions between pairs of\nobjects in images (e.g. \"man riding bicycle\" and \"man pushing bicycle\").\nConsequently, the set of possible relationships is extremely large and it is\ndifficult to obtain sufficient training examples for all possible\nrelationships. Because of this limitation, previous work on visual relationship\ndetection has concentrated on predicting only a handful of relationships.\nThough most relationships are infrequent, their objects (e.g. \"man\" and\n\"bicycle\") and predicates (e.g. \"riding\" and \"pushing\") independently occur\nmore frequently. We propose a model that uses this insight to train visual\nmodels for objects and predicates individually and later combines them together\nto predict multiple relationships per image. We improve on prior work by\nleveraging language priors from semantic word embeddings to finetune the\nlikelihood of a predicted relationship. Our model can scale to predict\nthousands of types of relationships from a few examples. Additionally, we\nlocalize the objects in the predicted relationships as bounding boxes in the\nimage. We further demonstrate that understanding relationships can improve\ncontent based image retrieval.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 05:54:13 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Lu", "Cewu", ""], ["Krishna", "Ranjay", ""], ["Bernstein", "Michael", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1608.00199", "submitter": "Soumitra Samanta", "authors": "Soumitra Samanta and Bhabatosh Chanda", "title": "A Data-driven Approach for Human Pose Tracking Based on Spatio-temporal\n  Pictorial Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a data-driven approach for human pose tracking in\nvideo data. We formulate the human pose tracking problem as a discrete\noptimization problem based on spatio-temporal pictorial structure model and\nsolve this problem in a greedy framework very efficiently. We propose the model\nto track the human pose by combining the human pose estimation from single\nimage and traditional object tracking in a video. Our pose tracking objective\nfunction consists of the following terms: likeliness of appearance of a part\nwithin a frame, temporal displacement of the part from previous frame to the\ncurrent frame, and the spatial dependency of a part with its parent in the\ngraph structure. Experimental evaluation on benchmark datasets (VideoPose2,\nPoses in the Wild and Outdoor Pose) as well as on our newly build ICDPose\ndataset shows the usefulness of our proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 08:50:47 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Samanta", "Soumitra", ""], ["Chanda", "Bhabatosh", ""]]}, {"id": "1608.00203", "submitter": "Balint Antal", "authors": "Balint Antal", "title": "Automatic 3D Point Set Reconstruction from Stereo Laparoscopic Images\n  using Deep Neural Networks", "comments": "In Proceedings of the 6th International Joint Conference on Pervasive\n  and Embedded Computing and Communication Systems (PECCS 2016), pages 116-121\n  ISBN: 978-989-758-195-3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an automatic approach to predict 3D coordinates from stereo\nlaparoscopic images is presented. The approach maps a vector of pixel\nintensities to 3D coordinates through training a six layer deep neural network.\nThe architectural aspects of the approach is presented and in detail and the\nmethod is evaluated on a publicly available dataset with promising results.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 09:28:28 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Antal", "Balint", ""]]}, {"id": "1608.00207", "submitter": "Zhiwen Shao", "authors": "Zhiwen Shao, Shouhong Ding, Yiru Zhao, Qinchuan Zhang, Lizhuang Ma", "title": "Learning deep representation from coarse to fine for face alignment", "comments": "This paper is accepted by 2016 IEEE International Conference on\n  Multimedia and Expo (ICME)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel face alignment method that trains deep\nconvolutional network from coarse to fine. It divides given landmarks into\nprincipal subset and elaborate subset. We firstly keep a large weight for\nprincipal subset to make our network primarily predict their locations while\nslightly take elaborate subset into account. Next the weight of principal\nsubset is gradually decreased until two subsets have equivalent weights. This\nprocess contributes to learn a good initial model and search the optimal model\nsmoothly to avoid missing fairly good intermediate models in subsequent\nprocedures. On the challenging COFW dataset [1], our method achieves 6.33% mean\nerror with a reduction of 21.37% compared with the best previous result [2].\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 11:02:40 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Shao", "Zhiwen", ""], ["Ding", "Shouhong", ""], ["Zhao", "Yiru", ""], ["Zhang", "Qinchuan", ""], ["Ma", "Lizhuang", ""]]}, {"id": "1608.00218", "submitter": "Ilija Ilievski", "authors": "Ilija Ilievski and Jiashi Feng", "title": "Hyperparameter Transfer Learning through Surrogate Alignment for\n  Efficient Deep Neural Network Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several optimization methods have been successfully applied to the\nhyperparameter optimization of deep neural networks (DNNs). The methods work by\nmodeling the joint distribution of hyperparameter values and corresponding\nerror. Those methods become less practical when applied to modern DNNs whose\ntraining may take a few days and thus one cannot collect sufficient\nobservations to accurately model the distribution. To address this challenging\nissue, we propose a method that learns to transfer optimal hyperparameter\nvalues for a small source dataset to hyperparameter values with comparable\nperformance on a dataset of interest. As opposed to existing transfer learning\nmethods, our proposed method does not use hand-designed features. Instead, it\nuses surrogates to model the hyperparameter-error distributions of the two\ndatasets and trains a neural network to learn the transfer function. Extensive\nexperiments on three CV benchmark datasets clearly demonstrate the efficiency\nof our method.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 14:09:17 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Ilievski", "Ilija", ""], ["Feng", "Jiashi", ""]]}, {"id": "1608.00220", "submitter": "Pierre Thodoroff", "authors": "Pierre Thodoroff, Joelle Pineau, Andrew Lim", "title": "Learning Robust Features using Deep Learning for Automatic Seizure\n  Detection", "comments": "Presented at 2016 Machine Learning and Healthcare Conference (MLHC\n  2016), Los Angeles, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and evaluate the capacity of a deep neural network to learn robust\nfeatures from EEG to automatically detect seizures. This is a challenging\nproblem because seizure manifestations on EEG are extremely variable both\ninter- and intra-patient. By simultaneously capturing spectral, temporal and\nspatial information our recurrent convolutional neural network learns a general\nspatially invariant representation of a seizure. The proposed approach exceeds\nsignificantly previous results obtained on cross-patient classifiers both in\nterms of sensitivity and false positive rate. Furthermore, our model proves to\nbe robust to missing channel and variable electrode montage.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 14:28:15 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Thodoroff", "Pierre", ""], ["Pineau", "Joelle", ""], ["Lim", "Andrew", ""]]}, {"id": "1608.00229", "submitter": "Konstantinos Makantasis", "authors": "Konstantinos Makantasis, Antonis Nikitakis, Anastasios Doulamis,\n  Nikolaos Doulamis, Yannis Papaefstathiou", "title": "Data-Driven Background Subtraction Algorithm for in-Camera Acceleration\n  in Thermal Imagery", "comments": "15 pages. arXiv admin note: text overlap with arXiv:1506.08581", "journal-ref": "IEEE Transactions on Circuits and Systems for Video Technology 99\n  (2017)", "doi": "10.1109/TCSVT.2017.2711259", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of moving objects in videos is a crucial step towards successful\nsurveillance and monitoring applications. A key component for such tasks is\ncalled background subtraction and tries to extract regions of interest from the\nimage background for further processing or action. For this reason, its\naccuracy and real-time performance is of great significance. Although,\neffective background subtraction methods have been proposed, only a few of them\ntake into consideration the special characteristics of thermal imagery. In this\nwork, we propose a background subtraction scheme, which models the thermal\nresponses of each pixel as a mixture of Gaussians with unknown number of\ncomponents. Following a Bayesian approach, our method automatically estimates\nthe mixture structure, while simultaneously it avoids over/under fitting. The\npixel density estimate is followed by an efficient and highly accurate updating\nmechanism, which permits our system to be automatically adapted to dynamically\nchanging operation conditions. We propose a reference implementation of our\nmethod in reconfigurable hardware achieving both adequate performance and low\npower consumption. Adopting a High Level Synthesis design, demanding floating\npoint arithmetic operations are mapped in reconfigurable hardware;\ndemonstrating fast-prototyping and on-field customization at the same time.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 15:17:46 GMT"}, {"version": "v2", "created": "Sun, 15 Oct 2017 14:23:43 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Makantasis", "Konstantinos", ""], ["Nikitakis", "Antonis", ""], ["Doulamis", "Anastasios", ""], ["Doulamis", "Nikolaos", ""], ["Papaefstathiou", "Yannis", ""]]}, {"id": "1608.00247", "submitter": "Francisco Vasconcelos", "authors": "Francisco Vasconcelos, Donald Peebles, Sebastien Ourselin, Danail\n  Stoyanov", "title": "Similarity Registration Problems for 2D/3D Ultrasound Calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a minimal solution for the similarity registration (rigid pose and\nscale) between two sets of 3D lines, and also between a set of co-planar points\nand a set of 3D lines. The first problem is solved up to 8 discrete solutions\nwith a minimum of 2 line-line correspondences, while the second is solved up to\n4 discrete solutions using 4 point-line correspondences. We use these\nalgorithms to perform the extrinsic calibration between a pose tracking sensor\nand a 2D/3D ultrasound (US) curvilinear probe using a tracked needle as\ncalibration target. The needle is tracked as a 3D line, and is scanned by the\nultrasound as either a 3D line (3D US) or as a 2D point (2D US). Since the\nscale factor that converts US scan units to metric coordinates is unknown, the\ncalibration is formulated as a similarity registration problem. We present\nresults with both synthetic and real data and show that the minimum solutions\noutperform the correspondent non-minimal linear formulations.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 18:04:54 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Vasconcelos", "Francisco", ""], ["Peebles", "Donald", ""], ["Ourselin", "Sebastien", ""], ["Stoyanov", "Danail", ""]]}, {"id": "1608.00265", "submitter": "Mario Mastriani", "authors": "Mario Mastriani", "title": "Denoising and compression in wavelet domain via projection onto\n  approximation coefficients", "comments": "11 pages, 10 figures, 2 tables. arXiv admin note: substantial text\n  overlap with arXiv:1405.0632, arXiv:1607.03105, arXiv:1608.00268,\n  arXiv:1608.00277", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new filtering approach in the wavelet domain for image\ndenoising and compression, based on the projections of details subbands\ncoefficients (resultants of the splitting procedure, typical in wavelet domain)\nonto the approximation subband coefficients (much less noisy). The new\nalgorithm is called Projection Onto Approximation Coefficients (POAC). As a\nresult of this approach, only the approximation subband coefficients and three\nscalars are stored and/or transmitted to the channel. Besides, with the\nelimination of the details subbands coefficients, we obtain a bigger\ncompression rate. Experimental results demonstrate that our approach compares\nfavorably to more typical methods of denoising and compression in wavelet\ndomain.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 21:33:17 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Mastriani", "Mario", ""]]}, {"id": "1608.00268", "submitter": "Mario Mastriani", "authors": "Mario Mastriani", "title": "Union is strength in lossy image compression", "comments": "18 pages, 21 figures, 4 tables. arXiv admin note: substantial text\n  overlap with arXiv:1607.03164, arXiv:1405.0632, arXiv:1608.00265", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a comparison between different techniques of image\ncompression. First, the image is divided in blocks which are organized\naccording to a certain scan. Later, several compression techniques are applied,\ncombined or alone. Such techniques are: wavelets (Haar's basis), Karhunen-Loeve\nTransform, etc. Simulations show that the combined versions are the best, with\nminor Mean Squared Error (MSE), and higher Peak Signal to Noise Ratio (PSNR)\nand better image quality, even in the presence of noise.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 21:57:54 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Mastriani", "Mario", ""]]}, {"id": "1608.00270", "submitter": "Mario Mastriani", "authors": "Mario Mastriani", "title": "New wavelet-based superresolution algorithm for speckle reduction in SAR\n  images", "comments": "8 pages, 6 figures, 1 table. arXiv admin note: substantial text\n  overlap with arXiv:1607.03105, arXiv:1608.00273, arXiv:1608.00279,\n  arXiv:1608.00277", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel projection algorithm, the Projection Onto Span\nAlgorithm (POSA) for wavelet-based superresolution and removing speckle (in\nwavelet domain) of unknown variance from Synthetic Aperture Radar (SAR) images.\nAlthough the POSA is good as a new superresolution algorithm for image\nenhancement, image metrology and biometric identification, here one will use it\nlike a tool of despeckling, being the first time that an algorithm of\nsuper-resolution is used for despeckling of SAR images. Specifically, the\nspeckled SAR image is decomposed into wavelet subbands, POSA is applied to the\nhigh subbands, and reconstruct a SAR image from the modified detail\ncoefficients. Experimental results demonstrate that the new method compares\nfavorably to several other despeckling methods on test SAR images.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 22:11:52 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Mastriani", "Mario", ""]]}, {"id": "1608.00272", "submitter": "Licheng Yu", "authors": "Licheng Yu, Patrick Poirson, Shan Yang, Alexander C. Berg, Tamara L.\n  Berg", "title": "Modeling Context in Referring Expressions", "comments": "19 pages, 6 figures, in ECCV 2016; authors, references and\n  acknowledgement updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans refer to objects in their environments all the time, especially in\ndialogue with other people. We explore generating and comprehending natural\nlanguage referring expressions for objects in images. In particular, we focus\non incorporating better measures of visual context into referring expression\nmodels and find that visual comparison to other objects within an image helps\nimprove performance significantly. We also develop methods to tie the language\ngeneration process together, so that we generate expressions for all objects of\na particular category jointly. Evaluation on three recent datasets - RefCOCO,\nRefCOCO+, and RefCOCOg, shows the advantages of our methods for both referring\nexpression generation and comprehension.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 22:21:42 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 22:52:17 GMT"}, {"version": "v3", "created": "Wed, 10 Aug 2016 19:01:37 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Yu", "Licheng", ""], ["Poirson", "Patrick", ""], ["Yang", "Shan", ""], ["Berg", "Alexander C.", ""], ["Berg", "Tamara L.", ""]]}, {"id": "1608.00273", "submitter": "Mario Mastriani", "authors": "Mario Mastriani, Alberto E. Giraldez", "title": "Kalman's shrinkage for wavelet-based despeckling of SAR images", "comments": "7 pages, 1 figure, 1 table. arXiv admin note: substantial text\n  overlap with arXiv:1607.03105, arXiv:1608.00270, arXiv:1608.00279,\n  arXiv:1608.00277, arXiv:1608.00274", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new probability density function (pdf) is proposed to model\nthe statistics of wavelet coefficients, and a simple Kalman's filter is derived\nfrom the new pdf using Bayesian estimation theory. Specifically, we decompose\nthe speckled image into wavelet subbands, we apply the Kalman's filter to the\nhigh subbands, and reconstruct a despeckled image from the modified detail\ncoefficients. Experimental results demonstrate that our method compares\nfavorably to several other despeckling methods on test synthetic aperture radar\n(SAR) images.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 22:24:53 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Mastriani", "Mario", ""], ["Giraldez", "Alberto E.", ""]]}, {"id": "1608.00274", "submitter": "Mario Mastriani", "authors": "Mario Mastriani", "title": "Denoising based on wavelets and deblurring via self-organizing map for\n  Synthetic Aperture Radar images", "comments": "10 pages, 7 figures, 2 tables. arXiv admin note: text overlap with\n  arXiv:1608.00273; text overlap with arXiv:1002.3985 by other authors without\n  attribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work deals with unsupervised image deblurring. We present a new\ndeblurring procedure on images provided by low-resolution synthetic aperture\nradar (SAR) or simply by multimedia in presence of multiplicative (speckle) or\nadditive noise, respectively. The method we propose is defined as a two-step\nprocess. First, we use an original technique for noise reduction in wavelet\ndomain. Then, the learning of a Kohonen self-organizing map (SOM) is performed\ndirectly on the denoised image to take out it the blur. This technique has been\nsuccessfully applied to real SAR images, and the simulation results are\npresented to demonstrate the effectiveness of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 22:32:55 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Mastriani", "Mario", ""]]}, {"id": "1608.00277", "submitter": "Mario Mastriani", "authors": "Mario Mastriani", "title": "Fuzzy thresholding in wavelet domain for speckle reduction in Synthetic\n  Aperture Radar images", "comments": "14 pages, 16 figures, 4 tables. arXiv admin note: substantial text\n  overlap with arXiv:1607.03105, arXiv:1608.00270, arXiv:1608.00273,\n  arXiv:1608.00279; text overlap with arXiv:chao-dyn/9905033 by other authors\n  without attribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of wavelet transforms to Synthetic Aperture Radar (SAR)\nimagery has improved despeckling performance. To deduce the problem of\nfiltering the multiplicative noise to the case of an additive noise, the\nwavelet decomposition is performed on the logarithm of the image gray levels.\nThe detail coefficients produced by the bidimensional discrete wavelet\ntransform (DWT-2D) needs to be thresholded to extract out the speckle in\nhighest subbands. An initial threshold value is estimated according to the\nnoise variance. In this paper, an additional fuzzy thresholding approach for\nautomatic determination of the rate threshold level around the traditional\nwavelet noise thresholding (initial threshold) is applied, and used for the\nsoft or hard-threshold performed on all the high frequency subimages. The\nfiltered logarithmic image is then obtained by reconstruction from the\nthresholded coefficients. This process is applied a single time, and\nexclusively to the first level of decomposition. The exponential function of\nthis reconstructed image gives the final filtered image. Experimental results\non test images have demonstrated the effectiveness of this method compared to\nthe most of methods in use at the moment.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 22:39:03 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Mastriani", "Mario", ""]]}, {"id": "1608.00279", "submitter": "Mario Mastriani", "authors": "Mario Mastriani, Alberto E. Giraldez", "title": "Neural shrinkage for wavelet-based SAR despeckling", "comments": "12 pages, 7 figures, 2 tables. arXiv admin note: text overlap with\n  arXiv:1607.03105, arXiv:1608.00273, arXiv:1608.00270, arXiv:1608.00277", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wavelet shrinkage denoising approach is able to maintain local regularity\nof a signal while suppressing noise. However, the conventional wavelet\nshrinkage based methods are not time-scale adaptive to track the local\ntime-scale variation. In this paper, a new type of Neural Shrinkage (NS) is\npresented with a new class of shrinkage architecture for speckle reduction in\nSynthetic Aperture Radar (SAR) images. The numerical results indicate that the\nnew method outperforms the standard filters, the standard wavelet shrinkage\ndespeckling method, and previous NS.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 22:44:45 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Mastriani", "Mario", ""], ["Giraldez", "Alberto E.", ""]]}, {"id": "1608.00310", "submitter": "Abir Das", "authors": "Rameswar Panda, Abir Das, Amit K. Roy-Chowdhury", "title": "Video Summarization in a Multi-View Camera Network", "comments": "Accepted in ICPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While most existing video summarization approaches aim to extract an\ninformative summary of a single video, we propose a novel framework for\nsummarizing multi-view videos by exploiting both intra- and inter-view content\ncorrelations in a joint embedding space. We learn the embedding by minimizing\nan objective function that has two terms: one due to intra-view correlations\nand another due to inter-view correlations across the multiple views. The\nsolution can be obtained directly by solving one Eigen-value problem that is\nlinear in the number of multi-view videos. We then employ a sparse\nrepresentative selection approach over the learned embedding space to summarize\nthe multi-view videos. Experimental results on several benchmark datasets\ndemonstrate that our proposed approach clearly outperforms the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 03:42:07 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Panda", "Rameswar", ""], ["Das", "Abir", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "1608.00361", "submitter": "Reza Arablouei", "authors": "Reza Arablouei, Ethan Goan, Stephen Gensemer, and Branislav Kusy", "title": "Fast and robust pushbroom hyperspectral imaging via DMD-based scanning", "comments": null, "journal-ref": null, "doi": "10.1117/12.2239107", "report-no": null, "categories": "cs.CV cs.DC physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new pushbroom hyperspectral imaging device that has no macro\nmoving part. The main components of the proposed hyperspectral imager are a\ndigital micromirror device (DMD), a CMOS image sensor with no filter as the\nspectral sensor, a CMOS color (RGB) image sensor as the auxiliary image sensor,\nand a diffraction grating. Using the image sensor pair, the device can\nsimultaneously capture hyperspectral data as well as RGB images of the scene.\nThe RGB images captured by the auxiliary image sensor can facilitate geometric\nco-registration of the hyperspectral image slices captured by the spectral\nsensor. In addition, the information discernible from the RGB images can lead\nto capturing the spectral data of only the regions of interest within the\nscene. The proposed hyperspectral imaging architecture is cost-effective, fast,\nand robust. It also enables a trade-off between resolution and speed. We have\nbuilt an initial prototype based on the proposed design. The prototype can\ncapture a hyperspectral image datacube with a spatial resolution of 192x192\npixels and a spectral resolution of 500 bands in less than thirty seconds.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 09:18:38 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 03:26:49 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Arablouei", "Reza", ""], ["Goan", "Ethan", ""], ["Gensemer", "Stephen", ""], ["Kusy", "Branislav", ""]]}, {"id": "1608.00367", "submitter": "Chao Dong", "authors": "Chao Dong and Chen Change Loy and Xiaoou Tang", "title": "Accelerating the Super-Resolution Convolutional Neural Network", "comments": "17 pages, 8 figures, ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a successful deep model applied in image super-resolution (SR), the\nSuper-Resolution Convolutional Neural Network (SRCNN) has demonstrated superior\nperformance to the previous hand-crafted models either in speed and restoration\nquality. However, the high computational cost still hinders it from practical\nusage that demands real-time performance (24 fps). In this paper, we aim at\naccelerating the current SRCNN, and propose a compact hourglass-shape CNN\nstructure for faster and better SR. We re-design the SRCNN structure mainly in\nthree aspects. First, we introduce a deconvolution layer at the end of the\nnetwork, then the mapping is learned directly from the original low-resolution\nimage (without interpolation) to the high-resolution one. Second, we\nreformulate the mapping layer by shrinking the input feature dimension before\nmapping and expanding back afterwards. Third, we adopt smaller filter sizes but\nmore mapping layers. The proposed model achieves a speed up of more than 40\ntimes with even superior restoration quality. Further, we present the parameter\nsettings that can achieve real-time performance on a generic CPU while still\nmaintaining good performance. A corresponding transfer strategy is also\nproposed for fast training and testing across different upscaling factors.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 09:44:41 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Dong", "Chao", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1608.00470", "submitter": "Arpit Mittal", "authors": "Nikolaos Aletras, Arpit Mittal", "title": "Labeling Topics with Images using Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topics generated by topic models are usually represented by lists of $t$\nterms or alternatively using short phrases and images. The current\nstate-of-the-art work on labeling topics using images selects images by\nre-ranking a small set of candidates for a given topic. In this paper, we\npresent a more generic method that can estimate the degree of association\nbetween any arbitrary pair of an unseen topic and image using a deep neural\nnetwork. Our method has better runtime performance $O(n)$ compared to $O(n^2)$\nfor the current state-of-the-art method, and is also significantly more\naccurate.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 15:27:16 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 16:49:39 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Aletras", "Nikolaos", ""], ["Mittal", "Arpit", ""]]}, {"id": "1608.00486", "submitter": "Conrad Sanderson", "authors": "ZongYuan Ge, Chris McCool, Conrad Sanderson, Peng Wang, Lingqiao Liu,\n  Ian Reid, Peter Corke", "title": "Exploiting Temporal Information for DCNN-based Fine-Grained Object\n  Classification", "comments": "International Conference on Digital Image Computing: Techniques and\n  Applications, 2016", "journal-ref": null, "doi": "10.1109/DICTA.2016.7797039", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained classification is a relatively new field that has concentrated\non using information from a single image, while ignoring the enormous potential\nof using video data to improve classification. In this work we present the\nnovel task of video-based fine-grained object classification, propose a\ncorresponding new video dataset, and perform a systematic study of several\nrecent deep convolutional neural network (DCNN) based approaches, which we\nspecifically adapt to the task. We evaluate three-dimensional DCNNs, two-stream\nDCNNs, and bilinear DCNNs. Two forms of the two-stream approach are used, where\nspatial and temporal data from two independent DCNNs are fused either via early\nfusion (combination of the fully-connected layers) and late fusion\n(concatenation of the softmax outputs of the DCNNs). For bilinear DCNNs,\ninformation from the convolutional layers of the spatial and temporal DCNNs is\ncombined via local co-occurrences. We then fuse the bilinear DCNN and early\nfusion of the two-stream approach to combine the spatial and temporal\ninformation at the local and global level (Spatio-Temporal Co-occurrence).\nUsing the new and challenging video dataset of birds, classification\nperformance is improved from 23.1% (using single images) to 41.1% when using\nthe Spatio-Temporal Co-occurrence system. Incorporating automatically detected\nbounding box location further improves the classification accuracy to 53.6%.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 16:34:16 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2016 01:23:47 GMT"}, {"version": "v3", "created": "Mon, 24 Oct 2016 06:40:02 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Ge", "ZongYuan", ""], ["McCool", "Chris", ""], ["Sanderson", "Conrad", ""], ["Wang", "Peng", ""], ["Liu", "Lingqiao", ""], ["Reid", "Ian", ""], ["Corke", "Peter", ""]]}, {"id": "1608.00501", "submitter": "Abhishek Maity", "authors": "Abhishek Maity", "title": "Supervised Classification of RADARSAT-2 Polarimetric Data for Different\n  Land Features", "comments": "3 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The pixel percentage belonging to the user defined area that are assigned to\ncluster in a confusion matrix for RADARSAT-2 over Vancouver area has been\nanalysed for classification. In this study, supervised Wishart and Support\nVector Machine (SVM) classifiers over RADARSAT-2 (RS2) fine quadpol mode Single\nLook Complex (SLC) product data is computed and compared. In comparison with\nconventional single channel or dual channel polarization, RADARSAT-2 is fully\npolarimetric, making it to offer better land feature contrast for\nclassification operation.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 17:24:30 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Maity", "Abhishek", ""]]}, {"id": "1608.00507", "submitter": "Jianming Zhang", "authors": "Jianming Zhang, Zhe Lin, Jonathan Brandt, Xiaohui Shen, Stan Sclaroff", "title": "Top-down Neural Attention by Excitation Backprop", "comments": "A shorter version of this paper is accepted at ECCV, 2016 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to model the top-down attention of a Convolutional Neural Network\n(CNN) classifier for generating task-specific attention maps. Inspired by a\ntop-down human visual attention model, we propose a new backpropagation scheme,\ncalled Excitation Backprop, to pass along top-down signals downwards in the\nnetwork hierarchy via a probabilistic Winner-Take-All process. Furthermore, we\nintroduce the concept of contrastive attention to make the top-down attention\nmaps more discriminative. In experiments, we demonstrate the accuracy and\ngeneralizability of our method in weakly supervised localization tasks on the\nMS COCO, PASCAL VOC07 and ImageNet datasets. The usefulness of our method is\nfurther validated in the text-to-region association task. On the Flickr30k\nEntities dataset, we achieve promising performance in phrase localization by\nleveraging the top-down attention of a CNN model that has been trained on\nweakly labeled web images.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 17:49:57 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Zhang", "Jianming", ""], ["Lin", "Zhe", ""], ["Brandt", "Jonathan", ""], ["Shen", "Xiaohui", ""], ["Sclaroff", "Stan", ""]]}, {"id": "1608.00514", "submitter": "Alireza Davoudi", "authors": "Alireza Davoudi, Saeed Shiry Ghidary, Khadijeh Sadatnejad", "title": "Dimensionality reduction based on Distance Preservation to Local Mean\n  (DPLM) for SPD matrices and its application in BCI", "comments": null, "journal-ref": null, "doi": "10.1088/1741-2552/aa61bb", "report-no": null, "categories": "cs.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a nonlinear dimensionality reduction algorithm for\nthe manifold of Symmetric Positive Definite (SPD) matrices that considers the\ngeometry of SPD matrices and provides a low dimensional representation of the\nmanifold with high class discrimination. The proposed algorithm, tries to\npreserve the local structure of the data by preserving distance to local mean\n(DPLM) and also provides an implicit projection matrix. DPLM is linear in terms\nof the number of training samples and may use the label information when they\nare available in order to performance improvement in classification tasks. We\nperformed several experiments on the multi-class dataset IIa from BCI\ncompetition IV. The results show that our approach as dimensionality reduction\ntechnique - leads to superior results in comparison with other competitor in\nthe related literature because of its robustness against outliers. The\nexperiments confirm that the combination of DPLM with FGMDM as the classifier\nleads to the state of the art performance on this dataset.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 15:17:16 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Davoudi", "Alireza", ""], ["Ghidary", "Saeed Shiry", ""], ["Sadatnejad", "Khadijeh", ""]]}, {"id": "1608.00525", "submitter": "Varun Nagaraja", "authors": "Varun K. Nagaraja, Vlad I. Morariu, Larry S. Davis", "title": "Modeling Context Between Objects for Referring Expression Understanding", "comments": "To appear at ECCV 16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Referring expressions usually describe an object using properties of the\nobject and relationships of the object with other objects. We propose a\ntechnique that integrates context between objects to understand referring\nexpressions. Our approach uses an LSTM to learn the probability of a referring\nexpression, with input features from a region and a context region. The context\nregions are discovered using multiple-instance learning (MIL) since annotations\nfor context objects are generally not available for training. We utilize\nmax-margin based MIL objective functions for training the LSTM. Experiments on\nthe Google RefExp and UNC RefExp datasets show that modeling context between\nobjects provides better performance than modeling only object properties. We\nalso qualitatively show that our technique can ground a referring expression to\nits referred region along with the supporting context region.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 19:03:27 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Nagaraja", "Varun K.", ""], ["Morariu", "Vlad I.", ""], ["Davis", "Larry S.", ""]]}, {"id": "1608.00530", "submitter": "Dan Hendrycks", "authors": "Dan Hendrycks, Kevin Gimpel", "title": "Early Methods for Detecting Adversarial Images", "comments": "ICLR 2017 Workshop Contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning classifiers are vulnerable to adversarial\nperturbations. An adversarial perturbation modifies an input to change a\nclassifier's prediction without causing the input to seem substantially\ndifferent to human perception. We deploy three methods to detect adversarial\nimages. Adversaries trying to bypass our detectors must make the adversarial\nimage less pathological or they will fail trying. Our best detection method\nreveals that adversarial images place abnormal emphasis on the lower-ranked\nprincipal components from PCA. Other detectors and a colorful saliency map are\nin an appendix.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 19:13:58 GMT"}, {"version": "v2", "created": "Thu, 23 Mar 2017 18:03:47 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Hendrycks", "Dan", ""], ["Gimpel", "Kevin", ""]]}, {"id": "1608.00567", "submitter": "Changchuan Yin Dr.", "authors": "Changchuan Yin", "title": "Identification of repeats in DNA sequences using nucleotide distribution\n  uniformity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CE cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Repetitive elements are important in genomic structures, functions and\nregulations, yet effective methods in precisely identifying repetitive elements\nin DNA sequences are not fully accessible, and the relationship between\nrepetitive elements and periodicities of genomes is not clearly understood. We\npresent an $\\textit{ab initio}$ method to quantitatively detect repetitive\nelements and infer the consensus repeat pattern in repetitive elements. The\nmethod uses the measure of the distribution uniformity of nucleotides at\nperiodic positions in DNA sequences or genomes. It can identify periodicities,\nconsensus repeat patterns, copy numbers and perfect levels of repetitive\nelements. The results of using the method on different DNA sequences and\ngenomes demonstrate efficacy and accuracy in identifying repeat patterns and\nperiodicities. The complexity of the method is linear with respect to the\nlengths of the analyzed sequences.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 21:16:18 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Yin", "Changchuan", ""]]}, {"id": "1608.00611", "submitter": "Priyadarshini Panda", "authors": "Priyadarshini Panda, and Kaushik Roy", "title": "Attention Tree: Learning Hierarchies of Visual Features for Large-Scale\n  Image Recognition", "comments": "11 pages, 8 figures, Under review in IEEE Transactions on Neural\n  Networks and Learning systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key challenges in machine learning is to design a computationally\nefficient multi-class classifier while maintaining the output accuracy and\nperformance. In this paper, we present a tree-based classifier: Attention Tree\n(ATree) for large-scale image classification that uses recursive Adaboost\ntraining to construct a visual attention hierarchy. The proposed attention\nmodel is inspired from the biological 'selective tuning mechanism for cortical\nvisual processing'. We exploit the inherent feature similarity across images in\ndatasets to identify the input variability and use recursive optimization\nprocedure, to determine data partitioning at each node, thereby, learning the\nattention hierarchy. A set of binary classifiers is organized on top of the\nlearnt hierarchy to minimize the overall test-time complexity. The attention\nmodel maximizes the margins for the binary classifiers for optimal decision\nboundary modelling, leading to better performance at minimal complexity. The\nproposed framework has been evaluated on both Caltech-256 and SUN datasets and\nachieves accuracy improvement over state-of-the-art tree-based methods at\nsignificantly lower computational cost.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 20:51:29 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Panda", "Priyadarshini", ""], ["Roy", "Kaushik", ""]]}, {"id": "1608.00641", "submitter": "Eyasu Mequanint Zemene", "authors": "Eyasu Zemene, Marcello Pelillo", "title": "Interactive Image Segmentation Using Constrained Dominant Sets", "comments": "Accepted at ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to interactive image segmentation based on some\nproperties of a family of quadratic optimization problems related to dominant\nsets, a well-known graph-theoretic notion of a cluster which generalizes the\nconcept of a maximal clique to edge-weighted graphs. In particular, we show\nthat by properly controlling a regularization parameter which determines the\nstructure and the scale of the underlying problem, we are in a position to\nextract groups of dominant-set clusters which are constrained to contain\nuser-selected elements. The resulting algorithm can deal naturally with any\ntype of input modality, including scribbles, sloppy contours, and bounding\nboxes, and is able to robustly handle noisy annotations on the part of the\nuser. Experiments on standard benchmark datasets show the effectiveness of our\napproach as compared to state-of-the-art algorithms on a variety of natural\nimages under several input conditions.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 23:37:41 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 17:32:04 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Zemene", "Eyasu", ""], ["Pelillo", "Marcello", ""]]}, {"id": "1608.00668", "submitter": "Konstantinos Raftopoulos", "authors": "Konstantinos A. Raftopoulos, Stefanos D. Kollias, Marin Ferecatu", "title": "Global Vertices and the Noising Paradox", "comments": "19 pages, 11 figures", "journal-ref": "Int J Comput Vis (2017)", "doi": "10.1007/s11263-017-1034-6", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A theoretical and experimental analysis related to the identification of\nvertices of unknown shapes is presented. Shapes are seen as real functions of\ntheir closed boundary. Unlike traditional approaches, which see curvature as\nthe rate of change of the tangent to the curve, an alternative global\nperspective of curvature is examined providing insight into the process of\nnoise-enabled vertex localization. The analysis leads to a paradox, that\ncertain vertices can be localized better in the presence of noise. The concept\nof noising is thus considered and a relevant global method for localizing\n\"Global Vertices\" is investigated. Theoretical analysis reveals that induced\nnoise can help localizing certain vertices if combined with global descriptors.\nExperiments with noise and a comparison to localized methods validate the\ntheoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 01:30:28 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Raftopoulos", "Konstantinos A.", ""], ["Kollias", "Stefanos D.", ""], ["Ferecatu", "Marin", ""]]}, {"id": "1608.00700", "submitter": "Qifei Wang", "authors": "Qifei Wang", "title": "A Survey of Visual Analysis of Human Motion and Its Applications", "comments": "5 pages, conference paper in VCIP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes the recent progress in human motion analysis and its\napplications. In the beginning, we reviewed the motion capture systems and the\nrepresentation model of human's motion data. Next, we sketched the advanced\nhuman motion data processing technologies, including motion data filtering,\ntemporal alignment, and segmentation. The following parts overview the\nstate-of-the-art approaches of action recognition and dynamics measuring since\nthese two are the most active research areas in human motion analysis. The last\npart discusses some emerging applications of the human motion analysis in\nhealthcare, human robot interaction, security surveillance, virtual reality and\nanimation. The promising research topics of human motion analysis in the future\nis also summarized in the last part.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 05:50:11 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 05:15:03 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Wang", "Qifei", ""]]}, {"id": "1608.00753", "submitter": "Nick Schneider", "authors": "Nick Schneider, Lukas Schneider, Peter Pinggera, Uwe Franke, Marc\n  Pollefeys, Christoph Stiller", "title": "Semantically Guided Depth Upsampling", "comments": "German Conference on Pattern Recognition 2016 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for accurate and efficient up- sampling of sparse\ndepth data, guided by high-resolution imagery. Our approach goes beyond the use\nof intensity cues only and additionally exploits object boundary cues through\nstructured edge detection and semantic scene labeling for guidance. Both cues\nare combined within a geodesic distance measure that allows for\nboundary-preserving depth in- terpolation while utilizing local context. We\nmodel the observed scene structure by locally planar elements and formulate the\nupsampling task as a global energy minimization problem. Our method determines\nglob- ally consistent solutions and preserves fine details and sharp depth\nbound- aries. In our experiments on several public datasets at different levels\nof application, we demonstrate superior performance of our approach over the\nstate-of-the-art, even for very sparse measurements.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 09:44:53 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Schneider", "Nick", ""], ["Schneider", "Lukas", ""], ["Pinggera", "Peter", ""], ["Franke", "Uwe", ""], ["Pollefeys", "Marc", ""], ["Stiller", "Christoph", ""]]}, {"id": "1608.00762", "submitter": "Han Gong", "authors": "Han Gong, Darren P. Cosker", "title": "Interactive Removal and Ground Truth for Difficult Shadow Scenes", "comments": "Accepted by JOSA A", "journal-ref": null, "doi": "10.1364/JOSAA.33.001798", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A user-centric method for fast, interactive, robust and high-quality shadow\nremoval is presented. Our algorithm can perform detection and removal in a\nrange of difficult cases: such as highly textured and colored shadows. To\nperform detection an on-the-fly learning approach is adopted guided by two\nrough user inputs for the pixels of the shadow and the lit area. After\ndetection, shadow removal is performed by registering the penumbra to a\nnormalized frame which allows us efficient estimation of non-uniform shadow\nillumination changes, resulting in accurate and robust removal. Another major\ncontribution of this work is the first validated and multi-scene category\nground truth for shadow removal algorithms. This data set containing 186 images\neliminates inconsistencies between shadow and shadow-free images and provides a\nrange of different shadow types such as soft, textured, colored and broken\nshadow. Using this data, the most thorough comparison of state-of-the-art\nshadow removal methods to date is performed, showing our proposed new algorithm\nto outperform the state-of-the-art across several measures and shadow category.\nTo complement our dataset, an online shadow removal benchmark website is also\npresented to encourage future open comparisons in this challenging field of\nresearch.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 10:51:07 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Gong", "Han", ""], ["Cosker", "Darren P.", ""]]}, {"id": "1608.00775", "submitter": "Michele Volpi Michele Volpi", "authors": "Michele Volpi, Devis Tuia", "title": "Dense semantic labeling of sub-decimeter resolution images with\n  convolutional neural networks", "comments": "Accepted in IEEE Transactions on Geoscience and Remote Sensing, 2016", "journal-ref": null, "doi": "10.1109/TGRS.2016.2616585", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic labeling (or pixel-level land-cover classification) in ultra-high\nresolution imagery (< 10cm) requires statistical models able to learn high\nlevel concepts from spatial data, with large appearance variations.\nConvolutional Neural Networks (CNNs) achieve this goal by learning\ndiscriminatively a hierarchy of representations of increasing abstraction.\n  In this paper we present a CNN-based system relying on an\ndownsample-then-upsample architecture. Specifically, it first learns a rough\nspatial map of high-level representations by means of convolutions and then\nlearns to upsample them back to the original resolution by deconvolutions. By\ndoing so, the CNN learns to densely label every pixel at the original\nresolution of the image. This results in many advantages, including i)\nstate-of-the-art numerical accuracy, ii) improved geometric accuracy of\npredictions and iii) high efficiency at inference time.\n  We test the proposed system on the Vaihingen and Potsdam sub-decimeter\nresolution datasets, involving semantic labeling of aerial images of 9cm and\n5cm resolution, respectively. These datasets are composed by many large and\nfully annotated tiles allowing an unbiased evaluation of models making use of\nspatial information. We do so by comparing two standard CNN architectures to\nthe proposed one: standard patch classification, prediction of local label\npatches by employing only convolutions and full patch labeling by employing\ndeconvolutions. All the systems compare favorably or outperform a\nstate-of-the-art baseline relying on superpixels and powerful appearance\ndescriptors. The proposed full patch labeling CNN outperforms these models by a\nlarge margin, also showing a very appealing inference time.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 11:33:44 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 15:07:33 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Volpi", "Michele", ""], ["Tuia", "Devis", ""]]}, {"id": "1608.00785", "submitter": "Ayman Alharbiy Ayman Alharbiy", "authors": "Yasser Mohammad Seddiq, A. A. Alharbiy and Moayyad Hamza Ghunaim", "title": "Shape and Centroid Independent Clustring Algorithm for Crowd Management\n  Applications", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering techniques play an important role in data mining and its related\napplications. Among the challenging applications that require robust and\nreal-time processing are crowd management and group trajectory applications. In\nthis paper, a robust and low-complexity clustering algorithm is proposed. It is\ncapable of processing data in a manner that is shape and centroid independent.\nThe algorithm is of low complexity due to the novel technique to compute the\nmatrix power. The algorithm was tested on real and synthetic data and test\nresults are reported.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 12:19:21 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Seddiq", "Yasser Mohammad", ""], ["Alharbiy", "A. A.", ""], ["Ghunaim", "Moayyad Hamza", ""]]}, {"id": "1608.00797", "submitter": "Yuanjun Xiong", "authors": "Yuanjun Xiong, Limin Wang, Zhe Wang, Bowen Zhang, Hang Song, Wei Li,\n  Dahua Lin, Yu Qiao, Luc Van Gool, Xiaoou Tang", "title": "CUHK & ETHZ & SIAT Submission to ActivityNet Challenge 2016", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the method that underlies our submission to the untrimmed\nvideo classification task of ActivityNet Challenge 2016. We follow the basic\npipeline of temporal segment networks and further raise the performance via a\nnumber of other techniques. Specifically, we use the latest deep model\narchitecture, e.g., ResNet and Inception V3, and introduce new aggregation\nschemes (top-k and attention-weighted pooling). Additionally, we incorporate\nthe audio as a complementary channel, extracting relevant information via a CNN\napplied to the spectrograms. With these techniques, we derive an ensemble of\ndeep models, which, together, attains a high classification accuracy (mAP\n$93.23\\%$) on the testing set and secured the first place in the challenge.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 12:58:30 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Xiong", "Yuanjun", ""], ["Wang", "Limin", ""], ["Wang", "Zhe", ""], ["Zhang", "Bowen", ""], ["Song", "Hang", ""], ["Li", "Wei", ""], ["Lin", "Dahua", ""], ["Qiao", "Yu", ""], ["Van Gool", "Luc", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1608.00813", "submitter": "Lucia Vadicamo", "authors": "Giuseppe Amato, Fabrizio Falchi, Lucia Vadicamo", "title": "Aggregating Binary Local Descriptors for Image Retrieval", "comments": null, "journal-ref": null, "doi": "10.1007/s11042-017-4450-2", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-Based Image Retrieval based on local features is computationally\nexpensive because of the complexity of both extraction and matching of local\nfeature. On one hand, the cost for extracting, representing, and comparing\nlocal visual descriptors has been dramatically reduced by recently proposed\nbinary local features. On the other hand, aggregation techniques provide a\nmeaningful summarization of all the extracted feature of an image into a single\ndescriptor, allowing us to speed up and scale up the image search. Only a few\nworks have recently mixed together these two research directions, defining\naggregation methods for binary local features, in order to leverage on the\nadvantage of both approaches. In this paper, we report an extensive comparison\namong state-of-the-art aggregation methods applied to binary features. Then, we\nmathematically formalize the application of Fisher Kernels to Bernoulli Mixture\nModels. Finally, we investigate the combination of the aggregated binary\nfeatures with the emerging Convolutional Neural Network (CNN) features. Our\nresults show that aggregation methods on binary features are effective and\nrepresent a worthwhile alternative to the direct matching. Moreover, the\ncombination of the CNN with the Fisher Vector (FV) built upon binary features\nallowed us to obtain a relative improvement over the CNN results that is in\nline with that recently obtained using the combination of the CNN with the FV\nbuilt upon SIFTs. The advantage of using the FV built upon binary features is\nthat the extraction process of binary features is about two order of magnitude\nfaster than SIFTs.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 13:28:16 GMT"}, {"version": "v2", "created": "Fri, 3 Feb 2017 15:19:15 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Amato", "Giuseppe", ""], ["Falchi", "Fabrizio", ""], ["Vadicamo", "Lucia", ""]]}, {"id": "1608.00853", "submitter": "Daniel Roy", "authors": "Gintare Karolina Dziugaite, Zoubin Ghahramani, Daniel M. Roy", "title": "A study of the effect of JPG compression on adversarial images", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network image classifiers are known to be vulnerable to adversarial\nimages, i.e., natural images which have been modified by an adversarial\nperturbation specifically designed to be imperceptible to humans yet fool the\nclassifier. Not only can adversarial images be generated easily, but these\nimages will often be adversarial for networks trained on disjoint subsets of\ndata or with different architectures. Adversarial images represent a potential\nsecurity risk as well as a serious machine learning challenge---it is clear\nthat vulnerable neural networks perceive images very differently from humans.\nNoting that virtually every image classification data set is composed of JPG\nimages, we evaluate the effect of JPG compression on the classification of\nadversarial images. For Fast-Gradient-Sign perturbations of small magnitude, we\nfound that JPG compression often reverses the drop in classification accuracy\nto a large extent, but not always. As the magnitude of the perturbations\nincreases, JPG recompression alone is insufficient to reverse the effect.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 14:57:18 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Dziugaite", "Gintare Karolina", ""], ["Ghahramani", "Zoubin", ""], ["Roy", "Daniel M.", ""]]}, {"id": "1608.00859", "submitter": "Limin Wang", "authors": "Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang,\n  and Luc Van Gool", "title": "Temporal Segment Networks: Towards Good Practices for Deep Action\n  Recognition", "comments": "Accepted by ECCV 2016. Based on this method, we won the ActivityNet\n  challenge 2016 in untrimmed video classification", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks have achieved great success for visual\nrecognition in still images. However, for action recognition in videos, the\nadvantage over traditional methods is not so evident. This paper aims to\ndiscover the principles to design effective ConvNet architectures for action\nrecognition in videos and learn these models given limited training samples.\nOur first contribution is temporal segment network (TSN), a novel framework for\nvideo-based action recognition. which is based on the idea of long-range\ntemporal structure modeling. It combines a sparse temporal sampling strategy\nand video-level supervision to enable efficient and effective learning using\nthe whole action video. The other contribution is our study on a series of good\npractices in learning ConvNets on video data with the help of temporal segment\nnetwork. Our approach obtains the state-the-of-art performance on the datasets\nof HMDB51 ( $ 69.4\\% $) and UCF101 ($ 94.2\\% $). We also visualize the learned\nConvNet models, which qualitatively demonstrates the effectiveness of temporal\nsegment network and the proposed good practices.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 15:06:50 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Wang", "Limin", ""], ["Xiong", "Yuanjun", ""], ["Wang", "Zhe", ""], ["Qiao", "Yu", ""], ["Lin", "Dahua", ""], ["Tang", "Xiaoou", ""], ["Van Gool", "Luc", ""]]}, {"id": "1608.00887", "submitter": "Connor Schenck", "authors": "Connor Schenck, Dieter Fox", "title": "Towards Learning to Perceive and Reason About Liquids", "comments": "Published in International Symposium on Experimental Robotics (ISER)\n  2016. arXiv admin note: text overlap with arXiv:1606.06266", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in AI and robotics have claimed many incredible results with\ndeep learning, yet no work to date has applied deep learning to the problem of\nliquid perception and reasoning. In this paper, we apply fully-convolutional\ndeep neural networks to the tasks of detecting and tracking liquids. We\nevaluate three models: a single-frame network, multi-frame network, and a LSTM\nrecurrent network. Our results show that the best liquid detection results are\nachieved when aggregating data over multiple frames and that the LSTM network\noutperforms the other two in both tasks. This suggests that LSTM-based neural\nnetworks have the potential to be a key component for enabling robots to handle\nliquids using robust, closed-loop controllers.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 16:30:18 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Schenck", "Connor", ""], ["Fox", "Dieter", ""]]}, {"id": "1608.00905", "submitter": "Sonal Goel", "authors": "Sonal Goel, Niharika Sachdeva, Ponnurangam Kumaraguru, A V Subramanyam\n  and Divam Gupta", "title": "PicHunt: Social Media Image Retrieval for Improved Law Enforcement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  First responders are increasingly using social media to identify and reduce\ncrime for well-being and safety of the society. Images shared on social media\nhurting religious, political, communal and other sentiments of people, often\ninstigate violence and create law & order situations in society. This results\nin the need for first responders to inspect the spread of such images and users\npropagating them on social media. In this paper, we present a comparison\nbetween different hand-crafted features and a Convolutional Neural Network\n(CNN) model to retrieve similar images, which outperforms state-of-art\nhand-crafted features. We propose an Open-Source-Intelligent (OSINT) real-time\nimage search system, robust to retrieve modified images that allows first\nresponders to analyze the current spread of images, sentiments floating and\ndetails of users propagating such content. The system also aids officials to\nsave time of manually analyzing the content by reducing the search space on an\naverage by 67%.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 17:09:19 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2016 11:16:36 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Goel", "Sonal", ""], ["Sachdeva", "Niharika", ""], ["Kumaraguru", "Ponnurangam", ""], ["Subramanyam", "A V", ""], ["Gupta", "Divam", ""]]}, {"id": "1608.00911", "submitter": "Wen-Sheng Chu", "authors": "Wen-Sheng Chu, Fernando De la Torre, Jeffrey F. Cohn", "title": "Modeling Spatial and Temporal Cues for Multi-label Facial Action Unit\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial action units (AUs) are essential to decode human facial expressions.\nResearchers have focused on training AU detectors with a variety of features\nand classifiers. However, several issues remain. These are spatial\nrepresentation, temporal modeling, and AU correlation. Unlike most studies that\ntackle these issues separately, we propose a hybrid network architecture to\njointly address them. Specifically, spatial representations are extracted by a\nConvolutional Neural Network (CNN), which, as analyzed in this paper, is able\nto reduce person-specific biases caused by hand-crafted features (eg, SIFT and\nGabor). To model temporal dependencies, Long Short-Term Memory (LSTMs) are\nstacked on top of these representations, regardless of the lengths of input\nvideos. The outputs of CNNs and LSTMs are further aggregated into a fusion\nnetwork to produce per-frame predictions of 12 AUs. Our network naturally\naddresses the three issues, and leads to superior performance compared to\nexisting methods that consider these issues independently. Extensive\nexperiments were conducted on two large spontaneous datasets, GFT and BP4D,\ncontaining more than 400,000 frames coded with 12 AUs. On both datasets, we\nreport significant improvement over a standard multi-label CNN and\nfeature-based state-of-the-art. Finally, we provide visualization of the\nlearned AU models, which, to our best knowledge, reveal how machines see facial\nAUs for the first time.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 17:37:38 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Chu", "Wen-Sheng", ""], ["De la Torre", "Fernando", ""], ["Cohn", "Jeffrey F.", ""]]}, {"id": "1608.01017", "submitter": "Thomas Rogers", "authors": "Thomas W. Rogers, Nicolas Jaccard, Edward J. Morton, Lewis D. Griffin", "title": "Automated X-ray Image Analysis for Cargo Security: Critical Review and\n  Future Promise", "comments": "Submission to Journal of X-ray Science and Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review the relatively immature field of automated image analysis for X-ray\ncargo imagery. There is increasing demand for automated analysis methods that\ncan assist in the inspection and selection of containers, due to the\never-growing volumes of traded cargo and the increasing concerns that customs-\nand security-related threats are being smuggled across borders by organised\ncrime and terrorist networks. We split the field into the classical pipeline of\nimage preprocessing and image understanding. Preprocessing includes: image\nmanipulation; quality improvement; Threat Image Projection (TIP); and material\ndiscrimination and segmentation. Image understanding includes: Automated Threat\nDetection (ATD); and Automated Contents Verification (ACV). We identify several\ngaps in the literature that need to be addressed and propose ideas for future\nresearch. Where the current literature is sparse we borrow from the\nsingle-view, multi-view, and CT X-ray baggage domains, which have some\ncharacteristics in common with X-ray cargo.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 22:22:41 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Rogers", "Thomas W.", ""], ["Jaccard", "Nicolas", ""], ["Morton", "Edward J.", ""], ["Griffin", "Lewis D.", ""]]}, {"id": "1608.01024", "submitter": "Iman Abbasnejad", "authors": "N Dinesh Reddy, Iman Abbasnejad, Sheetal Reddy, Amit Kumar Mondal,\n  Vindhya Devalla", "title": "Incremental Real-Time Multibody VSLAM with Trajectory Optimization Using\n  Stereo Camera", "comments": "Available on IROS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real time outdoor navigation in highly dynamic environments is an crucial\nproblem. The recent literature on real time static SLAM don't scale up to\ndynamic outdoor environments. Most of these methods assume moving objects as\noutliers or discard the information provided by them. We propose an algorithm\nto jointly infer the camera trajectory and the moving object trajectory\nsimultaneously. In this paper, we perform a sparse scene flow based motion\nsegmentation using a stereo camera. The segmented objects motion models are\nused for accurate localization of the camera trajectory as well as the moving\nobjects. We exploit the relationship between moving objects for improving the\naccuracy of the poses. We formulate the poses as a factor graph incorporating\nall the constraints. We achieve exact incremental solution by solving a full\nnonlinear optimization problem in real time. The evaluation is performed on the\nchallenging KITTI dataset with multiple moving cars.Our method outperforms the\nprevious baselines in outdoor navigation.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 23:03:19 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Reddy", "N Dinesh", ""], ["Abbasnejad", "Iman", ""], ["Reddy", "Sheetal", ""], ["Mondal", "Amit Kumar", ""], ["Devalla", "Vindhya", ""]]}, {"id": "1608.01026", "submitter": "Victor Fragoso", "authors": "Victor Fragoso, Walter Scheirer, Joao Hespanha, Matthew Turk", "title": "One-Class Slab Support Vector Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces the one-class slab SVM (OCSSVM), a one-class classifier\nthat aims at improving the performance of the one-class SVM. The proposed\nstrategy reduces the false positive rate and increases the accuracy of\ndetecting instances from novel classes. To this end, it uses two parallel\nhyperplanes to learn the normal region of the decision scores of the target\nclass. OCSSVM extends one-class SVM since it can scale and learn non-linear\ndecision functions via kernel methods. The experiments on two publicly\navailable datasets show that OCSSVM can consistently outperform the one-class\nSVM and perform comparable to or better than other state-of-the-art one-class\nclassifiers.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 23:06:35 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Fragoso", "Victor", ""], ["Scheirer", "Walter", ""], ["Hespanha", "Joao", ""], ["Turk", "Matthew", ""]]}, {"id": "1608.01041", "submitter": "Emad Barsoum", "authors": "Emad Barsoum, Cha Zhang, Cristian Canton Ferrer and Zhengyou Zhang", "title": "Training Deep Networks for Facial Expression Recognition with\n  Crowd-Sourced Label Distribution", "comments": "Submitted to ICMI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd sourcing has become a widely adopted scheme to collect ground truth\nlabels. However, it is a well-known problem that these labels can be very\nnoisy. In this paper, we demonstrate how to learn a deep convolutional neural\nnetwork (DCNN) from noisy labels, using facial expression recognition as an\nexample. More specifically, we have 10 taggers to label each input image, and\ncompare four different approaches to utilizing the multiple labels: majority\nvoting, multi-label learning, probabilistic label drawing, and cross-entropy\nloss. We show that the traditional majority voting scheme does not perform as\nwell as the last two approaches that fully leverage the label distribution. An\nenhanced FER+ data set with multiple labels for each face image will also be\nshared with the research community.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 01:26:32 GMT"}, {"version": "v2", "created": "Sat, 24 Sep 2016 01:20:03 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Barsoum", "Emad", ""], ["Zhang", "Cha", ""], ["Ferrer", "Cristian Canton", ""], ["Zhang", "Zhengyou", ""]]}, {"id": "1608.01059", "submitter": "Wenbing Huang", "authors": "Wenbing Huang, Fuchun Sun, Lele Cao and Mehrtash Harandi", "title": "Analyzing Linear Dynamical Systems: From Modeling to Coding and Learning", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encoding time-series with Linear Dynamical Systems (LDSs) leads to rich\nmodels with applications ranging from dynamical texture recognition to video\nsegmentation to name a few. In this paper, we propose to represent LDSs with\ninfinite-dimensional subspaces and derive an analytic solution to obtain stable\nLDSs. We then devise efficient algorithms to perform sparse coding and\ndictionary learning on the space of infinite-dimensional subspaces. In\nparticular, two solutions are developed to sparsely encode an LDS. In the first\nmethod, we map the subspaces into a Reproducing Kernel Hilbert Space (RKHS) and\nachieve our goal through kernel sparse coding. As for the second solution, we\npropose to embed the infinite-dimensional subspaces into the space of symmetric\nmatrices and formulate the sparse coding accordingly in the induced space. For\ndictionary learning, we encode time-series by introducing a novel concept,\nnamely the two-fold LDSs. We then make use of the two-fold LDSs to derive an\nanalytical form for updating atoms of an LDS dictionary, i.e., each atom is an\nLDS itself. Compared to several baselines and state-of-the-art methods, the\nproposed methods yield higher accuracies in various classification tasks\nincluding video classification and tactile recognition.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 02:44:57 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 02:42:26 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Huang", "Wenbing", ""], ["Sun", "Fuchun", ""], ["Cao", "Lele", ""], ["Harandi", "Mehrtash", ""]]}, {"id": "1608.01074", "submitter": "Tal Remez", "authors": "Tal Remez, Or Litany, Shachar Yoseff, Harel Haim and Alex Bronstein", "title": "FPGA system for real-time computational extended depth of field imaging\n  using phase aperture coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a proof-of-concept end-to-end system for computational extended\ndepth of field (EDOF) imaging. The acquisition is performed through a\nphase-coded aperture implemented by placing a thin wavelength-dependent optical\nmask inside the pupil of a conventional camera lens, as a result of which, each\ncolor channel is focused at a different depth. The reconstruction process\nreceives the raw Bayer image as the input, and performs blind estimation of the\noutput color image in focus at an extended range of depths using a patch-wise\nsparse prior. We present a fast non-iterative reconstruction algorithm\noperating with constant latency in fixed-point arithmetics and achieving\nreal-time performance in a prototype FPGA implementation. The output of the\nsystem, on simulated and real-life scenes, is qualitatively and quantitatively\nbetter than the result of clear-aperture imaging followed by state-of-the-art\nblind deblurring.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 05:28:18 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Remez", "Tal", ""], ["Litany", "Or", ""], ["Yoseff", "Shachar", ""], ["Haim", "Harel", ""], ["Bronstein", "Alex", ""]]}, {"id": "1608.01079", "submitter": "Dilip K. Prasad", "authors": "D. K. Prasad, C. K. Prasath, D. Rajan, L. Rachmawati, E. Rajabaly, C.\n  Quek", "title": "Challenges in video based object detection in maritime scenario using\n  computer vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the technical challenges in maritime image processing\nand machine vision problems for video streams generated by cameras. Even well\ndocumented problems of horizon detection and registration of frames in a video\nare very challenging in maritime scenarios. More advanced problems of\nbackground subtraction and object detection in video streams are very\nchallenging. Challenges arising from the dynamic nature of the background,\nunavailability of static cues, presence of small objects at distant\nbackgrounds, illumination effects, all contribute to the challenges as\ndiscussed here.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 05:55:18 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Prasad", "D. K.", ""], ["Prasath", "C. K.", ""], ["Rajan", "D.", ""], ["Rachmawati", "L.", ""], ["Rajabaly", "E.", ""], ["Quek", "C.", ""]]}, {"id": "1608.01082", "submitter": "Jinghua Wang", "authors": "Jinghua Wang, Zhenhua Wang, Dacheng Tao, Simon See, Gang Wang", "title": "Learning Common and Specific Features for RGB-D Semantic Segmentation\n  with Deconvolutional Networks", "comments": "ECCV 2016, 16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of RGB-D semantic segmentation of indoor\nimages. We take advantage of deconvolutional networks which can predict\npixel-wise class labels, and develop a new structure for deconvolution of\nmultiple modalities. We propose a novel feature transformation network to\nbridge the convolutional networks and deconvolutional networks. In the feature\ntransformation network, we correlate the two modalities by discovering common\nfeatures between them, as well as characterize each modality by discovering\nmodality specific features. With the common features, we not only closely\ncorrelate the two modalities, but also allow them to borrow features from each\nother to enhance the representation of shared information. With specific\nfeatures, we capture the visual patterns that are only visible in one modality.\nThe proposed network achieves competitive segmentation accuracy on NYU depth\ndataset V1 and V2.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 06:05:16 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Wang", "Jinghua", ""], ["Wang", "Zhenhua", ""], ["Tao", "Dacheng", ""], ["See", "Simon", ""], ["Wang", "Gang", ""]]}, {"id": "1608.01127", "submitter": "Alban Laflaqui\\`ere Dr", "authors": "Alban Laflaqui\\`ere", "title": "Autonomous Grounding of Visual Field Experience through Sensorimotor\n  Prediction", "comments": "6 pages, 4 figures, ICDL-Epirob 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a developmental framework, autonomous robots need to explore the world and\nlearn how to interact with it. Without an a priori model of the system, this\nopens the challenging problem of having robots master their interface with the\nworld: how to perceive their environment using their sensors, and how to act in\nit using their motors. The sensorimotor approach of perception claims that a\nnaive agent can learn to master this interface by capturing regularities in the\nway its actions transform its sensory inputs. In this paper, we apply such an\napproach to the discovery and mastery of the visual field associated with a\nvisual sensor. A computational model is formalized and applied to a simulated\nsystem to illustrate the approach.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 09:25:35 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Laflaqui\u00e8re", "Alban", ""]]}, {"id": "1608.01137", "submitter": "Enrique S\\'anchez Lozano", "authors": "Enrique S\\'anchez-Lozano and Brais Martinez and Georgios Tzimiropoulos\n  and Michel Valstar", "title": "Cascaded Continuous Regression for Real-time Incremental Face Tracking", "comments": "ECCV 2016 accepted paper, with supplementary material included as\n  appendices. References to Equations fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel real-time algorithm for facial landmark\ntracking. Compared to detection, tracking has both additional challenges and\nopportunities. Arguably the most important aspect in this domain is updating a\ntracker's models as tracking progresses, also known as incremental (face)\ntracking. While this should result in more accurate localisation, how to do\nthis online and in real time without causing a tracker to drift is still an\nimportant open research question. We address this question in the cascaded\nregression framework, the state-of-the-art approach for facial landmark\nlocalisation. Because incremental learning for cascaded regression is costly,\nwe propose a much more efficient yet equally accurate alternative using\ncontinuous regression. More specifically, we first propose cascaded continuous\nregression (CCR) and show its accuracy is equivalent to the Supervised Descent\nMethod. We then derive the incremental learning updates for CCR (iCCR) and show\nthat it is an order of magnitude faster than standard incremental learning for\ncascaded regression, bringing the time required for the update from seconds\ndown to a fraction of a second, thus enabling real-time tracking. Finally, we\nevaluate iCCR and show the importance of incremental learning in achieving\nstate-of-the-art performance. Code for our iCCR is available from\nhttp://www.cs.nott.ac.uk/~psxes1\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 10:17:22 GMT"}, {"version": "v2", "created": "Sat, 6 Aug 2016 21:03:34 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["S\u00e1nchez-Lozano", "Enrique", ""], ["Martinez", "Brais", ""], ["Tzimiropoulos", "Georgios", ""], ["Valstar", "Michel", ""]]}, {"id": "1608.01250", "submitter": "Shan Yang", "authors": "Shan Yang, Tanya Ambert, Zherong Pan, Ke Wang, Licheng Yu, Tamara Berg\n  and Ming C. Lin", "title": "Detailed Garment Recovery from a Single-View Image", "comments": "Comparison added. Algorithm added. Equations cleaned up", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recent garment capturing techniques rely on acquiring multiple views of\nclothing, which may not always be readily available, especially in the case of\npre-existing photographs from the web. As an alternative, we pro- pose a method\nthat is able to compute a rich and realistic 3D model of a human body and its\noutfits from a single photograph with little human in- teraction. Our algorithm\nis not only able to capture the global shape and geometry of the clothing, it\ncan also extract small but important details of cloth, such as occluded\nwrinkles and folds. Unlike previous methods using full 3D information (i.e.\ndepth, multi-view images, or sampled 3D geom- etry), our approach achieves\ndetailed garment recovery from a single-view image by using statistical,\ngeometric, and physical priors and a combina- tion of parameter estimation,\nsemantic parsing, shape recovery, and physics- based cloth simulation. We\ndemonstrate the effectiveness of our algorithm by re-purposing the\nreconstructed garments for virtual try-on and garment transfer applications, as\nwell as cloth animation for digital characters.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 16:42:04 GMT"}, {"version": "v2", "created": "Tue, 6 Sep 2016 03:23:26 GMT"}, {"version": "v3", "created": "Thu, 8 Sep 2016 20:14:25 GMT"}, {"version": "v4", "created": "Mon, 12 Sep 2016 19:04:56 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Yang", "Shan", ""], ["Ambert", "Tanya", ""], ["Pan", "Zherong", ""], ["Wang", "Ke", ""], ["Yu", "Licheng", ""], ["Berg", "Tamara", ""], ["Lin", "Ming C.", ""]]}, {"id": "1608.01276", "submitter": "Johannes Stegmaier", "authors": "Johannes Stegmaier and Ralf Mikut", "title": "Fuzzy-based Propagation of Prior Knowledge to Improve Large-Scale Image\n  Analysis Pipelines", "comments": "39 pages, 12 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0187535", "report-no": null, "categories": "cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many automatically analyzable scientific questions are well-posed and offer a\nvariety of information about the expected outcome a priori. Although often\nbeing neglected, this prior knowledge can be systematically exploited to make\nautomated analysis operations sensitive to a desired phenomenon or to evaluate\nextracted content with respect to this prior knowledge. For instance, the\nperformance of processing operators can be greatly enhanced by a more focused\ndetection strategy and the direct information about the ambiguity inherent in\nthe extracted data. We present a new concept for the estimation and propagation\nof uncertainty involved in image analysis operators. This allows using simple\nprocessing operators that are suitable for analyzing large-scale 3D+t\nmicroscopy images without compromising the result quality. On the foundation of\nfuzzy set theory, we transform available prior knowledge into a mathematical\nrepresentation and extensively use it enhance the result quality of various\nprocessing operators. All presented concepts are illustrated on a typical\nbioimage analysis pipeline comprised of seed point detection, segmentation,\nmultiview fusion and tracking. Furthermore, the functionality of the proposed\napproach is validated on a comprehensive simulated 3D+t benchmark data set that\nmimics embryonic development and on large-scale light-sheet microscopy data of\na zebrafish embryo. The general concept introduced in this contribution\nrepresents a new approach to efficiently exploit prior knowledge to improve the\nresult quality of image analysis pipelines. Especially, the automated analysis\nof terabyte-scale microscopy data will benefit from sophisticated and efficient\nalgorithms that enable a quantitative and fast readout. The generality of the\nconcept, however, makes it also applicable to practically any other field with\nprocessing strategies that are arranged as linear pipelines.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 18:21:02 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Stegmaier", "Johannes", ""], ["Mikut", "Ralf", ""]]}, {"id": "1608.01339", "submitter": "Martin Brooks", "authors": "Martin Brooks", "title": "Retinal Vessel Segmentation Using A New Topological Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel topological segmentation of retinal images represents blood vessels\nas connected regions in the continuous image plane, having shape-related\nanalytic and geometric properties. This paper presents topological segmentation\nresults from the DRIVE retinal image database.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 20:26:22 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Brooks", "Martin", ""]]}, {"id": "1608.01372", "submitter": "Giovanni Barbarino", "authors": "Giovanni Barbarino", "title": "Permutation NMF", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative Matrix Factorization(NMF) is a common used technique in machine\nlearning to extract features out of data such as text documents and images\nthanks to its natural clustering properties. In particular, it is popular in\nimage processing since it can decompose several pictures and recognize common\nparts if they're located in the same position over the photos. This paper's aim\nis to present a way to add the translation invariance to the classical NMF,\nthat is, the algorithms presented are able to detect common features, even when\nthey're shifted, in different original images.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 21:59:44 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Barbarino", "Giovanni", ""]]}, {"id": "1608.01391", "submitter": "Masoud Nosrati", "authors": "Masoud Nosrati, Fakhereh Rahimi, Ronak Karimi", "title": "Language free character recognition using character sketch and center of\n  gravity shifting", "comments": "World Applied Programming, Vol (6), Issue (2), July 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research, we present a heuristic method for character recognition.\nFor this purpose, a sketch is constructed from the image that contains the\ncharacter to be recognized. This sketch contains the most important pixels of\nimage that are representatives of original image. These points are the most\nprobable points in pixel-by-pixel matching of image that adapt to target image.\nFurthermore, a technique called gravity shifting is utilized for taking over\nthe problem of elongation of characters. The consequence of combining sketch\nand gravity techniques leaded to a language free character recognition method.\nThis method can be implemented independently for real-time uses or in\ncombination of other classifiers as a feature extraction algorithm. Low\ncomplexity and acceptable performance are the most impressive features of this\nmethod that let it to be simply implemented in mobile and battery-limited\ncomputing devices. Results show that in the best case 86% of accuracy is\nobtained and in the worst case 28% of recognized characters are accurate.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 23:13:01 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Nosrati", "Masoud", ""], ["Rahimi", "Fakhereh", ""], ["Karimi", "Ronak", ""]]}, {"id": "1608.01409", "submitter": "Jongsoo Park", "authors": "Jongsoo Park, Sheng Li, Wei Wen, Ping Tak Peter Tang, Hai Li, Yiran\n  Chen, Pradeep Dubey", "title": "Faster CNNs with Direct Sparse Convolutions and Guided Pruning", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phenomenally successful in practical inference problems, convolutional neural\nnetworks (CNN) are widely deployed in mobile devices, data centers, and even\nsupercomputers. The number of parameters needed in CNNs, however, are often\nlarge and undesirable. Consequently, various methods have been developed to\nprune a CNN once it is trained. Nevertheless, the resulting CNNs offer limited\nbenefits. While pruning the fully connected layers reduces a CNN's size\nconsiderably, it does not improve inference speed noticeably as the compute\nheavy parts lie in convolutions. Pruning CNNs in a way that increase inference\nspeed often imposes specific sparsity structures, thus limiting the achievable\nsparsity levels.\n  We present a method to realize simultaneously size economy and speed\nimprovement while pruning CNNs. Paramount to our success is an efficient\ngeneral sparse-with-dense matrix multiplication implementation that is\napplicable to convolution of feature maps with kernels of arbitrary sparsity\npatterns. Complementing this, we developed a performance model that predicts\nsweet spots of sparsity levels for different layers and on different computer\narchitectures. Together, these two allow us to demonstrate 3.1--7.3$\\times$\nconvolution speedups over dense convolution in AlexNet, on Intel Atom, Xeon,\nand Xeon Phi processors, spanning the spectrum from mobile devices to\nsupercomputers. We also open source our project at\nhttps://github.com/IntelLabs/SkimCaffe.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 01:16:39 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2016 18:42:12 GMT"}, {"version": "v3", "created": "Fri, 16 Sep 2016 03:00:38 GMT"}, {"version": "v4", "created": "Mon, 22 May 2017 16:33:49 GMT"}, {"version": "v5", "created": "Fri, 28 Jul 2017 22:26:27 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Park", "Jongsoo", ""], ["Li", "Sheng", ""], ["Wen", "Wei", ""], ["Tang", "Ping Tak Peter", ""], ["Li", "Hai", ""], ["Chen", "Yiran", ""], ["Dubey", "Pradeep", ""]]}, {"id": "1608.01431", "submitter": "Dong Wang", "authors": "Dong Wang, Haohan Li, Xiaoyu Wei, Xiaoping Wang", "title": "An efficient iterative thresholding method for image segmentation", "comments": "14 pages, 21 figures", "journal-ref": null, "doi": "10.1016/j.jcp.2017.08.020", "report-no": null, "categories": "cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposed an efficient iterative thresholding method for multi-phase image\nsegmentation. The algorithm is based on minimizing piecewise constant\nMumford-Shah functional in which the contour length (or perimeter) is\napproximated by a non-local multi-phase energy. The minimization problem is\nsolved by an iterative method. Each iteration consists of computing simple\nconvolutions followed by a thresholding step. The algorithm is easy to\nimplement and has the optimal complexity $O(N \\log N)$ per iteration. We also\nshow that the iterative algorithm has the total energy decaying property. We\npresent some numerical results to show the efficiency of our method.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 05:37:35 GMT"}, {"version": "v2", "created": "Fri, 12 Aug 2016 07:45:53 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Wang", "Dong", ""], ["Li", "Haohan", ""], ["Wei", "Xiaoyu", ""], ["Wang", "Xiaoping", ""]]}, {"id": "1608.01441", "submitter": "Hao Yang Dr", "authors": "Hao Yang, Joey Tianyi Zhou and Jianfei Cai", "title": "Improving Multi-label Learning with Missing Labels by Structured\n  Semantic Correlations", "comments": "Accepted in ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label learning has attracted significant interests in computer vision\nrecently, finding applications in many vision tasks such as multiple object\nrecognition and automatic image annotation. Associating multiple labels to a\ncomplex image is very difficult, not only due to the intricacy of describing\nthe image, but also because of the incompleteness nature of the observed\nlabels. Existing works on the problem either ignore the label-label and\ninstance-instance correlations or just assume these correlations are linear and\nunstructured. Considering that semantic correlations between images are\nactually structured, in this paper we propose to incorporate structured\nsemantic correlations to solve the missing label problem of multi-label\nlearning. Specifically, we project images to the semantic space with an\neffective semantic descriptor. A semantic graph is then constructed on these\nimages to capture the structured correlations between them. We utilize the\nsemantic graph Laplacian as a smooth term in the multi-label learning\nformulation to incorporate the structured semantic correlations. Experimental\nresults demonstrate the effectiveness of the proposed semantic descriptor and\nthe usefulness of incorporating the structured semantic correlations. We\nachieve better results than state-of-the-art multi-label learning methods on\nfour benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 06:58:32 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Yang", "Hao", ""], ["Zhou", "Joey Tianyi", ""], ["Cai", "Jianfei", ""]]}, {"id": "1608.01471", "submitter": "Jiahui Yu", "authors": "Jiahui Yu, Yuning Jiang, Zhangyang Wang, Zhimin Cao, Thomas Huang", "title": "UnitBox: An Advanced Object Detection Network", "comments": "To appear in ACM MM 2016, 5 pages, 6 figures", "journal-ref": null, "doi": "10.1145/2964284.2967274", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In present object detection systems, the deep convolutional neural networks\n(CNNs) are utilized to predict bounding boxes of object candidates, and have\ngained performance advantages over the traditional region proposal methods.\nHowever, existing deep CNN methods assume the object bounds to be four\nindependent variables, which could be regressed by the $\\ell_2$ loss\nseparately. Such an oversimplified assumption is contrary to the well-received\nobservation, that those variables are correlated, resulting to less accurate\nlocalization. To address the issue, we firstly introduce a novel Intersection\nover Union ($IoU$) loss function for bounding box prediction, which regresses\nthe four bounds of a predicted box as a whole unit. By taking the advantages of\n$IoU$ loss and deep fully convolutional networks, the UnitBox is introduced,\nwhich performs accurate and efficient localization, shows robust to objects of\nvaried shapes and scales, and converges fast. We apply UnitBox on face\ndetection task and achieve the best performance among all published methods on\nthe FDDB benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 09:06:15 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Yu", "Jiahui", ""], ["Jiang", "Yuning", ""], ["Wang", "Zhangyang", ""], ["Cao", "Zhimin", ""], ["Huang", "Thomas", ""]]}, {"id": "1608.01505", "submitter": "Han Gong", "authors": "Han Gong, Graham D. Finlayson, Robert B. Fisher", "title": "Recoding Color Transfer as a Color Homography", "comments": "Accepted by BMVC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color transfer is an image editing process that adjusts the colors of a\npicture to match a target picture's color theme. A natural color transfer not\nonly matches the color styles but also prevents after-transfer artifacts due to\nimage compression, noise, and gradient smoothness change. The recently\ndiscovered color homography theorem proves that colors across a change in\nphotometric viewing condition are related by a homography. In this paper, we\npropose a color-homography-based color transfer decomposition which encodes\ncolor transfer as a combination of chromaticity shift and shading adjustment. A\npowerful form of shading adjustment is shown to be a global shading curve by\nwhich the same shading homography can be applied elsewhere. Our experiments\nshow that the proposed color transfer decomposition provides a very close\napproximation to many popular color transfer methods. The advantage of our\napproach is that the learned color transfer can be applied to many other images\n(e.g. other frames in a video), instead of a frame-to-frame basis. We\ndemonstrate two applications for color transfer enhancement and video color\ngrading re-application. This simple model of color transfer is also important\nfor future color transfer algorithm design.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 11:54:37 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Gong", "Han", ""], ["Finlayson", "Graham D.", ""], ["Fisher", "Robert B.", ""]]}, {"id": "1608.01529", "submitter": "Suman Saha", "authors": "Suman Saha, Gurkirt Singh, Michael Sapienza, Philip H. S. Torr, Fabio\n  Cuzzolin", "title": "Deep Learning for Detecting Multiple Space-Time Action Tubes in Videos", "comments": "Accepted by British Machine Vision Conference 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose an approach to the spatiotemporal localisation\n(detection) and classification of multiple concurrent actions within temporally\nuntrimmed videos. Our framework is composed of three stages. In stage 1,\nappearance and motion detection networks are employed to localise and score\nactions from colour images and optical flow. In stage 2, the appearance network\ndetections are boosted by combining them with the motion detection scores, in\nproportion to their respective spatial overlap. In stage 3, sequences of\ndetection boxes most likely to be associated with a single action instance,\ncalled action tubes, are constructed by solving two energy maximisation\nproblems via dynamic programming. While in the first pass, action paths\nspanning the whole video are built by linking detection boxes over time using\ntheir class-specific scores and their spatial overlap, in the second pass,\ntemporal trimming is performed by ensuring label consistency for all\nconstituting detection boxes. We demonstrate the performance of our algorithm\non the challenging UCF101, J-HMDB-21 and LIRIS-HARL datasets, achieving new\nstate-of-the-art results across the board and significantly increasing\ndetection speed at test time. We achieve a huge leap forward in action\ndetection performance and report a 20% and 11% gain in mAP (mean average\nprecision) on UCF-101 and J-HMDB-21 datasets respectively when compared to the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 13:38:38 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Saha", "Suman", ""], ["Singh", "Gurkirt", ""], ["Sapienza", "Michael", ""], ["Torr", "Philip H. S.", ""], ["Cuzzolin", "Fabio", ""]]}, {"id": "1608.01536", "submitter": "Yingyue Xu", "authors": "Yingyue Xu, Xiaopeng Hong, Fatih Porikli, Xin Liu, Jie Chen, and\n  Guoying Zhao", "title": "Saliency Integration: An Arbitrator Model", "comments": "IEEE Transactions on Multimedia (2018)", "journal-ref": "Y. Xu, X. Hong, F. Porikli, X. Liu, J. Chen, and G. Zhao,\n  \"Saliency Integration: An Arbitrator Model\", IEEE Transactions on Multimedia.\n  2018", "doi": "10.1109/TMM.2018.2856126", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency integration has attracted much attention on unifying saliency maps\nfrom multiple saliency models. Previous offline integration methods usually\nface two challenges: 1. if most of the candidate saliency models misjudge the\nsaliency on an image, the integration result will lean heavily on those\ninferior candidate models; 2. an unawareness of the ground truth saliency\nlabels brings difficulty in estimating the expertise of each candidate model.\nTo address these problems, in this paper, we propose an arbitrator model (AM)\nfor saliency integration. Firstly, we incorporate the consensus of multiple\nsaliency models and the external knowledge into a reference map to effectively\nrectify the misleading by candidate models. Secondly, our quest for ways of\nestimating the expertise of the saliency models without ground truth labels\ngives rise to two distinct online model-expertise estimation methods. Finally,\nwe derive a Bayesian integration framework to reconcile the saliency models of\nvarying expertise and the reference map. To extensively evaluate the proposed\nAM model, we test twenty-seven state-of-the-art saliency models, covering both\ntraditional and deep learning ones, on various combinations over four datasets.\nThe evaluation results show that the AM model improves the performance\nsubstantially compared to the existing state-of-the-art integration methods,\nregardless of the chosen candidate saliency models.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 13:54:16 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 12:41:34 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Xu", "Yingyue", ""], ["Hong", "Xiaopeng", ""], ["Porikli", "Fatih", ""], ["Liu", "Xin", ""], ["Chen", "Jie", ""], ["Zhao", "Guoying", ""]]}, {"id": "1608.01647", "submitter": "Wei Li", "authors": "Wei Li and Christina Tsangouri and Farnaz Abtahi and Zhigang Zhu", "title": "A Recursive Framework for Expression Recognition: From Web Images to\n  Deep Models to Game Dataset", "comments": "Submit to Machine Vision Application Journal. arXiv admin note: text\n  overlap with arXiv:1607.02678", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a recursive framework to recognize facial\nexpressions from images in real scenes. Unlike traditional approaches that\ntypically focus on developing and refining algorithms for improving recognition\nperformance on an existing dataset, we integrate three important components in\na recursive manner: facial dataset generation, facial expression recognition\nmodel building, and interactive interfaces for testing and new data collection.\nTo start with, we first create a candid-images-for-facial-expression (CIFE)\ndataset. We then apply a convolutional neural network (CNN) to CIFE and build a\nCNN model for web image expression classification. In order to increase the\nexpression recognition accuracy, we also fine-tune the CNN model and thus\nobtain a better CNN facial expression recognition model. Based on the\nfine-tuned CNN model, we design a facial expression game engine and collect a\nnew and more balanced dataset, GaMo. The images of this dataset are collected\nfrom the different expressions our game users make when playing the game.\nFinally, we evaluate the GaMo and CIFE datasets and show that our recursive\nframework can help build a better facial expression model for dealing with real\nscene facial expression tasks.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 19:07:08 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Li", "Wei", ""], ["Tsangouri", "Christina", ""], ["Abtahi", "Farnaz", ""], ["Zhu", "Zhigang", ""]]}, {"id": "1608.01658", "submitter": "Hunter Jackson", "authors": "Richard Chen, Yating Jing, Hunter Jackson", "title": "Identifying Metastases in Sentinel Lymph Nodes with Deep Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metastatic presence in lymph nodes is one of the most important prognostic\nvariables of breast cancer. The current diagnostic procedure for manually\nreviewing sentinel lymph nodes, however, is very time-consuming and subjective.\nPathologists have to manually scan an entire digital whole-slide image (WSI)\nfor regions of metastasis that are sometimes only detectable under high\nresolution or entirely hidden from the human visual cortex. From October 2015\nto April 2016, the International Symposium on Biomedical Imaging (ISBI) held\nthe Camelyon Grand Challenge 2016 to crowd-source ideas and algorithms for\nautomatic detection of lymph node metastasis. Using a generalizable stain\nnormalization technique and the Proscia Pathology Cloud computing platform, we\ntrained a deep convolutional neural network on millions of tissue and tumor\nimage tiles to perform slide-based evaluation on our testing set of whole-slide\nimages images, with a sensitivity of 0.96, specificity of 0.89, and AUC score\nof 0.90. Our results indicate that our platform can automatically scan any WSI\nfor metastatic regions without institutional calibration to respective stain\nprofiles.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 19:47:30 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Chen", "Richard", ""], ["Jing", "Yating", ""], ["Jackson", "Hunter", ""]]}, {"id": "1608.01686", "submitter": "Chiwoo Park", "authors": "Chen Mu and Chiwoo Park", "title": "Sparse Filtered SIRT for Electron Tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electron tomographic reconstruction is a method for obtaining a\nthree-dimensional image of a specimen with a series of two dimensional\nmicroscope images taken from different viewing angles. Filtered backprojection,\none of the most popular tomographic reconstruction methods, does not work well\nunder the existence of image noises and missing wedges. This paper presents a\nnew approach to largely mitigate the effect of noises and missing wedges. We\npropose a novel filtered backprojection that optimizes the filter of the\nbackprojection operator in terms of a reconstruction error. This data-dependent\nfilter adaptively chooses the spectral domains of signals and noises,\nsuppressing the noise frequency bands, so it is very effective in denoising. We\nalso propose the new filtered backprojection embedded within the simultaneous\niterative reconstruction iteration for mitigating the effect of missing wedges.\nOur numerical study is presented to show the performance gain of the proposed\napproach over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 20:08:53 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 20:21:48 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Mu", "Chen", ""], ["Park", "Chiwoo", ""]]}, {"id": "1608.01745", "submitter": "Alireza Shafaei", "authors": "Alireza Shafaei and James J. Little and Mark Schmidt", "title": "Play and Learn: Using Video Games to Train Computer Vision Models", "comments": "To appear in the British Machine Vision Conference (BMVC), September\n  2016. -v2: fixed a typo in the references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video games are a compelling source of annotated data as they can readily\nprovide fine-grained groundtruth for diverse tasks. However, it is not clear\nwhether the synthetically generated data has enough resemblance to the\nreal-world images to improve the performance of computer vision models in\npractice. We present experiments assessing the effectiveness on real-world data\nof systems trained on synthetic RGB images that are extracted from a video\ngame. We collected over 60000 synthetic samples from a modern video game with\nsimilar conditions to the real-world CamVid and Cityscapes datasets. We provide\nseveral experiments to demonstrate that the synthetically generated RGB images\ncan be used to improve the performance of deep neural networks on both image\nsegmentation and depth estimation. These results show that a convolutional\nnetwork trained on synthetic data achieves a similar test error to a network\nthat is trained on real-world data for dense image classification. Furthermore,\nthe synthetically generated RGB images can provide similar or better results\ncompared to the real-world datasets if a simple domain adaptation technique is\napplied. Our results suggest that collaboration with game developers for an\naccessible interface to gather data is potentially a fruitful direction for\nfuture work in computer vision.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 03:16:07 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 19:41:47 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Shafaei", "Alireza", ""], ["Little", "James J.", ""], ["Schmidt", "Mark", ""]]}, {"id": "1608.01769", "submitter": "Abhimanyu Dubey", "authors": "Abhimanyu Dubey, Nikhil Naik, Devi Parikh, Ramesh Raskar and C\\'esar\n  A. Hidalgo", "title": "Deep Learning the City : Quantifying Urban Perception At A Global Scale", "comments": "23 pages, 8 figures. Accepted to the European Conference on Computer\n  Vision (ECCV), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision methods that quantify the perception of urban environment are\nincreasingly being used to study the relationship between a city's physical\nappearance and the behavior and health of its residents. Yet, the throughput of\ncurrent methods is too limited to quantify the perception of cities across the\nworld. To tackle this challenge, we introduce a new crowdsourced dataset\ncontaining 110,988 images from 56 cities, and 1,170,000 pairwise comparisons\nprovided by 81,630 online volunteers along six perceptual attributes: safe,\nlively, boring, wealthy, depressing, and beautiful. Using this data, we train a\nSiamese-like convolutional neural architecture, which learns from a joint\nclassification and ranking loss, to predict human judgments of pairwise image\ncomparisons. Our results show that crowdsourcing combined with neural networks\ncan produce urban perception data at the global scale.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 05:58:35 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2016 18:48:37 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Dubey", "Abhimanyu", ""], ["Naik", "Nikhil", ""], ["Parikh", "Devi", ""], ["Raskar", "Ramesh", ""], ["Hidalgo", "C\u00e9sar A.", ""]]}, {"id": "1608.01793", "submitter": "Qilin Li", "authors": "Qilin Li, Ling Li, Wanquan Liu", "title": "Sparse Subspace Clustering via Diffusion Process", "comments": "arXiv admin note: text overlap with arXiv:1203.1005, arXiv:1605.02633\n  by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering refers to the problem of clustering high-dimensional data\nthat lie in a union of low-dimensional subspaces. State-of-the-art subspace\nclustering methods are based on the idea of expressing each data point as a\nlinear combination of other data points while regularizing the matrix of\ncoefficients with L1, L2 or nuclear norms for a sparse solution. L1\nregularization is guaranteed to give a subspace-preserving affinity (i.e.,\nthere are no connections between points from different subspaces) under broad\ntheoretical conditions, but the clusters may not be fully connected. L2 and\nnuclear norm regularization often improve connectivity, but give a\nsubspace-preserving affinity only for independent subspaces. Mixed L1, L2 and\nnuclear norm regularization could offer a balance between the\nsubspace-preserving and connectedness properties, but this comes at the cost of\nincreased computational complexity. This paper focuses on using L1 norm and\nalleviating the corresponding connectivity problem by a simple yet efficient\ndiffusion process on subspace affinity graphs. Without adding any tuning\nparameter , our method can achieve state-of-the-art clustering performance on\nHopkins 155 and Extended Yale B data sets.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 08:05:24 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Li", "Qilin", ""], ["Li", "Ling", ""], ["Liu", "Wanquan", ""]]}, {"id": "1608.01807", "submitter": "Liang Zheng", "authors": "Liang Zheng, Yi Yang, Qi Tian", "title": "SIFT Meets CNN: A Decade Survey of Instance Retrieval", "comments": "Accepted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the early days, content-based image retrieval (CBIR) was studied with\nglobal features. Since 2003, image retrieval based on local descriptors (de\nfacto SIFT) has been extensively studied for over a decade due to the advantage\nof SIFT in dealing with image transformations. Recently, image representations\nbased on the convolutional neural network (CNN) have attracted increasing\ninterest in the community and demonstrated impressive performance. Given this\ntime of rapid evolution, this article provides a comprehensive survey of\ninstance retrieval over the last decade. Two broad categories, SIFT-based and\nCNN-based methods, are presented. For the former, according to the codebook\nsize, we organize the literature into using large/medium-sized/small codebooks.\nFor the latter, we discuss three lines of methods, i.e., using pre-trained or\nfine-tuned CNN models, and hybrid methods. The first two perform a single-pass\nof an image to the network, while the last category employs a patch-based\nfeature extraction scheme. This survey presents milestones in modern instance\nretrieval, reviews a broad selection of previous works in different categories,\nand provides insights on the connection between SIFT and CNN-based methods.\nAfter analyzing and comparing retrieval performance of different categories on\nseveral datasets, we discuss promising directions towards generic and\nspecialized instance retrieval.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 08:50:58 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 08:10:33 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Zheng", "Liang", ""], ["Yang", "Yi", ""], ["Tian", "Qi", ""]]}, {"id": "1608.01825", "submitter": "Sara Garbarino Dr", "authors": "Delbary Fabrice and Garbarino Sara", "title": "Compartmental analysis of dynamic nuclear medicine data: regularization\n  procedure and application to physiology", "comments": null, "journal-ref": "Inverse Problems in Science and Engineering 2018", "doi": "10.1080/17415977.2018.1512603", "report-no": null, "categories": "physics.med-ph cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compartmental models based on tracer mass balance are extensively used in\nclinical and pre-clinical nuclear medicine in order to obtain quantitative\ninformation on tracer metabolism in the biological tissue. This paper is the\nsecond of a series of two that deal with the problem of tracer coefficient\nestimation via compartmental modelling in an inverse problem framework. While\nthe previous work was devoted to the discussion of identifiability issues for\n2, 3 and n-dimension compartmental systems, here we discuss the problem of\nnumerically determining the tracer coefficients by means of a general\nregularized Multivariate Gauss Newton scheme. In this paper, applications\nconcerning cerebral, hepatic and renal functions are considered, involving\nexperimental measurements on FDG-PET data on different set of murine models.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 10:15:06 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Fabrice", "Delbary", ""], ["Sara", "Garbarino", ""]]}, {"id": "1608.01866", "submitter": "Mustafa Sert", "authors": "Hilal Ergun and Mustafa Sert", "title": "Fusing Deep Convolutional Networks for Large Scale Visual Concept\n  Classification", "comments": "To appear in The Second IEEE International Conference on Multimedia\n  Big Data (IEEE BigMM 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning architectures are showing great promise in various computer\nvision domains including image classification, object detection, event\ndetection and action recognition. In this study, we investigate various aspects\nof convolutional neural networks (CNNs) from the big data perspective. We\nanalyze recent studies and different network architectures both in terms of\nrunning time and accuracy. We present extensive empirical information along\nwith best practices for big data practitioners. Using these best practices we\npropose efficient fusion mechanisms both for single and multiple network\nmodels. We present state-of-the art results on benchmark datasets while keeping\ncomputational costs at a lower level. Another contribution of our paper is that\nthese state-of-the-art results can be reached without using extensive data\naugmentation techniques.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 12:50:28 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Ergun", "Hilal", ""], ["Sert", "Mustafa", ""]]}, {"id": "1608.01896", "submitter": "St\\'ephanie Gu\\'erit", "authors": "St\\'ephanie Gu\\'erit, Adriana Gonz\\'alez, Anne Bol, John A. Lee,\n  Laurent Jacques", "title": "Blind Deconvolution of PET Images using Anatomical Priors", "comments": "iTWIST2016, August 24-26, Aalborg, Denmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images from positron emission tomography (PET) provide metabolic information\nabout the human body. They present, however, a spatial resolution that is\nlimited by physical and instrumental factors often modeled by a blurring\nfunction. Since this function is typically unknown, blind deconvolution (BD)\ntechniques are needed in order to produce a useful restored PET image. In this\nwork, we propose a general BD technique that restores a low resolution blurry\nimage using information from data acquired with a high resolution modality\n(e.g., CT-based delineation of regions with uniform activity in PET images).\nThe proposed BD method is validated on synthetic and actual phantoms.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 14:32:18 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Gu\u00e9rit", "St\u00e9phanie", ""], ["Gonz\u00e1lez", "Adriana", ""], ["Bol", "Anne", ""], ["Lee", "John A.", ""], ["Jacques", "Laurent", ""]]}, {"id": "1608.01966", "submitter": "Maciej Wielgosz", "authors": "Maciej Wielgosz and Marcin Pietro\\'n", "title": "OpenCL-accelerated object classification in video streams using Spatial\n  Pooler of Hierarchical Temporal Memory", "comments": "Submitted to Journal of Circuits, Systems, and Computers (JCSC)", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications (IJACSA), 8 (2), 2017", "doi": "10.14569/IJACSA.2017.080245", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to classify objects in video streams using a\nbrain-inspired Hierarchical Temporal Memory (HTM) algorithm. Object\nclassification is a challenging task where humans still significantly\noutperform machine learning algorithms due to their unique capabilities. We\nhave implemented a system which achieves very promising performance in terms of\nrecognition accuracy. Unfortunately, conducting more advanced experiments is\nvery computationally demanding; some of the trials run on a standard CPU may\ntake as long as several days for 960x540 video streams frames. Therefore we\nhave decided to accelerate selected parts of the system using OpenCL. In\nparticular, we seek to determine to what extent porting selected and\ncomputationally demanding parts of a core may speed up calculations.\n  The classification accuracy of the system was examined through a series of\nexperiments and the performance was given in terms of F1 score as a function of\nthe number of columns, synapses, $min\\_overlap$ and $winners\\_set\\_size$. The\nsystem achieves the highest F1 score of 0.95 and 0.91 for $min\\_overlap=4$ and\n256 synapses, respectively. We have also conduced a series of experiments with\ndifferent hardware setups and measured CPU/GPU acceleration. The best kernel\nspeed-up of 632x and 207x was reached for 256 synapses and 1024 columns.\nHowever, overall acceleration including transfer time was significantly lower\nand amounted to 6.5x and 3.2x for the same setup.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 18:25:21 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Wielgosz", "Maciej", ""], ["Pietro\u0144", "Marcin", ""]]}, {"id": "1608.01993", "submitter": "Mario Mastriani", "authors": "Mario Mastriani, Alberto E. Giraldez", "title": "Enhanced Directional Smoothing Algorithm for Edge-Preserving Smoothing\n  of Synthetic-Aperture Radar Images", "comments": "10 pages, 3 figures, 1 table. arXiv admin note: text overlap with\n  arXiv:1608.00277, arXiv:1608.00273, arXiv:1608.00270, arXiv:1608.00279,\n  arXiv:1608.00274", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic aperture radar (SAR) images are subject to prominent speckle noise,\nwhich is generally considered a purely multiplicative noise process. In theory,\nthis multiplicative noise is that the ratio of the standard deviation to the\nsignal value, the \"coefficient of variation,\" is theoretically constant at\nevery point in a SAR image. Most of the filters for speckle reduction are based\non this property. Such property is irrelevant for the new filter structure,\nwhich is based on directional smoothing (DS) theory, the enhanced directional\nsmoothing (EDS) that removes speckle noise from SAR images without blurring\nedges. We demonstrate the effectiveness of this new filtering method by\ncomparing it to established speckle noise removal techniques on SAR images.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 18:18:38 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Mastriani", "Mario", ""], ["Giraldez", "Alberto E.", ""]]}, {"id": "1608.02023", "submitter": "Jun Zhou", "authors": "Jun Zhou, Haozhou Yu, Karen Smith, Colin Wilder, Hongkai Yu, Song Wang", "title": "Identifying Designs from Incomplete, Fragmented Cultural Heritage\n  Objects by Curve-Pattern Matching", "comments": "In Journal of Electronic Imaging, 2017", "journal-ref": null, "doi": "10.1117/1.JEI.26.1.011022", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Study of cultural-heritage objects with embellished realistic and abstract\ndesigns made up of connected and intertwined curves crosscuts a number of\nrelated disciplines, including archaeology, art history, and heritage\nmanagement. However, many objects, such as pottery sherds found in the\narchaeological record, are fragmentary, making the underlying complete designs\nunknowable at the scale of the sherd fragment. The challenge to reconstruct and\nstudy complete designs is stymied because 1) most fragmentary cultural-heritage\nobjects contain only a small portion of the underlying full design, 2) in the\ncase of a stamping application, the same design may be applied multiple times\nwith spatial overlap on one object, and 3) curve patterns detected on an object\nare usually incomplete and noisy. As a result, classical curve-pattern matching\nalgorithms, such as Chamfer matching, may perform poorly in identifying the\nunderlying design. In this paper, we develop a new partial-to-global curve\nmatching algorithm to address these challenges and better identify the full\ndesign from a fragmented cultural heritage object. Specifically, we develop the\nalgorithm to identify the designs of the carved wooden paddles of the\nSoutheastern Woodlands from unearthed pottery sherds. A set of pottery sherds\nfrom the Snow Collection, curated at Georgia Southern University, are used to\ntest the proposed algorithm, with promising results.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 21:09:53 GMT"}, {"version": "v2", "created": "Thu, 5 Jan 2017 21:40:03 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Zhou", "Jun", ""], ["Yu", "Haozhou", ""], ["Smith", "Karen", ""], ["Wilder", "Colin", ""], ["Yu", "Hongkai", ""], ["Wang", "Song", ""]]}, {"id": "1608.02026", "submitter": "Hatem Alismail", "authors": "Hatem Alismail and Brett Browning and Simon Lucey", "title": "Photometric Bundle Adjustment for Vision-Based SLAM", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel algorithm for the joint refinement of structure and motion\nparameters from image data directly without relying on fixed and known\ncorrespondences. In contrast to traditional bundle adjustment (BA) where the\noptimal parameters are determined by minimizing the reprojection error using\ntracked features, the proposed algorithm relies on maximizing the photometric\nconsistency and estimates the correspondences implicitly. Since the proposed\nalgorithm does not require correspondences, its application is not limited to\ncorner-like structure; any pixel with nonvanishing gradient could be used in\nthe estimation process. Furthermore, we demonstrate the feasibility of refining\nthe motion and structure parameters simultaneously using the photometric in\nunconstrained scenes and without requiring restrictive assumptions such as\nplanarity. The proposed algorithm is evaluated on range of challenging outdoor\ndatasets, and it is shown to improve upon the accuracy of the state-of-the-art\nVSLAM methods obtained using the minimization of the reprojection error using\ntraditional BA as well as loop closure.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 21:27:11 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Alismail", "Hatem", ""], ["Browning", "Brett", ""], ["Lucey", "Simon", ""]]}, {"id": "1608.02051", "submitter": "Kanji Tanaka", "authors": "Tomoya Murase and Kanji Tanaka", "title": "Compressive Change Retrieval for Moving Object Detection", "comments": "6 pages, 6 figures, Draft of a paper submitted to an International\n  Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection, or anomaly detection, from street-view images acquired by\nan autonomous robot at multiple different times, is a major problem in robotic\nmapping and autonomous driving. Formulation as an image comparison task, which\noperates on a given pair of query and reference images is common to many\nexisting approaches to this problem. Unfortunately, providing relevant\nreference images is not straightforward. In this paper, we propose a novel\nformulation for change detection, termed compressive change retrieval, which\ncan operate on a query image and similar reference images retrieved from the\nweb. Compared to previous formulations, there are two sources of difficulty.\nFirst, the retrieved reference images may frequently contain non-relevant\nreference images, because even state-of-the-art place-recognition techniques\nsuffer from retrieval noise. Second, image comparison needs to be conducted in\na compressed domain to minimize the storage cost of large collections of\nstreet-view images. To address the above issues, we also present a practical\nchange detection algorithm that uses compressed bag-of-words (BoW) image\nrepresentation as a scalable solution. The results of experiments conducted on\na practical change detection task, \"moving object detection (MOD),\" using the\npublicly available Malaga dataset validate the effectiveness of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 02:04:25 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Murase", "Tomoya", ""], ["Tanaka", "Kanji", ""]]}, {"id": "1608.02052", "submitter": "Kanji Tanaka", "authors": "Kanji Tanaka", "title": "Multi-Model Hypothesize-and-Verify Approach for Incremental Loop Closure\n  Verification", "comments": "6 pages, 8 figures, Draft of a paper submitted to an International\n  Conference. arXiv admin note: text overlap with arXiv:1509.07611", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loop closure detection, which is the task of identifying locations revisited\nby a robot in a sequence of odometry and perceptual observations, is typically\nformulated as a visual place recognition (VPR) task. However, even\nstate-of-the-art VPR techniques generate a considerable number of false\npositives as a result of confusing visual features and perceptual aliasing. In\nthis paper, we propose a robust incremental framework for loop closure\ndetection, termed incremental loop closure verification. Our approach\nreformulates the problem of loop closure detection as an instance of a\nmulti-model hypothesize-and-verify framework, in which multiple loop closure\nhypotheses are generated and verified in terms of the consistency between loop\nclosure hypotheses and VPR constraints at multiple viewpoints along the robot's\ntrajectory. Furthermore, we consider the general incremental setting of loop\nclosure detection, in which the system must update both the set of VPR\nconstraints and that of loop closure hypotheses when new constraints or\nhypotheses arrive during robot navigation. Experimental results using a stereo\nSLAM system and DCNN features and visual odometry validate effectiveness of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 02:06:23 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Tanaka", "Kanji", ""]]}, {"id": "1608.02059", "submitter": "Joon Son Chung", "authors": "Joon Son Chung, Andrew Zisserman", "title": "Signs in time: Encoding human motion as a temporal image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this work is to recognise and localise short temporal signals in\nimage time series, where strong supervision is not available for training.\n  To this end we propose an image encoding that concisely represents human\nmotion in a video sequence in a form that is suitable for learning with a\nConvNet. The encoding reduces the pose information from an image to a single\ncolumn, dramatically diminishing the input requirements for the network, but\nretaining the essential information for recognition.\n  The encoding is applied to the task of recognizing and localizing signed\ngestures in British Sign Language (BSL) videos. We demonstrate that using the\nproposed encoding, signs as short as 10 frames duration can be learnt from\nclips lasting hundreds of frames using only weak (clip level) supervision and\nwith considerable label noise.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 03:37:28 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Chung", "Joon Son", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1608.02128", "submitter": "Michael McCoyd", "authors": "Michael McCoyd and David Wagner", "title": "Spoofing 2D Face Detection: Machines See People Who Aren't There", "comments": "9 pages, 19 figures, submitted to AISec", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is increasingly used to make sense of the physical world yet\nmay suffer from adversarial manipulation. We examine the Viola-Jones 2D face\ndetection algorithm to study whether images can be created that humans do not\nnotice as faces yet the algorithm detects as faces. We show that it is possible\nto construct images that Viola-Jones recognizes as containing faces yet no\nhuman would consider a face. Moreover, we show that it is possible to construct\nimages that fool facial detection even when they are printed and then\nphotographed.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 16:50:26 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["McCoyd", "Michael", ""], ["Wagner", "David", ""]]}, {"id": "1608.02146", "submitter": "John Lipor", "authors": "John Lipor and Laura Balzano", "title": "Leveraging Union of Subspace Structure to Improve Constrained Clustering", "comments": "11 pages, 8 figures", "journal-ref": "Proceedings of the 34th International Conference on Machine\n  Learning, in PMLR 70:2130-2139 (2017)", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many clustering problems in computer vision and other contexts are also\nclassification problems, where each cluster shares a meaningful label. Subspace\nclustering algorithms in particular are often applied to problems that fit this\ndescription, for example with face images or handwritten digits. While it is\nstraightforward to request human input on these datasets, our goal is to reduce\nthis input as much as possible. We present a pairwise-constrained clustering\nalgorithm that actively selects queries based on the union-of-subspaces model.\nThe central step of the algorithm is in querying points of minimum margin\nbetween estimated subspaces; analogous to classifier margin, these lie near the\ndecision boundary. We prove that points lying near the intersection of\nsubspaces are points with low margin. Our procedure can be used after any\nsubspace clustering algorithm that outputs an affinity matrix. We demonstrate\non several datasets that our algorithm drives the clustering error down\nconsiderably faster than the state-of-the-art active query algorithms on\ndatasets with subspace structure and is competitive on other datasets.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 19:29:58 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 21:17:33 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Lipor", "John", ""], ["Balzano", "Laura", ""]]}, {"id": "1608.02164", "submitter": "Joshua Peterson", "authors": "Joshua C. Peterson, Joshua T. Abbott, Thomas L. Griffiths", "title": "Adapting Deep Network Features to Capture Psychological Representations", "comments": "6 pages, 4 figures, To appear in the Proceedings of the 38th Annual\n  Conference of the Cognitive Science Society, Winner of the Computational\n  Modeling Prize in Perception/Action", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have become increasingly successful at solving classic\nperception problems such as object recognition, semantic segmentation, and\nscene understanding, often reaching or surpassing human-level accuracy. This\nsuccess is due in part to the ability of DNNs to learn useful representations\nof high-dimensional inputs, a problem that humans must also solve. We examine\nthe relationship between the representations learned by these networks and\nhuman psychological representations recovered from similarity judgments. We\nfind that deep features learned in service of object classification account for\na significant amount of the variance in human similarity judgments for a set of\nanimal images. However, these features do not capture some qualitative\ndistinctions that are a key part of human representations. To remedy this, we\ndevelop a method for adapting deep features to align with human similarity\njudgments, resulting in image representations that can potentially be used to\nextend the scope of psychological experiments.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 23:49:48 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Peterson", "Joshua C.", ""], ["Abbott", "Joshua T.", ""], ["Griffiths", "Thomas L.", ""]]}, {"id": "1608.02165", "submitter": "Vladislav Voroninski", "authors": "Thomas Goldstein, Paul Hand, Choongbum Lee, Vladislav Voroninski,\n  Stefano Soatto", "title": "ShapeFit and ShapeKick for Robust, Scalable Structure from Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new method for location recovery from pair-wise directions\nthat leverages an efficient convex program that comes with exact recovery\nguarantees, even in the presence of adversarial outliers. When pairwise\ndirections represent scaled relative positions between pairs of views\n(estimated for instance with epipolar geometry) our method can be used for\nlocation recovery, that is the determination of relative pose up to a single\nunknown scale. For this task, our method yields performance comparable to the\nstate-of-the-art with an order of magnitude speed-up. Our proposed numerical\nframework is flexible in that it accommodates other approaches to location\nrecovery and can be used to speed up other methods. These properties are\ndemonstrated by extensively testing against state-of-the-art methods for\nlocation recovery on 13 large, irregular collections of images of real scenes\nin addition to simulated data with ground truth.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 00:29:53 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Goldstein", "Thomas", ""], ["Hand", "Paul", ""], ["Lee", "Choongbum", ""], ["Voroninski", "Vladislav", ""], ["Soatto", "Stefano", ""]]}, {"id": "1608.02183", "submitter": "Dapeng Tao", "authors": "Yanan Guo, Lei Li, Weifeng Liu, Jun Cheng, and Dapeng Tao", "title": "Multiview Cauchy Estimator Feature Embedding for Depth and Inertial\n  Sensor-Based Human Action Recognition", "comments": "This paper has been withdrawn by the author due to a crucial error", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ever-growing popularity of Kinect and inertial sensors has prompted\nintensive research efforts on human action recognition. Since human actions can\nbe characterized by multiple feature representations extracted from Kinect and\ninertial sensors, multiview features must be encoded into a unified space\noptimal for human action recognition. In this paper, we propose a new\nunsupervised feature fusion method termed Multiview Cauchy Estimator Feature\nEmbedding (MCEFE) for human action recognition. By minimizing empirical risk,\nMCEFE integrates the encoded complementary information in multiple views to\nfind the unified data representation and the projection matrices. To enhance\nrobustness to outliers, the Cauchy estimator is imposed on the reconstruction\nerror. Furthermore, ensemble manifold regularization is enforced on the\nprojection matrices to encode the correlations between different views and\navoid overfitting. Experiments are conducted on the new Chinese Academy of\nSciences - Yunnan University - Multimodal Human Action Database (CAS-YNU-MHAD)\nto demonstrate the effectiveness and robustness of MCEFE for human action\nrecognition.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 05:50:10 GMT"}, {"version": "v2", "created": "Sun, 4 Sep 2016 08:37:23 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Guo", "Yanan", ""], ["Li", "Lei", ""], ["Liu", "Weifeng", ""], ["Cheng", "Jun", ""], ["Tao", "Dapeng", ""]]}, {"id": "1608.02192", "submitter": "Stephan R Richter", "authors": "Stephan R. Richter, Vibhav Vineet, Stefan Roth, Vladlen Koltun", "title": "Playing for Data: Ground Truth from Computer Games", "comments": "Accepted to the 14th European Conference on Computer Vision (ECCV\n  2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in computer vision has been driven by high-capacity models\ntrained on large datasets. Unfortunately, creating large datasets with\npixel-level labels has been extremely costly due to the amount of human effort\nrequired. In this paper, we present an approach to rapidly creating\npixel-accurate semantic label maps for images extracted from modern computer\ngames. Although the source code and the internal operation of commercial games\nare inaccessible, we show that associations between image patches can be\nreconstructed from the communication between the game and the graphics\nhardware. This enables rapid propagation of semantic labels within and across\nimages synthesized by the game, with no access to the source code or the\ncontent. We validate the presented approach by producing dense pixel-level\nsemantic annotations for 25 thousand images synthesized by a photorealistic\nopen-world computer game. Experiments on semantic segmentation datasets show\nthat using the acquired data to supplement real-world images significantly\nincreases accuracy and that the acquired data enables reducing the amount of\nhand-labeled real-world data: models trained with game data and just 1/3 of the\nCamVid training set outperform models trained on the complete CamVid training\nset.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 08:20:14 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Richter", "Stephan R.", ""], ["Vineet", "Vibhav", ""], ["Roth", "Stefan", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1608.02201", "submitter": "Hussein Al-Barazanchi", "authors": "Hussein A. Al-Barazanchi, Hussam Qassim, Abhishek Verma", "title": "Residual CNDS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional Neural networks nowadays are of tremendous importance for any\nimage classification system. One of the most investigated methods to increase\nthe accuracy of CNN is by increasing the depth of CNN. Increasing the depth by\nstacking more layers also increases the difficulty of training besides making\nit computationally expensive. Some research found that adding auxiliary forks\nafter intermediate layers increases the accuracy. Specifying which intermediate\nlayer shoud have the fork just addressed recently. Where a simple rule were\nused to detect the position of intermediate layers that needs the auxiliary\nsupervision fork. This technique known as convolutional neural networks with\ndeep supervision (CNDS). This technique enhanced the accuracy of classification\nover the straight forward CNN used on the MIT places dataset and ImageNet. In\nthe other side, Residual Learning is another technique emerged recently to ease\nthe training of very deep CNN. Residual Learning framwork changed the learning\nof layers from unreferenced functions to learning residual function with regard\nto the layer's input. Residual Learning achieved state of arts results on\nImageNet 2015 and COCO competitions. In this paper, we study the effect of\nadding residual connections to CNDS network. Our experiments results show\nincreasing of accuracy over using CNDS only.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 10:34:02 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Al-Barazanchi", "Hussein A.", ""], ["Qassim", "Hussam", ""], ["Verma", "Abhishek", ""]]}, {"id": "1608.02236", "submitter": "Shaohua Wan", "authors": "Shaohua Wan, Zhijun Chen, Tao Zhang, Bo Zhang, Kong-kat Wong", "title": "Bootstrapping Face Detection with Hard Negative Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently significant performance improvement in face detection was made\npossible by deeply trained convolutional networks. In this report, a novel\napproach for training state-of-the-art face detector is described. The key is\nto exploit the idea of hard negative mining and iteratively update the Faster\nR-CNN based face detector with the hard negatives harvested from a large set of\nbackground examples. We demonstrate that our face detector outperforms\nstate-of-the-art detectors on the FDDB dataset, which is the de facto standard\nfor evaluating face detection algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 16:10:50 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Wan", "Shaohua", ""], ["Chen", "Zhijun", ""], ["Zhang", "Tao", ""], ["Zhang", "Bo", ""], ["Wong", "Kong-kat", ""]]}, {"id": "1608.02239", "submitter": "Edward Johns", "authors": "Edward Johns, Stefan Leutenegger and Andrew J. Davison", "title": "Deep Learning a Grasp Function for Grasping under Gripper Pose\n  Uncertainty", "comments": "IROS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method for parallel-jaw grasping of isolated\nobjects from depth images, under large gripper pose uncertainty. Whilst most\napproaches aim to predict the single best grasp pose from an image, our method\nfirst predicts a score for every possible grasp pose, which we denote the grasp\nfunction. With this, it is possible to achieve grasping robust to the gripper's\npose uncertainty, by smoothing the grasp function with the pose uncertainty\nfunction. Therefore, if the single best pose is adjacent to a region of poor\ngrasp quality, that pose will no longer be chosen, and instead a pose will be\nchosen which is surrounded by a region of high grasp quality. To learn this\nfunction, we train a Convolutional Neural Network which takes as input a single\ndepth image of an object, and outputs a score for each grasp pose across the\nimage. Training data for this is generated by use of physics simulation and\ndepth image simulation with 3D object meshes, to enable acquisition of\nsufficient data without requiring exhaustive real-world experiments. We\nevaluate with both synthetic and real experiments, and show that the learned\ngrasp score is more robust to gripper pose uncertainty than when this\nuncertainty is not accounted for.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 16:30:42 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Johns", "Edward", ""], ["Leutenegger", "Stefan", ""], ["Davison", "Andrew J.", ""]]}, {"id": "1608.02255", "submitter": "Xiaohua Huang", "authors": "Xiaohua Huang, Sujing Wang, Xin Liu, Guoying Zhao, Xiaoyi Feng, Matti\n  Pietikainen", "title": "Spontaneous Facial Micro-Expression Recognition using Discriminative\n  Spatiotemporal Local Binary Pattern with an Improved Integral Projection", "comments": "13pages, 8 figures, 5 tables, submitted to IEEE Transactions on Image\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there are increasing interests in inferring mirco-expression from\nfacial image sequences. Due to subtle facial movement of micro-expressions,\nfeature extraction has become an important and critical issue for spontaneous\nfacial micro-expression recognition. Recent works usually used spatiotemporal\nlocal binary pattern for micro-expression analysis. However, the commonly used\nspatiotemporal local binary pattern considers dynamic texture information to\nrepresent face images while misses the shape attribute of face images. On the\nother hand, their works extracted the spatiotemporal features from the global\nface regions, which ignore the discriminative information between two\nmicro-expression classes. The above-mentioned problems seriously limit the\napplication of spatiotemporal local binary pattern on micro-expression\nrecognition. In this paper, we propose a discriminative spatiotemporal local\nbinary pattern based on an improved integral projection to resolve the problems\nof spatiotemporal local binary pattern for micro-expression recognition.\nFirstly, we develop an improved integral projection for preserving the shape\nattribute of micro-expressions. Furthermore, an improved integral projection is\nincorporated with local binary pattern operators across spatial and temporal\ndomains. Specifically, we extract the novel spatiotemporal features\nincorporating shape attributes into spatiotemporal texture features. For\nincreasing the discrimination of micro-expressions, we propose a new feature\nselection based on Laplacian method to extract the discriminative information\nfor facial micro-expression recognition. Intensive experiments are conducted on\nthree availably published micro-expression databases. We compare our method\nwith the state-of-the-art algorithms. Experimental results demonstrate that our\nproposed method achieves promising performance for micro-expression\nrecognition.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 18:17:17 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Huang", "Xiaohua", ""], ["Wang", "Sujing", ""], ["Liu", "Xin", ""], ["Zhao", "Guoying", ""], ["Feng", "Xiaoyi", ""], ["Pietikainen", "Matti", ""]]}, {"id": "1608.02289", "submitter": "Rossano Schifanella", "authors": "Rossano Schifanella, Paloma de Juan, Joel Tetreault, Liangliang Cao", "title": "Detecting Sarcasm in Multimodal Social Platforms", "comments": "10 pages, 3 figures, final version published in the Proceedings of\n  ACM Multimedia 2016", "journal-ref": null, "doi": "10.1145/2964284.2964321", "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sarcasm is a peculiar form of sentiment expression, where the surface\nsentiment differs from the implied sentiment. The detection of sarcasm in\nsocial media platforms has been applied in the past mainly to textual\nutterances where lexical indicators (such as interjections and intensifiers),\nlinguistic markers, and contextual information (such as user profiles, or past\nconversations) were used to detect the sarcastic tone. However, modern social\nmedia platforms allow to create multimodal messages where audiovisual content\nis integrated with the text, making the analysis of a mode in isolation\npartial. In our work, we first study the relationship between the textual and\nvisual aspects in multimodal posts from three major social media platforms,\ni.e., Instagram, Tumblr and Twitter, and we run a crowdsourcing task to\nquantify the extent to which images are perceived as necessary by human\nannotators. Moreover, we propose two different computational frameworks to\ndetect sarcasm that integrate the textual and visual modalities. The first\napproach exploits visual semantics trained on an external dataset, and\nconcatenates the semantics features with state-of-the-art textual features. The\nsecond method adapts a visual neural network initialized with parameters\ntrained on ImageNet to multimodal sarcastic posts. Results show the positive\neffect of combining modalities for the detection of sarcasm across platforms\nand methods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 00:59:03 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Schifanella", "Rossano", ""], ["de Juan", "Paloma", ""], ["Tetreault", "Joel", ""], ["Cao", "Liangliang", ""]]}, {"id": "1608.02307", "submitter": "William Gray Roncal", "authors": "William Gray Roncal, Colin Lea, Akira Baruah, Gregory D. Hager", "title": "SANTIAGO: Spine Association for Neuron Topology Improvement and Graph\n  Optimization", "comments": "13 pp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing automated and semi-automated solutions for reconstructing wiring\ndiagrams of the brain from electron micrographs is important for advancing the\nfield of connectomics. While the ultimate goal is to generate a graph of neuron\nconnectivity, most prior automated methods have focused on volume segmentation\nrather than explicit graph estimation. In these approaches, one of the key,\ncommonly occurring error modes is dendritic shaft-spine fragmentation.\n  We posit that directly addressing this problem of connection identification\nmay provide critical insight into estimating more accurate brain graphs. To\nthis end, we develop a network-centric approach motivated by biological priors\nimage grammars. We build a computer vision pipeline to reconnect fragmented\nspines to their parent dendrites using both fully-automated and semi-automated\napproaches. Our experiments show we can learn valid connections despite\nuncertain segmentation paths. We curate the first known reference dataset for\nanalyzing the performance of various spine-shaft algorithms and demonstrate\npromising results that recover many previously lost connections. Our automated\napproach improves the local subgraph score by more than four times and the full\ngraph score by 60 percent. These data, results, and evaluation tools are all\navailable to the broader scientific community. This reframing of the\nconnectomics problem illustrates a semantic, biologically inspired solution to\nremedy a major problem with neuron tracking.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 03:37:29 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Roncal", "William Gray", ""], ["Lea", "Colin", ""], ["Baruah", "Akira", ""], ["Hager", "Gregory D.", ""]]}, {"id": "1608.02318", "submitter": "Karan Sikka", "authors": "Karan Sikka and Gaurav Sharma", "title": "Discriminatively Trained Latent Ordinal Model for Video Classification", "comments": "Paper accepted in IEEE TPAMI. arXiv admin note: substantial text\n  overlap with arXiv:1604.01500", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of video classification for facial analysis and human\naction recognition. We propose a novel weakly supervised learning method that\nmodels the video as a sequence of automatically mined, discriminative\nsub-events (eg. onset and offset phase for \"smile\", running and jumping for\n\"highjump\"). The proposed model is inspired by the recent works on Multiple\nInstance Learning and latent SVM/HCRF -- it extends such frameworks to model\nthe ordinal aspect in the videos, approximately. We obtain consistent\nimprovements over relevant competitive baselines on four challenging and\npublicly available video based facial analysis datasets for prediction of\nexpression, clinical pain and intent in dyadic conversations and on three\nchallenging human action datasets. We also validate the method with qualitative\nresults and show that they largely support the intuitions behind the method.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 05:15:54 GMT"}, {"version": "v2", "created": "Tue, 15 Aug 2017 03:25:31 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Sikka", "Karan", ""], ["Sharma", "Gaurav", ""]]}, {"id": "1608.02367", "submitter": "Mayu Otani", "authors": "Mayu Otani, Yuta Nakashima, Esa Rahtu, Janne Heikkil\\\"a, Naokazu\n  Yokoya", "title": "Learning Joint Representations of Videos and Sentences with Web Image\n  Search", "comments": "16 pages, 4th Workshop on Web-scale Vision and Social Media (VSM),\n  ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our objective is video retrieval based on natural language queries. In\naddition, we consider the analogous problem of retrieving sentences or\ngenerating descriptions given an input video. Recent work has addressed the\nproblem by embedding visual and textual inputs into a common space where\nsemantic similarities correlate to distances. We also adopt the embedding\napproach, and make the following contributions: First, we utilize web image\nsearch in sentence embedding process to disambiguate fine-grained visual\nconcepts. Second, we propose embedding models for sentence, image, and video\ninputs whose parameters are learned simultaneously. Finally, we show how the\nproposed model can be applied to description generation. Overall, we observe a\nclear improvement over the state-of-the-art methods in the video and sentence\nretrieval tasks. In description generation, the performance level is comparable\nto the current state-of-the-art, although our embeddings were trained for the\nretrieval tasks.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 09:54:15 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Otani", "Mayu", ""], ["Nakashima", "Yuta", ""], ["Rahtu", "Esa", ""], ["Heikkil\u00e4", "Janne", ""], ["Yokoya", "Naokazu", ""]]}, {"id": "1608.02373", "submitter": "Mohamed Ali Mahjoub", "authors": "Mahaman Sani Chaibou, Karim Kalti, Bassel Soulaiman, Mohamed Ali\n  Mahjoub", "title": "A combined Approach Based on Fuzzy Classification and Contextual Region\n  Growing to Image Segmentation", "comments": "CGIV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper an image segmentation approach that combines a fuzzy\nsemantic region classification and a context based region-growing. Input image\nis first over-segmented. Then, prior domain knowledge is used to perform a\nfuzzy classification of these regions to provide a fuzzy semantic labeling.\nThis allows the proposed approach to operate at high level instead of using\nlow-level features and consequently to remedy to the problem of the semantic\ngap. Each over-segmented region is represented by a vector giving its\ncorresponding membership degrees to the different thematic labels and the whole\nimage is therefore represented by a Regions Partition Matrix. The segmentation\nis achieved on this matrix instead of the image pixels through two main phases:\nfocusing and propagation. The focusing aims at selecting seeds regions from\nwhich information propagation will be performed. Thepropagation phase allows to\nspread toward others regions and using fuzzy contextual information the needed\nknowledge ensuring the semantic segmentation. An application of the proposed\napproach on mammograms shows promising results\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 10:26:53 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Chaibou", "Mahaman Sani", ""], ["Kalti", "Karim", ""], ["Soulaiman", "Bassel", ""], ["Mahjoub", "Mohamed Ali", ""]]}, {"id": "1608.02385", "submitter": "Mohamed Ali Mahjoub", "authors": "Mabrouka Hagui, Mohamed Ali Mahjoub, Ahmed Boukhris", "title": "Comparative study and enhancement of Camera Tampering Detection\n  algorithms", "comments": "CGIV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently the use of video surveillance systems is widely increasing.\nDifferent places are equipped by camera surveillances such as hospitals,\nschools, airports, museums and military places in order to ensure the safety\nand security of the persons and their property. Therefore it becomes\nsignificant to guarantee the proper working of these systems. Intelligent video\nsurveillance systems equipped by sophisticated digital camera can analyze video\ninformation s and automatically detect doubtful actions. The camera tampering\ndetection algorithms may indicate that accidental or suspicious activities have\noccurred and that causes the abnormality works of the video surveillance.\n  Camera Tampering Detection uses several techniques based on image processing\nand computer vision. In this paper, comparative study of performance of three\nalgorithms that can detect abnormal disturbance for video surveillance is\npresented.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 11:22:23 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Hagui", "Mabrouka", ""], ["Mahjoub", "Mohamed Ali", ""], ["Boukhris", "Ahmed", ""]]}, {"id": "1608.02388", "submitter": "Mohamed Ali Mahjoub", "authors": "Ibtissem Hadj Ali, Mohammed Ali Mahjoub", "title": "Database of handwritten Arabic mathematical formulas images", "comments": "CGIV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although publicly available, ground-truthed database have proven useful for\ntraining, evaluating, and comparing recognition systems in many domains, the\navailability of such database for handwritten Arabic mathematical formula\nrecognition in particular, is currently quite poor. In this paper, we present a\nnew public database that contains mathematical expressions available in their\noff-line handwritten form. Here, we describe the different steps that allowed\nus to acquire this database, from the creation of the mathematical expression\ncorpora to the transcription of the collected data. Currently, the dataset\ncontains 4 238 off-line handwritten mathematical expressions written by 66\nwriters and 20 300 handwritten isolated symbol images. The ground truth is also\nprovided for the handwritten expressions as XML files with the number of\nsymbols, and the MATHML structure.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 11:30:35 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Ali", "Ibtissem Hadj", ""], ["Mahjoub", "Mohammed Ali", ""]]}, {"id": "1608.02676", "submitter": "Krishna Kumar Singh", "authors": "Krishna Kumar Singh and Yong Jae Lee", "title": "End-to-End Localization and Ranking for Relative Attributes", "comments": "Appears in European Conference on Computer Vision (ECCV), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end deep convolutional network to simultaneously\nlocalize and rank relative visual attributes, given only weakly-supervised\npairwise image comparisons. Unlike previous methods, our network jointly learns\nthe attribute's features, localization, and ranker. The localization module of\nour network discovers the most informative image region for the attribute,\nwhich is then used by the ranking module to learn a ranking model of the\nattribute. Our end-to-end framework also significantly speeds up processing and\nis much faster than previous methods. We show state-of-the-art ranking results\non various relative attribute datasets, and our qualitative localization\nresults clearly demonstrate our network's ability to learn meaningful image\npatches.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 02:19:37 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Singh", "Krishna Kumar", ""], ["Lee", "Yong Jae", ""]]}, {"id": "1608.02680", "submitter": "Roberto Tron", "authors": "Roberto Tron", "title": "A Factorization Approach to Inertial Affine Structure from Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of reconstructing a 3-D scene from a moving camera\nwith high frame rate using the affine projection model. This problem is\ntraditionally known as Affine Structure from Motion (Affine SfM), and can be\nsolved using an elegant low-rank factorization formulation. In this paper, we\nassume that an accelerometer and gyro are rigidly mounted with the camera, so\nthat synchronized linear acceleration and angular velocity measurements are\navailable together with the image measurements. We extend the standard Affine\nSfM algorithm to integrate these measurements through the use of image\nderivatives.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 02:55:41 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Tron", "Roberto", ""]]}, {"id": "1608.02693", "submitter": "Mehul Bhatt", "authors": "Jakob Suchan and Mehul Bhatt and Carl Schultz", "title": "Deeply Semantic Inductive Spatio-Temporal Learning", "comments": "Accepted for publication at ILP 2016: 26th International Conference\n  on Inductive Logic Programming 4th - 6th September 2016, London. Keywords:\n  Spatio-Temporal Learning; Dynamic Visuo-Spatial Imagery; Declarative Spatial\n  Reasoning; Inductive Logic Programming; AI and Art", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an inductive spatio-temporal learning framework rooted in\ninductive logic programming. With an emphasis on visuo-spatial language, logic,\nand cognition, the framework supports learning with relational spatio-temporal\nfeatures identifiable in a range of domains involving the processing and\ninterpretation of dynamic visuo-spatial imagery. We present a prototypical\nsystem, and an example application in the domain of computing for visual arts\nand computational cognitive science.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 05:48:51 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Suchan", "Jakob", ""], ["Bhatt", "Mehul", ""], ["Schultz", "Carl", ""]]}, {"id": "1608.02702", "submitter": "Boris Landa", "authors": "Boris Landa and Yoel Shkolnisky", "title": "Steerable Principal Components for Space-Frequency Localized Images", "comments": null, "journal-ref": null, "doi": "10.1137/16M1085334", "report-no": null, "categories": "cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a fast and accurate method for obtaining steerable\nprincipal components from a large dataset of images, assuming the images are\nwell localized in space and frequency. The obtained steerable principal\ncomponents are optimal for expanding the images in the dataset and all of their\nrotations. The method relies upon first expanding the images using a series of\ntwo-dimensional Prolate Spheroidal Wave Functions (PSWFs), where the expansion\ncoefficients are evaluated using a specially designed numerical integration\nscheme. Then, the expansion coefficients are used to construct a\nrotationally-invariant covariance matrix which admits a block-diagonal\nstructure, and the eigen-decomposition of its blocks provides us with the\ndesired steerable principal components. The proposed method is shown to be\nfaster then existing methods, while providing appropriate error bounds which\nguarantee its accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 06:35:09 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 06:12:48 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Landa", "Boris", ""], ["Shkolnisky", "Yoel", ""]]}, {"id": "1608.02717", "submitter": "Mateusz Malinowski", "authors": "Ashkan Mokarian and Mateusz Malinowski and Mario Fritz", "title": "Mean Box Pooling: A Rich Image Representation and Output Embedding for\n  the Visual Madlibs Task", "comments": "Accepted to BMVC'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Mean Box Pooling, a novel visual representation that pools over\nCNN representations of a large number, highly overlapping object proposals. We\nshow that such representation together with nCCA, a successful multimodal\nembedding technique, achieves state-of-the-art performance on the Visual\nMadlibs task. Moreover, inspired by the nCCA's objective function, we extend\nclassical CNN+LSTM approach to train the network by directly maximizing the\nsimilarity between the internal representation of the deep learning\narchitecture and candidate answers. Again, such approach achieves a significant\nimprovement over the prior work that also uses CNN+LSTM approach on Visual\nMadlibs.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 08:24:02 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Mokarian", "Ashkan", ""], ["Malinowski", "Mateusz", ""], ["Fritz", "Mario", ""]]}, {"id": "1608.02728", "submitter": "Martin Simonovsky", "authors": "Martin Simonovsky and Nikos Komodakis", "title": "OnionNet: Sharing Features in Cascaded Deep Classifiers", "comments": "Accepted to BMVC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of our work is speeding up evaluation of deep neural networks in\nretrieval scenarios, where conventional architectures may spend too much time\non negative examples. We propose to replace a monolithic network with our novel\ncascade of feature-sharing deep classifiers, called OnionNet, where subsequent\nstages may add both new layers as well as new feature channels to the previous\nones. Importantly, intermediate feature maps are shared among classifiers,\npreventing them from the necessity of being recomputed. To accomplish this, the\nmodel is trained end-to-end in a principled way under a joint loss. We validate\nour approach in theory and on a synthetic benchmark. As a result demonstrated\nin three applications (patch matching, object detection, and image retrieval),\nour cascade can operate significantly faster than both monolithic networks and\ntraditional cascades without sharing at the cost of marginal decrease in\nprecision.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 08:59:47 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Simonovsky", "Martin", ""], ["Komodakis", "Nikos", ""]]}, {"id": "1608.02755", "submitter": "Jordi Pont-Tuset", "authors": "Kevis-Kokitsi Maninis and Jordi Pont-Tuset and Pablo Arbel\\'aez and\n  Luc Van Gool", "title": "Convolutional Oriented Boundaries", "comments": "ECCV 2016 Camera Ready", "journal-ref": null, "doi": "10.1007/978-3-319-46448-0_35", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Convolutional Oriented Boundaries (COB), which produces multiscale\noriented contours and region hierarchies starting from generic image\nclassification Convolutional Neural Networks (CNNs). COB is computationally\nefficient, because it requires a single CNN forward pass for contour detection\nand it uses a novel sparse boundary representation for hierarchical\nsegmentation; it gives a significant leap in performance over the\nstate-of-the-art, and it generalizes very well to unseen categories and\ndatasets. Particularly, we show that learning to estimate not only contour\nstrength but also orientation provides more accurate results. We perform\nextensive experiments on BSDS, PASCAL Context, PASCAL Segmentation, and\nMS-COCO, showing that COB provides state-of-the-art contours, region\nhierarchies, and object proposals in all datasets.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 10:37:52 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Maninis", "Kevis-Kokitsi", ""], ["Pont-Tuset", "Jordi", ""], ["Arbel\u00e1ez", "Pablo", ""], ["Van Gool", "Luc", ""]]}, {"id": "1608.02778", "submitter": "Ke Yu", "authors": "Ke Yu, Chao Dong, Chen Change Loy, Xiaoou Tang", "title": "Deep Convolution Networks for Compression Artifacts Reduction", "comments": "13 pages, 19 figures, an extension of our ICCV 2015 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lossy compression introduces complex compression artifacts, particularly\nblocking artifacts, ringing effects and blurring. Existing algorithms either\nfocus on removing blocking artifacts and produce blurred output, or restore\nsharpened images that are accompanied with ringing effects. Inspired by the\nsuccess of deep convolutional networks (DCN) on superresolution, we formulate a\ncompact and efficient network for seamless attenuation of different compression\nartifacts. To meet the speed requirement of real-world applications, we further\naccelerate the proposed baseline model by layer decomposition and joint use of\nlarge-stride convolutional and deconvolutional layers. This also leads to a\nmore general CNN framework that has a close relationship with the conventional\nMulti-Layer Perceptron (MLP). Finally, the modified network achieves a speed up\nof 7.5 times with almost no performance loss compared to the baseline model. We\nalso demonstrate that a deeper model can be effectively trained with features\nlearned in a shallow network. Following a similar \"easy to hard\" idea, we\nsystematically investigate three practical transfer settings and show the\neffectiveness of transfer learning in low-level vision problems. Our method\nshows superior performance than the state-of-the-art methods both on benchmark\ndatasets and a real-world use case.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 12:11:51 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Yu", "Ke", ""], ["Dong", "Chao", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1608.02824", "submitter": "Bronislav P\\v{r}ibyl", "authors": "Bronislav P\\v{r}ibyl, Pavel Zem\\v{c}\\'ik, Martin \\v{C}ad\\'ik", "title": "Camera Pose Estimation from Lines using Pl\\\"ucker Coordinates", "comments": "12 pages, 5 figures, In Proceedings of the British Machine Vision\n  Conference (BMVC 2015), pages 45.1-45.12. BMVA Press, September 2015", "journal-ref": null, "doi": "10.5244/C.29.45", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correspondences between 3D lines and their 2D images captured by a camera are\noften used to determine position and orientation of the camera in space. In\nthis work, we propose a novel algebraic algorithm to estimate the camera pose.\nWe parameterize 3D lines using Pl\\\"ucker coordinates that allow linear\nprojection of the lines into the image. A line projection matrix is estimated\nusing Linear Least Squares and the camera pose is then extracted from the\nmatrix. An algebraic approach to handle mismatched line correspondences is also\nincluded. The proposed algorithm is an order of magnitude faster yet comparably\naccurate and robust to the state-of-the-art, it does not require\ninitialization, and it yields only one solution. The described method requires\nat least 9 lines and is particularly suitable for scenarios with 25 and more\nlines, as also shown in the results.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 14:58:34 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["P\u0159ibyl", "Bronislav", ""], ["Zem\u010d\u00edk", "Pavel", ""], ["\u010cad\u00edk", "Martin", ""]]}, {"id": "1608.02833", "submitter": "Mundher Al-Shabi", "authors": "Mundher Al-Shabi, Wooi Ping Cheah, Tee Connie", "title": "Facial Expression Recognition Using a Hybrid CNN-SIFT Aggregator", "comments": "To be appear in LNAI", "journal-ref": null, "doi": "10.1007/978-3-319-69456-6_12", "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deriving an effective facial expression recognition component is important\nfor a successful human-computer interaction system. Nonetheless, recognizing\nfacial expression remains a challenging task. This paper describes a novel\napproach towards facial expression recognition task. The proposed method is\nmotivated by the success of Convolutional Neural Networks (CNN) on the face\nrecognition problem. Unlike other works, we focus on achieving good accuracy\nwhile requiring only a small sample data for training. Scale Invariant Feature\nTransform (SIFT) features are used to increase the performance on small data as\nSIFT does not require extensive training data to generate useful features. In\nthis paper, both Dense SIFT and regular SIFT are studied and compared when\nmerged with CNN features. Moreover, an aggregator of the models is developed.\nThe proposed approach is tested on the FER-2013 and CK+ datasets. Results\ndemonstrate the superiority of CNN with Dense SIFT over conventional CNN and\nCNN with SIFT. The accuracy even increased when all the models are aggregated\nwhich generates state-of-art results on FER-2013 and CK+ datasets, where it\nachieved 73.4% on FER-2013 and 99.1% on CK+.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 15:21:33 GMT"}, {"version": "v2", "created": "Wed, 10 Aug 2016 07:44:59 GMT"}, {"version": "v3", "created": "Sat, 3 Dec 2016 11:21:39 GMT"}, {"version": "v4", "created": "Thu, 22 Dec 2016 09:07:30 GMT"}, {"version": "v5", "created": "Sat, 12 Aug 2017 02:58:13 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Al-Shabi", "Mundher", ""], ["Cheah", "Wooi Ping", ""], ["Connie", "Tee", ""]]}, {"id": "1608.02908", "submitter": "Miao Sun", "authors": "Ke Zhang, Miao Sun, Tony X. Han, Xingfang Yuan, Liru Guo, and Tao Liu", "title": "Residual Networks of Residual Networks: Multilevel Residual Networks", "comments": "IEEE Transactions on Circuits and Systems for Video Technology 2017", "journal-ref": null, "doi": "10.1109/TCSVT.2017.2654543", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A residual-networks family with hundreds or even thousands of layers\ndominates major image recognition tasks, but building a network by simply\nstacking residual blocks inevitably limits its optimization ability. This paper\nproposes a novel residual-network architecture, Residual networks of Residual\nnetworks (RoR), to dig the optimization ability of residual networks. RoR\nsubstitutes optimizing residual mapping of residual mapping for optimizing\noriginal residual mapping. In particular, RoR adds level-wise shortcut\nconnections upon original residual networks to promote the learning capability\nof residual networks. More importantly, RoR can be applied to various kinds of\nresidual networks (ResNets, Pre-ResNets and WRN) and significantly boost their\nperformance. Our experiments demonstrate the effectiveness and versatility of\nRoR, where it achieves the best performance in all residual-network-like\nstructures. Our RoR-3-WRN58-4+SD models achieve new state-of-the-art results on\nCIFAR-10, CIFAR-100 and SVHN, with test errors 3.77%, 19.73% and 1.59%,\nrespectively. RoR-3 models also achieve state-of-the-art results compared to\nResNets on ImageNet data set.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 18:37:26 GMT"}, {"version": "v2", "created": "Sun, 5 Mar 2017 04:47:53 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Zhang", "Ke", ""], ["Sun", "Miao", ""], ["Han", "Tony X.", ""], ["Yuan", "Xingfang", ""], ["Guo", "Liru", ""], ["Liu", "Tao", ""]]}, {"id": "1608.02989", "submitter": "John Quinn", "authors": "John A. Quinn, Rose Nakasi, Pius K. B. Mugagga, Patrick Byanyima,\n  William Lubega, Alfred Andama", "title": "Deep Convolutional Neural Networks for Microscopy-Based Point of Care\n  Diagnostics", "comments": "Presented at 2016 Machine Learning and Healthcare Conference (MLHC\n  2016), Los Angeles, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point of care diagnostics using microscopy and computer vision methods have\nbeen applied to a number of practical problems, and are particularly relevant\nto low-income, high disease burden areas. However, this is subject to the\nlimitations in sensitivity and specificity of the computer vision methods used.\nIn general, deep learning has recently revolutionised the field of computer\nvision, in some cases surpassing human performance for other object recognition\ntasks. In this paper, we evaluate the performance of deep convolutional neural\nnetworks on three different microscopy tasks: diagnosis of malaria in thick\nblood smears, tuberculosis in sputum samples, and intestinal parasite eggs in\nstool samples. In all cases accuracy is very high and substantially better than\nan alternative approach more representative of traditional medical imaging\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 22:04:46 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Quinn", "John A.", ""], ["Nakasi", "Rose", ""], ["Mugagga", "Pius K. B.", ""], ["Byanyima", "Patrick", ""], ["Lubega", "William", ""], ["Andama", "Alfred", ""]]}, {"id": "1608.03049", "submitter": "Ziwei Liu", "authors": "Ziwei Liu, Sijie Yan, Ping Luo, Xiaogang Wang, Xiaoou Tang", "title": "Fashion Landmark Detection in the Wild", "comments": "To appear in European Conference on Computer Vision (ECCV) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual fashion analysis has attracted many attentions in the recent years.\nPrevious work represented clothing regions by either bounding boxes or human\njoints. This work presents fashion landmark detection or fashion alignment,\nwhich is to predict the positions of functional key points defined on the\nfashion items, such as the corners of neckline, hemline, and cuff. To encourage\nfuture studies, we introduce a fashion landmark dataset with over 120K images,\nwhere each image is labeled with eight landmarks. With this dataset, we study\nfashion alignment by cascading multiple convolutional neural networks in three\nstages. These stages gradually improve the accuracies of landmark predictions.\nExtensive experiments demonstrate the effectiveness of the proposed method, as\nwell as its generalization ability to pose estimation. Fashion landmark is also\ncompared to clothing bounding boxes and human joints in two applications,\nfashion attribute prediction and clothes retrieval, showing that fashion\nlandmark is a more discriminative representation to understand fashion images.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 05:07:10 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Liu", "Ziwei", ""], ["Yan", "Sijie", ""], ["Luo", "Ping", ""], ["Wang", "Xiaogang", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1608.03066", "submitter": "Benjamin Drayer", "authors": "Benjamin Drayer and Thomas Brox", "title": "Object Detection, Tracking, and Motion Segmentation for Object-level\n  Video Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for object segmentation in videos that combines\nframe-level object detection with concepts from object tracking and motion\nsegmentation. The approach extracts temporally consistent object tubes based on\nan off-the-shelf detector. Besides the class label for each tube, this provides\na location prior that is independent of motion. For the final video\nsegmentation, we combine this information with motion cues. The method\novercomes the typical problems of weakly supervised/unsupervised video\nsegmentation, such as scenes with no motion, dominant camera motion, and\nobjects that move as a unit. In contrast to most tracking methods, it provides\nan accurate, temporally consistent segmentation of each object. We report\nresults on four video segmentation datasets: YouTube Objects, SegTrackv2,\negoMotion, and FBMS.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 07:46:56 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Drayer", "Benjamin", ""], ["Brox", "Thomas", ""]]}, {"id": "1608.03075", "submitter": "Sungheon Park", "authors": "Sungheon Park, Jihye Hwang, Nojun Kwak", "title": "3D Human Pose Estimation Using Convolutional Neural Networks with 2D\n  Pose Information", "comments": "ECCV 2016 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While there has been a success in 2D human pose estimation with convolutional\nneural networks (CNNs), 3D human pose estimation has not been thoroughly\nstudied. In this paper, we tackle the 3D human pose estimation task with\nend-to-end learning using CNNs. Relative 3D positions between one joint and the\nother joints are learned via CNNs. The proposed method improves the performance\nof CNN with two novel ideas. First, we added 2D pose information to estimate a\n3D pose from an image by concatenating 2D pose estimation result with the\nfeatures from an image. Second, we have found that more accurate 3D poses are\nobtained by combining information on relative positions with respect to\nmultiple joints, instead of just one root joint. Experimental results show that\nthe proposed method achieves comparable performance to the state-of-the-art\nmethods on Human 3.6m dataset.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 08:18:30 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2016 02:25:08 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Park", "Sungheon", ""], ["Hwang", "Jihye", ""], ["Kwak", "Nojun", ""]]}, {"id": "1608.03217", "submitter": "Ali Diba", "authors": "Ali Diba, Ali Mohammad Pazandeh, Hamed Pirsiavash, Luc Van Gool", "title": "DeepCAMP: Deep Convolutional Action & Attribute Mid-Level Patterns", "comments": "in CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recognition of human actions and the determination of human attributes\nare two tasks that call for fine-grained classification. Indeed, often rather\nsmall and inconspicuous objects and features have to be detected to tell their\nclasses apart. In order to deal with this challenge, we propose a novel\nconvolutional neural network that mines mid-level image patches that are\nsufficiently dedicated to resolve the corresponding subtleties. In particular,\nwe train a newly de- signed CNN (DeepPattern) that learns discriminative patch\ngroups. There are two innovative aspects to this. On the one hand we pay\nattention to contextual information in an origi- nal fashion. On the other\nhand, we let an iteration of feature learning and patch clustering purify the\nset of dedicated patches that we use. We validate our method for action clas-\nsification on two challenging datasets: PASCAL VOC 2012 Action and Stanford 40\nActions, and for attribute recogni- tion we use the Berkeley Attributes of\nPeople dataset. Our discriminative mid-level mining CNN obtains state-of-the-\nart results on these datasets, without a need for annotations about parts and\nposes.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 15:43:10 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Diba", "Ali", ""], ["Pazandeh", "Ali Mohammad", ""], ["Pirsiavash", "Hamed", ""], ["Van Gool", "Luc", ""]]}, {"id": "1608.03235", "submitter": "Ulas Bagci", "authors": "Naji Khosravan, Haydar Celik, Baris Turkbey, Ruida Cheng, Evan\n  McCreedy, Matthew McAuliffe, Sandra Bednarova, Elizabeth Jones, Xinjian Chen,\n  Peter L. Choyke, Bradford J. Wood, Ulas Bagci", "title": "Gaze2Segment: A Pilot Study for Integrating Eye-Tracking Technology into\n  Medical Image Segmentation", "comments": "MICCAI-Medical Computer Vision 2016, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study introduced a novel system, called Gaze2Segment, integrating\nbiological and computer vision techniques to support radiologists' reading\nexperience with an automatic image segmentation task. During diagnostic\nassessment of lung CT scans, the radiologists' gaze information were used to\ncreate a visual attention map. This map was then combined with a\ncomputer-derived saliency map, extracted from the gray-scale CT images. The\nvisual attention map was used as an input for indicating roughly the location\nof a object of interest. With computer-derived saliency information, on the\nother hand, we aimed at finding foreground and background cues for the object\nof interest. At the final step, these cues were used to initiate a seed-based\ndelineation process. Segmentation accuracy of the proposed Gaze2Segment was\nfound to be 86% with dice similarity coefficient and 1.45 mm with Hausdorff\ndistance. To the best of our knowledge, Gaze2Segment is the first true\nintegration of eye-tracking technology into a medical image segmentation task\nwithout the need for any further user-interaction.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 16:44:00 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Khosravan", "Naji", ""], ["Celik", "Haydar", ""], ["Turkbey", "Baris", ""], ["Cheng", "Ruida", ""], ["McCreedy", "Evan", ""], ["McAuliffe", "Matthew", ""], ["Bednarova", "Sandra", ""], ["Jones", "Elizabeth", ""], ["Chen", "Xinjian", ""], ["Choyke", "Peter L.", ""], ["Wood", "Bradford J.", ""], ["Bagci", "Ulas", ""]]}, {"id": "1608.03240", "submitter": "YangQuan Chen Prof.", "authors": "Qi Yang, Dali Chen, Tiebiao Zhao, YangQuan Chen", "title": "Fractional Calculus In Image Processing: A Review", "comments": "26 pages, 9 figures, 117 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade, it has been demonstrated that many systems in science\nand engineering can be modeled more accurately by fractional-order than\ninteger-order derivatives, and many methods are developed to solve the problem\nof fractional systems. Due to the extra free parameter order, fractional-order\nbased methods provide additional degree of freedom in optimization performance.\nNot surprisingly, many fractional-order based methods have been used in image\nprocessing field. Herein recent studies are reviewed in ten sub-fields, which\ninclude image enhancement, image denoising, image edge detection, image\nsegmentation, image registration, image recognition, image fusion, image\nencryption, image compression and image restoration. In sum, it is well proved\nthat as a fundamental mathematic tool, fractional-order derivative shows great\nsuccess in image processing.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 17:45:33 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Yang", "Qi", ""], ["Chen", "Dali", ""], ["Zhao", "Tiebiao", ""], ["Chen", "YangQuan", ""]]}, {"id": "1608.03308", "submitter": "Himalaya Jain", "authors": "Himalaya Jain, Patrick P\\'erez, R\\'emi Gribonval, Joaquin Zepeda and\n  Herv\\'e J\\'egou", "title": "Approximate search with quantized sparse representations", "comments": "ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the task of storing a large collection of vectors, such as\nvisual descriptors, and of searching in it. To this end, we propose to\napproximate database vectors by constrained sparse coding, where possible atom\nweights are restricted to belong to a finite subset. This formulation\nencompasses, as particular cases, previous state-of-the-art methods such as\nproduct or residual quantization. As opposed to traditional sparse coding\nmethods, quantized sparse coding includes memory usage as a design constraint,\nthereby allowing us to index a large collection such as the BIGANN\nbillion-sized benchmark. Our experiments, carried out on standard benchmarks,\nshow that our formulation leads to competitive solutions when considering\ndifferent trade-offs between learning/coding time, index size and search\nquality.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 22:00:00 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Jain", "Himalaya", ""], ["P\u00e9rez", "Patrick", ""], ["Gribonval", "R\u00e9mi", ""], ["Zepeda", "Joaquin", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "1608.03369", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Ravi Kiran Sarvadevabhatla, Jogendra Kundu, Babu R. Venkatesh", "title": "Enabling My Robot To Play Pictionary : Recurrent Neural Networks For\n  Sketch Recognition", "comments": "Accepted at ACMMM 2016. Code and models at\n  https://github.com/val-iisc/sketch-object-recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Freehand sketching is an inherently sequential process. Yet, most approaches\nfor hand-drawn sketch recognition either ignore this sequential aspect or\nexploit it in an ad-hoc manner. In our work, we propose a recurrent neural\nnetwork architecture for sketch object recognition which exploits the long-term\nsequential and structural regularities in stroke data in a scalable manner.\nSpecifically, we introduce a Gated Recurrent Unit based framework which\nleverages deep sketch features and weighted per-timestep loss to achieve\nstate-of-the-art results on a large database of freehand object sketches across\na large number of object categories. The inherently online nature of our\nframework is especially suited for on-the-fly recognition of objects as they\nare being drawn. Thus, our framework can enable interesting applications such\nas camera-equipped robots playing the popular party game Pictionary with human\nplayers and generating sparsified yet recognizable sketches of objects.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 05:11:23 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Sarvadevabhatla", "Ravi Kiran", ""], ["Kundu", "Jogendra", ""], ["Venkatesh", "Babu R.", ""]]}, {"id": "1608.03374", "submitter": "Nitigya Sambyal", "authors": "Nitigya Sambyal and Pawanesh Abrol", "title": "Automatic text extraction and character segmentation using maximally\n  stable extremal regions", "comments": null, "journal-ref": "International Journal of Modern Computer Science,Vol 4,Issue\n  3,pp.136-141,2016", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text detection and segmentation is an important prerequisite for many content\nbased image analysis tasks. The paper proposes a novel text extraction and\ncharacter segmentation algorithm using Maximally Stable Extremal Regions as\nbasic letter candidates. These regions are then subjected to thresholding and\nthereafter various connected components are determined to identify separate\ncharacters. The algorithm is tested along a set of various JPEG, PNG and BMP\nimages over four different character sets; English, Russian, Hindi and Urdu.\nThe algorithm gives good results for English and Russian character set; however\ncharacter segmentation in Urdu and Hindi language is not much accurate. The\nalgorithm is simple, efficient, involves no overhead as required in training\nand gives good results for even low quality images. The paper also proposes\nvarious challenges in text extraction and segmentation for multilingual inputs.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 05:34:42 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Sambyal", "Nitigya", ""], ["Abrol", "Pawanesh", ""]]}, {"id": "1608.03396", "submitter": "Lun Liu", "authors": "Lun Liu, Hui Wang, Chunyang Wu", "title": "A machine learning method for the large-scale evaluation of urban visual\n  environment", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the size of modern cities in the urbanising age, it is beyond the\nperceptual capacity of most people to develop a good knowledge about the beauty\nand ugliness of the city at every street corner. Correspondingly, for planners,\nit is also difficult to accurately answer questions like 'where are the\nworst-looking places in the city that regeneration should give first\nconsideration', or 'in the fast urbanising cities, how is the city appearance\nchanging', etc. To address this issue, we here present a computer vision method\nfor the large-scale and automatic evaluation of the urban visual environment,\nby leveraging state-of-the-art machine learning techniques and the\nwide-coverage street view images. From the various factors that are at work, we\nchoose two key features, the visual quality of street facade and the continuity\nof street wall, as the starting point of this line of analysis. In order to\ntest the validity of this method, we further compare the machine ratings with\nratings collected on site from 752 passers-by on fifty-six locations. We show\nthat the machine learning model can produce a good estimation of people's real\nvisual experience, and it holds much potential for various tasks in terms of\nurban design evaluation, culture identification, etc.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 08:29:07 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Liu", "Lun", ""], ["Wang", "Hui", ""], ["Wu", "Chunyang", ""]]}, {"id": "1608.03410", "submitter": "Tatiana Tommasi", "authors": "Tatiana Tommasi, Arun Mallya, Bryan Plummer, Svetlana Lazebnik,\n  Alexander C. Berg, Tamara L. Berg", "title": "Solving Visual Madlibs with Multiple Cues", "comments": "accepted at BMVC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on answering fill-in-the-blank style multiple choice\nquestions from the Visual Madlibs dataset. Previous approaches to Visual\nQuestion Answering (VQA) have mainly used generic image features from networks\ntrained on the ImageNet dataset, despite the wide scope of questions. In\ncontrast, our approach employs features derived from networks trained for\nspecialized tasks of scene classification, person activity prediction, and\nperson and object attribute prediction. We also present a method for selecting\nsub-regions of an image that are relevant for evaluating the appropriateness of\na putative answer. Visual features are computed both from the whole image and\nfrom local regions, while sentences are mapped to a common space using a simple\nnormalized canonical correlation analysis (CCA) model. Our results show a\nsignificant improvement over the previous state of the art, and indicate that\nanswering different question types benefits from examining a variety of image\ncues and carefully choosing informative image sub-regions.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 09:51:21 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Tommasi", "Tatiana", ""], ["Mallya", "Arun", ""], ["Plummer", "Bryan", ""], ["Lazebnik", "Svetlana", ""], ["Berg", "Alexander C.", ""], ["Berg", "Tamara L.", ""]]}, {"id": "1608.03440", "submitter": "Emmanuel Maggiori", "authors": "Emmanuel Maggiori, Guillaume Charpiat, Yuliya Tarabalka, and Pierre\n  Alliez", "title": "Recurrent Neural Networks to Correct Satellite Image Classification Maps", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2017.2697453", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While initially devised for image categorization, convolutional neural\nnetworks (CNNs) are being increasingly used for the pixelwise semantic labeling\nof images. However, the proper nature of the most common CNN architectures\nmakes them good at recognizing but poor at localizing objects precisely. This\nproblem is magnified in the context of aerial and satellite image labeling,\nwhere a spatially fine object outlining is of paramount importance. Different\niterative enhancement algorithms have been presented in the literature to\nprogressively improve the coarse CNN outputs, seeking to sharpen object\nboundaries around real image edges. However, one must carefully design, choose\nand tune such algorithms. Instead, our goal is to directly learn the iterative\nprocess itself. For this, we formulate a generic iterative enhancement process\ninspired from partial differential equations, and observe that it can be\nexpressed as a recurrent neural network (RNN). Consequently, we train such a\nnetwork from manually labeled data for our enhancement task. In a series of\nexperiments we show that our RNN effectively learns an iterative process that\nsignificantly improves the quality of satellite image classification maps.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 12:49:46 GMT"}, {"version": "v2", "created": "Wed, 26 Oct 2016 08:35:44 GMT"}, {"version": "v3", "created": "Fri, 21 Apr 2017 11:08:37 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Maggiori", "Emmanuel", ""], ["Charpiat", "Guillaume", ""], ["Tarabalka", "Yuliya", ""], ["Alliez", "Pierre", ""]]}, {"id": "1608.03462", "submitter": "Muhammet Bastan", "authors": "Muhammet Bastan and Ozgur Yilmaz", "title": "Multi-View Product Image Search Using Deep ConvNets Representations", "comments": "13 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view product image queries can improve retrieval performance over\nsingle view queries significantly. In this paper, we investigated the\nperformance of deep convolutional neural networks (ConvNets) on multi-view\nproduct image search. First, we trained a VGG-like network to learn deep\nConvNets representations of product images. Then, we computed the deep ConvNets\nrepresentations of database and query images and performed single view queries,\nand multi-view queries using several early and late fusion approaches.\n  We performed extensive experiments on the publicly available Multi-View\nObject Image Dataset (MVOD 5K) with both clean background queries from the\nInternet and cluttered background queries from a mobile phone. We compared the\nperformance of ConvNets to the classical bag-of-visual-words (BoWs). We\nconcluded that (1) multi-view queries with deep ConvNets representations\nperform significantly better than single view queries, (2) ConvNets perform\nmuch better than BoWs and have room for further improvement, (3) pre-training\nof ConvNets on a different image dataset with background clutter is needed to\nobtain good performance on cluttered product image queries obtained with a\nmobile phone.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 13:50:07 GMT"}, {"version": "v2", "created": "Mon, 1 May 2017 08:08:28 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Bastan", "Muhammet", ""], ["Yilmaz", "Ozgur", ""]]}, {"id": "1608.03474", "submitter": "Buyu Liu", "authors": "Buyu Liu and Xuming He", "title": "Learning Dynamic Hierarchical Models for Anytime Scene Labeling", "comments": "Accepted by ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing demand for efficient image and video analysis, test-time cost\nof scene parsing becomes critical for many large-scale or time-sensitive vision\napplications. We propose a dynamic hierarchical model for anytime scene\nlabeling that allows us to achieve flexible trade-offs between efficiency and\naccuracy in pixel-level prediction. In particular, our approach incorporates\nthe cost of feature computation and model inference, and optimizes the model\nperformance for any given test-time budget by learning a sequence of\nimage-adaptive hierarchical models. We formulate this anytime representation\nlearning as a Markov Decision Process with a discrete-continuous state-action\nspace. A high-quality policy of feature and model selection is learned based on\nan approximate policy iteration method with action proposal mechanism. We\ndemonstrate the advantages of our dynamic non-myopic anytime scene parsing on\nthree semantic segmentation datasets, which achieves $90\\%$ of the\nstate-of-the-art performances by using $15\\%$ of their overall costs.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 14:19:31 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Liu", "Buyu", ""], ["He", "Xuming", ""]]}, {"id": "1608.03609", "submitter": "Evan Shelhamer", "authors": "Evan Shelhamer, Kate Rakelly, Judy Hoffman, Trevor Darrell", "title": "Clockwork Convnets for Video Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen tremendous progress in still-image segmentation;\nhowever the na\\\"ive application of these state-of-the-art algorithms to every\nvideo frame requires considerable computation and ignores the temporal\ncontinuity inherent in video. We propose a video recognition framework that\nrelies on two key observations: 1) while pixels may change rapidly from frame\nto frame, the semantic content of a scene evolves more slowly, and 2) execution\ncan be viewed as an aspect of architecture, yielding purpose-fit computation\nschedules for networks. We define a novel family of \"clockwork\" convnets driven\nby fixed or adaptive clock signals that schedule the processing of different\nlayers at different update rates according to their semantic stability. We\ndesign a pipeline schedule to reduce latency for real-time recognition and a\nfixed-rate schedule to reduce overall computation. Finally, we extend clockwork\nscheduling to adaptive video processing by incorporating data-driven clocks\nthat can be tuned on unlabeled video. The accuracy and efficiency of clockwork\nconvnets are evaluated on the Youtube-Objects, NYUD, and Cityscapes video\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 20:32:55 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Shelhamer", "Evan", ""], ["Rakelly", "Kate", ""], ["Hoffman", "Judy", ""], ["Darrell", "Trevor", ""]]}, {"id": "1608.03617", "submitter": "Larbi Guezouli", "authors": "Larbi Guezouli, Hanane Belhani", "title": "Automatic detection of moving objects in video surveillance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is in the field of video surveillance including motion detection.\nThe video surveillance is one of essential techniques for automatic video\nanalysis to extract crucial information or relevant scenes in video\nsurveillance systems. The aim of our work is to propose solutions for the\nautomatic detection of moving objects in real time with a surveillance camera.\nThe detected objects are objects that have some geometric shape (circle,\nellipse, square, and rectangle).\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 20:53:48 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Guezouli", "Larbi", ""], ["Belhani", "Hanane", ""]]}, {"id": "1608.03630", "submitter": "Andreas Mang", "authors": "Andreas Mang and Amir Gholami and George Biros", "title": "Distributed-memory large deformation diffeomorphic 3D image registration", "comments": "accepted for publication at SC16 in Salt Lake City, Utah, USA;\n  November 2016", "journal-ref": "Proceedings of the International Conference for High Performance\n  Computing, Networking, Storage and Analysis, Article No. 72, 2016", "doi": "10.1109/SC.2016.71", "report-no": null, "categories": "cs.DC cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a parallel distributed-memory algorithm for large deformation\ndiffeomorphic registration of volumetric images that produces large isochoric\ndeformations (locally volume preserving). Image registration is a key\ntechnology in medical image analysis. Our algorithm uses a partial differential\nequation constrained optimal control formulation. Finding the optimal\ndeformation map requires the solution of a highly nonlinear problem that\ninvolves pseudo-differential operators, biharmonic operators, and pure\nadvection operators both forward and back- ward in time. A key issue is the\ntime to solution, which poses the demand for efficient optimization methods as\nwell as an effective utilization of high performance computing resources. To\naddress this problem we use a preconditioned, inexact, Gauss-Newton- Krylov\nsolver. Our algorithm integrates several components: a spectral discretization\nin space, a semi-Lagrangian formulation in time, analytic adjoints, different\nregularization functionals (including volume-preserving ones), a spectral\npreconditioner, a highly optimized distributed Fast Fourier Transform, and a\ncubic interpolation scheme for the semi-Lagrangian time-stepping. We\ndemonstrate the scalability of our algorithm on images with resolution of up to\n$1024^3$ on the \"Maverick\" and \"Stampede\" systems at the Texas Advanced\nComputing Center (TACC). The critical problem in the medical imaging\napplication domain is strong scaling, that is, solving registration problems of\na moderate size of $256^3$---a typical resolution for medical images. We are\nable to solve the registration problem for images of this size in less than\nfive seconds on 64 x86 nodes of TACC's \"Maverick\" system.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 22:52:27 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Mang", "Andreas", ""], ["Gholami", "Amir", ""], ["Biros", "George", ""]]}, {"id": "1608.03644", "submitter": "Yanjun  Qi Dr.", "authors": "Jack Lanchantin, Ritambhara Singh, Beilun Wang, and Yanjun Qi", "title": "Deep Motif Dashboard: Visualizing and Understanding Genomic Sequences\n  Using Deep Neural Networks", "comments": "11 pages, 2 figures. Updated for PSB submission, Pacific Symposium on\n  Biocomputing (PSB) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network (DNN) models have recently obtained state-of-the-art\nprediction accuracy for the transcription factor binding (TFBS) site\nclassification task. However, it remains unclear how these approaches identify\nmeaningful DNA sequence signals and give insights as to why TFs bind to certain\nlocations. In this paper, we propose a toolkit called the Deep Motif Dashboard\n(DeMo Dashboard) which provides a suite of visualization strategies to extract\nmotifs, or sequence patterns from deep neural network models for TFBS\nclassification. We demonstrate how to visualize and understand three important\nDNN models: convolutional, recurrent, and convolutional-recurrent networks. Our\nfirst visualization method is finding a test sequence's saliency map which uses\nfirst-order derivatives to describe the importance of each nucleotide in making\nthe final prediction. Second, considering recurrent models make predictions in\na temporal manner (from one end of a TFBS sequence to the other), we introduce\ntemporal output scores, indicating the prediction score of a model over time\nfor a sequential input. Lastly, a class-specific visualization strategy finds\nthe optimal input sequence for a given TFBS positive class via stochastic\ngradient optimization. Our experimental results indicate that a\nconvolutional-recurrent architecture performs the best among the three\narchitectures. The visualization techniques indicate that CNN-RNN makes\npredictions by modeling both motifs as well as dependencies among them.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 00:43:59 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 14:00:44 GMT"}, {"version": "v3", "created": "Tue, 4 Oct 2016 16:37:45 GMT"}, {"version": "v4", "created": "Tue, 18 Oct 2016 20:20:22 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Lanchantin", "Jack", ""], ["Singh", "Ritambhara", ""], ["Wang", "Beilun", ""], ["Qi", "Yanjun", ""]]}, {"id": "1608.03658", "submitter": "Yadong Mu", "authors": "Yadong Mu and Zhu Liu", "title": "Deep Hashing: A Joint Approach for Image Signature Learning", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity-based image hashing represents crucial technique for visual data\nstorage reduction and expedited image search. Conventional hashing schemes\ntypically feed hand-crafted features into hash functions, which separates the\nprocedures of feature extraction and hash function learning. In this paper, we\npropose a novel algorithm that concurrently performs feature engineering and\nnon-linear supervised hashing function learning. Our technical contributions in\nthis paper are two-folds: 1) deep network optimization is often achieved by\ngradient propagation, which critically requires a smooth objective function.\nThe discrete nature of hash codes makes them not amenable for gradient-based\noptimization. To address this issue, we propose an exponentiated hashing loss\nfunction and its bilinear smooth approximation. Effective gradient calculation\nand propagation are thereby enabled; 2) pre-training is an important trick in\nsupervised deep learning. The impact of pre-training on the hash code quality\nhas never been discussed in current deep hashing literature. We propose a\npre-training scheme inspired by recent advance in deep network based image\nclassification, and experimentally demonstrate its effectiveness. Comprehensive\nquantitative evaluations are conducted on several widely-used image benchmarks.\nOn all benchmarks, our proposed deep hashing algorithm outperforms all\nstate-of-the-art competitors by significant margins. In particular, our\nalgorithm achieves a near-perfect 0.99 in terms of Hamming ranking accuracy\nwith only 12 bits on MNIST, and a new record of 0.74 on the CIFAR10 dataset. In\ncomparison, the best accuracies obtained on CIFAR10 by existing hashing\nalgorithms without or with deep networks are known to be 0.36 and 0.58\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 02:00:08 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Mu", "Yadong", ""], ["Liu", "Zhu", ""]]}, {"id": "1608.03667", "submitter": "Martin Lukac", "authors": "Martin Lukac, Kamila Abdiyeva, Michitaka Kameyama", "title": "Reasoning and Algorithm Selection Augmented Symbolic Segmentation", "comments": "arXiv admin note: substantial text overlap with arXiv:1505.07934", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an alternative method to symbolic segmentation: we\napproach symbolic segmentation as an algorithm selection problem. That is, let\nthere be a set A of available algorithms for symbolic segmentation, a set of\ninput features $F$, a set of image attribute $\\mathbb{A}$ and a selection\nmechanism $S(F,\\mathbb{A},A)$ that selects on a case by case basis the best\nalgorithm. The semantic segmentation is then an optimization process that\ncombines best component segments from multiple results into a single optimal\nresult. The experiments compare three different algorithm selection mechanisms\nusing three selected semantic segmentation algorithms. The results show that\nusing the current state of art algorithms and relatively low accuracy of\nalgorithm selection the accuracy of the semantic segmentation can be improved\nby 2\\%.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 03:42:28 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Lukac", "Martin", ""], ["Abdiyeva", "Kamila", ""], ["Kameyama", "Michitaka", ""]]}, {"id": "1608.03720", "submitter": "Alex James Dr", "authors": "Aibek Ryskaliyev, Sanzhar Askaruly, Alex Pappachen James", "title": "Speech Signal Analysis for the Estimation of Heart Rates Under Different\n  Emotional States", "comments": "to appear in Proceedings of International Conference on Advances in\n  Computing, Communications and Informatics, IEEE, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A non-invasive method for the monitoring of heart activity can help to reduce\nthe deaths caused by heart disorders such as stroke, arrhythmia and heart\nattack. The human voice can be considered as a biometric data that can be used\nfor estimation of heart rate. In this paper, we propose a method for estimating\nthe heart rate from human speech dynamically using voice signal analysis and by\nthe development of an empirical linear predictor model. The correlation between\nthe voice signal and heart rate are established by classifiers and prediction\nof the heart rates with or without emotions are done using linear models. The\nprediction accuracy was tested using the data collected from 15 subjects, it is\nabout 4050 samples of speech signals and corresponding electrocardiogram\nsamples. The proposed approach can use for early non-invasive detection of\nheart rate changes that can be correlated to an emotional state of the\nindividual and also can be used as a tool for diagnosis of heart conditions in\nreal-time situations.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 08:52:23 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Ryskaliyev", "Aibek", ""], ["Askaruly", "Sanzhar", ""], ["James", "Alex Pappachen", ""]]}, {"id": "1608.03748", "submitter": "Mengyi Liu", "authors": "Mengyi Liu, Lu Jiang, Shiguang Shan, Alexander G. Hauptmann", "title": "Self-paced Learning for Weakly Supervised Evidence Discovery in\n  Multimedia Event Search", "comments": "This paper has been withdrawn by the author due to a crucial error in\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia event detection has been receiving increasing attention in recent\nyears. Besides recognizing an event, the discovery of evidences (which is\nrefered to as \"recounting\") is also crucial for user to better understand the\nsearching result. Due to the difficulty of evidence annotation, only limited\nsupervision of event labels are available for training a recounting model. To\ndeal with the problem, we propose a weakly supervised evidence discovery method\nbased on self-paced learning framework, which follows a learning process from\neasy \"evidences\" to gradually more complex ones, and simultaneously exploit\nmore and more positive evidence samples from numerous weakly annotated video\nsegments. Moreover, to evaluate our method quantitatively, we also propose two\nmetrics, \\textit{PctOverlap} and \\textit{F1-score}, for measuring the\nperformance of evidence localization specifically. The experiments are\nconducted on a subset of TRECVID MED dataset and demonstrate the promising\nresults obtained by our method.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 11:01:31 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2017 09:23:19 GMT"}, {"version": "v3", "created": "Mon, 23 Oct 2017 06:39:45 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Liu", "Mengyi", ""], ["Jiang", "Lu", ""], ["Shan", "Shiguang", ""], ["Hauptmann", "Alexander G.", ""]]}, {"id": "1608.03773", "submitter": "Martin Danelljan", "authors": "Martin Danelljan, Andreas Robinson, Fahad Shahbaz Khan, Michael\n  Felsberg", "title": "Beyond Correlation Filters: Learning Continuous Convolution Operators\n  for Visual Tracking", "comments": "Accepted at ECCV 2016", "journal-ref": "ECCV 2016, Part V, LNCS 9909, pp. 472-488. Springer (2016)", "doi": "10.1007/978-3-319-46454-1_29", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative Correlation Filters (DCF) have demonstrated excellent\nperformance for visual object tracking. The key to their success is the ability\nto efficiently exploit available negative data by including all shifted\nversions of a training sample. However, the underlying DCF formulation is\nrestricted to single-resolution feature maps, significantly limiting its\npotential. In this paper, we go beyond the conventional DCF framework and\nintroduce a novel formulation for training continuous convolution filters. We\nemploy an implicit interpolation model to pose the learning problem in the\ncontinuous spatial domain. Our proposed formulation enables efficient\nintegration of multi-resolution deep feature maps, leading to superior results\non three object tracking benchmarks: OTB-2015 (+5.1% in mean OP), Temple-Color\n(+4.6% in mean OP), and VOT2015 (20% relative reduction in failure rate).\nAdditionally, our approach is capable of sub-pixel localization, crucial for\nthe task of accurate feature point tracking. We also demonstrate the\neffectiveness of our learning formulation in extensive feature point tracking\nexperiments. Code and supplementary material are available at\nhttp://www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/index.html.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 12:24:11 GMT"}, {"version": "v2", "created": "Mon, 29 Aug 2016 10:33:17 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Danelljan", "Martin", ""], ["Robinson", "Andreas", ""], ["Khan", "Fahad Shahbaz", ""], ["Felsberg", "Michael", ""]]}, {"id": "1608.03793", "submitter": "Rajiv Shah", "authors": "Rajiv Shah and Rob Romijnders", "title": "Applying Deep Learning to Basketball Trajectories", "comments": "KDD 2016, Large Scale Sports Analytic Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the emerging trends for sports analytics is the growing use of player\nand ball tracking data. A parallel development is deep learning predictive\napproaches that use vast quantities of data with less reliance on feature\nengineering. This paper applies recurrent neural networks in the form of\nsequence modeling to predict whether a three-point shot is successful. The\nmodels are capable of learning the trajectory of a basketball without any\nknowledge of physics. For comparison, a baseline static machine learning model\nwith a full set of features, such as angle and velocity, in addition to the\npositional data is also tested. Using a dataset of over 20,000 three pointers\nfrom NBA SportVu data, the models based simply on sequential positional data\noutperform a static feature rich machine learning model in predicting whether a\nthree-point shot is successful. This suggests deep learning models may offer an\nimprovement to traditional feature based machine learning methods for tracking\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 13:50:24 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 18:36:44 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Shah", "Rajiv", ""], ["Romijnders", "Rob", ""]]}, {"id": "1608.03819", "submitter": "Chenyou Fan", "authors": "Chenyou Fan, David J. Crandall", "title": "DeepDiary: Automatic Caption Generation for Lifelogging Image Streams", "comments": "This is an expanded preprint of a paper appearing at the ECCV\n  International Workshop on Egocentric Perception, Interaction, and Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lifelogging cameras capture everyday life from a first-person perspective,\nbut generate so much data that it is hard for users to browse and organize\ntheir image collections effectively. In this paper, we propose to use automatic\nimage captioning algorithms to generate textual representations of these\ncollections. We develop and explore novel techniques based on deep learning to\ngenerate captions for both individual images and image streams, using temporal\nconsistency constraints to create summaries that are both more compact and less\nnoisy. We evaluate our techniques with quantitative and qualitative results,\nand apply captioning to an image retrieval application for finding potentially\nprivate images. Our results suggest that our automatic captioning algorithms,\nwhile imperfect, may work well enough to help users manage lifelogging photo\ncollections.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 15:17:33 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Fan", "Chenyou", ""], ["Crandall", "David J.", ""]]}, {"id": "1608.03832", "submitter": "Martin Lukac", "authors": "Martin Lukac, Kamila Abdiyeva, Michitaka Kameyama", "title": "On Minimal Accuracy Algorithm Selection in Computer Vision and\n  Intelligent Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we discuss certain theoretical properties of algorithm\nselection approach to image processing and to intelligent system in general. We\nanalyze the theoretical limits of algorithm selection with respect to the\nalgorithm selection accuracy. We show the theoretical formulation of a crisp\nbound on the algorithm selector precision guaranteeing to always obtain better\nthan the best available algorithm result.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 15:55:12 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Lukac", "Martin", ""], ["Abdiyeva", "Kamila", ""], ["Kameyama", "Michitaka", ""]]}, {"id": "1608.03907", "submitter": "Ruizhi Liao", "authors": "Ruizhi Liao, Esra Turk, Miaomiao Zhang, Jie Luo, Ellen Grant, Elfar\n  Adalsteinsson, Polina Golland", "title": "Temporal Registration in In-Utero Volumetric MRI Time Series", "comments": "to appear in International Conference on Medical Image Computing and\n  Computer Assisted Intervention, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a robust method to correct for motion and deformations for\nin-utero volumetric MRI time series. Spatio-temporal analysis of dynamic MRI\nrequires robust alignment across time in the presence of substantial and\nunpredictable motion. We make a Markov assumption on the nature of deformations\nto take advantage of the temporal structure in the image data. Forward message\npassing in the corresponding hidden Markov model (HMM) yields an estimation\nalgorithm that only has to account for relatively small motion between\nconsecutive frames. We demonstrate the utility of the temporal model by showing\nthat its use improves the accuracy of the segmentation propagation through\ntemporal registration. Our results suggest that the proposed model captures\naccurately the temporal dynamics of deformations in in-utero MRI time series.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 20:40:24 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Liao", "Ruizhi", ""], ["Turk", "Esra", ""], ["Zhang", "Miaomiao", ""], ["Luo", "Jie", ""], ["Grant", "Ellen", ""], ["Adalsteinsson", "Elfar", ""], ["Golland", "Polina", ""]]}, {"id": "1608.03914", "submitter": "Sirion Vittayakorn", "authors": "Sirion Vittayakorn, Alexander C. Berg, Tamara L. Berg", "title": "When was that made?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore deep learning methods for estimating when objects\nwere made. Automatic methods for this task could potentially be useful for\nhistorians, collectors, or any individual interested in estimating when their\nartifact was created. Direct applications include large-scale data organization\nor retrieval. Toward this goal, we utilize features from existing deep networks\nand also fine-tune new networks for temporal estimation. In addition, we create\ntwo new datasets of 67,771 dated clothing items from Flickr and museum\ncollections. Our method outperforms both a color-based baseline and previous\nstate of the art methods for temporal estimation. We also provide several\nanalyses of what our networks have learned, and demonstrate applications to\nidentifying temporal inspiration in fashion collections.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 22:03:38 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Vittayakorn", "Sirion", ""], ["Berg", "Alexander C.", ""], ["Berg", "Tamara L.", ""]]}, {"id": "1608.03932", "submitter": "Liang Lin", "authors": "Keze Wang and Shengfu Zhai and Hui Cheng and Xiaodan Liang and Liang\n  Lin", "title": "Human Pose Estimation from Depth Images via Inference Embedded\n  Multi-task Learning", "comments": "To appear in ACM Multimedia 2016, full paper (oral), 10 pages, 11\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose estimation (i.e., locating the body parts / joints of a person) is\na fundamental problem in human-computer interaction and multimedia\napplications. Significant progress has been made based on the development of\ndepth sensors, i.e., accessible human pose prediction from still depth images\n[32]. However, most of the existing approaches to this problem involve several\ncomponents/models that are independently designed and optimized, leading to\nsuboptimal performances. In this paper, we propose a novel inference-embedded\nmulti-task learning framework for predicting human pose from still depth\nimages, which is implemented with a deep architecture of neural networks.\nSpecifically, we handle two cascaded tasks: i) generating the heat (confidence)\nmaps of body parts via a fully convolutional network (FCN); ii) seeking the\noptimal configuration of body parts based on the detected body part proposals\nvia an inference built-in MatchNet [10], which measures the appearance and\ngeometric kinematic compatibility of body parts and embodies the dynamic\nprogramming inference as an extra network layer. These two tasks are jointly\noptimized. Our extensive experiments show that the proposed deep model\nsignificantly improves the accuracy of human pose estimation over other several\nstate-of-the-art methods or SDKs. We also release a large-scale dataset for\ncomparison, which includes 100K depth images under challenging scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2016 03:16:47 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Wang", "Keze", ""], ["Zhai", "Shengfu", ""], ["Cheng", "Hui", ""], ["Liang", "Xiaodan", ""], ["Lin", "Liang", ""]]}, {"id": "1608.03974", "submitter": "Giovanni Montana", "authors": "Rudra P K Poudel and Pablo Lamata and Giovanni Montana", "title": "Recurrent Fully Convolutional Neural Networks for Multi-slice MRI\n  Cardiac Segmentation", "comments": "MICCAI Workshop RAMBO 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cardiac magnetic resonance imaging, fully-automatic segmentation of the\nheart enables precise structural and functional measurements to be taken, e.g.\nfrom short-axis MR images of the left-ventricle. In this work we propose a\nrecurrent fully-convolutional network (RFCN) that learns image representations\nfrom the full stack of 2D slices and has the ability to leverage inter-slice\nspatial dependences through internal memory units. RFCN combines anatomical\ndetection and segmentation into a single architecture that is trained\nend-to-end thus significantly reducing computational time, simplifying the\nsegmentation pipeline, and potentially enabling real-time applications. We\nreport on an investigation of RFCN using two datasets, including the publicly\navailable MICCAI 2009 Challenge dataset. Comparisons have been carried out\nbetween fully convolutional networks and deep restricted Boltzmann machines,\nincluding a recurrent version that leverages inter-slice spatial correlation.\nOur studies suggest that RFCN produces state-of-the-art results and can\nsubstantially improve the delineation of contours near the apex of the heart.\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2016 11:19:22 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Poudel", "Rudra P K", ""], ["Lamata", "Pablo", ""], ["Montana", "Giovanni", ""]]}, {"id": "1608.03981", "submitter": "Wangmeng Zuo", "authors": "Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang", "title": "Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image\n  Denoising", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2017.2662206", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative model learning for image denoising has been recently\nattracting considerable attentions due to its favorable denoising performance.\nIn this paper, we take one step forward by investigating the construction of\nfeed-forward denoising convolutional neural networks (DnCNNs) to embrace the\nprogress in very deep architecture, learning algorithm, and regularization\nmethod into image denoising. Specifically, residual learning and batch\nnormalization are utilized to speed up the training process as well as boost\nthe denoising performance. Different from the existing discriminative denoising\nmodels which usually train a specific model for additive white Gaussian noise\n(AWGN) at a certain noise level, our DnCNN model is able to handle Gaussian\ndenoising with unknown noise level (i.e., blind Gaussian denoising). With the\nresidual learning strategy, DnCNN implicitly removes the latent clean image in\nthe hidden layers. This property motivates us to train a single DnCNN model to\ntackle with several general image denoising tasks such as Gaussian denoising,\nsingle image super-resolution and JPEG image deblocking. Our extensive\nexperiments demonstrate that our DnCNN model can not only exhibit high\neffectiveness in several general image denoising tasks, but also be efficiently\nimplemented by benefiting from GPU computing.\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2016 13:33:28 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Zhang", "Kai", ""], ["Zuo", "Wangmeng", ""], ["Chen", "Yunjin", ""], ["Meng", "Deyu", ""], ["Zhang", "Lei", ""]]}, {"id": "1608.04042", "submitter": "Arturo Deza", "authors": "Arturo Deza and Miguel P. Eckstein", "title": "Can Peripheral Representations Improve Clutter Metrics on Complex\n  Scenes?", "comments": "Pre-Print to be presented at NIPS 2016 in Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies have proposed image-based clutter measures that correlate\nwith human search times and/or eye movements. However, most models do not take\ninto account the fact that the effects of clutter interact with the foveated\nnature of the human visual system: visual clutter further from the fovea has an\nincreasing detrimental influence on perception. Here, we introduce a new\nfoveated clutter model to predict the detrimental effects in target search\nutilizing a forced fixation search task. We use Feature Congestion (Rosenholtz\net al.) as our non foveated clutter model, and we stack a peripheral\narchitecture on top of Feature Congestion for our foveated model. We introduce\nthe Peripheral Integration Feature Congestion (PIFC) coefficient, as a\nfundamental ingredient of our model that modulates clutter as a non-linear gain\ncontingent on eccentricity. We finally show that Foveated Feature Congestion\n(FFC) clutter scores r(44) = -0.82 correlate better with target detection (hit\nrate) than regular Feature Congestion r(44) = -0.19 in forced fixation search.\nThus, our model allows us to enrich clutter perception research by computing\nfixation specific clutter maps. A toolbox for creating peripheral\narchitectures: Piranhas: Peripheral Architectures for Natural, Hybrid and\nArtificial Systems will be made available.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 01:07:29 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Deza", "Arturo", ""], ["Eckstein", "Miguel P.", ""]]}, {"id": "1608.04045", "submitter": "Kyle Simek", "authors": "Kyle Simek, Ravishankar Palanivelu, Kobus Barnard", "title": "Branching Gaussian Processes with Applications to Spatiotemporal\n  Reconstruction of 3D Trees", "comments": "ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a robust method for estimating dynamic 3D curvilinear branching\nstructure from monocular images. While 3D reconstruction from images has been\nwidely studied, estimating thin structure has received less attention. This\nproblem becomes more challenging in the presence of camera error, scene motion,\nand a constraint that curves are attached in a branching structure. We propose\na new general-purpose prior, a branching Gaussian processes (BGP), that models\nspatial smoothness and temporal dynamics of curves while enforcing attachment\nbetween them. We apply this prior to fit 3D trees directly to image data, using\nan efficient scheme for approximate inference based on expectation propagation.\nThe BGP prior's Gaussian form allows us to approximately marginalize over 3D\ntrees with a given model structure, enabling principled comparison between tree\nmodels with varying complexity. We test our approach on a novel multi-view\ndataset depicting plants with known 3D structures and topologies undergoing\nsmall nonrigid motion. Our method outperforms a state-of-the-art 3D\nreconstruction method designed for non-moving thin structure. We evaluate under\nseveral common measures, and we propose a new measure for reconstructions of\nbranching multi-part 3D scenes under motion.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 01:41:07 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Simek", "Kyle", ""], ["Palanivelu", "Ravishankar", ""], ["Barnard", "Kobus", ""]]}, {"id": "1608.04051", "submitter": "Ting Liu", "authors": "Ting Liu, Miaomiao Zhang, Mehran Javanmardi, Nisha Ramesh, Tolga\n  Tasdizen", "title": "SSHMT: Semi-supervised Hierarchical Merge Tree for Electron Microscopy\n  Image Segmentation", "comments": "Accepted by ECCV 2016", "journal-ref": "Computer Vision - 14th European Conference, ECCV 2016,\n  Proceedings, 144--159", "doi": "10.1007/978-3-319-46448-0_9", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Region-based methods have proven necessary for improving segmentation\naccuracy of neuronal structures in electron microscopy (EM) images. Most\nregion-based segmentation methods use a scoring function to determine region\nmerging. Such functions are usually learned with supervised algorithms that\ndemand considerable ground truth data, which are costly to collect. We propose\na semi-supervised approach that reduces this demand. Based on a merge tree\nstructure, we develop a differentiable unsupervised loss term that enforces\nconsistent predictions from the learned function. We then propose a Bayesian\nmodel that combines the supervised and the unsupervised information for\nprobabilistic learning. The experimental results on three EM data sets\ndemonstrate that by using a subset of only 3% to 7% of the entire ground truth\ndata, our approach consistently performs close to the state-of-the-art\nsupervised method with the full labeled data set, and significantly outperforms\nthe supervised method with the same labeled subset.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 02:34:29 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Liu", "Ting", ""], ["Zhang", "Miaomiao", ""], ["Javanmardi", "Mehran", ""], ["Ramesh", "Nisha", ""], ["Tasdizen", "Tolga", ""]]}, {"id": "1608.04059", "submitter": "Kriti Sen Sharma", "authors": "Kriti Sen Sharma", "title": "Scout-It: Interior tomography using modified scout acquisition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global scout views have been previously used to reduce interior\nreconstruction artifacts in high-resolution micro-CT and C-arm systems. However\nthese methods cannot be directly used in the all-important domain of clinical\nCT. This is because when the CT scan is truncated, the scout views are also\ntruncated. However many cases of truncation in clinical CT involve partial\ntruncation, where the anterio-posterior (AP) scout is truncated, but the\nmedio-lateral (ML) scout is non-truncated. In this paper, we show that in such\ncases of partially truncated CT scans, a modified configuration may be used to\nacquire non-truncated AP scout view, and ultimately allow for highly accurate\ninterior reconstruction.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 04:42:30 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Sharma", "Kriti Sen", ""]]}, {"id": "1608.04062", "submitter": "Zhangyang Wang", "authors": "Zhangyang Wang, Shiyu Chang, Qing Ling, Shuai Huang, Xia Hu, Honghui\n  Shi, Thomas S. Huang", "title": "Stacked Approximated Regression Machine: A Simple Deep Learning Approach", "comments": "This manuscript has been withdrawn by the authors. Please see the\n  updated text for details", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the agreement of my coauthors, I Zhangyang Wang would like to withdraw\nthe manuscript \"Stacked Approximated Regression Machine: A Simple Deep Learning\nApproach\". Some experimental procedures were not included in the manuscript,\nwhich makes a part of important claims not meaningful. In the relevant\nresearch, I was solely responsible for carrying out the experiments; the other\ncoauthors joined in the discussions leading to the main algorithm.\n  Please see the updated text for more details.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 05:35:11 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2016 17:46:13 GMT"}], "update_date": "2016-10-02", "authors_parsed": [["Wang", "Zhangyang", ""], ["Chang", "Shiyu", ""], ["Ling", "Qing", ""], ["Huang", "Shuai", ""], ["Hu", "Xia", ""], ["Shi", "Honghui", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1608.04064", "submitter": "Ihsan Ullah", "authors": "Ihsan Ullah and Alfredo Petrosino", "title": "About Pyramid Structure in Convolutional Neural Networks", "comments": "Published in 2016 International Joint Conference on Neural Networks\n  (IJCNN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNN) brought revolution without any doubt\nto various challenging tasks, mainly in computer vision. However, their model\ndesigning still requires attention to reduce number of learnable parameters,\nwith no meaningful reduction in performance. In this paper we investigate to\nwhat extend CNN may take advantage of pyramid structure typical of biological\nneurons. A generalized statement over convolutional layers from input till\nfully connected layer is introduced that helps further in understanding and\ndesigning a successful deep network. It reduces ambiguity, number of\nparameters, and their size on disk without degrading overall accuracy.\nPerformance are shown on state-of-the-art models for MNIST, Cifar-10,\nCifar-100, and ImageNet-12 datasets. Despite more than 80% reduction in\nparameters for Caffe_LENET, challenging results are obtained. Further, despite\n10-20% reduction in training data along with 10-40% reduction in parameters for\nAlexNet model and its variations, competitive results are achieved when\ncompared to similar well-engineered deeper architectures.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 06:03:09 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Ullah", "Ihsan", ""], ["Petrosino", "Alfredo", ""]]}, {"id": "1608.04080", "submitter": "Sungho Shin", "authors": "Sungho Shin and Wonyong Sung", "title": "Dynamic Hand Gesture Recognition for Wearable Devices with Low\n  Complexity Recurrent Neural Networks", "comments": "This paper was accepted in ISCAS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gesture recognition is a very essential technology for many wearable devices.\nWhile previous algorithms are mostly based on statistical methods including the\nhidden Markov model, we develop two dynamic hand gesture recognition techniques\nusing low complexity recurrent neural network (RNN) algorithms. One is based on\nvideo signal and employs a combined structure of a convolutional neural network\n(CNN) and an RNN. The other uses accelerometer data and only requires an RNN.\nFixed-point optimization that quantizes most of the weights into two bits is\nconducted to optimize the amount of memory size for weight storage and reduce\nthe power consumption in hardware and software based implementations.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 09:32:17 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Shin", "Sungho", ""], ["Sung", "Wonyong", ""]]}, {"id": "1608.04117", "submitter": "Michal Drozdzal", "authors": "Michal Drozdzal, Eugene Vorontsov, Gabriel Chartrand, Samuel Kadoury,\n  Chris Pal", "title": "The Importance of Skip Connections in Biomedical Image Segmentation", "comments": "Accepted to 2nd Workshop on Deep Learning in Medical Image Analysis\n  (DLMIA 2016); Added references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the influence of both long and short skip connections\non Fully Convolutional Networks (FCN) for biomedical image segmentation. In\nstandard FCNs, only long skip connections are used to skip features from the\ncontracting path to the expanding path in order to recover spatial information\nlost during downsampling. We extend FCNs by adding short skip connections, that\nare similar to the ones introduced in residual networks, in order to build very\ndeep FCNs (of hundreds of layers). A review of the gradient flow confirms that\nfor a very deep FCN it is beneficial to have both long and short skip\nconnections. Finally, we show that a very deep FCN can achieve\nnear-to-state-of-the-art results on the EM dataset without any further\npost-processing.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 17:10:30 GMT"}, {"version": "v2", "created": "Thu, 22 Sep 2016 20:14:09 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Drozdzal", "Michal", ""], ["Vorontsov", "Eugene", ""], ["Chartrand", "Gabriel", ""], ["Kadoury", "Samuel", ""], ["Pal", "Chris", ""]]}, {"id": "1608.04170", "submitter": "Zhiqiang Xia", "authors": "Zhiqiang Xia, Ce Zhu, Zhengtao Wang, Qi Guo, Yipeng Liu", "title": "Every Filter Extracts A Specific Texture In Convolutional Neural\n  Networks", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many works have concentrated on visualizing and understanding the inner\nmechanism of convolutional neural networks (CNNs) by generating images that\nactivate some specific neurons, which is called deep visualization. However, it\nis still unclear what the filters extract from images intuitively. In this\npaper, we propose a modified code inversion algorithm, called feature map\ninversion, to understand the function of filter of interest in CNNs. We reveal\nthat every filter extracts a specific texture. The texture from higher layer\ncontains more colours and more intricate structures. We also demonstrate that\nstyle of images could be a combination of these texture primitives. Two methods\nare proposed to reallocate energy distribution of feature maps randomly and\npurposefully. Then, we inverse the modified code and generate images of diverse\nstyles. With these results, we provide an explanation about why Gram matrix of\nfeature maps \\cite{Gatys_2016_CVPR} could represent image style.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 02:47:23 GMT"}, {"version": "v2", "created": "Thu, 18 Aug 2016 02:56:25 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Xia", "Zhiqiang", ""], ["Zhu", "Ce", ""], ["Wang", "Zhengtao", ""], ["Guo", "Qi", ""], ["Liu", "Yipeng", ""]]}, {"id": "1608.04187", "submitter": "Hao Zhu", "authors": "Hao Zhu, Qing Wang, Jingyi Yu", "title": "Occlusion-Model Guided Anti-Occlusion Depth Estimation in Light Field", "comments": "19 pages, 13 figures, pdflatex", "journal-ref": null, "doi": "10.1109/JSTSP.2017.2730818", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occlusion is one of the most challenging problems in depth estimation.\nPrevious work has modeled the single-occluder occlusion in light field and get\ngood results, however it is still difficult to obtain accurate depth for\nmulti-occluder occlusion. In this paper, we explore the multi-occluder\nocclusion model in light field, and derive the occluder-consistency between the\nspatial and angular space which is used as a guidance to select the un-occluded\nviews for each candidate occlusion point. Then an anti-occlusion energy\nfunction is built to regularize depth map. The experimental results on public\nlight field datasets have demonstrated the advantages of the proposed algorithm\ncompared with other state-of-the-art light field depth estimation algorithms,\nespecially in multi-occluder areas.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 06:21:24 GMT"}, {"version": "v2", "created": "Thu, 18 Aug 2016 06:03:09 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Zhu", "Hao", ""], ["Wang", "Qing", ""], ["Yu", "Jingyi", ""]]}, {"id": "1608.04188", "submitter": "Xin Jin", "authors": "Xin Jin and Xiaoyang Tan", "title": "Face Alignment In-the-Wild: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last two decades, face alignment or localizing fiducial facial\npoints has received increasing attention owing to its comprehensive\napplications in automatic face analysis. However, such a task has proven\nextremely challenging in unconstrained environments due to many confounding\nfactors, such as pose, occlusions, expression and illumination. While numerous\ntechniques have been developed to address these challenges, this problem is\nstill far away from being solved. In this survey, we present an up-to-date\ncritical review of the existing literatures on face alignment, focusing on\nthose methods addressing overall difficulties and challenges of this topic\nunder uncontrolled conditions. Specifically, we categorize existing face\nalignment techniques, present detailed descriptions of the prominent algorithms\nwithin each category, and discuss their advantages and disadvantages.\nFurthermore, we organize special discussions on the practical aspects of face\nalignment in-the-wild, towards the development of a robust face alignment\nsystem. In addition, we show performance statistics of the state of the art,\nand conclude this paper with several promising directions for future research.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 06:32:07 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Jin", "Xin", ""], ["Tan", "Xiaoyang", ""]]}, {"id": "1608.04200", "submitter": "Zhiwu Huang", "authors": "Zhiwu Huang, Ruiping Wang, Shiguang Shan, Luc Van Gool and Xilin Chen", "title": "Cross Euclidean-to-Riemannian Metric Learning with Application to Face\n  Recognition from Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Riemannian manifolds have been widely employed for video representations in\nvisual classification tasks including video-based face recognition. The success\nmainly derives from learning a discriminant Riemannian metric which encodes the\nnon-linear geometry of the underlying Riemannian manifolds. In this paper, we\npropose a novel metric learning framework to learn a distance metric across a\nEuclidean space and a Riemannian manifold to fuse the average appearance and\npattern variation of faces within one video. The proposed metric learning\nframework can handle three typical tasks of video-based face recognition:\nVideo-to-Still, Still-to-Video and Video-to-Video settings. To accomplish this\nnew framework, by exploiting typical Riemannian geometries for kernel\nembedding, we map the source Euclidean space and Riemannian manifold into a\ncommon Euclidean subspace, each through a corresponding high-dimensional\nReproducing Kernel Hilbert Space (RKHS). With this mapping, the problem of\nlearning a cross-view metric between the two source heterogeneous spaces can be\nexpressed as learning a single-view Euclidean distance metric in the target\ncommon Euclidean space. By learning information on heterogeneous data with the\nshared label, the discriminant metric in the common space improves face\nrecognition from videos. Extensive experiments on four challenging video face\ndatabases demonstrate that the proposed framework has a clear advantage over\nthe state-of-the-art methods in the three classical video-based face\nrecognition tasks.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 07:54:55 GMT"}, {"version": "v2", "created": "Fri, 6 Jan 2017 22:01:26 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Huang", "Zhiwu", ""], ["Wang", "Ruiping", ""], ["Shan", "Shiguang", ""], ["Van Gool", "Luc", ""], ["Chen", "Xilin", ""]]}, {"id": "1608.04224", "submitter": "Praveen Krishnan", "authors": "Praveen Krishnan and C.V. Jawahar", "title": "Generating Synthetic Data for Text Recognition", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating synthetic images is an art which emulates the natural process of\nimage generation in a closest possible manner. In this work, we exploit such a\nframework for data generation in handwritten domain. We render synthetic data\nusing open source fonts and incorporate data augmentation schemes. As part of\nthis work, we release 9M synthetic handwritten word image corpus which could be\nuseful for training deep network architectures and advancing the performance in\nhandwritten word spotting and recognition tasks.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 10:13:46 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Krishnan", "Praveen", ""], ["Jawahar", "C. V.", ""]]}, {"id": "1608.04233", "submitter": "Zhiwu Huang", "authors": "Zhiwu Huang and Luc Van Gool", "title": "A Riemannian Network for SPD Matrix Learning", "comments": "Revised arXiv version, AAAI-17 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetric Positive Definite (SPD) matrix learning methods have become popular\nin many image and video processing tasks, thanks to their ability to learn\nappropriate statistical representations while respecting Riemannian geometry of\nunderlying SPD manifolds. In this paper we build a Riemannian network\narchitecture to open up a new direction of SPD matrix non-linear learning in a\ndeep model. In particular, we devise bilinear mapping layers to transform input\nSPD matrices to more desirable SPD matrices, exploit eigenvalue rectification\nlayers to apply a non-linear activation function to the new SPD matrices, and\ndesign an eigenvalue logarithm layer to perform Riemannian computing on the\nresulting SPD matrices for regular output layers. For training the proposed\ndeep network, we exploit a new backpropagation with a variant of stochastic\ngradient descent on Stiefel manifolds to update the structured connection\nweights and the involved SPD matrix data. We show through experiments that the\nproposed SPD matrix network can be simply trained and outperform existing SPD\nmatrix learning and state-of-the-art methods in three typical visual\nclassification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 11:00:16 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 15:43:55 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Huang", "Zhiwu", ""], ["Van Gool", "Luc", ""]]}, {"id": "1608.04236", "submitter": "Andrew Brock", "authors": "Andrew Brock, Theodore Lim, J.M. Ritchie, Nick Weston", "title": "Generative and Discriminative Voxel Modeling with Convolutional Neural\n  Networks", "comments": "9 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When working with three-dimensional data, choice of representation is key. We\nexplore voxel-based models, and present evidence for the viability of\nvoxellated representations in applications including shape modeling and object\nclassification. Our key contributions are methods for training voxel-based\nvariational autoencoders, a user interface for exploring the latent space\nlearned by the autoencoder, and a deep convolutional neural network\narchitecture for object classification. We address challenges unique to\nvoxel-based representations, and empirically evaluate our models on the\nModelNet benchmark, where we demonstrate a 51.5% relative improvement in the\nstate of the art for object classification.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 11:14:35 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 08:06:24 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Brock", "Andrew", ""], ["Lim", "Theodore", ""], ["Ritchie", "J. M.", ""], ["Weston", "Nick", ""]]}, {"id": "1608.04267", "submitter": "Zihan Zhou", "authors": "Zihan Zhou, Farshid Farhat, James Z. Wang", "title": "Detecting Dominant Vanishing Points in Natural Scenes with Application\n  to Composition-Sensitive Image Retrieval", "comments": "15 pages, 18 figures, to appear in IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear perspective is widely used in landscape photography to create the\nimpression of depth on a 2D photo. Automated understanding of linear\nperspective in landscape photography has several real-world applications,\nincluding aesthetics assessment, image retrieval, and on-site feedback for\nphoto composition, yet adequate automated understanding has been elusive. We\naddress this problem by detecting the dominant vanishing point and the\nassociated line structures in a photo. However, natural landscape scenes pose\ngreat technical challenges because often the inadequate number of strong edges\nconverging to the dominant vanishing point is inadequate. To overcome this\ndifficulty, we propose a novel vanishing point detection method that exploits\nglobal structures in the scene via contour detection. We show that our method\nsignificantly outperforms state-of-the-art methods on a public ground truth\nlandscape image dataset that we have created. Based on the detection results,\nwe further demonstrate how our approach to linear perspective understanding\nprovides on-site guidance to amateur photographers on their work through a\nnovel viewpoint-specific image retrieval system.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 13:48:22 GMT"}, {"version": "v2", "created": "Sat, 13 May 2017 14:58:05 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Zhou", "Zihan", ""], ["Farhat", "Farshid", ""], ["Wang", "James Z.", ""]]}, {"id": "1608.04274", "submitter": "Andrew Calway Dr", "authors": "Pilailuck Panphattarasap and Andrew Calway", "title": "Visual place recognition using landmark distribution descriptors", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work by Suenderhauf et al. [1] demonstrated improved visual place\nrecognition using proposal regions coupled with features from convolutional\nneural networks (CNN) to match landmarks between views. In this work we extend\nthe approach by introducing descriptors built from landmark features which also\nencode the spatial distribution of the landmarks within a view. Matching\ndescriptors then enforces consistency of the relative positions of landmarks\nbetween views. This has a significant impact on performance. For example, in\nexperiments on 10 image-pair datasets, each consisting of 200 urban locations\nwith significant differences in viewing positions and conditions, we recorded\naverage precision of around 70% (at 100% recall), compared with 58% obtained\nusing whole image CNN features and 50% for the method in [1].\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 14:13:27 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Panphattarasap", "Pilailuck", ""], ["Calway", "Andrew", ""]]}, {"id": "1608.04307", "submitter": "Zhangjie Cao", "authors": "Zhangjie Cao, Mingsheng Long, Qiang Yang", "title": "Transitive Hashing Network for Heterogeneous Multimedia Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing has been widely applied to large-scale multimedia retrieval due to\nthe storage and retrieval efficiency. Cross-modal hashing enables efficient\nretrieval from database of one modality in response to a query of another\nmodality. Existing work on cross-modal hashing assumes heterogeneous\nrelationship across modalities for hash function learning. In this paper, we\nrelax the strong assumption by only requiring such heterogeneous relationship\nin an auxiliary dataset different from the query/database domain. We craft a\nhybrid deep architecture to simultaneously learn the cross-modal correlation\nfrom the auxiliary dataset, and align the dataset distributions between the\nauxiliary dataset and the query/database domain, which generates transitive\nhash codes for heterogeneous multimedia retrieval. Extensive experiments\nexhibit that the proposed approach yields state of the art multimedia retrieval\nperformance on public datasets, i.e. NUS-WIDE, ImageNet-YahooQA.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 15:36:41 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Cao", "Zhangjie", ""], ["Long", "Mingsheng", ""], ["Yang", "Qiang", ""]]}, {"id": "1608.04314", "submitter": "Miaojing Shi", "authors": "Miaojing Shi and Vittorio Ferrari", "title": "Weakly Supervised Object Localization Using Size Estimates", "comments": "ECCV 2016 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique for weakly supervised object localization (WSOL),\nbuilding on the observation that WSOL algorithms usually work better on images\nwith bigger objects. Instead of training the object detector on the entire\ntraining set at the same time, we propose a curriculum learning strategy to\nfeed training images into the WSOL learning loop in an order from images\ncontaining bigger objects down to smaller ones. To automatically determine the\norder, we train a regressor to estimate the size of the object given the whole\nimage as input. Furthermore, we use these size estimates to further improve the\nre-localization step of WSOL by assigning weights to object proposals according\nto how close their size matches the estimated object size. We demonstrate the\neffectiveness of using size order and size weighting on the challenging PASCAL\nVOC 2007 dataset, where we achieve a significant improvement over existing\nstate-of-the-art WSOL techniques.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 16:07:24 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 11:31:41 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Shi", "Miaojing", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1608.04337", "submitter": "Baoyuan Liu", "authors": "Min Wang, Baoyuan Liu, Hassan Foroosh", "title": "Design of Efficient Convolutional Layers using Single Intra-channel\n  Convolution, Topological Subdivisioning and Spatial \"Bottleneck\" Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks achieve remarkable visual recognition\nperformance, at the cost of high computational complexity. In this paper, we\nhave a new design of efficient convolutional layers based on three schemes. The\n3D convolution operation in a convolutional layer can be considered as\nperforming spatial convolution in each channel and linear projection across\nchannels simultaneously. By unravelling them and arranging the spatial\nconvolution sequentially, the proposed layer is composed of a single\nintra-channel convolution, of which the computation is negligible, and a linear\nchannel projection. A topological subdivisioning is adopted to reduce the\nconnection between the input channels and output channels. Additionally, we\nalso introduce a spatial \"bottleneck\" structure that utilizes a\nconvolution-projection-deconvolution pipeline to take advantage of the\ncorrelation between adjacent pixels in the input. Our experiments demonstrate\nthat the proposed layers remarkably outperform the standard convolutional\nlayers with regard to accuracy/complexity ratio. Our models achieve similar\naccuracy to VGG, ResNet-50, ResNet-101 while requiring 42, 4.5, 6.5 times less\ncomputation respectively.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 17:35:56 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 12:26:19 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Wang", "Min", ""], ["Liu", "Baoyuan", ""], ["Foroosh", "Hassan", ""]]}, {"id": "1608.04339", "submitter": "Yi Zhu", "authors": "Yi Zhu and Shawn Newsam", "title": "Depth2Action: Exploring Embedded Depth for Large-Scale Action\n  Recognition", "comments": "ECCVW 2016, Web-scale Vision and Social Media (VSM) workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper performs the first investigation into depth for large-scale human\naction recognition in video where the depth cues are estimated from the videos\nthemselves. We develop a new framework called depth2action and experiment\nthoroughly into how best to incorporate the depth information. We introduce\nspatio-temporal depth normalization (STDN) to enforce temporal consistency in\nour estimated depth sequences. We also propose modified depth motion maps\n(MDMM) to capture the subtle temporal changes in depth. These two components\nsignificantly improve the action recognition performance. We evaluate our\ndepth2action framework on three large-scale action recognition video\nbenchmarks. Our model achieves state-of-the-art performance when combined with\nappearance and motion information thus demonstrating that depth2action is\nindeed complementary to existing approaches.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 17:42:36 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Zhu", "Yi", ""], ["Newsam", "Shawn", ""]]}, {"id": "1608.04342", "submitter": "Elena Garces", "authors": "Elena Garces, Jose I. Echevarria, Wen Zhang, Hongzhi Wu, Kun Zhou,\n  Diego Gutierrez", "title": "Intrinsic Light Field Images", "comments": null, "journal-ref": "Computer Graphics Forum 2017", "doi": "10.1111/cgf.13154", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to automatically decompose a light field into its\nintrinsic shading and albedo components. Contrary to previous work targeted to\n2D single images and videos, a light field is a 4D structure that captures\nnon-integrated incoming radiance over a discrete angular domain. This higher\ndimensionality of the problem renders previous state-of-the-art algorithms\nimpractical either due to their cost of processing a single 2D slice, or their\ninability to enforce proper coherence in additional dimensions. We propose a\nnew decomposition algorithm that jointly optimizes the whole light field data\nfor proper angular coherence. For efficiency, we extend Retinex theory, working\non the gradient domain, where new albedo and occlusion terms are introduced.\nResults show our method provides 4D intrinsic decompositions difficult to\nachieve with previous state-of-the-art algorithms. We further provide a\ncomprehensive analysis and comparisons with existing intrinsic image/video\ndecomposition methods on light field images.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 17:50:16 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 15:22:59 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Garces", "Elena", ""], ["Echevarria", "Jose I.", ""], ["Zhang", "Wen", ""], ["Wu", "Hongzhi", ""], ["Zhou", "Kun", ""], ["Gutierrez", "Diego", ""]]}, {"id": "1608.04348", "submitter": "Jeff Calder", "authors": "Bilal Abbasi, Jeff Calder, Adam M. Oberman", "title": "Anomaly detection and classification for streaming data using PDEs", "comments": null, "journal-ref": "SIAM Journal on Applied Math, 78(2), 921--941, 2018", "doi": "10.1137/17M1121184", "report-no": null, "categories": "cs.LG cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nondominated sorting, also called Pareto Depth Analysis (PDA), is widely used\nin multi-objective optimization and has recently found important applications\nin multi-criteria anomaly detection. Recently, a partial differential equation\n(PDE) continuum limit was discovered for nondominated sorting leading to a very\nfast approximate sorting algorithm called PDE-based ranking. We propose in this\npaper a fast real-time streaming version of the PDA algorithm for anomaly\ndetection that exploits the computational advantages of PDE continuum limits.\nFurthermore, we derive new PDE continuum limits for sorting points within their\nnondominated layers and show how the new PDEs can be used to classify anomalies\nbased on which criterion was more significantly violated. We also prove\nstatistical convergence rates for PDE-based ranking, and present the results of\nnumerical experiments with both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 18:03:51 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 19:50:07 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Abbasi", "Bilal", ""], ["Calder", "Jeff", ""], ["Oberman", "Adam M.", ""]]}, {"id": "1608.04363", "submitter": "Justin Salamon", "authors": "Justin Salamon and Juan Pablo Bello", "title": "Deep Convolutional Neural Networks and Data Augmentation for\n  Environmental Sound Classification", "comments": "Accepted November 2016, IEEE Signal Processing Letters. Copyright\n  IEEE. Personal use of this material is permitted. Permission from IEEE must\n  be obtained for all other uses, in any current or future media, including\n  reprinting/republishing this material, creating new collective works, for\n  resale or redistribution, or reuse of any copyrighted component of this work\n  in other works", "journal-ref": null, "doi": "10.1109/LSP.2017.2657381", "report-no": null, "categories": "cs.SD cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of deep convolutional neural networks (CNN) to learn\ndiscriminative spectro-temporal patterns makes them well suited to\nenvironmental sound classification. However, the relative scarcity of labeled\ndata has impeded the exploitation of this family of high-capacity models. This\nstudy has two primary contributions: first, we propose a deep convolutional\nneural network architecture for environmental sound classification. Second, we\npropose the use of audio data augmentation for overcoming the problem of data\nscarcity and explore the influence of different augmentations on the\nperformance of the proposed CNN architecture. Combined with data augmentation,\nthe proposed model produces state-of-the-art results for environmental sound\nclassification. We show that the improved performance stems from the\ncombination of a deep, high-capacity model and an augmented training set: this\ncombination outperforms both the proposed CNN without augmentation and a\n\"shallow\" dictionary learning model with augmentation. Finally, we examine the\ninfluence of each augmentation on the model's classification accuracy for each\nclass, and observe that the accuracy for each class is influenced differently\nby each augmentation, suggesting that the performance of the model could be\nimproved further by applying class-conditional data augmentation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 18:57:10 GMT"}, {"version": "v2", "created": "Mon, 28 Nov 2016 17:48:04 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Salamon", "Justin", ""], ["Bello", "Juan Pablo", ""]]}, {"id": "1608.04369", "submitter": "Edward J. Kim", "authors": "Edward J. Kim and Robert J. Brunner", "title": "Star-galaxy Classification Using Deep Convolutional Neural Networks", "comments": "13 page, 13 figures. Accepted for publication in the MNRAS. Code\n  available at https://github.com/EdwardJKim/dl4astro", "journal-ref": null, "doi": "10.1093/mnras/stw2672", "report-no": null, "categories": "astro-ph.IM astro-ph.CO astro-ph.GA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing star-galaxy classifiers use the reduced summary information\nfrom catalogs, requiring careful feature extraction and selection. The latest\nadvances in machine learning that use deep convolutional neural networks allow\na machine to automatically learn the features directly from data, minimizing\nthe need for input from human experts. We present a star-galaxy classification\nframework that uses deep convolutional neural networks (ConvNets) directly on\nthe reduced, calibrated pixel values. Using data from the Sloan Digital Sky\nSurvey (SDSS) and the Canada-France-Hawaii Telescope Lensing Survey (CFHTLenS),\nwe demonstrate that ConvNets are able to produce accurate and well-calibrated\nprobabilistic classifications that are competitive with conventional machine\nlearning techniques. Future advances in deep learning may bring more success\nwith current and forthcoming photometric surveys, such as the Dark Energy\nSurvey (DES) and the Large Synoptic Survey Telescope (LSST), because deep\nneural networks require very little, manual feature engineering.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 19:21:58 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 15:03:38 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Kim", "Edward J.", ""], ["Brunner", "Robert J.", ""]]}, {"id": "1608.04381", "submitter": "Maitham Naeemi", "authors": "Maitham D Naeemi, Adam M Alessio, Sohini Roychowdhury", "title": "Automated Selection of Uniform Regions for CT Image Quality Detection", "comments": "5 pages, 8 figures, Asilomar Conference on Signals, Systems, and\n  Computers", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CT images are widely used in pathology detection and follow-up treatment\nprocedures. Accurate identification of pathological features requires\ndiagnostic quality CT images with minimal noise and artifact variation. In this\nwork, a novel Fourier-transform based metric for image quality (IQ) estimation\nis presented that correlates to additive CT image noise. In the proposed\nmethod, two windowed CT image subset regions are analyzed together to identify\nthe extent of variation in the corresponding Fourier-domain spectrum. The two\nsquare windows are chosen such that their center pixels coincide and one window\nis a subset of the other. The Fourier-domain spectral difference between these\ntwo sub-sampled windows is then used to isolate spatial regions-of-interest\n(ROI) with low signal variation (ROI-LV) and high signal variation (ROI-HV),\nrespectively. Finally, the spatial variance ($var$), standard deviation\n($std$), coefficient of variance ($cov$) and the fraction of abdominal ROI\npixels in ROI-LV ($\\nu'(q)$), are analyzed with respect to CT image noise. For\nthe phantom CT images, $var$ and $std$ correlate to CT image noise ($|r|>0.76$\n($p\\ll0.001$)), though not as well as $\\nu'(q)$ ($r=0.96$ ($p\\ll0.001$)).\nHowever, for the combined phantom and patient CT images, $var$ and $std$ do not\ncorrelate well with CT image noise ($|r|<0.46$ ($p\\ll0.001$)) as compared to\n$\\nu'(q)$ ($r=0.95$ ($p\\ll0.001$)). Thus, the proposed method and the metric,\n$\\nu'(q)$, can be useful to quantitatively estimate CT image noise.\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2016 06:56:04 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 19:08:58 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Naeemi", "Maitham D", ""], ["Alessio", "Adam M", ""], ["Roychowdhury", "Sohini", ""]]}, {"id": "1608.04489", "submitter": "Ferdous Barbhuiya", "authors": "Rahul Islam, Karan Ahuja, Sandip Karmakar, Ferdous Barbhuiya", "title": "SenTion: A framework for Sensing Facial Expressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expressions are an integral part of human cognition and communication,\nand can be applied in various real life applications. A vital precursor to\naccurate expression recognition is feature extraction. In this paper, we\npropose SenTion: A framework for sensing facial expressions. We propose a novel\nperson independent and scale invariant method of extracting Inter Vector Angles\n(IVA) as geometric features, which proves to be robust and reliable across\ndatabases. SenTion employs a novel framework of combining geometric (IVA's) and\nappearance based features (Histogram of Gradients) to create a hybrid model,\nthat achieves state of the art recognition accuracy. We evaluate the\nperformance of SenTion on two famous face expression data set, namely: CK+ and\nJAFFE; and subsequently evaluate the viability of facial expression systems by\na user study. Extensive experiments showed that SenTion framework yielded\ndramatic improvements in facial expression recognition and could be employed in\nreal-world applications with low resolution imaging and minimal computational\nresources in real-time, achieving 15-18 fps on a 2.4 GHz CPU with no GPU.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 05:27:30 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Islam", "Rahul", ""], ["Ahuja", "Karan", ""], ["Karmakar", "Sandip", ""], ["Barbhuiya", "Ferdous", ""]]}, {"id": "1608.04493", "submitter": "Anbang Yao", "authors": "Yiwen Guo, Anbang Yao, Yurong Chen", "title": "Dynamic Network Surgery for Efficient DNNs", "comments": "Accepted by NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has become a ubiquitous technology to improve machine\nintelligence. However, most of the existing deep models are structurally very\ncomplex, making them difficult to be deployed on the mobile platforms with\nlimited computational power. In this paper, we propose a novel network\ncompression method called dynamic network surgery, which can remarkably reduce\nthe network complexity by making on-the-fly connection pruning. Unlike the\nprevious methods which accomplish this task in a greedy way, we properly\nincorporate connection splicing into the whole process to avoid incorrect\npruning and make it as a continual network maintenance. The effectiveness of\nour method is proved with experiments. Without any accuracy loss, our method\ncan efficiently compress the number of parameters in LeNet-5 and AlexNet by a\nfactor of $\\bm{108}\\times$ and $\\bm{17.7}\\times$ respectively, proving that it\noutperforms the recent pruning method by considerable margins. Code and some\nmodels are available at https://github.com/yiwenguo/Dynamic-Network-Surgery.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 06:23:05 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 00:17:25 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Guo", "Yiwen", ""], ["Yao", "Anbang", ""], ["Chen", "Yurong", ""]]}, {"id": "1608.04509", "submitter": "Zhe Ji", "authors": "Chunping Zhang, Zhe Ji, Qing Wang", "title": "Unconstrained Two-parallel-plane Model for Focused Plenoptic Cameras\n  Calibration", "comments": "20 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The plenoptic camera can capture both angular and spatial information of the\nrays, enabling 3D reconstruction by single exposure. The geometry of the\nrecovered scene structure is affected by the calibration of the plenoptic\ncamera significantly. In this paper, we propose a novel unconstrained\ntwo-parallel-plane (TPP) model with 7 parameters to describe a 4D light field.\nBy reconstructing scene points from ray-ray association, a 3D projective\ntransformation is deduced to establish the relationship between the scene\nstructure and the TPP parameters. Based on the transformation, we simplify the\nfocused plenoptic camera as a TPP model and calibrate its intrinsic parameters.\nOur calibration method includes a close-form solution and a nonlinear\noptimization by minimizing re-projection error. Experiments on both simulated\ndata and real scene data verify the performance of the calibration on the\nfocused plenoptic camera.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 08:02:55 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Zhang", "Chunping", ""], ["Ji", "Zhe", ""], ["Wang", "Qing", ""]]}, {"id": "1608.04517", "submitter": "Zhiyuan Zha", "authors": "Zhiyuan Zha, Bihan Wen, Jiachao Zhang, Jiantao Zhou and Ce Zhu", "title": "A Comparative Study for the Nuclear Norms Minimization Methods", "comments": null, "journal-ref": "2019 IEEE International Conference on Image Processing", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The nuclear norm minimization (NNM) is commonly used to approximate the\nmatrix rank by shrinking all singular values equally. However, the singular\nvalues have clear physical meanings in many practical problems, and NNM may not\nbe able to faithfully approximate the matrix rank. To alleviate the\nabove-mentioned limitation of NNM, recent studies have suggested that the\nweighted nuclear norm minimization (WNNM) can achieve a better rank estimation\nthan NNM, which heuristically set the weight being inverse to the singular\nvalues. However, it still lacks a rigorous explanation why WNNM is more\neffective than NMM in various applications. In this paper, we analyze NNM and\nWNNM from the perspective of group sparse representation (GSR). Concretely, an\nadaptive dictionary learning method is devised to connect the rank minimization\nand GSR models. Based on the proposed dictionary, we prove that NNM and WNNM\nare equivalent to L1-norm minimization and the weighted L1-norm minimization in\nGSR, respectively. Inspired by enhancing sparsity of the weighted L1-norm\nminimization in comparison with L1-norm minimization in sparse representation,\nwe thus explain that WNNM is more effective than NMM. By integrating the image\nnonlocal self-similarity (NSS) prior with the WNNM model, we then apply it to\nsolve the image denoising problem. Experimental results demonstrate that WNNM\nis more effective than NNM and outperforms several state-of-the-art methods in\nboth objective and perceptual quality.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 08:31:35 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 03:00:13 GMT"}, {"version": "v3", "created": "Wed, 17 May 2017 01:39:00 GMT"}, {"version": "v4", "created": "Fri, 10 May 2019 08:58:01 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Zha", "Zhiyuan", ""], ["Wen", "Bihan", ""], ["Zhang", "Jiachao", ""], ["Zhou", "Jiantao", ""], ["Zhu", "Ce", ""]]}, {"id": "1608.04642", "submitter": "Peter Bertholet", "authors": "Peter Bertholet, Alexandru-Eugen Ichim, Matthias Zwicker", "title": "Temporally Consistent Motion Segmentation from RGB-D Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for temporally consistent motion segmentation from RGB-D\nvideos assuming a piecewise rigid motion model. We formulate global energies\nover entire RGB-D sequences in terms of the segmentation of each frame into a\nnumber of objects, and the rigid motion of each object through the sequence. We\ndevelop a novel initialization procedure that clusters feature tracks obtained\nfrom the RGB data by leveraging the depth information. We minimize the energy\nusing a coordinate descent approach that includes novel techniques to assemble\nobject motion hypotheses. A main benefit of our approach is that it enables us\nto fuse consistently labeled object segments from all RGB-D frames of an input\nsequence into individual 3D object reconstructions.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 15:56:11 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Bertholet", "Peter", ""], ["Ichim", "Alexandru-Eugen", ""], ["Zwicker", "Matthias", ""]]}, {"id": "1608.04644", "submitter": "Nicholas Carlini", "authors": "Nicholas Carlini and David Wagner", "title": "Towards Evaluating the Robustness of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks provide state-of-the-art results for most machine learning\ntasks. Unfortunately, neural networks are vulnerable to adversarial examples:\ngiven an input $x$ and any target classification $t$, it is possible to find a\nnew input $x'$ that is similar to $x$ but classified as $t$. This makes it\ndifficult to apply neural networks in security-critical areas. Defensive\ndistillation is a recently proposed approach that can take an arbitrary neural\nnetwork, and increase its robustness, reducing the success rate of current\nattacks' ability to find adversarial examples from $95\\%$ to $0.5\\%$.\n  In this paper, we demonstrate that defensive distillation does not\nsignificantly increase the robustness of neural networks by introducing three\nnew attack algorithms that are successful on both distilled and undistilled\nneural networks with $100\\%$ probability. Our attacks are tailored to three\ndistance metrics used previously in the literature, and when compared to\nprevious adversarial example generation algorithms, our attacks are often much\nmore effective (and never worse). Furthermore, we propose using high-confidence\nadversarial examples in a simple transferability test we show can also be used\nto break defensive distillation. We hope our attacks will be used as a\nbenchmark in future defense attempts to create neural networks that resist\nadversarial examples.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 15:59:35 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 17:46:16 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Carlini", "Nicholas", ""], ["Wagner", "David", ""]]}, {"id": "1608.04664", "submitter": "Stefanos Eleftheriadis", "authors": "Stefanos Eleftheriadis, Ognjen Rudovic, Marc P. Deisenroth, Maja\n  Pantic", "title": "Variational Gaussian Process Auto-Encoder for Ordinal Prediction of\n  Facial Action Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the task of simultaneous feature fusion and modeling of discrete\nordinal outputs. We propose a novel Gaussian process(GP) auto-encoder modeling\napproach. In particular, we introduce GP encoders to project multiple observed\nfeatures onto a latent space, while GP decoders are responsible for\nreconstructing the original features. Inference is performed in a novel\nvariational framework, where the recovered latent representations are further\nconstrained by the ordinal output labels. In this way, we seamlessly integrate\nthe ordinal structure in the learned manifold, while attaining robust fusion of\nthe input features. We demonstrate the representation abilities of our model on\nbenchmark datasets from machine learning and affect analysis. We further\nevaluate the model on the tasks of feature fusion and joint ordinal prediction\nof facial action units. Our experiments demonstrate the benefits of the\nproposed approach compared to the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 16:31:39 GMT"}, {"version": "v2", "created": "Mon, 5 Sep 2016 21:25:48 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Eleftheriadis", "Stefanos", ""], ["Rudovic", "Ognjen", ""], ["Deisenroth", "Marc P.", ""], ["Pantic", "Maja", ""]]}, {"id": "1608.04667", "submitter": "Lovedeep Gondara", "authors": "Lovedeep Gondara", "title": "Medical image denoising using convolutional denoising autoencoders", "comments": "To appear: 6 pages, paper to be published at the Fourth Workshop on\n  Data Mining in Biomedical Informatics and Healthcare at ICDM, 2016", "journal-ref": null, "doi": "10.1109/ICDMW.2016.0041", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image denoising is an important pre-processing step in medical image\nanalysis. Different algorithms have been proposed in past three decades with\nvarying denoising performances. More recently, having outperformed all\nconventional methods, deep learning based models have shown a great promise.\nThese methods are however limited for requirement of large training sample size\nand high computational costs. In this paper we show that using small sample\nsize, denoising autoencoders constructed using convolutional layers can be used\nfor efficient denoising of medical images. Heterogeneous images can be combined\nto boost sample size for increased denoising performance. Simplest of networks\ncan reconstruct images with corruption levels so high that noise and signal are\nnot differentiable to human eye.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 16:39:20 GMT"}, {"version": "v2", "created": "Sun, 18 Sep 2016 01:09:57 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Gondara", "Lovedeep", ""]]}, {"id": "1608.04695", "submitter": "Adrian Barbu", "authors": "Ajay Gupta, Adrian Barbu", "title": "Parameterized Principal Component Analysis", "comments": "36 pages, 15 figures", "journal-ref": "Pattern Recognition 78, No. 6, 215-227, 2018", "doi": "10.1016/j.patcog.2018.01.018", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When modeling multivariate data, one might have an extra parameter of\ncontextual information that could be used to treat some observations as more\nsimilar to others. For example, images of faces can vary by age, and one would\nexpect the face of a 40 year old to be more similar to the face of a 30 year\nold than to a baby face. We introduce a novel manifold approximation method,\nparameterized principal component analysis (PPCA) that models data with linear\nsubspaces that change continuously according to the extra parameter of\ncontextual information (e.g. age), instead of ad-hoc atlases. Special care has\nbeen taken in the loss function and the optimization method to encourage\nsmoothly changing subspaces across the parameter values. The approach ensures\nthat each observation's projection will share information with observations\nthat have similar parameter values, but not with observations that have large\nparameter differences. We tested PPCA on artificial data based on known, smooth\nfunctions of an added parameter, as well as on three real datasets with\ndifferent types of parameters. We compared PPCA to PCA, sparse PCA and to\nindependent principal component analysis (IPCA), which groups observations by\ntheir parameter values and projects each group using PCA with no sharing of\ninformation for different groups. PPCA recovers the known functions with less\nerror and projects the datasets' test set observations with consistently less\nreconstruction error than IPCA does. In some cases where the manifold is truly\nnonlinear, PCA outperforms all the other manifold approximation methods\ncompared.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 18:23:13 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 19:16:36 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Gupta", "Ajay", ""], ["Barbu", "Adrian", ""]]}, {"id": "1608.04846", "submitter": "Po-Hsuan Chen", "authors": "Po-Hsuan Chen, Xia Zhu, Hejia Zhang, Javier S. Turek, Janice Chen,\n  Theodore L. Willke, Uri Hasson, Peter J. Ramadge", "title": "A Convolutional Autoencoder for Multi-Subject fMRI Data Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the most effective way to aggregate multi-subject fMRI data is a\nlong-standing and challenging problem. It is of increasing interest in\ncontemporary fMRI studies of human cognition due to the scarcity of data per\nsubject and the variability of brain anatomy and functional response across\nsubjects. Recent work on latent factor models shows promising results in this\ntask but this approach does not preserve spatial locality in the brain. We\nexamine two ways to combine the ideas of a factor model and a searchlight based\nanalysis to aggregate multi-subject fMRI data while preserving spatial\nlocality. We first do this directly by combining a recent factor method known\nas a shared response model with searchlight analysis. Then we design a\nmulti-view convolutional autoencoder for the same task. Both approaches\npreserve spatial locality and have competitive or better performance compared\nwith standard searchlight analysis and the shared response model applied across\nthe whole brain. We also report a system design to handle the computational\nchallenge of training the convolutional autoencoder.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 03:49:56 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Chen", "Po-Hsuan", ""], ["Zhu", "Xia", ""], ["Zhang", "Hejia", ""], ["Turek", "Javier S.", ""], ["Chen", "Janice", ""], ["Willke", "Theodore L.", ""], ["Hasson", "Uri", ""], ["Ramadge", "Peter J.", ""]]}, {"id": "1608.04902", "submitter": "Xiang Zhang", "authors": "Xiang Zhang, Jiarui Sun, Siwei Ma, Zhouchen Lin, Jian Zhang, Shiqi\n  Wang, Wen Gao", "title": "Globally Variance-Constrained Sparse Representation and Its Application\n  in Image Set Coding", "comments": null, "journal-ref": "IEEE Transactions on Image Processing 27 (2018) 3753-3765", "doi": "10.1109/TIP.2018.2823546", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representation leads to an efficient way to approximately recover a\nsignal by the linear composition of a few bases from a learnt dictionary, based\non which various successful applications have been achieved. However, in the\nscenario of data compression, its efficiency and popularity are hindered. It is\nbecause of the fact that encoding sparsely distributed coefficients may consume\nmore bits for representing the index of nonzero coefficients. Therefore,\nintroducing an accurate rate-constraint in sparse coding and dictionary\nlearning becomes meaningful, which has not been fully exploited in the context\nof sparse representation. According to the Shannon entropy inequality, the\nvariance of a Gaussian distributed data bounds its entropy, indicating the\nactual bitrate can be well estimated by its variance. Hence, a Globally\nVariance-Constrained Sparse Representation (GVCSR) model is proposed in this\nwork, where a variance-constrained rate term is introduced to the optimization\nprocess. Specifically, we employ the Alternating Direction Method of\nMultipliers (ADMM) to solve the non-convex optimization problem for sparse\ncoding and dictionary learning, both of them have shown the state-of-the-art\nrate-distortion performance for image representation. Furthermore, we\ninvestigate the potential of applying the GVCSR algorithm in the practical\nimage set compression, where the optimized dictionary is trained to efficiently\nrepresent the images captured in similar scenarios by implicitly utilizing\ninter-image correlations. Experimental results have demonstrated superior\nrate-distortion performance against the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 09:34:51 GMT"}, {"version": "v2", "created": "Thu, 3 May 2018 03:23:28 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Zhang", "Xiang", ""], ["Sun", "Jiarui", ""], ["Ma", "Siwei", ""], ["Lin", "Zhouchen", ""], ["Zhang", "Jian", ""], ["Wang", "Shiqi", ""], ["Gao", "Wen", ""]]}, {"id": "1608.04914", "submitter": "Zhiwu Huang", "authors": "Zhiwu Huang, Ruiping Wang, Xianqiu Li, Wenxian Liu, Shiguang Shan, Luc\n  Van Gool, and Xilin Chen", "title": "Geometry-aware Similarity Learning on SPD Manifolds for Visual\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetric Positive Definite (SPD) matrices have been widely used for data\nrepresentation in many visual recognition tasks. The success mainly attributes\nto learning discriminative SPD matrices with encoding the Riemannian geometry\nof the underlying SPD manifold. In this paper, we propose a geometry-aware SPD\nsimilarity learning (SPDSL) framework to learn discriminative SPD features by\ndirectly pursuing manifold-manifold transformation matrix of column full-rank.\nSpecifically, by exploiting the Riemannian geometry of the manifold of\nfixed-rank Positive Semidefinite (PSD) matrices, we present a new solution to\nreduce optimizing over the space of column full-rank transformation matrices to\noptimizing on the PSD manifold which has a well-established Riemannian\nstructure. Under this solution, we exploit a new supervised SPD similarity\nlearning technique to learn the transformation by regressing the similarities\nof selected SPD data pairs to their ground-truth similarities on the target SPD\nmanifold. To optimize the proposed objective function, we further derive an\nalgorithm on the PSD manifold. Evaluations on three visual classification tasks\nshow the advantages of the proposed approach over the existing SPD-based\ndiscriminant learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 10:02:57 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Huang", "Zhiwu", ""], ["Wang", "Ruiping", ""], ["Li", "Xianqiu", ""], ["Liu", "Wenxian", ""], ["Shan", "Shiguang", ""], ["Van Gool", "Luc", ""], ["Chen", "Xilin", ""]]}, {"id": "1608.04959", "submitter": "Rakshith Shetty", "authors": "Rakshith Shetty and Jorma Laaksonen", "title": "Frame- and Segment-Level Features and Candidate Pool Evaluation for\n  Video Caption Generation", "comments": null, "journal-ref": null, "doi": "10.1145/2964284.2984062", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our submission to the Microsoft Video to Language Challenge of\ngenerating short captions describing videos in the challenge dataset. Our model\nis based on the encoder--decoder pipeline, popular in image and video\ncaptioning systems. We propose to utilize two different kinds of video\nfeatures, one to capture the video content in terms of objects and attributes,\nand the other to capture the motion and action information. Using these diverse\nfeatures we train models specializing in two separate input sub-domains. We\nthen train an evaluator model which is used to pick the best caption from the\npool of candidates generated by these domain expert models. We argue that this\napproach is better suited for the current video captioning task, compared to\nusing a single model, due to the diversity in the dataset.\n  Efficacy of our method is proven by the fact that it was rated best in MSR\nVideo to Language Challenge, as per human evaluation. Additionally, we were\nranked second in the automatic evaluation metrics based table.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 13:30:06 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Shetty", "Rakshith", ""], ["Laaksonen", "Jorma", ""]]}, {"id": "1608.05001", "submitter": "Fei Hu", "authors": "Fei Hu, Changjiu Pu, Haowei Gao, Mengzi Tang and Li Li", "title": "An image compression and encryption scheme based on deep learning", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stacked Auto-Encoder (SAE) is a kind of deep learning algorithm for\nunsupervised learning. Which has multi layers that project the vector\nrepresentation of input data into a lower vector space. These projection\nvectors are dense representations of the input data. As a result, SAE can be\nused for image compression. Using chaotic logistic map, the compression ones\ncan further be encrypted. In this study, an application of image compression\nand encryption is suggested using SAE and chaotic logistic map. Experiments\nshow that this application is feasible and effective. It can be used for image\ntransmission and image protection on internet simultaneously.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 14:51:25 GMT"}, {"version": "v2", "created": "Sun, 9 Oct 2016 02:27:20 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Hu", "Fei", ""], ["Pu", "Changjiu", ""], ["Gao", "Haowei", ""], ["Tang", "Mengzi", ""], ["Li", "Li", ""]]}, {"id": "1608.05045", "submitter": "Yong Khoo", "authors": "Hugo Martin, Raphael Fernandez, Yong Khoo", "title": "Large Angle based Skeleton Extraction for 3D Animation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a solution for arbitrary 3D character deformation\nby investigating rotation angle of decomposition and preserving the mesh\ntopology structure. In computer graphics, skeleton extraction and\nskeleton-driven animation is an active areas and gains increasing interests\nfrom researchers. The accuracy is critical for realistic animation and related\napplications. There have been extensive studies on skeleton based 3D\ndeformation. However for the scenarios of large angle rotation of different\nbody parts, it has been relatively less addressed by the state-of-the-art,\nwhich often yield unsatisfactory results. Besides 3D animation problems, we\nalso notice for many 3D skeleton detection or tracking applications from a\nvideo or depth streams, large angle rotation is also a critical factor in the\nregression accuracy and robustness. We introduced a distortion metric function\nto quantify the surface curviness before and after deformation, which is a\nmajor clue for large angle rotation detection. The intensive experimental\nresults show that our method is suitable for 3D modeling, animation, skeleton\nbased tracking applications.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 18:53:50 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Martin", "Hugo", ""], ["Fernandez", "Raphael", ""], ["Khoo", "Yong", ""]]}, {"id": "1608.05104", "submitter": "Nasim Souly", "authors": "Nasim Souly and Mubarak Shah", "title": "Scene Labeling Through Knowledge-Based Rules Employing Constrained\n  Integer Linear Programing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene labeling task is to segment the image into meaningful regions and\ncategorize them into classes of objects which comprised the image. Commonly\nused methods typically find the local features for each segment and label them\nusing classifiers. Afterward, labeling is smoothed in order to make sure that\nneighboring regions receive similar labels. However, they ignore expressive and\nnon-local dependencies among regions due to expensive training and inference.\nIn this paper, we propose to use high-level knowledge regarding rules in the\ninference to incorporate dependencies among regions in the image to improve\nscores of classification. Towards this aim, we extract these rules from data\nand transform them into constraints for Integer Programming to optimize the\nstructured problem of assigning labels to super-pixels (consequently pixels) of\nan image. In addition, we propose to use soft-constraints in some scenarios,\nallowing violating the constraint by imposing a penalty, to make the model more\nflexible. We assessed our approach on three datasets and obtained promising\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 21:14:51 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Souly", "Nasim", ""], ["Shah", "Mubarak", ""]]}, {"id": "1608.05137", "submitter": "Hamid Izadinia", "authors": "Hamid Izadinia, Qi Shan, Steven M. Seitz", "title": "IM2CAD", "comments": "To appear at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a single photo of a room and a large database of furniture CAD models,\nour goal is to reconstruct a scene that is as similar as possible to the scene\ndepicted in the photograph, and composed of objects drawn from the database. We\npresent a completely automatic system to address this IM2CAD problem that\nproduces high quality results on challenging imagery from interior home design\nand remodeling websites. Our approach iteratively optimizes the placement and\nscale of objects in the room to best match scene renderings to the input photo,\nusing image comparison metrics trained via deep convolutional neural nets. By\noperating jointly on the full scene at once, we account for inter-object\nocclusions. We also show the applicability of our method in standard scene\nunderstanding benchmarks where we obtain significant improvement.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 00:26:44 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 00:28:58 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Izadinia", "Hamid", ""], ["Shan", "Qi", ""], ["Seitz", "Steven M.", ""]]}, {"id": "1608.05143", "submitter": "Xiaoshui Huang", "authors": "Xiaoshui Huang, Jian Zhang, Lixin Fan, Qiang Wu, Chun Yuan", "title": "A Systematic Approach for Cross-source Point Cloud Registration by\n  Preserving Macro and Micro Structures", "comments": "Cross-source point cloud registration", "journal-ref": null, "doi": "10.1109/TIP.2017.2695888", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a systematic approach for registering cross-source point clouds.\nThe compelling need for cross-source point cloud registration is motivated by\nthe rapid development of a variety of 3D sensing techniques, but many existing\nregistration methods face critical challenges as a result of the large\nvariations in cross-source point clouds. This paper therefore illustrates a\nnovel registration method which successfully aligns two cross-source point\nclouds in the presence of significant missing data, large variations in point\ndensity, scale difference and so on. The robustness of the method is attributed\nto the extraction of macro and micro structures. Our work has three main\ncontributions: (1) a systematic pipeline to deal with cross-source point cloud\nregistration; (2) a graph construction method to maintain macro and micro\nstructures; (3) a new graph matching method is proposed which considers the\nglobal geometric constraint to robustly register these variable graphs.\nCompared to most of the related methods, the experiments show that the proposed\nmethod successfully registers in cross-source datasets, while other methods\nhave difficulty achieving satisfactory results. The proposed method also shows\ngreat ability in same-source datasets.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 00:53:13 GMT"}, {"version": "v2", "created": "Mon, 22 Aug 2016 23:04:03 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Huang", "Xiaoshui", ""], ["Zhang", "Jian", ""], ["Fan", "Lixin", ""], ["Wu", "Qiang", ""], ["Yuan", "Chun", ""]]}, {"id": "1608.05148", "submitter": "Nick Johnston", "authors": "George Toderici, Damien Vincent, Nick Johnston, Sung Jin Hwang, David\n  Minnen, Joel Shor, Michele Covell", "title": "Full Resolution Image Compression with Recurrent Neural Networks", "comments": "Updated with content for CVPR and removed supplemental material to an\n  external link for size limitations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a set of full-resolution lossy image compression methods\nbased on neural networks. Each of the architectures we describe can provide\nvariable compression rates during deployment without requiring retraining of\nthe network: each network need only be trained once. All of our architectures\nconsist of a recurrent neural network (RNN)-based encoder and decoder, a\nbinarizer, and a neural network for entropy coding. We compare RNN types (LSTM,\nassociative LSTM) and introduce a new hybrid of GRU and ResNet. We also study\n\"one-shot\" versus additive reconstruction architectures and introduce a new\nscaled-additive framework. We compare to previous work, showing improvements of\n4.3%-8.8% AUC (area under the rate-distortion curve), depending on the\nperceptual metric used. As far as we know, this is the first neural network\narchitecture that is able to outperform JPEG at image compression across most\nbitrates on the rate-distortion curve on the Kodak dataset images, with and\nwithout the aid of entropy coding.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 01:05:09 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 16:26:16 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Toderici", "George", ""], ["Vincent", "Damien", ""], ["Johnston", "Nick", ""], ["Hwang", "Sung Jin", ""], ["Minnen", "David", ""], ["Shor", "Joel", ""], ["Covell", "Michele", ""]]}, {"id": "1608.05159", "submitter": "Jianan Li", "authors": "Jianan Li, Xiaodan Liang, Jianshu Li, Tingfa Xu, Jiashi Feng,\n  Shuicheng Yan", "title": "Multi-stage Object Detection with Group Recursive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of existing detection pipelines treat object proposals independently and\npredict bounding box locations and classification scores over them separately.\nHowever, the important semantic and spatial layout correlations among proposals\nare often ignored, which are actually useful for more accurate object\ndetection. In this work, we propose a new EM-like group recursive learning\napproach to iteratively refine object proposals by incorporating such context\nof surrounding proposals and provide an optimal spatial configuration of object\ndetections. In addition, we propose to incorporate the weakly-supervised object\nsegmentation cues and region-based object detection into a multi-stage\narchitecture in order to fully exploit the learned segmentation features for\nbetter object detection in an end-to-end way. The proposed architecture\nconsists of three cascaded networks which respectively learn to perform\nweakly-supervised object segmentation, object proposal generation and recursive\ndetection refinement. Combining the group recursive learning and the\nmulti-stage architecture provides competitive mAPs of 78.6% and 74.9% on the\nPASCAL VOC2007 and VOC2012 datasets respectively, which outperforms many\nwell-established baselines [10] [20] significantly.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 02:37:28 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Li", "Jianan", ""], ["Liang", "Xiaodan", ""], ["Li", "Jianshu", ""], ["Xu", "Tingfa", ""], ["Feng", "Jiashi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1608.05167", "submitter": "Gui-Song Xia", "authors": "Gui-Song Xia, Jingwen Hu, Fan Hu, Baoguang Shi, Xiang Bai, Yanfei\n  Zhong, Liangpei Zhang", "title": "AID: A Benchmark Dataset for Performance Evaluation of Aerial Scene\n  Classification", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, Vol. 55, No.7,\n  pp.3965-3981 (July 2017)", "doi": "10.1109/TGRS.2017.2685945", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aerial scene classification, which aims to automatically label an aerial\nimage with a specific semantic category, is a fundamental problem for\nunderstanding high-resolution remote sensing imagery. In recent years, it has\nbecome an active task in remote sensing area and numerous algorithms have been\nproposed for this task, including many machine learning and data-driven\napproaches. However, the existing datasets for aerial scene classification like\nUC-Merced dataset and WHU-RS19 are with relatively small sizes, and the results\non them are already saturated. This largely limits the development of scene\nclassification algorithms. This paper describes the Aerial Image Dataset (AID):\na large-scale dataset for aerial scene classification. The goal of AID is to\nadvance the state-of-the-arts in scene classification of remote sensing images.\nFor creating AID, we collect and annotate more than ten thousands aerial scene\nimages. In addition, a comprehensive review of the existing aerial scene\nclassification techniques as well as recent widely-used deep learning methods\nis given. Finally, we provide a performance analysis of typical aerial scene\nclassification and deep learning approaches on AID, which can be served as the\nbaseline results on this benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 04:20:45 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Xia", "Gui-Song", ""], ["Hu", "Jingwen", ""], ["Hu", "Fan", ""], ["Shi", "Baoguang", ""], ["Bai", "Xiang", ""], ["Zhong", "Yanfei", ""], ["Zhang", "Liangpei", ""]]}, {"id": "1608.05177", "submitter": "Youbao Tang", "authors": "Youbao Tang, Xiangqian Wu, and Wei Bu", "title": "Deeply-Supervised Recurrent Convolutional Neural Network for Saliency\n  Detection", "comments": "5 pages, 5 figures, accepted by ACMMM 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel saliency detection method by developing a\ndeeply-supervised recurrent convolutional neural network (DSRCNN), which\nperforms a full image-to-image saliency prediction. For saliency detection, the\nlocal, global, and contextual information of salient objects is important to\nobtain a high quality salient map. To achieve this goal, the DSRCNN is designed\nbased on VGGNet-16. Firstly, the recurrent connections are incorporated into\neach convolutional layer, which can make the model more powerful for learning\nthe contextual information. Secondly, side-output layers are added to conduct\nthe deeply-supervised operation, which can make the model learn more\ndiscriminative and robust features by effecting the intermediate layers.\nFinally, all of the side-outputs are fused to integrate the local and global\ninformation to get the final saliency detection results. Therefore, the DSRCNN\ncombines the advantages of recurrent convolutional neural networks and\ndeeply-supervised nets. The DSRCNN model is tested on five benchmark datasets,\nand experimental results demonstrate that the proposed method significantly\noutperforms the state-of-the-art saliency detection approaches on all test\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 05:08:16 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Tang", "Youbao", ""], ["Wu", "Xiangqian", ""], ["Bu", "Wei", ""]]}, {"id": "1608.05180", "submitter": "Wenzheng Chen", "authors": "Huayong Xu, Yangyan Li, Wenzheng Chen, Dani Lischinski, Daniel\n  Cohen-Or, Baoquan Chen", "title": "A Holistic Approach for Data-Driven Object Cutout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Object cutout is a fundamental operation for image editing and manipulation,\nyet it is extremely challenging to automate it in real-world images, which\ntypically contain considerable background clutter. In contrast to existing\ncutout methods, which are based mainly on low-level image analysis, we propose\na more holistic approach, which considers the entire shape of the object of\ninterest by leveraging higher-level image analysis and learnt global shape\npriors. Specifically, we leverage a deep neural network (DNN) trained for\nobjects of a particular class (chairs) for realizing this mechanism. Given a\nrectangular image region, the DNN outputs a probability map (P-map) that\nindicates for each pixel inside the rectangle how likely it is to be contained\ninside an object from the class of interest. We show that the resulting P-maps\nmay be used to evaluate how likely a rectangle proposal is to contain an\ninstance of the class, and further process good proposals to produce an\naccurate object cutout mask. This amounts to an automatic end-to-end pipeline\nfor catergory-specific object cutout. We evaluate our approach on segmentation\nbenchmark datasets, and show that it significantly outperforms the\nstate-of-the-art on them.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 05:19:26 GMT"}, {"version": "v2", "created": "Fri, 16 Sep 2016 13:00:21 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Xu", "Huayong", ""], ["Li", "Yangyan", ""], ["Chen", "Wenzheng", ""], ["Lischinski", "Dani", ""], ["Cohen-Or", "Daniel", ""], ["Chen", "Baoquan", ""]]}, {"id": "1608.05186", "submitter": "Youbao Tang", "authors": "Youbao Tang, Xiangqian Wu", "title": "Saliency Detection via Combining Region-Level and Pixel-Level\n  Predictions with CNNs", "comments": "18 pages, 9 figures, accepted by ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel saliency detection method by combining\nregion-level saliency estimation and pixel-level saliency prediction with CNNs\n(denoted as CRPSD). For pixel-level saliency prediction, a fully convolutional\nneural network (called pixel-level CNN) is constructed by modifying the VGGNet\narchitecture to perform multi-scale feature learning, based on which an\nimage-to-image prediction is conducted to accomplish the pixel-level saliency\ndetection. For region-level saliency estimation, an adaptive superpixel based\nregion generation technique is first designed to partition an image into\nregions, based on which the region-level saliency is estimated by using a CNN\nmodel (called region-level CNN). The pixel-level and region-level saliencies\nare fused to form the final salient map by using another CNN (called fusion\nCNN). And the pixel-level CNN and fusion CNN are jointly learned. Extensive\nquantitative and qualitative experiments on four public benchmark datasets\ndemonstrate that the proposed method greatly outperforms the state-of-the-art\nsaliency detection approaches.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 06:00:18 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Tang", "Youbao", ""], ["Wu", "Xiangqian", ""]]}, {"id": "1608.05203", "submitter": "Yusuke Sugano", "authors": "Yusuke Sugano, Andreas Bulling", "title": "Seeing with Humans: Gaze-Assisted Neural Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaze reflects how humans process visual scenes and is therefore increasingly\nused in computer vision systems. Previous works demonstrated the potential of\ngaze for object-centric tasks, such as object localization and recognition, but\nit remains unclear if gaze can also be beneficial for scene-centric tasks, such\nas image captioning. We present a new perspective on gaze-assisted image\ncaptioning by studying the interplay between human gaze and the attention\nmechanism of deep neural networks. Using a public large-scale gaze dataset, we\nfirst assess the relationship between state-of-the-art object and scene\nrecognition models, bottom-up visual saliency, and human gaze. We then propose\na novel split attention model for image captioning. Our model integrates human\ngaze information into an attention-based long short-term memory architecture,\nand allows the algorithm to allocate attention selectively to both fixated and\nnon-fixated image regions. Through evaluation on the COCO/SALICON datasets we\nshow that our method improves image captioning performance and that gaze can\ncomplement machine attention for semantic scene understanding tasks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 08:13:22 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Sugano", "Yusuke", ""], ["Bulling", "Andreas", ""]]}, {"id": "1608.05204", "submitter": "Gyeongmin Choe", "authors": "Gyeongmin Choe, Jaesik Park, Yu-Wing Tai, In So Kweon", "title": "Refining Geometry from Depth Sensors using IR Shading Images", "comments": "Accepted to the International Journal of Computer Vision (IJCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to refine geometry of 3D meshes from a consumer level\ndepth camera, e.g. Kinect, by exploiting shading cues captured from an infrared\n(IR) camera. A major benefit to using an IR camera instead of an RGB camera is\nthat the IR images captured are narrow band images that filter out most\nundesired ambient light, which makes our system robust against natural indoor\nillumination. Moreover, for many natural objects with colorful textures in the\nvisible spectrum, the subjects appear to have a uniform albedo in the IR\nspectrum. Based on our analyses on the IR projector light of the Kinect, we\ndefine a near light source IR shading model that describes the captured\nintensity as a function of surface normals, albedo, lighting direction, and\ndistance between light source and surface points. To resolve the ambiguity in\nour model between the normals and distances, we utilize an initial 3D mesh from\nthe Kinect fusion and multi-view information to reliably estimate surface\ndetails that were not captured and reconstructed by the Kinect fusion. Our\napproach directly operates on the mesh model for geometry refinement. We ran\nexperiments on our algorithm for geometries captured by both the Kinect I and\nKinect II, as the depth acquisition in Kinect I is based on a structured-light\ntechnique and that of the Kinect II is based on a time-of-flight (ToF)\ntechnology. The effectiveness of our approach is demonstrated through several\nchallenging real-world examples. We have also performed a user study to\nevaluate the quality of the mesh models before and after our refinements.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 08:19:43 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Choe", "Gyeongmin", ""], ["Park", "Jaesik", ""], ["Tai", "Yu-Wing", ""], ["Kweon", "In So", ""]]}, {"id": "1608.05209", "submitter": "Felix J\\\"aremo Lawin", "authors": "Felix J\\\"aremo Lawin, Per-Erik Forss\\'en and Hannes Ovr\\'en", "title": "Efficient Multi-Frequency Phase Unwrapping using Kernel Density\n  Estimation", "comments": "Accepted at ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce an efficient method to unwrap multi-frequency\nphase estimates for time-of-flight ranging. The algorithm generates multiple\ndepth hypotheses and uses a spatial kernel density estimate (KDE) to rank them.\nThe confidence produced by the KDE is also an effective means to detect\noutliers. We also introduce a new closed-form expression for phase noise\nprediction, that better fits real data. The method is applied to depth decoding\nfor the Kinect v2 sensor, and compared to the Microsoft Kinect SDK and to the\nopen source driver libfreenect2. The intended Kinect v2 use case is scenes with\nless than 8m range, and for such cases we observe consistent improvements,\nwhile maintaining real-time performance. When extending the depth range to the\nmaximal value of 8.75m, we get about 52% more valid measurements than\nlibfreenect2. The effect is that the sensor can now be used in large depth\nscenes, where it was previously not a good choice. Code and supplementary\nmaterial are available at\nhttp://www.cvl.isy.liu.se/research/datasets/kinect2-dataset.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 08:49:13 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Lawin", "Felix J\u00e4remo", ""], ["Forss\u00e9n", "Per-Erik", ""], ["Ovr\u00e9n", "Hannes", ""]]}, {"id": "1608.05246", "submitter": "Kadir Kirtac", "authors": "Samil Karahan, Merve Kilinc Yildirim, Kadir Kirtac, Ferhat Sukru\n  Rende, Gultekin Butun, Hazim Kemal Ekenel", "title": "How Image Degradations Affect Deep CNN-based Face Recognition?", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": "10.1109/BIOSIG.2016.7736924", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition approaches that are based on deep convolutional neural\nnetworks (CNN) have been dominating the field. The performance improvements\nthey have provided in the so called in-the-wild datasets are significant,\nhowever, their performance under image quality degradations have not been\nassessed, yet. This is particularly important, since in real-world face\nrecognition applications, images may contain various kinds of degradations due\nto motion blur, noise, compression artifacts, color distortions, and occlusion.\nIn this work, we have addressed this problem and analyzed the influence of\nthese image degradations on the performance of deep CNN-based face recognition\napproaches using the standard LFW closed-set identification protocol. We have\nevaluated three popular deep CNN models, namely, the AlexNet, VGG-Face, and\nGoogLeNet. Results have indicated that blur, noise, and occlusion cause a\nsignificant decrease in performance, while deep CNN models are found to be\nrobust to distortions, such as color distortions and change in color balance.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 11:48:26 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Karahan", "Samil", ""], ["Yildirim", "Merve Kilinc", ""], ["Kirtac", "Kadir", ""], ["Rende", "Ferhat Sukru", ""], ["Butun", "Gultekin", ""], ["Ekenel", "Hazim Kemal", ""]]}, {"id": "1608.05267", "submitter": "Qiuhong Ke", "authors": "Qiuhong Ke, Mohammed Bennamoun, Senjian An, Farid Bossaid, Ferdous\n  Sohel", "title": "Leveraging Structural Context Models and Ranking Score Fusion for Human\n  Interaction Prediction", "comments": null, "journal-ref": null, "doi": "10.1109/TMM.2017.2778559", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting an interaction before it is fully executed is very important in\napplications such as human-robot interaction and video surveillance. In a\ntwo-human interaction scenario, there often contextual dependency structure\nbetween the global interaction context of the two humans and the local context\nof the different body parts of each human. In this paper, we propose to learn\nthe structure of the interaction contexts, and combine it with the spatial and\ntemporal information of a video sequence for a better prediction of the\ninteraction class. The structural models, including the spatial and the\ntemporal models, are learned with Long Short Term Memory (LSTM) networks to\ncapture the dependency of the global and local contexts of each RGB frame and\neach optical flow image, respectively. LSTM networks are also capable of\ndetecting the key information from the global and local interaction contexts.\nMoreover, to effectively combine the structural models with the spatial and\ntemporal models for interaction prediction, a ranking score fusion method is\nalso introduced to automatically compute the optimal weight of each model for\nscore fusion. Experimental results on the BIT Interaction and the\nUT-Interaction datasets clearly demonstrate the benefits of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 14:05:37 GMT"}, {"version": "v2", "created": "Thu, 30 Mar 2017 13:55:44 GMT"}, {"version": "v3", "created": "Wed, 7 Jun 2017 08:35:49 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Ke", "Qiuhong", ""], ["Bennamoun", "Mohammed", ""], ["An", "Senjian", ""], ["Bossaid", "Farid", ""], ["Sohel", "Ferdous", ""]]}, {"id": "1608.05339", "submitter": "Wei-Tse Sun", "authors": "Wei-Tse Sun, Ting-Hsuan Chao, Yin-Hsi Kuo, Winston H. Hsu", "title": "Photo Filter Recommendation by Category-Aware Aesthetic Learning", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, social media has become a popular platform for the public to share\nphotos. To make photos more visually appealing, users usually apply filters on\ntheir photos without domain knowledge. However, due to the growing number of\nfilter types, it becomes a major issue for users to choose the best filter\ntype. For this purpose, filter recommendation for photo aesthetics takes an\nimportant role in image quality ranking problems. In these years, several works\nhave declared that Convolutional Neural Networks (CNNs) outperform traditional\nmethods in image aesthetic categorization, which classifies images into high or\nlow quality. Most of them do not consider the effect on filtered images; hence,\nwe propose a novel image aesthetic learning for filter recommendation. Instead\nof binarizing image quality, we adjust the state-of-the-art CNN architectures\nand design a pairwise loss function to learn the embedded aesthetic responses\nin hidden layers for filtered images. Based on our pilot study, we observe\nimage categories (e.g., portrait, landscape, food) will affect user preference\non filter selection. We further integrate category classification into our\nproposed aesthetic-oriented models. To the best of our knowledge, there is no\npublic dataset for aesthetic judgment with filtered images. We create a new\ndataset called Filter Aesthetic Comparison Dataset (FACD). It contains 28,160\nfiltered images based on the AVA dataset and 42,240 reliable image pairs with\naesthetic annotations using Amazon Mechanical Turk. It is the first dataset\ncontaining filtered images and user preference labels. We conduct experiments\non the collected FACD for filter recommendation, and the results show that our\nproposed category-aware aesthetic learning outperforms aesthetic classification\nmethods (e.g., 12% relative improvement).\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 17:22:54 GMT"}, {"version": "v2", "created": "Mon, 27 Mar 2017 05:07:06 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Sun", "Wei-Tse", ""], ["Chao", "Ting-Hsuan", ""], ["Kuo", "Yin-Hsi", ""], ["Hsu", "Winston H.", ""]]}, {"id": "1608.05404", "submitter": "Siyu Tang", "authors": "Siyu Tang, Bjoern Andres, Mykhaylo Andriluka, Bernt Schiele", "title": "Multi-Person Tracking by Multicut and Deep Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In [1], we proposed a graph-based formulation that links and clusters person\nhypotheses over time by solving a minimum cost subgraph multicut problem. In\nthis paper, we modify and extend [1] in three ways: 1) We introduce a novel\nlocal pairwise feature based on local appearance matching that is robust to\npartial occlusion and camera motion. 2) We perform extensive experiments to\ncompare different pairwise potentials and to analyze the robustness of the\ntracking formulation. 3) We consider a plain multicut problem and remove\noutlying clusters from its solution. This allows us to employ an efficient\nprimal feasible optimization algorithm that is not applicable to the subgraph\nmulticut problem of [1]. Unlike the branch-and-cut algorithm used there, this\nefficient algorithm used here is applicable to long videos and many detections.\nTogether with the novel feature, it eliminates the need for the intermediate\ntracklet representation of [1]. We demonstrate the effectiveness of our overall\napproach on the MOT16 benchmark [2], achieving state-of-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 13:53:13 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Tang", "Siyu", ""], ["Andres", "Bjoern", ""], ["Andriluka", "Mykhaylo", ""], ["Schiele", "Bernt", ""]]}, {"id": "1608.05442", "submitter": "Bolei Zhou", "authors": "Bolei Zhou and Hang Zhao and Xavier Puig and Tete Xiao and Sanja\n  Fidler and Adela Barriuso and Antonio Torralba", "title": "Semantic Understanding of Scenes through the ADE20K Dataset", "comments": "IJCV extension", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene parsing, or recognizing and segmenting objects and stuff in an image,\nis one of the key problems in computer vision. Despite the community's efforts\nin data collection, there are still few image datasets covering a wide range of\nscenes and object categories with dense and detailed annotations for scene\nparsing. In this paper, we introduce and analyze the ADE20K dataset, spanning\ndiverse annotations of scenes, objects, parts of objects, and in some cases\neven parts of parts. A generic network design called Cascade Segmentation\nModule is then proposed to enable the segmentation networks to parse a scene\ninto stuff, objects, and object parts in a cascade. We evaluate the proposed\nmodule integrated within two existing semantic segmentation networks, yielding\nsignificant improvements for scene parsing. We further show that the scene\nparsing networks trained on ADE20K can be applied to a wide variety of scenes\nand objects.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 22:23:13 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 04:41:24 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Zhou", "Bolei", ""], ["Zhao", "Hang", ""], ["Puig", "Xavier", ""], ["Xiao", "Tete", ""], ["Fidler", "Sanja", ""], ["Barriuso", "Adela", ""], ["Torralba", "Antonio", ""]]}, {"id": "1608.05461", "submitter": "Jen-Yin Chang", "authors": "Jen-Yin Chang, Kuan-Ying Lee, Yu-Lin Wei, Kate Ching-Ju Lin, Winston\n  Hsu", "title": "We Can \"See\" You via Wi-Fi - WiFi Action Recognition via Vision-based\n  Methods", "comments": "10 pages, 10 figures, submit to IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Wi-Fi has caught tremendous attention for its ubiquity, and,\nmotivated by Wi-Fi's low cost and privacy preservation, researchers have been\nputting lots of investigation into its potential on action recognition and even\nperson identification. In this paper, we offer an comprehensive overview on\nthese two topics in Wi-Fi. Also, through looking at these two topics from an\nunprecedented perspective, we could achieve generality instead of designing\nspecific ad-hoc features for each scenario. Observing the great resemblance of\nChannel State Information (CSI, a fine-grained information captured from the\nreceived Wi-Fi signal) to texture, we proposed a brand-new framework based on\ncomputer vision methods. To minimize the effect of location dependency embedded\nin CSI, we propose a novel de-noising method based on Singular Value\nDecomposition (SVD) to eliminate the background energy and effectively extract\nthe channel information of signals reflected by human bodies. From the\nexperiments conducted, we demonstrate the feasibility and efficacy of the\nproposed methods. Also, we conclude factors that would affect the performance\nand highlight a few promising issues that require further deliberation.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 00:39:57 GMT"}, {"version": "v2", "created": "Mon, 3 Apr 2017 04:59:23 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Chang", "Jen-Yin", ""], ["Lee", "Kuan-Ying", ""], ["Wei", "Yu-Lin", ""], ["Lin", "Kate Ching-Ju", ""], ["Hsu", "Winston", ""]]}, {"id": "1608.05477", "submitter": "Xi Peng", "authors": "Xi Peng, Rogerio S. Feris, Xiaoyu Wang, Dimitris N. Metaxas", "title": "A Recurrent Encoder-Decoder Network for Sequential Face Alignment", "comments": "European Conference on Computer Vision (ECCV), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel recurrent encoder-decoder network model for real-time\nvideo-based face alignment. Our proposed model predicts 2D facial point maps\nregularized by a regression loss, while uniquely exploiting recurrent learning\nat both spatial and temporal dimensions. At the spatial level, we add a\nfeedback loop connection between the combined output response map and the\ninput, in order to enable iterative coarse-to-fine face alignment using a\nsingle network model. At the temporal level, we first decouple the features in\nthe bottleneck of the network into temporal-variant factors, such as pose and\nexpression, and temporal-invariant factors, such as identity information.\nTemporal recurrent learning is then applied to the decoupled temporal-variant\nfeatures, yielding better generalization and significantly more accurate\nresults at test time. We perform a comprehensive experimental analysis, showing\nthe importance of each component of our proposed model, as well as superior\nresults over the state-of-the-art in standard datasets.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 02:28:50 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 01:23:48 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Peng", "Xi", ""], ["Feris", "Rogerio S.", ""], ["Wang", "Xiaoyu", ""], ["Metaxas", "Dimitris N.", ""]]}, {"id": "1608.05512", "submitter": "Hon-Leung Lee", "authors": "Hon-Leung Lee", "title": "Critical Points for Two-view Triangulation", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-view triangulation is a problem of minimizing a quadratic polynomial\nunder an equality constraint. We derive a polynomial that encodes the local\nminimizers of this problem using the theory of Lagrange multipliers. This\noffers a simpler derivation of the critical points that are given in\nHartley-Sturm [6].\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 07:05:15 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Lee", "Hon-Leung", ""]]}, {"id": "1608.05518", "submitter": "Hon-Leung Lee", "authors": "Hon-Leung Lee", "title": "On the Existence of a Projective Reconstruction", "comments": "7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we study the connection between the existence of a projective\nreconstruction and the existence of a fundamental matrix satisfying the\nepipolar constraints.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 07:30:33 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 06:45:06 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Lee", "Hon-Leung", ""]]}, {"id": "1608.05562", "submitter": "Enzo Ferrante", "authors": "Roque Porchetto (1), Franco Stramana (1), Nikos Paragios (2), Enzo\n  Ferrante (2) ((1) UNICEN University, Tandil Argentina, (2) CVN,\n  CentraleSupelec-INRIA, Universite Paris-Saclay, France)", "title": "Rigid Slice-To-Volume Medical Image Registration through Markov Random\n  Fields", "comments": "Bayesian and Graphical Models for Biomedical Imaging Workshop, BAMBI\n  (MICCAI 2016, Athens, Greece)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rigid slice-to-volume registration is a challenging task, which finds\napplication in medical imaging problems like image fusion for image guided\nsurgeries and motion correction for volume reconstruction. It is usually\nformulated as an optimization problem and solved using standard continuous\nmethods. In this paper, we discuss how this task be formulated as a discrete\nlabeling problem on a graph. Inspired by previous works on discrete estimation\nof linear transformations using Markov Random Fields (MRFs), we model it using\na pairwise MRF, where the nodes are associated to the rigid parameters, and the\nedges encode the relation between the variables. We compare the performance of\nthe proposed method to a continuous formulation optimized using simplex, and we\ndiscuss how it can be used to further improve the accuracy of our approach.\nPromising results are obtained using a monomodal dataset composed of magnetic\nresonance images (MRI) of a beating heart.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 10:29:50 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Porchetto", "Roque", ""], ["Stramana", "Franco", ""], ["Paragios", "Nikos", ""], ["Ferrante", "Enzo", ""]]}, {"id": "1608.05571", "submitter": "Martin Danelljan", "authors": "Martin Danelljan, Gustav H\\\"ager, Fahad Shahbaz Khan, Michael Felsberg", "title": "Learning Spatially Regularized Correlation Filters for Visual Tracking", "comments": "ICCV 2015", "journal-ref": "International Conference on Computer Vision, (ICCV) 2015, pp.\n  4310-4318. IEEE (2015)", "doi": "10.1109/ICCV.2015.490", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust and accurate visual tracking is one of the most challenging computer\nvision problems. Due to the inherent lack of training data, a robust approach\nfor constructing a target appearance model is crucial. Recently,\ndiscriminatively learned correlation filters (DCF) have been successfully\napplied to address this problem for tracking. These methods utilize a periodic\nassumption of the training samples to efficiently learn a classifier on all\npatches in the target neighborhood. However, the periodic assumption also\nintroduces unwanted boundary effects, which severely degrade the quality of the\ntracking model.\n  We propose Spatially Regularized Discriminative Correlation Filters (SRDCF)\nfor tracking. A spatial regularization component is introduced in the learning\nto penalize correlation filter coefficients depending on their spatial\nlocation. Our SRDCF formulation allows the correlation filters to be learned on\na significantly larger set of negative training samples, without corrupting the\npositive samples. We further propose an optimization strategy, based on the\niterative Gauss-Seidel method, for efficient online learning of our SRDCF.\nExperiments are performed on four benchmark datasets: OTB-2013, ALOV++,\nOTB-2015, and VOT2014. Our approach achieves state-of-the-art results on all\nfour datasets. On OTB-2013 and OTB-2015, we obtain an absolute gain of 8.0% and\n8.2% respectively, in mean overlap precision, compared to the best existing\ntrackers.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 11:11:49 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Danelljan", "Martin", ""], ["H\u00e4ger", "Gustav", ""], ["Khan", "Fahad Shahbaz", ""], ["Felsberg", "Michael", ""]]}, {"id": "1608.05684", "submitter": "Menghua Zhai", "authors": "Menghua Zhai, Scott Workman, Nathan Jacobs", "title": "Detecting Vanishing Points using Global Image Context in a Non-Manhattan\n  World", "comments": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for detecting horizontal vanishing points and the\nzenith vanishing point in man-made environments. The dominant trend in existing\nmethods is to first find candidate vanishing points, then remove outliers by\nenforcing mutual orthogonality. Our method reverses this process: we propose a\nset of horizon line candidates and score each based on the vanishing points it\ncontains. A key element of our approach is the use of global image context,\nextracted with a deep convolutional network, to constrain the set of candidates\nunder consideration. Our method does not make a Manhattan-world assumption and\ncan operate effectively on scenes with only a single horizontal vanishing\npoint. We evaluate our approach on three benchmark datasets and achieve\nstate-of-the-art performance on each. In addition, our approach is\nsignificantly faster than the previous best method.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 18:08:55 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Zhai", "Menghua", ""], ["Workman", "Scott", ""], ["Jacobs", "Nathan", ""]]}, {"id": "1608.05813", "submitter": "Chee Seng Chan", "authors": "Ying Hua Tan, Chee Seng Chan", "title": "phi-LSTM: A Phrase-based Hierarchical LSTM Model for Image Captioning", "comments": "This paper introduces phrase-based image captioning. Accepted in\n  ACCV2016 (extended version, 21 pages, 12 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A picture is worth a thousand words. Not until recently, however, we noticed\nsome success stories in understanding of visual scenes: a model that is able to\ndetect/name objects, describe their attributes, and recognize their\nrelationships/interactions. In this paper, we propose a phrase-based\nhierarchical Long Short-Term Memory (phi-LSTM) model to generate image\ndescription. The proposed model encodes sentence as a sequence of combination\nof phrases and words, instead of a sequence of words alone as in those\nconventional solutions. The two levels of this model are dedicated to i) learn\nto generate image relevant noun phrases, and ii) produce appropriate image\ndescription from the phrases and other words in the corpus. Adopting a\nconvolutional neural network to learn image features and the LSTM to learn the\nword sequence in a sentence, the proposed model has shown better or competitive\nresults in comparison to the state-of-the-art models on Flickr8k and Flickr30k\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2016 12:12:09 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 09:02:18 GMT"}, {"version": "v3", "created": "Tue, 6 Sep 2016 14:27:03 GMT"}, {"version": "v4", "created": "Mon, 23 Oct 2017 22:05:38 GMT"}, {"version": "v5", "created": "Thu, 26 Oct 2017 15:16:57 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Tan", "Ying Hua", ""], ["Chan", "Chee Seng", ""]]}, {"id": "1608.05842", "submitter": "Jason Yu", "authors": "Jason J. Yu, Adam W. Harley and Konstantinos G. Derpanis", "title": "Back to Basics: Unsupervised Learning of Optical Flow via Brightness\n  Constancy and Motion Smoothness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, convolutional networks (convnets) have proven useful for predicting\noptical flow. Much of this success is predicated on the availability of large\ndatasets that require expensive and involved data acquisition and laborious la-\nbeling. To bypass these challenges, we propose an unsuper- vised approach\n(i.e., without leveraging groundtruth flow) to train a convnet end-to-end for\npredicting optical flow be- tween two images. We use a loss function that\ncombines a data term that measures photometric constancy over time with a\nspatial term that models the expected variation of flow across the image.\nTogether these losses form a proxy measure for losses based on the groundtruth\nflow. Empiri- cally, we show that a strong convnet baseline trained with the\nproposed unsupervised approach outperforms the same network trained with\nsupervision on the KITTI dataset.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2016 15:25:31 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Yu", "Jason J.", ""], ["Harley", "Adam W.", ""], ["Derpanis", "Konstantinos G.", ""]]}, {"id": "1608.05856", "submitter": "Jing Wang", "authors": "Jing Wang, Meng Wang, Xuegang Hu, Shuicheng Yan", "title": "Visual Processing by a Unified Schatten-$p$ Norm and $\\ell_q$ Norm\n  Regularized Principal Component Pursuit", "comments": "Pattern Recognition, 2015", "journal-ref": null, "doi": "10.1016/j.patcog.2015.01.024", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a non-convex formulation to recover the authentic\nstructure from the corrupted real data. Typically, the specific structure is\nassumed to be low rank, which holds for a wide range of data, such as images\nand videos. Meanwhile, the corruption is assumed to be sparse. In the\nliterature, such a problem is known as Robust Principal Component Analysis\n(RPCA), which usually recovers the low rank structure by approximating the rank\nfunction with a nuclear norm and penalizing the error by an $\\ell_1$-norm.\nAlthough RPCA is a convex formulation and can be solved effectively, the\nintroduced norms are not tight approximations, which may cause the solution to\ndeviate from the authentic one. Therefore, we consider here a non-convex\nrelaxation, consisting of a Schatten-$p$ norm and an $\\ell_q$-norm that promote\nlow rank and sparsity respectively. We derive a proximal iteratively reweighted\nalgorithm (PIRA) to solve the problem. Our algorithm is based on an alternating\ndirection method of multipliers, where in each iteration we linearize the\nunderlying objective function that allows us to have a closed form solution. We\ndemonstrate that solutions produced by the linearized approximation always\nconverge and have a tighter approximation than the convex counterpart.\nExperimental results on benchmarks show encouraging results of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2016 18:18:39 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Wang", "Jing", ""], ["Wang", "Meng", ""], ["Hu", "Xuegang", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1608.05889", "submitter": "Jing Wang", "authors": "Jing Wang and Meng Wang and Peipei Li and Luoqi Liu and Zhongqiu Zhao\n  and Xuegang Hu and Xindong Wu", "title": "Online Feature Selection with Group Structure Analysis", "comments": "IEEE Transactions on Knowledge and Data Engineering,2015", "journal-ref": null, "doi": "10.1109/TKDE.2015.2441716", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online selection of dynamic features has attracted intensive interest in\nrecent years. However, existing online feature selection methods evaluate\nfeatures individually and ignore the underlying structure of feature stream.\nFor instance, in image analysis, features are generated in groups which\nrepresent color, texture and other visual information. Simply breaking the\ngroup structure in feature selection may degrade performance. Motivated by this\nfact, we formulate the problem as an online group feature selection. The\nproblem assumes that features are generated individually but there are group\nstructure in the feature stream. To the best of our knowledge, this is the\nfirst time that the correlation among feature stream has been considered in the\nonline feature selection process. To solve this problem, we develop a novel\nonline group feature selection method named OGFS. Our proposed approach\nconsists of two stages: online intra-group selection and online inter-group\nselection. In the intra-group selection, we design a criterion based on\nspectral analysis to select discriminative features in each group. In the\ninter-group selection, we utilize a linear regression model to select an\noptimal subset. This two-stage procedure continues until there are no more\nfeatures arriving or some predefined stopping conditions are met. %Our method\nhas been applied Finally, we apply our method to multiple tasks including image\nclassification %, face verification and face verification. Extensive empirical\nstudies performed on real-world and benchmark data sets demonstrate that our\nmethod outperforms other state-of-the-art online feature selection %method\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 02:39:48 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Wang", "Jing", ""], ["Wang", "Meng", ""], ["Li", "Peipei", ""], ["Liu", "Luoqi", ""], ["Zhao", "Zhongqiu", ""], ["Hu", "Xuegang", ""], ["Wu", "Xindong", ""]]}, {"id": "1608.05895", "submitter": "Hao Chen", "authors": "Hao Chen, Qi Dou, Lequan Yu, Pheng-Ann Heng", "title": "VoxResNet: Deep Voxelwise Residual Networks for Volumetric Brain\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently deep residual learning with residual units for training very deep\nneural networks advanced the state-of-the-art performance on 2D image\nrecognition tasks, e.g., object detection and segmentation. However, how to\nfully leverage contextual representations for recognition tasks from volumetric\ndata has not been well studied, especially in the field of medical image\ncomputing, where a majority of image modalities are in volumetric format. In\nthis paper we explore the deep residual learning on the task of volumetric\nbrain segmentation. There are at least two main contributions in our work.\nFirst, we propose a deep voxelwise residual network, referred as VoxResNet,\nwhich borrows the spirit of deep residual learning in 2D image recognition\ntasks, and is extended into a 3D variant for handling volumetric data. Second,\nan auto-context version of VoxResNet is proposed by seamlessly integrating the\nlow-level image appearance features, implicit shape information and high-level\ncontext together for further improving the volumetric segmentation performance.\nExtensive experiments on the challenging benchmark of brain segmentation from\nmagnetic resonance (MR) images corroborated the efficacy of our proposed method\nin dealing with volumetric data. We believe this work unravels the potential of\n3D deep learning to advance the recognition performance on volumetric image\nsegmentation.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 04:39:12 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Chen", "Hao", ""], ["Dou", "Qi", ""], ["Yu", "Lequan", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1608.05924", "submitter": "Bernd Sturmfels", "authors": "Jean Ponce, Bernd Sturmfels and Matthew Trager", "title": "Congruences and Concurrent Lines in Multi-View Geometry", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG cs.CV cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new framework for multi-view geometry in computer vision. A\ncamera is a mapping between $\\mathbb{P}^3$ and a line congruence. This model,\nwhich ignores image planes and measurements, is a natural abstraction of\ntraditional pinhole cameras. It includes two-slit cameras, pushbroom cameras,\ncatadioptric cameras, and many more. We study the concurrent lines variety,\nwhich consists of $n$-tuples of lines in $\\mathbb{P}^3$ that intersect at a\npoint. Combining its equations with those of various congruences, we derive\nconstraints for corresponding images in multiple views. We also study\nphotographic cameras which use image measurements and are modeled as rational\nmaps from $\\mathbb{P}^3$ to $\\mathbb{P}^2$ or $\\mathbb{P}^1\\times\n\\mathbb{P}^1$.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 12:07:14 GMT"}, {"version": "v2", "created": "Sun, 25 Dec 2016 20:52:46 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Ponce", "Jean", ""], ["Sturmfels", "Bernd", ""], ["Trager", "Matthew", ""]]}, {"id": "1608.05971", "submitter": "Mohsen Fayyaz", "authors": "Mohsen Fayyaz, Mohammad Hajizadeh Saffar, Mohammad Sabokrou, Mahmood\n  Fathy, Reinhard Klette, Fay Huang", "title": "STFCN: Spatio-Temporal FCN for Semantic Video Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a novel method to involve both spatial and temporal\nfeatures for semantic video segmentation. Current work on convolutional neural\nnetworks(CNNs) has shown that CNNs provide advanced spatial features supporting\na very good performance of solutions for both image and video analysis,\nespecially for the semantic segmentation task. We investigate how involving\ntemporal features also has a good effect on segmenting video data. We propose a\nmodule based on a long short-term memory (LSTM) architecture of a recurrent\nneural network for interpreting the temporal characteristics of video frames\nover time. Our system takes as input frames of a video and produces a\ncorrespondingly-sized output; for segmenting the video our method combines the\nuse of three components: First, the regional spatial features of frames are\nextracted using a CNN; then, using LSTM the temporal features are added;\nfinally, by deconvolving the spatio-temporal features we produce pixel-wise\npredictions. Our key insight is to build spatio-temporal convolutional networks\n(spatio-temporal CNNs) that have an end-to-end architecture for semantic video\nsegmentation. We adapted fully some known convolutional network architectures\n(such as FCN-AlexNet and FCN-VGG16), and dilated convolution into our\nspatio-temporal CNNs. Our spatio-temporal CNNs achieve state-of-the-art\nsemantic segmentation, as demonstrated for the Camvid and NYUDv2 datasets.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 17:34:08 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 15:51:49 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Fayyaz", "Mohsen", ""], ["Saffar", "Mohammad Hajizadeh", ""], ["Sabokrou", "Mohammad", ""], ["Fathy", "Mahmood", ""], ["Klette", "Reinhard", ""], ["Huang", "Fay", ""]]}, {"id": "1608.06010", "submitter": "Yun Wang", "authors": "Yun Wang, Xu Chen and Peter J. Ramadge", "title": "Feedback-Controlled Sequential Lasso Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One way to solve lasso problems when the dictionary does not fit into\navailable memory is to first screen the dictionary to remove unneeded features.\nPrior research has shown that sequential screening methods offer the greatest\npromise in this endeavor. Most existing work on sequential screening targets\nthe context of tuning parameter selection, where one screens and solves a\nsequence of $N$ lasso problems with a fixed grid of geometrically spaced\nregularization parameters. In contrast, we focus on the scenario where a target\nregularization parameter has already been chosen via cross-validated model\nselection, and we then need to solve many lasso instances using this fixed\nvalue. In this context, we propose and explore a feedback controlled sequential\nscreening scheme. Feedback is used at each iteration to select the next problem\nto be solved. This allows the sequence of problems to be adapted to the\ninstance presented and the number of intermediate problems to be automatically\nselected. We demonstrate our feedback scheme using several datasets including a\ndictionary of approximate size 100,000 by 300,000.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 23:40:56 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2016 22:52:30 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Wang", "Yun", ""], ["Chen", "Xu", ""], ["Ramadge", "Peter J.", ""]]}, {"id": "1608.06014", "submitter": "Yun Wang", "authors": "Yun Wang and Peter J. Ramadge", "title": "The Symmetry of a Simple Optimization Problem in Lasso Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently dictionary screening has been proposed as an effective way to\nimprove the computational efficiency of solving the lasso problem, which is one\nof the most commonly used method for learning sparse representations. To\naddress today's ever increasing large dataset, effective screening relies on a\ntight region bound on the solution to the dual lasso. Typical region bounds are\nin the form of an intersection of a sphere and multiple half spaces. One way to\ntighten the region bound is using more half spaces, which however, adds to the\noverhead of solving the high dimensional optimization problem in lasso\nscreening. This paper reveals the interesting property that the optimization\nproblem only depends on the projection of features onto the subspace spanned by\nthe normals of the half spaces. This property converts an optimization problem\nin high dimension to much lower dimension, and thus sheds light on reducing the\ncomputation overhead of lasso screening based on tighter region bounds.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 23:48:43 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2016 22:05:24 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Wang", "Yun", ""], ["Ramadge", "Peter J.", ""]]}, {"id": "1608.06019", "submitter": "Konstantinos Bousmalis", "authors": "Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip\n  Krishnan, Dumitru Erhan", "title": "Domain Separation Networks", "comments": "This work will be presented at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cost of large scale data collection and annotation often makes the\napplication of machine learning algorithms to new tasks or datasets\nprohibitively expensive. One approach circumventing this cost is training\nmodels on synthetic data where annotations are provided automatically. Despite\ntheir appeal, such models often fail to generalize from synthetic to real\nimages, necessitating domain adaptation algorithms to manipulate these models\nbefore they can be successfully applied. Existing approaches focus either on\nmapping representations from one domain to the other, or on learning to extract\nfeatures that are invariant to the domain from which they were extracted.\nHowever, by focusing only on creating a mapping or shared representation\nbetween the two domains, they ignore the individual characteristics of each\ndomain. We suggest that explicitly modeling what is unique to each domain can\nimprove a model's ability to extract domain-invariant features. Inspired by\nwork on private-shared component analysis, we explicitly learn to extract image\nrepresentations that are partitioned into two subspaces: one component which is\nprivate to each domain and one which is shared across domains. Our model is\ntrained not only to perform the task we care about in the source domain, but\nalso to use the partitioned representation to reconstruct the images from both\ndomains. Our novel architecture results in a model that outperforms the\nstate-of-the-art on a range of unsupervised domain adaptation scenarios and\nadditionally produces visualizations of the private and shared representations\nenabling interpretation of the domain adaptation process.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 00:12:27 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Bousmalis", "Konstantinos", ""], ["Trigeorgis", "George", ""], ["Silberman", "Nathan", ""], ["Krishnan", "Dilip", ""], ["Erhan", "Dumitru", ""]]}, {"id": "1608.06037", "submitter": "SeyyedHossein Hasanpour Matikolaee", "authors": "Seyyed Hossein Hasanpour, Mohammad Rouhani, Mohsen Fayyaz, Mohammad\n  Sabokrou", "title": "Lets keep it simple, Using simple architectures to outperform deeper and\n  more complex architectures", "comments": "replaced low-res images with high-res versions, minor corrections in\n  the appendix, switched to LaTex", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Major winning Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet,\nResNet, GoogleNet, include tens to hundreds of millions of parameters, which\nimpose considerable computation and memory overhead. This limits their\npractical use for training, optimization and memory efficiency. On the\ncontrary, light-weight architectures, being proposed to address this issue,\nmainly suffer from low accuracy. These inefficiencies mostly stem from\nfollowing an ad hoc procedure. We propose a simple architecture, called\nSimpleNet, based on a set of designing principles, with which we empirically\nshow, a well-crafted yet simple and reasonably deep architecture can perform on\npar with deeper and more complex architectures. SimpleNet provides a good\ntradeoff between the computation/memory efficiency and the accuracy. Our simple\n13-layer architecture outperforms most of the deeper and complex architectures\nto date such as VGGNet, ResNet, and GoogleNet on several well-known benchmarks\nwhile having 2 to 25 times fewer number of parameters and operations. This\nmakes it very handy for embedded system or system with computational and memory\nlimitations. We achieved state-of-the-art result on CIFAR10 outperforming\nseveral heavier architectures, near state of the art on MNIST and competitive\nresults on CIFAR100 and SVHN. Models are made available at:\nhttps://github.com/Coderx7/SimpleNet\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 02:50:57 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 18:36:14 GMT"}, {"version": "v3", "created": "Wed, 2 Nov 2016 14:08:46 GMT"}, {"version": "v4", "created": "Mon, 15 May 2017 17:30:05 GMT"}, {"version": "v5", "created": "Sat, 6 Jan 2018 19:08:55 GMT"}, {"version": "v6", "created": "Tue, 13 Feb 2018 18:09:24 GMT"}, {"version": "v7", "created": "Wed, 14 Feb 2018 09:19:10 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Hasanpour", "Seyyed Hossein", ""], ["Rouhani", "Mohammad", ""], ["Fayyaz", "Mohsen", ""], ["Sabokrou", "Mohammad", ""]]}, {"id": "1608.06049", "submitter": "Felix Juefei-Xu", "authors": "Felix Juefei-Xu, Vishnu Naresh Boddeti, Marios Savvides", "title": "Local Binary Convolutional Neural Networks", "comments": "To appear in CVPR 2017 as Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose local binary convolution (LBC), an efficient alternative to\nconvolutional layers in standard convolutional neural networks (CNN). The\ndesign principles of LBC are motivated by local binary patterns (LBP). The LBC\nlayer comprises of a set of fixed sparse pre-defined binary convolutional\nfilters that are not updated during the training process, a non-linear\nactivation function and a set of learnable linear weights. The linear weights\ncombine the activated filter responses to approximate the corresponding\nactivated filter responses of a standard convolutional layer. The LBC layer\naffords significant parameter savings, 9x to 169x in the number of learnable\nparameters compared to a standard convolutional layer. Furthermore, the sparse\nand binary nature of the weights also results in up to 9x to 169x savings in\nmodel size compared to a standard convolutional layer. We demonstrate both\ntheoretically and experimentally that our local binary convolution layer is a\ngood approximation of a standard convolutional layer. Empirically, CNNs with\nLBC layers, called local binary convolutional neural networks (LBCNN), achieves\nperformance parity with regular CNNs on a range of visual datasets (MNIST,\nSVHN, CIFAR-10, and ImageNet) while enjoying significant computational savings.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 04:32:21 GMT"}, {"version": "v2", "created": "Sat, 1 Jul 2017 17:02:44 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Juefei-Xu", "Felix", ""], ["Boddeti", "Vishnu Naresh", ""], ["Savvides", "Marios", ""]]}, {"id": "1608.06148", "submitter": "Chandrajit M", "authors": "Chandrajit M, Girisha R and Vasudev T", "title": "Multiple objects tracking in surveillance video using color and Hu\n  moments", "comments": "13 pages, Signal & Image Processing : An International Journal\n  (SIPIJ) Vol.7, No.3, June 2016", "journal-ref": null, "doi": "10.5121/sipij.2016.7302", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple objects tracking finds its applications in many high level vision\nanalysis like object behaviour interpretation and gait recognition. In this\npaper, a feature based method to track the multiple moving objects in\nsurveillance video sequence is proposed. Object tracking is done by extracting\nthe color and Hu moments features from the motion segmented object blob and\nestablishing the association of objects in the successive frames of the video\nsequence based on Chi-Square dissimilarity measure and nearest neighbor\nclassifier. The benchmark IEEE PETS and IEEE Change Detection datasets has been\nused to show the robustness of the proposed method. The proposed method is\nassessed quantitatively using the precision and recall accuracy metrics.\nFurther, comparative evaluation with related works has been carried out to\nexhibit the efficacy of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 12:42:46 GMT"}, {"version": "v2", "created": "Sun, 28 Aug 2016 13:11:35 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["M", "Chandrajit", ""], ["R", "Girisha", ""], ["T", "Vasudev", ""]]}, {"id": "1608.06192", "submitter": "Alban Desmaison", "authors": "Alban Desmaison, Rudy Bunel, Pushmeet Kohli, Philip H.S. Torr and M.\n  Pawan Kumar", "title": "Efficient Continuous Relaxations for Dense CRF", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dense conditional random fields (CRF) with Gaussian pairwise potentials have\nemerged as a popular framework for several computer vision applications such as\nstereo correspondence and semantic segmentation. By modeling long-range\ninteractions, dense CRFs provide a more detailed labelling compared to their\nsparse counterparts. Variational inference in these dense models is performed\nusing a filtering-based mean-field algorithm in order to obtain a\nfully-factorized distribution minimising the Kullback-Leibler divergence to the\ntrue distribution. In contrast to the continuous relaxation-based energy\nminimisation algorithms used for sparse CRFs, the mean-field algorithm fails to\nprovide strong theoretical guarantees on the quality of its solutions. To\naddress this deficiency, we show that it is possible to use the same filtering\napproach to speed-up the optimisation of several continuous relaxations.\nSpecifically, we solve a convex quadratic programming (QP) relaxation using the\nefficient Frank-Wolfe algorithm. This also allows us to solve\ndifference-of-convex relaxations via the iterative concave-convex procedure\nwhere each iteration requires solving a convex QP. Finally, we develop a novel\ndivide-and-conquer method to compute the subgradients of a linear programming\nrelaxation that provides the best theoretical bounds for energy minimisation.\nWe demonstrate the advantage of continuous relaxations over the widely used\nmean-field algorithm on publicly available datasets.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 15:24:25 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Desmaison", "Alban", ""], ["Bunel", "Rudy", ""], ["Kohli", "Pushmeet", ""], ["Torr", "Philip H. S.", ""], ["Kumar", "M. Pawan", ""]]}, {"id": "1608.06197", "submitter": "Srinivas S S Kruthiventi", "authors": "Lokesh Boominathan, Srinivas S S Kruthiventi and R. Venkatesh Babu", "title": "CrowdNet: A Deep Convolutional Network for Dense Crowd Counting", "comments": "Accepted at ACM Multimedia (MM) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work proposes a novel deep learning framework for estimating crowd\ndensity from static images of highly dense crowds. We use a combination of deep\nand shallow, fully convolutional networks to predict the density map for a\ngiven crowd image. Such a combination is used for effectively capturing both\nthe high-level semantic information (face/body detectors) and the low-level\nfeatures (blob detectors), that are necessary for crowd counting under large\nscale variations. As most crowd datasets have limited training samples (<100\nimages) and deep learning based approaches require large amounts of training\ndata, we perform multi-scale data augmentation. Augmenting the training samples\nin such a manner helps in guiding the CNN to learn scale invariant\nrepresentations. Our method is tested on the challenging UCF_CC_50 dataset, and\nshown to outperform the state of the art methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 15:43:29 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Boominathan", "Lokesh", ""], ["Kruthiventi", "Srinivas S S", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1608.06277", "submitter": "Micah Richert", "authors": "Micah Richert, Dimitry Fisher, Filip Piekniewski, Eugene M.\n  Izhikevich, Todd L. Hylton", "title": "Fundamental principles of cortical computation: unsupervised learning\n  with prediction, compression and feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been great progress in understanding of anatomical and functional\nmicrocircuitry of the primate cortex. However, the fundamental principles of\ncortical computation - the principles that allow the visual cortex to bind\nretinal spikes into representations of objects, scenes and scenarios - have so\nfar remained elusive. In an attempt to come closer to understanding the\nfundamental principles of cortical computation, here we present a functional,\nphenomenological model of the primate visual cortex. The core part of the model\ndescribes four hierarchical cortical areas with feedforward, lateral, and\nrecurrent connections. The three main principles implemented in the model are\ninformation compression, unsupervised learning by prediction, and use of\nlateral and top-down context. We show that the model reproduces key aspects of\nthe primate ventral stream of visual processing including Simple and Complex\ncells in V1, increasingly complicated feature encoding, and increased\nseparability of object representations in higher cortical areas. The model\nlearns representations of the visual environment that allow for accurate\nclassification and state-of-the-art visual tracking performance on novel\nobjects.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 21:05:35 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Richert", "Micah", ""], ["Fisher", "Dimitry", ""], ["Piekniewski", "Filip", ""], ["Izhikevich", "Eugene M.", ""], ["Hylton", "Todd L.", ""]]}, {"id": "1608.06338", "submitter": "Pichao Wang", "authors": "Pichao Wang, Wanqing Li, Song Liu, Yuyao Zhang, Zhimin Gao and Philip\n  Ogunbona", "title": "Large-scale Continuous Gesture Recognition Using Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of continuous gesture recognition from\nsequences of depth maps using convolutional neutral networks (ConvNets). The\nproposed method first segments individual gestures from a depth sequence based\non quantity of movement (QOM). For each segmented gesture, an Improved Depth\nMotion Map (IDMM), which converts the depth sequence into one image, is\nconstructed and fed to a ConvNet for recognition. The IDMM effectively encodes\nboth spatial and temporal information and allows the fine-tuning with existing\nConvNet models for classification without introducing millions of parameters to\nlearn. The proposed method is evaluated on the Large-scale Continuous Gesture\nRecognition of the ChaLearn Looking at People (LAP) challenge 2016. It achieved\nthe performance of 0.2655 (Mean Jaccard Index) and ranked $3^{rd}$ place in\nthis challenge.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 23:44:34 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2016 11:11:52 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Wang", "Pichao", ""], ["Li", "Wanqing", ""], ["Liu", "Song", ""], ["Zhang", "Yuyao", ""], ["Gao", "Zhimin", ""], ["Ogunbona", "Philip", ""]]}, {"id": "1608.06374", "submitter": "Zhangyang Wang", "authors": "Zhangyang Wang, Thomas S. Huang", "title": "Deep Double Sparsity Encoder: Learning to Sparsify Not Only Features But\n  Also Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper emphasizes the significance to jointly exploit the problem\nstructure and the parameter structure, in the context of deep modeling. As a\nspecific and interesting example, we describe the deep double sparsity encoder\n(DDSE), which is inspired by the double sparsity model for dictionary learning.\nDDSE simultaneously sparsities the output features and the learned model\nparameters, under one unified framework. In addition to its intuitive model\ninterpretation, DDSE also possesses compact model size and low complexity.\nExtensive simulations compare DDSE with several carefully-designed baselines,\nand verify the consistently superior performance of DDSE. We further apply DDSE\nto the novel application domain of brain encoding, with promising preliminary\nresults achieved.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 03:50:01 GMT"}, {"version": "v2", "created": "Sun, 2 Oct 2016 03:01:51 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Wang", "Zhangyang", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1608.06434", "submitter": "Mu Li", "authors": "Mu Li and Wangmeng Zuo and David Zhang", "title": "Convolutional Network for Attribute-driven and Identity-preserving Human\n  Face Generation", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the problem of generating human face pictures from\nspecific attributes. The existing CNN-based face generation models, however,\neither ignore the identity of the generated face or fail to preserve the\nidentity of the reference face image. Here we address this problem from the\nview of optimization, and suggest an optimization model to generate human face\nwith the given attributes while keeping the identity of the reference image.\nThe attributes can be obtained from the attribute-guided image or by tuning the\nattribute features of the reference image. With the deep convolutional network\n\"VGG-Face\", the loss is defined on the convolutional feature maps. We then\napply the gradient decent algorithm to solve this optimization problem. The\nresults validate the effectiveness of our method for attribute driven and\nidentity-preserving face generation.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 09:32:47 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Li", "Mu", ""], ["Zuo", "Wangmeng", ""], ["Zhang", "David", ""]]}, {"id": "1608.06440", "submitter": "Ahmad Masoud Dr", "authors": "Rachana Ashok Gupta, Ahmad A. Masoud, Mo-Yuen Chow", "title": "A Delay-Tolerant Potential-Field-Based Network Implementation of an\n  Integrated Navigation System", "comments": null, "journal-ref": "The IEEE Transactions On Industrial Electronics, Vol. 57, No.2,\n  February 2010, PP. 769-783", "doi": "10.1109/TIE.2009.2026764", "report-no": null, "categories": "cs.RO cs.CV cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network controllers (NCs) are devices that are capable of converting dynamic,\nspatially extended, and functionally specialized modules into a taskable\ngoal-oriented group called networked control system. This paper examines the\npractical aspects of designing and building an NC that uses the Internet as a\ncommunication medium. It focuses on finding compatible controller components\nthat can be integrated via a host structure in a manner that makes it possible\nto network, in real-time, a webcam, an unmanned ground vehicle (UGV), and a\nremote computer server along with the necessary operator software interface.\nThe aim is to deskill the UGV navigation process and yet maintain a robust\nperformance. The structure of the suggested controller, its components, and the\nmanner in which they are interfaced are described. Thorough experimental\nresults along with performance assessment and comparisons to a previously\nimplemented NC are provided.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 09:49:10 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Gupta", "Rachana Ashok", ""], ["Masoud", "Ahmad A.", ""], ["Chow", "Mo-Yuen", ""]]}, {"id": "1608.06451", "submitter": "Andreas Steger", "authors": "Andreas Steger, Radu Timofte, Luc Van Gool", "title": "Failure Detection for Facial Landmark Detectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most face applications depend heavily on the accuracy of the face and facial\nlandmarks detectors employed. Prediction of attributes such as gender, age, and\nidentity usually completely fail when the faces are badly aligned due to\ninaccurate facial landmark detection. Despite the impressive recent advances in\nface and facial landmark detection, little study is on the recovery from and\ndetection of failures or inaccurate predictions. In this work we study two top\nrecent facial landmark detectors and devise confidence models for their\noutputs. We validate our failure detection approaches on standard benchmarks\n(AFLW, HELEN) and correctly identify more than 40% of the failures in the\noutputs of the landmark detectors. Moreover, with our failure detection we can\nachieve a 12% error reduction on a gender estimation application at the cost of\na small increase in computation.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 10:28:41 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Steger", "Andreas", ""], ["Timofte", "Radu", ""], ["Van Gool", "Luc", ""]]}, {"id": "1608.06495", "submitter": "Dan Xu", "authors": "Nannan Li, Dan Xu, Zhenqiang Ying, Zhihao Li, Ge Li", "title": "Searching Action Proposals via Spatial Actionness Estimation and\n  Temporal Path Inference and Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of searching action proposals in\nunconstrained video clips. Our approach starts from actionness estimation on\nframe-level bounding boxes, and then aggregates the bounding boxes belonging to\nthe same actor across frames via linking, associating, tracking to generate\nspatial-temporal continuous action paths. To achieve the target, a novel\nactionness estimation method is firstly proposed by utilizing both human\nappearance and motion cues. Then, the association of the action paths is\nformulated as a maximum set coverage problem with the results of actionness\nestimation as a priori. To further promote the performance, we design an\nimproved optimization objective for the problem and provide a greedy search\nalgorithm to solve it. Finally, a tracking-by-detection scheme is designed to\nfurther refine the searched action paths. Extensive experiments on two\nchallenging datasets, UCF-Sports and UCF-101, show that the proposed approach\nadvances state-of-the-art proposal generation performance in terms of both\naccuracy and proposal quantity.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 13:08:30 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Li", "Nannan", ""], ["Xu", "Dan", ""], ["Ying", "Zhenqiang", ""], ["Li", "Zhihao", ""], ["Li", "Ge", ""]]}, {"id": "1608.06521", "submitter": "Vivek Sharma", "authors": "Vivek Sharma and Luc Van Gool", "title": "Does V-NIR based Image Enhancement Come with Better Features?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image enhancement using the visible (V) and near-infrared (NIR) usually\nenhances useful image details. The enhanced images are evaluated by observers\nperception, instead of quantitative feature evaluation. Thus, can we say that\nthese enhanced images using NIR information has better features in comparison\nto the computed features in the Red, Green, and Blue color channels directly?\nIn this work, we present a new method to enhance the visible images using NIR\ninformation via edge-preserving filters, and also investigate which method\nperforms best from a image features standpoint. We then show that our proposed\nenhancement method produces more stable features than the existing\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 14:27:14 GMT"}, {"version": "v2", "created": "Wed, 24 Aug 2016 09:20:59 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Sharma", "Vivek", ""], ["Van Gool", "Luc", ""]]}, {"id": "1608.06557", "submitter": "Le Hou", "authors": "Le Hou, Dimitris Samaras, Tahsin M. Kurc, Yi Gao, Joel H. Saltz", "title": "Neural Networks with Smooth Adaptive Activation Functions for Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Neural Networks (NN), Adaptive Activation Functions (AAF) have parameters\nthat control the shapes of activation functions. These parameters are trained\nalong with other parameters in the NN. AAFs have improved performance of Neural\nNetworks (NN) in multiple classification tasks. In this paper, we propose and\napply AAFs on feedforward NNs for regression tasks. We argue that applying AAFs\nin the regression (second-to-last) layer of a NN can significantly decrease the\nbias of the regression NN. However, using existing AAFs may lead to\noverfitting. To address this problem, we propose a Smooth Adaptive Activation\nFunction (SAAF) with piecewise polynomial form which can approximate any\ncontinuous function to arbitrary degree of error. NNs with SAAFs can avoid\noverfitting by simply regularizing the parameters. In particular, an NN with\nSAAFs is Lipschitz continuous given a bounded magnitude of the NN parameters.\nWe prove an upper-bound for model complexity in terms of fat-shattering\ndimension for any Lipschitz continuous regression model. Thus, regularizing the\nparameters in NNs with SAAFs avoids overfitting. We empirically evaluated NNs\nwith SAAFs and achieved state-of-the-art results on multiple regression\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 15:56:08 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Hou", "Le", ""], ["Samaras", "Dimitris", ""], ["Kurc", "Tahsin M.", ""], ["Gao", "Yi", ""], ["Saltz", "Joel H.", ""]]}, {"id": "1608.06558", "submitter": "Sona Morajab", "authors": "Sona Morajab, Mehregan Mahdavi", "title": "A Non-Local Conventional Approach for Noise Removal in 3D MRI", "comments": "1st International Conference on New Perspective in Electrical &\n  Computer Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a filtering approach for the 3D magnetic resonance imaging\n(MRI) assuming a Rician model for noise is addressed. Our denoising method is\nbased on the Conventional Approach (CA) proposed to deal with the noise issue\nin the squared domain of the acquired magnitude MRI, where the noise\ndistribution follows a Chi-square model rather than the Rician one. In the CA\nfiltering method, the local samples around each voxel is used to estimate the\nunknown signal value. Intrinsically, such a method fails to achieve the best\nresults where the underlying signal values have different statistical\nproperties. On the contrary, our proposal takes advantage of the data\nredundancy and self-similarity properties of real MR images to improve the\nnoise removal performance. In other words, in our approach, the statistical\nmomentums of the given 3D MR volume are first calculated to explore the similar\npatches inside a defined search volume. Then, these patches are put together to\nobtain the noise-free value for each voxel under processing. The experimental\nresults on the synthetic as well as the clinical MR data show our proposed\nmethod outperforms the other compared denoising filters.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 15:58:29 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Morajab", "Sona", ""], ["Mahdavi", "Mehregan", ""]]}, {"id": "1608.06627", "submitter": "Purnima Pandit", "authors": "Purnima Pandit, A. Anand", "title": "Artificial Neural Networks for Detection of Malaria in RBCs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malaria is one of the most common diseases caused by mosquitoes and is a\ngreat public health problem worldwide. Currently, for malaria diagnosis the\nstandard technique is microscopic examination of a stained blood film. We\npropose use of Artificial Neural Networks (ANN) for the diagnosis of the\ndisease in the red blood cell. For this purpose features / parameters are\ncomputed from the data obtained by the digital holographic images of the blood\ncells and is given as input to ANN which classifies the cell as the infected\none or otherwise.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 06:01:19 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Pandit", "Purnima", ""], ["Anand", "A.", ""]]}, {"id": "1608.06668", "submitter": "Edgar Gardu\\~no", "authors": "Edgar Gardu\\~no and Gabor T. Herman", "title": "Computerized Tomography with Total Variation and with Shearlets", "comments": "25 pages, 7 figures, Special Issue on Superiorization, Inverse\n  Problems 2016", "journal-ref": null, "doi": "10.1088/1361-6420/33/4/044011", "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To reduce the x-ray dose in computerized tomography (CT), many constrained\noptimization approaches have been proposed aiming at minimizing a regularizing\nfunction that measures lack of consistency with some prior knowledge about the\nobject that is being imaged, subject to a (predetermined) level of consistency\nwith the detected attenuation of x-rays. Proponents of the shearlet transform\nin the regularizing function claim that the reconstructions so obtained are\nbetter than those produced using TV for texture preservation (but may be worse\nfor noise reduction). In this paper we report results related to this claim. In\nour reported experiments using simulated CT data collection of the head,\nreconstructions whose shearlet transform has a small $\\ell_1$-norm are not more\nefficacious than reconstructions that have a small TV value. Our experiments\nfor making such comparisons use the recently-developed superiorization\nmethodology for both regularizing functions. Superiorization is an automated\nprocedure for turning an iterative algorithm for producing images that satisfy\na primary criterion (such as consistency with the observed measurements) into\nits superiorized version that will produce results that, according to the\nprimary criterion are as good as those produced by the original algorithm, but\nin addition are superior to them according to a secondary (regularizing)\ncriterion. The method presented for superiorization involving the $\\ell_1$-norm\nof the shearlet transform is novel and is quite general: It can be used for any\nregularizing function that is defined as the $\\ell_1$-norm of a transform\nspecified by the application of a matrix. Because in the previous literature\nthe split Bregman algorithm is used for similar purposes, a section is included\ncomparing the results of the superiorization algorithm with the split Bregman\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 23:38:43 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Gardu\u00f1o", "Edgar", ""], ["Herman", "Gabor T.", ""]]}, {"id": "1608.06669", "submitter": "Arun Saranathan", "authors": "Arun M. Saranathan and Mario Parente", "title": "On Clustering and Embedding Mixture Manifolds using a Low Rank\n  Neighborhood Approach", "comments": "11 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Samples from intimate (non-linear) mixtures are generally modeled as being\ndrawn from a smooth manifold. Scenarios where the data contains multiple\nintimate mixtures with some constituent materials in common can be thought of\nas manifolds which share a boundary. Two important steps in the processing of\nsuch data are (i) to identify (cluster) the different mixture-manifolds present\nin the data and (ii) to eliminate the non-linearities present the data by\nmapping each mixture-manifold into some low-dimensional euclidean space\n(embedding). Manifold clustering and embedding techniques appear to be an ideal\ntool for this task, but the present state-of-the-art algorithms perform poorly\nfor hyperspectral data, particularly in the embedding task. We propose a novel\nreconstruction-based algorithm for improved clustering and embedding of\nmixture-manifolds. The algorithms attempts to reconstruct each target-point as\nan affine combination of its nearest neighbors with an additional rank penalty\non the neighborhood to ensure that only neighbors on the same manifold as the\ntarget-point are used in the reconstruction. The reconstruction matrix\ngenerated by using this technique is block-diagonal and can be used for\nclustering (using spectral clustering) and embedding. The improved performance\nof the algorithms vis-a-vis its competitors is exhibited on a variety of\nsimulated and real mixture datasets.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 23:40:14 GMT"}, {"version": "v2", "created": "Fri, 16 Sep 2016 02:16:48 GMT"}, {"version": "v3", "created": "Sat, 12 Aug 2017 19:28:38 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Saranathan", "Arun M.", ""], ["Parente", "Mario", ""]]}, {"id": "1608.06709", "submitter": "Toru Tamaki", "authors": "Toru Tamaki, Shoji Sonoyama, Tsubasa Hirakawa, Bisser Raytchev,\n  Kazufumi Kaneda, Tetsushi Koide, Shigeto Yoshida, Hiroshi Mieno, Shinji\n  Tanaka", "title": "Computer-Aided Colorectal Tumor Classification in NBI Endoscopy Using\n  CNN Features", "comments": "5 pages, FCV2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we report results for recognizing colorectal NBI endoscopic\nimages by using features extracted from convolutional neural network (CNN). In\nthis comparative study, we extract features from different layers from\ndifferent CNN models, and then train linear SVM classifiers. Experimental\nresults with 10-fold cross validations show that features from first few\nconvolution layers are enough to achieve similar performance (i.e., recognition\nrate of 95%) with non-CNN local features such as Bag-of-Visual words, Fisher\nvector, and VLAD.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 04:36:55 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Tamaki", "Toru", ""], ["Sonoyama", "Shoji", ""], ["Hirakawa", "Tsubasa", ""], ["Raytchev", "Bisser", ""], ["Kaneda", "Kazufumi", ""], ["Koide", "Tetsushi", ""], ["Yoshida", "Shigeto", ""], ["Mieno", "Hiroshi", ""], ["Tanaka", "Shinji", ""]]}, {"id": "1608.06713", "submitter": "Toru Tamaki", "authors": "Shoji Sonoyama, Toru Tamaki, Tsubasa Hirakawa, Bisser Raytchev,\n  Kazufumi Kaneda, Tetsushi Koide, Shigeto Yoshida, Hiroshi Mieno, Shinji\n  Tanaka", "title": "Transfer Learning for Endoscopic Image Classification", "comments": "5 pages, FCV2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a method for transfer learning of endoscopic images.\nFor transferring between features obtained from images taken by different (old\nand new) endoscopes, we extend the Max-Margin Domain Transfer (MMDT) proposed\nby Hoffman et al. in order to use L2 distance constraints as regularization,\ncalled Max-Margin Domain Transfer with L2 Distance Constraints (MMDTL2).\nFurthermore, we develop the dual formulation of the optimization problem in\norder to reduce the computation cost. Experimental results demonstrate that the\nproposed MMDTL2 outperforms MMDT for real data sets taken by different\nendoscopes.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 04:51:53 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Sonoyama", "Shoji", ""], ["Tamaki", "Toru", ""], ["Hirakawa", "Tsubasa", ""], ["Raytchev", "Bisser", ""], ["Kaneda", "Kazufumi", ""], ["Koide", "Tetsushi", ""], ["Yoshida", "Shigeto", ""], ["Mieno", "Hiroshi", ""], ["Tanaka", "Shinji", ""]]}, {"id": "1608.06716", "submitter": "Mahamad Suhil", "authors": "D. S. Guru, Mahamad Suhil and P. Lolika", "title": "A Novel Approach for Shot Boundary Detection in Videos", "comments": "12 pages, 2 figures, 2 tables; Conference: Multimedia Processing,\n  Communication and Computing Applications, 2012", "journal-ref": "ICMCCA, Springer LNEE 213, pp. 209-220, Springer India (2013)", "doi": "10.1007/978-81-322-1143-3_17", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for video shot boundary detection. The\nproposed approach is based on split and merge concept. A fisher linear\ndiscriminant criterion is used to guide the process of both splitting and\nmerging. For the purpose of capturing the between class and within class\nscatter we employ 2D2 FLD method which works on texture feature of regions in\neach frame of a video. Further to reduce the complexity of the process we\npropose to employ spectral clustering to group related regions together to a\nsingle there by achieving reduction in dimension. The proposed method is\nexperimentally also validated on a cricket video. It is revealed that shots\nobtained by the proposed approach are highly cohesive and loosely coupled\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 05:19:52 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Guru", "D. S.", ""], ["Suhil", "Mahamad", ""], ["Lolika", "P.", ""]]}, {"id": "1608.06761", "submitter": "Geetanjali Kale", "authors": "Geetanjali Vinayak Kale and Varsha Hemant Patil", "title": "A Study of Vision based Human Motion Recognition and Analysis", "comments": "5 Figures, 18 Pages, International Journal of Ambient Computing and\n  Intelligence, Volume 7 Issue 2,July-December 2016", "journal-ref": null, "doi": "10.4018/IJACI.2016070104", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Vision based human motion recognition has fascinated many researchers due to\nits critical challenges and a variety of applications. The applications range\nfrom simple gesture recognition to complicated behaviour understanding in\nsurveillance system. This leads to major development in the techniques related\nto human motion representation and recognition. This paper discusses\napplications, general framework of human motion recognition, and the details of\neach of its components. The paper emphasizes on human motion representation and\nthe recognition methods along with their advantages and disadvantages. This\nstudy also discusses the selected literature, popular datasets, and concludes\nwith the challenges in the domain along with a future direction. The human\nmotion recognition domain has been active for more than two decades, and has\nprovided a large amount of literature. A bird's eye view for new researchers in\nthe domain is presented in the paper.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 09:23:14 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Kale", "Geetanjali Vinayak", ""], ["Patil", "Varsha Hemant", ""]]}, {"id": "1608.06770", "submitter": "Emanuele Sansone", "authors": "E. Sansone, K. Apostolidis, N. Conci, G. Boato, V. Mezaris, F.G.B. De\n  Natale", "title": "Automatic Synchronization of Multi-User Photo Galleries", "comments": "ACCEPTED to IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the issue of photo galleries synchronization, where\npictures related to the same event are collected by different users. Existing\nsolutions to address the problem are usually based on unrealistic assumptions,\nlike time consistency across photo galleries, and often heavily rely on\nheuristics, limiting therefore the applicability to real-world scenarios. We\npropose a solution that achieves better generalization performance for the\nsynchronization task compared to the available literature. The method is\ncharacterized by three stages: at first, deep convolutional neural network\nfeatures are used to assess the visual similarity among the photos; then, pairs\nof similar photos are detected across different galleries and used to construct\na graph; eventually, a probabilistic graphical model is used to estimate the\ntemporal offset of each pair of galleries, by traversing the minimum spanning\ntree extracted from this graph. The experimental evaluation is conducted on\nfour publicly available datasets covering different types of events,\ndemonstrating the strength of our proposed method. A thorough discussion of the\nobtained results is provided for a critical assessment of the quality in\nsynchronization.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 10:17:16 GMT"}, {"version": "v2", "created": "Mon, 16 Jan 2017 11:19:49 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Sansone", "E.", ""], ["Apostolidis", "K.", ""], ["Conci", "N.", ""], ["Boato", "G.", ""], ["Mezaris", "V.", ""], ["De Natale", "F. G. B.", ""]]}, {"id": "1608.06800", "submitter": "Javier Alejandro Aldana Iuit", "authors": "Javier Aldana-Iuit, Dmytro Mishkin, Ondrej Chum, Jiri Matas", "title": "In the Saddle: Chasing Fast and Repeatable Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel similarity-covariant feature detector that extracts points whose\nneighbourhoods, when treated as a 3D intensity surface, have a saddle-like\nintensity profile. The saddle condition is verified efficiently by intensity\ncomparisons on two concentric rings that must have exactly two dark-to-bright\nand two bright-to-dark transitions satisfying certain geometric constraints.\nExperiments show that the Saddle features are general, evenly spread and\nappearing in high density in a range of images. The Saddle detector is among\nthe fastest proposed. In comparison with detector with similar speed, the\nSaddle features show superior matching performance on number of challenging\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 12:57:34 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Aldana-Iuit", "Javier", ""], ["Mishkin", "Dmytro", ""], ["Chum", "Ondrej", ""], ["Matas", "Jiri", ""]]}, {"id": "1608.06863", "submitter": "Victoria Peterson Mrs", "authors": "Victoria Peterson, Hugo Leonardo Rufiner, Ruben Daniel Spies", "title": "Kullback-Leibler Penalized Sparse Discriminant Analysis for\n  Event-Related Potential Classification", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A brain computer interface (BCI) is a system which provides direct\ncommunication between the mind of a person and the outside world by using only\nbrain activity (EEG). The event-related potential (ERP)-based BCI problem\nconsists of a binary pattern recognition. Linear discriminant analysis (LDA) is\nwidely used to solve this type of classification problems, but it fails when\nthe number of features is large relative to the number of observations. In this\nwork we propose a penalized version of the sparse discriminant analysis (SDA),\ncalled Kullback-Leibler penalized sparse discriminant analysis (KLSDA). This\nmethod inherits both the discriminative feature selection and classification\nproperties of SDA and it also improves SDA performance through the addition of\nKullback-Leibler class discrepancy information. The KLSDA method is design to\nautomatically select the optimal regularization parameters. Numerical\nexperiments with two real ERP-EEG datasets show that this new method\noutperforms standard SDA.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 15:32:51 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Peterson", "Victoria", ""], ["Rufiner", "Hugo Leonardo", ""], ["Spies", "Ruben Daniel", ""]]}, {"id": "1608.06884", "submitter": "Hao Wang", "authors": "Hao Wang and Dit-Yan Yeung", "title": "Towards Bayesian Deep Learning: A Framework and Some Existing Methods", "comments": "To appear in IEEE Transactions on Knowledge and Data Engineering\n  (TKDE), 2016. This is a slightly shorter version of the survey\n  arXiv:1604.01662", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While perception tasks such as visual object recognition and text\nunderstanding play an important role in human intelligence, the subsequent\ntasks that involve inference, reasoning and planning require an even higher\nlevel of intelligence. The past few years have seen major advances in many\nperception tasks using deep learning models. For higher-level inference,\nhowever, probabilistic graphical models with their Bayesian nature are still\nmore powerful and flexible. To achieve integrated intelligence that involves\nboth perception and inference, it is naturally desirable to tightly integrate\ndeep learning and Bayesian models within a principled probabilistic framework,\nwhich we call Bayesian deep learning. In this unified framework, the perception\nof text or images using deep learning can boost the performance of higher-level\ninference and in return, the feedback from the inference process is able to\nenhance the perception of text or images. This paper proposes a general\nframework for Bayesian deep learning and reviews its recent applications on\nrecommender systems, topic models, and control. In this paper, we also discuss\nthe relationship and differences between Bayesian deep learning and other\nrelated topics like Bayesian treatment of neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 16:15:22 GMT"}, {"version": "v2", "created": "Sat, 3 Sep 2016 15:32:04 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Wang", "Hao", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1608.06891", "submitter": "Bronislav P\\v{r}ibyl", "authors": "Bronislav P\\v{r}ibyl, Pavel Zem\\v{c}\\'ik, Martin \\v{C}ad\\'ik", "title": "Absolute Pose Estimation from Line Correspondences using Direct Linear\n  Transformation", "comments": "37 pages, 6 figures, 4 tables. Accepted for publication in Computer\n  Vision and Image Understanding", "journal-ref": null, "doi": "10.1016/j.cviu.2017.05.002", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is concerned with camera pose estimation from correspondences of\n3D/2D lines, i. e. with the Perspective-n-Line (PnL) problem. We focus on large\nline sets, which can be efficiently solved by methods using linear formulation\nof PnL. We propose a novel method \"DLT-Combined-Lines\" based on the Direct\nLinear Transformation (DLT) algorithm, which benefits from a new combination of\ntwo existing DLT methods for pose estimation. The method represents 2D\nstructure by lines, and 3D structure by both points and lines. The redundant 3D\ninformation reduces the minimum required line correspondences to 5. A\ncornerstone of the method is a combined projection matri xestimated by the DLT\nalgorithm. It contains multiple estimates of camera rotation and translation,\nwhich can be recovered after enforcing constraints of the matrix. Multiplicity\nof the estimates is exploited to improve the accuracy of the proposed method.\nFor large line sets (10 and more), the method is comparable to the\nstate-of-theart in accuracy of orientation estimation. It achieves\nstate-of-the-art accuracy in estimation of camera position and it yields the\nsmallest reprojection error under strong image noise. The method achieves top-3\nresults on real world data. The proposed method is also highly computationally\neffective, estimating the pose of 1000 lines in 12 ms on a desktop computer.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 16:37:03 GMT"}, {"version": "v2", "created": "Sat, 13 May 2017 10:38:02 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["P\u0159ibyl", "Bronislav", ""], ["Zem\u010d\u00edk", "Pavel", ""], ["\u010cad\u00edk", "Martin", ""]]}, {"id": "1608.06985", "submitter": "Ting-Chun Wang", "authors": "Ting-Chun Wang, Jun-Yan Zhu, Ebi Hiroaki, Manmohan Chandraker, Alexei\n  A. Efros, Ravi Ramamoorthi", "title": "A 4D Light-Field Dataset and CNN Architectures for Material Recognition", "comments": "European Conference on Computer Vision (ECCV) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new light-field dataset of materials, and take advantage of\nthe recent success of deep learning to perform material recognition on the 4D\nlight-field. Our dataset contains 12 material categories, each with 100 images\ntaken with a Lytro Illum, from which we extract about 30,000 patches in total.\nTo the best of our knowledge, this is the first mid-size dataset for\nlight-field images. Our main goal is to investigate whether the additional\ninformation in a light-field (such as multiple sub-aperture views and\nview-dependent reflectance effects) can aid material recognition. Since\nrecognition networks have not been trained on 4D images before, we propose and\ncompare several novel CNN architectures to train on light-field images. In our\nexperiments, the best performing CNN architecture achieves a 7% boost compared\nwith 2D image classification (70% to 77%). These results constitute important\nbaselines that can spur further research in the use of CNNs for light-field\napplications. Upon publication, our dataset also enables other novel\napplications of light-fields, including object detection, image segmentation\nand view interpolation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 23:30:22 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Wang", "Ting-Chun", ""], ["Zhu", "Jun-Yan", ""], ["Hiroaki", "Ebi", ""], ["Chandraker", "Manmohan", ""], ["Efros", "Alexei A.", ""], ["Ramamoorthi", "Ravi", ""]]}, {"id": "1608.06993", "submitter": "Gao Huang", "authors": "Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger", "title": "Densely Connected Convolutional Networks", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that convolutional networks can be substantially\ndeeper, more accurate, and efficient to train if they contain shorter\nconnections between layers close to the input and those close to the output. In\nthis paper, we embrace this observation and introduce the Dense Convolutional\nNetwork (DenseNet), which connects each layer to every other layer in a\nfeed-forward fashion. Whereas traditional convolutional networks with L layers\nhave L connections - one between each layer and its subsequent layer - our\nnetwork has L(L+1)/2 direct connections. For each layer, the feature-maps of\nall preceding layers are used as inputs, and its own feature-maps are used as\ninputs into all subsequent layers. DenseNets have several compelling\nadvantages: they alleviate the vanishing-gradient problem, strengthen feature\npropagation, encourage feature reuse, and substantially reduce the number of\nparameters. We evaluate our proposed architecture on four highly competitive\nobject recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet).\nDenseNets obtain significant improvements over the state-of-the-art on most of\nthem, whilst requiring less computation to achieve high performance. Code and\npre-trained models are available at https://github.com/liuzhuang13/DenseNet .\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 00:44:55 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 14:50:55 GMT"}, {"version": "v3", "created": "Sat, 3 Dec 2016 08:35:43 GMT"}, {"version": "v4", "created": "Sun, 27 Aug 2017 02:56:24 GMT"}, {"version": "v5", "created": "Sun, 28 Jan 2018 17:12:02 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Huang", "Gao", ""], ["Liu", "Zhuang", ""], ["van der Maaten", "Laurens", ""], ["Weinberger", "Kilian Q.", ""]]}, {"id": "1608.07017", "submitter": "Andrew Owens", "authors": "Andrew Owens, Jiajun Wu, Josh H. McDermott, William T. Freeman,\n  Antonio Torralba", "title": "Ambient Sound Provides Supervision for Visual Learning", "comments": "ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sound of crashing waves, the roar of fast-moving cars -- sound conveys\nimportant information about the objects in our surroundings. In this work, we\nshow that ambient sounds can be used as a supervisory signal for learning\nvisual models. To demonstrate this, we train a convolutional neural network to\npredict a statistical summary of the sound associated with a video frame. We\nshow that, through this process, the network learns a representation that\nconveys information about objects and scenes. We evaluate this representation\non several recognition tasks, finding that its performance is comparable to\nthat of other state-of-the-art unsupervised learning methods. Finally, we show\nthrough visualizations that the network learns units that are selective to\nobjects that are often associated with characteristic sounds.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 04:50:16 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 19:14:26 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Owens", "Andrew", ""], ["Wu", "Jiajun", ""], ["McDermott", "Josh H.", ""], ["Freeman", "William T.", ""], ["Torralba", "Antonio", ""]]}, {"id": "1608.07068", "submitter": "Kuo-Hao Zeng", "authors": "Kuo-Hao Zeng and Tseng-Hung Chen and Juan Carlos Niebles and Min Sun", "title": "Title Generation for User Generated Videos", "comments": "14 pages, 4 figures, ECCV2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A great video title describes the most salient event compactly and captures\nthe viewer's attention. In contrast, video captioning tends to generate\nsentences that describe the video as a whole. Although generating a video title\nautomatically is a very useful task, it is much less addressed than video\ncaptioning. We address video title generation for the first time by proposing\ntwo methods that extend state-of-the-art video captioners to this new task.\nFirst, we make video captioners highlight sensitive by priming them with a\nhighlight detector. Our framework allows for jointly training a model for title\ngeneration and video highlight localization. Second, we induce high sentence\ndiversity in video captioners, so that the generated titles are also diverse\nand catchy. This means that a large number of sentences might be required to\nlearn the sentence structure of titles. Hence, we propose a novel sentence\naugmentation method to train a captioner with additional sentence-only examples\nthat come without corresponding videos. We collected a large-scale Video Titles\nin the Wild (VTW) dataset of 18100 automatically crawled user-generated videos\nand titles. On VTW, our methods consistently improve title prediction accuracy,\nand achieve the best performance in both automatic and human evaluation.\nFinally, our sentence augmentation method also outperforms the baselines on the\nM-VAD dataset.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 09:49:23 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2016 17:36:13 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Zeng", "Kuo-Hao", ""], ["Chen", "Tseng-Hung", ""], ["Niebles", "Juan Carlos", ""], ["Sun", "Min", ""]]}, {"id": "1608.07138", "submitter": "Cesar Roberto de Souza", "authors": "C\\'esar Roberto de Souza, Adrien Gaidon, Eleonora Vig, Antonio Manuel\n  L\\'opez", "title": "Sympathy for the Details: Dense Trajectories and Hybrid Classification\n  Architectures for Action Recognition", "comments": "Accepted for publication in the 14th European Conference on Computer\n  Vision (ECCV), Amsterdam, 2016, plus supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition in videos is a challenging task due to the complexity of\nthe spatio-temporal patterns to model and the difficulty to acquire and learn\non large quantities of video data. Deep learning, although a breakthrough for\nimage classification and showing promise for videos, has still not clearly\nsuperseded action recognition methods using hand-crafted features, even when\ntraining on massive datasets. In this paper, we introduce hybrid video\nclassification architectures based on carefully designed unsupervised\nrepresentations of hand-crafted spatio-temporal features classified by\nsupervised deep networks. As we show in our experiments on five popular\nbenchmarks for action recognition, our hybrid model combines the best of both\nworlds: it is data efficient (trained on 150 to 10000 short clips) and yet\nimproves significantly on the state of the art, including recent deep models\ntrained on millions of manually labelled images and videos.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 13:37:15 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["de Souza", "C\u00e9sar Roberto", ""], ["Gaidon", "Adrien", ""], ["Vig", "Eleonora", ""], ["L\u00f3pez", "Antonio Manuel", ""]]}, {"id": "1608.07242", "submitter": "Mooyeol Baek", "authors": "Hyeonseob Nam, Mooyeol Baek, Bohyung Han", "title": "Modeling and Propagating CNNs in a Tree Structure for Visual Tracking", "comments": "10 pages, Hyeonseob Nam and Mooyeol Baek have equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an online visual tracking algorithm by managing multiple target\nappearance models in a tree structure. The proposed algorithm employs\nConvolutional Neural Networks (CNNs) to represent target appearances, where\nmultiple CNNs collaborate to estimate target states and determine the desirable\npaths for online model updates in the tree. By maintaining multiple CNNs in\ndiverse branches of tree structure, it is convenient to deal with\nmulti-modality in target appearances and preserve model reliability through\nsmooth updates along tree paths. Since multiple CNNs share all parameters in\nconvolutional layers, it takes advantage of multiple models with little extra\ncost by saving memory space and avoiding redundant network evaluations. The\nfinal target state is estimated by sampling target candidates around the state\nin the previous frame and identifying the best sample in terms of a weighted\naverage score from a set of active CNNs. Our algorithm illustrates outstanding\nperformance compared to the state-of-the-art techniques in challenging datasets\nsuch as online tracking benchmark and visual object tracking challenge.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 18:29:53 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Nam", "Hyeonseob", ""], ["Baek", "Mooyeol", ""], ["Han", "Bohyung", ""]]}, {"id": "1608.07338", "submitter": "Daniel L. Marino", "authors": "Daniel L. Marino, Milos Manic", "title": "Fast Trajectory Simplification Algorithm for Natural User Interfaces in\n  Robot Programming by Demonstration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory simplification is a problem encountered in areas like Robot\nprogramming by demonstration, CAD/CAM, computer vision, and in GPS-based\napplications like traffic analysis. This problem entails reduction of the\npoints in a given trajectory while keeping the relevant points which preserve\nimportant information. The benefits include storage reduction, computational\nexpense, while making data more manageable. Common techniques formulate a\nminimization problem to be solved, where the solution is found iteratively\nunder some error metric, which causes the algorithms to work in super-linear\ntime. We present an algorithm called FastSTray, which selects the relevant\npoints in the trajectory in linear time by following an open loop heuristic\napproach. While most current trajectory simplification algorithms are tailored\nfor GPS trajectories, our approach focuses on smooth trajectories for robot\nprogramming by demonstration recorded using motion capture systems.Two\nvariations of the algorithm are presented: 1. aims to preserve shape and\ntemporal information; 2. preserves only shape information. Using the points in\nthe simplified trajectory we use cubic splines to interpolate between these\npoints and recreate the original trajectory. The presented algorithm was tested\non trajectories recorded from a hand-tracking system. It was able to eliminate\nabout 90% of the points in the original trajectories while maintaining errors\nbetween 0.78-2cm which corresponds to 1%-2.4% relative error with respect to\nthe bounding box of the trajectories.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 23:33:23 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Marino", "Daniel L.", ""], ["Manic", "Milos", ""]]}, {"id": "1608.07365", "submitter": "Xing Wang", "authors": "Xing Wang and Jie Liang", "title": "Scalable Compression of Deep Neural Networks", "comments": "5 pages, 4 figures, ACM Multimedia 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks generally involve some layers with mil- lions of\nparameters, making them difficult to be deployed and updated on devices with\nlimited resources such as mobile phones and other smart embedded systems. In\nthis paper, we propose a scalable representation of the network parameters, so\nthat different applications can select the most suitable bit rate of the\nnetwork based on their own storage constraints. Moreover, when a device needs\nto upgrade to a high-rate network, the existing low-rate network can be reused,\nand only some incremental data are needed to be downloaded. We first\nhierarchically quantize the weights of a pre-trained deep neural network to\nenforce weight sharing. Next, we adaptively select the bits assigned to each\nlayer given the total bit budget. After that, we retrain the network to\nfine-tune the quantized centroids. Experimental results show that our method\ncan achieve scalable compression with graceful degradation in the performance.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 05:08:24 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Wang", "Xing", ""], ["Liang", "Jie", ""]]}, {"id": "1608.07411", "submitter": "Wadim Kehl", "authors": "Wadim Kehl, Tobias Holl, Federico Tombari, Slobodan Ilic, Nassir Navab", "title": "An Octree-Based Approach towards Efficient Variational Range Data Fusion", "comments": "BMVC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volume-based reconstruction is usually expensive both in terms of memory\nconsumption and runtime. Especially for sparse geometric structures, volumetric\nrepresentations produce a huge computational overhead. We present an efficient\nway to fuse range data via a variational Octree-based minimization approach by\ntaking the actual range data geometry into account. We transform the data into\nOctree-based truncated signed distance fields and show how the optimization can\nbe conducted on the newly created structures. The main challenge is to uphold\nspeed and a low memory footprint without sacrificing the solutions' accuracy\nduring optimization. We explain how to dynamically adjust the optimizer's\ngeometric structure via joining/splitting of Octree nodes and how to define the\noperators. We evaluate on various datasets and outline the suitability in terms\nof performance and geometric accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 10:01:51 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Kehl", "Wadim", ""], ["Holl", "Tobias", ""], ["Tombari", "Federico", ""], ["Ilic", "Slobodan", ""], ["Navab", "Nassir", ""]]}, {"id": "1608.07433", "submitter": "Hossein Ziaei Nafchi", "authors": "Hossein Ziaei Nafchi, Atena Shahkolaei, Rachid Hedjam, Mohamed Cheriet", "title": "Mean Deviation Similarity Index: Efficient and Reliable Full-Reference\n  Image Quality Evaluator", "comments": "11 pages, 8 figures, 6 tables", "journal-ref": null, "doi": "10.1109/ACCESS.2016.2604042", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications of perceptual image quality assessment (IQA) in image and video\nprocessing, such as image acquisition, image compression, image restoration and\nmultimedia communication, have led to the development of many IQA metrics. In\nthis paper, a reliable full reference IQA model is proposed that utilize\ngradient similarity (GS), chromaticity similarity (CS), and deviation pooling\n(DP). By considering the shortcomings of the commonly used GS to model human\nvisual system (HVS), a new GS is proposed through a fusion technique that is\nmore likely to follow HVS. We propose an efficient and effective formulation to\ncalculate the joint similarity map of two chromatic channels for the purpose of\nmeasuring color changes. In comparison with a commonly used formulation in the\nliterature, the proposed CS map is shown to be more efficient and provide\ncomparable or better quality predictions. Motivated by a recent work that\nutilizes the standard deviation pooling, a general formulation of the DP is\npresented in this paper and used to compute a final score from the proposed GS\nand CS maps. This proposed formulation of DP benefits from the Minkowski\npooling and a proposed power pooling as well. The experimental results on six\ndatasets of natural images, a synthetic dataset, and a digitally retouched\ndataset show that the proposed index provides comparable or better quality\npredictions than the most recent and competing state-of-the-art IQA metrics in\nthe literature, it is reliable and has low complexity. The MATLAB source code\nof the proposed metric is available at\nhttps://www.mathworks.com/matlabcentral/fileexchange/59809.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 12:16:09 GMT"}, {"version": "v2", "created": "Mon, 29 Aug 2016 12:10:59 GMT"}, {"version": "v3", "created": "Fri, 16 Sep 2016 08:17:07 GMT"}, {"version": "v4", "created": "Wed, 19 Apr 2017 05:41:11 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Nafchi", "Hossein Ziaei", ""], ["Shahkolaei", "Atena", ""], ["Hedjam", "Rachid", ""], ["Cheriet", "Mohamed", ""]]}, {"id": "1608.07441", "submitter": "Maxime Bucher", "authors": "Maxime Bucher (Palaiseau), St\\'ephane Herbin (Palaiseau), Fr\\'ed\\'eric\n  Jurie", "title": "Hard Negative Mining for Metric Learning Based Zero-Shot Classification", "comments": null, "journal-ref": "ECCV 16 WS TASK-CV: Transferring and Adapting Source Knowledge in\n  Computer Vision, Oct 2016, Amsterdam, Netherlands. ECCV 16 WS TASK-CV:\n  Transferring and Adapting Source Knowledge in Computer Vision", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-Shot learning has been shown to be an efficient strategy for domain\nadaptation. In this context, this paper builds on the recent work of Bucher et\nal. [1], which proposed an approach to solve Zero-Shot classification problems\n(ZSC) by introducing a novel metric learning based objective function. This\nobjective function allows to learn an optimal embedding of the attributes\njointly with a measure of similarity between images and attributes. This paper\nextends their approach by proposing several schemes to control the generation\nof the negative pairs, resulting in a significant improvement of the\nperformance and giving above state-of-the-art results on three challenging ZSC\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 12:42:43 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Bucher", "Maxime", "", "Palaiseau"], ["Herbin", "St\u00e9phane", "", "Palaiseau"], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1608.07444", "submitter": "Qin Zou", "authors": "Qin Zou, Zheng Zhang, Qian Wang, Qingquan Li, Long Chen, Song Wang", "title": "Who Leads the Clothing Fashion: Style, Color, or Texture? A\n  Computational Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that clothing fashion is a distinctive and often habitual\ntrend in the style in which a person dresses. Clothing fashions are usually\nexpressed with visual stimuli such as style, color, and texture. However, it is\nnot clear which visual stimulus places higher/lower influence on the updating\nof clothing fashion. In this study, computer vision and machine learning\ntechniques are employed to analyze the influence of different visual stimuli on\nclothing-fashion updates. Specifically, a classification-based model is\nproposed to quantify the influence of different visual stimuli, in which each\nvisual stimulus's influence is quantified by its corresponding accuracy in\nfashion classification. Experimental results demonstrate that, on\nclothing-fashion updates, the style holds a higher influence than the color,\nand the color holds a higher influence than the texture.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 12:53:56 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Zou", "Qin", ""], ["Zhang", "Zheng", ""], ["Wang", "Qian", ""], ["Li", "Qingquan", ""], ["Chen", "Long", ""], ["Wang", "Song", ""]]}, {"id": "1608.07454", "submitter": "Vincent Lepetit", "authors": "Tadej Vodopivec, Vincent Lepetit, Peter Peer", "title": "Fine Hand Segmentation using Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for extracting very accurate masks of hands in egocentric\nviews. Our method is based on a novel Deep Learning architecture: In contrast\nwith current Deep Learning methods, we do not use upscaling layers applied to a\nlow-dimensional representation of the input image. Instead, we extract features\nwith convolutional layers and map them directly to a segmentation mask with a\nfully connected layer. We show that this approach, when applied in a\nmulti-scale fashion, is both accurate and efficient enough for real-time. We\ndemonstrate it on a new dataset made of images captured in various\nenvironments, from the outdoors to offices.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 13:40:08 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Vodopivec", "Tadej", ""], ["Lepetit", "Vincent", ""], ["Peer", "Peter", ""]]}, {"id": "1608.07470", "submitter": "Qi Jia", "authors": "Qi Jia, Xin Fan, Zhongxuan Luo, Lianbo Song, and Tie Qiu", "title": "A Fast Ellipse Detector Using Projective Invariant Pruning", "comments": "14 pages, 34 figures, journal", "journal-ref": null, "doi": "10.1109/TIP.2017.2704660", "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting elliptical objects from an image is a central task in robot\nnavigation and industrial diagnosis where the detection time is always a\ncritical issue. Existing methods are hardly applicable to these real-time\nscenarios of limited hardware resource due to the huge number of fragment\ncandidates (edges or arcs) for fitting ellipse equations. In this paper, we\npresent a fast algorithm detecting ellipses with high accuracy. The algorithm\nleverage a newly developed projective invariant to significantly prune the\nundesired candidates and to pick out elliptical ones. The invariant is able to\nreflect the intrinsic geometry of a planar curve, giving the value of -1 on any\nthree collinear points and +1 for any six points on an ellipse. Thus, we apply\nthe pruning and picking by simply comparing these binary values. Moreover, the\ncalculation of the invariant only involves the determinant of a 3*3 matrix.\nExtensive experiments on three challenging data sets with 650 images\ndemonstrate that our detector runs 20%-50% faster than the state-of-the-art\nalgorithms with the comparable or higher precision.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 14:25:15 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Jia", "Qi", ""], ["Fan", "Xin", ""], ["Luo", "Zhongxuan", ""], ["Song", "Lianbo", ""], ["Qiu", "Tie", ""]]}, {"id": "1608.07616", "submitter": "Fausto Milletari", "authors": "Gerda Bortsova, Michael Sterr, Lichao Wang, Fausto Milletari, Nassir\n  Navab, Anika B\\\"ottcher, Heiko Lickert, Fabian Theis and Tingying Peng", "title": "Mitosis Detection in Intestinal Crypt Images with Hough Forest and\n  Conditional Random Fields", "comments": "Accepted at the 7th International Conference on Machine Learning in\n  Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intestinal enteroendocrine cells secrete hormones that are vital for the\nregulation of glucose metabolism but their differentiation from intestinal stem\ncells is not fully understood. Asymmetric stem cell divisions have been linked\nto intestinal stem cell homeostasis and secretory fate commitment. We monitored\ncell divisions using 4D live cell imaging of cultured intestinal crypts to\ncharacterize division modes by means of measurable features such as orientation\nor shape. A statistical analysis of these measurements requires annotation of\nmitosis events, which is currently a tedious and time-consuming task that has\nto be performed manually. To assist data processing, we developed a learning\nbased method to automatically detect mitosis events. The method contains a\ndual-phase framework for joint detection of dividing cells (mothers) and their\nprogeny (daughters). In the first phase we detect mother and daughters\nindependently using Hough Forest whilst in the second phase we associate mother\nand daughters by modelling their joint probability as Conditional Random Field\n(CRF). The method has been evaluated on 32 movies and has achieved an AUC of\n72%, which can be used in conjunction with manual correction and dramatically\nspeed up the processing pipeline.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 21:53:30 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Bortsova", "Gerda", ""], ["Sterr", "Michael", ""], ["Wang", "Lichao", ""], ["Milletari", "Fausto", ""], ["Navab", "Nassir", ""], ["B\u00f6ttcher", "Anika", ""], ["Lickert", "Heiko", ""], ["Theis", "Fabian", ""], ["Peng", "Tingying", ""]]}, {"id": "1608.07639", "submitter": "Yuval Atzmon", "authors": "Yuval Atzmon, Jonathan Berant, Vahid Kezami, Amir Globerson and Gal\n  Chechik", "title": "Learning to generalize to new compositions in image understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks have recently been used for learning to describe\nimages using natural language. However, it has been observed that these models\ngeneralize poorly to scenes that were not observed during training, possibly\ndepending too strongly on the statistics of the text in the training data. Here\nwe propose to describe images using short structured representations, aiming to\ncapture the crux of a description. These structured representations allow us to\ntease-out and evaluate separately two types of generalization: standard\ngeneralization to new images with similar scenes, and generalization to new\ncombinations of known entities. We compare two learning approaches on the\nMS-COCO dataset: a state-of-the-art recurrent network based on an LSTM (Show,\nAttend and Tell), and a simple structured prediction model on top of a deep\nnetwork. We find that the structured model generalizes to new compositions\nsubstantially better than the LSTM, ~7 times the accuracy of predicting\nstructured representations. By providing a concrete method to quantify\ngeneralization for unseen combinations, we argue that structured\nrepresentations and compositional splits are a useful benchmark for image\ncaptioning, and advocate compositional models that capture linguistic and\nvisual structure.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 00:34:00 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Atzmon", "Yuval", ""], ["Berant", "Jonathan", ""], ["Kezami", "Vahid", ""], ["Globerson", "Amir", ""], ["Chechik", "Gal", ""]]}, {"id": "1608.07664", "submitter": "Tian Lan", "authors": "Jianhong Wang, Tian Lan, Xu Zhang, Limin Luo", "title": "Spatio-temporal Aware Non-negative Component Representation for Action\n  Recognition", "comments": "11 pages, 5 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel mid-level representation for action recognition,\nnamed spatio-temporal aware non-negative component representation (STANNCR).\nThe proposed STANNCR is based on action component and incorporates the\nspatial-temporal information. We first introduce a spatial-temporal\ndistribution vector (STDV) to model the distributions of local feature\nlocations in a compact and discriminative manner. Then we employ non-negative\nmatrix factorization (NMF) to learn the action components and encode the video\nsamples. The action component considers the correlations of visual words, which\neffectively bridge the sematic gap in action recognition. To incorporate the\nspatial-temporal cues for final representation, the STDV is used as the part of\ngraph regularization for NMF. The fusion of spatial-temporal information makes\nthe STANNCR more discriminative, and our fusion manner is more compact than\ntraditional method of concatenating vectors. The proposed approach is\nextensively evaluated on three public datasets. The experimental results\ndemonstrate the effectiveness of STANNCR for action recognition.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 06:30:34 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Wang", "Jianhong", ""], ["Lan", "Tian", ""], ["Zhang", "Xu", ""], ["Luo", "Limin", ""]]}, {"id": "1608.07706", "submitter": "Xiaojie Jin Mr.", "authors": "Xiaojie Jin, Yunpeng Chen, Jiashi Feng, Zequn Jie, Shuicheng Yan", "title": "Multi-Path Feedback Recurrent Neural Network for Scene Parsing", "comments": "Accepted by AAAI-17. Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the scene parsing problem and propose a novel\nMulti-Path Feedback recurrent neural network (MPF-RNN) for parsing scene\nimages. MPF-RNN can enhance the capability of RNNs in modeling long-range\ncontext information at multiple levels and better distinguish pixels that are\neasy to confuse. Different from feedforward CNNs and RNNs with only single\nfeedback, MPF-RNN propagates the contextual features learned at top layer\nthrough \\textit{multiple} weighted recurrent connections to learn bottom\nfeatures. For better training MPF-RNN, we propose a new strategy that considers\naccumulative loss at multiple recurrent steps to improve performance of the\nMPF-RNN on parsing small objects. With these two novel components, MPF-RNN has\nachieved significant improvement over strong baselines (VGG16 and Res101) on\nfive challenging scene parsing benchmarks, including traditional SiftFlow,\nBarcelona, CamVid, Stanford Background as well as the recently released\nlarge-scale ADE20K.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 13:19:23 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 16:52:40 GMT"}, {"version": "v3", "created": "Tue, 22 Nov 2016 11:44:06 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Jin", "Xiaojie", ""], ["Chen", "Yunpeng", ""], ["Feng", "Jiashi", ""], ["Jie", "Zequn", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1608.07711", "submitter": "Xiaozhi Chen", "authors": "Xiaozhi Chen and Kaustav Kundu and Yukun Zhu and Huimin Ma and Sanja\n  Fidler and Raquel Urtasun", "title": "3D Object Proposals using Stereo Imagery for Accurate Object Class\n  Detection", "comments": "14 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to perform 3D object detection in the context of\nautonomous driving. Our method first aims at generating a set of high-quality\n3D object proposals by exploiting stereo imagery. We formulate the problem as\nminimizing an energy function that encodes object size priors, placement of\nobjects on the ground plane as well as several depth informed features that\nreason about free space, point cloud densities and distance to the ground. We\nthen exploit a CNN on top of these proposals to perform object detection. In\nparticular, we employ a convolutional neural net (CNN) that exploits context\nand depth information to jointly regress to 3D bounding box coordinates and\nobject pose. Our experiments show significant performance gains over existing\nRGB and RGB-D object proposal methods on the challenging KITTI benchmark. When\ncombined with the CNN, our approach outperforms all existing results in object\ndetection and orientation estimation tasks for all three KITTI object classes.\nFurthermore, we experiment also with the setting where LIDAR information is\navailable, and show that using both LIDAR and stereo leads to the best result.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 13:39:04 GMT"}, {"version": "v2", "created": "Tue, 25 Apr 2017 07:58:07 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Chen", "Xiaozhi", ""], ["Kundu", "Kaustav", ""], ["Zhu", "Yukun", ""], ["Ma", "Huimin", ""], ["Fidler", "Sanja", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1608.07724", "submitter": "Yipin Zhou", "authors": "Yipin Zhou, Tamara L. Berg", "title": "Learning Temporal Transformations From Time-Lapse Videos", "comments": "ECCV2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on life-long observations of physical, chemical, and biologic phenomena\nin the natural world, humans can often easily picture in their minds what an\nobject will look like in the future. But, what about computers? In this paper,\nwe learn computational models of object transformations from time-lapse videos.\nIn particular, we explore the use of generative models to create depictions of\nobjects at future times. These models explore several different prediction\ntasks: generating a future state given a single depiction of an object,\ngenerating a future state given two depictions of an object at different times,\nand generating future states recursively in a recurrent framework. We provide\nboth qualitative and quantitative evaluations of the generated results, and\nalso conduct a human evaluation to compare variations of our models.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 16:33:48 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Zhou", "Yipin", ""], ["Berg", "Tamara L.", ""]]}, {"id": "1608.07802", "submitter": "Mohamed Aly", "authors": "Mohamed Aly and Wolfgang Heidrich", "title": "MindX: Denoising Mixed Impulse Poisson-Gaussian Noise Using Proximal\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm for blind denoising of images corrupted by mixed\nimpulse, Poisson, and Gaussian noises. The algorithm starts by applying the\nAnscombe variance-stabilizing transformation to convert the Poisson into white\nGaussian noise. Then it applies a combinatorial optimization technique to\ndenoise the mixed impulse Gaussian noise using proximal algorithms. The result\nis then processed by the inverse Anscombe transform. We compare our algorithm\nto state of the art methods on standard images, and show its superior\nperformance in various noise conditions.\n", "versions": [{"version": "v1", "created": "Sun, 28 Aug 2016 12:07:01 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Aly", "Mohamed", ""], ["Heidrich", "Wolfgang", ""]]}, {"id": "1608.07807", "submitter": "Chandrajit M", "authors": "Chandrajit M, Girisha R, Vasudev T and Ashok C B", "title": "Cast and Self Shadow Segmentation in Video Sequences using Interval\n  based Eigen Value Representation", "comments": "6 pages journal article", "journal-ref": "International Journal of Computer Applications 142(4):27-32, May\n  2016", "doi": "10.5120/ijca2016909752", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking of motion objects in the surveillance videos is useful for the\nmonitoring and analysis. The performance of the surveillance system will\ndeteriorate when shadows are detected as moving objects. Therefore, shadow\ndetection and elimination usually benefits the next stages. To overcome this\nissue, a method for detection and elimination of shadows is proposed. This\npaper presents a method for segmenting moving objects in video sequences based\non determining the Euclidian distance between two pixels considering\nneighborhood values in temporal domain. Further, a method that segments cast\nand self shadows in video sequences by computing the Eigen values for the\nneighborhood of each pixel is proposed. The dual-map for cast and self shadow\npixels is represented based on the interval of Eigen values. The proposed\nmethods are tested on the benchmark IEEE CHANGE DETECTION 2014 dataset.\n", "versions": [{"version": "v1", "created": "Sun, 28 Aug 2016 13:07:16 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["M", "Chandrajit", ""], ["R", "Girisha", ""], ["T", "Vasudev", ""], ["B", "Ashok C", ""]]}, {"id": "1608.07813", "submitter": "Trinh Van Chien", "authors": "Trinh Van Chien and Khanh Quoc Dinh and Viet Anh Nguyen and Byeungwoo\n  Jeon", "title": "Total variation reconstruction for compressive sensing using nonlocal\n  Lagrangian multiplier", "comments": "5 pages, 3 figures, 3 tables. EUSIPCO2014. Matlab software:\n  https://www.researchgate.net/profile/Trinh_Chien/publications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Total variation has proved its effectiveness in solving inverse problems for\ncompressive sensing. Besides, the nonlocal means filter used as regularization\npreserves texture better for recovered images, but it is quite complex to\nimplement. In this paper, based on existence of both noise and image\ninformation in the Lagrangian multiplier, we propose a simple method in term of\nimplementation called nonlocal Lagrangian multiplier (NLLM) in order to reduce\nnoise and boost useful image information. Experimental results show that the\nproposed NLLM is superior both in subjective and objective qualities of\nrecovered image over other recovery algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 28 Aug 2016 14:11:02 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Van Chien", "Trinh", ""], ["Dinh", "Khanh Quoc", ""], ["Nguyen", "Viet Anh", ""], ["Jeon", "Byeungwoo", ""]]}, {"id": "1608.07876", "submitter": "Hirokatsu Kataoka", "authors": "Yun He, Soma Shirakabe, Yutaka Satoh, Hirokatsu Kataoka", "title": "Human Action Recognition without Human", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this paper is to evaluate \"human action recognition without\nhuman\". Motion representation is frequently discussed in human action\nrecognition. We have examined several sophisticated options, such as dense\ntrajectories (DT) and the two-stream convolutional neural network (CNN).\nHowever, some features from the background could be too strong, as shown in\nsome recent studies on human action recognition. Therefore, we considered\nwhether a background sequence alone can classify human actions in current\nlarge-scale action datasets (e.g., UCF101).\n  In this paper, we propose a novel concept for human action analysis that is\nnamed \"human action recognition without human\". An experiment clearly shows the\neffect of a background sequence for understanding an action label.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 01:22:38 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["He", "Yun", ""], ["Shirakabe", "Soma", ""], ["Satoh", "Yutaka", ""], ["Kataoka", "Hirokatsu", ""]]}, {"id": "1608.07897", "submitter": "Qinghai Gao", "authors": "Qinghai Gao", "title": "Using k-nearest neighbors to construct cancelable minutiae templates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprint is widely used in a variety of applications. Security measures\nhave to be taken to protect the privacy of fingerprint data. Cancelable\nbiometrics is proposed as an effective mechanism of using and protecting\nbiometrics. In this paper we propose a new method of constructing cancelable\nfingerprint template by combining real template with synthetic template.\nSpecifically, each user is given one synthetic minutia template generated with\nrandom number generator. Every minutia point from the real template is\nindividually thrown into the synthetic template, from which its k-nearest\nneighbors are found. The verification template is constructed by combining an\narbitrary set of the k-nearest neighbors. To prove the validity of the scheme,\ntesting is carried out on three databases. The results show that the\nconstructed templates satisfy the requirements of cancelable biometrics.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 02:48:32 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 12:32:20 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Gao", "Qinghai", ""]]}, {"id": "1608.07916", "submitter": "Bo Li", "authors": "Bo Li, Tianlei Zhang, Tian Xia", "title": "Vehicle Detection from 3D Lidar Using Fully Convolutional Network", "comments": "Robotics: Science and Systems, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional network techniques have recently achieved great success in\nvision based detection tasks. This paper introduces the recent development of\nour research on transplanting the fully convolutional network technique to the\ndetection tasks on 3D range scan data. Specifically, the scenario is set as the\nvehicle detection task from the range data of Velodyne 64E lidar. We proposes\nto present the data in a 2D point map and use a single 2D end-to-end fully\nconvolutional network to predict the objectness confidence and the bounding\nboxes simultaneously. By carefully design the bounding box encoding, it is able\nto predict full 3D bounding boxes even using a 2D convolutional network.\nExperiments on the KITTI dataset shows the state-of-the-art performance of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 05:57:36 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Li", "Bo", ""], ["Zhang", "Tianlei", ""], ["Xia", "Tian", ""]]}, {"id": "1608.07951", "submitter": "Seoung Wug Oh", "authors": "Seoung Wug Oh and Seon Joo Kim", "title": "Approaching the Computational Color Constancy as a Classification\n  Problem through Deep Learning", "comments": "This is a preprint of an article accepted for publication in Pattern\n  Recognition, ELSEVIER", "journal-ref": "Pattern Recognition, Volume 61, January 2017, Pages 405 to 416", "doi": "10.1016/j.patcog.2016.08.013", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational color constancy refers to the problem of computing the\nilluminant color so that the images of a scene under varying illumination can\nbe normalized to an image under the canonical illumination. In this paper, we\nadopt a deep learning framework for the illumination estimation problem. The\nproposed method works under the assumption of uniform illumination over the\nscene and aims for the accurate illuminant color computation. Specifically, we\ntrained the convolutional neural network to solve the problem by casting the\ncolor constancy problem as an illumination classification problem. We designed\nthe deep learning architecture so that the output of the network can be\ndirectly used for computing the color of the illumination. Experimental results\nshow that our deep network is able to extract useful features for the\nillumination estimation and our method outperforms all previous color constancy\nmethods on multiple test datasets.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 08:41:55 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Oh", "Seoung Wug", ""], ["Kim", "Seon Joo", ""]]}, {"id": "1608.07973", "submitter": "Aviv Eisenschtat", "authors": "Aviv Eisenschtat and Lior Wolf", "title": "Linking Image and Text with 2-Way Nets", "comments": "14 pages, 2 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linking two data sources is a basic building block in numerous computer\nvision problems. Canonical Correlation Analysis (CCA) achieves this by\nutilizing a linear optimizer in order to maximize the correlation between the\ntwo views. Recent work makes use of non-linear models, including deep learning\ntechniques, that optimize the CCA loss in some feature space. In this paper, we\nintroduce a novel, bi-directional neural network architecture for the task of\nmatching vectors from two data sources. Our approach employs two tied neural\nnetwork channels that project the two views into a common, maximally correlated\nspace using the Euclidean loss. We show a direct link between the\ncorrelation-based loss and Euclidean loss, enabling the use of Euclidean loss\nfor correlation maximization. To overcome common Euclidean regression\noptimization problems, we modify well-known techniques to our problem,\nincluding batch normalization and dropout. We show state of the art results on\na number of computer vision matching tasks including MNIST image matching and\nsentence-image matching on the Flickr8k, Flickr30k and COCO datasets.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 09:57:47 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 06:10:56 GMT"}, {"version": "v3", "created": "Fri, 10 Feb 2017 20:38:46 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Eisenschtat", "Aviv", ""], ["Wolf", "Lior", ""]]}, {"id": "1608.07997", "submitter": "William Liu", "authors": "William X. Liu and Tat-Jun Chin", "title": "Correspondence Insertion for As-Projective-As-Possible Image Stitching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatially varying warps are increasingly popular for image alignment. In\nparticular, as-projective-as-possible (APAP) warps have been proven effective\nfor accurate panoramic stitching, especially in cases with significant depth\nparallax that defeat standard homographic warps. However, estimating spatially\nvarying warps requires a sufficient number of feature matches. In image regions\nwhere feature detection or matching fail, the warp loses guidance and is unable\nto accurately model the true underlying warp, thus resulting in poor\nregistration. In this paper, we propose a correspondence insertion method for\nAPAP warps, with a focus on panoramic stitching. Our method automatically\nidentifies misaligned regions, and inserts appropriate point correspondences to\nincrease the flexibility of the warp and improve alignment. Unlike other warp\nvarieties, the underlying projective regularization of APAP warps reduces\noverfitting and geometric distortion, despite increases to the warp complexity.\nComparisons with recent techniques for parallax-tolerant image stitching\ndemonstrate the effectiveness and simplicity of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 11:09:18 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Liu", "William X.", ""], ["Chin", "Tat-Jun", ""]]}, {"id": "1608.08021", "submitter": "Sanghoon Hong", "authors": "Kye-Hyeon Kim, Sanghoon Hong, Byungseok Roh, Yeongjae Cheon, Minje\n  Park", "title": "PVANET: Deep but Lightweight Neural Networks for Real-time Object\n  Detection", "comments": "Full details about \"PVANet 9.0\" in the VOC2012 leaderboard\n  (https://goo.gl/DuQBku). The test codes are available at\n  https://github.com/sanghoon/pva-faster-rcnn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents how we can achieve the state-of-the-art accuracy in\nmulti-category object detection task while minimizing the computational cost by\nadapting and combining recent technical innovations. Following the common\npipeline of \"CNN feature extraction + region proposal + RoI classification\", we\nmainly redesign the feature extraction part, since region proposal part is not\ncomputationally expensive and classification part can be efficiently compressed\nwith common techniques like truncated SVD. Our design principle is \"less\nchannels with more layers\" and adoption of some building blocks including\nconcatenated ReLU, Inception, and HyperNet. The designed network is deep and\nthin and trained with the help of batch normalization, residual connections,\nand learning rate scheduling based on plateau detection. We obtained solid\nresults on well-known object detection benchmarks: 83.8% mAP (mean average\nprecision) on VOC2007 and 82.5% mAP on VOC2012 (2nd place), while taking only\n750ms/image on Intel i7-6700K CPU with a single core and 46ms/image on NVIDIA\nTitan X GPU. Theoretically, our network requires only 12.3% of the\ncomputational cost compared to ResNet-101, the winner on VOC2012.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 12:32:00 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 10:00:44 GMT"}, {"version": "v3", "created": "Fri, 30 Sep 2016 07:17:13 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Kim", "Kye-Hyeon", ""], ["Hong", "Sanghoon", ""], ["Roh", "Byungseok", ""], ["Cheon", "Yeongjae", ""], ["Park", "Minje", ""]]}, {"id": "1608.08029", "submitter": "Xiang Wang", "authors": "Xiang Wang, Huimin Ma, Xiaozhi Chen, Shaodi You", "title": "Edge Preserving and Multi-Scale Contextual Neural Network for Salient\n  Object Detection", "comments": null, "journal-ref": "IEEE TIP 2017", "doi": "10.1109/TIP.2017.2756825", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel edge preserving and multi-scale contextual\nneural network for salient object detection. The proposed framework is aiming\nto address two limits of the existing CNN based methods. First, region-based\nCNN methods lack sufficient context to accurately locate salient object since\nthey deal with each region independently. Second, pixel-based CNN methods\nsuffer from blurry boundaries due to the presence of convolutional and pooling\nlayers. Motivated by these, we first propose an end-to-end edge-preserved\nneural network based on Fast R-CNN framework (named RegionNet) to efficiently\ngenerate saliency map with sharp object boundaries. Later, to further improve\nit, multi-scale spatial context is attached to RegionNet to consider the\nrelationship between regions and the global scenes. Furthermore, our method can\nbe generally applied to RGB-D saliency detection by depth refinement. The\nproposed framework achieves both clear detection boundary and multi-scale\ncontextual robustness simultaneously for the first time, and thus achieves an\noptimized performance. Experiments on six RGB and two RGB-D benchmark datasets\ndemonstrate that the proposed method achieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 12:43:43 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 14:48:25 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Wang", "Xiang", ""], ["Ma", "Huimin", ""], ["Chen", "Xiaozhi", ""], ["You", "Shaodi", ""]]}, {"id": "1608.08049", "submitter": "Samaneh Abbasi Sureshjani", "authors": "Samaneh Abbasi-Sureshjani, Marta Favali, Giovanna Citti, Alessandro\n  Sarti, Bart M. ter Haar Romeny", "title": "Curvature Integration in a 5D Kernel for Extracting Vessel Connections\n  in Retinal Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree-like structures such as retinal images are widely studied in\ncomputer-aided diagnosis systems for large-scale screening programs. Despite\nseveral segmentation and tracking methods proposed in the literature, there\nstill exist several limitations specifically when two or more curvilinear\nstructures cross or bifurcate, or in the presence of interrupted lines or\nhighly curved blood vessels. In this paper, we propose a novel approach based\non multi-orientation scores augmented with a contextual affinity matrix, which\nboth are inspired by the geometry of the primary visual cortex (V1) and their\ncontextual connections. The connectivity is described with a five-dimensional\nkernel obtained as the fundamental solution of the Fokker-Planck equation\nmodelling the cortical connectivity in the lifted space of positions,\norientations, curvatures and intensity. It is further used in a self-tuning\nspectral clustering step to identify the main perceptual units in the stimuli.\nThe proposed method has been validated on several easy and challenging\nstructures in a set of artificial images and actual retinal patches. Supported\nby quantitative and qualitative results, the method is capable of overcoming\nthe limitations of current state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 13:57:58 GMT"}, {"version": "v2", "created": "Fri, 3 Mar 2017 15:44:03 GMT"}, {"version": "v3", "created": "Mon, 26 Jun 2017 17:18:58 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Abbasi-Sureshjani", "Samaneh", ""], ["Favali", "Marta", ""], ["Citti", "Giovanna", ""], ["Sarti", "Alessandro", ""], ["Romeny", "Bart M. ter Haar", ""]]}, {"id": "1608.08104", "submitter": "Fred Ngol\\`e", "authors": "F. M. Ngol\\`e Mboula, J.-L. Starck, K. Okumura, J. Amiaux, P. Hudelot", "title": "Constraint matrix factorization for space variant PSFs field restoration", "comments": "33 pages", "journal-ref": null, "doi": "10.1088/0266-5611/32/12/124001", "report-no": null, "categories": "cs.CV astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: in large-scale spatial surveys, the Point Spread Function (PSF)\nvaries across the instrument field of view (FOV). Local measurements of the\nPSFs are given by the isolated stars images. Yet, these estimates may not be\ndirectly usable for post-processings because of the observational noise and\npotentially the aliasing. Aims: given a set of aliased and noisy stars images\nfrom a telescope, we want to estimate well-resolved and noise-free PSFs at the\nobserved stars positions, in particular, exploiting the spatial correlation of\nthe PSFs across the FOV. Contributions: we introduce RCA (Resolved Components\nAnalysis) which is a noise-robust dimension reduction and super-resolution\nmethod based on matrix factorization. We propose an original way of using the\nPSFs spatial correlation in the restoration process through sparsity. The\nintroduced formalism can be applied to correlated data sets with respect to any\neuclidean parametric space. Results: we tested our method on simulated\nmonochromatic PSFs of Euclid telescope (launch planned for 2020). The proposed\nmethod outperforms existing PSFs restoration and dimension reduction methods.\nWe show that a coupled sparsity constraint on individual PSFs and their spatial\ndistribution yields a significant improvement on both the restored PSFs shapes\nand the PSFs subspace identification, in presence of aliasing. Perspectives:\nRCA can be naturally extended to account for the wavelength dependency of the\nPSFs.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 15:30:25 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 12:40:23 GMT"}, {"version": "v3", "created": "Wed, 31 Aug 2016 07:10:41 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Mboula", "F. M. Ngol\u00e8", ""], ["Starck", "J. -L.", ""], ["Okumura", "K.", ""], ["Amiaux", "J.", ""], ["Hudelot", "P.", ""]]}, {"id": "1608.08128", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Alberto Montes, Amaia Salvador, Santiago Pascual and Xavier\n  Giro-i-Nieto", "title": "Temporal Activity Detection in Untrimmed Videos with Recurrent Neural\n  Networks", "comments": "Best Poster Award at the 1st NIPS Workshop on Large Scale Computer\n  Vision Systems (Barcelona, December 2016). Source code available at\n  https://imatge-upc.github.io/activitynet-2016-cvprw/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis explore different approaches using Convolutional and Recurrent\nNeural Networks to classify and temporally localize activities on videos,\nfurthermore an implementation to achieve it has been proposed. As the first\nstep, features have been extracted from video frames using an state of the art\n3D Convolutional Neural Network. This features are fed in a recurrent neural\nnetwork that solves the activity classification and temporally location tasks\nin a simple and flexible way. Different architectures and configurations have\nbeen tested in order to achieve the best performance and learning of the video\ndataset provided. In addition it has been studied different kind of post\nprocessing over the trained network's output to achieve a better results on the\ntemporally localization of activities on the videos. The results provided by\nthe neural network developed in this thesis have been submitted to the\nActivityNet Challenge 2016 of the CVPR, achieving competitive results using a\nsimple and flexible architecture.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 16:14:52 GMT"}, {"version": "v2", "created": "Sun, 11 Dec 2016 16:25:11 GMT"}, {"version": "v3", "created": "Thu, 2 Mar 2017 23:07:00 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Montes", "Alberto", ""], ["Salvador", "Amaia", ""], ["Pascual", "Santiago", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1608.08139", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Cristian Reyes, Eva Mohedano, Kevin McGuinness, Noel E. O'Connor and\n  Xavier Giro-i-Nieto", "title": "Where is my Phone ? Personal Object Retrieval from Egocentric Images", "comments": "Lifelogging Tools and Applications Workshop (LTA'16) at ACM\n  Multimedia 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a retrieval pipeline and evaluation scheme for the problem\nof finding the last appearance of personal objects in a large dataset of images\ncaptured from a wearable camera. Each personal object is modelled by a small\nset of images that define a query for a visual search engine.The retrieved\nresults are reranked considering the temporal timestamps of the images to\nincrease the relevance of the later detections. Finally, a temporal\ninterleaving of the results is introduced for robustness against false\ndetections. The Mean Reciprocal Rank is proposed as a metric to evaluate this\nproblem. This application could help into developing personal assistants\ncapable of helping users when they do not remember where they left their\npersonal belongings.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 16:41:52 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 23:13:09 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Reyes", "Cristian", ""], ["Mohedano", "Eva", ""], ["McGuinness", "Kevin", ""], ["O'Connor", "Noel E.", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1608.08149", "submitter": "Nader Mahmoud", "authors": "Nader Mahmoud, I\\~nigo Cirauqui, Alexandre Hostettler, Christophe\n  Doignon, Luc Soler, Jacques Marescaux, and J.M.M. Montiel", "title": "ORBSLAM-based Endoscope Tracking and 3D Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to track the endoscope location inside the surgical scene and provide\n3D reconstruction, in real-time, from the sole input of the image sequence\ncaptured by the monocular endoscope. This information offers new possibilities\nfor developing surgical navigation and augmented reality applications. The main\nbenefit of this approach is the lack of extra tracking elements which can\ndisturb the surgeon performance in the clinical routine. It is our first\ncontribution to exploit ORBSLAM, one of the best performing monocular SLAM\nalgorithms, to estimate both of the endoscope location, and 3D structure of the\nsurgical scene. However, the reconstructed 3D map poorly describe textureless\nsoft organ surfaces such as liver. It is our second contribution to extend\nORBSLAM to be able to reconstruct a semi-dense map of soft organs. Experimental\nresults on in-vivo pigs, shows a robust endoscope tracking even with organs\ndeformations and partial instrument occlusions. It also shows the\nreconstruction density, and accuracy against ground truth surface obtained from\nCT.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 17:10:26 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Mahmoud", "Nader", ""], ["Cirauqui", "I\u00f1igo", ""], ["Hostettler", "Alexandre", ""], ["Doignon", "Christophe", ""], ["Soler", "Luc", ""], ["Marescaux", "Jacques", ""], ["Montiel", "J. M. M.", ""]]}, {"id": "1608.08171", "submitter": "Richard Wang", "authors": "Yao Sui, Guanghui Wang, Yafei Tang, Li Zhang", "title": "Tracking Completion", "comments": "Published at ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental component of modern trackers is an online learned tracking\nmodel, which is typically modeled either globally or locally. The two kinds of\nmodels perform differently in terms of effectiveness and robustness under\ndifferent challenging situations. This work exploits the advantages of both\nmodels. A subspace model, from a global perspective, is learned from previously\nobtained targets via rank-minimization to address the tracking, and a\npixel-level local observation is leveraged si- multaneously, from a local point\nof view, to augment the subspace model. A matrix completion method is employed\nto integrate the two models. Unlike previous tracking methods, which locate the\ntarget among all fully observed target candidates, the proposed approach first\nestimates an expected target via the matrix completion through partially\nobserved target candidates, and then, identifies the target according to the\nestimation accuracy with respect to the target candidates. Specifically, the\ntracking is formulated as a problem of target appearance estimation. Extensive\nexperiments on various challenging video sequences verify the effectiveness of\nthe proposed approach and demonstrate that the proposed tracker outperforms\nother popular state-of-the-art trackers.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 18:33:23 GMT"}, {"version": "v2", "created": "Fri, 9 Sep 2016 01:49:46 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Sui", "Yao", ""], ["Wang", "Guanghui", ""], ["Tang", "Yafei", ""], ["Zhang", "Li", ""]]}, {"id": "1608.08173", "submitter": "Richard Wang", "authors": "Yao Sui, Ziming Zhang, Guanghui Wang, Yafei Tang, Li Zhang", "title": "Real-Time Visual Tracking: Promoting the Robustness of Correlation\n  Filter Learning", "comments": "Published at ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation filtering based tracking model has received lots of attention and\nachieved great success in real-time tracking, however, the lost function in\ncurrent correlation filtering paradigm could not reliably response to the\nappearance changes caused by occlusion and illumination variations. This study\nintends to promote the robustness of the correlation filter learning. By\nexploiting the anisotropy of the filter response, three sparsity related loss\nfunctions are proposed to alleviate the overfitting issue of previous methods\nand improve the overall tracking performance. As a result, three real-time\ntrackers are implemented. Extensive experiments in various challenging\nsituations demonstrate that the robustness of the learned correlation filter\nhas been greatly improved via the designed loss functions. In addition, the\nstudy reveals, from an experimental perspective, how different loss functions\nessentially influence the tracking performance. An important conclusion is that\nthe sensitivity of the peak values of the filter in successive frames is\nconsistent with the tracking performance. This is a useful reference criterion\nin designing a robust correlation filter for visual tracking.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 18:36:36 GMT"}, {"version": "v2", "created": "Fri, 9 Sep 2016 01:50:14 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Sui", "Yao", ""], ["Zhang", "Ziming", ""], ["Wang", "Guanghui", ""], ["Tang", "Yafei", ""], ["Zhang", "Li", ""]]}, {"id": "1608.08188", "submitter": "Danna Gurari", "authors": "Danna Gurari and Kristen Grauman", "title": "Visual Question: Predicting If a Crowd Will Agree on the Answer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (VQA) systems are emerging from a desire to empower\nusers to ask any natural language question about visual content and receive a\nvalid answer in response. However, close examination of the VQA problem reveals\nan unavoidable, entangled problem that multiple humans may or may not always\nagree on a single answer to a visual question. We train a model to\nautomatically predict from a visual question whether a crowd would agree on a\nsingle answer. We then propose how to exploit this system in a novel\napplication to efficiently allocate human effort to collect answers to visual\nquestions. Specifically, we propose a crowdsourcing system that automatically\nsolicits fewer human responses when answer agreement is expected and more human\nresponses when answer disagreement is expected. Our system improves upon\nexisting crowdsourcing systems, typically eliminating at least 20% of human\neffort with no loss to the information collected from the crowd.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 19:24:25 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Gurari", "Danna", ""], ["Grauman", "Kristen", ""]]}, {"id": "1608.08242", "submitter": "Colin Lea", "authors": "Colin Lea, Rene Vidal, Austin Reiter, Gregory D. Hager", "title": "Temporal Convolutional Networks: A Unified Approach to Action\n  Segmentation", "comments": "Submitted to the ECCV workshop on \"Brave new ideas for motion\n  representations in videos\" (http://bravenewmotion.github.io/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dominant paradigm for video-based action segmentation is composed of two\nsteps: first, for each frame, compute low-level features using Dense\nTrajectories or a Convolutional Neural Network that encode spatiotemporal\ninformation locally, and second, input these features into a classifier that\ncaptures high-level temporal relationships, such as a Recurrent Neural Network\n(RNN). While often effective, this decoupling requires specifying two separate\nmodels, each with their own complexities, and prevents capturing more nuanced\nlong-range spatiotemporal relationships. We propose a unified approach, as\ndemonstrated by our Temporal Convolutional Network (TCN), that hierarchically\ncaptures relationships at low-, intermediate-, and high-level time-scales. Our\nmodel achieves superior or competitive performance using video or sensor data\non three public action segmentation datasets and can be trained in a fraction\nof the time it takes to train an RNN.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 20:48:15 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Lea", "Colin", ""], ["Vidal", "Rene", ""], ["Reiter", "Austin", ""], ["Hager", "Gregory D.", ""]]}, {"id": "1608.08251", "submitter": "Igor Polk", "authors": "Igor Polkovnikov", "title": "Construction of Convex Sets on Quadrilateral Ordered Tiles or Graphs\n  with Propagation Neighborhood Operations. Dales, Concavity Structures.\n  Application to Gray Image Analysis of Human-Readable Shapes", "comments": "58 pages, more than 50 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An effort has been made to show mathematicians some new ideas applied to\nimage analysis. Gray images are presented as tilings. Based on topological\nproperties of the tiling, a number of gray convex hulls: maximal, minimal, and\noriented ones are constructed and some are proved. They are constructed with\nonly one operation. Two tilings are used in the Constraint and Allowance types\nof operations. New type of concavity described: a dale. All operations are\nparallel, possible to realize clock-less. Convexities define what is the\nbackground. They are treated as separate gray objects. There are multiple\nrelations among them and their descendants. Via that, topological size of\nconcavities is proposed. Constructed with the same type of operations, Rays and\nAngles in a tiling define possible spatial relations. Notions like \"strokes\"\nare defined through concavities. Unusual effects on levelized gray objects are\nshown. It is illustrated how alphabet and complex hieroglyphs can be described\nthrough concavities and their relations. A hypothesis of living organisms image\nanalysis is proposed. A number of examples with symbols and a human face are\ncalculated with new Asynchwave C++ software library.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 21:12:49 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Polkovnikov", "Igor", ""]]}, {"id": "1608.08305", "submitter": "Ronghang Hu", "authors": "Ronghang Hu, Marcus Rohrbach, Subhashini Venugopalan, Trevor Darrell", "title": "Utilizing Large Scale Vision and Text Datasets for Image Segmentation\n  from Referring Expressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation from referring expressions is a joint vision and language\nmodeling task, where the input is an image and a textual expression describing\na particular region in the image; and the goal is to localize and segment the\nspecific image region based on the given expression. One major difficulty to\ntrain such language-based image segmentation systems is the lack of datasets\nwith joint vision and text annotations. Although existing vision datasets such\nas MS COCO provide image captions, there are few datasets with region-level\ntextual annotations for images, and these are often smaller in scale. In this\npaper, we explore how existing large scale vision-only and text-only datasets\ncan be utilized to train models for image segmentation from referring\nexpressions. We propose a method to address this problem, and show in\nexperiments that our method can help this joint vision and language modeling\ntask with vision-only and text-only data and outperforms previous results.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 02:27:41 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Hu", "Ronghang", ""], ["Rohrbach", "Marcus", ""], ["Venugopalan", "Subhashini", ""], ["Darrell", "Trevor", ""]]}, {"id": "1608.08334", "submitter": "Shervin Ardeshir", "authors": "Shervin Ardeshir and Ali Borji", "title": "Egocentric Meets Top-view", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the availability and increasing popularity of Egocentric cameras\nsuch as GoPro cameras, glasses, and etc. we have been provided with a plethora\nof videos captured from the first person perspective. Surveillance cameras and\nUnmanned Aerial Vehicles(also known as drones) also offer tremendous amount of\nvideos, mostly with top-down or oblique view-point. Egocentric vision and\ntop-view surveillance videos have been studied extensively in the past in the\ncomputer vision community. However, the relationship between the two has yet to\nbe explored thoroughly. In this effort, we attempt to explore this relationship\nby approaching two questions. First, having a set of egocentric videos and a\ntop-view video, can we verify if the top-view video contains all, or some of\nthe egocentric viewers present in the egocentric set? And second, can we\nidentify the egocentric viewers in the content of the top-view video? In other\nwords, can we find the cameramen in the surveillance videos? These problems can\nbecome more challenging when the videos are not time-synchronous. Thus we\nformalize the problem in a way which handles and also estimates the unknown\nrelative time-delays between the egocentric videos and the top-view video. We\nformulate the problem as a spectral graph matching instance, and jointly seek\nthe optimal assignments and relative time-delays of the videos. As a result, we\nspatiotemporally localize the egocentric observers in the top-view video. We\nmodel each view (egocentric or top) using a graph, and compute the assignment\nand time-delays in an iterative-alternative fashion.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 05:42:07 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 18:51:14 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Ardeshir", "Shervin", ""], ["Borji", "Ali", ""]]}, {"id": "1608.08336", "submitter": "Ming Yin", "authors": "Ming Yin, Junbin Gao, Shengli Xie and Yi Guo", "title": "Low-rank Multi-view Clustering in Third-Order Tensor Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The plenty information from multiple views data as well as the complementary\ninformation among different views are usually beneficial to various tasks,\ne.g., clustering, classification, de-noising. Multi-view subspace clustering is\nbased on the fact that the multi-view data are generated from a latent\nsubspace. To recover the underlying subspace structure, the success of the\nsparse and/or low-rank subspace clustering has been witnessed recently. Despite\nsome state-of-the-art subspace clustering approaches can numerically handle\nmulti-view data, by simultaneously exploring all possible pairwise correlation\nwithin views, the high order statistics is often disregarded which can only be\ncaptured by simultaneously utilizing all views. As a consequence, the\nclustering performance for multi-view data is compromised. To address this\nissue, in this paper, a novel multi-view clustering method is proposed by using\n\\textit{t-product} in third-order tensor space. Based on the circular\nconvolution operation, multi-view data can be effectively represented by a\n\\textit{t-linear} combination with sparse and low-rank penalty using\n\"self-expressiveness\". Our extensive experimental results on facial, object,\ndigits image and text data demonstrate that the proposed method outperforms the\nstate-of-the-art methods in terms of many criteria.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 05:57:31 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2016 23:37:37 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Yin", "Ming", ""], ["Gao", "Junbin", ""], ["Xie", "Shengli", ""], ["Guo", "Yi", ""]]}, {"id": "1608.08339", "submitter": "Taehwan Kim", "authors": "Taehwan Kim", "title": "American Sign Language fingerspelling recognition from video: Methods\n  for unrestricted recognition and signer-independence", "comments": "PhD Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis, we study the problem of recognizing video sequences of\nfingerspelled letters in American Sign Language (ASL). Fingerspelling comprises\na significant but relatively understudied part of ASL, and recognizing it is\nchallenging for a number of reasons: It involves quick, small motions that are\noften highly coarticulated; it exhibits significant variation between signers;\nand there has been a dearth of continuous fingerspelling data collected. In\nthis work, we propose several types of recognition approaches, and explore the\nsigner variation problem. Our best-performing models are segmental\n(semi-Markov) conditional random fields using deep neural network-based\nfeatures. In the signer-dependent setting, our recognizers achieve up to about\n8% letter error rates. The signer-independent setting is much more challenging,\nbut with neural network adaptation we achieve up to 17% letter error rates.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 06:12:22 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Kim", "Taehwan", ""]]}, {"id": "1608.08395", "submitter": "Hirokatsu Kataoka", "authors": "Hirokatsu Kataoka, Yun He, Soma Shirakabe, Yutaka Satoh", "title": "Motion Representation with Acceleration Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information of time differentiation is extremely important cue for a motion\nrepresentation. We have applied first-order differential velocity from a\npositional information, moreover we believe that second-order differential\nacceleration is also a significant feature in a motion representation. However,\nan acceleration image based on a typical optical flow includes motion noises.\nWe have not employed the acceleration image because the noises are too strong\nto catch an effective motion feature in an image sequence. On one hand, the\nrecent convolutional neural networks (CNN) are robust against input noises. In\nthis paper, we employ acceleration-stream in addition to the spatial- and\ntemporal-stream based on the two-stream CNN. We clearly show the effectiveness\nof adding the acceleration stream to the two-stream CNN.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 10:23:07 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Kataoka", "Hirokatsu", ""], ["He", "Yun", ""], ["Shirakabe", "Soma", ""], ["Satoh", "Yutaka", ""]]}, {"id": "1608.08434", "submitter": "Byungjae Lee", "authors": "Byungjae Lee, Enkhbayar Erdenee, Songguo Jin, and Phill Kyu Rhee", "title": "Multi-Class Multi-Object Tracking using Changing Point Detection", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-48881-3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a robust multi-class multi-object tracking (MCMOT)\nformulated by a Bayesian filtering framework. Multi-object tracking for\nunlimited object classes is conducted by combining detection responses and\nchanging point detection (CPD) algorithm. The CPD model is used to observe\nabrupt or abnormal changes due to a drift and an occlusion based spatiotemporal\ncharacteristics of track states. The ensemble of convolutional neural network\n(CNN) based object detector and Lucas-Kanede Tracker (KLT) based motion\ndetector is employed to compute the likelihoods of foreground regions as the\ndetection responses of different object classes. Extensive experiments are\nperformed using lately introduced challenging benchmark videos; ImageNet VID\nand MOT benchmark dataset. The comparison to state-of-the-art video tracking\ntechniques shows very encouraging results.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 13:07:05 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Lee", "Byungjae", ""], ["Erdenee", "Enkhbayar", ""], ["Jin", "Songguo", ""], ["Rhee", "Phill Kyu", ""]]}, {"id": "1608.08471", "submitter": "Johannes Stegmaier", "authors": "Johannes Stegmaier", "title": "New Methods to Improve Large-Scale Microscopy Image Analysis with Prior\n  Knowledge and Uncertainty", "comments": "218 pages, 58 figures, PhD thesis, Department of Mechanical\n  Engineering, Karlsruhe Institute of Technology, published online with KITopen\n  (License: CC BY-SA 3.0, http://dx.doi.org/10.5445/IR/1000057821)", "journal-ref": null, "doi": "10.5445/IR/1000057821", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Multidimensional imaging techniques provide powerful ways to examine various\nkinds of scientific questions. The routinely produced datasets in the\nterabyte-range, however, can hardly be analyzed manually and require an\nextensive use of automated image analysis. The present thesis introduces a new\nconcept for the estimation and propagation of uncertainty involved in image\nanalysis operators and new segmentation algorithms that are suitable for\nterabyte-scale analyses of 3D+t microscopy images.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 14:21:55 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Stegmaier", "Johannes", ""]]}, {"id": "1608.08526", "submitter": "Umar Iqbal", "authors": "Umar Iqbal and Juergen Gall", "title": "Multi-Person Pose Estimation with Local Joint-to-Person Associations", "comments": "Accepted to European Conference on Computer Vision (ECCV) Workshops,\n  Crowd Understanding, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite of the recent success of neural networks for human pose estimation,\ncurrent approaches are limited to pose estimation of a single person and cannot\nhandle humans in groups or crowds. In this work, we propose a method that\nestimates the poses of multiple persons in an image in which a person can be\noccluded by another person or might be truncated. To this end, we consider\nmulti-person pose estimation as a joint-to-person association problem. We\nconstruct a fully connected graph from a set of detected joint candidates in an\nimage and resolve the joint-to-person association and outlier detection using\ninteger linear programming. Since solving joint-to-person association jointly\nfor all persons in an image is an NP-hard problem and even approximations are\nexpensive, we solve the problem locally for each person. On the challenging\nMPII Human Pose Dataset for multiple persons, our approach achieves the\naccuracy of a state-of-the-art method, but it is 6,000 to 19,000 times faster.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 16:00:42 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2016 09:26:57 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Iqbal", "Umar", ""], ["Gall", "Juergen", ""]]}, {"id": "1608.08596", "submitter": "Matti Raitoharju", "authors": "Matti Raitoharju and Samu Kallio and Matti Pellikka", "title": "A statistical model of tristimulus measurements within and between OLED\n  displays", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an empirical model for noises in color measurements from OLED\ndisplays. According to measured data the noise is not isotropic in the XYZ\nspace, instead most of the noise is along an axis that is parallel to a vector\nfrom origin to measured XYZ vector. The presented empirical model is simple and\ndepends only on the measured XYZ values. Our tests show that the variations\nbetween multiple panels of the same type have similar distribution as the\ntemporal noise in measurements from a single panel, but a larger magnitude.\n", "versions": [{"version": "v1", "created": "Sun, 28 Aug 2016 09:11:22 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2017 10:49:10 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Raitoharju", "Matti", ""], ["Kallio", "Samu", ""], ["Pellikka", "Matti", ""]]}, {"id": "1608.08614", "submitter": "Minyoung Huh", "authors": "Minyoung Huh, Pulkit Agrawal, Alexei A. Efros", "title": "What makes ImageNet good for transfer learning?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The tremendous success of ImageNet-trained deep features on a wide range of\ntransfer tasks begs the question: what are the properties of the ImageNet\ndataset that are critical for learning good, general-purpose features? This\nwork provides an empirical investigation of various facets of this question: Is\nmore pre-training data always better? How does feature quality depend on the\nnumber of training examples per class? Does adding more object classes improve\nperformance? For the same data budget, how should the data be split into\nclasses? Is fine-grained recognition necessary for learning good features?\nGiven the same number of training classes, is it better to have coarse classes\nor fine-grained classes? Which is better: more classes or more examples per\nclass? To answer these and related questions, we pre-trained CNN features on\nvarious subsets of the ImageNet dataset and evaluated transfer performance on\nPASCAL detection, PASCAL action classification, and SUN scene classification\ntasks. Our overall findings suggest that most changes in the choice of\npre-training data long thought to be critical do not significantly affect\ntransfer performance.? Given the same number of training classes, is it better\nto have coarse classes or fine-grained classes? Which is better: more classes\nor more examples per class?\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 19:45:09 GMT"}, {"version": "v2", "created": "Sat, 10 Dec 2016 13:37:06 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Huh", "Minyoung", ""], ["Agrawal", "Pulkit", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1608.08710", "submitter": "Hao Li", "authors": "Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, Hans Peter Graf", "title": "Pruning Filters for Efficient ConvNets", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of CNNs in various applications is accompanied by a significant\nincrease in the computation and parameter storage costs. Recent efforts toward\nreducing these overheads involve pruning and compressing the weights of various\nlayers without hurting original accuracy. However, magnitude-based pruning of\nweights reduces a significant number of parameters from the fully connected\nlayers and may not adequately reduce the computation costs in the convolutional\nlayers due to irregular sparsity in the pruned networks. We present an\nacceleration method for CNNs, where we prune filters from CNNs that are\nidentified as having a small effect on the output accuracy. By removing whole\nfilters in the network together with their connecting feature maps, the\ncomputation costs are reduced significantly. In contrast to pruning weights,\nthis approach does not result in sparse connectivity patterns. Hence, it does\nnot need the support of sparse convolution libraries and can work with existing\nefficient BLAS libraries for dense matrix multiplications. We show that even\nsimple filter pruning techniques can reduce inference costs for VGG-16 by up to\n34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the\noriginal accuracy by retraining the networks.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 02:29:59 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2016 02:12:36 GMT"}, {"version": "v3", "created": "Fri, 10 Mar 2017 17:57:56 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Li", "Hao", ""], ["Kadav", "Asim", ""], ["Durdanovic", "Igor", ""], ["Samet", "Hanan", ""], ["Graf", "Hans Peter", ""]]}, {"id": "1608.08711", "submitter": "Ghassem Tofighi", "authors": "Maria Frank, Ghassem Tofighi, Haisong Gu, Renate Fruchter", "title": "Engagement Detection in Meetings", "comments": "The paper has been published on ICCCBE 2016.\n  http://www.see.eng.osaka-u.ac.jp/seeit/icccbe2016/\n  http://www.see.eng.osaka-u.ac.jp/seeit/icccbe2016/download/Tentative_Time_Table_ICCCBE2016_2016-05-10.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Group meetings are frequent business events aimed to develop and conduct\nproject work, such as Big Room design and construction project meetings. To be\neffective in these meetings, participants need to have an engaged mental state.\nThe mental state of participants however, is hidden from other participants,\nand thereby difficult to evaluate. Mental state is understood as an inner\nprocess of thinking and feeling, that is formed of a conglomerate of mental\nrepresentations and propositional attitudes. There is a need to create\ntransparency of these hidden states to understand, evaluate and influence them.\nFacilitators need to evaluate the meeting situation and adjust for higher\nengagement and productivity. This paper presents a framework that defines a\nspectrum of engagement states and an array of classifiers aimed to detect the\nengagement state of participants in real time. The Engagement Framework\nintegrates multi-modal information from 2D and 3D imaging and sound. Engagement\nis detected and evaluated at participants and aggregated at group level. We use\nempirical data collected at the lab of Konica Minolta, Inc. to test initial\napplications of this framework. The paper presents examples of the tested\nengagement classifiers, which are based on research in psychology,\ncommunication, and human computer interaction. Their accuracy is illustrated in\ndyadic interaction for engagement detection. In closing we discuss the\npotential extension to complex group collaboration settings and future feedback\nimplementations.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 02:46:37 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Frank", "Maria", ""], ["Tofighi", "Ghassem", ""], ["Gu", "Haisong", ""], ["Fruchter", "Renate", ""]]}, {"id": "1608.08716", "submitter": "Aishwarya Agrawal", "authors": "C. Lawrence Zitnick, Aishwarya Agrawal, Stanislaw Antol, Margaret\n  Mitchell, Dhruv Batra, Devi Parikh", "title": "Measuring Machine Intelligence Through Visual Question Answering", "comments": "AI Magazine, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machines have become more intelligent, there has been a renewed interest\nin methods for measuring their intelligence. A common approach is to propose\ntasks for which a human excels, but one which machines find difficult. However,\nan ideal task should also be easy to evaluate and not be easily gameable. We\nbegin with a case study exploring the recently popular task of image captioning\nand its limitations as a task for measuring machine intelligence. An\nalternative and more promising task is Visual Question Answering that tests a\nmachine's ability to reason about language and vision. We describe a dataset\nunprecedented in size created for the task that contains over 760,000 human\ngenerated questions about images. Using around 10 million human generated\nanswers, machines may be easily evaluated.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 02:56:00 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Zitnick", "C. Lawrence", ""], ["Agrawal", "Aishwarya", ""], ["Antol", "Stanislaw", ""], ["Mitchell", "Margaret", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1608.08792", "submitter": "Miguel \\'Angel Bautista Martin", "authors": "Miguel A. Bautista, Artsiom Sanakoyeu, Ekaterina Sutter, Bj\\\"orn Ommer", "title": "CliqueCNN: Deep Unsupervised Exemplar Learning", "comments": "Accepted for publication at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exemplar learning is a powerful paradigm for discovering visual similarities\nin an unsupervised manner. In this context, however, the recent breakthrough in\ndeep learning could not yet unfold its full potential. With only a single\npositive sample, a great imbalance between one positive and many negatives, and\nunreliable relationships between most samples, training of Convolutional Neural\nnetworks is impaired. Given weak estimates of local distance we propose a\nsingle optimization problem to extract batches of samples with mutually\nconsistent relations. Conflicting relations are distributed over different\nbatches and similar samples are grouped into compact cliques. Learning exemplar\nsimilarities is framed as a sequence of clique categorization tasks. The CNN\nthen consolidates transitivity relations within and between cliques and learns\na single representation for all samples without the need for labels. The\nproposed unsupervised approach has shown competitive performance on detailed\nposture analysis and object classification.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 09:49:56 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Bautista", "Miguel A.", ""], ["Sanakoyeu", "Artsiom", ""], ["Sutter", "Ekaterina", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "1608.08831", "submitter": "Guillaume Noyel", "authors": "Guillaume Noyel (IPRI), Michel Jourlin (IPRI, LHC)", "title": "Spatio-Colour Aspl\\\"und 's Metric and Logarithmic Image Processing for\n  Colour Images (LIPC)", "comments": null, "journal-ref": "C\\'esar Beltr\\'an-Casta\\~n\\'on, Ingela Nystr\\\"om, Fazel Famili\n  CIARP2016 - XXI IberoAmerican Congress on Pattern Recognition, Nov 2016,\n  Lima, Peru. Springer, 10125 2017, pp.36-43, 2016, Progress in Pattern\n  Recognition, Image Analysis, Computer Vision, and Applications: 21st\n  Iberoamerican Congress, CIARP 2016, Lima, Peru, November 8--11, 2016,\n  Proceedings", "doi": "10.1007/978-3-319-52277-7_5", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspl\\\"und 's metric, which is useful for pattern matching, consists in a\ndouble-sided probing, i.e. the over-graph and the sub-graph of a function are\nprobed jointly. This paper extends the Aspl\\\"und 's metric we previously\ndefined for colour and multivariate images using a marginal approach (i.e.\ncomponent by component) to the first spatio-colour Aspl\\\"und 's metric based on\nthe vectorial colour LIP model (LIPC). LIPC is a non-linear model with\noperations between colour images which are consistent with the human visual\nsystem. The defined colour metric is insensitive to lighting variations and a\nvariant which is robust to noise is used for colour pattern matching.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 12:49:12 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 16:08:29 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Noyel", "Guillaume", "", "IPRI"], ["Jourlin", "Michel", "", "IPRI, LHC"]]}, {"id": "1608.08851", "submitter": "Ali Diba", "authors": "Ali Diba, Ali Mohammad Pazandeh, Luc Van Gool", "title": "Efficient Two-Stream Motion and Appearance 3D CNNs for Video\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The video and action classification have extremely evolved by deep neural\nnetworks specially with two stream CNN using RGB and optical flow as inputs and\nthey present outstanding performance in terms of video analysis. One of the\nshortcoming of these methods is handling motion information extraction which is\ndone out side of the CNNs and relatively time consuming also on GPUs. So\nproposing end-to-end methods which are exploring to learn motion\nrepresentation, like 3D-CNN can achieve faster and accurate performance. We\npresent some novel deep CNNs using 3D architecture to model actions and motion\nrepresentation in an efficient way to be accurate and also as fast as\nreal-time. Our new networks learn distinctive models to combine deep motion\nfeatures into appearance model via learning optical flow features inside the\nnetwork.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 13:52:54 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 10:39:24 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Diba", "Ali", ""], ["Pazandeh", "Ali Mohammad", ""], ["Van Gool", "Luc", ""]]}, {"id": "1608.08878", "submitter": "Rachid Ahdid", "authors": "Rachid Ahdid, El Mahdi Barrah, Said Safi and Bouzid Manaut", "title": "Facial Surface Analysis using Iso-Geodesic Curves in Three Dimensional\n  Face Recognition System", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an automatic 3D face recognition system. This\nsystem is based on the representation of human faces surfaces as collections of\nIso-Geodesic Curves (IGC) using 3D Fast Marching algorithm. To compare two\nfacial surfaces, we compute a geodesic distance between a pair of facial curves\nusing a Riemannian geometry. In the classifying step, we use: Neural Networks\n(NN), K-Nearest Neighbor (KNN) and Support Vector Machines (SVM). To test this\nmethod and evaluate its performance, a simulation series of experiments were\nperformed on 3D Shape REtrieval Contest 2008 database (SHREC2008).\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 14:24:15 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Ahdid", "Rachid", ""], ["Barrah", "El Mahdi", ""], ["Safi", "Said", ""], ["Manaut", "Bouzid", ""]]}, {"id": "1608.08967", "submitter": "Seyed-Mohsen Moosavi-Dezfooli", "authors": "Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard", "title": "Robustness of classifiers: from adversarial to random noise", "comments": "Accepted to NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent works have shown that state-of-the-art classifiers are\nvulnerable to worst-case (i.e., adversarial) perturbations of the datapoints.\nOn the other hand, it has been empirically observed that these same classifiers\nare relatively robust to random noise. In this paper, we propose to study a\n\\textit{semi-random} noise regime that generalizes both the random and\nworst-case noise regimes. We propose the first quantitative analysis of the\nrobustness of nonlinear classifiers in this general noise regime. We establish\nprecise theoretical bounds on the robustness of classifiers in this general\nregime, which depend on the curvature of the classifier's decision boundary.\nOur bounds confirm and quantify the empirical observations that classifiers\nsatisfying curvature constraints are robust to random noise. Moreover, we\nquantify the robustness of classifiers in terms of the subspace dimension in\nthe semi-random noise regime, and show that our bounds remarkably interpolate\nbetween the worst-case and random noise regimes. We perform experiments and\nshow that the derived bounds provide very accurate estimates when applied to\nvarious state-of-the-art deep neural networks and datasets. This result\nsuggests bounds on the curvature of the classifiers' decision boundaries that\nwe support experimentally, and more generally offers important insights onto\nthe geometry of high dimensional classification problems.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 17:54:34 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Fawzi", "Alhussein", ""], ["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Frossard", "Pascal", ""]]}, {"id": "1608.08974", "submitter": "Yash Goyal", "authors": "Yash Goyal, Akrit Mohapatra, Devi Parikh, Dhruv Batra", "title": "Towards Transparent AI Systems: Interpreting Visual Question Answering\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have shown striking progress and obtained\nstate-of-the-art results in many AI research fields in the recent years.\nHowever, it is often unsatisfying to not know why they predict what they do. In\nthis paper, we address the problem of interpreting Visual Question Answering\n(VQA) models. Specifically, we are interested in finding what part of the input\n(pixels in images or words in questions) the VQA model focuses on while\nanswering the question. To tackle this problem, we use two visualization\ntechniques -- guided backpropagation and occlusion -- to find important words\nin the question and important regions in the image. We then present qualitative\nand quantitative analyses of these importance maps. We found that even without\nexplicit attention mechanisms, VQA models may sometimes be implicitly attending\nto relevant regions in the image, and often to appropriate words in the\nquestion.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 18:11:29 GMT"}, {"version": "v2", "created": "Fri, 9 Sep 2016 19:51:06 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Goyal", "Yash", ""], ["Mohapatra", "Akrit", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1608.09005", "submitter": "Paritosh Parmar", "authors": "Paritosh Parmar and Brendan Tran Morris", "title": "Measuring the Quality of Exercises", "comments": "EMBC'16 (The 38th Annual International Conference of the IEEE\n  Engineering in Medicine and Biology Society)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work explores the problem of exercise quality measurement since it is\nessential for effective management of diseases like cerebral palsy (CP). This\nwork examines the assessment of quality of large amplitude movement (LAM)\nexercises designed to treat CP in an automated fashion. Exercise data was\ncollected by trained participants to generate ideal examples to use as a\npositive samples for machine learning. Following that, subjects were asked to\ndeliberately make subtle errors during the exercise, such as restricting\nmovements, as is commonly seen in cases of patients suffering from CP. The\nquality measurement problem was then posed as a classification to determine\nwhether an example exercise was either \"good\" or \"bad\". Popular machine\nlearning techniques for classification, including support vector machines\n(SVM), single and doublelayered neural networks (NN), boosted decision trees,\nand dynamic time warping (DTW), were compared. The AdaBoosted tree performed\nbest with an accuracy of 94.68% demonstrating the feasibility of assessing\nexercise quality.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 19:28:49 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Parmar", "Paritosh", ""], ["Morris", "Brendan Tran", ""]]}]