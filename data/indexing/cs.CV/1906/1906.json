[{"id": "1906.00001", "submitter": "Cassidy Laidlaw", "authors": "Cassidy Laidlaw and Soheil Feizi", "title": "Functional Adversarial Attacks", "comments": "Accepted to NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose functional adversarial attacks, a novel class of threat models for\ncrafting adversarial examples to fool machine learning models. Unlike a\nstandard $\\ell_p$-ball threat model, a functional adversarial threat model\nallows only a single function to be used to perturb input features to produce\nan adversarial example. For example, a functional adversarial attack applied on\ncolors of an image can change all red pixels simultaneously to light red. Such\nglobal uniform changes in images can be less perceptible than perturbing pixels\nof the image individually. For simplicity, we refer to functional adversarial\nattacks on image colors as ReColorAdv, which is the main focus of our\nexperiments. We show that functional threat models can be combined with\nexisting additive ($\\ell_p$) threat models to generate stronger threat models\nthat allow both small, individual perturbations and large, uniform changes to\nan input. Moreover, we prove that such combinations encompass perturbations\nthat would not be allowed in either constituent threat model. In practice,\nReColorAdv can significantly reduce the accuracy of a ResNet-32 trained on\nCIFAR-10. Furthermore, to the best of our knowledge, combining ReColorAdv with\nother attacks leads to the strongest existing attack even after adversarial\ntraining. An implementation of ReColorAdv is available at\nhttps://github.com/cassidylaidlaw/ReColorAdv .\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 12:26:12 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 14:52:57 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Laidlaw", "Cassidy", ""], ["Feizi", "Soheil", ""]]}, {"id": "1906.00038", "submitter": "Stephanie Lukin", "authors": "Stephanie M. Lukin, Claire Bonial, and Clare R. Voss", "title": "Visual Understanding and Narration: A Deeper Understanding and\n  Explanation of Visual Scenes", "comments": "2-page extended abstract, presented at the Workshop on Shortcomings\n  in Vision and Language (SiVL), 2019, at the North American Association for\n  Computational Linguistics (NAACL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the task of Visual Understanding and Narration, in which a robot\n(or agent) generates text for the images that it collects when navigating its\nenvironment, by answering open-ended questions, such as 'what happens, or might\nhave happened, here?'\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 19:12:55 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 20:27:47 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Lukin", "Stephanie M.", ""], ["Bonial", "Claire", ""], ["Voss", "Clare R.", ""]]}, {"id": "1906.00050", "submitter": "Kunal Swami", "authors": "Kunal Swami, Kaushik Raghavan, Nikhilanj Pelluri, Rituparna Sarkar,\n  Pankaj Bajpai", "title": "DISCO: Depth Inference from Stereo using Context", "comments": "This work was completed in October 2018 and is accepted in IEEE\n  International Conference on Multimedia & Expo (ICME) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep learning based approaches have outperformed classical stereo\nmatching methods. However, current deep learning based end-to-end stereo\nmatching methods adopt a generic encoder-decoder style network with skip\nconnections. To limit computational requirement, many networks perform\nexcessive down sampling, which results in significant loss of useful low-level\ninformation. Additionally, many network designs do not exploit the rich\nmulti-scale contextual information. In this work, we address these\naforementioned problems by carefully designing the network architecture to\npreserve required spatial information throughout the network, while at the same\ntime achieve large effective receptive field to extract multiscale contextual\ninformation. For the first time, we create a synthetic disparity dataset\nreflecting real life images captured using a smartphone; this enables us to\nobtain state-of-the-art results on common real life images. The proposed model\nDISCO is pre-trained on the synthetic Scene Flow dataset and evaluated on\npopular benchmarks and our in-house dataset of challenging real life images.\nThe proposed model outperforms existing state-of-the-art methods in terms of\nquality as well as quantitative metrics.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 20:01:43 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Swami", "Kunal", ""], ["Raghavan", "Kaushik", ""], ["Pelluri", "Nikhilanj", ""], ["Sarkar", "Rituparna", ""], ["Bajpai", "Pankaj", ""]]}, {"id": "1906.00067", "submitter": "Kenneth Marino", "authors": "Kenneth Marino, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi", "title": "OK-VQA: A Visual Question Answering Benchmark Requiring External\n  Knowledge", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) in its ideal form lets us study reasoning in\nthe joint space of vision and language and serves as a proxy for the AI task of\nscene understanding. However, most VQA benchmarks to date are focused on\nquestions such as simple counting, visual attributes, and object detection that\ndo not require reasoning or knowledge beyond what is in the image. In this\npaper, we address the task of knowledge-based visual question answering and\nprovide a benchmark, called OK-VQA, where the image content is not sufficient\nto answer the questions, encouraging methods that rely on external knowledge\nresources. Our new dataset includes more than 14,000 questions that require\nexternal knowledge to answer. We show that the performance of the\nstate-of-the-art VQA models degrades drastically in this new setting. Our\nanalysis shows that our knowledge-based VQA task is diverse, difficult, and\nlarge compared to previous knowledge-based VQA datasets. We hope that this\ndataset enables researchers to open up new avenues for research in this domain.\nSee http://okvqa.allenai.org to download and browse the dataset.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 20:29:01 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 10:43:20 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Marino", "Kenneth", ""], ["Rastegari", "Mohammad", ""], ["Farhadi", "Ali", ""], ["Mottaghi", "Roozbeh", ""]]}, {"id": "1906.00093", "submitter": "Luis Riera", "authors": "Luis Riera, Koray Ozcan, Jennifer Merickel, Mathew Rizzo, Soumik\n  Sarkar, and Anuj Sharma", "title": "Driver Behavior Analysis Using Lane Departure Detection Under\n  Challenging Conditions", "comments": "6 pages, 4 figures, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel model to detect lane regions and extract\nlane departure events (changes and incursions) from challenging,\nlower-resolution videos recorded with mobile cameras. Our algorithm used a\nMask-RCNN based lane detection model as pre-processor. Recently, deep\nlearning-based models provide state-of-the-art technology for object detection\ncombined with segmentation. Among the several deep learning architectures,\nconvolutional neural networks (CNNs) outperformed other machine learning\nmodels, especially for region proposal and object detection tasks. Recent\ndevelopment in object detection has been driven by the success of region\nproposal methods and region-based CNNs (R-CNNs). Our algorithm utilizes lane\nsegmentation mask for detection and Fix-lag Kalman filter for tracking, rather\nthan the usual approach of detecting lane lines from single video frames. The\nalgorithm permits detection of driver lane departures into left or right lanes\nfrom continuous lane detections. Preliminary results show promise for robust\ndetection of lane departure events. The overall sensitivity for lane departure\nevents on our custom test dataset is 81.81%.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 21:55:41 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Riera", "Luis", ""], ["Ozcan", "Koray", ""], ["Merickel", "Jennifer", ""], ["Rizzo", "Mathew", ""], ["Sarkar", "Soumik", ""], ["Sharma", "Anuj", ""]]}, {"id": "1906.00107", "submitter": "Mehul Bhatt", "authors": "Jakob Suchan, Mehul Bhatt, and Srikrishna Varadarajan", "title": "Out of Sight But Not Out of Mind: An Answer Set Programming Based Online\n  Abduction Framework for Visual Sensemaking in Autonomous Driving", "comments": "IJCAI 2019: the 28th International Joint Conference on Artificial\n  Intelligence (IJCAI) 2019, August 10 - 16, Macao. (Preprint / to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate the need and potential of systematically integrated vision and\nsemantics} solutions for visual sensemaking (in the backdrop of autonomous\ndriving). A general method for online visual sensemaking using answer set\nprogramming is systematically formalised and fully implemented. The method\nintegrates state of the art in (deep learning based) visual computing, and is\ndeveloped as a modular framework usable within hybrid architectures for\nperception & control. We evaluate and demo with community established\nbenchmarks KITTIMOD and MOT. As use-case, we focus on the significance of\nhuman-centred visual sensemaking ---e.g., semantic representation and\nexplainability, question-answering, commonsense interpolation--- in\nsafety-critical autonomous driving situations.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 22:23:15 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Suchan", "Jakob", ""], ["Bhatt", "Mehul", ""], ["Varadarajan", "Srikrishna", ""]]}, {"id": "1906.00133", "submitter": "Ziyu Jiang", "authors": "Ziyu Jiang, Kate Von Ness, Julie Loisel, Zhangyang Wang", "title": "ArcticNet: A Deep Learning Solution to Classify Arctic Wetlands", "comments": "Published at CVPR 2019 Detecting Objects in Aerial Images Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arctic environments are rapidly changing under the warming climate. Of\nparticular interest are wetlands, a type of ecosystem that constitutes the most\neffective terrestrial long-term carbon store. As permafrost thaws, the carbon\nthat was locked in these wetland soils for millennia becomes available for\naerobic and anaerobic decomposition, which releases CO2 and CH4, respectively,\nback to the atmosphere.As CO2 and CH4 are potent greenhouse gases, this\ntransfer of carbon from the land to the atmosphere further contributes to\nglobal warming, thereby increasing the rate of permafrost degradation in a\npositive feedback loop. Therefore, monitoring Arctic wetland health and\ndynamics is a key scientific task that is also of importance for policy.\nHowever, the identification and delineation of these important wetland\necosystems, remain incomplete and often inaccurate. Mapping the extent of\nArctic wetlands remains a challenge for the scientific community. Conventional,\ncoarser remote sensing methods are inadequate at distinguishing the diverse and\nmicro-topographically complex non-vascular vegetation that characterize Arctic\nwetlands, presenting the need for better identification methods. To tackle this\nchallenging problem, we constructed and annotated the first-of-its-kind Arctic\nWetland Dataset (AWD). Based on that, we present ArcticNet, a deep neural\nnetwork that exploits the multi-spectral, high-resolution imagery captured from\nnanosatellites (Planet Dove CubeSats) with additional DEM from the ArcticDEM\nproject, to semantically label a Arctic study area into six types, in which\nthree Arctic wetland functional types are included. We present multi-fold\nefforts to handle the arising challenges, including class imbalance, and the\nchoice of fusion strategies. Preliminary results endorse the high promise of\nArcticNet, achieving 93.12% in labelling a hold-out set of regions in our\nArctic study area.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 02:40:47 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Jiang", "Ziyu", ""], ["Von Ness", "Kate", ""], ["Loisel", "Julie", ""], ["Wang", "Zhangyang", ""]]}, {"id": "1906.00139", "submitter": "Zhengyang Shen", "authors": "Zhengyang Shen, Fran\\c{c}ois-Xavier Vialard, Marc Niethammer", "title": "Region-specific Diffeomorphic Metric Mapping", "comments": "Accepted by NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a region-specific diffeomorphic metric mapping (RDMM)\nregistration approach. RDMM is non-parametric, estimating spatio-temporal\nvelocity fields which parameterize the sought-for spatial transformation.\nRegularization of these velocity fields is necessary. However, while existing\nnon-parametric registration approaches, e.g., the large displacement\ndiffeomorphic metric mapping (LDDMM) model, use a fixed spatially-invariant\nregularization our model advects a spatially-varying regularizer with the\nestimated velocity field, thereby naturally attaching a spatio-temporal\nregularizer to deforming objects. We explore a family of RDMM registration\napproaches: 1) a registration model where regions with separate regularizations\nare pre-defined (e.g., in an atlas space), 2) a registration model where a\ngeneral spatially-varying regularizer is estimated, and 3) a registration model\nwhere the spatially-varying regularizer is obtained via an end-to-end trained\ndeep learning (DL) model. We provide a variational derivation of RDMM, show\nthat the model can assure diffeomorphic transformations in the continuum, and\nthat LDDMM is a particular instance of RDMM. To evaluate RDMM performance we\nexperiment 1) on synthetic 2D data and 2) on two 3D datasets: knee magnetic\nresonance images (MRIs) of the Osteoarthritis Initiative (OAI) and computed\ntomography images (CT) of the lung. Results show that our framework achieves\nstate-of-the-art image registration performance, while providing additional\ninformation via a learned spatio-temoporal regularizer. Further, our deep\nlearning approach allows for very fast RDMM and LDDMM estimations. Our code\nwill be open-sourced. Code is available at\nhttps://github.com/uncbiag/registration.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 03:14:15 GMT"}, {"version": "v2", "created": "Sat, 9 Nov 2019 03:19:44 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Shen", "Zhengyang", ""], ["Vialard", "Fran\u00e7ois-Xavier", ""], ["Niethammer", "Marc", ""]]}, {"id": "1906.00161", "submitter": "Naveed Akhtar Dr.", "authors": "Jian Liu, Naveed Akhtar, Ajmal Mian", "title": "Temporally Coherent Full 3D Mesh Human Pose Recovery from Monocular\n  Video", "comments": "Updated bibliography", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in Deep Learning have recently made it possible to recover full 3D\nmeshes of human poses from individual images. However, extension of this notion\nto videos for recovering temporally coherent poses still remains unexplored. A\nmajor challenge in this regard is the lack of appropriately annotated video\ndata for learning the desired deep models. Existing human pose datasets only\nprovide 2D or 3D skeleton joint annotations, whereas the datasets are also\nrecorded in constrained environments. We first contribute a technique to\nsynthesize monocular action videos with rich 3D annotations that are suitable\nfor learning computational models for full mesh 3D human pose recovery.\nCompared to the existing methods which simply \"texture-map\" clothes onto the 3D\nhuman pose models, our approach incorporates Physics based realistic cloth\ndeformations with the human body movements. The generated videos cover a large\nvariety of human actions, poses, and visual appearances, whereas the\nannotations record accurate human pose dynamics and human body surface\ninformation. Our second major contribution is an end-to-end trainable Recurrent\nNeural Network for full pose mesh recovery from monocular video. Using the\nproposed video data and LSTM based recurrent structure, our network explicitly\nlearns to model the temporal coherence in videos and imposes geometric\nconsistency over the recovered meshes. We establish the effectiveness of the\nproposed model with quantitative and qualitative analysis using the proposed\nand benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 06:39:35 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 03:53:35 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Liu", "Jian", ""], ["Akhtar", "Naveed", ""], ["Mian", "Ajmal", ""]]}, {"id": "1906.00181", "submitter": "Jianxin Lin", "authors": "Jianxin Lin, Yijun Wang, Tianyu He, Zhibo Chen", "title": "Learning to Transfer: Unsupervised Meta Domain Translation", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain translation has recently achieved impressive performance\nwith Generative Adversarial Network (GAN) and sufficient (unpaired) training\ndata. However, existing domain translation frameworks form in a disposable way\nwhere the learning experiences are ignored and the obtained model cannot be\nadapted to a new coming domain. In this work, we take on unsupervised domain\ntranslation problems from a meta-learning perspective. We propose a model\ncalled Meta-Translation GAN (MT-GAN) to find good initialization of translation\nmodels. In the meta-training procedure, MT-GAN is explicitly trained with a\nprimary translation task and a synthesized dual translation task. A\ncycle-consistency meta-optimization objective is designed to ensure the\ngeneralization ability. We demonstrate effectiveness of our model on ten\ndiverse two-domain translation tasks and multiple face identity translation\ntasks. We show that our proposed approach significantly outperforms the\nexisting domain translation methods when each domain contains no more than ten\ntraining samples.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 08:24:26 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2019 08:48:45 GMT"}, {"version": "v3", "created": "Tue, 10 Sep 2019 06:28:53 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Lin", "Jianxin", ""], ["Wang", "Yijun", ""], ["He", "Tianyu", ""], ["Chen", "Zhibo", ""]]}, {"id": "1906.00184", "submitter": "Jianxin Lin", "authors": "Jianxin Lin, Yingce Xia, Sen Liu, Shuqin Zhao, Zhibo Chen", "title": "ZstGAN: An Adversarial Approach for Unsupervised Zero-Shot\n  Image-to-Image Translation", "comments": "Accepted by Nuerocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image translation models have shown remarkable ability on\ntransferring images among different domains. Most of existing work follows the\nsetting that the source domain and target domain keep the same at training and\ninference phases, which cannot be generalized to the scenarios for translating\nan image from an unseen domain to another unseen domain. In this work, we\npropose the Unsupervised Zero-Shot Image-to-image Translation (UZSIT) problem,\nwhich aims to learn a model that can translate samples from image domains that\nare not observed during training. Accordingly, we propose a framework called\nZstGAN: By introducing an adversarial training scheme, ZstGAN learns to model\neach domain with domain-specific feature distribution that is semantically\nconsistent on vision and attribute modalities. Then the domain-invariant\nfeatures are disentangled with an shared encoder for image generation. We carry\nout extensive experiments on CUB and FLO datasets, and the results demonstrate\nthe effectiveness of proposed method on UZSIT task. Moreover, ZstGAN shows\nsignificant accuracy improvements over state-of-the-art zero-shot learning\nmethods on CUB and FLO.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 08:43:44 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 05:05:04 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Lin", "Jianxin", ""], ["Xia", "Yingce", ""], ["Liu", "Sen", ""], ["Zhao", "Shuqin", ""], ["Chen", "Zhibo", ""]]}, {"id": "1906.00204", "submitter": "Sid Ahmed Fezza", "authors": "Sid Ahmed Fezza, Yassine Bakhti, Wassim Hamidouche, Olivier D\\'eforges", "title": "Perceptual Evaluation of Adversarial Attacks for CNN-based Image\n  Classification", "comments": "Eleventh International Conference on Quality of Multimedia Experience\n  (QoMEX 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have recently achieved state-of-the-art\nperformance and provide significant progress in many machine learning tasks,\nsuch as image classification, speech processing, natural language processing,\netc. However, recent studies have shown that DNNs are vulnerable to adversarial\nattacks. For instance, in the image classification domain, adding small\nimperceptible perturbations to the input image is sufficient to fool the DNN\nand to cause misclassification. The perturbed image, called \\textit{adversarial\nexample}, should be visually as close as possible to the original image.\nHowever, all the works proposed in the literature for generating adversarial\nexamples have used the $L_{p}$ norms ($L_{0}$, $L_{2}$ and $L_{\\infty}$) as\ndistance metrics to quantify the similarity between the original image and the\nadversarial example. Nonetheless, the $L_{p}$ norms do not correlate with human\njudgment, making them not suitable to reliably assess the perceptual\nsimilarity/fidelity of adversarial examples. In this paper, we present a\ndatabase for visual fidelity assessment of adversarial examples. We describe\nthe creation of the database and evaluate the performance of fifteen\nstate-of-the-art full-reference (FR) image fidelity assessment metrics that\ncould substitute $L_{p}$ norms. The database as well as subjective scores are\npublicly available to help designing new metrics for adversarial examples and\nto facilitate future research works.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 11:26:22 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Fezza", "Sid Ahmed", ""], ["Bakhti", "Yassine", ""], ["Hamidouche", "Wassim", ""], ["D\u00e9forges", "Olivier", ""]]}, {"id": "1906.00208", "submitter": "Senthil Yogamani", "authors": "Khaled El Madawy, Hazem Rashed, Ahmad El Sallab, Omar Nasr, Hanan\n  Kamel and Senthil Yogamani", "title": "RGB and LiDAR fusion based 3D Semantic Segmentation for Autonomous\n  Driving", "comments": "Accepted for Oral Presentation at IEEE Intelligent Transportation\n  Systems Conference (ITSC) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LiDAR has become a standard sensor for autonomous driving applications as\nthey provide highly precise 3D point clouds. LiDAR is also robust for low-light\nscenarios at night-time or due to shadows where the performance of cameras is\ndegraded. LiDAR perception is gradually becoming mature for algorithms\nincluding object detection and SLAM. However, semantic segmentation algorithm\nremains to be relatively less explored. Motivated by the fact that semantic\nsegmentation is a mature algorithm on image data, we explore sensor fusion\nbased 3D segmentation. Our main contribution is to convert the RGB image to a\npolar-grid mapping representation used for LiDAR and design early and mid-level\nfusion architectures. Additionally, we design a hybrid fusion architecture that\ncombines both fusion algorithms. We evaluate our algorithm on KITTI dataset\nwhich provides segmentation annotation for cars, pedestrians and cyclists. We\nevaluate two state-of-the-art architectures namely SqueezeSeg and PointSeg and\nimprove the mIoU score by 10 % in both cases relative to the LiDAR only\nbaseline.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 11:57:38 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 16:28:00 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Madawy", "Khaled El", ""], ["Rashed", "Hazem", ""], ["Sallab", "Ahmad El", ""], ["Nasr", "Omar", ""], ["Kamel", "Hanan", ""], ["Yogamani", "Senthil", ""]]}, {"id": "1906.00216", "submitter": "Duc Tam Nguyen", "authors": "Duc Tam Nguyen, Thi-Phuong-Nhung Ngo, Zhongyu Lou, Michael Klar, Laura\n  Beggel, Thomas Brox", "title": "Robust Learning Under Label Noise With Iterative Noise-Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of training a model under the presence of label\nnoise. Current approaches identify samples with potentially incorrect labels\nand reduce their influence on the learning process by either assigning lower\nweights to them or completely removing them from the training set. In the first\ncase the model however still learns from noisy labels; in the latter approach,\ngood training data can be lost. In this paper, we propose an iterative\nsemi-supervised mechanism for robust learning which excludes noisy labels but\nis still able to learn from the corresponding samples. To this end, we add an\nunsupervised loss term that also serves as a regularizer against the remaining\nlabel noise. We evaluate our approach on common classification tasks with\ndifferent noise ratios. Our robust models outperform the state-of-the-art\nmethods by a large margin. Especially for very large noise ratios, we achieve\nup to 20 % absolute improvement compared to the previous best model.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 12:34:41 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Nguyen", "Duc Tam", ""], ["Ngo", "Thi-Phuong-Nhung", ""], ["Lou", "Zhongyu", ""], ["Klar", "Michael", ""], ["Beggel", "Laura", ""], ["Brox", "Thomas", ""]]}, {"id": "1906.00225", "submitter": "Fanda Fan", "authors": "Fanda Fan, Yunyou Huang, Lei Wang, Xingwang Xiong, Zihan Jiang, Zhifei\n  Zhang and Jianfeng Zhan", "title": "A Semantic-based Medical Image Fusion Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is necessary for clinicians to comprehensively analyze patient information\nfrom different sources. Medical image fusion is a promising approach to\nproviding overall information from medical images of different modalities.\nHowever, existing medical image fusion approaches ignore the semantics of\nimages, making the fused image difficult to understand. In this work, we\npropose a new evaluation index to measure the semantic loss of fused image, and\nput forward a Fusion W-Net (FW-Net) for multimodal medical image fusion. The\nexperimental results are promising: the fused image generated by our approach\ngreatly reduces the semantic information loss, and has better visual effects in\ncontrast to five state-of-art approaches. Our approach and tool have great\npotential to be applied in the clinical setting.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 14:13:02 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 06:44:13 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Fan", "Fanda", ""], ["Huang", "Yunyou", ""], ["Wang", "Lei", ""], ["Xiong", "Xingwang", ""], ["Jiang", "Zihan", ""], ["Zhang", "Zhifei", ""], ["Zhan", "Jianfeng", ""]]}, {"id": "1906.00240", "submitter": "Jason L Causey Ph.D.", "authors": "Jason L. Causey, Yuanfang Guan, Wei Dong, Karl Walker, Jake A. Qualls,\n  Fred Prior, Xiuzhen Huang", "title": "Lung cancer screening with low-dose CT scans using a deep learning\n  approach", "comments": "6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung cancer is the leading cause of cancer deaths. Early detection through\nlow-dose computed tomography (CT) screening has been shown to significantly\nreduce mortality but suffers from a high false positive rate that leads to\nunnecessary diagnostic procedures. Quantitative image analysis coupled to deep\nlearning techniques has the potential to reduce this false positive rate. We\nconducted a computational analysis of 1449 low-dose CT studies drawn from the\nNational Lung Screening Trial (NLST) cohort. We applied to this cohort our\nnewly developed algorithm, DeepScreener, which is based on a novel deep\nlearning approach. The algorithm, after the training process using about 3000\nCT studies, does not require lung nodule annotations to conduct cancer\nprediction. The algorithm uses consecutive slices and multi-task features to\ndetermine whether a nodule is likely to be cancer, and a spatial pyramid to\ndetect nodules at different scales. We find that the algorithm can predict a\npatient's cancer status from a volumetric lung CT image with high accuracy\n(78.2%, with area under the Receiver Operating Characteristic curve (AUC) of\n0.858). Our preliminary framework ranked 16th of 1972 teams (top 1%) in the\nData Science Bowl 2017 (DSB2017) competition, based on the challenge datasets.\nWe report here the application of DeepScreener on an independent NLST test set.\nThis study indicates that the deep learning approach has the potential to\nsignificantly reduce the false positive rate in lung cancer screening with\nlow-dose CT scans.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 15:19:34 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Causey", "Jason L.", ""], ["Guan", "Yuanfang", ""], ["Dong", "Wei", ""], ["Walker", "Karl", ""], ["Qualls", "Jake A.", ""], ["Prior", "Fred", ""], ["Huang", "Xiuzhen", ""]]}, {"id": "1906.00254", "submitter": "Ivan Kiskin", "authors": "Ivan Kiskin, Udeepa Meepegama, Steven Roberts", "title": "Super-resolution of Time-series Labels for Bootstrapped Event Detection", "comments": "Accepted at the Time-series workshop at ICML 2019, Long Beach", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving real-world problems, particularly with deep learning, relies on the\navailability of abundant, quality data. In this paper we develop a novel\nframework that maximises the utility of time-series datasets that contain only\nsmall quantities of expertly-labelled data, larger quantities of weakly (or\ncoarsely) labelled data and a large volume of unlabelled data. This represents\nscenarios commonly encountered in the real world, such as in crowd-sourcing\napplications. In our work, we use a nested loop using a Kernel Density\nEstimator (KDE) to super-resolve the abundant low-quality data labels, thereby\nenabling effective training of a Convolutional Neural Network (CNN). We\ndemonstrate two key results: a) The KDE is able to super-resolve labels more\naccurately, and with better calibrated probabilities, than well-established\nclassifiers acting as baselines; b) Our CNN, trained on super-resolved labels\nfrom the KDE, achieves an improvement in F1 score of 22.1% over the next best\nbaseline system in our candidate problem domain.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 16:29:50 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Kiskin", "Ivan", ""], ["Meepegama", "Udeepa", ""], ["Roberts", "Steven", ""]]}, {"id": "1906.00258", "submitter": "Connie Kou", "authors": "Connie Kou, Hwee Kuan Lee, Ee-Chien Chang, Teck Khim Ng", "title": "Enhancing Transformation-based Defenses using a Distribution Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks on convolutional neural networks (CNN) have gained\nsignificant attention and there have been active research efforts on defense\nmechanisms. Stochastic input transformation methods have been proposed, where\nthe idea is to recover the image from adversarial attack by random\ntransformation, and to take the majority vote as consensus among the random\nsamples. However, the transformation improves the accuracy on adversarial\nimages at the expense of the accuracy on clean images. While it is intuitive\nthat the accuracy on clean images would deteriorate, the exact mechanism in\nwhich how this occurs is unclear. In this paper, we study the distribution of\nsoftmax induced by stochastic transformations. We observe that with random\ntransformations on the clean images, although the mass of the softmax\ndistribution could shift to the wrong class, the resulting distribution of\nsoftmax could be used to correct the prediction. Furthermore, on the\nadversarial counterparts, with the image transformation, the resulting shapes\nof the distribution of softmax are similar to the distributions from the clean\nimages. With these observations, we propose a method to improve existing\ntransformation-based defenses. We train a separate lightweight distribution\nclassifier to recognize distinct features in the distributions of softmax\noutputs of transformed images. Our empirical studies show that our distribution\nclassifier, by training on distributions obtained from clean images only,\noutperforms majority voting for both clean and adversarial images. Our method\nis generic and can be integrated with existing transformation-based defenses.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 16:59:17 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 05:34:00 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Kou", "Connie", ""], ["Lee", "Hwee Kuan", ""], ["Chang", "Ee-Chien", ""], ["Ng", "Teck Khim", ""]]}, {"id": "1906.00265", "submitter": "Sedat Ozer", "authors": "Sedat Ozer", "title": "Parametric Shape Modeling and Skeleton Extraction with Radial Basis\n  Functions using Similarity Domains Network", "comments": "This is the pre-print for the CVPR submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate the use of similarity domains (SDs) for shape modeling and\nskeleton extraction. SDs are recently proposed and they can be utilized in a\nneural network framework to help us analyze shapes. SDs are modeled with radial\nbasis functions with varying shape parameters in Similarity Domains Networks\n(SDNs). In this paper, we demonstrate how using SDN can first help us model a\npixel-based image in terms of SDs and then demonstrate how those learned SDs\ncan be used to extract the skeleton of a shape.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 18:34:10 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Ozer", "Sedat", ""]]}, {"id": "1906.00270", "submitter": "Benoit Brummer", "authors": "Benoit Brummer, Christophe De Vleeschouwer", "title": "Natural Image Noise Dataset", "comments": "NTIRE at CVPR 2019", "journal-ref": null, "doi": "10.1109/CVPRW.2019.00228", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Convolutional neural networks have been the focus of research aiming to solve\nimage denoising problems, but their performance remains unsatisfactory for most\napplications. These networks are trained with synthetic noise distributions\nthat do not accurately reflect the noise captured by image sensors. Some\ndatasets of clean-noisy image pairs have been introduced but they are usually\nmeant for benchmarking or specific applications. We introduce the Natural Image\nNoise Dataset (NIND), a dataset of DSLR-like images with varying levels of ISO\nnoise which is large enough to train models for blind denoising over a wide\nrange of noise. We demonstrate a denoising model trained with the NIND and show\nthat it significantly outperforms BM3D on ISO noise from unseen images, even\nwhen generalizing to images from a different type of camera. The Natural Image\nNoise Dataset is published on Wikimedia Commons such that it remains open for\ncuration and contributions. We expect that this dataset will prove useful for\nfuture image denoising applications.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 18:53:29 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Brummer", "Benoit", ""], ["De Vleeschouwer", "Christophe", ""]]}, {"id": "1906.00283", "submitter": "Chih-Yao Ma", "authors": "Chih-Yao Ma, Yannis Kalantidis, Ghassan AlRegib, Peter Vajda, Marcus\n  Rohrbach, Zsolt Kira", "title": "Learning to Generate Grounded Visual Captions without Localization\n  Supervision", "comments": "ECCV 2020. Code is available at\n  https://github.com/chihyaoma/cyclical-visual-captioning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When automatically generating a sentence description for an image or video,\nit often remains unclear how well the generated caption is grounded, that is\nwhether the model uses the correct image regions to output particular words, or\nif the model is hallucinating based on priors in the dataset and/or the\nlanguage model. The most common way of relating image regions with words in\ncaption models is through an attention mechanism over the regions that are used\nas input to predict the next word. The model must therefore learn to predict\nthe attentional weights without knowing the word it should localize. This is\ndifficult to train without grounding supervision since recurrent models can\npropagate past information and there is no explicit signal to force the\ncaptioning model to properly ground the individual decoded words. In this work,\nwe help the model to achieve this via a novel cyclical training regimen that\nforces the model to localize each word in the image after the sentence decoder\ngenerates it, and then reconstruct the sentence from the localized image\nregion(s) to match the ground-truth. Our proposed framework only requires\nlearning one extra fully-connected layer (the localizer), a layer that can be\nremoved at test time. We show that our model significantly improves grounding\naccuracy without relying on grounding supervision or introducing extra\ncomputation during inference, for both image and video captioning tasks. Code\nis available at https://github.com/chihyaoma/cyclical-visual-captioning .\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 20:21:24 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 22:25:36 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 23:56:28 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ma", "Chih-Yao", ""], ["Kalantidis", "Yannis", ""], ["AlRegib", "Ghassan", ""], ["Vajda", "Peter", ""], ["Rohrbach", "Marcus", ""], ["Kira", "Zsolt", ""]]}, {"id": "1906.00330", "submitter": "Liuyu Xiang", "authors": "Liuyu Xiang, Xiaoming Jin, Guiguang Ding, Jungong Han, Leida Li", "title": "Incremental Few-Shot Learning for Pedestrian Attribute Recognition", "comments": "IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian attribute recognition has received increasing attention due to its\nimportant role in video surveillance applications. However, most existing\nmethods are designed for a fixed set of attributes. They are unable to handle\nthe incremental few-shot learning scenario, i.e. adapting a well-trained model\nto newly added attributes with scarce data, which commonly exists in the real\nworld. In this work, we present a meta learning based method to address this\nissue. The core of our framework is a meta architecture capable of\ndisentangling multiple attribute information and generalizing rapidly to new\ncoming attributes. By conducting extensive experiments on the benchmark dataset\nPETA and RAP under the incremental few-shot setting, we show that our method is\nable to perform the task with competitive performances and low resource\nrequirements.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 02:49:01 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 12:42:01 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Xiang", "Liuyu", ""], ["Jin", "Xiaoming", ""], ["Ding", "Guiguang", ""], ["Han", "Jungong", ""], ["Li", "Leida", ""]]}, {"id": "1906.00332", "submitter": "Haekyu Park", "authors": "Haekyu Park, Fred Hohman, Duen Horng Chau", "title": "NeuralDivergence: Exploring and Understanding Neural Networks by\n  Comparing Activation Distributions", "comments": "Published in PacificVis2019, Poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep neural networks are increasingly used in solving high-stake problems,\nthere is a pressing need to understand their internal decision mechanisms.\nVisualization has helped address this problem by assisting with interpreting\ncomplex deep neural networks. However, current tools often support only single\ndata instances, or visualize layers in isolation. We present NeuralDivergence,\nan interactive visualization system that uses activation distributions as a\nhigh-level summary of what a model has learned. NeuralDivergence enables users\nto interactively summarize and compare activation distributions across layers,\nclasses, and instances (e.g., pairs of adversarial attacked and benign images),\nhelping them gain better understanding of neural network models.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 03:03:51 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Park", "Haekyu", ""], ["Hohman", "Fred", ""], ["Chau", "Duen Horng", ""]]}, {"id": "1906.00335", "submitter": "Christian Cosgrove", "authors": "Christian Cosgrove and Alan L. Yuille", "title": "Adversarial Examples for Edge Detection: They Exist, and They Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have recently advanced the state of the art in\nmany tasks including edge and object boundary detection. However, in this\npaper, we demonstrate that these edge detectors inherit a troubling property of\nneural networks: they can be fooled by adversarial examples. We show that\nadding small perturbations to an image causes HED, a CNN-based edge detection\nmodel, to fail to locate edges, to detect nonexistent edges, and even to\nhallucinate arbitrary configurations of edges. More surprisingly, we find that\nthese adversarial examples transfer to other CNN-based vision models. In\nparticular, attacks on edge detection result in significant drops in accuracy\nin models trained to perform unrelated, high-level tasks like image\nclassification and semantic segmentation. Our code will be made public.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 03:51:21 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Cosgrove", "Christian", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1906.00358", "submitter": "Hao Wang", "authors": "Hao Wang, Qilong Wang, Fan Yang, Weiqi Zhang, Wangmeng Zuo", "title": "Data Augmentation for Object Detection via Progressive and Selective\n  Instance-Switching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Collection of massive well-annotated samples is effective in improving object\ndetection performance but is extremely laborious and costly. Instead of data\ncollection and annotation, the recently proposed Cut-Paste methods [12, 15]\nshow the potential to augment training dataset by cutting foreground objects\nand pasting them on proper new backgrounds. However, existing Cut-Paste methods\ncannot guarantee synthetic images always precisely model visual context, and\nall of them require external datasets. To handle above issues, this paper\nproposes a simple yet effective instance-switching (IS) strategy, which\ngenerates new training data by switching instances of same class from different\nimages. Our IS naturally preserves contextual coherence in the original images\nwhile requiring no external dataset. For guiding our IS to obtain better object\nperformance, we explore issues of instance imbalance and class importance in\ndatasets, which frequently occur and bring adverse effect on detection\nperformance. To this end, we propose a novel Progressive and Selective\nInstance-Switching (PSIS) method to augment training data for object detection.\nThe proposed PSIS enhances instance balance by combining selective re-sampling\nwith a class-balanced loss, and considers class importance by progressively\naugmenting training dataset guided by detection performance. The experiments\nare conducted on the challenging MS COCO benchmark, and results demonstrate our\nPSIS brings clear improvement over various state-of-the-art detectors (e.g.,\nFaster R-CNN, FPN, Mask R-CNN and SNIPER), showing the superiority and\ngenerality of our PSIS. Code and models are available at:\nhttps://github.com/Hwang64/PSIS.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 07:31:36 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 01:40:05 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Wang", "Hao", ""], ["Wang", "Qilong", ""], ["Yang", "Fan", ""], ["Zhang", "Weiqi", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "1906.00360", "submitter": "Arno Solin", "authors": "Santiago Cort\\'es Reina, Yuxin Hou, Juho Kannala, Arno Solin", "title": "Iterative Path Reconstruction for Large-Scale Inertial Navigation on\n  Smartphones", "comments": "To appear in Proceedings FUSION 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern smartphones have all the sensing capabilities required for accurate\nand robust navigation and tracking. In specific environments some data streams\nmay be absent, less reliable, or flat out wrong. In particular, the GNSS signal\ncan become flawed or silent inside buildings or in streets with tall buildings.\nIn this application paper, we aim to advance the current state-of-the-art in\nmotion estimation using inertial measurements in combination with partial GNSS\ndata on standard smartphones. We show how iterative estimation methods help\nrefine the positioning path estimates in retrospective use cases that can cover\nboth fixed-interval and fixed-lag scenarios. We compare estimation results\nprovided by global iterated Kalman filtering methods to those of a\nvisual-inertial tracking scheme (Apple ARKit). The practical applicability is\ndemonstrated on real-world use cases on empirical data acquired from both\nsmartphones and tablet devices.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 07:41:01 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Reina", "Santiago Cort\u00e9s", ""], ["Hou", "Yuxin", ""], ["Kannala", "Juho", ""], ["Solin", "Arno", ""]]}, {"id": "1906.00377", "submitter": "Feng Mao", "authors": "Feng Mao, Xiang Wu, Hui Xue, Rong Zhang", "title": "Hierarchical Video Frame Sequence Representation with Deep Convolutional\n  Graph Network", "comments": "ECCV 2018", "journal-ref": "ECCV 2018 Workshops pp 262-270", "doi": "10.1007/978-3-030-11018-5_24", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High accuracy video label prediction (classification) models are attributed\nto large scale data. These data could be frame feature sequences extracted by a\npre-trained convolutional-neural-network, which promote the efficiency for\ncreating models. Unsupervised solutions such as feature average pooling, as a\nsimple label-independent parameter-free based method, has limited ability to\nrepresent the video. While the supervised methods, like RNN, can greatly\nimprove the recognition accuracy. However, the video length is usually long,\nand there are hierarchical relationships between frames across events in the\nvideo, the performance of RNN based models are decreased. In this paper, we\nproposes a novel video classification method based on a deep convolutional\ngraph neural network(DCGN). The proposed method utilize the characteristics of\nthe hierarchical structure of the video, and performed multi-level feature\nextraction on the video frame sequence through the graph network, obtained a\nvideo representation re ecting the event semantics hierarchically. We test our\nmodel on YouTube-8M Large-Scale Video Understanding dataset, and the result\noutperforms RNN based benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 10:02:39 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Mao", "Feng", ""], ["Wu", "Xiang", ""], ["Xue", "Hui", ""], ["Zhang", "Rong", ""]]}, {"id": "1906.00378", "submitter": "Shizhe Chen", "authors": "Shizhe Chen, Qin Jin and Alexander Hauptmann", "title": "Unsupervised Bilingual Lexicon Induction from Mono-lingual Multimodal\n  Data", "comments": "Accepted by AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bilingual lexicon induction, translating words from the source language to\nthe target language, is a long-standing natural language processing task.\nRecent endeavors prove that it is promising to employ images as pivot to learn\nthe lexicon induction without reliance on parallel corpora. However, these\nvision-based approaches simply associate words with entire images, which are\nconstrained to translate concrete words and require object-centered images. We\nhumans can understand words better when they are within a sentence with\ncontext. Therefore, in this paper, we propose to utilize images and their\nassociated captions to address the limitations of previous approaches. We\npropose a multi-lingual caption model trained with different mono-lingual\nmultimodal data to map words in different languages into joint spaces. Two\ntypes of word representation are induced from the multi-lingual caption model:\nlinguistic features and localized visual features. The linguistic feature is\nlearned from the sentence contexts with visual semantic constraints, which is\nbeneficial to learn translation for words that are less visual-relevant. The\nlocalized visual feature is attended to the region in the image that correlates\nto the word, so that it alleviates the image restriction for salient visual\nrepresentation. The two types of features are complementary for word\ntranslation. Experimental results on multiple language pairs demonstrate the\neffectiveness of our proposed method, which substantially outperforms previous\nvision-based approaches without using any parallel sentences or supervision of\nseed word pairs.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 10:05:26 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Chen", "Shizhe", ""], ["Jin", "Qin", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "1906.00446", "submitter": "A\\\"aron van den Oord", "authors": "Ali Razavi, Aaron van den Oord, Oriol Vinyals", "title": "Generating Diverse High-Fidelity Images with VQ-VAE-2", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE)\nmodels for large scale image generation. To this end, we scale and enhance the\nautoregressive priors used in VQ-VAE to generate synthetic samples of much\nhigher coherence and fidelity than possible before. We use simple feed-forward\nencoder and decoder networks, making our model an attractive candidate for\napplications where the encoding and/or decoding speed is critical.\nAdditionally, VQ-VAE requires sampling an autoregressive model only in the\ncompressed latent space, which is an order of magnitude faster than sampling in\nthe pixel space, especially for large images. We demonstrate that a multi-scale\nhierarchical organization of VQ-VAE, augmented with powerful priors over the\nlatent codes, is able to generate samples with quality that rivals that of\nstate of the art Generative Adversarial Networks on multifaceted datasets such\nas ImageNet, while not suffering from GAN's known shortcomings such as mode\ncollapse and lack of diversity.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 16:46:42 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Razavi", "Ali", ""], ["Oord", "Aaron van den", ""], ["Vinyals", "Oriol", ""]]}, {"id": "1906.00460", "submitter": "Vladislav Malyshkin", "authors": "Vladislav Gennadievich Malyshkin", "title": "On The Radon-Nikodym Spectral Approach With Optimal Clustering", "comments": "Relation to PCA variation expansion is added. Whereas a regular PCA\n  variation expansion depends on attributes normalizing, the PCA variation\n  expansion in the Lebesgue quadrature arXiv:1807.06007 basis is unique thus\n  does not depend on attributes scale, moreover it is invariant relatively any\n  non-degenerated linear transform of input vector components. Christoffel\n  function solution to vector label", "journal-ref": null, "doi": "10.2139/ssrn.3398755", "report-no": null, "categories": "cs.LG cs.CV cs.NA math.NA stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Problems of interpolation, classification, and clustering are considered. In\nthe tenets of Radon--Nikodym approach $\\langle f(\\mathbf{x})\\psi^2 \\rangle /\n\\langle\\psi^2\\rangle$, where the $\\psi(\\mathbf{x})$ is a linear function on\ninput attributes, all the answers are obtained from a generalized eigenproblem\n$|f|\\psi^{[i]}\\rangle = \\lambda^{[i]} |\\psi^{[i]}\\rangle$. The solution to the\ninterpolation problem is a regular Radon-Nikodym derivative. The solution to\nthe classification problem requires prior and posterior probabilities that are\nobtained using the Lebesgue quadrature[1] technique. Whereas in a Bayesian\napproach new observations change only outcome probabilities, in the\nRadon-Nikodym approach not only outcome probabilities but also the probability\nspace $|\\psi^{[i]}\\rangle$ change with new observations. This is a remarkable\nfeature of the approach: both the probabilities and the probability space are\nconstructed from the data. The Lebesgue quadrature technique can be also\napplied to the optimal clustering problem. The problem is solved by\nconstructing a Gaussian quadrature on the Lebesgue measure. A distinguishing\nfeature of the Radon-Nikodym approach is the knowledge of the invariant group:\nall the answers are invariant relatively any non-degenerated linear transform\nof input vector $\\mathbf{x}$ components. A software product implementing the\nalgorithms of interpolation, classification, and optimal clustering is\navailable from the authors.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 17:57:08 GMT"}, {"version": "v10", "created": "Mon, 6 Apr 2020 21:37:36 GMT"}, {"version": "v11", "created": "Tue, 7 Jul 2020 11:50:54 GMT"}, {"version": "v12", "created": "Sun, 18 Oct 2020 12:58:48 GMT"}, {"version": "v13", "created": "Sun, 31 Jan 2021 23:57:44 GMT"}, {"version": "v14", "created": "Sun, 14 Feb 2021 12:44:25 GMT"}, {"version": "v15", "created": "Sun, 9 May 2021 20:39:06 GMT"}, {"version": "v16", "created": "Sun, 13 Jun 2021 22:54:37 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 08:48:17 GMT"}, {"version": "v3", "created": "Wed, 19 Jun 2019 11:29:56 GMT"}, {"version": "v4", "created": "Thu, 27 Jun 2019 15:22:16 GMT"}, {"version": "v5", "created": "Mon, 15 Jul 2019 10:12:08 GMT"}, {"version": "v6", "created": "Wed, 17 Jul 2019 15:26:51 GMT"}, {"version": "v7", "created": "Fri, 30 Aug 2019 13:08:31 GMT"}, {"version": "v8", "created": "Mon, 28 Oct 2019 00:52:34 GMT"}, {"version": "v9", "created": "Mon, 24 Feb 2020 17:11:37 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Malyshkin", "Vladislav Gennadievich", ""]]}, {"id": "1906.00495", "submitter": "Naiyang Guan", "authors": "Naiyang Guan, Tongliang Liu, Yangmuzi Zhang, Dacheng Tao, Larry S.\n  Davis", "title": "Truncated Cauchy Non-negative Matrix Factorization", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (IEEE T-PAMI), vol. 41, no. 1, pp. 246-259, Jan. 2019", "doi": "10.1109/TPAMI.2017.2777841", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative matrix factorization (NMF) minimizes the Euclidean distance\nbetween the data matrix and its low rank approximation, and it fails when\napplied to corrupted data because the loss function is sensitive to outliers.\nIn this paper, we propose a Truncated CauchyNMF loss that handle outliers by\ntruncating large errors, and develop a Truncated CauchyNMF to robustly learn\nthe subspace on noisy datasets contaminated by outliers. We theoretically\nanalyze the robustness of Truncated CauchyNMF comparing with the competing\nmodels and theoretically prove that Truncated CauchyNMF has a generalization\nbound which converges at a rate of order $O(\\sqrt{{\\ln n}/{n}})$, where $n$ is\nthe sample size. We evaluate Truncated CauchyNMF by image clustering on both\nsimulated and real datasets. The experimental results on the datasets\ncontaining gross corruptions validate the effectiveness and robustness of\nTruncated CauchyNMF for learning robust subspaces.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 22:21:30 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Guan", "Naiyang", ""], ["Liu", "Tongliang", ""], ["Zhang", "Yangmuzi", ""], ["Tao", "Dacheng", ""], ["Davis", "Larry S.", ""]]}, {"id": "1906.00513", "submitter": "Jialin Wu", "authors": "Jialin Wu, Zeyuan Hu and Raymond J. Mooney", "title": "Generating Question Relevant Captions to Aid Visual Question Answering", "comments": "ACL 2019 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (VQA) and image captioning require a shared body of\ngeneral knowledge connecting language and vision. We present a novel approach\nto improve VQA performance that exploits this connection by jointly generating\ncaptions that are targeted to help answer a specific visual question. The model\nis trained using an existing caption dataset by automatically determining\nquestion-relevant captions using an online gradient-based method. Experimental\nresults on the VQA v2 challenge demonstrates that our approach obtains\nstate-of-the-art VQA performance (e.g. 68.4% on the Test-standard set using a\nsingle model) by simultaneously generating question-relevant captions.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 00:42:08 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 14:41:07 GMT"}, {"version": "v3", "created": "Fri, 3 Jan 2020 19:12:39 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Wu", "Jialin", ""], ["Hu", "Zeyuan", ""], ["Mooney", "Raymond J.", ""]]}, {"id": "1906.00544", "submitter": "Yudong Guo", "authors": "Yudong Guo, Luo Jiang, Lin Cai, Juyong Zhang", "title": "3D Magic Mirror: Automatic Video to 3D Caricature Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caricature is an abstraction of a real person which distorts or exaggerates\ncertain features, but still retains a likeness. While most existing works focus\non 3D caricature reconstruction from 2D caricatures or translating 2D photos to\n2D caricatures, this paper presents a real-time and automatic algorithm for\ncreating expressive 3D caricatures with caricature style texture map from 2D\nphotos or videos. To solve this challenging ill-posed reconstruction problem\nand cross-domain translation problem, we first reconstruct the 3D face shape\nfor each frame, and then translate 3D face shape from normal style to\ncaricature style by a novel identity and expression preserving VAE-CycleGAN.\nBased on a labeling formulation, the caricature texture map is constructed from\na set of multi-view caricature images generated by CariGANs. The effectiveness\nand efficiency of our method are demonstrated by comparison with baseline\nimplementations. The perceptual study shows that the 3D caricatures generated\nby our method meet people's expectations of 3D caricature style.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 03:12:29 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Guo", "Yudong", ""], ["Jiang", "Luo", ""], ["Cai", "Lin", ""], ["Zhang", "Juyong", ""]]}, {"id": "1906.00546", "submitter": "Zhaoqun Li", "authors": "Zhaoqun Li, Cheng Xu, Biao Leng", "title": "Rethinking Loss Design for Large-scale 3D Shape Retrieval", "comments": "Accepted by IJCAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning discriminative shape representations is a crucial issue for\nlarge-scale 3D shape retrieval. In this paper, we propose the Collaborative\nInner Product Loss (CIP Loss) to obtain ideal shape embedding that\ndiscriminative among different categories and clustered within the same class.\nUtilizing simple inner product operation, CIP loss explicitly enforces the\nfeatures of the same class to be clustered in a linear subspace, while\ninter-class subspaces are constrained to be at least orthogonal. Compared to\nprevious metric loss functions, CIP loss could provide more clear geometric\ninterpretation for the embedding than Euclidean margin, and is easy to\nimplement without normalization operation referring to cosine margin. Moreover,\nour proposed loss term can combine with other commonly used loss functions and\ncan be easily plugged into existing off-the-shelf architectures. Extensive\nexperiments conducted on the two public 3D object retrieval datasets, ModelNet\nand ShapeNetCore 55, demonstrate the effectiveness of our proposal, and our\nmethod has achieved state-of-the-art results on both datasets.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 03:17:22 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Li", "Zhaoqun", ""], ["Xu", "Cheng", ""], ["Leng", "Biao", ""]]}, {"id": "1906.00562", "submitter": "Qianru Sun", "authors": "Xinzhe Li, Qianru Sun, Yaoyao Liu, Shibao Zheng, Qin Zhou, Tat-Seng\n  Chua, and Bernt Schiele", "title": "Learning to Self-Train for Semi-Supervised Few-Shot Classification", "comments": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Few-shot classification (FSC) is challenging due to the scarcity of labeled\ntraining data (e.g. only one labeled data point per class). Meta-learning has\nshown to achieve promising results by learning to initialize a classification\nmodel for FSC. In this paper we propose a novel semi-supervised meta-learning\nmethod called learning to self-train (LST) that leverages unlabeled data and\nspecifically meta-learns how to cherry-pick and label such unsupervised data to\nfurther improve performance. To this end, we train the LST model through a\nlarge number of semi-supervised few-shot tasks. On each task, we train a\nfew-shot model to predict pseudo labels for unlabeled data, and then iterate\nthe self-training steps on labeled and pseudo-labeled data with each step\nfollowed by fine-tuning. We additionally learn a soft weighting network (SWN)\nto optimize the self-training weights of pseudo labels so that better ones can\ncontribute more to gradient descent optimization. We evaluate our LST method on\ntwo ImageNet benchmarks for semi-supervised few-shot classification and achieve\nlarge improvements over the state-of-the-art method. Code is at\nhttps://github.com/xinzheli1217/learning-to-self-train.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 04:09:32 GMT"}, {"version": "v2", "created": "Sun, 29 Sep 2019 04:43:05 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Li", "Xinzhe", ""], ["Sun", "Qianru", ""], ["Liu", "Yaoyao", ""], ["Zheng", "Shibao", ""], ["Zhou", "Qin", ""], ["Chua", "Tat-Seng", ""], ["Schiele", "Bernt", ""]]}, {"id": "1906.00586", "submitter": "Mitchell Wortsman", "authors": "Mitchell Wortsman, Ali Farhadi, Mohammad Rastegari", "title": "Discovering Neural Wirings", "comments": "NeurIPS 2019 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of neural networks has driven a shift in focus from feature\nengineering to architecture engineering. However, successful networks today are\nconstructed using a small and manually defined set of building blocks. Even in\nmethods of neural architecture search (NAS) the network connectivity patterns\nare largely constrained. In this work we propose a method for discovering\nneural wirings. We relax the typical notion of layers and instead enable\nchannels to form connections independent of each other. This allows for a much\nlarger space of possible networks. The wiring of our network is not fixed\nduring training -- as we learn the network parameters we also learn the\nstructure itself. Our experiments demonstrate that our learned connectivity\noutperforms hand engineered and randomly wired networks. By learning the\nconnectivity of MobileNetV1we boost the ImageNet accuracy by 10% at ~41M FLOPs.\nMoreover, we show that our method generalizes to recurrent and continuous time\nnetworks. Our work may also be regarded as unifying core aspects of the neural\narchitecture search problem with sparse neural network learning. As NAS becomes\nmore fine grained, finding a good architecture is akin to finding a sparse\nsubnetwork of the complete graph. Accordingly, DNW provides an effective\nmechanism for discovering sparse subnetworks of predefined architectures in a\nsingle training run. Though we only ever use a small percentage of the weights\nduring the forward pass, we still play the so-called initialization lottery\nwith a combinatorial number of subnetworks. Code and pretrained models are\navailable at https://github.com/allenai/dnw while additional visualizations may\nbe found at https://mitchellnw.github.io/blog/2019/dnw/.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 05:58:33 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 22:49:47 GMT"}, {"version": "v3", "created": "Thu, 19 Sep 2019 23:16:39 GMT"}, {"version": "v4", "created": "Mon, 28 Oct 2019 04:04:44 GMT"}, {"version": "v5", "created": "Sun, 17 Nov 2019 02:54:50 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Wortsman", "Mitchell", ""], ["Farhadi", "Ali", ""], ["Rastegari", "Mohammad", ""]]}, {"id": "1906.00590", "submitter": "Yuan Hu", "authors": "Yuan Hu, Yingtian Zou, Jiashi Feng", "title": "Panoptic Edge Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pursuing more complete and coherent scene understanding towards realistic\nvision applications drives edge detection from category-agnostic to\ncategory-aware semantic level. However, finer delineation of instance-level\nboundaries still remains unexcavated. In this work, we address a new\nfiner-grained task, termed panoptic edge detection (PED), which aims at\npredicting semantic-level boundaries for stuff categories and instance-level\nboundaries for instance categories, in order to provide more comprehensive and\nunified scene understanding from the perspective of edges.We then propose a\nversatile framework, Panoptic Edge Network (PEN), which aggregates different\ntasks of object detection, semantic and instance edge detection into a single\nholistic network with multiple branches. Based on the same feature\nrepresentation, the semantic edge branch produces semantic-level boundaries for\nall categories and the object detection branch generates instance proposals.\nConditioned on the prior information from these two branches, the instance edge\nbranch aims at instantiating edge predictions for instance categories. Besides,\nwe also devise a Panoptic Dual F-measure (F2) metric for the new PED task to\nuniformly measure edge prediction quality for both stuff and instances. By\njoint end-to-end training, the proposed PEN framework outperforms all\ncompetitive baselines on Cityscapes and ADE20K datasets.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 06:18:30 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Hu", "Yuan", ""], ["Zou", "Yingtian", ""], ["Feng", "Jiashi", ""]]}, {"id": "1906.00604", "submitter": "Xuelun Shen", "authors": "Xuelun Shen, Cheng Wang, Xin Li, Zenglei Yu, Jonathan Li, Chenglu Wen,\n  Ming Cheng, Zijian He", "title": "RF-Net: An End-to-End Image Matching Network based on Receptive Field", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new end-to-end trainable matching network based on\nreceptive field, RF-Net, to compute sparse correspondence between images.\nBuilding end-to-end trainable matching framework is desirable and challenging.\nThe very recent approach, LF-Net, successfully embeds the entire feature\nextraction pipeline into a jointly trainable pipeline, and produces the\nstate-of-the-art matching results. This paper introduces two modifications to\nthe structure of LF-Net. First, we propose to construct receptive feature maps,\nwhich lead to more effective keypoint detection. Second, we introduce a general\nloss function term, neighbor mask, to facilitate training patch selection. This\nresults in improved stability in descriptor training. We trained RF-Net on the\nopen dataset HPatches, and compared it with other methods on multiple benchmark\ndatasets. Experiments show that RF-Net outperforms existing state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 07:11:48 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Shen", "Xuelun", ""], ["Wang", "Cheng", ""], ["Li", "Xin", ""], ["Yu", "Zenglei", ""], ["Li", "Jonathan", ""], ["Wen", "Chenglu", ""], ["Cheng", "Ming", ""], ["He", "Zijian", ""]]}, {"id": "1906.00617", "submitter": "Amal Lahiani", "authors": "Amal Lahiani, Nassir Navab, Shadi Albarqouni, Eldad Klaiman", "title": "Perceptual Embedding Consistency for Seamless Reconstruction of Tilewise\n  Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Style transfer is a field with growing interest and use cases in deep\nlearning. Recent work has shown Generative Adversarial Networks(GANs) can be\nused to create realistic images of virtually stained slide images in digital\npathology with clinically validated interpretability. Digital pathology images\nare typically of extremely high resolution, making tilewise analysis necessary\nfor deep learning applications. It has been shown that image generators with\ninstance normalization can cause a tiling artifact when a large image is\nreconstructed from the tilewise analysis. We introduce a novel perceptual\nembedding consistency loss significantly reducing the tiling artifact created\nin the reconstructed whole slide image (WSI). We validate our results by\ncomparing virtually stained slide images with consecutive real stained tissue\nslide images. We also demonstrate that our model is more robust to contrast,\ncolor and brightness perturbations by running comparative sensitivity analysis\ntests.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 07:57:13 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Lahiani", "Amal", ""], ["Navab", "Nassir", ""], ["Albarqouni", "Shadi", ""], ["Klaiman", "Eldad", ""]]}, {"id": "1906.00619", "submitter": "Jayashree Karlekar", "authors": "Jayashree Karlekar, Jiashi Feng, Zi Sian Wong, Sugiri Pranata", "title": "Deep Face Recognition Model Compression via Knowledge Transfer and\n  Distillation", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully convolutional networks (FCNs) have become de facto tool to achieve very\nhigh-level performance for many vision and non-vision tasks in general and face\nrecognition in particular. Such high-level accuracies are normally obtained by\nvery deep networks or their ensemble. However, deploying such high performing\nmodels to resource constraint devices or real-time applications is challenging.\nIn this paper, we present a novel model compression approach based on\nstudent-teacher paradigm for face recognition applications. The proposed\napproach consists of training teacher FCN at bigger image resolution while\nstudent FCNs are trained at lower image resolutions than that of teacher FCN.\nWe explored three different approaches to train student FCNs: knowledge\ntransfer (KT), knowledge distillation (KD) and their combination. Experimental\nevaluation on LFW and IJB-C datasets demonstrate comparable improvements in\naccuracies with these approaches. Training low-resolution student FCNs from\nhigher resolution teacher offer fourfold advantage of accelerated training,\naccelerated inference, reduced memory requirements and improved accuracies. We\nevaluated all models on IJB-C dataset and achieved state-of-the-art results on\nthis benchmark. The teacher network and some student networks even achieved\nTop-1 performance on IJB-C dataset. The proposed approach is simple and\nhardware friendly, thus enables the deployment of high performing face\nrecognition deep models to resource constraint devices.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 07:59:53 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Karlekar", "Jayashree", ""], ["Feng", "Jiashi", ""], ["Wong", "Zi Sian", ""], ["Pranata", "Sugiri", ""]]}, {"id": "1906.00629", "submitter": "Kosuke Tanizaki", "authors": "Kosuke Tanizaki, Noriaki Hashimoto, Yu Inatsu, Hidekata Hontani and\n  Ichiro Takeuchi", "title": "Computing Valid p-values for Image Segmentation by Selective Inference", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is one of the most fundamental tasks of computer vision.\nIn many practical applications, it is essential to properly evaluate the\nreliability of individual segmentation results. In this study, we propose a\nnovel framework to provide the statistical significance of segmentation results\nin the form of p-values. Specifically, we consider a statistical hypothesis\ntest for determining the difference between the object and the background\nregions. This problem is challenging because the difference can be deceptively\nlarge (called segmentation bias) due to the adaptation of the segmentation\nalgorithm to the data. To overcome this difficulty, we introduce a statistical\napproach called selective inference, and develop a framework to compute valid\np-values in which the segmentation bias is properly accounted for. Although the\nproposed framework is potentially applicable to various segmentation\nalgorithms, we focus in this paper on graph cut-based and threshold-based\nsegmentation algorithms, and develop two specific methods to compute valid\np-values for the segmentation results obtained by these algorithms. We prove\nthe theoretical validity of these two methods and demonstrate their\npracticality by applying them to segmentation problems for medical images.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 08:27:27 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 10:23:18 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Tanizaki", "Kosuke", ""], ["Hashimoto", "Noriaki", ""], ["Inatsu", "Yu", ""], ["Hontani", "Hidekata", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "1906.00634", "submitter": "Alejandro Cartas", "authors": "Alejandro Cartas, Jordi Luque, Petia Radeva, Carlos Segura, Mariella\n  Dimiccoli", "title": "How Much Does Audio Matter to Recognize Egocentric Object Interactions?", "comments": "Accepted for presentation at EPIC@CVPR2019 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Sounds are an important source of information on our daily interactions with\nobjects. For instance, a significant amount of people can discern the\ntemperature of water that it is being poured just by using the sense of\nhearing. However, only a few works have explored the use of audio for the\nclassification of object interactions in conjunction with vision or as single\nmodality. In this preliminary work, we propose an audio model for egocentric\naction recognition and explore its usefulness on the parts of the problem\n(noun, verb, and action classification). Our model achieves a competitive\nresult in terms of verb classification (34.26% accuracy) on a standard\nbenchmark with respect to vision-based state of the art systems, using a\ncomparatively lighter architecture.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 08:40:49 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Cartas", "Alejandro", ""], ["Luque", "Jordi", ""], ["Radeva", "Petia", ""], ["Segura", "Carlos", ""], ["Dimiccoli", "Mariella", ""]]}, {"id": "1906.00649", "submitter": "Thibaud Ehret", "authors": "Thibaud Ehret", "title": "Robust copy-move forgery detection by false alarms control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting reliably copy-move forgeries is difficult because images do contain\nsimilar objects. The question is: how to discard natural image\nself-similarities while still detecting copy-moved parts as being \"unnaturally\nsimilar\"? Copy-move may have been performed after a rotation, a change of scale\nand followed by JPEG compression or the addition of noise. For this reason, we\nbase our method on SIFT, which provides sparse keypoints with scale, rotation\nand illumination invariant descriptors. To discriminate natural descriptor\nmatches from artificial ones, we introduce an a contrario method which gives\ntheoretical guarantees on the number of false alarms. We validate our method on\nseveral databases. Being fully unsupervised it can be integrated into any\ngeneric automated image tampering detection pipeline.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 09:09:48 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Ehret", "Thibaud", ""]]}, {"id": "1906.00651", "submitter": "Alexander Krull", "authors": "Alexander Krull, Tomas Vicar, Florian Jug", "title": "Probabilistic Noise2Void: Unsupervised Content-Aware Denoising", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": "10.3389/fcomp.2020.00005", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, Convolutional Neural Networks (CNNs) are the leading method for image\ndenoising. They are traditionally trained on pairs of images, which are often\nhard to obtain for practical applications. This motivates self-supervised\ntraining methods such as Noise2Void~(N2V) that operate on single noisy images.\nSelf-supervised methods are, unfortunately, not competitive with models trained\non image pairs. Here, we present 'Probabilistic Noise2Void' (PN2V), a method to\ntrain CNNs to predict per-pixel intensity distributions. Combining these with a\nsuitable description of the noise, we obtain a complete probabilistic model for\nthe noisy observations and true signal in every pixel. We evaluate PN2V on\npublicly available microscopy datasets, under a broad range of noise regimes,\nand achieve competitive results with respect to supervised state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 09:13:52 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 08:25:30 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Krull", "Alexander", ""], ["Vicar", "Tomas", ""], ["Jug", "Florian", ""]]}, {"id": "1906.00668", "submitter": "Anbang Yao", "authors": "Ming Lu, Hao Zhao, Anbang Yao, Yurong Chen, Feng Xu, Li Zhang", "title": "A Closed-form Solution to Universal Style Transfer", "comments": "Accepted to ICCV 2019. Code is available at\n  https://github.com/lu-m13/OptimalStyleTransfer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Universal style transfer tries to explicitly minimize the losses in feature\nspace, thus it does not require training on any pre-defined styles. It usually\nuses different layers of VGG network as the encoders and trains several\ndecoders to invert the features into images. Therefore, the effect of style\ntransfer is achieved by feature transform. Although plenty of methods have been\nproposed, a theoretical analysis of feature transform is still missing. In this\npaper, we first propose a novel interpretation by treating it as the optimal\ntransport problem. Then, we demonstrate the relations of our formulation with\nformer works like Adaptive Instance Normalization (AdaIN) and Whitening and\nColoring Transform (WCT). Finally, we derive a closed-form solution named\nOptimal Style Transfer (OST) under our formulation by additionally considering\nthe content loss of Gatys. Comparatively, our solution can preserve better\nstructure and achieve visually pleasing results. It is simple yet effective and\nwe demonstrate its advantages both quantitatively and qualitatively. Besides,\nwe hope our theoretical analysis can inspire future works in neural style\ntransfer. Code is available at https://github.com/lu-m13/OptimalStyleTransfer.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 09:48:14 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 02:47:49 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Lu", "Ming", ""], ["Zhao", "Hao", ""], ["Yao", "Anbang", ""], ["Chen", "Yurong", ""], ["Xu", "Feng", ""], ["Zhang", "Li", ""]]}, {"id": "1906.00675", "submitter": "Anbang Yao", "authors": "Dawei Sun, Anbang Yao, Aojun Zhou, Hao Zhao", "title": "Deeply-supervised Knowledge Synergy", "comments": "Added supplementary materials, and the code is available at\n  https://github.com/sundw2014/DKS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have become deeper and more complicated\ncompared with the pioneering AlexNet. However, current prevailing training\nscheme follows the previous way of adding supervision to the last layer of the\nnetwork only and propagating error information up layer-by-layer. In this\npaper, we propose Deeply-supervised Knowledge Synergy (DKS), a new method\naiming to train CNNs with improved generalization ability for image\nclassification tasks without introducing extra computational cost during\ninference. Inspired by the deeply-supervised learning scheme, we first append\nauxiliary supervision branches on top of certain intermediate network layers.\nWhile properly using auxiliary supervision can improve model accuracy to some\ndegree, we go one step further to explore the possibility of utilizing the\nprobabilistic knowledge dynamically learnt by the classifiers connected to the\nbackbone network as a new regularization to improve the training. A novel\nsynergy loss, which considers pairwise knowledge matching among all supervision\nbranches, is presented. Intriguingly, it enables dense pairwise knowledge\nmatching operations in both top-down and bottom-up directions at each training\niteration, resembling a dynamic synergy process for the same task. We evaluate\nDKS on image classification datasets using state-of-the-art CNN architectures,\nand show that the models trained with it are consistently better than the\ncorresponding counterparts. For instance, on the ImageNet classification\nbenchmark, our ResNet-152 model outperforms the baseline model with a 1.47%\nmargin in Top-1 accuracy. Code is available at\nhttps://github.com/sundw2014/DKS.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 09:54:26 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 02:45:53 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Sun", "Dawei", ""], ["Yao", "Anbang", ""], ["Zhou", "Aojun", ""], ["Zhao", "Hao", ""]]}, {"id": "1906.00705", "submitter": "Ananda Chowdhury", "authors": "Arindam Sikdar and Ananda S. Chowdhury", "title": "An Adaptive Training-less System for Anomaly Detection in Crowd Scenes", "comments": "29 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection in crowd videos has become a popular area of research for\nthe computer vision community. Several existing methods generally perform a\nprior training about the scene with or without the use of labeled data.\nHowever, it is difficult to always guarantee the availability of prior data,\nespecially, for scenarios like remote area surveillance. To address such\nchallenge, we propose an adaptive training-less system capable of detecting\nanomaly on-the-fly while dynamically estimating and adjusting response based on\ncertain parameters. This makes our system both training-less and adaptive in\nnature. Our pipeline consists of three main components, namely, adaptive 3D-DCT\nmodel for multi-object detection-based association, local motion structure\ndescription through saliency modulated optic flow, and anomaly detection based\non earth movers distance (EMD). The proposed model, despite being\ntraining-free, is found to achieve comparable performance with several\nstate-of-the-art methods on the publicly available UCSD, UMN, CHUK-Avenue and\nShanghaiTech datasets.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 11:04:20 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Sikdar", "Arindam", ""], ["Chowdhury", "Ananda S.", ""]]}, {"id": "1906.00709", "submitter": "Min-Cheol Sagong", "authors": "Min-Cheol Sagong, Yong-Goo Shin, Yoon-Jae Yeo, Seung Park, Sung-Jea Ko", "title": "cGANs with Conditional Convolution Layer", "comments": "Submitted to IEEE Trans. Neural Networks and Learning Systems (TNNLS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional generative adversarial networks (cGANs) have been widely\nresearched to generate class conditional images using a single generator.\nHowever, in the conventional cGANs techniques, it is still challenging for the\ngenerator to learn condition-specific features, since a standard convolutional\nlayer with the same weights is used regardless of the condition. In this paper,\nwe propose a novel convolution layer, called the conditional convolution layer,\nwhich directly generates different feature maps by employing the weights which\nare adjusted depending on the conditions. More specifically, in each\nconditional convolution layer, the weights are conditioned in a simple but\neffective way through filter-wise scaling and channel-wise shifting operations.\nIn contrast to the conventional methods, the proposed method with a single\ngenerator can effectively handle condition-specific characteristics. The\nexperimental results on CIFAR, LSUN and ImageNet datasets show that the\ngenerator with the proposed conditional convolution layer achieves a higher\nquality of conditional image generation than that with the standard convolution\nlayer.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 11:15:51 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 10:09:15 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Sagong", "Min-Cheol", ""], ["Shin", "Yong-Goo", ""], ["Yeo", "Yoon-Jae", ""], ["Park", "Seung", ""], ["Ko", "Sung-Jea", ""]]}, {"id": "1906.00717", "submitter": "Junlong Gao", "authors": "Junlong Gao, Xi Meng, Shiqi Wang, Xia Li, Shanshe Wang, Siwei Ma, Wen\n  Gao", "title": "Masked Non-Autoregressive Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing captioning models often adopt the encoder-decoder architecture,\nwhere the decoder uses autoregressive decoding to generate captions, such that\neach token is generated sequentially given the preceding generated tokens.\nHowever, autoregressive decoding results in issues such as sequential error\naccumulation, slow generation, improper semantics and lack of diversity.\nNon-autoregressive decoding has been proposed to tackle slow generation for\nneural machine translation but suffers from multimodality problem due to the\nindirect modeling of the target distribution. In this paper, we propose masked\nnon-autoregressive decoding to tackle the issues of both autoregressive\ndecoding and non-autoregressive decoding. In masked non-autoregressive\ndecoding, we mask several kinds of ratios of the input sequences during\ntraining, and generate captions parallelly in several stages from a totally\nmasked sequence to a totally non-masked sequence in a compositional manner\nduring inference. Experimentally our proposed model can preserve semantic\ncontent more effectively and can generate more diverse captions.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 11:34:41 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Gao", "Junlong", ""], ["Meng", "Xi", ""], ["Wang", "Shiqi", ""], ["Li", "Xia", ""], ["Wang", "Shanshe", ""], ["Ma", "Siwei", ""], ["Gao", "Wen", ""]]}, {"id": "1906.00734", "submitter": "Yunfei Liu", "authors": "Yunfei Liu and Feng Lu", "title": "Separate In Latent Space: Unsupervised Single Image Layer Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real world vision tasks, such as reflection removal from a transparent\nsurface and intrinsic image decomposition, can be modeled as single image layer\nseparation. However, this problem is highly ill-posed, requiring accurately\naligned and hard to collect triplet data to train the CNN models. To address\nthis problem, this paper proposes an unsupervised method that requires no\nground truth data triplet in training. At the core of the method are two\nassumptions about data distributions in the latent spaces of different layers,\nbased on which a novel unsupervised layer separation pipeline can be derived.\nThen the method can be constructed based on the GANs framework with\nself-supervision and cycle consistency constraints, etc. Experimental results\ndemonstrate its successfulness in outperforming existing unsupervised methods\nin both synthetic and real world tasks. The method also shows its ability to\nsolve a more challenging multi-layer separation task.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 12:17:07 GMT"}, {"version": "v2", "created": "Sat, 27 Jul 2019 13:52:29 GMT"}, {"version": "v3", "created": "Sat, 7 Sep 2019 07:56:52 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Liu", "Yunfei", ""], ["Lu", "Feng", ""]]}, {"id": "1906.00768", "submitter": "Ophir Gozes", "authors": "Ophir Gozes, Hayit Greenspan", "title": "Deep Feature Learning from a Hospital-Scale Chest X-ray Dataset with\n  Application to TB Detection on a Small-Scale Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of ImageNet pre-trained networks is becoming widespread in the\nmedical imaging community. It enables training on small datasets, commonly\navailable in medical imaging tasks. The recent emergence of a large Chest X-ray\ndataset opened the possibility for learning features that are specific to the\nX-ray analysis task. In this work, we demonstrate that the features learned\nallow for better classification results for the problem of Tuberculosis\ndetection and enable generalization to an unseen dataset. To accomplish the\ntask of feature learning, we train a DenseNet-121 CNN on 112K images from the\nChestXray14 dataset which includes labels of 14 common thoracic pathologies. In\naddition to the pathology labels, we incorporate metadata which is available in\nthe dataset: Patient Positioning, Gender and Patient Age. We term this\narchitecture MetaChexNet. As a by-product of the feature learning, we\ndemonstrate state of the art performance on the task of patient Age \\& Gender\nestimation using CNN's. Finally, we show the features learned using ChestXray14\nallow for better transfer learning on small-scale datasets for Tuberculosis.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 13:03:15 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Gozes", "Ophir", ""], ["Greenspan", "Hayit", ""]]}, {"id": "1906.00786", "submitter": "Subrahmanyam Vaddi", "authors": "Subrahmanyam Vaddi, Chandan Kumar, Ali Jannesari", "title": "Efficient Object Detection Model for Real-Time UAV Applications", "comments": "10 pages, 4 figures, Under Review. arXiv admin note: substantial text\n  overlap with arXiv:1808.07256 by other authors without attribution;\n  substantial text overlap with arXiv:1807.06789, arXiv:1612.03144,\n  arXiv:1809.03193 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmanned Aerial Vehicles (UAVs) especially drones, equipped with vision\ntechniques have become very popular in recent years, with their extensive use\nin wide range of applications. Many of these applications require use of\ncomputer vision techniques, particularly object detection from the information\ncaptured by on-board camera. In this paper, we propose an end to end object\ndetection model running on a UAV platform which is suitable for real-time\napplications. We propose a deep feature pyramid architecture which makes use of\ninherent properties of features extracted from Convolutional Networks by\ncapturing more generic features in the images (such as edge, color etc.) along\nwith the minute detailed features specific to the classes contained in our\nproblem. We use VisDrone-18 dataset for our studies which contain different\nobjects such as pedestrians, vehicles, bicycles etc. We provide software and\nhardware architecture of our platform used in this study. We implemented our\nmodel with both ResNet and MobileNet as convolutional bases. Our model combined\nwith modified focal loss function, produced a desirable performance of 30.6 mAP\nfor object detection with an inference time of 14 fps. We compared our results\nwith RetinaNet-ResNet-50 and HAL-RetinaNet and shown that our model combined\nwith MobileNet as backend feature extractor gave the best results in terms of\naccuracy, speed and memory efficiency and is best suitable for real time object\ndetection with drones.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 20:24:13 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Vaddi", "Subrahmanyam", ""], ["Kumar", "Chandan", ""], ["Jannesari", "Ali", ""]]}, {"id": "1906.00804", "submitter": "Thomas Robert", "authors": "Thomas Robert, Nicolas Thome, Matthieu Cord", "title": "DualDis: Dual-Branch Disentangling with Adversarial Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision, disentangling techniques aim at improving latent\nrepresentations of images by modeling factors of variation. In this paper, we\npropose DualDis, a new auto-encoder-based framework that disentangles and\nlinearizes class and attribute information. This is achieved thanks to a\ntwo-branch architecture forcing the separation of the two kinds of information,\naccompanied by a decoder for image reconstruction and generation. To\neffectively separate the information, we propose to use a combination of\nregular and adversarial classifiers to guide the two branches in specializing\nfor class and attribute information respectively. We also investigate the\npossibility of using semi-supervised learning for an effective disentangling\neven using few labels. We leverage the linearization property of the latent\nspaces for semantic image editing and generation of new images. We validate our\napproach on CelebA, Yale-B and NORB by measuring the efficiency of information\nseparation via classification metrics, visual image manipulation and data\naugmentation.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 13:42:59 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Robert", "Thomas", ""], ["Thome", "Nicolas", ""], ["Cord", "Matthieu", ""]]}, {"id": "1906.00805", "submitter": "Xueying Qin", "authors": "Jichao Zhang, Meng Sun, Jingjing Chen, Hao Tang, Yan Yan, Xueying Qin,\n  Nicu Sebe", "title": "GazeCorrection:Self-Guided Eye Manipulation in the wild using\n  Self-Supervised Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaze correction aims to redirect the person's gaze into the camera by\nmanipulating the eye region, and it can be considered as a specific image\nresynthesis problem. Gaze correction has a wide range of applications in real\nlife, such as taking a picture with staring at the camera. In this paper, we\npropose a novel method that is based on the inpainting model to learn from the\nface image to fill in the missing eye regions with new contents representing\ncorrected eye gaze. Moreover, our model does not require the training dataset\nlabeled with the specific head pose and eye angle information, thus, the\ntraining data is easy to collect. To retain the identity information of the eye\nregion in the original input, we propose a self-guided pretrained model to\nlearn the angle-invariance feature. Experiments show our model achieves very\ncompelling gaze-corrected results in the wild dataset which is collected from\nthe website and will be introduced in details. Code is available at\nhttps://github.com/zhangqianhui/GazeCorrection.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 13:43:01 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Zhang", "Jichao", ""], ["Sun", "Meng", ""], ["Chen", "Jingjing", ""], ["Tang", "Hao", ""], ["Yan", "Yan", ""], ["Qin", "Xueying", ""], ["Sebe", "Nicu", ""]]}, {"id": "1906.00817", "submitter": "Maxime Bucher", "authors": "Maxime Bucher, Tuan-Hung Vu, Matthieu Cord, Patrick P\\'erez", "title": "Zero-Shot Semantic Segmentation", "comments": "NeurIPS 2019 (accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation models are limited in their ability to scale to large\nnumbers of object classes. In this paper, we introduce the new task of\nzero-shot semantic segmentation: learning pixel-wise classifiers for never-seen\nobject categories with zero training examples. To this end, we present a novel\narchitecture, ZS3Net, combining a deep visual segmentation model with an\napproach to generate visual representations from semantic word embeddings. By\nthis way, ZS3Net addresses pixel classification tasks where both seen and\nunseen categories are faced at test time (so called \"generalized\" zero-shot\nclassification). Performance is further improved by a self-training step that\nrelies on automatic pseudo-labeling of pixels from unseen classes. On the two\nstandard segmentation datasets, Pascal-VOC and Pascal-Context, we propose\nzero-shot benchmarks and set competitive baselines. For complex scenes as ones\nin the Pascal-Context dataset, we extend our approach by using a graph-context\nencoding to fully leverage spatial context priors coming from class-wise\nsegmentation maps.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 13:53:00 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 11:10:40 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Bucher", "Maxime", ""], ["Vu", "Tuan-Hung", ""], ["Cord", "Matthieu", ""], ["P\u00e9rez", "Patrick", ""]]}, {"id": "1906.00825", "submitter": "Alban Laflaqui\\`ere Dr", "authors": "Alban Laflaqui\\`ere, Verena V. Hafner", "title": "Self-supervised Body Image Acquisition Using a Deep Neural Network for\n  Sensorimotor Prediction", "comments": "6 pages, 7 figures, submitted to ICDL-Epirob 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates how a naive agent can acquire its own body image in a\nself-supervised way, based on the predictability of its sensorimotor\nexperience. Our working hypothesis is that, due to its temporal stability, an\nagent's body produces more consistent sensory experiences than the environment,\nwhich exhibits a greater variability. Given its motor experience, an agent can\nthus reliably predict what appearance its body should have. This intrinsic\npredictability can be used to automatically isolate the body image from the\nrest of the environment. We propose a two-branches deconvolutional neural\nnetwork to predict the visual sensory state associated with an input motor\nstate, as well as the prediction error associated with this input. We train the\nnetwork on a dataset of first-person images collected with a simulated Pepper\nrobot, and show how the network outputs can be used to automatically isolate\nits visible arm from the rest of the environment. Finally, the quality of the\nbody image produced by the network is evaluated.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 14:10:17 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Laflaqui\u00e8re", "Alban", ""], ["Hafner", "Verena V.", ""]]}, {"id": "1906.00872", "submitter": "Shizhe Chen", "authors": "Shizhe Chen, Qin Jin and Jianlong Fu", "title": "From Words to Sentences: A Progressive Learning Approach for\n  Zero-resource Machine Translation with Visual Pivots", "comments": "Accepted by IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The neural machine translation model has suffered from the lack of\nlarge-scale parallel corpora. In contrast, we humans can learn multi-lingual\ntranslations even without parallel texts by referring our languages to the\nexternal world. To mimic such human learning behavior, we employ images as\npivots to enable zero-resource translation learning. However, a picture tells a\nthousand words, which makes multi-lingual sentences pivoted by the same image\nnoisy as mutual translations and thus hinders the translation model learning.\nIn this work, we propose a progressive learning approach for image-pivoted\nzero-resource machine translation. Since words are less diverse when grounded\nin the image, we first learn word-level translation with image pivots, and then\nprogress to learn the sentence-level translation by utilizing the learned word\ntranslation to suppress noises in image-pivoted multi-lingual sentences.\nExperimental results on two widely used image-pivot translation datasets,\nIAPR-TC12 and Multi30k, show that the proposed approach significantly\noutperforms other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 15:28:48 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Chen", "Shizhe", ""], ["Jin", "Qin", ""], ["Fu", "Jianlong", ""]]}, {"id": "1906.00882", "submitter": "Wenqi Lu", "authors": "Wenqi Lu, Jinming Duan, Joshua Deepak Veesa, Iain B. Styles", "title": "A new nonlocal forward model for diffuse optical tomography", "comments": "7 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The forward model in diffuse optical tomography (DOT) describes how light\npropagates through a turbid medium. It is often approximated by a diffusion\nequation (DE) that is numerically discretized by the classical finite element\nmethod (FEM). We propose a nonlocal diffusion equation (NDE) as a new forward\nmodel for DOT, the discretization of which is carried out with an efficient\ngraph-based numerical method (GNM). To quantitatively evaluate the new forward\nmodel, we first conduct experiments on a homogeneous slab, where the numerical\naccuracy of both NDE and DE is compared against the existing analytical\nsolution. We further evaluate NDE by comparing its image reconstruction\nperformance (inverse problem) to that of DE. Our experiments show that NDE is\nquantitatively comparable to DE and is up to 64% faster due to the efficient\ngraph-based representation that can be implemented identically for geometries\nin different dimensions.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 15:40:51 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Lu", "Wenqi", ""], ["Duan", "Jinming", ""], ["Veesa", "Joshua Deepak", ""], ["Styles", "Iain B.", ""]]}, {"id": "1906.00884", "submitter": "Haoye Dong", "authors": "Haoye Dong, Xiaodan Liang, Yixuan Zhang, Xujie Zhang, Zhenyu Xie,\n  Bowen Wu, Ziqi Zhang, Xiaohui Shen, Jian Yin", "title": "Fashion Editing with Adversarial Parsing Learning", "comments": "22 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive fashion image manipulation, which enables users to edit images\nwith sketches and color strokes, is an interesting research problem with great\napplication value. Existing works often treat it as a general inpainting task\nand do not fully leverage the semantic structural information in fashion\nimages. Moreover, they directly utilize conventional convolution and\nnormalization layers to restore the incomplete image, which tends to wash away\nthe sketch and color information. In this paper, we propose a novel Fashion\nEditing Generative Adversarial Network (FE-GAN), which is capable of\nmanipulating fashion images by free-form sketches and sparse color strokes.\nFE-GAN consists of two modules: 1) a free-form parsing network that learns to\ncontrol the human parsing generation by manipulating sketch and color; 2) a\nparsing-aware inpainting network that renders detailed textures with semantic\nguidance from the human parsing map. A new attention normalization layer is\nfurther applied at multiple scales in the decoder of the inpainting network to\nenhance the quality of the synthesized image. Extensive experiments on\nhigh-resolution fashion image datasets demonstrate that the proposed method\nsignificantly outperforms the state-of-the-art methods on image manipulation.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 15:43:33 GMT"}, {"version": "v2", "created": "Sat, 28 Sep 2019 16:47:46 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Dong", "Haoye", ""], ["Liang", "Xiaodan", ""], ["Zhang", "Yixuan", ""], ["Zhang", "Xujie", ""], ["Xie", "Zhenyu", ""], ["Wu", "Bowen", ""], ["Zhang", "Ziqi", ""], ["Shen", "Xiaohui", ""], ["Yin", "Jian", ""]]}, {"id": "1906.00891", "submitter": "Benzhang Qiu", "authors": "Zhun Fan, Jiewei Lu, Benzhang Qiu, Tao Jiang, Kang An, Alex Noel\n  Josephraj, and Chuliang Wei", "title": "Automated Steel Bar Counting and Center Localization with Convolutional\n  Neural Networks", "comments": "Ready to submit IEEE Transactions on Industrial Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated steel bar counting and center localization plays an important role\nin the factory automation of steel bars. Traditional methods only focus on\nsteel bar counting and their performances are often limited by complex\nindustrial environments. Convolutional neural network (CNN), which has great\ncapability to deal with complex tasks in challenging environments, is applied\nin this work. A framework called CNN-DC is proposed to achieve automated steel\nbar counting and center localization simultaneously. The proposed framework\nCNN-DC first detects the candidate center points with a deep CNN. Then an\neffective clustering algorithm named as Distance Clustering(DC) is proposed to\ncluster the candidate center points and locate the true centers of steel bars.\nThe proposed CNN-DC can achieve 99.26% accuracy for steel bar counting and 4.1%\ncenter offset for center localization on the established steel bar dataset,\nwhich demonstrates that the proposed CNN-DC can perform well on automated steel\nbar counting and center localization. Code is made publicly available at:\nhttps://github.com/BenzhangQiu/Steel-bar-Detection.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 15:49:27 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 08:02:47 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Fan", "Zhun", ""], ["Lu", "Jiewei", ""], ["Qiu", "Benzhang", ""], ["Jiang", "Tao", ""], ["An", "Kang", ""], ["Josephraj", "Alex Noel", ""], ["Wei", "Chuliang", ""]]}, {"id": "1906.00901", "submitter": "Chenyang Zhang", "authors": "Chenyang Zhang, Christine Kaeser-Chen, Grace Vesom, Jennie Choi, Maria\n  Kessler, and Serge Belongie", "title": "The iMet Collection 2019 Challenge Dataset", "comments": "3 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing computer vision technologies in artwork recognition focus mainly on\ninstance retrieval or coarse-grained attribute classification. In this work, we\npresent a novel dataset for fine-grained artwork attribute recognition. The\nimages in the dataset are professional photographs of classic artworks from the\nMetropolitan Museum of Art, and annotations are curated and verified by\nworld-class museum experts. In addition, we also present the iMet Collection\n2019 Challenge as part of the FGVC6 workshop. Through the competition, we aim\nto spur the enthusiasm of the fine-grained visual recognition research\ncommunity and advance the state-of-the-art in digital curation of museum\ncollections.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 16:10:14 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 02:37:29 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Zhang", "Chenyang", ""], ["Kaeser-Chen", "Christine", ""], ["Vesom", "Grace", ""], ["Choi", "Jennie", ""], ["Kessler", "Maria", ""], ["Belongie", "Serge", ""]]}, {"id": "1906.00925", "submitter": "Yawei Li", "authors": "Yawei Li, Vagia Tsiminaki, Radu Timofte, Marc Pollefeys, Luc van Gool", "title": "3D Appearance Super-Resolution with Deep Learning", "comments": "In CVPR 2019. Github papge:\n  https://github.com/ofsoundof/3D_Appearance_SR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of retrieving high-resolution (HR) texture maps of\nobjects that are captured from multiple view points. In the multi-view case,\nmodel-based super-resolution (SR) methods have been recently proved to recover\nhigh quality texture maps. On the other hand, the advent of deep learning-based\nmethods has already a significant impact on the problem of video and image SR.\nYet, a deep learning-based approach to super-resolve the appearance of 3D\nobjects is still missing. The main limitation of exploiting the power of deep\nlearning techniques in the multi-view case is the lack of data. We introduce a\n3D appearance SR (3DASR) dataset based on the existing ETH3D [42], SyB3R [31],\nMiddleBury, and our Collection of 3D scenes from TUM [21], Fountain [51] and\nRelief [53]. We provide the high- and low-resolution texture maps, the 3D\ngeometric model, images and projection matrices. We exploit the power of 2D\nlearning-based SR methods and design networks suitable for the 3D multi-view\ncase. We incorporate the geometric information by introducing normal maps and\nfurther improve the learning process. Experimental results demonstrate that our\nproposed networks successfully incorporate the 3D geometric information and\nsuper-resolve the texture maps.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 16:51:35 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 12:52:53 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Li", "Yawei", ""], ["Tsiminaki", "Vagia", ""], ["Timofte", "Radu", ""], ["Pollefeys", "Marc", ""], ["van Gool", "Luc", ""]]}, {"id": "1906.00932", "submitter": "Miguel Alonso Jr", "authors": "Miguel Alonso Jr", "title": "Y-GAN: A Generative Adversarial Network for Depthmap Estimation from\n  Multi-camera Stereo Images", "comments": "Accepted for Presentation at the ICML 2019 LatinX in AI Research\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth perception is a key component for autonomous systems that interact in\nthe real world, such as delivery robots, warehouse robots, and self-driving\ncars. Tasks in autonomous robotics such as 3D object recognition, simultaneous\nlocalization and mapping (SLAM), path planning and navigation, require some\nform of 3D spatial information. Depth perception is a long-standing research\nproblem in computer vision and robotics and has had a long history. Many\napproaches using deep learning, ranging from structure from motion,\nshape-from-X, monocular, binocular, and multi-view stereo, have yielded\nacceptable results. However, there are several shortcomings of these methods\nsuch as requiring expensive hardware, needing supervised training data, no\nground truth data for comparison, and disregard for occlusion. In order to\naddress these shortcomings, this work proposes a new deep convolutional\ngenerative adversarial network architecture, called Y-GAN, that uses data from\nthree cameras to estimate a depth map for each frame in a multi-camera video\nstream.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 17:11:18 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Alonso", "Miguel", "Jr"]]}, {"id": "1906.00938", "submitter": "Feiyu Chen", "authors": "Feiyu Chen, Yuchen Yang, Liwei Xu, Taiping Zhang, Yin Zhang", "title": "Big-Data Clustering: K-Means or K-Indicators?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The K-means algorithm is arguably the most popular data clustering method,\ncommonly applied to processed datasets in some \"feature spaces\", as is in\nspectral clustering. Highly sensitive to initializations, however, K-means\nencounters a scalability bottleneck with respect to the number of clusters K as\nthis number grows in big data applications. In this work, we promote a closely\nrelated model called K-indicators model and construct an efficient,\nsemi-convex-relaxation algorithm that requires no randomized initializations.\nWe present extensive empirical results to show advantages of the new algorithm\nwhen K is large. In particular, using the new algorithm to start the K-means\nalgorithm, without any replication, can significantly outperform the standard\nK-means with a large number of currently state-of-the-art random replications.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 17:30:24 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Chen", "Feiyu", ""], ["Yang", "Yuchen", ""], ["Xu", "Liwei", ""], ["Zhang", "Taiping", ""], ["Zhang", "Yin", ""]]}, {"id": "1906.00945", "submitter": "Dimitris Tsipras", "authors": "Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras,\n  Brandon Tran, Aleksander Madry", "title": "Adversarial Robustness as a Prior for Learned Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important goal in deep learning is to learn versatile, high-level feature\nrepresentations of input data. However, standard networks' representations seem\nto possess shortcomings that, as we illustrate, prevent them from fully\nrealizing this goal. In this work, we show that robust optimization can be\nre-cast as a tool for enforcing priors on the features learned by deep neural\nnetworks. It turns out that representations learned by robust models address\nthe aforementioned shortcomings and make significant progress towards learning\na high-level encoding of inputs. In particular, these representations are\napproximately invertible, while allowing for direct visualization and\nmanipulation of salient input features. More broadly, our results indicate\nadversarial robustness as a promising avenue for improving learned\nrepresentations. Our code and models for reproducing these results is available\nat https://git.io/robust-reps .\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 17:55:20 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 17:39:54 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Engstrom", "Logan", ""], ["Ilyas", "Andrew", ""], ["Santurkar", "Shibani", ""], ["Tsipras", "Dimitris", ""], ["Tran", "Brandon", ""], ["Madry", "Aleksander", ""]]}, {"id": "1906.01003", "submitter": "Zsolt Levente Kucsvan", "authors": "Zsolt Levente Kucsv\\'an", "title": "Comparing two- and three-view Computer Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To reconstruct the points in three dimensional space, we need at least two\nimages. In this paper we compared two different methods: the first uses only\ntwo images, the second one uses three. During the research we measured how\ncamera resolution, camera angles and camera distances influence the number of\nreconstructed points and the dispersion of them. The paper presents that using\nthe two-view method, we can reconstruct significantly more points than using\nthe other one, but the dispersion of points is smaller if we use the three-view\nmethod. Taking into consideration the different camera settings, we can say\nthat both the two- and three-view method behaves the same, and the best\nparameters are also the same for both methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 18:11:52 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Kucsv\u00e1n", "Zsolt Levente", ""]]}, {"id": "1906.01004", "submitter": "Yan Zhang", "authors": "Yan Zhang, Krikamol Muandet, Qianli Ma, Heiko Neumann and Siyu Tang", "title": "Frontal Low-rank Random Tensors for Fine-grained Action Segmentation", "comments": "19 pages (4 pages appendix), 3 figures. Revised theories and models,\n  new experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained action segmentation in long untrimmed videos is an important\ntask for many applications such as surveillance, robotics, and human-computer\ninteraction. To understand subtle and precise actions within a long time\nperiod, second-order information (e.g. feature covariance) or higher is\nreported to be effective in the literature. However, extracting such high-order\ninformation is considerably non-trivial. In particular, the dimensionality\nincreases exponentially with the information order, and hence gaining more\nrepresentation power also increases the computational cost and the risk of\noverfitting. In this paper, we propose an approach to representing high-order\ninformation for temporal action segmentation via a simple yet effective\nbilinear form. Specifically, our contributions are: (1) From the multilinear\nperspective, we derive a bilinear form of low complexity, assuming that the\nthree-way tensor has low-rank frontal slices. (2) Rather than learning the\ntensor entries from data, we sample the entries from different underlying\ndistributions, and prove that the underlying distribution influences the\ninformation order. (3) We employed our bilinear form as an intermediate layer\nin state-of-the-art deep neural networks, enabling to represent high-order\ninformation in complex deep models effectively and efficiently. Our\nexperimental results demonstrate that the proposed bilinear form outperforms\nthe previous state-of-the-art methods on the challenging temporal action\nsegmentation task. One can see our project page for data, model and code:\n\\url{https://vlg.inf.ethz.ch/projects/BilinearTCN/}.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 18:12:26 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 14:42:57 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Zhang", "Yan", ""], ["Muandet", "Krikamol", ""], ["Ma", "Qianli", ""], ["Neumann", "Heiko", ""], ["Tang", "Siyu", ""]]}, {"id": "1906.01012", "submitter": "Hilde Kuehne", "authors": "Hilde Kuehne, Ahsan Iqbal, Alexander Richard, Juergen Gall", "title": "Mining YouTube - A dataset for learning fine-grained action concepts\n  from webly supervised video data", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition is so far mainly focusing on the problem of classification\nof hand selected preclipped actions and reaching impressive results in this\nfield. But with the performance even ceiling on current datasets, it also\nappears that the next steps in the field will have to go beyond this fully\nsupervised classification. One way to overcome those problems is to move\ntowards less restricted scenarios. In this context we present a large-scale\nreal-world dataset designed to evaluate learning techniques for human action\nrecognition beyond hand-crafted datasets. To this end we put the process of\ncollecting data on its feet again and start with the annotation of a test set\nof 250 cooking videos. The training data is then gathered by searching for the\nrespective annotated classes within the subtitles of freely available videos.\nThe uniqueness of the dataset is attributed to the fact that the whole process\nof collecting the data and training does not involve any human intervention. To\naddress the problem of semantic inconsistencies that arise with this kind of\ntraining data, we further propose a semantical hierarchical structure for the\nmined classes.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 18:18:01 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Kuehne", "Hilde", ""], ["Iqbal", "Ahsan", ""], ["Richard", "Alexander", ""], ["Gall", "Juergen", ""]]}, {"id": "1906.01028", "submitter": "Hilde Kuehne", "authors": "Hilde Kuehne, Alexander Richard, Juergen Gall", "title": "A Hybrid RNN-HMM Approach for Weakly Supervised Temporal Action\n  Segmentation", "comments": "15 pages, preprint for IEEE TPAMI\n  https://ieeexplore.ieee.org/document/8585084 (open access). arXiv admin note:\n  substantial text overlap with arXiv:1703.08132", "journal-ref": null, "doi": "10.1109/TPAMI.2018.2884469", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition has become a rapidly developing research field within the\nlast decade. But with the increasing demand for large scale data, the need of\nhand annotated data for the training becomes more and more impractical. One way\nto avoid frame-based human annotation is the use of action order information to\nlearn the respective action classes. In this context, we propose a hierarchical\napproach to address the problem of weakly supervised learning of human actions\nfrom ordered action labels by structuring recognition in a coarse-to-fine\nmanner. Given a set of videos and an ordered list of the occurring actions, the\ntask is to infer start and end frames of the related action classes within the\nvideo and to train the respective action classifiers without any need for hand\nlabeled frame boundaries. We address this problem by combining a framewise RNN\nmodel with a coarse probabilistic inference. This combination allows for the\ntemporal alignment of long sequences and thus, for an iterative training of\nboth elements. While this system alone already generates good results, we show\nthat the performance can be further improved by approximating the number of\nsubactions to the characteristics of the different action classes as well as by\nthe introduction of a regularizing length prior. The proposed system is\nevaluated on two benchmark datasets, the Breakfast and the Hollywood extended\ndataset, showing a competitive performance on various weak learning tasks such\nas temporal action segmentation and action alignment.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 19:12:29 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Kuehne", "Hilde", ""], ["Richard", "Alexander", ""], ["Gall", "Juergen", ""]]}, {"id": "1906.01030", "submitter": "Yichen Yang", "authors": "Yichen Yang, Martin Rinard", "title": "Correctness Verification of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first verification that a neural network produces a correct\noutput within a specified tolerance for every input of interest. We define\ncorrectness relative to a specification which identifies 1) a state space\nconsisting of all relevant states of the world and 2) an observation process\nthat produces neural network inputs from the states of the world. Tiling the\nstate and input spaces with a finite number of tiles, obtaining ground truth\nbounds from the state tiles and network output bounds from the input tiles,\nthen comparing the ground truth and network output bounds delivers an upper\nbound on the network output error for any input of interest. Results from a\ncase study highlight the ability of our technique to deliver tight error bounds\nfor all inputs of interest and show how the error bounds vary over the state\nand input spaces.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 19:13:24 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 16:03:29 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Yang", "Yichen", ""], ["Rinard", "Martin", ""]]}, {"id": "1906.01049", "submitter": "Sahar Zafari", "authors": "Sahar Zafari, Mariia Murashkina, Tuomas Eerola, Jouni Sampo, Heikki\n  K\\\"alvi\\\"ainen, Heikki Haario", "title": "Resolving Overlapping Convex Objects in Silhouette Images by Concavity\n  Analysis and Gaussian Process", "comments": "16 pages, 11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Segmentation of overlapping convex objects has various applications, for\nexample, in nanoparticles and cell imaging. Often the segmentation method has\nto rely purely on edges between the background and foreground making the\nanalyzed images essentially silhouette images. Therefore, to segment the\nobjects, the method needs to be able to resolve the overlaps between multiple\nobjects by utilizing prior information about the shape of the objects. This\npaper introduces a novel method for segmentation of clustered partially\noverlapping convex objects in silhouette images. The proposed method involves\nthree main steps: pre-processing, contour evidence extraction, and contour\nestimation. Contour evidence extraction starts by recovering contour segments\nfrom a binarized image by detecting concave points. After this, the contour\nsegments which belong to the same objects are grouped. The grouping is\nformulated as a combinatorial optimization problem and solved using the branch\nand bound algorithm. Finally, the full contours of the objects are estimated by\na Gaussian process regression method. The experiments on a challenging dataset\nconsisting of nanoparticles demonstrate that the proposed method outperforms\nthree current state-of-art approaches in overlapping convex objects\nsegmentation. The method relies only on edge information and can be applied to\nany segmentation problems where the objects are partially overlapping and have\na convex shape.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 19:55:26 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Zafari", "Sahar", ""], ["Murashkina", "Mariia", ""], ["Eerola", "Tuomas", ""], ["Sampo", "Jouni", ""], ["K\u00e4lvi\u00e4inen", "Heikki", ""], ["Haario", "Heikki", ""]]}, {"id": "1906.01054", "submitter": "Sumita Mishra", "authors": "Sumita Mishra, Naresh Kumar Chaudhary, Pallavi Asthana, Anil Kumar", "title": "Deep 3D Convolutional Neural Network for Automated Lung Cancer Diagnosis", "comments": "Initial draft of PAPER Presented at IRSCNS 2018 , Goa , India final\n  version available at Mishra S., Chaudhary N.K., Asthana P., Kumar A. (2019)\n  Deep 3D Convolutional Neural Network for Automated Lung Cancer Diagnosis. In:\n  Peng SL., Dey N., Bundele M. (eds) Computing and Network Sustainability.\n  Lecture Notes in Networks and Systems, vol 75. Springer, Singapore", "journal-ref": null, "doi": "10.1007/978-981-13-7150-9_16", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer Aided Diagnosis has emerged as an indispensible technique for\nvalidating the opinion of radiologists in CT interpretation. This paper\npresents a deep 3D Convolutional Neural Network (CNN) architecture for\nautomated CT scan-based lung cancer detection system. It utilizes three\ndimensional spatial information to learn highly discriminative 3 dimensional\nfeatures instead of 2D features like texture or geometric shape whick need to\nbe generated manually. The proposed deep learning method automatically extracts\nthe 3D features on the basis of spatio-temporal statistics.The developed model\nis end-to-end and is able to predict malignancy of each voxel for given input\nscan. Simulation results demonstrate the effectiveness of proposed 3D CNN\nnetwork for classification of lung nodule in-spite of limited computational\ncapabilities.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 15:56:50 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Mishra", "Sumita", ""], ["Chaudhary", "Naresh Kumar", ""], ["Asthana", "Pallavi", ""], ["Kumar", "Anil", ""]]}, {"id": "1906.01062", "submitter": "Azim Ahmadzadeh", "authors": "Azim Ahmadzadeh, Dustin J. Kempton, Rafal A. Angryk", "title": "A Curated Image Parameter Dataset from Solar Dynamics Observatory\n  Mission", "comments": "Accepted to The Astrophysical Journal Supplement Series, 2019, 29\n  pages", "journal-ref": null, "doi": "10.3847/1538-4365/ab253a", "report-no": null, "categories": "astro-ph.SR astro-ph.IM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a large image parameter dataset extracted from the Solar Dynamics\nObservatory (SDO) mission's AIA instrument, for the period of January 2011\nthrough the current date, with the cadence of six minutes, for nine wavelength\nchannels. The volume of the dataset for each year is just short of 1 TiB.\nTowards achieving better results in the region classification of active regions\nand coronal holes, we improve upon the performance of a set of ten image\nparameters, through an in depth evaluation of various assumptions that are\nnecessary for calculation of these image parameters. Then, where possible, a\nmethod for finding an appropriate settings for the parameter calculations was\ndevised, as well as a validation task to show our improved results. In\naddition, we include comparisons of JP2 and FITS image formats using supervised\nclassification models, by tuning the parameters specific to the format of the\nimages from which they are extracted, and specific to each wavelength. The\nresults of these comparisons show that utilizing JP2 images, which are\nsignificantly smaller files, is not detrimental to the region classification\ntask that these parameters were originally intended for. Finally, we compute\nthe tuned parameters on the AIA images and provide a public API\n(http://dmlab.cs.gsu.edu/dmlabapi) to access the dataset. This dataset can be\nused in a range of studies on AIA images, such as content-based image retrieval\nor tracking of solar events, where dimensionality reduction on the images is\nnecessary for feasibility of the tasks.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 20:17:44 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Ahmadzadeh", "Azim", ""], ["Kempton", "Dustin J.", ""], ["Angryk", "Rafal A.", ""]]}, {"id": "1906.01082", "submitter": "Zhizhen Zhao", "authors": "Yifeng Fan, Tingran Gao, Zhizhen Zhao", "title": "Representation Theoretic Patterns in Multi-Frequency Class Averaging for\n  Three-Dimensional Cryo-Electron Microscopy", "comments": "38 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop in this paper a novel intrinsic classification algorithm --\nmulti-frequency class averaging (MFCA) -- for classifying noisy projection\nimages obtained from three-dimensional cryo-electron microscopy (cryo-EM) by\nthe similarity among their viewing directions. This new algorithm leverages\nmultiple irreducible representations of the unitary group to introduce\nadditional redundancy into the representation of the optimal in-plane\nrotational alignment, extending and outperforming the existing class averaging\nalgorithm that uses only a single representation. The formal algebraic model\nand representation theoretic patterns of the proposed MFCA algorithm extend the\nframework of Hadani and Singer to arbitrary irreducible representations of the\nunitary group. We conceptually establish the consistency and stability of MFCA\nby inspecting the spectral properties of a generalized local parallel transport\noperator through the lens of Wigner $D$-matrices. We demonstrate the efficacy\nof the proposed algorithm with numerical experiments.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 16:21:55 GMT"}, {"version": "v2", "created": "Sat, 12 Oct 2019 06:08:38 GMT"}, {"version": "v3", "created": "Tue, 2 Mar 2021 18:56:40 GMT"}, {"version": "v4", "created": "Mon, 5 Jul 2021 20:25:16 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Fan", "Yifeng", ""], ["Gao", "Tingran", ""], ["Zhao", "Zhizhen", ""]]}, {"id": "1906.01120", "submitter": "Jathushan Rajasegaran", "authors": "Jathushan Rajasegaran, Munawar Hayat, Salman Khan, Fahad Shahbaz Khan,\n  Ling Shao, Ming-Hsuan Yang", "title": "An Adaptive Random Path Selection Approach for Incremental Learning", "comments": "Extended version of Random Path Selection for Incremental Learning,\n  published at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a conventional supervised learning setting, a machine learning model has\naccess to examples of all object classes that are desired to be recognized\nduring the inference stage. This results in a fixed model that lacks the\nflexibility to adapt to new learning tasks. In practical settings, learning\ntasks often arrive in a sequence and the models must continually learn to\nincrement their previously acquired knowledge. Existing incremental learning\napproaches fall well below the state-of-the-art cumulative models that use all\ntraining classes at once. In this paper, we propose a random path selection\nalgorithm, called Adaptive RPS-Net, that progressively chooses optimal paths\nfor the new tasks while encouraging parameter sharing between tasks. We\nintroduce a new network capacity measure that enables us to automatically\nswitch paths if the already used resources are saturated. Since the proposed\npath-reuse strategy ensures forward knowledge transfer, our approach is\nefficient and has considerably less computation overhead. As an added novelty,\nthe proposed model integrates knowledge distillation and retrospection along\nwith the path selection strategy to overcome catastrophic forgetting. In order\nto maintain an equilibrium between previous and newly acquired knowledge, we\npropose a simple controller to dynamically balance the model plasticity.\nThrough extensive experiments, we demonstrate that the Adaptive RPS-Net method\nsurpasses the state-of-the-art performance for incremental learning and by\nutilizing parallel computation this method can run in constant time with nearly\nthe same efficiency as a conventional deep convolutional neural network.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 23:32:06 GMT"}, {"version": "v2", "created": "Sun, 15 Sep 2019 08:33:47 GMT"}, {"version": "v3", "created": "Fri, 24 Jan 2020 07:09:38 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Rajasegaran", "Jathushan", ""], ["Hayat", "Munawar", ""], ["Khan", "Salman", ""], ["Khan", "Fahad Shahbaz", ""], ["Shao", "Ling", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1906.01123", "submitter": "Victor Kitov", "authors": "Victor Kitov, Konstantin Kozlovtsev, Margarita Mishustina", "title": "Depth-Aware Arbitrary Style Transfer Using Instance Normalization", "comments": "Replacement of the previous version due to the following\n  improvements: depth estimation methods comparison added, better depth\n  estimation network used, transformation to proximity map added with offset\n  and contrast parameters. Dependency on these parameters shown, comparison of\n  AdaIN and proposed method added, user evaluation study completely remade for\n  improved version of the proposed method", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Style transfer is the process of rendering one image with some content in the\nstyle of another image, representing the style. Recent studies of Liu et al.\n(2017) show that traditional style transfer methods of Gatys et al. (2016) and\nJohnson et al. (2016) fail to reproduce the depth of the content image, which\nis critical for human perception. They suggest to preserve the depth map by\nadditional regularizer in the optimized loss function, forcing preservation of\nthe depth map. However these traditional methods are either computationally\ninefficient or require training a separate neural network for each style. AdaIN\nmethod of Huang et al. (2017) allows efficient transferring of arbitrary style\nwithout training a separate model but is not able to reproduce the depth map of\nthe content image. We propose an extension to this method, allowing depth map\npreservation by applying variable stylization strength. Qualitative analysis\nand results of user evaluation study indicate that the proposed method provides\nbetter stylizations, compared to the original AdaIN style transfer method.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 23:39:25 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 17:53:11 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Kitov", "Victor", ""], ["Kozlovtsev", "Konstantin", ""], ["Mishustina", "Margarita", ""]]}, {"id": "1906.01134", "submitter": "Victor Kitov", "authors": "Alexey Schekalev, Victor Kitov", "title": "Style Transfer With Adaptation to the Central Objects of the Scene", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Style transfer is a problem of rendering image with some content in the style\nof another image, for example a family photo in the style of a painting of some\nfamous artist. The drawback of classical style transfer algorithm is that it\nimposes style uniformly on all parts of the content image, which perturbs\ncentral objects on the content image, such as faces or text, and makes them\nunrecognizable. This work proposes a novel style transfer algorithm which\nautomatically detects central objects on the content image, generates spatial\nimportance mask and imposes style non-uniformly: central objects are stylized\nless to preserve their recognizability and other parts of the image are\nstylized as usual to preserve the style. Three methods of automatic central\nobject detection are proposed and evaluated qualitatively and via a user\nevaluation study. Both comparisons demonstrate higher quality of stylization\ncompared to the classical style transfer method.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 00:07:46 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Schekalev", "Alexey", ""], ["Kitov", "Victor", ""]]}, {"id": "1906.01140", "submitter": "Bo Yang", "authors": "Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew\n  Markham, Niki Trigoni", "title": "Learning Object Bounding Boxes for 3D Instance Segmentation on Point\n  Clouds", "comments": "NeurIPS 2019 Spotlight. Code and data are available at\n  https://github.com/Yang7879/3D-BoNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel, conceptually simple and general framework for instance\nsegmentation on 3D point clouds. Our method, called 3D-BoNet, follows the\nsimple design philosophy of per-point multilayer perceptrons (MLPs). The\nframework directly regresses 3D bounding boxes for all instances in a point\ncloud, while simultaneously predicting a point-level mask for each instance. It\nconsists of a backbone network followed by two parallel network branches for 1)\nbounding box regression and 2) point mask prediction. 3D-BoNet is single-stage,\nanchor-free and end-to-end trainable. Moreover, it is remarkably\ncomputationally efficient as, unlike existing approaches, it does not require\nany post-processing steps such as non-maximum suppression, feature sampling,\nclustering or voting. Extensive experiments show that our approach surpasses\nexisting work on both ScanNet and S3DIS datasets while being approximately 10x\nmore computationally efficient. Comprehensive ablation studies demonstrate the\neffectiveness of our design.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 00:33:56 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 06:47:05 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Yang", "Bo", ""], ["Wang", "Jianan", ""], ["Clark", "Ronald", ""], ["Hu", "Qingyong", ""], ["Wang", "Sen", ""], ["Markham", "Andrew", ""], ["Trigoni", "Niki", ""]]}, {"id": "1906.01160", "submitter": "Naimul Mefraz Khan", "authors": "Naimul Mefraz Khan, Marcia Hon, Nabila Abraham", "title": "Transfer Learning with intelligent training data selection for\n  prediction of Alzheimer's Disease", "comments": "Accepted to IEEE Access", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of Alzheimer's Disease (AD) from neuroimaging data such as MRI\nthrough machine learning has been a subject of intense research in recent\nyears. Recent success of deep learning in computer vision has progressed such\nresearch further. However, common limitations with such algorithms are reliance\non a large number of training images, and requirement of careful optimization\nof the architecture of deep networks. In this paper, we attempt solving these\nissues with transfer learning, where the state-of-the-art VGG architecture is\ninitialized with pre-trained weights from large benchmark datasets consisting\nof natural images. The network is then fine-tuned with layer-wise tuning, where\nonly a pre-defined group of layers are trained on MRI images. To shrink the\ntraining data size, we employ image entropy to select the most informative\nslices. Through experimentation on the ADNI dataset, we show that with training\nsize of 10 to 20 times smaller than the other contemporary methods, we reach\nstate-of-the-art performance in AD vs. NC, AD vs. MCI, and MCI vs. NC\nclassification problems, with a 4% and a 7% increase in accuracy over the\nstate-of-the-art for AD vs. MCI and MCI vs. NC, respectively. We also provide\ndetailed analysis of the effect of the intelligent training data selection\nmethod, changing the training size, and changing the number of layers to be\nfine-tuned. Finally, we provide Class Activation Maps (CAM) that demonstrate\nhow the proposed model focuses on discriminative image regions that are\nneuropathologically relevant, and can help the healthcare practitioner in\ninterpreting the model's decision making process.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 02:23:59 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Khan", "Naimul Mefraz", ""], ["Hon", "Marcia", ""], ["Abraham", "Nabila", ""]]}, {"id": "1906.01166", "submitter": "Yuchao Li", "authors": "Yuchao Li, Rongrong Ji, Shaohui Lin, Baochang Zhang, Chenqian Yan,\n  Yongjian Wu, Feiyue Huang, Ling Shao", "title": "Interpretable Neural Network Decoupling", "comments": "20 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The remarkable performance of convolutional neural networks (CNNs) is\nentangled with their huge number of uninterpretable parameters, which has\nbecome the bottleneck limiting the exploitation of their full potential.\nTowards network interpretation, previous endeavors mainly resort to the single\nfilter analysis, which however ignores the relationship between filters. In\nthis paper, we propose a novel architecture decoupling method to interpret the\nnetwork from a perspective of investigating its calculation paths. More\nspecifically, we introduce a novel architecture controlling module in each\nlayer to encode the network architecture by a vector. By maximizing the mutual\ninformation between the vectors and input images, the module is trained to\nselect specific filters to distill a unique calculation path for each input.\nFurthermore, to improve the interpretability and compactness of the decoupled\nnetwork, the output of each layer is encoded to align the architecture encoding\nvector with the constraint of sparsity regularization. Unlike conventional\npixel-level or filter-level network interpretation methods, we propose a\npath-level analysis to explore the relationship between the combination of\nfilter and semantic concepts, which is more suitable to interpret the working\nrationale of the decoupled network. Extensive experiments show that the\ndecoupled network achieves several applications, i.e., network interpretation,\nnetwork acceleration, and adversarial samples detection.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 02:40:38 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 13:22:34 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Li", "Yuchao", ""], ["Ji", "Rongrong", ""], ["Lin", "Shaohui", ""], ["Zhang", "Baochang", ""], ["Yan", "Chenqian", ""], ["Wu", "Yongjian", ""], ["Huang", "Feiyue", ""], ["Shao", "Ling", ""]]}, {"id": "1906.01193", "submitter": "Jinglu Wang", "authors": "Zengyi Qin, Jinglu Wang, Yan Lu", "title": "Triangulation Learning Network: from Monocular to Stereo 3D Object\n  Detection", "comments": "9 pages, accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of 3D object detection from stereo\nimages, in which the key challenge is how to effectively utilize stereo\ninformation. Different from previous methods using pixel-level depth maps, we\npropose employing 3D anchors to explicitly construct object-level\ncorrespondences between the regions of interest in stereo images, from which\nthe deep neural network learns to detect and triangulate the targeted object in\n3D space. We also introduce a cost-efficient channel reweighting strategy that\nenhances representational features and weakens noisy signals to facilitate the\nlearning process. All of these are flexibly integrated into a solid baseline\ndetector that uses monocular images. We demonstrate that both the monocular\nbaseline and the stereo triangulation learning network outperform the prior\nstate-of-the-arts in 3D object detection and localization on the challenging\nKITTI dataset.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 04:50:46 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Qin", "Zengyi", ""], ["Wang", "Jinglu", ""], ["Lu", "Yan", ""]]}, {"id": "1906.01205", "submitter": "Fangyu Liu", "authors": "Fangyu Liu, Rongtian Ye", "title": "A Strong and Robust Baseline for Text-Image Matching", "comments": "6 pages (excluding references); 2019 ACL Student Research Workshop\n  (to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We review the current schemes of text-image matching models and propose\nimprovements for both training and inference. First, we empirically show\nlimitations of two popular loss (sum and max-margin loss) widely used in\ntraining text-image embeddings and propose a trade-off: a kNN-margin loss which\n1) utilizes information from hard negatives and 2) is robust to noise as all\n$K$-most hardest samples are taken into account, tolerating \\emph{pseudo}\nnegatives and outliers. Second, we advocate the use of Inverted Softmax\n(\\textsc{Is}) and Cross-modal Local Scaling (\\textsc{Csls}) during inference to\nmitigate the so-called hubness problem in high-dimensional embedding space,\nenhancing scores of all metrics by a large margin.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 05:42:58 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Liu", "Fangyu", ""], ["Ye", "Rongtian", ""]]}, {"id": "1906.01223", "submitter": "Abdelaziz Djelouah", "authors": "Joaquim Campos, Simon Meierhans, Abdelaziz Djelouah and Christopher\n  Schroers", "title": "Content Adaptive Optimization for Neural Image Compression", "comments": "CVPR Workshop and Challenge on Learned Image Compression (2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of neural image compression has witnessed exciting progress as\nrecently proposed architectures already surpass the established transform\ncoding based approaches. While, so far, research has mainly focused on\narchitecture and model improvements, in this work we explore content adaptive\noptimization. To this end, we introduce an iterative procedure which adapts the\nlatent representation to the specific content we wish to compress while keeping\nthe parameters of the network and the predictive model fixed. Our experiments\nshow that this allows for an overall increase in rate-distortion performance,\nindependently of the specific architecture used. Furthermore, we also evaluate\nthis strategy in the context of adapting a pretrained network to other content\nthat is different in visual appearance or resolution. Here, our experiments\nshow that our adaptation strategy can largely close the gap as compared to\nmodels specifically trained for the given content while having the benefit that\nno additional data in the form of model parameter updates has to be\ntransmitted.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 06:39:50 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 07:21:47 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Campos", "Joaquim", ""], ["Meierhans", "Simon", ""], ["Djelouah", "Abdelaziz", ""], ["Schroers", "Christopher", ""]]}, {"id": "1906.01256", "submitter": "Jeremy Kawahara", "authors": "Jeremy Kawahara and Ghassan Hamarneh", "title": "Visual Diagnosis of Dermatological Disorders: Human and Machine\n  Performance", "comments": "15 pages, 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin conditions are a global health concern, ranking the fourth highest cause\nof nonfatal disease burden when measured as years lost due to disability. As\ndiagnosing, or classifying, skin diseases can help determine effective\ntreatment, dermatologists have extensively researched how to diagnose\nconditions from a patient's history and the lesion's visual appearance.\nComputer vision researchers are attempting to encode this diagnostic ability\ninto machines, and several recent studies report machine level performance\ncomparable with dermatologists.\n  This report reviews machine approaches to classify skin images and consider\ntheir performance when compared to human dermatologists. Following an overview\nof common image modalities, dermatologists' diagnostic approaches and common\ntasks, and publicly available datasets, we discuss approaches to machine skin\nlesion classification. We then review works that directly compare human and\nmachine performance. Finally, this report addresses the limitations and sources\nof errors in image-based skin disease diagnosis, applicable to both machines\nand dermatologists in a teledermatology setting.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 08:06:50 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Kawahara", "Jeremy", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "1906.01258", "submitter": "Massimiliano Mancini", "authors": "Massimiliano Mancini, Hakan Karaoguz, Elisa Ricci, Patric Jensfelt,\n  Barbara Caputo", "title": "Knowledge is Never Enough: Towards Web Aided Deep Open World Recognition", "comments": "ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While today's robots are able to perform sophisticated tasks, they can only\nact on objects they have been trained to recognize. This is a severe\nlimitation: any robot will inevitably see new objects in unconstrained\nsettings, and thus will always have visual knowledge gaps. However, standard\nvisual modules are usually built on a limited set of classes and are based on\nthe strong prior that an object must belong to one of those classes.\nIdentifying whether an instance does not belong to the set of known categories\n(i.e. open set recognition), only partially tackles this problem, as a truly\nautonomous agent should be able not only to detect what it does not know, but\nalso to extend dynamically its knowledge about the world. We contribute to this\nchallenge with a deep learning architecture that can dynamically update its\nknown classes in an end-to-end fashion. The proposed deep network, based on a\ndeep extension of a non-parametric model, detects whether a perceived object\nbelongs to the set of categories known by the system and learns it without the\nneed to retrain the whole system from scratch. Annotated images about the new\ncategory can be provided by an 'oracle' (i.e. human supervision), or by\nautonomous mining of the Web. Experiments on two different databases and on a\nrobot platform demonstrate the promise of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 08:14:22 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Mancini", "Massimiliano", ""], ["Karaoguz", "Hakan", ""], ["Ricci", "Elisa", ""], ["Jensfelt", "Patric", ""], ["Caputo", "Barbara", ""]]}, {"id": "1906.01259", "submitter": "Xianxu Hou", "authors": "Xianxu Hou, Hongming Luo, Jingxin Liu, Bolei Xu, Ke Sun, Yuanhao Gong,\n  Bozhi Liu, Guoping Qiu", "title": "Learning Deep Image Priors for Blind Image Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image denoising is the process of removing noise from noisy images, which is\nan image domain transferring task, i.e., from a single or several noise level\ndomains to a photo-realistic domain. In this paper, we propose an effective\nimage denoising method by learning two image priors from the perspective of\ndomain alignment. We tackle the domain alignment on two levels. 1) the\nfeature-level prior is to learn domain-invariant features for corrupted images\nwith different level noise; 2) the pixel-level prior is used to push the\ndenoised images to the natural image manifold. The two image priors are based\non $\\mathcal{H}$-divergence theory and implemented by learning classifiers in\nadversarial training manners. We evaluate our approach on multiple datasets.\nThe results demonstrate the effectiveness of our approach for robust image\ndenoising on both synthetic and real-world noisy images. Furthermore, we show\nthat the feature-level prior is capable of alleviating the discrepancy between\ndifferent level noise. It can be used to improve the blind denoising\nperformance in terms of distortion measures (PSNR and SSIM), while pixel-level\nprior can effectively improve the perceptual quality to ensure the realistic\noutputs, which is further validated by subjective evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 08:16:01 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Hou", "Xianxu", ""], ["Luo", "Hongming", ""], ["Liu", "Jingxin", ""], ["Xu", "Bolei", ""], ["Sun", "Ke", ""], ["Gong", "Yuanhao", ""], ["Liu", "Bozhi", ""], ["Qiu", "Guoping", ""]]}, {"id": "1906.01272", "submitter": "Thomas Rogers", "authors": "Thomas W. Rogers, Nicolas Jaccard, Francis Carbonaro, Hans G. Lemij,\n  Koenraad A. Vermeer, Nicolaas J. Reus, Sameer Trikha", "title": "Evaluation of an AI system for the automated detection of glaucoma from\n  stereoscopic optic disc photographs: the European Optic Disc Assessment Study", "comments": "24 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives: To evaluate the performance of a deep learning based Artificial\nIntelligence (AI) software for detection of glaucoma from stereoscopic optic\ndisc photographs, and to compare this performance to the performance of a large\ncohort of ophthalmologists and optometrists.\n  Methods: A retrospective study evaluating the diagnostic performance of an AI\nsoftware (Pegasus v1.0, Visulytix Ltd., London UK) and comparing it to that of\n243 European ophthalmologists and 208 British optometrists, as determined in\nprevious studies, for the detection of glaucomatous optic neuropathy from 94\nscanned stereoscopic photographic slides scanned into digital format.\n  Results: Pegasus was able to detect glaucomatous optic neuropathy with an\naccuracy of 83.4% (95% CI: 77.5-89.2). This is comparable to an average\nophthalmologist accuracy of 80.5% (95% CI: 67.2-93.8) and average optometrist\naccuracy of 80% (95% CI: 67-88) on the same images. In addition, the AI system\nhad an intra-observer agreement (Cohen's Kappa, $\\kappa$) of 0.74 (95% CI:\n0.63-0.85), compared to 0.70 (range: -0.13-1.00; 95% CI: 0.67-0.73) and 0.71\n(range: 0.08-1.00) for ophthalmologists and optometrists, respectively. There\nwas no statistically significant difference between the performance of the deep\nlearning system and ophthalmologists or optometrists. There was no\nstatistically significant difference between the performance of the deep\nlearning system and ophthalmologists or optometrists.\n  Conclusion: The AI system obtained a diagnostic performance and repeatability\ncomparable to that of the ophthalmologists and optometrists. We conclude that\ndeep learning based AI systems, such as Pegasus, demonstrate significant\npromise in the assisted detection of glaucomatous optic neuropathy.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 08:40:03 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Rogers", "Thomas W.", ""], ["Jaccard", "Nicolas", ""], ["Carbonaro", "Francis", ""], ["Lemij", "Hans G.", ""], ["Vermeer", "Koenraad A.", ""], ["Reus", "Nicolaas J.", ""], ["Trikha", "Sameer", ""]]}, {"id": "1906.01288", "submitter": "Jie Hu", "authors": "Jie Hu, Rongrong Ji, ShengChuan Zhang, Xiaoshuai Sun, Qixiang Ye,\n  Chia-Wen Lin, Qi Tian", "title": "Information Competing Process for Learning Diversified Representations", "comments": "Accept as a NeurIPS 2019 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning representations with diversified information remains as an open\nproblem. Towards learning diversified representations, a new approach, termed\nInformation Competing Process (ICP), is proposed in this paper. Aiming to\nenrich the information carried by feature representations, ICP separates a\nrepresentation into two parts with different mutual information constraints.\nThe separated parts are forced to accomplish the downstream task independently\nin a competitive environment which prevents the two parts from learning what\neach other learned for the downstream task. Such competing parts are then\ncombined synergistically to complete the task. By fusing representation parts\nlearned competitively under different conditions, ICP facilitates obtaining\ndiversified representations which contain rich information. Experiments on\nimage classification and image reconstruction tasks demonstrate the great\npotential of ICP to learn discriminative and disentangled representations in\nboth supervised and self-supervised learning settings.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 09:10:43 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 09:19:32 GMT"}, {"version": "v3", "created": "Thu, 28 Nov 2019 04:17:10 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Hu", "Jie", ""], ["Ji", "Rongrong", ""], ["Zhang", "ShengChuan", ""], ["Sun", "Xiaoshuai", ""], ["Ye", "Qixiang", ""], ["Lin", "Chia-Wen", ""], ["Tian", "Qi", ""]]}, {"id": "1906.01290", "submitter": "Jingyi Hou", "authors": "Jingyi Hou, Xinxiao Wu, Yayun Qi, Wentian Zhao, Jiebo Luo and Yunde\n  Jia", "title": "Relational Reasoning using Prior Knowledge for Visual Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting relationships among objects has achieved remarkable progress in\ninterpreting images or videos by natural language. Most existing methods resort\nto first detecting objects and their relationships, and then generating textual\ndescriptions, which heavily depends on pre-trained detectors and leads to\nperformance drop when facing problems of heavy occlusion, tiny-size objects and\nlong-tail in object detection. In addition, the separate procedure of detecting\nand captioning results in semantic inconsistency between the pre-defined\nobject/relation categories and the target lexical words. We exploit prior human\ncommonsense knowledge for reasoning relationships between objects without any\npre-trained detectors and reaching semantic coherency within one image or video\nin captioning. The prior knowledge (e.g., in the form of knowledge graph)\nprovides commonsense semantic correlation and constraint between objects that\nare not explicit in the image and video, serving as useful guidance to build\nsemantic graph for sentence generation. Particularly, we present a joint\nreasoning method that incorporates 1) commonsense reasoning for embedding image\nor video regions into semantic space to build semantic graph and 2) relational\nreasoning for encoding semantic graph to generate sentences. Extensive\nexperiments on the MS-COCO image captioning benchmark and the MSVD video\ncaptioning benchmark validate the superiority of our method on leveraging prior\ncommonsense knowledge to enhance relational reasoning for visual captioning.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 09:15:54 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Hou", "Jingyi", ""], ["Wu", "Xinxiao", ""], ["Qi", "Yayun", ""], ["Zhao", "Wentian", ""], ["Luo", "Jiebo", ""], ["Jia", "Yunde", ""]]}, {"id": "1906.01292", "submitter": "Emmanuel de B\\'ezenac", "authors": "Emmanuel de B\\'ezenac, Ibrahim Ayed, Patrick Gallinari", "title": "Optimal Unsupervised Domain Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain Translation is the problem of finding a meaningful correspondence\nbetween two domains. Since in a majority of settings paired supervision is not\navailable, much work focuses on Unsupervised Domain Translation (UDT) where\ndata samples from each domain are unpaired. Following the seminal work of\nCycleGAN for UDT, many variants and extensions of this model have been\nproposed. However, there is still little theoretical understanding behind their\nsuccess. We observe that these methods yield solutions which are approximately\nminimal w.r.t. a given transportation cost, leading us to reformulate the\nproblem in the Optimal Transport (OT) framework. This viewpoint gives us a new\nperspective on Unsupervised Domain Translation and allows us to prove the\nexistence and uniqueness of the retrieved mapping, given a large family of\ntransport costs. We then propose a novel framework to efficiently compute\noptimal mappings in a dynamical setting. We show that it generalizes previous\nmethods and enables a more explicit control over the computed optimal mapping.\nIt also provides smooth interpolations between the two domains. Experiments on\ntoy and real world datasets illustrate the behavior of our method.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 09:19:47 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["de B\u00e9zenac", "Emmanuel", ""], ["Ayed", "Ibrahim", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1906.01304", "submitter": "Abhinav Valada", "authors": "Mayank Mittal, Rohit Mohan, Wolfram Burgard, and Abhinav Valada", "title": "Vision-Based Autonomous UAV Navigation and Landing for Urban Search and\n  Rescue", "comments": "Accepted for publication in the proceedings of the International\n  Symposium on Robotics Research (ISRR) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmanned Aerial Vehicles (UAVs) equipped with bioradars are a life-saving\ntechnology that can enable identification of survivors under collapsed\nbuildings in the aftermath of natural disasters such as earthquakes or gas\nexplosions. However, these UAVs have to be able to autonomously navigate in\ndisaster struck environments and land on debris piles in order to accurately\nlocate the survivors. This problem is extremely challenging as pre-existing\nmaps cannot be leveraged for navigation due to structural changes that may have\noccurred. Furthermore, existing landing site detection algorithms are not\nsuitable to identify safe landing regions on debris piles. In this work, we\npresent a computationally efficient system for autonomous UAV navigation and\nlanding that does not require any prior knowledge about the environment. We\npropose a novel landing site detection algorithm that computes costmaps based\non several hazard factors including terrain flatness, steepness, depth\naccuracy, and energy consumption information. We also introduce a\nfirst-of-a-kind synthetic dataset of over 1.2 million images of collapsed\nbuildings with groundtruth depth, surface normals, semantics and camera pose\ninformation. We demonstrate the efficacy of our system using experiments from a\ncity scale hyperrealistic simulation environment and in real-world scenarios\nwith collapsed buildings.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 09:54:38 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 11:03:34 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Mittal", "Mayank", ""], ["Mohan", "Rohit", ""], ["Burgard", "Wolfram", ""], ["Valada", "Abhinav", ""]]}, {"id": "1906.01308", "submitter": "Guodong Ding", "authors": "Guodong Ding, Salman Khan, Zhenmin Tang, Jian Zhang, Fatih Porikli", "title": "Towards better Validity: Dispersion based Clustering for Unsupervised\n  Person Re-identification", "comments": "10 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification aims to establish the correct identity\ncorrespondences of a person moving through a non-overlapping multi-camera\ninstallation. Recent advances based on deep learning models for this task\nmainly focus on supervised learning scenarios where accurate annotations are\nassumed to be available for each setup. Annotating large scale datasets for\nperson re-identification is demanding and burdensome, which renders the\ndeployment of such supervised approaches to real-world applications infeasible.\nTherefore, it is necessary to train models without explicit supervision in an\nautonomous manner. In this paper, we propose an elegant and practical\nclustering approach for unsupervised person re-identification based on the\ncluster validity consideration. Concretely, we explore a fundamental concept in\nstatistics, namely \\emph{dispersion}, to achieve a robust clustering criterion.\nDispersion reflects the compactness of a cluster when employed at the\nintra-cluster level and reveals the separation when measured at the\ninter-cluster level. With this insight, we design a novel Dispersion-based\nClustering (DBC) approach which can discover the underlying patterns in data.\nThis approach considers a wider context of sample-level pairwise relationships\nto achieve a robust cluster affinity assessment which handles the complications\nmay arise due to prevalent imbalanced data distributions. Additionally, our\nsolution can automatically prioritize standalone data points and prevents\ninferior clustering. Our extensive experimental analysis on image and video\nre-identification benchmarks demonstrate that our method outperforms the\nstate-of-the-art unsupervised methods by a significant margin. Code is\navailable at https://github.com/gddingcs/Dispersion-based-Clustering.git.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 10:03:08 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Ding", "Guodong", ""], ["Khan", "Salman", ""], ["Tang", "Zhenmin", ""], ["Zhang", "Jian", ""], ["Porikli", "Fatih", ""]]}, {"id": "1906.01314", "submitter": "Miao Wang", "authors": "Miao Wang, Guo-Ye Yang, Ruilong Li, Run-Ze Liang, Song-Hai Zhang,\n  Peter. M. Hall, Shi-Min Hu", "title": "Example-Guided Style Consistent Image Synthesis from Semantic Labeling", "comments": "CVPR 2019 - Code and data - https://github.com/cxjyxxme/pix2pixSC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Example-guided image synthesis aims to synthesize an image from a semantic\nlabel map and an exemplary image indicating style. We use the term \"style\" in\nthis problem to refer to implicit characteristics of images, for example: in\nportraits \"style\" includes gender, racial identity, age, hairstyle; in full\nbody pictures it includes clothing; in street scenes, it refers to weather and\ntime of day and such like. A semantic label map in these cases indicates facial\nexpression, full body pose, or scene segmentation. We propose a solution to the\nexample-guided image synthesis problem using conditional generative adversarial\nnetworks with style consistency. Our key contributions are (i) a novel style\nconsistency discriminator to determine whether a pair of images are consistent\nin style; (ii) an adaptive semantic consistency loss; and (iii) a training data\nsampling strategy, for synthesizing style-consistent results to the exemplar.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 10:09:47 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 01:31:01 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Wang", "Miao", ""], ["Yang", "Guo-Ye", ""], ["Li", "Ruilong", ""], ["Liang", "Run-Ze", ""], ["Zhang", "Song-Hai", ""], ["Hall", "Peter. M.", ""], ["Hu", "Shi-Min", ""]]}, {"id": "1906.01340", "submitter": "Firas Laakom", "authors": "Firas Laakom, Jenni Raitoharju, Alexandros Iosifidis, Jarno Nikkanen,\n  Moncef Gabbouj", "title": "Color Constancy Convolutional Autoencoder", "comments": "6 pages, 1 figure, 3 tables", "journal-ref": "2019 IEEE Symposium Series on Computational Intelligence (SSCI)", "doi": "10.1109/SSCI44817.2019.9002684", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the importance of pre-training for the generalization\ncapability in the color constancy problem. We propose two novel approaches\nbased on convolutional autoencoders: an unsupervised pre-training algorithm\nusing a fine-tuned encoder and a semi-supervised pre-training algorithm using a\nnovel composite-loss function. This enables us to solve the data scarcity\nproblem and achieve competitive, to the state-of-the-art, results while\nrequiring much fewer parameters on ColorChecker RECommended dataset. We further\nstudy the over-fitting phenomenon on the recently introduced version of\nINTEL-TUT Dataset for Camera Invariant Color Constancy Research, which has both\nfield and non-field scenes acquired by three different camera models.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 10:56:32 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Laakom", "Firas", ""], ["Raitoharju", "Jenni", ""], ["Iosifidis", "Alexandros", ""], ["Nikkanen", "Jarno", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "1906.01342", "submitter": "Hao Yang", "authors": "Jinpeng Lin, Hao Yang, Dong Chen, Ming Zeng, Fang Wen, Lu Yuan", "title": "Face Parsing with RoI Tanh-Warping", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face parsing computes pixel-wise label maps for different semantic components\n(e.g., hair, mouth, eyes) from face images. Existing face parsing literature\nhave illustrated significant advantages by focusing on individual regions of\ninterest (RoIs) for faces and facial components. However, the traditional\ncrop-and-resize focusing mechanism ignores all contextual area outside the\nRoIs, and thus is not suitable when the component area is unpredictable, e.g.\nhair. Inspired by the physiological vision system of human, we propose a novel\nRoI Tanh-warping operator that combines the central vision and the peripheral\nvision together. It addresses the dilemma between a limited sized RoI for\nfocusing and an unpredictable area of surrounding context for peripheral\ninformation. To this end, we propose a novel hybrid convolutional neural\nnetwork for face parsing. It uses hierarchical local based method for inner\nfacial components and global methods for outer facial components. The whole\nframework is simple and principled, and can be trained end-to-end. To\nfacilitate future research of face parsing, we also manually relabel the\ntraining data of the HELEN dataset and will make it public. Experiments on both\nHELEN and LFW-PL benchmarks demonstrate that our method surpasses\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 11:01:17 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Lin", "Jinpeng", ""], ["Yang", "Hao", ""], ["Chen", "Dong", ""], ["Zeng", "Ming", ""], ["Wen", "Fang", ""], ["Yuan", "Lu", ""]]}, {"id": "1906.01344", "submitter": "Zheng Zhu", "authors": "Rui Zhang, Zheng Zhu, Peng Li, Rui Wu, Chaoxu Guo, Guan Huang, Hailun\n  Xia", "title": "Exploiting Offset-guided Network for Pose Estimation and Tracking", "comments": "To appear in CVPR-2019 VUHCS Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose estimation has witnessed a significant advance thanks to the\ndevelopment of deep learning. Recent human pose estimation approaches tend to\ndirectly predict the location heatmaps, which causes quantization errors and\ninevitably deteriorates the performance within the reduced network output. Aim\nat solving it, we revisit the heatmap-offset aggregation method and propose the\nOffset-guided Network (OGN) with an intuitive but effective fusion strategy for\nboth two-stages pose estimation and Mask R-CNN. For two-stages pose estimation,\na greedy box generation strategy is also proposed to keep more necessary\ncandidates while performing person detection. For mask R-CNN, ratio-consistent\nis adopted to improve the generalization ability of the network.\nState-of-the-art results on COCO and PoseTrack dataset verify the effectiveness\nof our offset-guided pose estimation and tracking.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 11:05:24 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Zhang", "Rui", ""], ["Zhu", "Zheng", ""], ["Li", "Peng", ""], ["Wu", "Rui", ""], ["Guo", "Chaoxu", ""], ["Huang", "Guan", ""], ["Xia", "Hailun", ""]]}, {"id": "1906.01347", "submitter": "Thibaut Issenhuth", "authors": "Thibaut Issenhuth and J\\'er\\'emie Mary and Cl\\'ement Calauz\\`enes", "title": "End-to-End Learning of Geometric Deformations of Feature Maps for\n  Virtual Try-On", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2D virtual try-on task has recently attracted a lot of interest from the\nresearch community, for its direct potential applications in online shopping as\nwell as for its inherent and non-addressed scientific challenges. This task\nrequires to fit an in-shop cloth image on the image of a person. It is highly\nchallenging because it requires to warp the cloth on the target person while\npreserving its patterns and characteristics, and to compose the item with the\nperson in a realistic manner. Current state-of-the-art models generate images\nwith visible artifacts, due either to a pixel-level composition step or to the\ngeometric transformation. In this paper, we propose WUTON: a Warping U-net for\na Virtual Try-On system. It is a siamese U-net generator whose skip connections\nare geometrically transformed by a convolutional geometric matcher. The whole\narchitecture is trained end-to-end with a multi-task loss including an\nadversarial one. This enables our network to generate and use realistic spatial\ntransformations of the cloth to synthesize images of high visual quality. The\nproposed architecture can be trained end-to-end and allows us to advance\ntowards a detail-preserving and photo-realistic 2D virtual try-on system. Our\nmethod outperforms the current state-of-the-art with visual results as well as\nwith the Learned Perceptual Image Similarity (LPIPS) metric.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 11:09:36 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 20:13:13 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Issenhuth", "Thibaut", ""], ["Mary", "J\u00e9r\u00e9mie", ""], ["Calauz\u00e8nes", "Cl\u00e9ment", ""]]}, {"id": "1906.01357", "submitter": "Zheng Zhu", "authors": "Peng Li, Jiabin Zhang, Zheng Zhu, Yanwei Li, Lu Jiang, Guan Huang", "title": "State-aware Re-identification Feature for Multi-target Multi-camera\n  Tracking", "comments": "To appear in CVPR-2019 TRMTMCT Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-target Multi-camera Tracking (MTMCT) aims to extract the trajectories\nfrom videos captured by a set of cameras. Recently, the tracking performance of\nMTMCT is significantly enhanced with the employment of re-identification\n(Re-ID) model. However, the appearance feature usually becomes unreliable due\nto the occlusion and orientation variance of the targets. Directly applying\nRe-ID model in MTMCT will encounter the problem of identity switches (IDS) and\ntracklet fragment caused by occlusion. To solve these problems, we propose a\nnovel tracking framework in this paper. In this framework, the occlusion status\nand orientation information are utilized in Re-ID model with human pose\ninformation considered. In addition, the tracklet association using the\nproposed fused tracking feature is adopted to handle the fragment problem. The\nproposed tracker achieves 81.3\\% IDF1 on the multiple-camera hard sequence,\nwhich outperforms all other reference methods by a large margin.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 11:43:02 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Li", "Peng", ""], ["Zhang", "Jiabin", ""], ["Zhu", "Zheng", ""], ["Li", "Yanwei", ""], ["Jiang", "Lu", ""], ["Huang", "Guan", ""]]}, {"id": "1906.01363", "submitter": "Shanka Subhra Mondal", "authors": "Shanka Subhra Mondal, Abhilash Nandy, Ritesh Agrawal, Debashis Sen", "title": "KarNet: An Efficient Boolean Function Simplifier", "comments": "5 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many approaches such as Quine-McCluskey algorithm, Karnaugh map solving,\nPetrick's method and McBoole's method have been devised to simplify Boolean\nexpressions in order to optimize hardware implementation of digital circuits.\nHowever, the algorithmic implementations of these methods are hard-coded and\nalso their computation time is proportional to the number of minterms involved\nin the expression. In this paper, we propose KarNet, where the ability of\nConvolutional Neural Networks to model relationships between various cell\nlocations and values by capturing spatial dependencies is exploited to solve\nKarnaugh maps. In order to do so, a Karnaugh map is represented as an image\nsignal, where each cell is considered as a pixel. Experimental results show\nthat the computation time of KarNet is independent of the number of minterms\nand is of the order of one-hundredth to one-tenth that of the rule-based\nmethods. KarNet being a learned system is found to achieve nearly a hundred\npercent accuracy, precision, and recall. We train KarNet to solve four variable\nKarnaugh maps and also show that a similar method can be applied on Karnaugh\nmaps with more variables. Finally, we show a way to build a fully accurate and\ncomputationally fast system using KarNet.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 11:55:22 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Mondal", "Shanka Subhra", ""], ["Nandy", "Abhilash", ""], ["Agrawal", "Ritesh", ""], ["Sen", "Debashis", ""]]}, {"id": "1906.01379", "submitter": "Zhongling Huang", "authors": "Zhongling Huang, Zongxu Pan, Bin Lei", "title": "What, Where and How to Transfer in SAR Target Recognition Based on Deep\n  CNNs", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing 2019", "doi": "10.1109/TGRS.2019.2947634", "report-no": null, "categories": "eess.SP cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep convolutional neural networks (DCNNs) have attracted much attention in\nremote sensing recently. Compared with the large-scale annotated dataset in\nnatural images, the lack of labeled data in remote sensing becomes an obstacle\nto train a deep network very well, especially in SAR image interpretation.\nTransfer learning provides an effective way to solve this problem by borrowing\nthe knowledge from the source task to the target task. In optical remote\nsensing application, a prevalent mechanism is to fine-tune on an existing model\npre-trained with a large-scale natural image dataset, such as ImageNet.\nHowever, this scheme does not achieve satisfactory performance for SAR\napplication because of the prominent discrepancy between SAR and optical\nimages. In this paper, we attempt to discuss three issues that are seldom\nstudied before in detail: (1) what network and source tasks are better to\ntransfer to SAR targets, (2) in which layer are transferred features more\ngeneric to SAR targets and (3) how to transfer effectively to SAR targets\nrecognition. Based on the analysis, a transitive transfer method via\nmulti-source data with domain adaptation is proposed in this paper to decrease\nthe discrepancy between the source data and SAR targets. Several experiments\nare conducted on OpenSARShip. The results indicate that the universal\nconclusions about transfer learning in natural images cannot be completely\napplied to SAR targets, and the analysis of what and where to transfer in SAR\ntarget recognition is helpful to decide how to transfer more effectively.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 12:41:30 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Huang", "Zhongling", ""], ["Pan", "Zongxu", ""], ["Lei", "Bin", ""]]}, {"id": "1906.01399", "submitter": "Norimichi Ukita", "authors": "Norimichi Ukita and Yusuke Uematsu", "title": "Semi- and Weakly-supervised Human Pose Estimation", "comments": "Revised preprint submitted to CVIU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For human pose estimation in still images, this paper proposes three semi-\nand weakly-supervised learning schemes. While recent advances of convolutional\nneural networks improve human pose estimation using supervised training data,\nour focus is to explore the semi- and weakly-supervised schemes. Our proposed\nschemes initially learn conventional model(s) for pose estimation from a small\namount of standard training images with human pose annotations. For the first\nsemi-supervised learning scheme, this conventional pose model detects candidate\nposes in training images with no human annotation. From these candidate poses,\nonly true-positives are selected by a classifier using a pose feature\nrepresenting the configuration of all body parts. The accuracies of these\ncandidate pose estimation and true-positive pose selection are improved by\naction labels provided to these images in our second and third learning\nschemes, which are semi- and weakly-supervised learning. While the first and\nsecond learning schemes select only poses that are similar to those in the\nsupervised training data, the third scheme selects more true-positive poses\nthat are significantly different from any supervised poses. This pose selection\nis achieved by pose clustering using outlier pose detection with Dirichlet\nprocess mixtures and the Bayes factor. The proposed schemes are validated with\nlarge-scale human pose datasets.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 13:12:28 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Ukita", "Norimichi", ""], ["Uematsu", "Yusuke", ""]]}, {"id": "1906.01415", "submitter": "Satoshi Tsutsui", "authors": "Satoshi Tsutsui, Dian Zhi, Md Alimoor Reza, David Crandall, Chen Yu", "title": "Active Object Manipulation Facilitates Visual Object Learning: An\n  Egocentric Vision Study", "comments": "Accepted at 2019 CVPR Workshop on Egocentric Perception, Interaction\n  and Computing (EPIC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the remarkable ability of the infant visual learning system, a\nrecent study collected first-person images from children to analyze the\n`training data' that they receive. We conduct a follow-up study that\ninvestigates two additional directions. First, given that infants can quickly\nlearn to recognize a new object without much supervision (i.e. few-shot\nlearning), we limit the number of training images. Second, we investigate how\nchildren control the supervision signals they receive during learning based on\nhand manipulation of objects. Our experimental results suggest that supervision\nwith hand manipulation is better than without hands, and the trend is\nconsistent even when a small number of images is available.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 13:32:28 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Tsutsui", "Satoshi", ""], ["Zhi", "Dian", ""], ["Reza", "Md Alimoor", ""], ["Crandall", "David", ""], ["Yu", "Chen", ""]]}, {"id": "1906.01452", "submitter": "Lin Ma", "authors": "Wei Zhang and Bairui Wang and Lin Ma and Wei Liu", "title": "Reconstruct and Represent Video Contents for Captioning via\n  Reinforcement Learning", "comments": "Accepted by TPAMI. arXiv admin note: substantial text overlap with\n  arXiv:1803.11438", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the problem of describing visual contents of a video sequence\nwith natural language is addressed. Unlike previous video captioning work\nmainly exploiting the cues of video contents to make a language description, we\npropose a reconstruction network (RecNet) in a novel\nencoder-decoder-reconstructor architecture, which leverages both forward (video\nto sentence) and backward (sentence to video) flows for video captioning.\nSpecifically, the encoder-decoder component makes use of the forward flow to\nproduce a sentence description based on the encoded video semantic features.\nTwo types of reconstructors are subsequently proposed to employ the backward\nflow and reproduce the video features from local and global perspectives,\nrespectively, capitalizing on the hidden state sequence generated by the\ndecoder. Moreover, in order to make a comprehensive reconstruction of the video\nfeatures, we propose to fuse the two types of reconstructors together. The\ngeneration loss yielded by the encoder-decoder component and the reconstruction\nloss introduced by the reconstructor are jointly cast into training the\nproposed RecNet in an end-to-end fashion. Furthermore, the RecNet is fine-tuned\nby CIDEr optimization via reinforcement learning, which significantly boosts\nthe captioning performance. Experimental results on benchmark datasets\ndemonstrate that the proposed reconstructor can boost the performance of video\ncaptioning consistently.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 06:04:00 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Zhang", "Wei", ""], ["Wang", "Bairui", ""], ["Ma", "Lin", ""], ["Liu", "Wei", ""]]}, {"id": "1906.01466", "submitter": "Raul Gomez", "authors": "Raul Gomez, Ali Furkan Biten, Lluis Gomez, Jaume Gibert, Mar\\c{c}al\n  Rusi\\~nol and Dimosthenis Karatzas", "title": "Selective Style Transfer for Text", "comments": "Accepted in ICDAR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the possibilities of image style transfer applied to text\nmaintaining the original transcriptions. Results on different text domains\n(scene text, machine printed text and handwritten text) and cross modal results\ndemonstrate that this is feasible, and open different research lines.\nFurthermore, two architectures for selective style transfer, which means\ntransferring style to only desired image pixels, are proposed. Finally, scene\ntext selective style transfer is evaluated as a data augmentation technique to\nexpand scene text detection datasets, resulting in a boost of text detectors\nperformance. Our implementation of the described models is publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 14:16:19 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Gomez", "Raul", ""], ["Biten", "Ali Furkan", ""], ["Gomez", "Lluis", ""], ["Gibert", "Jaume", ""], ["Rusi\u00f1ol", "Mar\u00e7al", ""], ["Karatzas", "Dimosthenis", ""]]}, {"id": "1906.01478", "submitter": "Vegard Antun", "authors": "Laura Thesing, Vegard Antun and Anders C. Hansen", "title": "What do AI algorithms actually learn? - On false structures in deep\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two big unsolved mathematical questions in artificial intelligence\n(AI): (1) Why is deep learning so successful in classification problems and (2)\nwhy are neural nets based on deep learning at the same time universally\nunstable, where the instabilities make the networks vulnerable to adversarial\nattacks. We present a solution to these questions that can be summed up in two\nwords; false structures. Indeed, deep learning does not learn the original\nstructures that humans use when recognising images (cats have whiskers, paws,\nfur, pointy ears, etc), but rather different false structures that correlate\nwith the original structure and hence yield the success. However, the false\nstructure, unlike the original structure, is unstable. The false structure is\nsimpler than the original structure, hence easier to learn with less data and\nthe numerical algorithm used in the training will more easily converge to the\nneural network that captures the false structure. We formally define the\nconcept of false structures and formulate the solution as a conjecture. Given\nthat trained neural networks always are computed with approximations, this\nconjecture can only be established through a combination of theoretical and\ncomputational results similar to how one establishes a postulate in theoretical\nphysics (e.g. the speed of light is constant). Establishing the conjecture\nfully will require a vast research program characterising the false structures.\nWe provide the foundations for such a program establishing the existence of the\nfalse structures in practice. Finally, we discuss the far reaching consequences\nthe existence of the false structures has on state-of-the-art AI and Smale's\n18th problem.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 14:35:32 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Thesing", "Laura", ""], ["Antun", "Vegard", ""], ["Hansen", "Anders C.", ""]]}, {"id": "1906.01480", "submitter": "Rahil Mehrizi", "authors": "Rahil Mehrizi, Xi Peng, Shaoting Zhang, Ruisong Liao, Kang Li", "title": "Automatic Health Problem Detection from Gait Videos Using Deep Neural\n  Networks", "comments": "The claims in the conclusion are disproportionate to the merits of\n  the research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this study is developing an automatic system for detection of\ngait-related health problems using Deep Neural Networks (DNNs). The proposed\nsystem takes a video of patients as the input and estimates their 3D body pose\nusing a DNN based method. Our code is publicly available at\nhttps://github.com/rmehrizi/multi-view-pose-estimation. The resulting 3D body\npose time series are then analyzed in a classifier, which classifies input gait\nvideos into four different groups including Healthy, with Parkinsons disease,\nPost Stroke patient, and with orthopedic problems. The proposed system removes\nthe requirement of complex and heavy equipment and large laboratory space, and\nmakes the system practical for home use. Moreover, it does not need domain\nknowledge for feature engineering since it is capable of extracting semantic\nand high level features from the input data. The experimental results showed\nthe classification accuracy of 56% to 96% for different groups. Furthermore,\nonly 1 out of 25 healthy subjects were misclassified (False positive), and only\n1 out of 70 patients were classified as a healthy subject (False negative).\nThis study presents a starting point toward a powerful tool for automatic\nclassification of gait disorders and can be used as a basis for future\napplications of Deep Learning in clinical gait analysis. Since the system uses\ndigital cameras as the only required equipment, it can be employed in domestic\nenvironment of patients and elderly people for consistent gait monitoring and\nearly detection of gait alterations.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 14:37:00 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2020 13:47:56 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Mehrizi", "Rahil", ""], ["Peng", "Xi", ""], ["Zhang", "Shaoting", ""], ["Liao", "Ruisong", ""], ["Li", "Kang", ""]]}, {"id": "1906.01493", "submitter": "Indranil Chakraborty", "authors": "Indranil Chakraborty, Deboleena Roy, Isha Garg, Aayush Ankit and\n  Kaushik Roy", "title": "Constructing Energy-efficient Mixed-precision Neural Networks through\n  Principal Component Analysis for Edge Intelligence", "comments": "14 pages, 4 figures, 8 tables", "journal-ref": "Nature Machine Intelligence, 2, 43-55 (2020)", "doi": "10.1038/s42256-019-0134-0", "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The `Internet of Things' has brought increased demand for AI-based edge\ncomputing in applications ranging from healthcare monitoring systems to\nautonomous vehicles. Quantization is a powerful tool to address the growing\ncomputational cost of such applications, and yields significant compression\nover full-precision networks. However, quantization can result in substantial\nloss of performance for complex image classification tasks. To address this, we\npropose a Principal Component Analysis (PCA) driven methodology to identify the\nimportant layers of a binary network, and design mixed-precision networks. The\nproposed Hybrid-Net achieves a more than 10% improvement in classification\naccuracy over binary networks such as XNOR-Net for ResNet and VGG architectures\non CIFAR-100 and ImageNet datasets while still achieving up to 94% of the\nenergy-efficiency of XNOR-Nets. This work furthers the feasibility of using\nhighly compressed neural networks for energy-efficient neural computing in edge\ndevices.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 15:02:30 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 02:05:07 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Chakraborty", "Indranil", ""], ["Roy", "Deboleena", ""], ["Garg", "Isha", ""], ["Ankit", "Aayush", ""], ["Roy", "Kaushik", ""]]}, {"id": "1906.01524", "submitter": "Ohad Fried", "authors": "Ohad Fried, Ayush Tewari, Michael Zollh\\\"ofer, Adam Finkelstein, Eli\n  Shechtman, Dan B Goldman, Kyle Genova, Zeyu Jin, Christian Theobalt, Maneesh\n  Agrawala", "title": "Text-based Editing of Talking-head Video", "comments": "A version with higher resolution images can be downloaded from the\n  authors' website", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Editing talking-head video to change the speech content or to remove filler\nwords is challenging. We propose a novel method to edit talking-head video\nbased on its transcript to produce a realistic output video in which the\ndialogue of the speaker has been modified, while maintaining a seamless\naudio-visual flow (i.e. no jump cuts). Our method automatically annotates an\ninput talking-head video with phonemes, visemes, 3D face pose and geometry,\nreflectance, expression and scene illumination per frame. To edit a video, the\nuser has to only edit the transcript, and an optimization strategy then chooses\nsegments of the input corpus as base material. The annotated parameters\ncorresponding to the selected segments are seamlessly stitched together and\nused to produce an intermediate video representation in which the lower half of\nthe face is rendered with a parametric face model. Finally, a recurrent video\ngeneration network transforms this representation to a photorealistic video\nthat matches the edited transcript. We demonstrate a large variety of edits,\nsuch as the addition, removal, and alteration of words, as well as convincing\nlanguage translation and full sentence synthesis.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 15:35:16 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Fried", "Ohad", ""], ["Tewari", "Ayush", ""], ["Zollh\u00f6fer", "Michael", ""], ["Finkelstein", "Adam", ""], ["Shechtman", "Eli", ""], ["Goldman", "Dan B", ""], ["Genova", "Kyle", ""], ["Jin", "Zeyu", ""], ["Theobalt", "Christian", ""], ["Agrawala", "Maneesh", ""]]}, {"id": "1906.01526", "submitter": "Oren Katzir", "authors": "Oren Katzir, Dani Lischinski, Daniel Cohen-Or", "title": "Cross-Domain Cascaded Deep Feature Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years we have witnessed tremendous progress in unpaired\nimage-to-image translation methods, propelled by the emergence of DNNs and\nadversarial training strategies. However, most existing methods focus on\ntransfer of style and appearance, rather than on shape translation. The latter\ntask is challenging, due to its intricate non-local nature, which calls for\nadditional supervision. We mitigate this by descending the deep layers of a\npre-trained network, where the deep features contain more semantics, and\napplying the translation from and between these deep features. Specifically, we\nleverage VGG, which is a classification network, pre-trained with large-scale\nsemantic supervision. Our translation is performed in a cascaded,\ndeep-to-shallow, fashion, along the deep feature hierarchy: we first translate\nbetween the deepest layers that encode the higher-level semantic content of the\nimage, proceeding to translate the shallower layers, conditioned on the deeper\nones. We show that our method is able to translate between different domains,\nwhich exhibit significantly different shapes. We evaluate our method both\nqualitatively and quantitatively and compare it to state-of-the-art\nimage-to-image translation methods. Our code and trained models will be made\navailable.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 15:37:09 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Katzir", "Oren", ""], ["Lischinski", "Dani", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "1906.01529", "submitter": "Zhengwei Wang", "authors": "Zhengwei Wang, Qi She, Tomas E. Ward", "title": "Generative Adversarial Networks in Computer Vision: A Survey and\n  Taxonomy", "comments": "Accepted by ACM Computing Surveys, 23 November 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have been extensively studied in the\npast few years. Arguably their most significant impact has been in the area of\ncomputer vision where great advances have been made in challenges such as\nplausible image generation, image-to-image translation, facial attribute\nmanipulation and similar domains. Despite the significant successes achieved to\ndate, applying GANs to real-world problems still poses significant challenges,\nthree of which we focus on here. These are: (1) the generation of high quality\nimages, (2) diversity of image generation, and (3) stable training. Focusing on\nthe degree to which popular GAN technologies have made progress against these\nchallenges, we provide a detailed review of the state of the art in GAN-related\nresearch in the published scientific literature. We further structure this\nreview through a convenient taxonomy we have adopted based on variations in GAN\narchitectures and loss functions. While several reviews for GANs have been\npresented to date, none have considered the status of this field based on their\nprogress towards addressing practical challenges relevant to computer vision.\nAccordingly, we review and critically discuss the most popular\narchitecture-variant, and loss-variant GANs, for tackling these challenges. Our\nobjective is to provide an overview as well as a critical analysis of the\nstatus of GAN research in terms of relevant progress towards important computer\nvision application requirements. As we do this we also discuss the most\ncompelling applications in computer vision in which GANs have demonstrated\nconsiderable success along with some suggestions for future research\ndirections. Code related to GAN-variants studied in this work is summarized on\nhttps://github.com/sheqi/GAN_Review.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 15:40:53 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 19:20:16 GMT"}, {"version": "v3", "created": "Mon, 3 Feb 2020 01:04:42 GMT"}, {"version": "v4", "created": "Mon, 15 Jun 2020 16:05:14 GMT"}, {"version": "v5", "created": "Mon, 21 Dec 2020 23:51:15 GMT"}, {"version": "v6", "created": "Tue, 29 Dec 2020 11:49:06 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Wang", "Zhengwei", ""], ["She", "Qi", ""], ["Ward", "Tomas E.", ""]]}, {"id": "1906.01530", "submitter": "Janosch Haber", "authors": "Janosch Haber, Tim Baumg\\\"artner, Ece Takmaz, Lieke Gelderloos, Elia\n  Bruni and Raquel Fern\\'andez", "title": "The PhotoBook Dataset: Building Common Ground through Visually-Grounded\n  Dialogue", "comments": "Updates 26-06-2019: Changed caption sizes to comply with the ACL\n  style guidelines and corrected some references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the PhotoBook dataset, a large-scale collection of\nvisually-grounded, task-oriented dialogues in English designed to investigate\nshared dialogue history accumulating during conversation. Taking inspiration\nfrom seminal work on dialogue analysis, we propose a data-collection task\nformulated as a collaborative game prompting two online participants to refer\nto images utilising both their visual context as well as previously established\nreferring expressions. We provide a detailed description of the task setup and\na thorough analysis of the 2,500 dialogues collected. To further illustrate the\nnovel features of the dataset, we propose a baseline model for reference\nresolution which uses a simple method to take into account shared information\naccumulated in a reference chain. Our results show that this information is\nparticularly important to resolve later descriptions and underline the need to\ndevelop more sophisticated models of common ground in dialogue interaction.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 15:41:32 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 17:36:47 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Haber", "Janosch", ""], ["Baumg\u00e4rtner", "Tim", ""], ["Takmaz", "Ece", ""], ["Gelderloos", "Lieke", ""], ["Bruni", "Elia", ""], ["Fern\u00e1ndez", "Raquel", ""]]}, {"id": "1906.01536", "submitter": "Yuntao Liu", "authors": "Yuntao Liu, Yong Dou, Ruochun Jin, Peng Qiao", "title": "Visual Tree Convolutional Neural Network in Image Classification", "comments": "7 pages, 2 figures, conference", "journal-ref": "2018 24th International Conference on Pattern Recognition (ICPR)", "doi": "10.1109/ICPR.2018.8546126", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image classification, Convolutional Neural Network(CNN) models have\nachieved high performance with the rapid development in deep learning. However,\nsome categories in the image datasets are more difficult to distinguished than\nothers. Improving the classification accuracy on these confused categories is\nbenefit to the overall performance. In this paper, we build a Confusion Visual\nTree(CVT) based on the confused semantic level information to identify the\nconfused categories. With the information provided by the CVT, we can lead the\nCNN training procedure to pay more attention on these confused categories.\nTherefore, we propose Visual Tree Convolutional Neural Networks(VT-CNN) based\non the original deep CNN embedded with our CVT. We evaluate our VT-CNN model on\nthe benchmark datasets CIFAR-10 and CIFAR-100. In our experiments, we build up\n3 different VT-CNN models and they obtain improvement over their based CNN\nmodels by 1.36%, 0.89% and 0.64%, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 15:49:57 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Liu", "Yuntao", ""], ["Dou", "Yong", ""], ["Jin", "Ruochun", ""], ["Qiao", "Peng", ""]]}, {"id": "1906.01540", "submitter": "Haohao Hu", "authors": "Haohao Hu, Junyi Zhu, Sascha Wirges and Martin Lauer", "title": "Localization in Aerial Imagery with Grid Maps using LocGAN", "comments": "8 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present LocGAN, our localization approach based on a\ngeo-referenced aerial imagery and LiDAR grid maps. Currently, most\nself-localization approaches relate the current sensor observations to a map\ngenerated from previously acquired data. Unfortunately, this data is not always\navailable and the generated maps are usually sensor setup specific. Global\nNavigation Satellite Systems (GNSS) can overcome this problem. However, they\nare not always reliable especially in urban areas due to multi-path and\nshadowing effects. Since aerial imagery is usually available, we can use it as\nprior information. To match aerial images with grid maps, we use conditional\nGenerative Adversarial Networks (cGANs) which transform aerial images to the\ngrid map domain. The transformation between the predicted and measured grid map\nis estimated using a localization network (LocNet). Given the geo-referenced\naerial image transformation the vehicle pose can be estimated. Evaluations\nperformed on the data recorded in region Karlsruhe, Germany show that our\nLocGAN approach provides reliable global localization results.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 15:53:20 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 14:48:16 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Hu", "Haohao", ""], ["Zhu", "Junyi", ""], ["Wirges", "Sascha", ""], ["Lauer", "Martin", ""]]}, {"id": "1906.01542", "submitter": "Jordi Pont-Tuset", "authors": "Jordi Pont-Tuset and Michael Gygli and Vittorio Ferrari", "title": "Natural Vocabulary Emerges from Free-Form Annotations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach for annotating object classes using free-form text\nwritten by undirected and untrained annotators. Free-form labeling is natural\nfor annotators, they intuitively provide very specific and exhaustive labels,\nand no training stage is necessary. We first collect 729 labels on 15k images\nusing 124 different annotators. Then we automatically enrich the structure of\nthese free-form annotations by discovering a natural vocabulary of 4020 classes\nwithin them. This vocabulary represents the natural distribution of objects\nwell and is learned directly from data, instead of being an educated guess done\nbefore collecting any labels. Hence, the natural vocabulary emerges from a\nlarge mass of free-form annotations. To do so, we (i) map the raw input strings\nto entities in an ontology of physical objects (which gives them an unambiguous\nmeaning); and (ii) leverage inter-annotator co-occurrences, as well as biases\nand knowledge specific to individual annotators. Finally, we also automatically\nextract natural vocabularies of reduced size that have high object coverage\nwhile remaining specific. These reduced vocabularies represent the natural\ndistribution of objects much better than commonly used predefined vocabularies.\nMoreover, they feature more uniform sample distribution over classes.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 15:57:46 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Pont-Tuset", "Jordi", ""], ["Gygli", "Michael", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1906.01551", "submitter": "Litu Rout", "authors": "Litu Rout, Priya Mariam Raju, Deepak Mishra, Rama Krishna Sai\n  Subrahmanyam Gorthi", "title": "Learning Rotation Adaptive Correlation Filters in Robust Visual Object\n  Tracking", "comments": "Published in ACCV 2018", "journal-ref": "ACCV: Asian Conference on Computer Vision 2018", "doi": "10.1007/978-3-030-20890-5_41", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Visual object tracking is one of the major challenges in the field of\ncomputer vision. Correlation Filter (CF) trackers are one of the most widely\nused categories in tracking. Though numerous tracking algorithms based on CFs\nare available today, most of them fail to efficiently detect the object in an\nunconstrained environment with dynamically changing object appearance. In order\nto tackle such challenges, the existing strategies often rely on a particular\nset of algorithms. Here, we propose a robust framework that offers the\nprovision to incorporate illumination and rotation invariance in the standard\nDiscriminative Correlation Filter (DCF) formulation. We also supervise the\ndetection stage of DCF trackers by eliminating false positives in the\nconvolution response map. Further, we demonstrate the impact of displacement\nconsistency on CF trackers. The generality and efficiency of the proposed\nframework is illustrated by integrating our contributions into two\nstate-of-the-art CF trackers: SRDCF and ECO. As per the comprehensive\nexperiments on the VOT2016 dataset, our top trackers show substantial\nimprovement of 14.7% and 6.41% in robustness, 11.4% and 1.71% in Average\nExpected Overlap (AEO) over the baseline SRDCF and ECO, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 16:10:49 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Rout", "Litu", ""], ["Raju", "Priya Mariam", ""], ["Mishra", "Deepak", ""], ["Gorthi", "Rama Krishna Sai Subrahmanyam", ""]]}, {"id": "1906.01558", "submitter": "Drew Linsley", "authors": "Junkyung Kim, Drew Linsley, Kalpit Thakkar, Thomas Serre", "title": "Disentangling neural mechanisms for perceptual grouping", "comments": "Published in ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forming perceptual groups and individuating objects in visual scenes is an\nessential step towards visual intelligence. This ability is thought to arise in\nthe brain from computations implemented by bottom-up, horizontal, and top-down\nconnections between neurons. However, the relative contributions of these\nconnections to perceptual grouping are poorly understood. We address this\nquestion by systematically evaluating neural network architectures featuring\ncombinations bottom-up, horizontal, and top-down connections on two synthetic\nvisual tasks, which stress low-level \"Gestalt\" vs. high-level object cues for\nperceptual grouping. We show that increasing the difficulty of either task\nstrains learning for networks that rely solely on bottom-up connections.\nHorizontal connections resolve straining on tasks with Gestalt cues by\nsupporting incremental grouping, whereas top-down connections rescue learning\non tasks with high-level object cues by modifying coarse predictions about the\nposition of the target object. Our findings dissociate the computational roles\nof bottom-up, horizontal and top-down connectivity, and demonstrate how a model\nfeaturing all of these interactions can more flexibly learn to form perceptual\ngroups.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 16:21:46 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 15:53:55 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Kim", "Junkyung", ""], ["Linsley", "Drew", ""], ["Thakkar", "Kalpit", ""], ["Serre", "Thomas", ""]]}, {"id": "1906.01566", "submitter": "Yuanfu Luo", "authors": "Yuanfu Luo and Panpan Cai and David Hsu and Wee Sun Lee", "title": "GAMMA: A General Agent Motion Prediction Model for Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving in mixed traffic requires reliable motion prediction of\nnearby traffic agents such as pedestrians, bicycles, cars, buses, etc.. This\nprediction problem is extremely challenging because of the diverse dynamics and\ngeometry of traffic agents, complex road conditions, and intensive interactions\namong the agents. In this paper, we proposed GAMMA, a general agent motion\nprediction model for autonomous driving, that can predict the motion of\nheterogeneous traffic agents with different kinematics, geometry, human agents'\ninner states, etc.. GAMMA formalizes motion prediction as geometric\noptimization in the velocity space, and integrates physical constraints and\nhuman inner states into this unified framework. Our results show that GAMMA\noutperforms state-of-the-art approaches significantly on diverse real-world\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 16:33:36 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 11:23:52 GMT"}, {"version": "v3", "created": "Wed, 23 Oct 2019 03:39:30 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Luo", "Yuanfu", ""], ["Cai", "Panpan", ""], ["Hsu", "David", ""], ["Lee", "Wee Sun", ""]]}, {"id": "1906.01568", "submitter": "Shangzhe Wu", "authors": "Shangzhe Wu, Christian Rupprecht, Andrea Vedaldi", "title": "Photo-Geometric Autoencoding to Learn 3D Objects from Unlabelled Images", "comments": "Appendix included, 17 pages. Project page:\n  https://elliottwu.com/projects/unsup3d", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that generative models can be used to capture visual geometry\nconstraints statistically. We use this fact to infer the 3D shape of object\ncategories from raw single-view images. Differently from prior work, we use no\nexternal supervision, nor do we use multiple views or videos of the objects. We\nachieve this by a simple reconstruction task, exploiting the symmetry of the\nobjects' shape and albedo. Specifically, given a single image of the object\nseen from an arbitrary viewpoint, our model predicts a symmetric canonical\nview, the corresponding 3D shape and a viewpoint transformation, and trains\nwith the goal of reconstructing the input view, resembling an auto-encoder. Our\nexperiments show that this method can recover the 3D shape of human faces, cat\nfaces, and cars from single view images, without supervision. On benchmarks, we\ndemonstrate superior accuracy compared to other methods that use supervision at\nthe level of 2D image correspondences.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 16:36:07 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Wu", "Shangzhe", ""], ["Rupprecht", "Christian", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1906.01592", "submitter": "Chu Wang", "authors": "Chu Wang, Marcello Pelillo, Kaleem Siddiqi", "title": "Dominant Set Clustering and Pooling for Multi-View 3D Object Recognition", "comments": "British Machine Vision Conference 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  View based strategies for 3D object recognition have proven to be very\nsuccessful. The state-of-the-art methods now achieve over 90% correct category\nlevel recognition performance on appearance images. We improve upon these\nmethods by introducing a view clustering and pooling layer based on dominant\nsets. The key idea is to pool information from views which are similar and thus\nbelong to the same cluster. The pooled feature vectors are then fed as inputs\nto the same layer, in a recurrent fashion. This recurrent clustering and\npooling module, when inserted in an off-the-shelf pretrained CNN, boosts\nperformance for multi-view 3D object recognition, achieving a new state of the\nart test set recognition accuracy of 93.8% on the ModelNet 40 database. We also\nexplore a fast approximate learning strategy for our cluster-pooling CNN,\nwhich, while sacrificing end-to-end learning, greatly improves its training\nefficiency with only a slight reduction of recognition accuracy to 93.3%. Our\nimplementation is available at https://github.com/fate3439/dscnn.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 17:09:23 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Wang", "Chu", ""], ["Pelillo", "Marcello", ""], ["Siddiqi", "Kaleem", ""]]}, {"id": "1906.01618", "submitter": "Vincent Sitzmann", "authors": "Vincent Sitzmann, Michael Zollh\\\"ofer, Gordon Wetzstein", "title": "Scene Representation Networks: Continuous 3D-Structure-Aware Neural\n  Scene Representations", "comments": "Video: https://youtu.be/6vMEBWD8O20 Project page:\n  https://vsitzmann.github.io/srns/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning with generative models has the potential of discovering\nrich representations of 3D scenes. While geometric deep learning has explored\n3D-structure-aware representations of scene geometry, these models typically\nrequire explicit 3D supervision. Emerging neural scene representations can be\ntrained only with posed 2D images, but existing methods ignore the\nthree-dimensional structure of scenes. We propose Scene Representation Networks\n(SRNs), a continuous, 3D-structure-aware scene representation that encodes both\ngeometry and appearance. SRNs represent scenes as continuous functions that map\nworld coordinates to a feature representation of local scene properties. By\nformulating the image formation as a differentiable ray-marching algorithm,\nSRNs can be trained end-to-end from only 2D images and their camera poses,\nwithout access to depth or shape. This formulation naturally generalizes across\nscenes, learning powerful geometry and appearance priors in the process. We\ndemonstrate the potential of SRNs by evaluating them for novel view synthesis,\nfew-shot reconstruction, joint shape and appearance interpolation, and\nunsupervised discovery of a non-rigid face model.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 17:53:11 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 23:06:36 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Sitzmann", "Vincent", ""], ["Zollh\u00f6fer", "Michael", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "1906.01620", "submitter": "Fredrik K. Gustafsson", "authors": "Fredrik K. Gustafsson, Martin Danelljan, Thomas B. Sch\\\"on", "title": "Evaluating Scalable Bayesian Deep Learning Methods for Robust Computer\n  Vision", "comments": "CVPR Workshops 2020. Code is available at\n  https://github.com/fregu856/evaluating_bdl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep neural networks have become the go-to approach in computer vision,\nthe vast majority of these models fail to properly capture the uncertainty\ninherent in their predictions. Estimating this predictive uncertainty can be\ncrucial, for example in automotive applications. In Bayesian deep learning,\npredictive uncertainty is commonly decomposed into the distinct types of\naleatoric and epistemic uncertainty. The former can be estimated by letting a\nneural network output the parameters of a certain probability distribution.\nEpistemic uncertainty estimation is a more challenging problem, and while\ndifferent scalable methods recently have emerged, no extensive comparison has\nbeen performed in a real-world setting. We therefore accept this task and\npropose a comprehensive evaluation framework for scalable epistemic uncertainty\nestimation methods in deep learning. Our proposed framework is specifically\ndesigned to test the robustness required in real-world computer vision\napplications. We also apply this framework to provide the first properly\nextensive and conclusive comparison of the two current state-of-the-art\nscalable methods: ensembling and MC-dropout. Our comparison demonstrates that\nensembling consistently provides more reliable and practically useful\nuncertainty estimates. Code is available at\nhttps://github.com/fregu856/evaluating_bdl.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 17:54:20 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 10:48:25 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2020 12:49:00 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Gustafsson", "Fredrik K.", ""], ["Danelljan", "Martin", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1906.01675", "submitter": "Robert Wagner", "authors": "Robert Wagner, Daniel Crispell, Patrick Feeney, Joe Mundy", "title": "4-D Scene Alignment in Surveillance Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing robust activity detectors for fixed camera surveillance video\nrequires knowledge of the 3-D scene. This paper presents an automatic camera\ncalibration process that provides a mechanism to reason about the spatial\nproximity between objects at different times. It combines a CNN-based camera\npose estimator with a vertical scale provided by pedestrian observations to\nestablish the 4-D scene geometry. Unlike some previous methods, the people do\nnot need to be tracked nor do the head and feet need to be explicitly detected.\nIt is robust to individual height variations and camera parameter estimation\nerrors.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 18:39:20 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 14:16:19 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Wagner", "Robert", ""], ["Crispell", "Daniel", ""], ["Feeney", "Patrick", ""], ["Mundy", "Joe", ""]]}, {"id": "1906.01737", "submitter": "Grace Chu", "authors": "Grace Chu, Brian Potetz, Weijun Wang, Andrew Howard, Yang Song,\n  Fernando Brucher, Thomas Leung, Hartwig Adam", "title": "Geo-Aware Networks for Fine-Grained Recognition", "comments": "ICCVW 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained recognition distinguishes among categories with subtle visual\ndifferences. In order to differentiate between these challenging visual\ncategories, it is helpful to leverage additional information. Geolocation is a\nrich source of additional information that can be used to improve fine-grained\nclassification accuracy, but has been understudied. Our contributions to this\nfield are twofold. First, to the best of our knowledge, this is the first paper\nwhich systematically examined various ways of incorporating geolocation\ninformation into fine-grained image classification through the use of\ngeolocation priors, post-processing or feature modulation. Secondly, to\novercome the situation where no fine-grained dataset has complete geolocation\ninformation, we release two fine-grained datasets with geolocation by providing\ncomplementary information to existing popular datasets - iNaturalist and\nYFCC100M. By leveraging geolocation information we improve top-1 accuracy in\niNaturalist from 70.1% to 79.0% for a strong baseline image-only model.\nComparing several models, we found that best performance was achieved by a\npost-processing model that consumed the output of the image-only baseline\nalongside geolocation. However, for a resource-constrained model (MobileNetV2),\nperformance was better with a feature modulation model that trains jointly over\npixels and geolocation: accuracy increased from 59.6% to 72.2%. Our work makes\na strong case for incorporating geolocation information in fine-grained\nrecognition models for both server and on-device.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 21:53:07 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 21:56:58 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Chu", "Grace", ""], ["Potetz", "Brian", ""], ["Wang", "Weijun", ""], ["Howard", "Andrew", ""], ["Song", "Yang", ""], ["Brucher", "Fernando", ""], ["Leung", "Thomas", ""], ["Adam", "Hartwig", ""]]}, {"id": "1906.01751", "submitter": "Keiller Nogueira", "authors": "Keiller Nogueira and Jocelyn Chanussot and Mauro Dalla Mura and\n  Jefersson A. dos Santos", "title": "An Introduction to Deep Morphological Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent impressive results of deep learning-based methods on computer\nvision applications brought fresh air to the research and industrial community.\nThis success is mainly due to the process that allows those methods to learn\ndata-driven features, generally based upon linear operations. However, in some\nscenarios, such operations do not have a good performance because of their\ninherited process that blurs edges, losing notions of corners, borders, and\ngeometry of objects. Overcoming this, non-linear operations, such as\nmorphological ones, may preserve such properties of the objects, being\npreferable and even state-of-the-art in some applications. Encouraged by this,\nin this work, we propose a novel network, called Deep Morphological Network\n(DeepMorphNet), capable of doing non-linear morphological operations while\nperforming the feature learning process by optimizing the structuring elements.\nThe DeepMorphNets can be trained and optimized end-to-end using traditional\nexisting techniques commonly employed in the training of deep learning\napproaches. A systematic evaluation of the proposed algorithm is conducted\nusing two synthetic and two traditional image classification datasets. Results\nshow that the proposed DeepMorphNets is a promising technique that can learn\ndistinct features when compared to the ones learned by current deep learning\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 23:13:10 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 18:33:01 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Nogueira", "Keiller", ""], ["Chanussot", "Jocelyn", ""], ["Mura", "Mauro Dalla", ""], ["Santos", "Jefersson A. dos", ""]]}, {"id": "1906.01769", "submitter": "Karthikeyan Natesan Ramamurthy", "authors": "Anirudh Som and Hongjun Choi and Karthikeyan Natesan Ramamurthy and\n  Matthew Buman and Pavan Turaga", "title": "PI-Net: A Deep Learning Approach to Extract Topological Persistence\n  Images", "comments": "10 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological features such as persistence diagrams and their functional\napproximations like persistence images (PIs) have been showing substantial\npromise for machine learning and computer vision applications. This is greatly\nattributed to the robustness topological representations provide against\ndifferent types of physical nuisance variables seen in real-world data, such as\nview-point, illumination, and more. However, key bottlenecks to their large\nscale adoption are computational expenditure and difficulty incorporating them\nin a differentiable architecture. We take an important step in this paper to\nmitigate these bottlenecks by proposing a novel one-step approach to generate\nPIs directly from the input data. We design two separate convolutional neural\nnetwork architectures, one designed to take in multi-variate time series\nsignals as input and another that accepts multi-channel images as input. We\ncall these networks Signal PI-Net and Image PI-Net respectively. To the best of\nour knowledge, we are the first to propose the use of deep learning for\ncomputing topological features directly from data. We explore the use of the\nproposed PI-Net architectures on two applications: human activity recognition\nusing tri-axial accelerometer sensor data and image classification. We\ndemonstrate the ease of fusion of PIs in supervised deep learning architectures\nand speed up of several orders of magnitude for extracting PIs from data. Our\ncode is available at https://github.com/anirudhsom/PI-Net.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 00:54:06 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 14:51:14 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Som", "Anirudh", ""], ["Choi", "Hongjun", ""], ["Ramamurthy", "Karthikeyan Natesan", ""], ["Buman", "Matthew", ""], ["Turaga", "Pavan", ""]]}, {"id": "1906.01784", "submitter": "Daqing Liu", "authors": "Richang Hong, Daqing Liu, Xiaoyu Mo, Xiangnan He, Hanwang Zhang", "title": "Learning to Compose and Reason with Language Tree Structures for Visual\n  Grounding", "comments": "Accepted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (T-PAMI)", "journal-ref": null, "doi": "10.1109/TPAMI.2019.2911066", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grounding natural language in images, such as localizing \"the black dog on\nthe left of the tree\", is one of the core problems in artificial intelligence,\nas it needs to comprehend the fine-grained and compositional language space.\nHowever, existing solutions rely on the association between the holistic\nlanguage features and visual features, while neglect the nature of\ncompositional reasoning implied in the language. In this paper, we propose a\nnatural language grounding model that can automatically compose a binary tree\nstructure for parsing the language and then perform visual reasoning along the\ntree in a bottom-up fashion. We call our model RVG-TREE: Recursive Grounding\nTree, which is inspired by the intuition that any language expression can be\nrecursively decomposed into two constituent parts, and the grounding confidence\nscore can be recursively accumulated by calculating their grounding scores\nreturned by sub-trees. RVG-TREE can be trained end-to-end by using the\nStraight-Through Gumbel-Softmax estimator that allows the gradients from the\ncontinuous score functions passing through the discrete tree construction.\nExperiments on several benchmarks show that our model achieves the\nstate-of-the-art performance with more explainable reasoning.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 02:03:55 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Hong", "Richang", ""], ["Liu", "Daqing", ""], ["Mo", "Xiaoyu", ""], ["He", "Xiangnan", ""], ["Zhang", "Hanwang", ""]]}, {"id": "1906.01792", "submitter": "Chengyuan Zhang", "authors": "Chengyuan Zhang, Lei Zhu, Shichao Zhang", "title": "PAC-GAN: An Effective Pose Augmentation Scheme for Unsupervised\n  Cross-View Person Re-identification", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (person Re-Id) aims to retrieve the pedestrian\nimages of a same person that captured by disjoint and non-overlapping cameras.\nLots of researchers recently focuse on this hot issue and propose deep learning\nbased methods to enhance the recognition rate in a supervised or unsupervised\nmanner. However, two limitations that cannot be ignored: firstly, compared with\nother image retrieval benchmarks, the size of existing person Re-Id datasets\nare far from meeting the requirement, which cannot provide sufficient\npedestrian samples for the training of deep model; secondly, the samples in\nexisting datasets do not have sufficient human motions or postures coverage to\nprovide more priori knowledges for learning. In this paper, we introduce a\nnovel unsupervised pose augmentation cross-view person Re-Id scheme called\nPAC-GAN to overcome these limitations. We firstly present the formal definition\nof cross-view pose augmentation and then propose the framework of PAC-GAN that\nis a novel conditional generative adversarial network (CGAN) based approach to\nimprove the performance of unsupervised corss-view person Re-Id. Specifically,\nThe pose generation model in PAC-GAN called CPG-Net is to generate enough\nquantity of pose-rich samples from original image and skeleton samples. The\npose augmentation dataset is produced by combining the synthesized pose-rich\nsamples with the original samples, which is fed into the corss-view person\nRe-Id model named Cross-GAN. Besides, we use weight-sharing strategy in the\nCPG-Net to improve the quality of new generated samples. To the best of our\nknowledge, we are the first try to enhance the unsupervised cross-view person\nRe-Id by pose augmentation, and the results of extensive experiments show that\nthe proposed scheme can combat the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 02:38:26 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Zhang", "Chengyuan", ""], ["Zhu", "Lei", ""], ["Zhang", "Shichao", ""]]}, {"id": "1906.01795", "submitter": "Ningning Zhao", "authors": "Ningning Zhao, Nuo Tong, Dan Ruan and Ke Sheng", "title": "Fully Automated Pancreas Segmentation with Two-stage 3D Convolutional\n  Neural Networks", "comments": "This paper has been accepted by MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the fact that pancreas is an abdominal organ with very large\nvariations in shape and size, automatic and accurate pancreas segmentation can\nbe challenging for medical image analysis. In this work, we proposed a fully\nautomated two stage framework for pancreas segmentation based on convolutional\nneural networks (CNN). In the first stage, a U-Net is trained for the\ndown-sampled 3D volume segmentation. Then a candidate region covering the\npancreas is extracted from the estimated labels. Motivated by the superior\nperformance reported by renowned region based CNN, in the second stage, another\n3D U-Net is trained on the candidate region generated in the first stage. We\nevaluated the performance of the proposed method on the NIH computed tomography\n(CT) dataset, and verified its superiority over other state-of-the-art 2D and\n3D approaches for pancreas segmentation in terms of dice-sorensen coefficient\n(DSC) accuracy in testing. The mean DSC of the proposed method is 85.99%.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 02:48:24 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 22:29:47 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Zhao", "Ningning", ""], ["Tong", "Nuo", ""], ["Ruan", "Dan", ""], ["Sheng", "Ke", ""]]}, {"id": "1906.01796", "submitter": "Changxing Ding", "authors": "Chenhong Zhou, Changxing Ding, Xinchao Wang, Zhentai Lu, Dacheng Tao", "title": "One-pass Multi-task Networks with Cross-task Guided Attention for Brain\n  Tumor Segmentation", "comments": "14 pages, 7 figures, To appear in IEEE Transactions on Image\n  Processing", "journal-ref": null, "doi": "10.1109/TIP.2020.2973510", "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class imbalance has emerged as one of the major challenges for medical image\nsegmentation. The model cascade (MC) strategy significantly alleviates the\nclass imbalance issue via running a set of individual deep models for\ncoarse-to-fine segmentation. Despite its outstanding performance, however, this\nmethod leads to undesired system complexity and also ignores the correlation\namong the models. To handle these flaws, we propose a light-weight deep model,\ni.e., the One-pass Multi-task Network (OM-Net) to solve class imbalance better\nthan MC does, while requiring only one-pass computation. First, OM-Net\nintegrates the separate segmentation tasks into one deep model, which consists\nof shared parameters to learn joint features, as well as task-specific\nparameters to learn discriminative features. Second, to more effectively\noptimize OM-Net, we take advantage of the correlation among tasks to design\nboth an online training data transfer strategy and a curriculum learning-based\ntraining strategy. Third, we further propose sharing prediction results between\ntasks and design a cross-task guided attention (CGA) module which can\nadaptively recalibrate channel-wise feature responses based on the\ncategory-specific statistics. Finally, a simple yet effective post-processing\nmethod is introduced to refine the segmentation results. Extensive experiments\nare conducted to demonstrate the effectiveness of the proposed techniques. Most\nimpressively, we achieve state-of-the-art performance on the BraTS 2015 testing\nset and BraTS 2017 online validation set. Using these proposed approaches, we\nalso won joint third place in the BraTS 2018 challenge among 64 participating\nteams. The code is publicly available at\nhttps://github.com/chenhong-zhou/OM-Net.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 02:50:08 GMT"}, {"version": "v2", "created": "Sat, 8 Feb 2020 03:34:20 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Zhou", "Chenhong", ""], ["Ding", "Changxing", ""], ["Wang", "Xinchao", ""], ["Lu", "Zhentai", ""], ["Tao", "Dacheng", ""]]}, {"id": "1906.01797", "submitter": "Deheng Qian", "authors": "Yanliang Zhu, Deheng Qian, Dongchun Ren, Huaxia Xia", "title": "StarNet: Pedestrian Trajectory Prediction using Deep Neural Network in\n  Star Topology", "comments": "Accepted by the 2019 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian trajectory prediction is crucial for many important applications.\nThis problem is a great challenge because of complicated interactions among\npedestrians. Previous methods model only the pairwise interactions between\npedestrians, which not only oversimplifies the interactions among pedestrians\nbut also is computationally inefficient. In this paper, we propose a novel\nmodel StarNet to deal with these issues. StarNet has a star topology which\nincludes a unique hub network and multiple host networks. The hub network takes\nobserved trajectories of all pedestrians to produce a comprehensive description\nof the interpersonal interactions. Then the host networks, each of which\ncorresponds to one pedestrian, consult the description and predict future\ntrajectories. The star topology gives StarNet two advantages over conventional\nmodels. First, StarNet is able to consider the collective influence among all\npedestrians in the hub network, making more accurate predictions. Second,\nStarNet is computationally efficient since the number of host network is linear\nto the number of pedestrians. Experiments on multiple public datasets\ndemonstrate that StarNet outperforms multiple state-of-the-arts by a large\nmargin in terms of both accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 03:04:51 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 03:38:19 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Zhu", "Yanliang", ""], ["Qian", "Deheng", ""], ["Ren", "Dongchun", ""], ["Xia", "Huaxia", ""]]}, {"id": "1906.01806", "submitter": "Haofu Liao", "authors": "Haofu Liao, Wei-An Lin, Jianbo Yuan, S. Kevin Zhou, Jiebo Luo", "title": "Artifact Disentanglement Network for Unsupervised Metal Artifact\n  Reduction", "comments": "This work is accepted to MICCAI 2019. An extended version can be\n  found at arXiv:1908.01104", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Current deep neural network based approaches to computed tomography (CT)\nmetal artifact reduction (MAR) are supervised methods which rely heavily on\nsynthesized data for training. However, as synthesized data may not perfectly\nsimulate the underlying physical mechanisms of CT imaging, the supervised\nmethods often generalize poorly to clinical applications. To address this\nproblem, we propose, to the best of our knowledge, the first unsupervised\nlearning approach to MAR. Specifically, we introduce a novel artifact\ndisentanglement network that enables different forms of generations and\nregularizations between the artifact-affected and artifact-free image domains\nto support unsupervised learning. Extensive experiments show that our method\nsignificantly outperforms the existing unsupervised models for image-to-image\ntranslation problems, and achieves comparable performance to existing\nsupervised models on a synthesized dataset. When applied to clinical datasets,\nour method achieves considerable improvements over the supervised models. The\nsource code of this paper is publicly available at\nhttps://github.com/liaohaofu/adn.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 03:27:02 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 07:42:29 GMT"}, {"version": "v3", "created": "Thu, 27 Jun 2019 00:03:34 GMT"}, {"version": "v4", "created": "Sat, 29 Jun 2019 22:09:22 GMT"}, {"version": "v5", "created": "Thu, 28 Nov 2019 01:33:36 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Liao", "Haofu", ""], ["Lin", "Wei-An", ""], ["Yuan", "Jianbo", ""], ["Zhou", "S. Kevin", ""], ["Luo", "Jiebo", ""]]}, {"id": "1906.01815", "submitter": "Santiago Castro", "authors": "Santiago Castro, Devamanyu Hazarika, Ver\\'onica P\\'erez-Rosas, Roger\n  Zimmermann, Rada Mihalcea and Soujanya Poria", "title": "Towards Multimodal Sarcasm Detection (An _Obviously_ Perfect Paper)", "comments": "Accepted at ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sarcasm is often expressed through several verbal and non-verbal cues, e.g.,\na change of tone, overemphasis in a word, a drawn-out syllable, or a straight\nlooking face. Most of the recent work in sarcasm detection has been carried out\non textual data. In this paper, we argue that incorporating multimodal cues can\nimprove the automatic classification of sarcasm. As a first step towards\nenabling the development of multimodal approaches for sarcasm detection, we\npropose a new sarcasm dataset, Multimodal Sarcasm Detection Dataset (MUStARD),\ncompiled from popular TV shows. MUStARD consists of audiovisual utterances\nannotated with sarcasm labels. Each utterance is accompanied by its context of\nhistorical utterances in the dialogue, which provides additional information on\nthe scenario where the utterance occurs. Our initial results show that the use\nof multimodal information can reduce the relative error rate of sarcasm\ndetection by up to 12.9% in F-score when compared to the use of individual\nmodalities. The full dataset is publicly available for use at\nhttps://github.com/soujanyaporia/MUStARD\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 04:08:47 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Castro", "Santiago", ""], ["Hazarika", "Devamanyu", ""], ["P\u00e9rez-Rosas", "Ver\u00f3nica", ""], ["Zimmermann", "Roger", ""], ["Mihalcea", "Rada", ""], ["Poria", "Soujanya", ""]]}, {"id": "1906.01821", "submitter": "Sarah Ostadabbas", "authors": "Xiaofei Huang, Alaina Martens, Emily Zimmerman, and Sarah Ostadabbas", "title": "Infant Contact-less Non-Nutritive Sucking Pattern Quantification via\n  Facial Gesture Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-nutritive sucking (NNS) is defined as the sucking action that occurs when\na finger, pacifier, or other object is placed in the baby's mouth, but there is\nno nutrient delivered. In addition to providing a sense of safety, NNS even can\nbe regarded as an indicator of infant's central nervous system development. The\nrich data, such as sucking frequency, the number of cycles, and their amplitude\nduring baby's non-nutritive sucking is important clue for judging the brain\ndevelopment of infants or preterm infants. Nowadays most researchers are\ncollecting NNS data by using some contact devices such as pressure transducers.\nHowever, such invasive contact will have a direct impact on the baby's natural\nsucking behavior, resulting in significant distortion in the collected data.\nTherefore, we propose a novel contact-less NNS data acquisition and\nquantification scheme, which leverages the facial landmarks tracking technology\nto extract the movement signals of baby's jaw from recorded baby's sucking\nvideo. Since completion of the sucking action requires a large amount of\nsynchronous coordination and neural integration of the facial muscles and the\ncranial nerves, the facial muscle movement signals accompanying baby's sucking\npacifier can indirectly replace the NNS signal. We have evaluated our method on\nvideos collected from several infants during their NNS behaviors and we have\nachieved the quantified NNS patterns closely comparable to results from visual\ninspection as well as contact-based sensor readings.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 04:45:58 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Huang", "Xiaofei", ""], ["Martens", "Alaina", ""], ["Zimmerman", "Emily", ""], ["Ostadabbas", "Sarah", ""]]}, {"id": "1906.01828", "submitter": "Fei Gao", "authors": "Fei Gao, Hyunsoo Yoon, Teresa Wu, Xianghua Chu", "title": "A Feature Transfer Enabled Multi-Task Deep Learning Model on Medical\n  Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection, segmentation and classification are three common tasks in\nmedical image analysis. Multi-task deep learning (MTL) tackles these three\ntasks jointly, which provides several advantages saving computing time and\nresources and improving robustness against overfitting. However, existing\nmultitask deep models start with each task as an individual task and integrate\nparallelly conducted tasks at the end of the architecture with one cost\nfunction. Such architecture fails to take advantage of the combined power of\nthe features from each individual task at an early stage of the training. In\nthis research, we propose a new architecture, FTMTLNet, an MTL enabled by\nfeature transferring. Traditional transfer learning deals with the same or\nsimilar task from different data sources (a.k.a. domain). The underlying\nassumption is that the knowledge gained from source domains may help the\nlearning task on the target domain. Our proposed FTMTLNet utilizes the\ndifferent tasks from the same domain. Considering features from the tasks are\ndifferent views of the domain, the combined feature maps can be well exploited\nusing knowledge from multiple views to enhance the generalizability. To\nevaluate the validity of the proposed approach, FTMTLNet is compared with\nmodels from literature including 8 classification models, 4 detection models\nand 3 segmentation models using a public full field digital mammogram dataset\nfor breast cancer diagnosis. Experimental results show that the proposed\nFTMTLNet outperforms the competing models in classification and detection and\nhas comparable results in segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 05:11:33 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Gao", "Fei", ""], ["Yoon", "Hyunsoo", ""], ["Wu", "Teresa", ""], ["Chu", "Xianghua", ""]]}, {"id": "1906.01843", "submitter": "Amir Ziai", "authors": "Amir Ziai", "title": "Detecting Kissing Scenes in a Database of Hollywood Films", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting scene types in a movie can be very useful for application such as\nvideo editing, ratings assignment, and personalization. We propose a system for\ndetecting kissing scenes in a movie. This system consists of two components.\nThe first component is a binary classifier that predicts a binary label (i.e.\nkissing or not) given a features exctracted from both the still frames and\naudio waves of a one-second segment. The second component aggregates the binary\nlabels for contiguous non-overlapping segments into a set of kissing scenes. We\nexperimented with a variety of 2D and 3D convolutional architectures such as\nResNet, DesnseNet, and VGGish and developed a highly accurate kissing detector\nthat achieves a validation F1 score of 0.95 on a diverse database of Hollywood\nfilms ranging many genres and spanning multiple decades. The code for this\nproject is available at http://github.com/amirziai/kissing-detector.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 06:31:26 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Ziai", "Amir", ""]]}, {"id": "1906.01851", "submitter": "Yusuke Mukuta", "authors": "Yusuke Mukuta, Tatsuaki Machida and Tatsuya Harada", "title": "Compact Approximation for Polynomial of Covariance Feature", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariance pooling is a feature pooling method with good classification\naccuracy. Because covariance features consist of second-order statistics, the\nscale of the feature elements are varied. Therefore, normalizing covariance\nfeatures using a matrix square root affects the performance improvement. When\npooling methods are applied to local features extracted from CNN models, the\naccuracy increases when the pooling function is back-propagatable and the\nfeature-extraction model is learned in an end-to-end manner. Recently, the\niterative polynomial approximation method for the matrix square root of a\ncovariance feature was proposed, and resulted in a faster and more stable\ntraining than the methods based on singular-value decomposition. In this paper,\nwe propose an extension of compact bilinear pooling, which is a compact\napproximation of the standard covariance feature, to the polynomials of the\ncovariance feature. Subsequently, we apply the proposed approximation to the\npolynomial corresponding to the matrix square root to obtain a compact\napproximation for the square root of the covariance feature. Our method\napproximates a higher-dimensional polynomial of a covariance by the weighted\nsum of the approximate features corresponding to a pair of local features based\non the similarity of the local features. We apply our method for standard\nfine-grained image recognition datasets and demonstrate that the proposed\nmethod shows comparable accuracy with fewer dimensions than the original\nfeature.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 06:46:58 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Mukuta", "Yusuke", ""], ["Machida", "Tatsuaki", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1906.01857", "submitter": "Yusuke Mukuta", "authors": "Yusuke Mukuta and Tatsuya Harada", "title": "Invariant Tensor Feature Coding", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel feature coding method that exploits invariance. We\nconsider the setting where the transformations that preserve the image contents\ncompose a finite group of orthogonal matrices. This is the case in many image\ntransformations, such as image rotations and image flipping. We prove that the\ngroup-invariant feature vector contains sufficient discriminative information\nwhen learning a linear classifier using convex loss minimization. From this\nresult, we propose a novel feature modeling for principal component analysis\nand k-means clustering, which are used for most feature coding methods, and\nglobal feature functions that explicitly consider the group action. Although\nthe global feature functions are complex nonlinear functions in general, we can\ncalculate the group action on this space easily by constructing the functions\nas the tensor product representations of basic representations, resulting in\nthe explicit form of invariant feature functions. We demonstrate the\neffectiveness of our methods on several image datasets.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 07:15:17 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 09:03:50 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Mukuta", "Yusuke", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1906.01862", "submitter": "Pierrick Coupe", "authors": "Pierrick Coup\\'e, Boris Mansencal, Micha\\\"el Cl\\'ement, R\\'emi Giraud,\n  Baudouin Denis de Senneville, Vinh-Thong Ta, Vincent Lepetit, Jos\\'e V.\n  Manjon", "title": "AssemblyNet: A Novel Deep Decision-Making Process for Whole Brain MRI\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whole brain segmentation using deep learning (DL) is a very challenging task\nsince the number of anatomical labels is very high compared to the number of\navailable training images. To address this problem, previous DL methods\nproposed to use a global convolution neural network (CNN) or few independent\nCNNs. In this paper, we present a novel ensemble method based on a large number\nof CNNs processing different overlapping brain areas. Inspired by parliamentary\ndecision-making systems, we propose a framework called AssemblyNet, made of two\n\"assemblies\" of U-Nets. Such a parliamentary system is capable of dealing with\ncomplex decisions and reaching a consensus quickly. AssemblyNet introduces\nsharing of knowledge among neighboring U-Nets, an \"amendment\" procedure made by\nthe second assembly at higher-resolution to refine the decision taken by the\nfirst one, and a final decision obtained by majority voting. When using the\nsame 45 training images, AssemblyNet outperforms global U-Net by 28% in terms\nof the Dice metric, patch-based joint label fusion by 15% and SLANT-27 by 10%.\nFinally, AssemblyNet demonstrates high capacity to deal with limited training\ndata to achieve whole brain segmentation in practical training and testing\ntimes.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 07:35:37 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Coup\u00e9", "Pierrick", ""], ["Mansencal", "Boris", ""], ["Cl\u00e9ment", "Micha\u00ebl", ""], ["Giraud", "R\u00e9mi", ""], ["de Senneville", "Baudouin Denis", ""], ["Ta", "Vinh-Thong", ""], ["Lepetit", "Vincent", ""], ["Manjon", "Jos\u00e9 V.", ""]]}, {"id": "1906.01885", "submitter": "Mohammad Ibrahim Sarker", "authors": "Mohammad Ibrahim Sarker, Hyongsuk Kim", "title": "Farm land weed detection with region-based deep convolutional neural\n  networks", "comments": "7 Pages, Published in ICROS 2017 32nd Control Robot System Society\n  Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has become a major field of research in order to handle more\nand more complex image detection problems. Among the existing state-of-the-art\nCNN models, in this paper a region-based, fully convolutional network, for fast\nand accurate object detection has been proposed based on the experimental\nresults. Among the region based networks, ResNet is regarded as the most recent\nCNN architecture which has obtained the best results at ImageNet Large-Scale\nVisual Recognition Challenge (ILSVRC) in 2015. Deep residual networks (ResNets)\ncan make the training process faster and attain more accuracy compared to their\nequivalent conventional neural networks. Being motivated with such unique\nattributes of ResNet, this paper evaluates the performance of fine-tuned ResNet\nfor object classification of our weeds dataset. The dataset of farm land weeds\ndetection is insufficient to train such deep CNN models. To overcome this\nshortcoming, we perform dropout techniques along with deep residual network for\nreducing over-fitting problem as well as applying data augmentation with the\nproposed ResNet to achieve a significant outperforming result from our weeds\ndataset. We achieved better object detection performance with Region-based\nFully Convolutional Networks (R-FCN) technique which is latched with our\nproposed ResNet-101.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 08:57:34 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Sarker", "Mohammad Ibrahim", ""], ["Kim", "Hyongsuk", ""]]}, {"id": "1906.01891", "submitter": "Florian Dubost", "authors": "Florian Dubost, Hieab Adams, Pinar Yilmaz, Gerda Bortsova, Gijs van\n  Tulder, M. Arfan Ikram, Wiro Niessen, Meike Vernooij, Marleen de Bruijne", "title": "Weakly Supervised Object Detection with 2D and 3D Regression Neural\n  Networks", "comments": "New formatting. A few changes in introduction, discussion and\n  conclusion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding automatically multiple lesions in large images is a common problem in\nmedical image analysis. Solving this problem can be challenging if, during\noptimization, the automated method cannot access information about the location\nof the lesions nor is given single examples of the lesions. We propose a new\nweakly supervised detection method using neural networks, that computes\nattention maps revealing the locations of brain lesions. These attention maps\nare computed using the last feature maps of a segmentation network optimized\nonly with global image-level labels. The proposed method can generate attention\nmaps at full input resolution without need for interpolation during\npreprocessing, which allows small lesions to appear in attention maps. For\ncomparison, we modify state-of-the-art methods to compute attention maps for\nweakly supervised object detection, by using a global regression objective\ninstead of the more conventional classification objective. This regression\nobjective optimizes the number of occurrences of the target object in an image,\ne.g. the number of brain lesions in a scan, or the number of digits in an\nimage. We study the behavior of the proposed method in MNIST-based detection\ndatasets, and evaluate it for the challenging detection of enlarged\nperivascular spaces - a type of brain lesion - in a dataset of 2202 3D scans\nwith point-wise annotations in the center of all lesions in four brain regions.\nIn the brain dataset, the weakly supervised detection methods come close to the\nhuman intrarater agreement in each region. The proposed method reaches the best\narea under the curve in two out of four regions, and has the lowest number of\nfalse positive detections in all regions, while its average sensitivity over\nall regions is similar to that of the other best methods. The proposed method\ncan facilitate epidemiological and clinical studies of enlarged perivascular\nspaces.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 09:08:38 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 15:44:55 GMT"}, {"version": "v3", "created": "Wed, 2 Oct 2019 21:17:34 GMT"}, {"version": "v4", "created": "Thu, 19 Mar 2020 18:30:35 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Dubost", "Florian", ""], ["Adams", "Hieab", ""], ["Yilmaz", "Pinar", ""], ["Bortsova", "Gerda", ""], ["van Tulder", "Gijs", ""], ["Ikram", "M. Arfan", ""], ["Niessen", "Wiro", ""], ["Vernooij", "Meike", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1906.01894", "submitter": "Rui Fan", "authors": "Rui Fan, Lujia Wang, Ming Liu, Ioannis Pitas", "title": "A Robust Roll Angle Estimation Algorithm Based on Gradient Descent", "comments": "5 pages, six figures, 2019 EUSIPCO", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a robust roll angle estimation algorithm, which is\ndeveloped from our previously published work, where the roll angle was\nestimated from a dense disparity map by minimizing a global energy using golden\nsection search algorithm. In this paper, to achieve greater computational\nefficiency, we utilize gradient descent to optimize the aforementioned global\nenergy. The experimental results illustrate that the proposed roll angle\nestimation algorithm takes fewer iterations to achieve the same precision as\nthe previous method.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 09:11:51 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Fan", "Rui", ""], ["Wang", "Lujia", ""], ["Liu", "Ming", ""], ["Pitas", "Ioannis", ""]]}, {"id": "1906.01895", "submitter": "Ping Zhou", "authors": "Min Chen, Ping Zhou, Di Wu, Long Hu, Mohammad Mehedi Hassan, Atif\n  Alamri", "title": "AI-Skin : Skin Disease Recognition based on Self-learning and Wide Data\n  Collection through a Closed Loop Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are a lot of hidden dangers in the change of human skin conditions,\nsuch as the sunburn caused by long-time exposure to ultraviolet radiation,\nwhich not only has aesthetic impact causing psychological depression and lack\nof self-confidence, but also may even be life-threatening due to skin\ncanceration. Current skin disease researches adopt the auto-classification\nsystem for improving the accuracy rate of skin disease classification. However,\nthe excessive dependence on the image sample database is unable to provide\nindividualized diagnosis service for different population groups. To overcome\nthis problem, a medical AI framework based on data width evolution and\nself-learning is put forward in this paper to provide skin disease medical\nservice meeting the requirement of real time, extendibility and\nindividualization. First, the wide collection of data in the close-loop\ninformation flow of user and remote medical data center is discussed. Next, a\ndata set filter algorithm based on information entropy is given, to lighten the\nload of edge node and meanwhile improve the learning ability of remote cloud\nanalysis model. In addition, the framework provides an external algorithm load\nmodule, which can be compatible with the application requirements according to\nthe model selected. Three kinds of deep learning model, i.e. LeNet-5, AlexNet\nand VGG16, are loaded and compared, which have verified the universality of the\nalgorithm load module. The experiment platform for the proposed real-time,\nindividualized and extensible skin disease recognition system is built. And the\nsystem's computation and communication delay under the interaction scenario\nbetween tester and remote data center are analyzed. It is demonstrated that the\nsystem we put forward is reliable and effective.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 09:13:29 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Chen", "Min", ""], ["Zhou", "Ping", ""], ["Wu", "Di", ""], ["Hu", "Long", ""], ["Hassan", "Mohammad Mehedi", ""], ["Alamri", "Atif", ""]]}, {"id": "1906.01900", "submitter": "Mohammad Ibrahim Sarker", "authors": "Mohammad Ibrahim Sarker, Heechan Yang, Hyongsuk Kim", "title": "Corn leaf detection using Region based convolutional neural network", "comments": "3 pages, published in ICROS 2017 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of machine learning has become an increasingly budding area of\nresearch as more efficient methods are needed in the quest to handle more\ncomplex image detection challenges. To solve the problems of agriculture is\nmore and more important because food is the fundamental of life. However, the\ndetection accuracy in recent corn field systems are still far away from the\ndemands in practice due to a number of different weeds. This paper presents a\nmodel to handle the problem of corn leaf detection in given digital images\ncollected from farm field. Based on results of experiments conducted with\nseveral state-of-the-art models adopted by CNN, a region-based method has been\nproposed as a faster and more accurate method of corn leaf detection. Being\nmotivated with such unique attributes of ResNet, we combine it with region\nbased network (such as faster rcnn), which is able to automatically detect corn\nleaf in heavy weeds occlusion. The method is evaluated on the dataset from farm\nand we make an annotation ourselves. Our proposed method achieves significantly\noutperform in corn detection system.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 09:20:33 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Sarker", "Mohammad Ibrahim", ""], ["Yang", "Heechan", ""], ["Kim", "Hyongsuk", ""]]}, {"id": "1906.01905", "submitter": "Eli Schwartz", "authors": "Eli Schwartz, Leonid Karlinsky, Rogerio Feris, Raja Giryes, Alex M.\n  Bronstein", "title": "Baby steps towards few-shot learning with multiple semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Learning from one or few visual examples is one of the key capabilities of\nhumans since early infancy, but is still a significant challenge for modern AI\nsystems. While considerable progress has been achieved in few-shot learning\nfrom a few image examples, much less attention has been given to the verbal\ndescriptions that are usually provided to infants when they are presented with\na new object. In this paper, we focus on the role of additional semantics that\ncan significantly facilitate few-shot visual learning. Building upon recent\nadvances in few-shot learning with additional semantic information, we\ndemonstrate that further improvements are possible by combining multiple and\nricher semantics (category labels, attributes, and natural language\ndescriptions). Using these ideas, we offer the community new results on the\npopular miniImageNet and CUB few-shot benchmarks, comparing favorably to the\nprevious state-of-the-art results for both visual only and visual plus\nsemantics-based approaches. We also performed an ablation study investigating\nthe components and design choices of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 09:28:59 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 12:11:47 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Schwartz", "Eli", ""], ["Karlinsky", "Leonid", ""], ["Feris", "Rogerio", ""], ["Giryes", "Raja", ""], ["Bronstein", "Alex M.", ""]]}, {"id": "1906.01906", "submitter": "Stephan Rasp", "authors": "Stephan Rasp, Hauke Schulz, Sandrine Bony, Bjorn Stevens", "title": "Combining crowd-sourcing and deep learning to explore the meso-scale\n  organization of shallow convection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans excel at detecting interesting patterns in images, for example those\ntaken from satellites. This kind of anecdotal evidence can lead to the\ndiscovery of new phenomena. However, it is often difficult to gather enough\ndata of subjective features for significant analysis. This paper presents an\nexample of how two tools that have recently become accessible to a wide range\nof researchers, crowd-sourcing and deep learning, can be combined to explore\nsatellite imagery at scale. In particular, the focus is on the organization of\nshallow cumulus convection in the trade wind regions. Shallow clouds play a\nlarge role in the Earth's radiation balance yet are poorly represented in\nclimate models. For this project four subjective patterns of organization were\ndefined: Sugar, Flower, Fish and Gravel. On cloud labeling days at two\ninstitutes, 67 scientists screened 10,000 satellite images on a crowd-sourcing\nplatform and classified almost 50,000 mesoscale cloud clusters. This dataset is\nthen used as a training dataset for deep learning algorithms that make it\npossible to automate the pattern detection and create global climatologies of\nthe four patterns. Analysis of the geographical distribution and large-scale\nenvironmental conditions indicates that the four patterns have some overlap\nwith established modes of organization, such as open and closed cellular\nconvection, but also differ in important ways. The results and dataset from\nthis project suggests promising research questions. Further, this study\nillustrates that crowd-sourcing and deep learning complement each other well\nfor the exploration of image datasets.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 09:35:19 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 15:08:57 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2020 12:05:30 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Rasp", "Stephan", ""], ["Schulz", "Hauke", ""], ["Bony", "Sandrine", ""], ["Stevens", "Bjorn", ""]]}, {"id": "1906.01907", "submitter": "Hongyu Li", "authors": "Hongyu Li, Fan Zhu, Junhua Qiu", "title": "Towards Document Image Quality Assessment: A Text Line Based Framework\n  and A Synthetic Text Line Image Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the low quality of document images will greatly undermine the chances\nof success in automatic text recognition and analysis, it is necessary to\nassess the quality of document images uploaded in online business process, so\nas to reject those images of low quality. In this paper, we attempt to achieve\ndocument image quality assessment and our contributions are twofold. Firstly,\nsince document image quality assessment is more interested in text, we propose\na text line based framework to estimate document image quality, which is\ncomposed of three stages: text line detection, text line quality prediction,\nand overall quality assessment. Text line detection aims to find potential text\nlines with a detector. In the text line quality prediction stage, the quality\nscore is computed for each text line with a CNN-based prediction model. The\noverall quality of document images is finally assessed with the ensemble of all\ntext line quality. Secondly, to train the prediction model, a large-scale\ndataset, comprising 52,094 text line images, is synthesized with diverse\nattributes. For each text line image, a quality label is computed with a\npiece-wise function. To demonstrate the effectiveness of the proposed\nframework, comprehensive experiments are evaluated on two popular document\nimage quality assessment benchmarks. Our framework significantly outperforms\nthe state-of-the-art methods by large margins on the large and complicated\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 09:40:34 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Li", "Hongyu", ""], ["Zhu", "Fan", ""], ["Qiu", "Junhua", ""]]}, {"id": "1906.01916", "submitter": "Geoffrey French", "authors": "Geoff French, Samuli Laine, Timo Aila, Michal Mackiewicz and Graham\n  Finlayson", "title": "Semi-supervised semantic segmentation needs strong, varied perturbations", "comments": "21 pages, 7 figures, accepted to BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consistency regularization describes a class of approaches that have yielded\nground breaking results in semi-supervised classification problems. Prior work\nhas established the cluster assumption - under which the data distribution\nconsists of uniform class clusters of samples separated by low density regions\n- as important to its success. We analyze the problem of semantic segmentation\nand find that its' distribution does not exhibit low density regions separating\nclasses and offer this as an explanation for why semi-supervised segmentation\nis a challenging problem, with only a few reports of success. We then identify\nchoice of augmentation as key to obtaining reliable performance without such\nlow-density regions. We find that adapted variants of the recently proposed\nCutOut and CutMix augmentation techniques yield state-of-the-art\nsemi-supervised semantic segmentation results in standard datasets.\nFurthermore, given its challenging nature we propose that semantic segmentation\nacts as an effective acid test for evaluating semi-supervised regularizers.\nImplementation at: https://github.com/Britefury/cutmix-semisup-seg.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 10:09:27 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 07:03:48 GMT"}, {"version": "v3", "created": "Sat, 2 May 2020 10:44:55 GMT"}, {"version": "v4", "created": "Wed, 8 Jul 2020 22:34:51 GMT"}, {"version": "v5", "created": "Tue, 11 Aug 2020 16:23:40 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["French", "Geoff", ""], ["Laine", "Samuli", ""], ["Aila", "Timo", ""], ["Mackiewicz", "Michal", ""], ["Finlayson", "Graham", ""]]}, {"id": "1906.01963", "submitter": "Tushar Nagarajan", "authors": "Tushar Nagarajan, Christoph Feichtenhofer, Kristen Grauman", "title": "Grounded Human-Object Interaction Hotspots from Video (Extended\n  Abstract)", "comments": "arXiv admin note: substantial text overlap with arXiv:1812.04558", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning how to interact with objects is an important step towards embodied\nvisual intelligence, but existing techniques suffer from heavy supervision or\nsensing requirements. We propose an approach to learn human-object interaction\n\"hotspots\" directly from video. Rather than treat affordances as a manually\nsupervised semantic segmentation task, our approach learns about interactions\nby watching videos of real human behavior and anticipating afforded actions.\nGiven a novel image or video, our model infers a spatial hotspot map indicating\nhow an object would be manipulated in a potential interaction, even if the\nobject is currently at rest. Through results with both first and third person\nvideo, we show the value of grounding affordances in real human-object\ninteractions. Not only are our weakly supervised hotspots competitive with\nstrongly supervised affordance methods, but they can also anticipate object\ninteraction for novel object categories. Project page:\nhttp://vision.cs.utexas.edu/projects/interaction-hotspots/\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 19:12:55 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Nagarajan", "Tushar", ""], ["Feichtenhofer", "Christoph", ""], ["Grauman", "Kristen", ""]]}, {"id": "1906.01969", "submitter": "Marcin Namysl", "authors": "Marcin Namysl, Iuliu Konya", "title": "Efficient, Lexicon-Free OCR using Deep Learning", "comments": "Accepted for presentation in the 15th International Conference on\n  Document Analysis and Recognition (ICDAR 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrary to popular belief, Optical Character Recognition (OCR) remains a\nchallenging problem when text occurs in unconstrained environments, like\nnatural scenes, due to geometrical distortions, complex backgrounds, and\ndiverse fonts. In this paper, we present a segmentation-free OCR system that\ncombines deep learning methods, synthetic training data generation, and data\naugmentation techniques. We render synthetic training data using large text\ncorpora and over 2000 fonts. To simulate text occurring in complex natural\nscenes, we augment extracted samples with geometric distortions and with a\nproposed data augmentation technique - alpha-compositing with background\ntextures. Our models employ a convolutional neural network encoder to extract\nfeatures from text images. Inspired by the recent progress in neural machine\ntranslation and language modeling, we examine the capabilities of both\nrecurrent and convolutional neural networks in modeling the interactions\nbetween input elements.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 12:21:54 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Namysl", "Marcin", ""], ["Konya", "Iuliu", ""]]}, {"id": "1906.01972", "submitter": "Pierre Jacob", "authors": "Pierre Jacob, David Picard, Aymeric Histace, Edouard Klein", "title": "Efficient Codebook and Factorization for Second Order Representation\n  Learning", "comments": "Accepted at IEEE International Conference on Image Processing (ICIP)\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning rich and compact representations is an open topic in many fields\nsuch as object recognition or image retrieval. Deep neural networks have made a\nmajor breakthrough during the last few years for these tasks but their\nrepresentations are not necessary as rich as needed nor as compact as expected.\nTo build richer representations, high order statistics have been exploited and\nhave shown excellent performances, but they produce higher dimensional\nfeatures. While this drawback has been partially addressed with factorization\nschemes, the original compactness of first order models has never been\nretrieved, or at the cost of a strong performance decrease. Our method, by\njointly integrating codebook strategy to factorization scheme, is able to\nproduce compact representations while keeping the second order performances\nwith few additional parameters. This formulation leads to state-of-the-art\nresults on three image retrieval datasets.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 12:27:55 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Jacob", "Pierre", ""], ["Picard", "David", ""], ["Histace", "Aymeric", ""], ["Klein", "Edouard", ""]]}, {"id": "1906.01984", "submitter": "Xianxu Hou", "authors": "Xianxu Hou, Ke Sun, Linlin Shen, Guoping Qiu", "title": "Improving Variational Autoencoder with Deep Feature Consistent and\n  Generative Adversarial Training", "comments": "Accepted in Neurocomputing, 2019. arXiv admin note: text overlap with\n  arXiv:1610.00291", "journal-ref": null, "doi": "10.1016/j.neucom.2019.03.013", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for improving the performances of variational\nautoencoder (VAE). In addition to enforcing the deep feature consistent\nprinciple thus ensuring the VAE output and its corresponding input images to\nhave similar deep features, we also implement a generative adversarial training\nmechanism to force the VAE to output realistic and natural images. We present\nexperimental results to show that the VAE trained with our new method\noutperforms state of the art in generating face images with much clearer and\nmore natural noses, eyes, teeth, hair textures as well as reasonable\nbackgrounds. We also show that our method can learn powerful embeddings of\ninput face images, which can be used to achieve facial attribute manipulation.\nMoreover we propose a multi-view feature extraction strategy to extract\neffective image representations, which can be used to achieve state of the art\nperformance in facial attribute prediction.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 03:17:30 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Hou", "Xianxu", ""], ["Sun", "Ke", ""], ["Shen", "Linlin", ""], ["Qiu", "Guoping", ""]]}, {"id": "1906.02012", "submitter": "Yuntao Liu", "authors": "Yuntao Liu, Yong Dou, Ruochun Jin, Rongchun Li", "title": "Visual Confusion Label Tree For Image Classification", "comments": "9 pages, 5 figures, conference", "journal-ref": "2018 IEEE International Conference on Multimedia and Expo (ICME)", "doi": "10.1109/ICME.2018.8486612", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution neural network models are widely used in image classification\ntasks. However, the running time of such models is so long that it is not the\nconforming to the strict real-time requirement of mobile devices. In order to\noptimize models and meet the requirement mentioned above, we propose a method\nthat replaces the fully-connected layers of convolution neural network models\nwith a tree classifier. Specifically, we construct a Visual Confusion Label\nTree based on the output of the convolution neural network models, and use a\nmulti-kernel SVM plus classifier with hierarchical constraints to train the\ntree classifier. Focusing on those confusion subsets instead of the entire set\nof categories makes the tree classifier more discriminative and the replacement\nof the fully-connected layers reduces the original running time. Experiments\nshow that our tree classifier obtains a significant improvement over the\nstate-of-the-art tree classifier by 4.3% and 2.4% in terms of top-1 accuracy on\nCIFAR-100 and ImageNet datasets respectively. Additionally, our method achieves\n124x and 115x speedup ratio compared with fully-connected layers on AlexNet and\nVGG16 without accuracy decline.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 13:06:11 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Liu", "Yuntao", ""], ["Dou", "Yong", ""], ["Jin", "Ruochun", ""], ["Li", "Rongchun", ""]]}, {"id": "1906.02031", "submitter": "Yuexiang Li", "authors": "Yu Chen, Jiawei Chen, Dong Wei, Yuexiang Li and Yefeng Zheng", "title": "OctopusNet: A Deep Learning Segmentation Network for Multi-modal Medical\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models, such as the fully convolutional network (FCN), have\nbeen widely used in 3D biomedical segmentation and achieved state-of-the-art\nperformance. Multiple modalities are often used for disease diagnosis and\nquantification. Two approaches are widely used in the literature to fuse\nmultiple modalities in the segmentation networks: early-fusion (which stacks\nmultiple modalities as different input channels) and late-fusion (which fuses\nthe segmentation results from different modalities at the very end). These\nfusion methods easily suffer from the cross-modal interference caused by the\ninput modalities which have wide variations. To address the problem, we propose\na novel deep learning architecture, namely OctopusNet, to better leverage and\nfuse the information contained in multi-modalities. The proposed framework\nemploys a separate encoder for each modality for feature extraction and\nexploits a hyper-fusion decoder to fuse the extracted features while avoiding\nfeature explosion. We evaluate the proposed OctopusNet on two publicly\navailable datasets, i.e. ISLES-2018 and MRBrainS-2013. The experimental results\nshow that our framework outperforms the commonly-used feature fusion approaches\nand yields the state-of-the-art segmentation accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 13:47:12 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 06:36:43 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Chen", "Yu", ""], ["Chen", "Jiawei", ""], ["Wei", "Dong", ""], ["Li", "Yuexiang", ""], ["Zheng", "Yefeng", ""]]}, {"id": "1906.02033", "submitter": "Donghyun Kim", "authors": "Donghyun Kim, Sarah Adel Bargal, Jianming Zhang, Stan Sclaroff", "title": "Multi-way Encoding for Robustness", "comments": "Accepted at WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep models are state-of-the-art for many computer vision tasks including\nimage classification and object detection. However, it has been shown that deep\nmodels are vulnerable to adversarial examples. We highlight how one-hot\nencoding directly contributes to this vulnerability and propose breaking away\nfrom this widely-used, but highly-vulnerable mapping. We demonstrate that by\nleveraging a different output encoding, multi-way encoding, we decorrelate\nsource and target models, making target models more secure. Our approach makes\nit more difficult for adversaries to find useful gradients for generating\nadversarial attacks. We present robustness for black-box and white-box attacks\non four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN. The strength\nof our approach is also presented in the form of an attack for model\nwatermarking, raising challenges in detecting stolen models.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 13:51:11 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 18:30:50 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Kim", "Donghyun", ""], ["Bargal", "Sarah Adel", ""], ["Zhang", "Jianming", ""], ["Sclaroff", "Stan", ""]]}, {"id": "1906.02040", "submitter": "Yifan Hu", "authors": "Yifan Hu and Yefeng Zheng", "title": "A GLCM Embedded CNN Strategy for Computer-aided Diagnosis in\n  Intracerebral Hemorrhage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-aided diagnosis (CADx) systems have been shown to assist\nradiologists by providing classifications of all kinds of medical images like\nComputed tomography (CT) and Magnetic resonance (MR). Currently, convolutional\nneural networks play an important role in CADx. However, since CNN model should\nhave a square-like input, it is usually difficult to directly apply the CNN\nalgorithms on the irregular segmentation region of interests (ROIs) where the\nradiologists are interested in. In this paper, we propose a new approach to\nconstruct the model by extracting and converting the information of the\nirregular region into a fixed-size Gray-Level Co-Occurrence Matrix (GLCM) and\nthen utilize the GLCM as one input of our CNN model. In this way, as an useful\nimplementary to the original CNN, a couple of GLCM-based features are also\nextracted by CNN. Meanwhile, the network will pay more attention to the\nimportant lesion area and achieve a higher accuracy in classification.\nExperiments are performed on three classification databases: Hemorrhage,\nBraTS18 and Cervix to validate the universality of our innovative model. In\nconclusion, the proposed framework outperforms the corresponding state-of-art\nalgorithms on each database with both test losses and classification accuracy\nas the evaluation criteria.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 14:12:21 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Hu", "Yifan", ""], ["Zheng", "Yefeng", ""]]}, {"id": "1906.02042", "submitter": "Adri\\`a Arbu\\'es-Sang\\\"uesa", "authors": "Adri\\`a Arbu\\'es-Sang\\\"uesa, Coloma Ballester, Gloria Haro", "title": "Single-Camera Basketball Tracker through Pose and Semantic Feature\n  Fusion", "comments": "Accepted in the International Conference on Artificial Intelligence\n  in Sports 2019 (ICAIS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking sports players is a widely challenging scenario, specially in\nsingle-feed videos recorded in tight courts, where cluttering and occlusions\ncannot be avoided. This paper presents an analysis of several geometric and\nsemantic visual features to detect and track basketball players. An ablation\nstudy is carried out and then used to remark that a robust tracker can be built\nwith Deep Learning features, without the need of extracting contextual ones,\nsuch as proximity or color similarity, nor applying camera stabilization\ntechniques. The presented tracker consists of: (1) a detection step, which uses\na pretrained deep learning model to estimate the players pose, followed by (2)\na tracking step, which leverages pose and semantic information from the output\nof a convolutional layer in a VGG network. Its performance is analyzed in terms\nof MOTA over a basketball dataset with more than 10k instances.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 14:18:07 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 09:42:43 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Arbu\u00e9s-Sang\u00fcesa", "Adri\u00e0", ""], ["Ballester", "Coloma", ""], ["Haro", "Gloria", ""]]}, {"id": "1906.02076", "submitter": "David Calhas", "authors": "David Calhas, Enrique Romero, Rui Henriques", "title": "On the use of Pairwise Distance Learning for Brain Signal Classification\n  with Limited Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE eess.SP q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The increasing access to brain signal data using electroencephalography\ncreates new opportunities to study electrophysiological brain activity and\nperform ambulatory diagnoses of neuronal diseases. This work proposes a\npairwise distance learning approach for Schizophrenia classification relying on\nthe spectral properties of the signal. Given the limited number of observations\n(i.e. the case and/or control individuals) in clinical trials, we propose a\nSiamese neural network architecture to learn a discriminative feature space\nfrom pairwise combinations of observations per channel. In this way, the\nmultivariate order of the signal is used as a form of data augmentation,\nfurther supporting the network generalization ability. Convolutional layers\nwith parameters learned under a cosine contrastive loss are proposed to\nadequately explore spectral images derived from the brain signal. Results on a\ncase-control population show that the features extracted using the proposed\nneural network lead to an improved Schizophrenia diagnosis (+10pp in accuracy\nand sensitivity) against spectral features, thus suggesting the existence of\nnon-trivial, discriminative electrophysiological brain patterns.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 15:36:57 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 11:40:27 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Calhas", "David", ""], ["Romero", "Enrique", ""], ["Henriques", "Rui", ""]]}, {"id": "1906.02112", "submitter": "Stavros Petridis", "authors": "Pingchuan Ma, Stavros Petridis, Maja Pantic", "title": "Investigating the Lombard Effect Influence on End-to-End Audio-Visual\n  Speech Recognition", "comments": "Accepted for publication at Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several audio-visual speech recognition models have been recently proposed\nwhich aim to improve the robustness over audio-only models in the presence of\nnoise. However, almost all of them ignore the impact of the Lombard effect,\ni.e., the change in speaking style in noisy environments which aims to make\nspeech more intelligible and affects both the acoustic characteristics of\nspeech and the lip movements. In this paper, we investigate the impact of the\nLombard effect in audio-visual speech recognition. To the best of our\nknowledge, this is the first work which does so using end-to-end deep\narchitectures and presents results on unseen speakers. Our results show that\nproperly modelling Lombard speech is always beneficial. Even if a relatively\nsmall amount of Lombard speech is added to the training set then the\nperformance in a real scenario, where noisy Lombard speech is present, can be\nsignificantly improved. We also show that the standard approach followed in the\nliterature, where a model is trained and tested on noisy plain speech, provides\na correct estimate of the video-only performance and slightly underestimates\nthe audio-visual performance. In case of audio-only approaches, performance is\noverestimated for SNRs higher than -3dB and underestimated for lower SNRs.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 16:40:54 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 18:40:50 GMT"}, {"version": "v3", "created": "Wed, 3 Jul 2019 17:11:14 GMT"}, {"version": "v4", "created": "Tue, 9 Jul 2019 17:51:21 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Ma", "Pingchuan", ""], ["Petridis", "Stavros", ""], ["Pantic", "Maja", ""]]}, {"id": "1906.02168", "submitter": "Vaishaal Shankar", "authors": "Vaishaal Shankar, Achal Dave, Rebecca Roelofs, Deva Ramanan, Benjamin\n  Recht, Ludwig Schmidt", "title": "Do Image Classifiers Generalize Across Time?", "comments": "23 pages, 11 tables, 11 figures. Paper Website:\n  https://modestyachts.github.io/natural-perturbations-website/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the robustness of image classifiers to temporal perturbations\nderived from videos. As part of this study, we construct two datasets,\nImageNet-Vid-Robust and YTBB-Robust , containing a total 57,897 images grouped\ninto 3,139 sets of perceptually similar images. Our datasets were derived from\nImageNet-Vid and Youtube-BB respectively and thoroughly re-annotated by human\nexperts for image similarity. We evaluate a diverse array of classifiers\npre-trained on ImageNet and show a median classification accuracy drop of 16\nand 10 on our two datasets. Additionally, we evaluate three detection models\nand show that natural perturbations induce both classification as well as\nlocalization errors, leading to a median drop in detection mAP of 14 points.\nOur analysis demonstrates that perturbations occurring naturally in videos pose\na substantial and realistic challenge to deploying convolutional neural\nnetworks in environments that require both reliable and low-latency predictions\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 17:55:42 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 18:03:18 GMT"}, {"version": "v3", "created": "Mon, 9 Dec 2019 17:30:11 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Shankar", "Vaishaal", ""], ["Dave", "Achal", ""], ["Roelofs", "Rebecca", ""], ["Ramanan", "Deva", ""], ["Recht", "Benjamin", ""], ["Schmidt", "Ludwig", ""]]}, {"id": "1906.02182", "submitter": "Huijuan Xu", "authors": "Huijuan Xu, Abir Das and Kate Saenko", "title": "Two-Stream Region Convolutional 3D Network for Temporal Activity\n  Detection", "comments": "Published in TPAMI. arXiv admin note: substantial text overlap with\n  arXiv:1703.07814, 1710.08011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of temporal activity detection in continuous,\nuntrimmed video streams. This is a difficult task that requires extracting\nmeaningful spatio-temporal features to capture activities, accurately\nlocalizing the start and end times of each activity. We introduce a new model,\nRegion Convolutional 3D Network (R-C3D), which encodes the video streams using\na three-dimensional fully convolutional network, then generates candidate\ntemporal regions containing activities and finally classifies selected regions\ninto specific activities. Computation is saved due to the sharing of\nconvolutional features between the proposal and the classification pipelines.\nWe further improve the detection performance by efficiently integrating an\noptical flow based motion stream with the original RGB stream. The two-stream\nnetwork is jointly optimized by fusing the flow and RGB feature maps at\ndifferent levels. Additionally, the training stage incorporates an online hard\nexample mining strategy to address the extreme foreground-background imbalance\ntypically observed in any detection pipeline. Instead of heuristically sampling\nthe candidate segments for the final activity classification stage, we rank\nthem according to their performance and only select the worst performers to\nupdate the model. This improves the model without heavy hyper-parameter tuning.\nExtensive experiments on three benchmark datasets are carried out to show\nsuperior performance over existing temporal activity detection methods. Our\nmodel achieves state-of-the-art results on the THUMOS'14 and Charades datasets.\nWe further demonstrate that our model is a general temporal activity detection\nframework that does not rely on assumptions about particular dataset properties\nby evaluating our approach on the ActivityNet dataset.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 02:48:37 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Xu", "Huijuan", ""], ["Das", "Abir", ""], ["Saenko", "Kate", ""]]}, {"id": "1906.02222", "submitter": "Brendan Duke", "authors": "Brendan Duke, Abdalla Ahmed, Edmund Phung, Irina Kezele, Parham Aarabi", "title": "Nail Polish Try-On: Realtime Semantic Segmentation of Small Objects for\n  Native and Browser Smartphone AR Applications", "comments": "4 pages, 3 figures. CVPRW 2019: Third Workshop on Computer Vision for\n  AR/VR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a system for semantic segmentation of small objects that enables\nnail polish try-on AR applications to run client-side in realtime in native and\nweb mobile applications. By adjusting input resolution and neural network\ndepth, our model design enables a smooth trade-off of performance and runtime,\nwith the highest performance setting achieving~\\num{94.5} mIoU at 29.8ms\nruntime in native applications on an iPad Pro. We also provide a postprocessing\nand rendering algorithm for nail polish try-on, which integrates with our\nsemantic segmentation and fingernail base-tip direction predictions.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 18:04:58 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 14:25:46 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Duke", "Brendan", ""], ["Ahmed", "Abdalla", ""], ["Phung", "Edmund", ""], ["Kezele", "Irina", ""], ["Aarabi", "Parham", ""]]}, {"id": "1906.02238", "submitter": "Shuyang Dai", "authors": "Shuyang Dai, Kihyuk Sohn, Yi-Hsuan Tsai, Lawrence Carin, Manmohan\n  Chandraker", "title": "Adaptation Across Extreme Variations using Unlabeled Domain Bridges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle an unsupervised domain adaptation problem for which the domain\ndiscrepancy between labeled source and unlabeled target domains is large, due\nto many factors of inter and intra-domain variation. While deep domain\nadaptation methods have been realized by reducing the domain discrepancy, these\nare difficult to apply when domains are significantly unalike. In this work, we\npropose to decompose domain discrepancy into multiple but smaller, and thus\neasier to minimize, discrepancies by introducing unlabeled bridging domains\nthat connect the source and target domains. We realize our proposal through an\nextension of the domain adversarial neural network with multiple\ndiscriminators, each of which accounts for reducing discrepancies between\nunlabeled (bridge, target) domains and a mix of all precedent domains including\nsource. We validate the effectiveness of our method on several adaptation tasks\nincluding object recognition and semantic segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 18:32:16 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 23:38:03 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Dai", "Shuyang", ""], ["Sohn", "Kihyuk", ""], ["Tsai", "Yi-Hsuan", ""], ["Carin", "Lawrence", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "1906.02256", "submitter": "Mohammad Rastegari", "authors": "Keivan Alizadeh Vahid, Anish Prabhu, Ali Farhadi, Mohammad Rastegari", "title": "Butterfly Transform: An Efficient FFT Based Neural Architecture Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that extending the butterfly operations from the FFT\nalgorithm to a general Butterfly Transform (BFT) can be beneficial in building\nan efficient block structure for CNN designs. Pointwise convolutions, which we\nrefer to as channel fusions, are the main computational bottleneck in the\nstate-of-the-art efficient CNNs (e.g. MobileNets ). We introduce a set of\ncriteria for channel fusion and prove that BFT yields an asymptotically optimal\nFLOP count with respect to these criteria. By replacing pointwise convolutions\nwith BFT, we reduce the computational complexity of these layers from O(n^2) to\nO(n\\log n) with respect to the number of channels. Our experimental evaluations\nshow that our method results in significant accuracy gains across a wide range\nof network architectures, especially at low FLOP ranges. For example, BFT\nresults in up to a 6.75% absolute Top-1 improvement for MobileNetV1, 4.4 \\% for\nShuffleNet V2 and 5.4% for MobileNetV3 on ImageNet under a similar number of\nFLOPS. Notably, ShuffleNet-V2+BFT outperforms state-of-the-art architecture\nsearch methods MNasNet, FBNet and MobilenetV3 in the low FLOP regime.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 19:04:06 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 23:28:09 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Vahid", "Keivan Alizadeh", ""], ["Prabhu", "Anish", ""], ["Farhadi", "Ali", ""], ["Rastegari", "Mohammad", ""]]}, {"id": "1906.02260", "submitter": "Zhi Yu", "authors": "TianXing Li, Zhi Yu, Edmund Phung, Brendan Duke, Irina Kezele, Parham\n  Aarabi", "title": "Lightweight Real-time Makeup Try-on in Mobile Browsers with Tiny CNN\n  Models for Facial Tracking", "comments": "4 pages, Third Workshop on Computer Vision for AR/VR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works on convolutional neural networks (CNNs) for facial alignment\nhave demonstrated unprecedented accuracy on a variety of large, publicly\navailable datasets. However, the developed models are often both cumbersome and\ncomputationally expensive, and are not adapted to applications on resource\nrestricted devices. In this work, we look into developing and training compact\nfacial alignment models that feature fast inference speed and small deployment\nsize, making them suitable for applications on the aforementioned category of\ndevices. Our main contribution lies in designing such small models while\nmaintaining high accuracy of facial alignment. The models we propose make use\nof light CNN architectures adapted to the facial alignment problem for accurate\ntwo-stage prediction of facial landmark coordinates from low-resolution output\nheatmaps. We further combine the developed facial tracker with a rendering\nmethod, and build a real-time makeup try-on demo that runs client-side in\nsmartphone Web browsers. More results and demo are in our project page:\nhttp://research.modiface.com/makeup-try-on-cvprw2019/\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 19:16:17 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 13:25:36 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Li", "TianXing", ""], ["Yu", "Zhi", ""], ["Phung", "Edmund", ""], ["Duke", "Brendan", ""], ["Kezele", "Irina", ""], ["Aarabi", "Parham", ""]]}, {"id": "1906.02281", "submitter": "Fabian Balsiger", "authors": "Fabian Balsiger, Yannick Soom, Olivier Scheidegger, Mauricio Reyes", "title": "Learning Shape Representation on Sparse Point Clouds for Volumetric\n  Image Segmentation", "comments": "Accepted at MICCAI 2019", "journal-ref": null, "doi": "10.1007/978-3-030-32245-8_31", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volumetric image segmentation with convolutional neural networks (CNNs)\nencounters several challenges, which are specific to medical images. Among\nthese challenges are large volumes of interest, high class imbalances, and\ndifficulties in learning shape representations. To tackle these challenges, we\npropose to improve over traditional CNN-based volumetric image segmentation\nthrough point-wise classification of point clouds. The sparsity of point clouds\nallows processing of entire image volumes, balancing highly imbalanced\nsegmentation problems, and explicitly learning an anatomical shape. We build\nupon PointCNN, a neural network proposed to process point clouds, and propose\nhere to jointly encode shape and volumetric information within the point cloud\nin a compact and computationally effective manner. We demonstrate how this\napproach can then be used to refine CNN-based segmentation, which yields\nsignificantly improved results in our experiments on the difficult task of\nperipheral nerve segmentation from magnetic resonance neurography images. By\nsynthetic experiments, we further show the capability of our approach in\nlearning an explicit anatomical shape representation.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 19:56:50 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Balsiger", "Fabian", ""], ["Soom", "Yannick", ""], ["Scheidegger", "Olivier", ""], ["Reyes", "Mauricio", ""]]}, {"id": "1906.02283", "submitter": "Ben Glocker", "authors": "Martin Zlocha, Qi Dou, Ben Glocker", "title": "Improving RetinaNet for CT Lesion Detection with Dense Masks from Weak\n  RECIST Labels", "comments": "Accepted at MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate, automated lesion detection in Computed Tomography (CT) is an\nimportant yet challenging task due to the large variation of lesion types,\nsizes, locations and appearances. Recent work on CT lesion detection employs\ntwo-stage region proposal based methods trained with centroid or bounding-box\nannotations. We propose a highly accurate and efficient one-stage lesion\ndetector, by re-designing a RetinaNet to meet the particular challenges in\nmedical imaging. Specifically, we optimize the anchor configurations using a\ndifferential evolution search algorithm. For training, we leverage the response\nevaluation criteria in solid tumors (RECIST) annotation which are measured in\nclinical routine. We incorporate dense masks from weak RECIST labels, obtained\nautomatically using GrabCut, into the training objective, which in combination\nwith other advancements yields new state-of-the-art performance. We evaluate\nour method on the public DeepLesion benchmark, consisting of 32,735 lesions\nacross the body. Our one-stage detector achieves a sensitivity of 90.77% at 4\nfalse positives per image, significantly outperforming the best reported\nmethods by over 5%.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 20:04:11 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Zlocha", "Martin", ""], ["Dou", "Qi", ""], ["Glocker", "Ben", ""]]}, {"id": "1906.02290", "submitter": "Daniel Barath", "authors": "Daniel Barath, Jiri Matas", "title": "Progressive-X: Efficient, Anytime, Multi-Model Fitting Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Progressive-X algorithm, Prog-X in short, is proposed for geometric\nmulti-model fitting. The method interleaves sampling and consolidation of the\ncurrent data interpretation via repetitive hypothesis proposal, fast rejection,\nand integration of the new hypothesis into the kept instance set by labeling\nenergy minimization. Due to exploring the data progressively, the method has\nseveral beneficial properties compared with the state-of-the-art. First, a\nclear criterion, adopted from RANSAC, controls the termination and stops the\nalgorithm when the probability of finding a new model with a reasonable number\nof inliers falls below a threshold. Second, Prog-X is an any-time algorithm.\nThus, whenever is interrupted, e.g. due to a time limit, the returned instances\ncover real and, likely, the most dominant ones. The method is superior to the\nstate-of-the-art in terms of accuracy in both synthetic experiments and on\npublicly available real-world datasets for homography, two-view motion, and\nmotion segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 20:15:57 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Barath", "Daniel", ""], ["Matas", "Jiri", ""]]}, {"id": "1906.02295", "submitter": "Daniel Barath", "authors": "Daniel Barath, Maksym Ivashechkin, Jiri Matas", "title": "Progressive NAPSAC: sampling from gradually growing neighborhoods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Progressive NAPSAC, P-NAPSAC in short, which merges the advantages\nof local and global sampling by drawing samples from gradually growing\nneighborhoods. Exploiting the fact that nearby points are more likely to\noriginate from the same geometric model, P-NAPSAC finds local structures\nearlier than global samplers. We show that the progressive spatial sampling in\nP-NAPSAC can be integrated with PROSAC sampling, which is applied to the first,\nlocation-defining, point. P-NAPSAC is embedded in USAC, a state-of-the-art\nrobust estimation pipeline, which we further improve by implementing its local\noptimization as in Graph-Cut RANSAC. We call the resulting estimator USAC*. The\nmethod is tested on homography and fundamental matrix fitting on a total of\n10,691 models from seven publicly available datasets. USAC* with P-NAPSAC\noutperforms reference methods in terms of speed on all problems.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 20:28:18 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Barath", "Daniel", ""], ["Ivashechkin", "Maksym", ""], ["Matas", "Jiri", ""]]}, {"id": "1906.02331", "submitter": "Thiago H. Silva", "authors": "Wyverson B. de Oliveira, Leyza B. Dorini, Rodrigo Minetto, Thiago H.\n  Silva", "title": "OutdoorSent: Sentiment Analysis of Urban Outdoor Images by Using\n  Semantic and Deep Features", "comments": "Accepted on the ACM Transactions on Information Systems (TOIS)", "journal-ref": "ACM Transactions on Information Systems (TOIS) 2020", "doi": "10.1145/3385186", "report-no": null, "categories": "cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opinion mining in outdoor images posted by users during different activities\ncan provide valuable information to better understand urban areas. In this\nregard, we propose a framework to classify the sentiment of outdoor images\nshared by users on social networks. We compare the performance of\nstate-of-the-art ConvNet architectures, and one specifically designed for\nsentiment analysis. We also evaluate how the merging of deep features and\nsemantic information derived from the scene attributes can improve\nclassification and cross-dataset generalization performance. The evaluation\nexplores a novel dataset, namely OutdoorSent, and other datasets publicly\navailable. We observe that the incorporation of knowledge about semantic\nattributes improves the accuracy of all ConvNet architectures studied. Besides,\nwe found that exploring only images related to the context of the study,\noutdoor in our case, is recommended, i.e., indoor images were not significantly\nhelpful. Furthermore, we demonstrated the applicability of our results in the\ncity of Chicago, USA, showing that they can help to improve the knowledge of\nsubjective characteristics of different areas of the city. For instance,\nparticular areas of the city tend to concentrate more images of a specific\nclass of sentiment, which are also correlated with median income, opening up\nopportunities in different fields.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 22:08:37 GMT"}, {"version": "v2", "created": "Sat, 8 Jun 2019 14:19:45 GMT"}, {"version": "v3", "created": "Tue, 19 Nov 2019 17:04:48 GMT"}, {"version": "v4", "created": "Thu, 27 Feb 2020 22:25:02 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["de Oliveira", "Wyverson B.", ""], ["Dorini", "Leyza B.", ""], ["Minetto", "Rodrigo", ""], ["Silva", "Thiago H.", ""]]}, {"id": "1906.02337", "submitter": "Norman Mu", "authors": "Norman Mu, Justin Gilmer", "title": "MNIST-C: A Robustness Benchmark for Computer Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the MNIST-C dataset, a comprehensive suite of 15 corruptions\napplied to the MNIST test set, for benchmarking out-of-distribution robustness\nin computer vision. Through several experiments and visualizations we\ndemonstrate that our corruptions significantly degrade performance of\nstate-of-the-art computer vision models while preserving the semantic content\nof the test images. In contrast to the popular notion of adversarial\nrobustness, our model-agnostic corruptions do not seek worst-case performance\nbut are instead designed to be broad and diverse, capturing multiple failure\nmodes of modern models. In fact, we find that several previously published\nadversarial defenses significantly degrade robustness as measured by MNIST-C.\nWe hope that our benchmark serves as a useful tool for future work in designing\nsystems that are able to learn robust feature representations that capture the\nunderlying semantics of the input.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 22:23:43 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Mu", "Norman", ""], ["Gilmer", "Justin", ""]]}, {"id": "1906.02343", "submitter": "Enzo Ferrante", "authors": "Agostina J. Larrazabal and Cesar Martinez and Enzo Ferrante", "title": "Anatomical Priors for Image Segmentation via Post-Processing with\n  Denoising Autoencoders", "comments": "Accepted for publication in MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep convolutional neural networks (CNN) proved to be highly accurate to\nperform anatomical segmentation of medical images. However, some of the most\npopular CNN architectures for image segmentation still rely on post-processing\nstrategies (e.g. Conditional Random Fields) to incorporate connectivity\nconstraints into the resulting masks. These post-processing steps are based on\nthe assumption that objects are usually continuous and therefore nearby pixels\nshould be assigned the same object label. Even if it is a valid assumption in\ngeneral, these methods do not offer a straightforward way to incorporate more\ncomplex priors like convexity or arbitrary shape restrictions. In this work we\npropose Post-DAE, a post-processing method based on denoising autoencoders\n(DAE) trained using only segmentation masks. We learn a low-dimensional space\nof anatomically plausible segmentations, and use it as a post-processing step\nto impose shape constraints on the resulting masks obtained with arbitrary\nsegmentation methods. Our approach is independent of image modality and\nintensity information since it employs only segmentation masks for training.\nThis enables the use of anatomical segmentations that do not need to be paired\nwith intensity images, making the approach very flexible. Our experimental\nresults on anatomical segmentation of X-ray images show that Post-DAE can\nimprove the quality of noisy and incorrect segmentation masks obtained with a\nvariety of standard methods, by bringing them back to a feasible space, with\nalmost no extra computational time.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 22:41:11 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Larrazabal", "Agostina J.", ""], ["Martinez", "Cesar", ""], ["Ferrante", "Enzo", ""]]}, {"id": "1906.02355", "submitter": "Xuanqing Liu", "authors": "Xuanqing Liu, Tesi Xiao, Si Si, Qin Cao, Sanjiv Kumar, Cho-Jui Hsieh", "title": "Neural SDE: Stabilizing Neural ODE Networks with Stochastic Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Ordinary Differential Equation (Neural ODE) has been proposed as a\ncontinuous approximation to the ResNet architecture. Some commonly used\nregularization mechanisms in discrete neural networks (e.g. dropout, Gaussian\nnoise) are missing in current Neural ODE networks. In this paper, we propose a\nnew continuous neural network framework called Neural Stochastic Differential\nEquation (Neural SDE) network, which naturally incorporates various commonly\nused regularization mechanisms based on random noise injection. Our framework\ncan model various types of noise injection frequently used in discrete networks\nfor regularization purpose, such as dropout and additive/multiplicative noise\nin each block. We provide theoretical analysis explaining the improved\nrobustness of Neural SDE models against input perturbations/adversarial\nattacks. Furthermore, we demonstrate that the Neural SDE network can achieve\nbetter generalization than the Neural ODE and is more resistant to adversarial\nand non-adversarial input perturbations.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 23:19:50 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Liu", "Xuanqing", ""], ["Xiao", "Tesi", ""], ["Si", "Si", ""], ["Cao", "Qin", ""], ["Kumar", "Sanjiv", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "1906.02365", "submitter": "Daqing Liu", "authors": "Zheng-Jun Zha, Daqing Liu, Hanwang Zhang, Yongdong Zhang, Feng Wu", "title": "Context-Aware Visual Policy Network for Fine-Grained Image Captioning", "comments": "Accepted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (T-PAMI). Extended version of \"Context-Aware Visual Policy\n  Network for Sequence-Level Image Captioning\", ACM MM 2018 (arXiv:1808.05864)", "journal-ref": null, "doi": "10.1109/TPAMI.2019.2909864", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the maturity of visual detection techniques, we are more ambitious in\ndescribing visual content with open-vocabulary, fine-grained and free-form\nlanguage, i.e., the task of image captioning. In particular, we are interested\nin generating longer, richer and more fine-grained sentences and paragraphs as\nimage descriptions. Image captioning can be translated to the task of\nsequential language prediction given visual content, where the output sequence\nforms natural language description with plausible grammar. However, existing\nimage captioning methods focus only on language policy while not visual policy,\nand thus fail to capture visual context that are crucial for compositional\nreasoning such as object relationships (e.g., \"man riding horse\") and visual\ncomparisons (e.g., \"small(er) cat\"). This issue is especially severe when\ngenerating longer sequences such as a paragraph. To fill the gap, we propose a\nContext-Aware Visual Policy network (CAVP) for fine-grained image-to-language\ngeneration: image sentence captioning and image paragraph captioning. During\ncaptioning, CAVP explicitly considers the previous visual attentions as\ncontext, and decides whether the context is used for the current word/sentence\ngeneration given the current visual attention. Compared against traditional\nvisual attention mechanism that only fixes a single visual region at each step,\nCAVP can attend to complex visual compositions over time. The whole image\ncaptioning model -- CAVP and its subsequent language policy network -- can be\nefficiently optimized end-to-end by using an actor-critic policy gradient\nmethod. We have demonstrated the effectiveness of CAVP by state-of-the-art\nperformances on MS-COCO and Stanford captioning datasets, using various metrics\nand sensible visualizations of qualitative visual context.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 00:37:33 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Zha", "Zheng-Jun", ""], ["Liu", "Daqing", ""], ["Zhang", "Hanwang", ""], ["Zhang", "Yongdong", ""], ["Wu", "Feng", ""]]}, {"id": "1906.02371", "submitter": "Yuliang Liu", "authors": "Yuliang Liu, Sheng Zhang, Lianwen Jin, Lele Xie, Yaqiang Wu, Zhepeng\n  Wang", "title": "Omnidirectional Scene Text Detection with Sequential-free Box\n  Discretization", "comments": "Accepted by IJCAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Scene text in the wild is commonly presented with high variant\ncharacteristics. Using quadrilateral bounding box to localize the text instance\nis nearly indispensable for detection methods. However, recent researches\nreveal that introducing quadrilateral bounding box for scene text detection\nwill bring a label confusion issue which is easily overlooked, and this issue\nmay significantly undermine the detection performance. To address this issue,\nin this paper, we propose a novel method called Sequential-free Box\nDiscretization (SBD) by discretizing the bounding box into key edges (KE) which\ncan further derive more effective methods to improve detection performance.\nExperiments showed that the proposed method can outperform state-of-the-art\nmethods in many popular scene text benchmarks, including ICDAR 2015, MLT, and\nMSRA-TD500. Ablation study also showed that simply integrating the SBD into\nMask R-CNN framework, the detection performance can be substantially improved.\nFurthermore, an experiment on the general object dataset HRSC2016\n(multi-oriented ships) showed that our method can outperform recent\nstate-of-the-art methods by a large margin, demonstrating its powerful\ngeneralization ability. Source code:\nhttps://github.com/Yuliang-Liu/Box_Discretization_Network.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 01:13:02 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 07:25:02 GMT"}, {"version": "v3", "created": "Sun, 28 Jul 2019 12:30:15 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Liu", "Yuliang", ""], ["Zhang", "Sheng", ""], ["Jin", "Lianwen", ""], ["Xie", "Lele", ""], ["Wu", "Yaqiang", ""], ["Wang", "Zhepeng", ""]]}, {"id": "1906.02374", "submitter": "Xiaoyu Xiang", "authors": "Xiaoyu Xiang, Renee Jessome, Eric Maggard, Yousun Bang, Minki Cho, and\n  Jan Allebach", "title": "Blockwise Based Detection of Local Defects", "comments": "7 pages, 13 figures, IS&T Electronic Imaging 2019 Proceedings", "journal-ref": "Electronic Imaging Symposium Proceedings, Image Quality and System\n  Performance XVI. Society for Imaging Science and Technology, 2019", "doi": "10.2352/ISSN.2470-1173.2016.13.IQSP-207", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Print quality is an important criterion for a printer's performance. The\ndetection, classification, and assessment of printing defects can reflect the\nprinter's working status and help to locate mechanical problems inside. To\nhandle all these questions, an efficient algorithm is needed to replace the\ntraditionally visual checking method. In this paper, we focus on pages with\nlocal defects including gray spots and solid spots. We propose a coarse-to-fine\nmethod to detect local defects in a block-wise manner, and aggregate the\nblockwise attributes to generate the feature vector of the whole test page for\na further ranking task. In the detection part, we first select candidate\nregions by thresholding a single feature. Then more detailed features of\ncandidate blocks are calculated and sent to a decision tree that is previously\ntrained on our training dataset. The final result is given by the decision tree\nmodel to control the false alarm rate while maintaining the required miss rate.\nOur algorithm is proved to be effective in detecting and classifying local\ndefects compared with previous methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 01:27:32 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Xiang", "Xiaoyu", ""], ["Jessome", "Renee", ""], ["Maggard", "Eric", ""], ["Bang", "Yousun", ""], ["Cho", "Minki", ""], ["Allebach", "Jan", ""]]}, {"id": "1906.02392", "submitter": "Tao Song", "authors": "Tao Song", "title": "Generative Model-Based Ischemic Stroke Lesion Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CT perfusion (CTP) has been used to triage ischemic stroke patients in the\nearly stage, because of its speed, availability, and lack of contraindications.\nPerfusion parameters including cerebral blood volume (CBV), cerebral blood flow\n(CBF), mean transit time (MTT) and time of peak (Tmax) could also be computed\nfrom CTP data. However, CTP data or the perfusion parameters, are ambiguous to\nlocate the infarct core or tissue at risk (penumbra), which is normally\nconfirmed by the follow-up Diffusion Weighted Imaging (DWI) or perfusion\ndiffusion mismatch. In this paper, we propose a novel generative modelbased\nsegmentation framework composed of an extractor, a generator and a segmentor\nfor ischemic stroke lesion segmentation. First, an extractor is used to\ndirectly extract the representative feature images from the CTP feature images.\nSecond, a generator is used to generate the clinical relevant DWI images using\nthe output from the extractor and perfusion parameters. Finally, the segmentor\nis used to precisely segment the ischemic stroke lesion using the generated DWI\nfrom the generator. Meanwhile, a novel pixel-region loss function, generalized\ndice combined with weighted cross entropy, is used to handle data unbalance\nproblem which is commonly encountered in medical image segmentation. All\nnetworks are trained end-to-end from scratch using the 2018 Ischemic Stroke\nLesion Segmentation Challenge (ISLES) dataset and our method won the first\nplace in the 2018 ischemic stroke lesions segmentation challenge in the test\nstage.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 03:25:11 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Song", "Tao", ""]]}, {"id": "1906.02398", "submitter": "Jiawei Du", "authors": "Jiawei Du, Hu Zhang, Joey Tianyi Zhou, Yi Yang, Jiashi Feng", "title": "Query-efficient Meta Attack to Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Black-box attack methods aim to infer suitable attack patterns to targeted\nDNN models by only using output feedback of the models and the corresponding\ninput queries. However, due to lack of prior and inefficiency in leveraging the\nquery and feedback information, existing methods are mostly query-intensive for\nobtaining effective attack patterns. In this work, we propose a meta attack\napproach that is capable of attacking a targeted model with much fewer queries.\nIts high queryefficiency stems from effective utilization of meta learning\napproaches in learning generalizable prior abstraction from the previously\nobserved attack patterns and exploiting such prior to help infer attack\npatterns from only a few queries and outputs. Extensive experiments on MNIST,\nCIFAR10 and tiny-Imagenet demonstrate that our meta-attack method can\nremarkably reduce the number of model queries without sacrificing the attack\nperformance. Besides, the obtained meta attacker is not restricted to a\nparticular model but can be used easily with a fast adaptive ability to attack\na variety of models.The code of our work is available at\nhttps://github.com/dydjw9/MetaAttack_ICLR2020/.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 03:49:00 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 12:46:24 GMT"}, {"version": "v3", "created": "Sat, 15 Feb 2020 03:51:45 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Du", "Jiawei", ""], ["Zhang", "Hu", ""], ["Zhou", "Joey Tianyi", ""], ["Yang", "Yi", ""], ["Feng", "Jiashi", ""]]}, {"id": "1906.02415", "submitter": "Vinicius Ribeiro", "authors": "Vinicius Ribeiro, Sandra Avila, Eduardo Valle", "title": "Handling Inter-Annotator Agreement for Automated Skin Lesion\n  Segmentation", "comments": "10 pages, 5 images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we explore the issue of the inter-annotator agreement for\ntraining and evaluating automated segmentation of skin lesions. We explore what\ndifferent degrees of agreement represent, and how they affect different use\ncases for segmentation. We also evaluate how conditioning the ground truths\nusing different (but very simple) algorithms may help to enhance agreement and\nmay be appropriate for some use cases. The segmentation of skin lesions is a\ncornerstone task for automated skin lesion analysis, useful both as an\nend-result to locate/detect the lesions and as an ancillary task for lesion\nclassification. Lesion segmentation, however, is a very challenging task, due\nnot only to the challenge of image segmentation itself but also to the\ndifficulty in obtaining properly annotated data. Detecting accurately the\nborders of lesions is challenging even for trained humans, since, for many\nlesions, those borders are fuzzy and ill-defined. Using lesions and annotations\nfrom the ISIC Archive, we estimate inter-annotator agreement for skin-lesion\nsegmentation and propose several simple procedures that may help to improve\ninter-annotator agreement if used to condition the ground truths.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 05:01:04 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Ribeiro", "Vinicius", ""], ["Avila", "Sandra", ""], ["Valle", "Eduardo", ""]]}, {"id": "1906.02421", "submitter": "Shadab Khan", "authors": "Shadab Khan, Ahmed H. Shahin, Javier Villafruela, Jianbing Shen, Ling\n  Shao", "title": "Extreme Points Derived Confidence Map as a Cue For Class-Agnostic\n  Segmentation Using Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": "2019MICCAI", "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To automate the process of segmenting an anatomy of interest, we can learn a\nmodel from previously annotated data. The learning-based approach uses\nannotations to train a model that tries to emulate the expert labeling on a new\ndata set. While tremendous progress has been made using such approaches,\nlabeling of medical images remains a time-consuming and expensive task. In this\npaper, we evaluate the utility of extreme points in learning to segment.\nSpecifically, we propose a novel approach to compute a confidence map from\nextreme points that quantitatively encodes the priors derived from extreme\npoints. We use the confidence map as a cue to train a deep neural network based\non ResNet-101 and PSP module to develop a class-agnostic segmentation model\nthat outperforms state-of-the-art method that employs extreme points as a cue.\nFurther, we evaluate a realistic use-case by using our model to generate\ntraining data for supervised learning (U-Net) and observed that U-Net performs\ncomparably when trained with either the generated data or the ground truth\ndata. These findings suggest that models trained using cues can be used to\ngenerate reliable training data.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 05:19:22 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Khan", "Shadab", ""], ["Shahin", "Ahmed H.", ""], ["Villafruela", "Javier", ""], ["Shen", "Jianbing", ""], ["Shao", "Ling", ""]]}, {"id": "1906.02425", "submitter": "Sayna Ebrahimi", "authors": "Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, Marcus Rohrbach", "title": "Uncertainty-guided Continual Learning with Bayesian Neural Networks", "comments": "Accepted at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning aims to learn new tasks without forgetting previously\nlearned ones. This is especially challenging when one cannot access data from\nprevious tasks and when the model has a fixed capacity. Current\nregularization-based continual learning algorithms need an external\nrepresentation and extra computation to measure the parameters'\n\\textit{importance}. In contrast, we propose Uncertainty-guided Continual\nBayesian Neural Networks (UCB), where the learning rate adapts according to the\nuncertainty defined in the probability distribution of the weights in networks.\nUncertainty is a natural way to identify \\textit{what to remember} and\n\\textit{what to change} as we continually learn, and thus mitigate catastrophic\nforgetting. We also show a variant of our model, which uses uncertainty for\nweight pruning and retains task performance after pruning by saving binary\nmasks per tasks. We evaluate our UCB approach extensively on diverse object\nclassification datasets with short and long sequences of tasks and report\nsuperior or on-par performance compared to existing approaches. Additionally,\nwe show that our model does not necessarily need task information at test time,\ni.e. it does not presume knowledge of which task a sample belongs to.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 05:40:25 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 01:08:22 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Ebrahimi", "Sayna", ""], ["Elhoseiny", "Mohamed", ""], ["Darrell", "Trevor", ""], ["Rohrbach", "Marcus", ""]]}, {"id": "1906.02426", "submitter": "Cho Ying Wu", "authors": "Cho-Ying Wu, Ulrich Neumann", "title": "Salient Building Outline Enhancement and Extraction Using Iterative L0\n  Smoothing and Line Enhancing", "comments": "Accepted to ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, our goal is salient building outline enhancement and\nextraction from images taken from consumer cameras using L0 smoothing. We\naddress weak outlines and over-smoothing problem. Weak outlines are often\nundetected by edge extractors or easily smoothed out. We propose an iterative\nmethod, including the smoothing cell and sharpening cell. In the smoothing\ncell, we iteratively enlarge the smoothing level of the L0 smoothing. In the\nsharpening cell, we use Hough Transform to extract lines, based on the\nassumption that salient outlines for buildings are usually straight, and\nenhance those extracted lines. Our goal is to enhance line structures and do\nthe L0 smoothing simultaneously. Also, we propose to create building masks from\nsemantic segmentation using an encoder-decoder network. The masks filter out\nirrelevant edges. We also provide an evaluation dataset on this task.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 05:44:12 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Wu", "Cho-Ying", ""], ["Neumann", "Ulrich", ""]]}, {"id": "1906.02429", "submitter": "Cho Ying Wu", "authors": "Cho-Ying Wu, Jian-Jiun Ding", "title": "Occluded Face Recognition Using Low-rank Regression with Generalized\n  Gradient Direction", "comments": null, "journal-ref": "Pattern Recognition (PR), Elsevier, vol. 80, pp. 256-268, 2018", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a very effective method to solve the contiguous face occlusion\nrecognition problem is proposed. It utilizes the robust image gradient\ndirection features together with a variety of mapping functions and adopts a\nhierarchical sparse and low-rank regression model. This model unites the sparse\nrepresentation in dictionary learning and the low-rank representation on the\nerror term that is usually messy in the gradient domain. We call it the \"weak\nlow-rankness\" optimization problem, which can be efficiently solved by the\nframework of Alternating Direction Method of Multipliers (ADMM). The optimum of\nthe error term has a similar weak low-rank structure as the reference error map\nand the recognition performance can be enhanced by leaps and bounds using weak\nlow-rankness optimization. Extensive experiments are conducted on real-world\ndisguise / occlusion data and synthesized contiguous occlusion data. These\nexperiments show that the proposed gradient direction-based hierarchical\nadaptive sparse and low-rank (GD-HASLR) algorithm has the best performance\ncompared to state-of-the-art methods, including popular convolutional neural\nnetwork-based methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 05:58:36 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Wu", "Cho-Ying", ""], ["Ding", "Jian-Jiun", ""]]}, {"id": "1906.02467", "submitter": "Zhou Yu", "authors": "Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang,\n  Dacheng Tao", "title": "ActivityNet-QA: A Dataset for Understanding Complex Web Videos via\n  Question Answering", "comments": "Accepted at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in modeling language and vision have been successfully\napplied to image question answering. It is both crucial and natural to extend\nthis research direction to the video domain for video question answering\n(VideoQA). Compared to the image domain where large scale and fully annotated\nbenchmark datasets exists, VideoQA datasets are limited to small scale and are\nautomatically generated, etc. These limitations restrict their applicability in\npractice. Here we introduce ActivityNet-QA, a fully annotated and large scale\nVideoQA dataset. The dataset consists of 58,000 QA pairs on 5,800 complex web\nvideos derived from the popular ActivityNet dataset. We present a statistical\nanalysis of our ActivityNet-QA dataset and conduct extensive experiments on it\nby comparing existing VideoQA baselines. Moreover, we explore various video\nrepresentation strategies to improve VideoQA performance, especially for long\nvideos. The dataset is available at https://github.com/MILVLG/activitynet-qa\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 08:08:14 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Yu", "Zhou", ""], ["Xu", "Dejing", ""], ["Yu", "Jun", ""], ["Yu", "Ting", ""], ["Zhao", "Zhou", ""], ["Zhuang", "Yueting", ""], ["Tao", "Dacheng", ""]]}, {"id": "1906.02470", "submitter": "Jie An", "authors": "Jie An, Haoyi Xiong, Jinwen Ma, Jiebo Luo and Jun Huan", "title": "StyleNAS: An Empirical Study of Neural Architecture Search to Uncover\n  Surprisingly Fast End-to-End Universal Style Transfer Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) has been widely studied for designing\ndiscriminative deep learning models such as image classification, object\ndetection, and semantic segmentation. As a large number of priors have been\nobtained through the manual design of architectures in the fields, NAS is\nusually considered as a supplement approach. In this paper, we have\nsignificantly expanded the application areas of NAS by performing an empirical\nstudy of NAS to search generative models, or specifically, auto-encoder based\nuniversal style transfer, which lacks systematic exploration, if any, from the\narchitecture search aspect. In our work, we first designed a search space where\ncommon operators for image style transfer such as VGG-based encoders, whitening\nand coloring transforms (WCT), convolution kernels, instance normalization\noperators, and skip connections were searched in a combinatorial approach. With\na simple yet effective parallel evolutionary NAS algorithm with multiple\nobjectives, we derived the first group of end-to-end deep networks for\nuniversal photorealistic style transfer. Comparing to random search, a NAS\nmethod that is gaining popularity recently, we demonstrated that carefully\ndesigned search strategy leads to much better architecture design. Finally\ncompared to existing universal style transfer networks for photorealistic\nrendering such as PhotoWCT that stacks multiple well-trained auto-encoders and\nWCT transforms in a non-end-to-end manner, the architectures designed by\nStyleNAS produce better style-transferred images with details preserving, using\na tiny number of operators/parameters, and enjoying around 500x inference time\nspeed-up.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 08:21:04 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["An", "Jie", ""], ["Xiong", "Haoyi", ""], ["Ma", "Jinwen", ""], ["Luo", "Jiebo", ""], ["Huan", "Jun", ""]]}, {"id": "1906.02495", "submitter": "Annika Meyer", "authors": "Annika Meyer, Jonas Walter, Martin Lauer and Christoph Stiller", "title": "Anytime Lane-Level Intersection Estimation Based on Trajectories of\n  Other Traffic Participants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating and understanding the current scene is an inevitable capability of\nautomated vehicles. Usually, maps are used as prior for interpreting sensor\nmeasurements in order to drive safely and comfortably. Only few approaches take\ninto account that maps might be outdated and lead to wrong assumptions on the\nenvironment. This work estimates a lane-level intersection topology without any\nmap prior by observing the trajectories of other traffic participants.\n  We are able to deliver both a coarse lane-level topology as well as the lane\ncourse inside and outside of the intersection using Markov chain Monte Carlo\nsampling. The model is neither limited to a number of lanes or arms nor to the\ntopology of the intersection.\n  We present our results on an evaluation set of 1000 simulated intersections\nand achieve 99.9% accuracy on the topology estimation that takes only 36ms,\nwhen utilizing tracked object detections. The precise lane course on these\nintersections is estimated with an error of 15cm on average after 140ms. Our\napproach shows a similar level of precision on 14 real-world intersections with\n18cm average deviation on simple intersections and 27cm for more complex\nscenarios. Here the estimation takes only 113ms in total.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 09:39:41 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 14:15:55 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Meyer", "Annika", ""], ["Walter", "Jonas", ""], ["Lauer", "Martin", ""], ["Stiller", "Christoph", ""]]}, {"id": "1906.02497", "submitter": "Zhu Zhang", "authors": "Zhu Zhang, Zhijie Lin, Zhou Zhao and Zhenxin Xiao", "title": "Cross-Modal Interaction Networks for Query-Based Moment Retrieval in\n  Videos", "comments": "Accepted by SIGIR 2019 as a full paper", "journal-ref": "SIGIR, 2019, pages 655-664", "doi": "10.1145/3331184.3331235", "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query-based moment retrieval aims to localize the most relevant moment in an\nuntrimmed video according to the given natural language query. Existing works\noften only focus on one aspect of this emerging task, such as the query\nrepresentation learning, video context modeling or multi-modal fusion, thus\nfail to develop a comprehensive system for further performance improvement. In\nthis paper, we introduce a novel Cross-Modal Interaction Network (CMIN) to\nconsider multiple crucial factors for this challenging task, including (1) the\nsyntactic structure of natural language queries; (2) long-range semantic\ndependencies in video context and (3) the sufficient cross-modal interaction.\nSpecifically, we devise a syntactic GCN to leverage the syntactic structure of\nqueries for fine-grained representation learning, propose a multi-head\nself-attention to capture long-range semantic dependencies from video context,\nand next employ a multi-stage cross-modal interaction to explore the potential\nrelations of video and query contents. The extensive experiments demonstrate\nthe effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 09:45:58 GMT"}, {"version": "v2", "created": "Sat, 27 Jul 2019 10:18:43 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Zhang", "Zhu", ""], ["Lin", "Zhijie", ""], ["Zhao", "Zhou", ""], ["Xiao", "Zhenxin", ""]]}, {"id": "1906.02526", "submitter": "Tie Liu", "authors": "Tie Liu, Mai Xu, Zulin Wang", "title": "Removing Rain in Videos: A Large-scale Database and A Two-stream\n  ConvLSTM Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rain removal has recently attracted increasing research attention, as it is\nable to enhance the visibility of rain videos. However, the existing learning\nbased rain removal approaches for videos suffer from insufficient training\ndata, especially when applying deep learning to remove rain. In this paper, we\nestablish a large-scale video database for rain removal (LasVR), which consists\nof 316 rain videos. Then, we observe from our database that there exist the\ntemporal correlation of clean content and similar patterns of rain across video\nframes. According to these two observations, we propose a two-stream\nconvolutional long- and short- term memory (ConvLSTM) approach for rain removal\nin videos. The first stream is composed of the subnet for rain detection, while\nthe second stream is the subnet of rain removal that leverages the features\nfrom the rain detection subnet. Finally, the experimental results on both\nsynthetic and real rain videos show the proposed approach performs better than\nother state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 11:31:44 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Liu", "Tie", ""], ["Xu", "Mai", ""], ["Wang", "Zulin", ""]]}, {"id": "1906.02534", "submitter": "Nicolas Pugeault Dr", "authors": "Faisal Alamri and Nicolas Pugeault", "title": "Contextual Relabelling of Detected Objects", "comments": "Presented at the IEEE ICDL-Epirob'2019 conference, Oslo, Norway", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual information, such as the co-occurrence of objects and the spatial\nand relative size among objects provides deep and complex information about\nscenes. It also can play an important role in improving object detection. In\nthis work, we present two contextual models (rescoring and re-labeling models)\nthat leverage contextual information (16 contextual relationships are applied\nin this paper) to enhance the state-of-the-art RCNN-based object detection\n(Faster RCNN). We experimentally demonstrate that our models lead to\nenhancement in detection performance using the most common dataset used in this\nfield (MSCOCO).\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 11:48:36 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Alamri", "Faisal", ""], ["Pugeault", "Nicolas", ""]]}, {"id": "1906.02539", "submitter": "Qiang Zhou", "authors": "Qiang Zhou, Xin Li", "title": "STN-Homography: estimate homography parameters directly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the STN-Homography model to directly estimate the\nhomography matrix between image pair. Different most CNN-based homography\nestimation methods which use an alternative 4-point homography\nparameterization, we use prove that, after coordinate normalization, the\nvariance of elements of coordinate normalized $3\\times3$ homography matrix is\nvery small and suitable to be regressed well with CNN. Based on proposed\nSTN-Homography, we use a hierarchical architecture which stacks several\nSTN-Homography models and successively reduce the estimation error.\nEffectiveness of the proposed method is shown through experiments on MSCOCO\ndataset, in which it significantly outperforms the state-of-the-art. The\naverage processing time of our hierarchical STN-Homography with 1 stage is only\n4.87 ms on the GPU, and the processing time for hierarchical STN-Homography\nwith 3 stages is 17.85 ms. The code will soon be open sourced.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 12:07:22 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Zhou", "Qiang", ""], ["Li", "Xin", ""]]}, {"id": "1906.02540", "submitter": "Cian M. Scannell", "authors": "Cian M. Scannell, Amedeo Chiribiri, Adriana D.M. Villa, Marcel\n  Breeuwer, Jack Lee", "title": "Hierarchical Bayesian myocardial perfusion quantification", "comments": "Published in Medical Image Analysis", "journal-ref": null, "doi": "10.1016/j.media.2019.101611", "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Purpose: Tracer-kinetic models can be used for the quantitative assessment of\ncontrast-enhanced MRI data. However, the model-fitting can produce unreliable\nresults due to the limited data acquired and the high noise levels. Such\nproblems are especially prevalent in myocardial perfusion MRI leading to the\ncompromise of constrained numerical deconvolutions and segmental signal\naveraging being commonly used as alternatives to the more complex\ntracer-kinetic models. Methods: In this work, the use of hierarchical Bayesian\ninference for the parameter estimation is explored. It is shown that with\nBayesian inference it is possible to reliably fit the two-compartment exchange\nmodel to perfusion data. The use of prior knowledge on the ranges of kinetic\nparameters and the fact that neighbouring voxels are likely to have similar\nkinetic properties combined with a Markov chain Monte Carlo based fitting\nprocedure significantly improves the reliability of the perfusion estimates\nwith compared to the traditional least-squares approach. The method is assessed\nusing both simulated and patient data. Results: The average (standard\ndeviation) normalised mean square error for the distinct noise realisations of\na simulation phantom falls from 0.32 (0.55) with the least-squares fitting to\n0.13 (0.2) using Bayesian inference. The assessment of the presence of coronary\nartery disease based purely on the quantitative MBF maps obtained using\nBayesian inference matches the visual assessment in all 24 slices. When using\nthe maps obtained by the least-squares fitting, a corresponding assessment is\nonly achieved in 16/24 slices. Conclusion: Bayesian inference allows a\nreliable, fully automated and user-independent assessment of myocardial\nperfusion on a voxel-wise level using the two-compartment exchange model.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 12:07:52 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 21:39:57 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Scannell", "Cian M.", ""], ["Chiribiri", "Amedeo", ""], ["Villa", "Adriana D. M.", ""], ["Breeuwer", "Marcel", ""], ["Lee", "Jack", ""]]}, {"id": "1906.02549", "submitter": "Zhenfang Chen", "authors": "Zhenfang Chen, Lin Ma, Wenhan Luo, Kwan-Yee K. Wong", "title": "Weakly-Supervised Spatio-Temporally Grounding Natural Sentence in Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address a novel task, namely weakly-supervised\nspatio-temporally grounding natural sentence in video. Specifically, given a\nnatural sentence and a video, we localize a spatio-temporal tube in the video\nthat semantically corresponds to the given sentence, with no reliance on any\nspatio-temporal annotations during training. First, a set of spatio-temporal\ntubes, referred to as instances, are extracted from the video. We then encode\nthese instances and the sentence using our proposed attentive interactor which\ncan exploit their fine-grained relationships to characterize their matching\nbehaviors. Besides a ranking loss, a novel diversity loss is introduced to\ntrain the proposed attentive interactor to strengthen the matching behaviors of\nreliable instance-sentence pairs and penalize the unreliable ones. Moreover, we\nalso contribute a dataset, called VID-sentence, based on the ImageNet video\nobject detection dataset, to serve as a benchmark for our task. Extensive\nexperimental results demonstrate the superiority of our model over the baseline\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 12:32:55 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Chen", "Zhenfang", ""], ["Ma", "Lin", ""], ["Luo", "Wenhan", ""], ["Wong", "Kwan-Yee K.", ""]]}, {"id": "1906.02567", "submitter": "Tanju Sirmen R", "authors": "R.Tanju Sirmen, B. Burak Ustundag", "title": "An Information-Theoretical Approach to the Information Capacity and\n  Cost-Effectiveness Evaluation of Color Palettes", "comments": "9 pages", "journal-ref": "International Journal of Computing and Optimization, Vol.4, 2017,\n  No.1, pp. 43-51", "doi": "10.12988/ijco.2017.759", "report-no": null, "categories": "cs.HC cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colors are used as effective tools of representing and transferring\ninformation. Number of colors in a palette is the direct arbiter of the\ninformation conveying capacity. Yet it should be well elaborated, since\nincreasing the entropy by adding colors comes with its cost on decoding.\nDespite the possible effects upon diverse applications, a methodology for\ncost-effectiveness evaluation of palettes seems deficient. In this work, this\nneed is being addressed from an information-theoretical perspective, via the\narticulated metrics and formulae. Besides, the proposed metrics are computed\nfor some developed and known palettes, and observed results are evaluated.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 13:38:01 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Sirmen", "R. Tanju", ""], ["Ustundag", "B. Burak", ""]]}, {"id": "1906.02595", "submitter": "Hengameh Mirzaalian", "authors": "Hengameh Mirzaalian, Mohamed Hussein, Wael Abd-Almageed", "title": "On the Effectiveness of Laser Speckle Contrast Imaging and Deep Neural\n  Networks for Detecting Known and Unknown Fingerprint Presentation Attacks", "comments": "International Conference on Biometrics (ICB 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprint presentation attack detection (FPAD) is becoming an increasingly\nchallenging problem due to the continuous advancement of attack techniques,\nwhich generate `realistic-looking' fake fingerprint presentations. Recently,\nlaser speckle contrast imaging (LSCI) has been introduced as a new sensing\nmodality for FPAD. LSCI has the interesting characteristic of capturing the\nblood flow under the skin surface. Toward studying the importance and\neffectiveness of LSCI for FPAD, we conduct a comprehensive study using\ndifferent patch-based deep neural network architectures. Our studied\narchitectures include 2D and 3D convolutional networks as well as a recurrent\nnetwork using long short-term memory (LSTM) units. The study demonstrates that\nstrong FPAD performance can be achieved using LSCI. We evaluate the different\nmodels over a new large dataset. The dataset consists of 3743 bona fide\nsamples, collected from 335 unique subjects, and 218 presentation attack\nsamples, including six different types of attacks. To examine the effect of\nchanging the training and testing sets, we conduct a 3-fold cross validation\nevaluation. To examine the effect of the presence of an unseen attack, we apply\na leave-one-attack out strategy. The FPAD classification results of the\nnetworks, which are separately optimized and tuned for the temporal and spatial\npatch-sizes, indicate that the best performance is achieved by LSTM.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 13:58:44 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Mirzaalian", "Hengameh", ""], ["Hussein", "Mohamed", ""], ["Abd-Almageed", "Wael", ""]]}, {"id": "1906.02611", "submitter": "Raphael Gontijo Lopes", "authors": "Raphael Gontijo Lopes, Dong Yin, Ben Poole, Justin Gilmer, Ekin D.\n  Cubuk", "title": "Improving Robustness Without Sacrificing Accuracy with Patch Gaussian\n  Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deploying machine learning systems in the real world requires both high\naccuracy on clean data and robustness to naturally occurring corruptions. While\narchitectural advances have led to improved accuracy, building robust models\nremains challenging. Prior work has argued that there is an inherent trade-off\nbetween robustness and accuracy, which is exemplified by standard data augment\ntechniques such as Cutout, which improves clean accuracy but not robustness,\nand additive Gaussian noise, which improves robustness but hurts accuracy. To\novercome this trade-off, we introduce Patch Gaussian, a simple augmentation\nscheme that adds noise to randomly selected patches in an input image. Models\ntrained with Patch Gaussian achieve state of the art on the CIFAR-10 and\nImageNetCommon Corruptions benchmarks while also improving accuracy on clean\ndata. We find that this augmentation leads to reduced sensitivity to high\nfrequency noise(similar to Gaussian) while retaining the ability to take\nadvantage of relevant high frequency information in the image (similar to\nCutout). Finally, we show that Patch Gaussian can be used in conjunction with\nother regularization methods and data augmentation policies such as\nAutoAugment, and improves performance on the COCO object detection benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 17:54:24 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Lopes", "Raphael Gontijo", ""], ["Yin", "Dong", ""], ["Poole", "Ben", ""], ["Gilmer", "Justin", ""], ["Cubuk", "Ekin D.", ""]]}, {"id": "1906.02634", "submitter": "Oscar T\\\"ackstr\\\"om", "authors": "Dirk Weissenborn, Oscar T\\\"ackstr\\\"om, Jakob Uszkoreit", "title": "Scaling Autoregressive Video Models", "comments": "International Conference on Learning Representations (ICLR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the statistical complexity of video, the high degree of inherent\nstochasticity, and the sheer amount of data, generating natural video remains a\nchallenging task. State-of-the-art video generation models often attempt to\naddress these issues by combining sometimes complex, usually video-specific\nneural network architectures, latent variable models, adversarial training and\na range of other methods. Despite their often high complexity, these approaches\nstill fall short of generating high quality video continuations outside of\nnarrow domains and often struggle with fidelity. In contrast, we show that\nconceptually simple autoregressive video generation models based on a\nthree-dimensional self-attention mechanism achieve competitive results across\nmultiple metrics on popular benchmark datasets, for which they produce\ncontinuations of high fidelity and realism. We also present results from\ntraining our models on Kinetics, a large scale action recognition dataset\ncomprised of YouTube videos exhibiting phenomena such as camera movement,\ncomplex object interactions and diverse human movement. While modeling these\nphenomena consistently remains elusive, we hope that our results, which include\noccasional realistic continuations encourage further research on comparatively\ncomplex, large scale datasets such as Kinetics.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 15:06:21 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 17:43:56 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2020 19:29:56 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Weissenborn", "Dirk", ""], ["T\u00e4ckstr\u00f6m", "Oscar", ""], ["Uszkoreit", "Jakob", ""]]}, {"id": "1906.02659", "submitter": "Ishan Misra", "authors": "Terrance DeVries, Ishan Misra, Changhan Wang, Laurens van der Maaten", "title": "Does Object Recognition Work for Everyone?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper analyzes the accuracy of publicly available object-recognition\nsystems on a geographically diverse dataset. This dataset contains household\nitems and was designed to have a more representative geographical coverage than\ncommonly used image datasets in object recognition. We find that the systems\nperform relatively poorly on household items that commonly occur in countries\nwith a low household income. Qualitative analyses suggest the drop in\nperformance is primarily due to appearance differences within an object class\n(e.g., dish soap) and due to items appearing in a different context (e.g.,\ntoothbrushes appearing outside of bathrooms). The results of our study suggest\nthat further work is needed to make object-recognition systems work equally\nwell for people across different countries and income levels.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 16:00:18 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 17:37:44 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["DeVries", "Terrance", ""], ["Misra", "Ishan", ""], ["Wang", "Changhan", ""], ["van der Maaten", "Laurens", ""]]}, {"id": "1906.02679", "submitter": "Yan Shi", "authors": "Yan Shi, Dezhi Feng, and Subir Biswas", "title": "A Natural Language-Inspired Multi-label Video Streaming Traffic\n  Classification Method Based on Deep Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1007/s11760-020-01844-8", "report-no": null, "categories": "eess.SP cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a deep-learning based traffic classification method for\nidentifying multiple streaming video sources at the same time within an\nencrypted tunnel. The work defines a novel feature inspired by Natural Language\nProcessing (NLP) that allows existing NLP techniques to help the traffic\nclassification. The feature extraction method is described, and a large dataset\ncontaining video streaming and web traffic is created to verify its\neffectiveness. Results are obtained by applying several NLP methods to show\nthat the proposed method performs well on both binary and multilabel traffic\nclassification problems. We also show the ability to achieve zero-shot learning\nwith the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 00:21:21 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Shi", "Yan", ""], ["Feng", "Dezhi", ""], ["Biswas", "Subir", ""]]}, {"id": "1906.02728", "submitter": "Jie Cai", "authors": "Jie Cai, Zibo Meng, Ahmed Shehab Khan, Zhiyuan Li, James O'Reilly,\n  Shizhong Han, Ping Liu, Min Chen, Yan Tong", "title": "Feature-level and Model-level Audiovisual Fusion for Emotion Recognition\n  in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion recognition plays an important role in human-computer interaction\n(HCI) and has been extensively studied for decades. Although tremendous\nimprovements have been achieved for posed expressions, recognizing human\nemotions in \"close-to-real-world\" environments remains a challenge. In this\npaper, we proposed two strategies to fuse information extracted from different\nmodalities, i.e., audio and visual. Specifically, we utilized LBP-TOP, an\nensemble of CNNs, and a bi-directional LSTM (BLSTM) to extract features from\nthe visual channel, and the OpenSmile toolkit to extract features from the\naudio channel. Two kinds of fusion methods, i,e., feature-level fusion and\nmodel-level fusion, were developed to utilize the information extracted from\nthe two channels. Experimental results on the EmotiW2018 AFEW dataset have\nshown that the proposed fusion methods outperform the baseline methods\nsignificantly and achieve better or at least comparable performance compared\nwith the state-of-the-art methods, where the model-level fusion performs better\nwhen one of the channels totally fails.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 17:49:41 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Cai", "Jie", ""], ["Meng", "Zibo", ""], ["Khan", "Ahmed Shehab", ""], ["Li", "Zhiyuan", ""], ["O'Reilly", "James", ""], ["Han", "Shizhong", ""], ["Liu", "Ping", ""], ["Chen", "Min", ""], ["Tong", "Yan", ""]]}, {"id": "1906.02729", "submitter": "Nilesh Kulkarni", "authors": "Nilesh Kulkarni, Ishan Misra, Shubham Tulsiani, Abhinav Gupta", "title": "3D-RelNet: Joint Object and Relational Network for 3D Prediction", "comments": "Project page: https://nileshkulkarni.github.io/relative3d/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to predict the 3D shape and pose for the objects\npresent in a scene. Existing learning based methods that pursue this goal make\nindependent predictions per object, and do not leverage the relationships\namongst them. We argue that reasoning about these relationships is crucial, and\npresent an approach to incorporate these in a 3D prediction framework. In\naddition to independent per-object predictions, we predict pairwise relations\nin the form of relative 3D pose, and demonstrate that these can be easily\nincorporated to improve object level estimates. We report performance across\ndifferent datasets (SUNCG, NYUv2), and show that our approach significantly\nimproves over independent prediction approaches while also outperforming\nalternate implicit reasoning methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 17:50:48 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 17:49:01 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2020 04:10:25 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Kulkarni", "Nilesh", ""], ["Misra", "Ishan", ""], ["Tulsiani", "Shubham", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1906.02739", "submitter": "Georgia Gkioxari", "authors": "Georgia Gkioxari, Jitendra Malik, Justin Johnson", "title": "Mesh R-CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid advances in 2D perception have led to systems that accurately detect\nobjects in real-world images. However, these systems make predictions in 2D,\nignoring the 3D structure of the world. Concurrently, advances in 3D shape\nprediction have mostly focused on synthetic benchmarks and isolated objects. We\nunify advances in these two areas. We propose a system that detects objects in\nreal-world images and produces a triangle mesh giving the full 3D shape of each\ndetected object. Our system, called Mesh R-CNN, augments Mask R-CNN with a mesh\nprediction branch that outputs meshes with varying topological structure by\nfirst predicting coarse voxel representations which are converted to meshes and\nrefined with a graph convolution network operating over the mesh's vertices and\nedges. We validate our mesh prediction branch on ShapeNet, where we outperform\nprior work on single-image shape prediction. We then deploy our full Mesh R-CNN\nsystem on Pix3D, where we jointly detect objects and predict their 3D shapes.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 17:56:09 GMT"}, {"version": "v2", "created": "Sat, 25 Jan 2020 21:56:36 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Gkioxari", "Georgia", ""], ["Malik", "Jitendra", ""], ["Johnson", "Justin", ""]]}, {"id": "1906.02783", "submitter": "Antti Hietanen", "authors": "Antti Hietanen, Jyrki Latokartano, Alessandro Foi, Roel Pieters, Ville\n  Kyrki, Minna Lanz and Joni-Kristian K\\\"am\\\"ar\\\"ainen", "title": "Object Pose Estimation in Robotics Revisited", "comments": "29 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision based object grasping and manipulation in robotics require accurate\nestimation of object's 6D pose. The 6D pose estimation has received significant\nattention in computer vision community and multiple datasets and evaluation\nmetrics have been proposed. However, the existing metrics measure how well two\ngeometrical surfaces are aligned - ground truth vs. estimated pose - which does\nnot directly measure how well a robot can perform the task with the given\nestimate. In this work we propose a probabilistic metric that directly measures\nsuccess in robotic tasks. The evaluation metric is based on non-parametric\nprobability density that is estimated from samples of a real physical setup.\nDuring the pose evaluation stage the physical setup is not needed. The\nevaluation metric is validated in controlled experiments and a new pose\nestimation dataset of industrial parts is introduced. The experimental results\nwith the parts confirm that the proposed evaluation metric better reflects the\ntrue performance in robotics than the existing metrics.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 19:27:48 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 07:12:42 GMT"}, {"version": "v3", "created": "Thu, 21 May 2020 17:09:25 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Hietanen", "Antti", ""], ["Latokartano", "Jyrki", ""], ["Foi", "Alessandro", ""], ["Pieters", "Roel", ""], ["Kyrki", "Ville", ""], ["Lanz", "Minna", ""], ["K\u00e4m\u00e4r\u00e4inen", "Joni-Kristian", ""]]}, {"id": "1906.02792", "submitter": "Manjot Bilkhu", "authors": "Manjot Bilkhu, Siyang Wang, Tushar Dobhal", "title": "Attention is all you need for Videos: Self-attention based Video\n  Summarization using Universal Transformers", "comments": "15 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video Captioning and Summarization have become very popular in the recent\nyears due to advancements in Sequence Modelling, with the resurgence of\nLong-Short Term Memory networks (LSTMs) and introduction of Gated Recurrent\nUnits (GRUs). Existing architectures extract spatio-temporal features using\nCNNs and utilize either GRUs or LSTMs to model dependencies with soft attention\nlayers. These attention layers do help in attending to the most prominent\nfeatures and improve upon the recurrent units, however, these models suffer\nfrom the inherent drawbacks of the recurrent units themselves. The introduction\nof the Transformer model has driven the Sequence Modelling field into a new\ndirection. In this project, we implement a Transformer-based model for Video\ncaptioning, utilizing 3D CNN architectures like C3D and Two-stream I3D for\nvideo extraction. We also apply certain dimensionality reduction techniques so\nas to keep the overall size of the model within limits. We finally present our\nresults on the MSVD and ActivityNet datasets for Single and Dense video\ncaptioning tasks respectively.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 19:59:56 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Bilkhu", "Manjot", ""], ["Wang", "Siyang", ""], ["Dobhal", "Tushar", ""]]}, {"id": "1906.02809", "submitter": "Vasileios Argyriou", "authors": "Mahdi Maktabdar Oghaz, Manzoor Razaak, Hamideh Kerdegari, Vasileios\n  Argyriou, Paolo Remagnino", "title": "Scene and Environment Monitoring Using Aerial Imagery and Deep Learning", "comments": "8", "journal-ref": "IoTI4 Workshop 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmanned Aerial vehicles (UAV) are a promising technology for smart farming\nrelated applications. Aerial monitoring of agriculture farms with UAV enables\nkey decision-making pertaining to crop monitoring. Advancements in deep\nlearning techniques have further enhanced the precision and reliability of\naerial imagery based analysis. The capabilities to mount various kinds of\nsensors (RGB, spectral cameras) on UAV allows remote crop analysis applications\nsuch as vegetation classification and segmentation, crop counting, yield\nmonitoring and prediction, crop mapping, weed detection, disease and nutrient\ndeficiency detection and others. A significant amount of studies are found in\nthe literature that explores UAV for smart farming applications. In this paper,\na review of studies applying deep learning on UAV imagery for smart farming is\npresented. Based on the application, we have classified these studies into five\nmajor groups including: vegetation identification, classification and\nsegmentation, crop counting and yield predictions, crop mapping, weed detection\nand crop disease and nutrient deficiency detection. An in depth critical\nanalysis of each study is provided.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 20:58:39 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Oghaz", "Mahdi Maktabdar", ""], ["Razaak", "Manzoor", ""], ["Kerdegari", "Hamideh", ""], ["Argyriou", "Vasileios", ""], ["Remagnino", "Paolo", ""]]}, {"id": "1906.02817", "submitter": "Zhuotun Zhu", "authors": "Zhuotun Zhu, Chenxi Liu, Dong Yang, Alan Yuille and Daguang Xu", "title": "V-NAS: Neural Architecture Search for Volumetric Medical Image\n  Segmentation", "comments": "Accepted by 3DV, camera ready version. 8 pages, 4 figures and 5\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning algorithms, in particular 2D and 3D fully convolutional neural\nnetworks (FCNs), have rapidly become the mainstream methodology for volumetric\nmedical image segmentation. However, 2D convolutions cannot fully leverage the\nrich spatial information along the third axis, while 3D convolutions suffer\nfrom the demanding computation and high GPU memory consumption. In this paper,\nwe propose to automatically search the network architecture tailoring to\nvolumetric medical image segmentation problem. Concretely, we formulate the\nstructure learning as differentiable neural architecture search, and let the\nnetwork itself choose between 2D, 3D or Pseudo-3D (P3D) convolutions at each\nlayer. We evaluate our method on 3 public datasets, i.e., the NIH Pancreas\ndataset, the Lung and Pancreas dataset from the Medical Segmentation Decathlon\n(MSD) Challenge. Our method, named V-NAS, consistently outperforms other\nstate-of-the-arts on the segmentation task of both normal organ (NIH Pancreas)\nand abnormal organs (MSD Lung tumors and MSD Pancreas tumors), which shows the\npower of chosen architecture. Moreover, the searched architecture on one\ndataset can be well generalized to other datasets, which demonstrates the\nrobustness and practical use of our proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 21:07:40 GMT"}, {"version": "v2", "created": "Sun, 4 Aug 2019 22:04:33 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Zhu", "Zhuotun", ""], ["Liu", "Chenxi", ""], ["Yang", "Dong", ""], ["Yuille", "Alan", ""], ["Xu", "Daguang", ""]]}, {"id": "1906.02823", "submitter": "Vasileios Argyriou", "authors": "Robert Dupre, Jiri Fajtl, Vasileios Argyriou, Paolo Remagnin", "title": "Iterative Self-Learning: Semi-Supervised Improvement to Dataset Volumes\n  and Model Accuracy", "comments": null, "journal-ref": "CVPR'2019 workshop - Uncertainty and Robustness in Deep Visual\n  Learning", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel semi-supervised learning technique is introduced based on a simple\niterative learning cycle together with learned thresholding techniques and an\nensemble decision support system. State-of-the-art model performance and\nincreased training data volume are demonstrated, through the use of unlabelled\ndata when training deeply learned classification models. Evaluation of the\nproposed approach is performed on commonly used datasets when evaluating\nsemi-supervised learning techniques as well as a number of more challenging\nimage classification datasets (CIFAR-100 and a 200 class subset of ImageNet).\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 21:20:12 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Dupre", "Robert", ""], ["Fajtl", "Jiri", ""], ["Argyriou", "Vasileios", ""], ["Remagnin", "Paolo", ""]]}, {"id": "1906.02825", "submitter": "Tolga Bolukbasi", "authors": "Andrei Kapishnikov, Tolga Bolukbasi, Fernanda Vi\\'egas, Michael Terry", "title": "XRAI: Better Attributions Through Regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency methods can aid understanding of deep neural networks. Recent years\nhave witnessed many improvements to saliency methods, as well as new ways for\nevaluating them. In this paper, we 1) present a novel region-based attribution\nmethod, XRAI, that builds upon integrated gradients (Sundararajan et al. 2017),\n2) introduce evaluation methods for empirically assessing the quality of\nimage-based saliency maps (Performance Information Curves (PICs)), and 3)\ncontribute an axiom-based sanity check for attribution methods. Through\nempirical experiments and example results, we show that XRAI produces better\nresults than other saliency methods for common models and the ImageNet dataset.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 21:21:39 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 21:09:28 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Kapishnikov", "Andrei", ""], ["Bolukbasi", "Tolga", ""], ["Vi\u00e9gas", "Fernanda", ""], ["Terry", "Michael", ""]]}, {"id": "1906.02831", "submitter": "Zheheng Jiang", "authors": "Zheheng Jiang, Zhihua Liu, Long Chen, Lei Tong, Xiangrong Zhang,\n  Xiangyuan Lan, Danny Crookes, Ming-Hsuan Yang and Huiyu Zhou", "title": "Detection and Tracking of Multiple Mice Using Part Proposal Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of mouse social behaviours has been increasingly undertaken in\nneuroscience research. However, automated quantification of mouse behaviours\nfrom the videos of interacting mice is still a challenging problem, where\nobject tracking plays a key role in locating mice in their living spaces.\nArtificial markers are often applied for multiple mice tracking, which are\nintrusive and consequently interfere with the movements of mice in a dynamic\nenvironment. In this paper, we propose a novel method to continuously track\nseveral mice and individual parts without requiring any specific tagging.\nFirstly, we propose an efficient and robust deep learning based mouse part\ndetection scheme to generate part candidates. Subsequently, we propose a novel\nBayesian Integer Linear Programming Model that jointly assigns the part\ncandidates to individual targets with necessary geometric constraints whilst\nestablishing pair-wise association between the detected parts. There is no\npublicly available dataset in the research community that provides a\nquantitative test-bed for the part detection and tracking of multiple mice, and\nwe here introduce a new challenging Multi-Mice PartsTrack dataset that is made\nof complex behaviours and actions. Finally, we evaluate our proposed approach\nagainst several baselines on our new datasets, where the results show that our\nmethod outperforms the other state-of-the-art approaches in terms of accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 22:04:12 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 13:42:56 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Jiang", "Zheheng", ""], ["Liu", "Zhihua", ""], ["Chen", "Long", ""], ["Tong", "Lei", ""], ["Zhang", "Xiangrong", ""], ["Lan", "Xiangyuan", ""], ["Crookes", "Danny", ""], ["Yang", "Ming-Hsuan", ""], ["Zhou", "Huiyu", ""]]}, {"id": "1906.02839", "submitter": "Dim Papadopoulos P", "authors": "Dim P. Papadopoulos, Youssef Tamaazousti, Ferda Ofli, Ingmar Weber,\n  Antonio Torralba", "title": "How to make a pizza: Learning a compositional layer-based GAN model", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A food recipe is an ordered set of instructions for preparing a particular\ndish. From a visual perspective, every instruction step can be seen as a way to\nchange the visual appearance of the dish by adding extra objects (e.g., adding\nan ingredient) or changing the appearance of the existing ones (e.g., cooking\nthe dish). In this paper, we aim to teach a machine how to make a pizza by\nbuilding a generative model that mirrors this step-by-step procedure. To do so,\nwe learn composable module operations which are able to either add or remove a\nparticular ingredient. Each operator is designed as a Generative Adversarial\nNetwork (GAN). Given only weak image-level supervision, the operators are\ntrained to generate a visual layer that needs to be added to or removed from\nthe existing image. The proposed model is able to decompose an image into an\nordered sequence of layers by applying sequentially in the right order the\ncorresponding removing modules. Experimental results on synthetic and real\npizza images demonstrate that our proposed model is able to: (1) segment pizza\ntoppings in a weaklysupervised fashion, (2) remove them by revealing what is\noccluded underneath them (i.e., inpainting), and (3) infer the ordering of the\ntoppings without any depth ordering supervision. Code, data, and models are\navailable online.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 23:22:31 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Papadopoulos", "Dim P.", ""], ["Tamaazousti", "Youssef", ""], ["Ofli", "Ferda", ""], ["Weber", "Ingmar", ""], ["Torralba", "Antonio", ""]]}, {"id": "1906.02849", "submitter": "Jose Dolz", "authors": "Ashish Sinha and Jose Dolz", "title": "Multi-scale self-guided attention for medical image segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though convolutional neural networks (CNNs) are driving progress in\nmedical image segmentation, standard models still have some drawbacks. First,\nthe use of multi-scale approaches, i.e., encoder-decoder architectures, leads\nto a redundant use of information, where similar low-level features are\nextracted multiple times at multiple scales. Second, long-range feature\ndependencies are not efficiently modeled, resulting in non-optimal\ndiscriminative feature representations associated with each semantic class. In\nthis paper we attempt to overcome these limitations with the proposed\narchitecture, by capturing richer contextual dependencies based on the use of\nguided self-attention mechanisms. This approach is able to integrate local\nfeatures with their corresponding global dependencies, as well as highlight\ninterdependent channel maps in an adaptive manner. Further, the additional loss\nbetween different modules guides the attention mechanisms to neglect irrelevant\ninformation and focus on more discriminant regions of the image by emphasizing\nrelevant feature associations. We evaluate the proposed model in the context of\nsemantic segmentation on three different datasets: abdominal organs,\ncardiovascular structures and brain tumors. A series of ablation experiments\nsupport the importance of these attention modules in the proposed architecture.\nIn addition, compared to other state-of-the-art segmentation networks our model\nyields better segmentation performance, increasing the accuracy of the\npredictions while reducing the standard deviation. This demonstrates the\nefficiency of our approach to generate precise and reliable automatic\nsegmentations of medical images. Our code is made publicly available at\nhttps://github.com/sinAshish/Multi-Scale-Attention\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 00:54:05 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2019 01:34:12 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2020 20:23:08 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Sinha", "Ashish", ""], ["Dolz", "Jose", ""]]}, {"id": "1906.02850", "submitter": "Charles Chen", "authors": "Charles Chen, Ruiyi Zhang, Eunyee Koh, Sungchul Kim, Scott Cohen, Tong\n  Yu, Ryan Rossi, Razvan Bunescu", "title": "Figure Captioning with Reasoning and Sequence-Level Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Figures, such as bar charts, pie charts, and line plots, are widely used to\nconvey important information in a concise format. They are usually\nhuman-friendly but difficult for computers to process automatically. In this\nwork, we investigate the problem of figure captioning where the goal is to\nautomatically generate a natural language description of the figure. While\nnatural image captioning has been studied extensively, figure captioning has\nreceived relatively little attention and remains a challenging problem. First,\nwe introduce a new dataset for figure captioning, FigCAP, based on FigureQA.\nSecond, we propose two novel attention mechanisms. To achieve accurate\ngeneration of labels in figures, we propose Label Maps Attention. To model the\nrelations between figure labels, we propose Relation Maps Attention. Third, we\nuse sequence-level training with reinforcement learning in order to directly\noptimizes evaluation metrics, which alleviates the exposure bias issue and\nfurther improves the models in generating long captions. Extensive experiments\nshow that the proposed method outperforms the baselines, thus demonstrating a\nsignificant potential for the automatic captioning of vast repositories of\nfigures.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 00:54:53 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Chen", "Charles", ""], ["Zhang", "Ruiyi", ""], ["Koh", "Eunyee", ""], ["Kim", "Sungchul", ""], ["Cohen", "Scott", ""], ["Yu", "Tong", ""], ["Rossi", "Ryan", ""], ["Bunescu", "Razvan", ""]]}, {"id": "1906.02851", "submitter": "Longlong Jing", "authors": "Longlong Jing, Elahe Vahdani, Matt Huenerfauth, Yingli Tian", "title": "Recognizing American Sign Language Manual Signs from RGB-D Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a 3D Convolutional Neural Network (3DCNN) based\nmulti-stream framework to recognize American Sign Language (ASL) manual signs\n(consisting of movements of the hands, as well as non-manual face movements in\nsome cases) in real-time from RGB-D videos, by fusing multimodality features\nincluding hand gestures, facial expressions, and body poses from multi-channels\n(RGB, depth, motion, and skeleton joints). To learn the overall temporal\ndynamics in a video, a proxy video is generated by selecting a subset of frames\nfor each video which are then used to train the proposed 3DCNN model. We\ncollect a new ASL dataset, ASL-100-RGBD, which contains 42 RGB-D videos\ncaptured by a Microsoft Kinect V2 camera, each of 100 ASL manual signs,\nincluding RGB channel, depth maps, skeleton joints, face features, and HDface.\nThe dataset is fully annotated for each semantic region (i.e. the time duration\nof each word that the human signer performs). Our proposed method achieves\n92.88 accuracy for recognizing 100 ASL words in our newly collected\nASL-100-RGBD dataset. The effectiveness of our framework for recognizing hand\ngestures from RGB-D videos is further demonstrated on the Chalearn IsoGD\ndataset and achieves 76 accuracy which is 5.51 higher than the state-of-the-art\nwork in terms of average fusion by using only 5 channels instead of 12 channels\nin the previous work.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 00:56:11 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Jing", "Longlong", ""], ["Vahdani", "Elahe", ""], ["Huenerfauth", "Matt", ""], ["Tian", "Yingli", ""]]}, {"id": "1906.02858", "submitter": "Iacopo Masi", "authors": "Joe Mathai, Iacopo Masi, Wael AbdAlmageed", "title": "Does Generative Face Completion Help Face Recognition?", "comments": "In Proceedings Of IAPR International Conference On Biometrics 2019\n  (ICB'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face occlusions, covering either the majority or discriminative parts of the\nface, can break facial perception and produce a drastic loss of information.\nBiometric systems such as recent deep face recognition models are not immune to\nobstructions or other objects covering parts of the face. While most of the\ncurrent face recognition methods are not optimized to handle occlusions, there\nhave been a few attempts to improve robustness directly in the training stage.\nUnlike those, we propose to study the effect of generative face completion on\nthe recognition. We offer a face completion encoder-decoder, based on a\nconvolutional operator with a gating mechanism, trained with an ample set of\nface occlusions. To systematically evaluate the impact of realistic occlusions\non recognition, we propose to play the occlusion game: we render 3D objects\nonto different face parts, providing precious knowledge of what the impact is\nof effectively removing those occlusions. Extensive experiments on the Labeled\nFaces in the Wild (LFW), and its more difficult variant LFW-BLUFR, testify that\nface completion is able to partially restore face perception in machine vision\nsystems for improved recognition.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 01:48:28 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Mathai", "Joe", ""], ["Masi", "Iacopo", ""], ["AbdAlmageed", "Wael", ""]]}, {"id": "1906.02859", "submitter": "Ekim Yurtsever", "authors": "Ekim Yurtsever, Yongkang Liu, Jacob Lambert, Chiyomi Miyajima, Eijiro\n  Takeuchi, Kazuya Takeda, John H. L. Hansen", "title": "Risky Action Recognition in Lane Change Video Clips using Deep\n  Spatiotemporal Networks with Segmentation Mask Transfer", "comments": "8 pages, 3 figures, 1 table. The code is open-source", "journal-ref": "2019 IEEE Intelligent Transportation Systems Conference (ITSC),\n  Auckland, New Zealand, 2019, pp. 3100-3107", "doi": "10.1109/ITSC.2019.8917362", "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advanced driver assistance and automated driving systems rely on risk\nestimation modules to predict and avoid dangerous situations. Current methods\nuse expensive sensor setups and complex processing pipeline, limiting their\navailability and robustness. To address these issues, we introduce a novel deep\nlearning based action recognition framework for classifying dangerous lane\nchange behavior in short video clips captured by a monocular camera. We\ndesigned a deep spatiotemporal classification network that uses pre-trained\nstate-of-the-art instance segmentation network Mask R-CNN as its spatial\nfeature extractor for this task. The Long-Short Term Memory (LSTM) and\nshallower final classification layers of the proposed method were trained on a\nsemi-naturalistic lane change dataset with annotated risk labels. A\ncomprehensive comparison of state-of-the-art feature extractors was carried out\nto find the best network layout and training strategy. The best result, with a\n0.937 AUC score, was obtained with the proposed network. Our code and trained\nmodels are available open-source.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 01:53:23 GMT"}, {"version": "v2", "created": "Sun, 2 Feb 2020 16:01:54 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Yurtsever", "Ekim", ""], ["Liu", "Yongkang", ""], ["Lambert", "Jacob", ""], ["Miyajima", "Chiyomi", ""], ["Takeuchi", "Eijiro", ""], ["Takeda", "Kazuya", ""], ["Hansen", "John H. L.", ""]]}, {"id": "1906.02865", "submitter": "Sepehr Eghbali", "authors": "Sepehr Eghbali and Ladan Tahvildari", "title": "Deep Spherical Quantization for Image Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing methods, which encode high-dimensional images with compact discrete\ncodes, have been widely applied to enhance large-scale image retrieval. In this\npaper, we put forward Deep Spherical Quantization (DSQ), a novel method to make\ndeep convolutional neural networks generate supervised and compact binary codes\nfor efficient image search. Our approach simultaneously learns a mapping that\ntransforms the input images into a low-dimensional discriminative space, and\nquantizes the transformed data points using multi-codebook quantization. To\neliminate the negative effect of norm variance on codebook learning, we force\nthe network to L_2 normalize the extracted features and then quantize the\nresulting vectors using a new supervised quantization technique specifically\ndesigned for points lying on a unit hypersphere. Furthermore, we introduce an\neasy-to-implement extension of our quantization technique that enforces\nsparsity on the codebooks. Extensive experiments demonstrate that DSQ and its\nsparse variant can generate semantically separable compact binary codes\noutperforming many state-of-the-art image retrieval methods on three\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 02:21:16 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Eghbali", "Sepehr", ""], ["Tahvildari", "Ladan", ""]]}, {"id": "1906.02885", "submitter": "Pulak Purkait", "authors": "Pulak Purkait and Christopher Zach and Ian Reid", "title": "Seeing Behind Things: Extending Semantic Segmentation to Occluded\n  Regions", "comments": null, "journal-ref": "IROS 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Semantic segmentation and instance level segmentation made substantial\nprogress in recent years due to the emergence of deep neural networks (DNNs). A\nnumber of deep architectures with Convolution Neural Networks (CNNs) were\nproposed that surpass the traditional machine learning approaches for\nsegmentation by a large margin. These architectures predict the directly\nobservable semantic category of each pixel by usually optimizing a cross\nentropy loss. In this work we push the limit of semantic segmentation towards\npredicting semantic labels of directly visible as well as occluded objects or\nobjects parts, where the network's input is a single depth image. We group the\nsemantic categories into one background and multiple foreground object groups,\nand we propose a modification of the standard cross-entropy loss to cope with\nthe settings. In our experiments we demonstrate that a CNN trained by\nminimizing the proposed loss is able to predict semantic categories for visible\nand occluded object parts without requiring to increase the network size\n(compared to a standard segmentation task). The results are validated on a\nnewly generated dataset (augmented from SUNCG) dataset.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 03:30:10 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 07:26:21 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Purkait", "Pulak", ""], ["Zach", "Christopher", ""], ["Reid", "Ian", ""]]}, {"id": "1906.02890", "submitter": "Haoyue Shi", "authors": "Haoyue Shi, Jiayuan Mao, Kevin Gimpel, Karen Livescu", "title": "Visually Grounded Neural Syntax Acquisition", "comments": "ACL 2019. Project page:\n  https://ttic.uchicago.edu/~freda/project/vgnsl/", "journal-ref": null, "doi": "10.18653/v1/P19-1180", "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Visually Grounded Neural Syntax Learner (VG-NSL), an approach\nfor learning syntactic representations and structures without any explicit\nsupervision. The model learns by looking at natural images and reading paired\ncaptions. VG-NSL generates constituency parse trees of texts, recursively\ncomposes representations for constituents, and matches them with images. We\ndefine concreteness of constituents by their matching scores with images, and\nuse it to guide the parsing of text. Experiments on the MSCOCO data set show\nthat VG-NSL outperforms various unsupervised parsing approaches that do not use\nvisual grounding, in terms of F1 scores against gold parse trees. We find that\nVGNSL is much more stable with respect to the choice of random initialization\nand the amount of training data. We also find that the concreteness acquired by\nVG-NSL correlates well with a similar measure defined by linguists. Finally, we\nalso apply VG-NSL to multiple languages in the Multi30K data set, showing that\nour model consistently outperforms prior unsupervised approaches.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 04:03:53 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 18:29:51 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Shi", "Haoyue", ""], ["Mao", "Jiayuan", ""], ["Gimpel", "Kevin", ""], ["Livescu", "Karen", ""]]}, {"id": "1906.02899", "submitter": "Yue He", "authors": "Yue He, Zheyan Shen, Peng Cui", "title": "Towards Non-I.I.D. Image Classification: A Dataset and Baselines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I.I.D. hypothesis between training and testing data is the basis of numerous\nimage classification methods. Such property can hardly be guaranteed in\npractice where the Non-IIDness is common, causing instable performances of\nthese models. In literature, however, the Non-I.I.D. image classification\nproblem is largely understudied. A key reason is lacking of a well-designed\ndataset to support related research. In this paper, we construct and release a\nNon-I.I.D. image dataset called NICO, which uses contexts to create Non-IIDness\nconsciously. Compared to other datasets, extended analyses prove NICO can\nsupport various Non-I.I.D. situations with sufficient flexibility. Meanwhile,\nwe propose a baseline model with ConvNet structure for General Non-I.I.D. image\nclassification, where distribution of testing data is unknown but different\nfrom training data. The experimental results demonstrate that NICO can well\nsupport the training of ConvNet model from scratch, and a batch balancing\nmodule can help ConvNets to perform better in Non-I.I.D. settings.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 05:07:07 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 16:09:00 GMT"}, {"version": "v3", "created": "Wed, 14 Aug 2019 09:18:14 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["He", "Yue", ""], ["Shen", "Zheyan", ""], ["Cui", "Peng", ""]]}, {"id": "1906.02901", "submitter": "Yizhe Zhang", "authors": "Yizhe Zhang, Michael T. C. Ying, Danny Z. Chen", "title": "Decompose-and-Integrate Learning for Multi-class Segmentation in Medical\n  Images", "comments": "To appear in MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation maps of medical images annotated by medical experts contain rich\nspatial information. In this paper, we propose to decompose annotation maps to\nlearn disentangled and richer feature transforms for segmentation problems in\nmedical images. Our new scheme consists of two main stages: decompose and\nintegrate. Decompose: by annotation map decomposition, the original\nsegmentation problem is decomposed into multiple segmentation sub-problems;\nthese new segmentation sub-problems are modeled by training multiple deep\nlearning modules, each with its own set of feature transforms. Integrate: a\nprocedure summarizes the solutions of the modules in the previous stage; a\nfinal solution is then formed for the original segmentation problem. Multiple\nways of annotation map decomposition are presented and a new end-to-end\ntrainable K-to-1 deep network framework is developed for implementing our\nproposed \"decompose-and-integrate\" learning scheme. In experiments, we\ndemonstrate that our decompose-and-integrate segmentation, utilizing\nstate-of-the-art fully convolutional networks (e.g., DenseVoxNet in 3D and\nCUMedNet in 2D), improves segmentation performance on multiple 3D and 2D\ndatasets. Ablation study confirms the effectiveness of our proposed learning\nscheme for medical images.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 05:10:35 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Zhang", "Yizhe", ""], ["Ying", "Michael T. C.", ""], ["Chen", "Danny Z.", ""]]}, {"id": "1906.02909", "submitter": "Wei Wen", "authors": "Wei Wen, Feng Yan, Yiran Chen, Hai Li", "title": "AutoGrow: Automatic Layer Growing in Deep Convolutional Networks", "comments": "KDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth is a key component of Deep Neural Networks (DNNs), however, designing\ndepth is heuristic and requires many human efforts. We propose AutoGrow to\nautomate depth discovery in DNNs: starting from a shallow seed architecture,\nAutoGrow grows new layers if the growth improves the accuracy; otherwise, stops\ngrowing and thus discovers the depth. We propose robust growing and stopping\npolicies to generalize to different network architectures and datasets. Our\nexperiments show that by applying the same policy to different network\narchitectures, AutoGrow can always discover near-optimal depth on various\ndatasets of MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. For\nexample, in terms of accuracy-computation trade-off, AutoGrow discovers a\nbetter depth combination in ResNets than human experts. Our AutoGrow is\nefficient. It discovers depth within similar time of training a single DNN. Our\ncode is available at https://github.com/wenwei202/autogrow.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 05:54:41 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 01:57:53 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 00:04:22 GMT"}, {"version": "v4", "created": "Wed, 10 Jun 2020 17:06:41 GMT"}, {"version": "v5", "created": "Mon, 15 Jun 2020 18:09:02 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Wen", "Wei", ""], ["Yan", "Feng", ""], ["Chen", "Yiran", ""], ["Li", "Hai", ""]]}, {"id": "1906.02913", "submitter": "Jan Svoboda", "authors": "Jan Svoboda, Asha Anoosheh, Christian Osendorfer, Jonathan Masci", "title": "Two-Stage Peer-Regularized Feature Recombination for Arbitrary Image\n  Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a neural style transfer model to generate a stylized\nimage conditioning on a set of examples describing the desired style. The\nproposed solution produces high-quality images even in the zero-shot setting\nand allows for more freedom in changes to the content geometry. This is made\npossible by introducing a novel Two-Stage Peer-Regularization Layer that\nrecombines style and content in latent space by means of a custom graph\nconvolutional layer. Contrary to the vast majority of existing solutions, our\nmodel does not depend on any pre-trained networks for computing perceptual\nlosses and can be trained fully end-to-end thanks to a new set of cyclic losses\nthat operate directly in latent space and not on the RGB images. An extensive\nablation study confirms the usefulness of the proposed losses and of the\nTwo-Stage Peer-Regularization Layer, with qualitative results that are\ncompetitive with respect to the current state of the art using a single model\nfor all presented styles. This opens the door to more abstract and artistic\nneural image generation scenarios, along with simpler deployment of the model.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 06:14:07 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 07:22:31 GMT"}, {"version": "v3", "created": "Sat, 11 Apr 2020 20:05:06 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Svoboda", "Jan", ""], ["Anoosheh", "Asha", ""], ["Osendorfer", "Christian", ""], ["Masci", "Jonathan", ""]]}, {"id": "1906.02919", "submitter": "Nitin J. Sanket", "authors": "Nitin J. Sanket, Chethan M. Parameshwara, Chahat Deep Singh, Ashwin V.\n  Kuruttukulam, Cornelia Ferm\\\"uller, Davide Scaramuzza, Yiannis Aloimonos", "title": "EVDodgeNet: Deep Dynamic Obstacle Dodging with Event Cameras", "comments": "15 pages, 16 figures, Code and Video can be found at:\n  https://prg.cs.umd.edu/EVDodgeNet", "journal-ref": "IEEE International Conference on Robotics and Automation (ICRA),\n  2020", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic obstacle avoidance on quadrotors requires low latency. A class of\nsensors that are particularly suitable for such scenarios are event cameras. In\nthis paper, we present a deep learning -- based solution for dodging multiple\ndynamic obstacles on a quadrotor with a single event camera and on-board\ncomputation. Our approach uses a series of shallow neural networks for\nestimating both the ego-motion and the motion of independently moving objects.\nThe networks are trained in simulation and directly transfer to the real world\nwithout any fine-tuning or retraining. We successfully evaluate and demonstrate\nthe proposed approach in many real-world experiments with obstacles of\ndifferent shapes and sizes, achieving an overall success rate of 70% including\nobjects of unknown shape and a low light testing scenario. To our knowledge,\nthis is the first deep learning -- based solution to the problem of dynamic\nobstacle avoidance using event cameras on a quadrotor. Finally, we also extend\nour work to the pursuit task by merely reversing the control policy, proving\nthat our navigation stack can cater to different scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 06:27:46 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 00:06:37 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2020 00:33:09 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Sanket", "Nitin J.", ""], ["Parameshwara", "Chethan M.", ""], ["Singh", "Chahat Deep", ""], ["Kuruttukulam", "Ashwin V.", ""], ["Ferm\u00fcller", "Cornelia", ""], ["Scaramuzza", "Davide", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "1906.02924", "submitter": "Inwan Yoo", "authors": "Inwan Yoo, Donggeun Yoo and Kyunghyun Paeng", "title": "PseudoEdgeNet: Nuclei Segmentation only with Point Annotations", "comments": "MICCAI 2019 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nuclei segmentation is one of the important tasks for whole slide image\nanalysis in digital pathology. With the drastic advance of deep learning,\nrecent deep networks have demonstrated successful performance of the nuclei\nsegmentation task. However, a major bottleneck to achieving good performance is\nthe cost for annotation. A large network requires a large number of\nsegmentation masks, and this annotation task is given to pathologists, not the\npublic. In this paper, we propose a weakly supervised nuclei segmentation\nmethod, which requires only point annotations for training. This method can\nscale to large training set as marking a point of a nucleus is much cheaper\nthan the fine segmentation mask. To this end, we introduce a novel auxiliary\nnetwork, called PseudoEdgeNet, which guides the segmentation network to\nrecognize nuclei edges even without edge annotations. We evaluate our method\nwith two public datasets, and the results demonstrate that the method\nconsistently outperforms other weakly supervised methods.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 06:55:45 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 10:08:04 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Yoo", "Inwan", ""], ["Yoo", "Donggeun", ""], ["Paeng", "Kyunghyun", ""]]}, {"id": "1906.02939", "submitter": "Rui Fan", "authors": "Rui Fan, Jianhao Jiao, Haoyang Ye, Yang Yu, Ioannis Pitas, Ming Liu", "title": "Key Ingredients of Self-Driving Cars", "comments": "5 pages, 2 figures, EUSIPCO 2019 Satellite Workshop: Signal\n  Processing, Computer Vision and Deep Learning for Autonomous Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG cs.SY eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, many research articles have been published in the area\nof autonomous driving. However, most of them focus only on a specific\ntechnological area, such as visual environment perception, vehicle control,\netc. Furthermore, due to fast advances in the self-driving car technology, such\narticles become obsolete very fast. In this paper, we give a brief but\ncomprehensive overview on key ingredients of autonomous cars (ACs), including\ndriving automation levels, AC sensors, AC software, open source datasets,\nindustry leaders, AC applications and existing challenges.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 07:41:27 GMT"}, {"version": "v2", "created": "Sat, 10 Aug 2019 04:23:01 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Fan", "Rui", ""], ["Jiao", "Jianhao", ""], ["Ye", "Haoyang", ""], ["Yu", "Yang", ""], ["Pitas", "Ioannis", ""], ["Liu", "Ming", ""]]}, {"id": "1906.02940", "submitter": "Trieu Trinh", "authors": "Trieu H. Trinh, Minh-Thang Luong, Quoc V. Le", "title": "Selfie: Self-supervised Pretraining for Image Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a pretraining technique called Selfie, which stands for SELFie\nsupervised Image Embedding. Selfie generalizes the concept of masked language\nmodeling of BERT (Devlin et al., 2019) to continuous data, such as images, by\nmaking use of the Contrastive Predictive Coding loss (Oord et al., 2018). Given\nmasked-out patches in an input image, our method learns to select the correct\npatch, among other \"distractor\" patches sampled from the same image, to fill in\nthe masked location. This classification objective sidesteps the need for\npredicting exact pixel values of the target patches. The pretraining\narchitecture of Selfie includes a network of convolutional blocks to process\npatches followed by an attention pooling network to summarize the content of\nunmasked patches before predicting masked ones. During finetuning, we reuse the\nconvolutional weights found by pretraining. We evaluate Selfie on three\nbenchmarks (CIFAR-10, ImageNet 32 x 32, and ImageNet 224 x 224) with varying\namounts of labeled data, from 5% to 100% of the training sets. Our pretraining\nmethod provides consistent improvements to ResNet-50 across all settings\ncompared to the standard supervised training of the same network. Notably, on\nImageNet 224 x 224 with 60 examples per class (5%), our method improves the\nmean accuracy of ResNet-50 from 35.6% to 46.7%, an improvement of 11.1 points\nin absolute accuracy. Our pretraining method also improves ResNet-50 training\nstability, especially on low data regime, by significantly lowering the\nstandard deviation of test accuracies across different runs.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 07:47:24 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 01:30:56 GMT"}, {"version": "v3", "created": "Sat, 27 Jul 2019 08:03:46 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Trinh", "Trieu H.", ""], ["Luong", "Minh-Thang", ""], ["Le", "Quoc V.", ""]]}, {"id": "1906.02944", "submitter": "Han-Jia Ye", "authors": "Han-Jia Ye, Hexiang Hu, De-Chuan Zhan", "title": "Learning Adaptive Classifiers Synthesis for Generalized Few-Shot\n  Learning", "comments": "Accepted by IJCV; The code is available at\n  https://github.com/Sha-Lab/aCASTLE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object recognition in the real-world requires handling long-tailed or even\nopen-ended data. An ideal visual system needs to recognize the populated head\nvisual concepts reliably and meanwhile efficiently learn about emerging new\ntail categories with a few training instances. Class-balanced many-shot\nlearning and few-shot learning tackle one side of this problem, by either\nlearning strong classifiers for head or learning to learn few-shot classifiers\nfor the tail. In this paper, we investigate the problem of generalized few-shot\nlearning (GFSL) -- a model during the deployment is required to learn about\ntail categories with few shots and simultaneously classify the head classes. We\npropose the ClAssifier SynThesis LEarning (CASTLE), a learning framework that\nlearns how to synthesize calibrated few-shot classifiers in addition to the\nmulti-class classifiers of head classes with a shared neural dictionary,\nshedding light upon the inductive GFSL. Furthermore, we propose an adaptive\nversion of CASTLE (ACASTLE) that adapts the head classifiers conditioned on the\nincoming tail training examples, yielding a framework that allows effective\nbackward knowledge transfer. As a consequence, ACASTLE can handle GFSL with\nclasses from heterogeneous domains effectively. CASTLE and ACASTLE demonstrate\nsuperior performances than existing GFSL algorithms and strong baselines on\nMiniImageNet as well as TieredImageNet datasets. More interestingly, they\noutperform previous state-of-the-art methods when evaluated with standard\nfew-shot learning criteria.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 08:07:05 GMT"}, {"version": "v2", "created": "Sat, 28 Sep 2019 06:27:48 GMT"}, {"version": "v3", "created": "Sun, 29 Dec 2019 02:39:02 GMT"}, {"version": "v4", "created": "Wed, 28 Oct 2020 04:34:30 GMT"}, {"version": "v5", "created": "Sun, 27 Jun 2021 02:17:15 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ye", "Han-Jia", ""], ["Hu", "Hexiang", ""], ["Zhan", "De-Chuan", ""]]}, {"id": "1906.02961", "submitter": "Chonho Lee", "authors": "Chonho Lee, Chihiro Tanikawa, Jae-Yeon Lim and Takashi Yamashiro", "title": "Deep Learning based Cephalometric Landmark Identification using\n  Landmark-dependent Multi-scale Patches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A deep neural network based cephalometric landmark identification model is\nproposed. Two neural networks, named patch classification and point estimation,\nare trained by multi-scale image patches cropped from 935 Cephalograms (of\nJapanese young patients), whose size and orientation vary based on\nlandmark-dependent criteria examined by orthodontists. The proposed model\nidentifies both 22 hard and 11 soft tissue landmarks. In order to evaluate the\nproposed model, (i) landmark estimation accuracy by Euclidean distance error\nbetween true and estimated values, and (ii) success rate that the estimated\nlandmark was located within the corresponding norm using confidence ellipse,\nare computed. The proposed model successfully identified hard tissue landmarks\nwithin the error range of 1.32 - 3.5 mm and with a mean success rate of 96.4%,\nand soft tissue landmarks with the error range of 1.16 - 4.37 mm and with a\nmean success rate of 75.2%. We verify that considering the landmark-dependent\nsize and orientation of patches helps improve the estimation accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 08:44:00 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Lee", "Chonho", ""], ["Tanikawa", "Chihiro", ""], ["Lim", "Jae-Yeon", ""], ["Yamashiro", "Takashi", ""]]}, {"id": "1906.02990", "submitter": "Ya Lu", "authors": "Ya Lu, Thomai Stathopoulou, Maria F. Vasiloglou, Stergios\n  Christodoulidis, Beat Blum, Thomas Walser, Vinzenz Meier, Zeno Stanga,\n  Stavroula G. Mougiakakou", "title": "An Artificial Intelligence-Based System for Nutrient Intake Assessment\n  of Hospitalised Patients", "comments": "EMBC2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regular nutrient intake monitoring in hospitalised patients plays a critical\nrole in reducing the risk of disease-related malnutrition (DRM). Although\nseveral methods to estimate nutrient intake have been developed, there is still\na clear demand for a more reliable and fully automated technique, as this could\nimprove the data accuracy and reduce both the participant burden and the health\ncosts. In this paper, we propose a novel system based on artificial\nintelligence to accurately estimate nutrient intake, by simply processing RGB\ndepth image pairs captured before and after a meal consumption. For the\ndevelopment and evaluation of the system, a dedicated and new database of\nimages and recipes of 322 meals was assembled, coupled to data annotation using\ninnovative strategies. With this database, a system was developed that employed\na novel multi-task neural network and an algorithm for 3D surface construction.\nThis allowed sequential semantic food segmentation and estimation of the volume\nof the consumed food, and permitted fully automatic estimation of nutrient\nintake for each food type with a 15% estimation error.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 09:54:14 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 09:11:47 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Lu", "Ya", ""], ["Stathopoulou", "Thomai", ""], ["Vasiloglou", "Maria F.", ""], ["Christodoulidis", "Stergios", ""], ["Blum", "Beat", ""], ["Walser", "Thomas", ""], ["Meier", "Vinzenz", ""], ["Stanga", "Zeno", ""], ["Mougiakakou", "Stavroula G.", ""]]}, {"id": "1906.02999", "submitter": "Luyang Luo", "authors": "Luyang Luo, Hao Chen, Xi Wang, Qi Dou, Huangjin Lin, Juan Zhou,\n  Gongjie Li, Pheng-Ann Heng", "title": "Deep Angular Embedding and Feature Correlation Attention for Breast MRI\n  Cancer Analysis", "comments": "Accepted by MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and automatic analysis of breast MRI plays an important role in\nearly diagnosis and successful treatment planning for breast cancer. Due to the\nheterogeneity nature, accurate diagnosis of tumors remains a challenging task.\nIn this paper, we propose to identify breast tumor in MRI by Cosine Margin\nSigmoid Loss (CMSL) with deep learning (DL) and localize possible cancer lesion\nby COrrelation Attention Map (COAM) based on the learned features. The CMSL\nembeds tumor features onto a hypersphere and imposes a decision margin through\ncosine constraints. In this way, the DL model could learn more separable\ninter-class features and more compact intra-class features in the angular\nspace. Furthermore, we utilize the correlations among feature vectors to\ngenerate attention maps that could accurately localize cancer candidates with\nonly image-level label. We build the largest breast cancer dataset involving\n10,290 DCE-MRI scan volumes for developing and evaluating the proposed methods.\nThe model driven by CMSL achieved classification accuracy of 0.855 and AUC of\n0.902 on the testing set, with sensitivity and specificity of 0.857 and 0.852,\nrespectively, outperforming other competitive methods overall. In addition, the\nproposed COAM accomplished more accurate localization of the cancer center\ncompared with other state-of-the-art weakly supervised localization method.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 10:23:06 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Luo", "Luyang", ""], ["Chen", "Hao", ""], ["Wang", "Xi", ""], ["Dou", "Qi", ""], ["Lin", "Huangjin", ""], ["Zhou", "Juan", ""], ["Li", "Gongjie", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1906.03019", "submitter": "Alexander Hermans", "authors": "Kilian Pfeiffer, Alexander Hermans, Istv\\'an S\\'ar\\'andi, Mark Weber,\n  Bastian Leibe", "title": "Visual Person Understanding through Multi-Task and Multi-Dataset\n  Learning", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-33676-9_39", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of learning a single model for person\nre-identification, attribute classification, body part segmentation, and pose\nestimation. With predictions for these tasks we gain a more holistic\nunderstanding of persons, which is valuable for many applications. This is a\nclassical multi-task learning problem. However, no dataset exists that these\ntasks could be jointly learned from. Hence several datasets need to be combined\nduring training, which in other contexts has often led to reduced performance\nin the past. We extensively evaluate how the different task and datasets\ninfluence each other and how different degrees of parameter sharing between the\ntasks affect performance. Our final model matches or outperforms its\nsingle-task counterparts without creating significant computational overhead,\nrendering it highly interesting for resource-constrained scenarios such as\nmobile robotics.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 11:24:41 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Pfeiffer", "Kilian", ""], ["Hermans", "Alexander", ""], ["S\u00e1r\u00e1ndi", "Istv\u00e1n", ""], ["Weber", "Mark", ""], ["Leibe", "Bastian", ""]]}, {"id": "1906.03033", "submitter": "Gabriele Civitarese Dr.", "authors": "Gabriele Civitarese, Riccardo Presotto and Claudio Bettini", "title": "Context-driven Active and Incremental Activity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activity recognition based on mobile device sensor data has been an\nactive research area in mobile and pervasive computing for several years. While\nthe majority of the proposed techniques are based on supervised learning,\nsemi-supervised approaches are being considered to significantly reduce the\nsize of the training set required to initialize the recognition model. These\napproaches usually apply self-training or active learning to incrementally\nrefine the model, but their effectiveness seems to be limited to a restricted\nset of physical activities. We claim that the context which surrounds the user\n(e.g., semantic location, proximity to transportation routes, time of the day)\ncombined with common knowledge about the relationship between this context and\nhuman activities could be effective in significantly increasing the set of\nrecognized activities including those that are difficult to discriminate only\nconsidering inertial sensors, and the ones that are highly context-dependent.\nIn this paper, we propose CAVIAR, a novel hybrid semi-supervised and\nknowledge-based system for real-time activity recognition. Our method applies\nsemantic reasoning to context data to refine the prediction of a\nsemi-supervised classifier. The context-refined predictions are used as new\nlabeled samples to update the classifier combining self-training and active\nlearning techniques. Results on a real dataset obtained from 26 subjects show\nthe effectiveness of the context-aware approach both on the recognition rates\nand on the number of queries to the subjects generated by the active learning\nmodule. In order to evaluate the impact of context reasoning, we also compare\nCAVIAR with a purely statistical version, considering features computed on\ncontext data as part of the machine learning process.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 12:06:34 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Civitarese", "Gabriele", ""], ["Presotto", "Riccardo", ""], ["Bettini", "Claudio", ""]]}, {"id": "1906.03038", "submitter": "Divyat Mahajan", "authors": "Varun Khare, Divyat Mahajan, Homanga Bharadhwaj, Vinay Verma, Piyush\n  Rai", "title": "A Generative Framework for Zero-Shot Learning with Adversarial Domain\n  Adaptation", "comments": "Proceedings of Winter Conference on Applications of Computer Vision\n  (WACV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a domain adaptation based generative framework for zero-shot\nlearning. Our framework addresses the problem of domain shift between the seen\nand unseen class distributions in zero-shot learning and minimizes the shift by\ndeveloping a generative model trained via adversarial domain adaptation. Our\napproach is based on end-to-end learning of the class distributions of seen\nclasses and unseen classes. To enable the model to learn the class\ndistributions of unseen classes, we parameterize these class distributions in\nterms of the class attribute information (which is available for both seen and\nunseen classes). This provides a very simple way to learn the class\ndistribution of any unseen class, given only its class attribute information,\nand no labeled training data. Training this model with adversarial domain\nadaptation further provides robustness against the distribution mismatch\nbetween the data from seen and unseen classes. Our approach also provides a\nnovel way for training neural net based classifiers to overcome the hubness\nproblem in zero-shot learning. Through a comprehensive set of experiments, we\nshow that our model yields superior accuracies as compared to various\nstate-of-the-art zero shot learning models, on a variety of benchmark datasets.\nCode for the experiments is available at github.com/vkkhare/ZSL-ADA\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 12:11:22 GMT"}, {"version": "v2", "created": "Sun, 11 Aug 2019 20:38:42 GMT"}, {"version": "v3", "created": "Sat, 22 Feb 2020 18:49:21 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Khare", "Varun", ""], ["Mahajan", "Divyat", ""], ["Bharadhwaj", "Homanga", ""], ["Verma", "Vinay", ""], ["Rai", "Piyush", ""]]}, {"id": "1906.03039", "submitter": "Lingjing Wang", "authors": "Lingjing Wang, Xiang Li, Jianchun Chen and Yi Fang", "title": "Coherent Point Drift Networks: Unsupervised Learning of Non-Rigid Point\n  Set Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given new pairs of source and target point sets, standard point set\nregistration methods often repeatedly conduct the independent iterative search\nof desired geometric transformation to align the source point set with the\ntarget one. This limits their use in applications to handle the real-time point\nset registration with large volume dataset. This paper presents a novel method,\nnamed coherent point drift networks (CPD-Net), for the unsupervised learning of\ngeometric transformation towards real-time non-rigid point set registration. In\ncontrast to previous efforts (e.g. coherent point drift), CPD-Net can learn\ndisplacement field function to estimate geometric transformation from a\ntraining dataset, consequently, to predict the desired geometric transformation\nfor the alignment of previously unseen pairs without any additional iterative\noptimization process. Furthermore, CPD-Net leverages the power of deep neural\nnetworks to fit an arbitrary function, that adaptively accommodates different\nlevels of complexity of the desired geometric transformation. Particularly,\nCPD-Net is proved with a theoretical guarantee to learn a continuous\ndisplacement vector function that could further avoid imposing additional\nparametric smoothness constraint as in previous works. Our experiments verify\nthe impressive performance of CPD-Net for non-rigid point set registration on\nvarious 2D/3D datasets, even in the presence of significant displacement noise,\noutliers, and missing points. Our code will be available at\nhttps://github.com/nyummvc/CPD-Net.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 12:12:22 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 11:06:57 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2019 12:29:46 GMT"}, {"version": "v4", "created": "Sun, 14 Jul 2019 04:33:23 GMT"}, {"version": "v5", "created": "Sun, 28 Jul 2019 17:43:16 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Wang", "Lingjing", ""], ["Li", "Xiang", ""], ["Chen", "Jianchun", ""], ["Fang", "Yi", ""]]}, {"id": "1906.03142", "submitter": "Jianwu Lin", "authors": "Jian-Wu Lin and Hao Li", "title": "HPILN: A feature learning framework for cross-modality person\n  re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most video surveillance systems use both RGB and infrared cameras, making it\na vital technique to re-identify a person cross the RGB and infrared\nmodalities. This task can be challenging due to both the cross-modality\nvariations caused by heterogeneous images in RGB and infrared, and the\nintra-modality variations caused by the heterogeneous human poses, camera\nviews, light brightness, etc. To meet these challenges a novel feature learning\nframework, HPILN, is proposed. In the framework existing single-modality\nre-identification models are modified to fit for the cross-modality scenario,\nfollowing which specifically designed hard pentaplet loss and identity loss are\nused to improve the performance of the modified cross-modality\nre-identification models. Based on the benchmark of the SYSU-MM01 dataset,\nextensive experiments have been conducted, which show that the proposed method\noutperforms all existing methods in terms of Cumulative Match Characteristic\ncurve (CMC) and Mean Average Precision (MAP).\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 14:55:47 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 08:06:40 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Lin", "Jian-Wu", ""], ["Li", "Hao", ""]]}, {"id": "1906.03153", "submitter": "Yifan Peng", "authors": "Tiarnan D. Keenan, Shazia Dharssi, Yifan Peng, Qingyu Chen, Elvira\n  Agr\\'on, Wai T. Wong, Zhiyong Lu, Emily Y. Chew", "title": "A deep learning approach for automated detection of geographic atrophy\n  from color fundus photographs", "comments": "Accepted for publication in Ophthalmology", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To assess the utility of deep learning in the detection of\ngeographic atrophy (GA) from color fundus photographs; secondary aim to explore\npotential utility in detecting central GA (CGA). Design: A deep learning model\nwas developed to detect the presence of GA in color fundus photographs, and two\nadditional models to detect CGA in different scenarios. Participants: 59,812\ncolor fundus photographs from longitudinal follow up of 4,582 participants in\nthe AREDS dataset. Gold standard labels were from human expert reading center\ngraders using a standardized protocol. Methods: A deep learning model was\ntrained to use color fundus photographs to predict GA presence from a\npopulation of eyes with no AMD to advanced AMD. A second model was trained to\npredict CGA presence from the same population. A third model was trained to\npredict CGA presence from the subset of eyes with GA. For training and testing,\n5-fold cross-validation was employed. For comparison with human clinician\nperformance, model performance was compared with that of 88 retinal\nspecialists. Results: The deep learning models (GA detection, CGA detection\nfrom all eyes, and centrality detection from GA eyes) had AUC of 0.933-0.976,\n0.939-0.976, and 0.827-0.888, respectively. The GA detection model had\naccuracy, sensitivity, specificity, and precision of 0.965, 0.692, 0.978, and\n0.584, respectively. The CGA detection model had equivalent values of 0.966,\n0.763, 0.971, and 0.394. The centrality detection model had equivalent values\nof 0.762, 0.782, 0.729, and 0.799. Conclusions: A deep learning model\ndemonstrated high accuracy for the automated detection of GA. The AUC was\nnon-inferior to that of human retinal specialists. Deep learning approaches may\nalso be applied to the identification of CGA. The code and pretrained models\nare publicly available at https://github.com/ncbi-nlp/DeepSeeNet.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 15:12:38 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Keenan", "Tiarnan D.", ""], ["Dharssi", "Shazia", ""], ["Peng", "Yifan", ""], ["Chen", "Qingyu", ""], ["Agr\u00f3n", "Elvira", ""], ["Wong", "Wai T.", ""], ["Lu", "Zhiyong", ""], ["Chew", "Emily Y.", ""]]}, {"id": "1906.03173", "submitter": "Ye Yuan", "authors": "Ye Yuan, Kris Kitani", "title": "Ego-Pose Estimation and Forecasting as Real-Time PD Control", "comments": "ICCV 2019; Webpage: https://www.ye-yuan.com/ego-pose; Video:\n  https://youtu.be/968IIDZeWE0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the use of a proportional-derivative (PD) control based policy\nlearned via reinforcement learning (RL) to estimate and forecast 3D human pose\nfrom egocentric videos. The method learns directly from unsegmented egocentric\nvideos and motion capture data consisting of various complex human motions\n(e.g., crouching, hopping, bending, and motion transitions). We propose a\nvideo-conditioned recurrent control technique to forecast physically-valid and\nstable future motions of arbitrary length. We also introduce a value function\nbased fail-safe mechanism which enables our method to run as a single pass\nalgorithm over the video data. Experiments with both controlled and in-the-wild\ndata show that our approach outperforms previous art in both quantitative\nmetrics and visual quality of the motions, and is also robust enough to\ntransfer directly to real-world scenarios. Additionally, our time analysis\nshows that the combined use of our pose estimation and forecasting can run at\n30 FPS, making it suitable for real-time applications.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 15:39:21 GMT"}, {"version": "v2", "created": "Sun, 4 Aug 2019 21:21:33 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Yuan", "Ye", ""], ["Kitani", "Kris", ""]]}, {"id": "1906.03199", "submitter": "Yi Xiao", "authors": "Yi Xiao, Felipe Codevilla, Akhil Gurram, Onay Urfalioglu, Antonio M.\n  L\\'opez", "title": "Multimodal End-to-End Autonomous Driving", "comments": "The paper has been accepted by IEEE Transactions on Intelligent\n  Transportation Systems 2020", "journal-ref": null, "doi": "10.1109/TITS.2020.3013234", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A crucial component of an autonomous vehicle (AV) is the artificial\nintelligence (AI) is able to drive towards a desired destination. Today, there\nare different paradigms addressing the development of AI drivers. On the one\nhand, we find modular pipelines, which divide the driving task into sub-tasks\nsuch as perception and maneuver planning and control. On the other hand, we\nfind end-to-end driving approaches that try to learn a direct mapping from\ninput raw sensor data to vehicle control signals. The later are relatively less\nstudied, but are gaining popularity since they are less demanding in terms of\nsensor data annotation. This paper focuses on end-to-end autonomous driving. So\nfar, most proposals relying on this paradigm assume RGB images as input sensor\ndata. However, AVs will not be equipped only with cameras, but also with active\nsensors providing accurate depth information (e.g., LiDARs). Accordingly, this\npaper analyses whether combining RGB and depth modalities, i.e. using RGBD\ndata, produces better end-to-end AI drivers than relying on a single modality.\nWe consider multimodality based on early, mid and late fusion schemes, both in\nmultisensory and single-sensor (monocular depth estimation) settings. Using the\nCARLA simulator and conditional imitation learning (CIL), we show how, indeed,\nearly fusion multimodality outperforms single-modality.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 16:08:19 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 10:42:37 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Xiao", "Yi", ""], ["Codevilla", "Felipe", ""], ["Gurram", "Akhil", ""], ["Urfalioglu", "Onay", ""], ["L\u00f3pez", "Antonio M.", ""]]}, {"id": "1906.03219", "submitter": "Yazhou Yao", "authors": "Yazhou Yao, Jian Zhang, Xiansheng Hua, Fumin Shen, Zhenmin Tang", "title": "Extracting Visual Knowledge from the Internet: Making Sense of Image\n  Data", "comments": "Accepted by International Conference on MultiMedia Modeling, 2016\n  (MMM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent successes in visual recognition can be primarily attributed to feature\nrepresentation, learning algorithms, and the ever-increasing size of labeled\ntraining data. Extensive research has been devoted to the first two, but much\nless attention has been paid to the third. Due to the high cost of manual\nlabeling, the size of recent efforts such as ImageNet is still relatively small\nin respect to daily applications. In this work, we mainly focus on how to\nautomatically generate identifying image data for a given visual concept on a\nvast scale. With the generated image data, we can train a robust recognition\nmodel for the given concept. We evaluate the proposed webly supervised approach\non the benchmark Pascal VOC 2007 dataset and the results demonstrates the\nsuperiority of our proposed approach in image data collection.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 16:35:33 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Yao", "Yazhou", ""], ["Zhang", "Jian", ""], ["Hua", "Xiansheng", ""], ["Shen", "Fumin", ""], ["Tang", "Zhenmin", ""]]}, {"id": "1906.03248", "submitter": "Aj Piergiovanni", "authors": "AJ Piergiovanni, Anelia Angelova, and Michael S. Ryoo", "title": "Evolving Losses for Unlabeled Video Representation Learning", "comments": "Non-archival abstract for CVPR Workshop on Learning from Unlabeled\n  Videos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method to learn video representations from unlabeled data.\nGiven large-scale unlabeled video data, the objective is to benefit from such\ndata by learning a generic and transferable representation space that can be\ndirectly used for a new task such as zero/few-shot learning. We formulate our\nunsupervised representation learning as a multi-modal, multi-task learning\nproblem, where the representations are also shared across different modalities\nvia distillation. Further, we also introduce the concept of finding a better\nloss function to train such multi-task multi-modal representation space using\nan evolutionary algorithm; our method automatically searches over different\ncombinations of loss functions capturing multiple (self-supervised) tasks and\nmodalities. Our formulation allows for the distillation of audio, optical flow\nand temporal information into a single, RGB-based convolutional neural network.\nWe also compare the effects of using additional unlabeled video data and\nevaluate our representation learning on standard public video datasets.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 17:22:54 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Piergiovanni", "AJ", ""], ["Angelova", "Anelia", ""], ["Ryoo", "Michael S.", ""]]}, {"id": "1906.03279", "submitter": "Haoyu Ren", "authors": "Haoyu Ren, Mostafa El-khamy, Jungwon Lee", "title": "Deep Robust Single Image Depth Estimation Neural Network Using Scene\n  Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image depth estimation (SIDE) plays a crucial role in 3D computer\nvision. In this paper, we propose a two-stage robust SIDE framework that can\nperform blind SIDE for both indoor and outdoor scenes. At the first stage, the\nscene understanding module will categorize the RGB image into different\ndepth-ranges. We introduce two different scene understanding modules based on\nscene classification and coarse depth estimation respectively. At the second\nstage, SIDE networks trained by the images of specific depth-range are applied\nto obtain an accurate depth map. In order to improve the accuracy, we further\ndesign a multi-task encoding-decoding SIDE network DS-SIDENet based on\ndepthwise separable convolutions. DS-SIDENet is optimized to minimize both\ndepth classification and depth regression losses. This improves the accuracy\ncompared to a single-task SIDE network. Experimental results demonstrate that\ntraining DS-SIDENet on an individual dataset such as NYU achieves competitive\nperformance to the state-of-art methods with much better efficiency. Ours\nproposed robust SIDE framework also shows good performance for the ScanNet\nindoor images and KITTI outdoor images simultaneously. It achieves the top\nperformance compared to the Robust Vision Challenge (ROB) 2018 submissions.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 18:08:34 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Ren", "Haoyu", ""], ["El-khamy", "Mostafa", ""], ["Lee", "Jungwon", ""]]}, {"id": "1906.03299", "submitter": "Zhiheng Kang", "authors": "Kang Zhiheng and Li Ning", "title": "PyramNet: Point Cloud Pyramid Attention Network and Graph Embedding\n  Module for Classification and Segmentation", "comments": "Accepted for presentation at ICONIP2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the tide of artificial intelligence, we try to apply deep learning to\nunderstand 3D data. Point cloud is an important 3D data structure, which can\naccurately and directly reflect the real world. In this paper, we propose a\nsimple and effective network, which is named PyramNet, suites for point cloud\nobject classification and semantic segmentation in 3D scene. We design two new\noperators: Graph Embedding Module(GEM) and Pyramid Attention Network(PAN).\nSpecifically, GEM projects point cloud onto the graph and practices the\ncovariance matrix to explore the relationship between points, so as to improve\nthe local feature expression ability of the model. PAN assigns some strong\nsemantic features to each point to retain fine geometric features as much as\npossible. Furthermore, we provide extensive evaluation and analysis for the\neffectiveness of PyramNet. Empirically, we evaluate our model on ModelNet40,\nShapeNet and S3DIS.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 19:06:24 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 04:04:47 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Zhiheng", "Kang", ""], ["Ning", "Li", ""]]}, {"id": "1906.03327", "submitter": "Antoine Miech", "authors": "Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand\n  Tapaswi, Ivan Laptev, Josef Sivic", "title": "HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million\n  Narrated Video Clips", "comments": "Accepted at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning text-video embeddings usually requires a dataset of video clips with\nmanually provided captions. However, such datasets are expensive and time\nconsuming to create and therefore difficult to obtain on a large scale. In this\nwork, we propose instead to learn such embeddings from video data with readily\navailable natural language annotations in the form of automatically transcribed\nnarrations. The contributions of this work are three-fold. First, we introduce\nHowTo100M: a large-scale dataset of 136 million video clips sourced from 1.22M\nnarrated instructional web videos depicting humans performing and describing\nover 23k different visual tasks. Our data collection procedure is fast,\nscalable and does not require any additional manual annotation. Second, we\ndemonstrate that a text-video embedding trained on this data leads to\nstate-of-the-art results for text-to-video retrieval and action localization on\ninstructional video datasets such as YouCook2 or CrossTask. Finally, we show\nthat this embedding transfers well to other domains: fine-tuning on generic\nYoutube videos (MSR-VTT dataset) and movies (LSMDC dataset) outperforms models\ntrained on these datasets alone. Our dataset, code and models will be publicly\navailable at: www.di.ens.fr/willow/research/howto100m/.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 20:48:19 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 13:47:49 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Miech", "Antoine", ""], ["Zhukov", "Dimitri", ""], ["Alayrac", "Jean-Baptiste", ""], ["Tapaswi", "Makarand", ""], ["Laptev", "Ivan", ""], ["Sivic", "Josef", ""]]}, {"id": "1906.03340", "submitter": "Iljung Kwak", "authors": "Iljung S. Kwak, Jian-Zhong Guo, Adam Hantman, David Kriegman, Kristin\n  Branson", "title": "Detecting the Starting Frame of Actions in Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem of precisely localizing key frames of an\naction, for example, the precise time that a pitcher releases a baseball, or\nthe precise time that a crowd begins to applaud. Key frame localization is a\nlargely overlooked and important action-recognition problem, for example in the\nfield of neuroscience, in which we would like to understand the neural activity\nthat produces the start of a bout of an action. To address this problem, we\nintroduce a novel structured loss function that properly weights the types of\nerrors that matter in such applications: it more heavily penalizes extra and\nmissed action start detections over small misalignments. Our structured loss is\nbased on the best matching between predicted and labeled action starts. We\ntrain recurrent neural networks (RNNs) to minimize differentiable\napproximations of this loss. To evaluate these methods, we introduce the Mouse\nReach Dataset, a large, annotated video dataset of mice performing a sequence\nof actions. The dataset was collected and labeled by experts for the purpose of\nneuroscience research. On this dataset, we demonstrate that our method\noutperforms related approaches and baseline methods using an unstructured loss.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 21:48:09 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 20:11:54 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Kwak", "Iljung S.", ""], ["Guo", "Jian-Zhong", ""], ["Hantman", "Adam", ""], ["Kriegman", "David", ""], ["Branson", "Kristin", ""]]}, {"id": "1906.03347", "submitter": "Ling Zhang", "authors": "Ling Zhang, Xiaosong Wang, Dong Yang, Thomas Sanford, Stephanie\n  Harmon, Baris Turkbey, Holger Roth, Andriy Myronenko, Daguang Xu, Ziyue Xu", "title": "When Unseen Domain Generalization is Unnecessary? Rethinking Data\n  Augmentation", "comments": "9 pages, 3 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning for medical image segmentation demonstrate\nexpert-level accuracy. However, in clinically realistic environments, such\nmethods have marginal performance due to differences in image domains,\nincluding different imaging protocols, device vendors and patient populations.\nHere we consider the problem of domain generalization, when a model is trained\nonce, and its performance generalizes to unseen domains. Intuitively, within a\nspecific medical imaging modality the domain differences are smaller relative\nto natural images domain variability. We rethink data augmentation for medical\n3D images and propose a deep stacked transformations (DST) approach for domain\ngeneralization. Specifically, a series of n stacked transformations are applied\nto each image in each mini-batch during network training to account for the\ncontribution of domain-specific shifts in medical images. We comprehensively\nevaluate our method on three tasks: segmentation of whole prostate from 3D MRI,\nleft atrial from 3D MRI, and left ventricle from 3D ultrasound. We demonstrate\nthat when trained on a small source dataset, (i) on average, DST models on\nunseen datasets degrade only by 11% (Dice score change), compared to the\nconventional augmentation (degrading 39%) and CycleGAN-based domain adaptation\nmethod (degrading 25%); (ii) when evaluation on the same domain, DST is also\nbetter albeit only marginally. (iii) When training on large-sized data, DST on\nunseen domains reaches performance of state-of-the-art fully supervised models.\nThese findings establish a strong benchmark for the study of domain\ngeneralization in medical imaging, and can be generalized to the design of\nrobust deep segmentation models for clinical deployment.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 22:25:06 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 14:18:26 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Zhang", "Ling", ""], ["Wang", "Xiaosong", ""], ["Yang", "Dong", ""], ["Sanford", "Thomas", ""], ["Harmon", "Stephanie", ""], ["Turkbey", "Baris", ""], ["Roth", "Holger", ""], ["Myronenko", "Andriy", ""], ["Xu", "Daguang", ""], ["Xu", "Ziyue", ""]]}, {"id": "1906.03349", "submitter": "Heng Wang", "authors": "Heng Wang, Du Tran, Lorenzo Torresani, Matt Feiszli", "title": "Video Modeling with Correlation Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion is a salient cue to recognize actions in video. Modern action\nrecognition models leverage motion information either explicitly by using\noptical flow as input or implicitly by means of 3D convolutional filters that\nsimultaneously capture appearance and motion information. This paper proposes\nan alternative approach based on a learnable correlation operator that can be\nused to establish frame-toframe matches over convolutional feature maps in the\ndifferent layers of the network. The proposed architecture enables the fusion\nof this explicit temporal matching information with traditional appearance cues\ncaptured by 2D convolution. Our correlation network compares favorably with\nwidely-used 3D CNNs for video modeling, and achieves competitive results over\nthe prominent two-stream network while being much faster to train. We\nempirically demonstrate that correlation networks produce strong results on a\nvariety of video datasets, and outperform the state of the art on four popular\nbenchmarks for action recognition: Kinetics, Something-Something, Diving48 and\nSports1M.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 22:36:00 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 00:13:12 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Wang", "Heng", ""], ["Tran", "Du", ""], ["Torresani", "Lorenzo", ""], ["Feiszli", "Matt", ""]]}, {"id": "1906.03355", "submitter": "Thomas Nestmeyer", "authors": "Thomas Nestmeyer, Jean-Fran\\c{c}ois Lalonde, Iain Matthews, Andreas M.\n  Lehrmann", "title": "Learning Physics-guided Face Relighting under Directional Light", "comments": "CVPR 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relighting is an essential step in realistically transferring objects from a\ncaptured image into another environment. For example, authentic telepresence in\nAugmented Reality requires faces to be displayed and relit consistent with the\nobserver's scene lighting. We investigate end-to-end deep learning\narchitectures that both de-light and relight an image of a human face. Our\nmodel decomposes the input image into intrinsic components according to a\ndiffuse physics-based image formation model. We enable non-diffuse effects\nincluding cast shadows and specular highlights by predicting a residual\ncorrection to the diffuse render. To train and evaluate our model, we collected\na portrait database of 21 subjects with various expressions and poses. Each\nsample is captured in a controlled light stage setup with 32 individual light\nsources. Our method creates precise and believable relighting results and\ngeneralizes to complex illumination conditions and challenging poses, including\nwhen the subject is not looking straight at the camera.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 23:16:34 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2020 12:33:08 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Nestmeyer", "Thomas", ""], ["Lalonde", "Jean-Fran\u00e7ois", ""], ["Matthews", "Iain", ""], ["Lehrmann", "Andreas M.", ""]]}, {"id": "1906.03359", "submitter": "Euijoon Ahn", "authors": "Euijoon Ahn, Ashnil Kumar, Dagan Feng, Michael Fulham, Jinman Kim", "title": "Unsupervised Feature Learning with K-means and An Ensemble of Deep\n  Convolutional Neural Networks for Medical Image Classification", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image analysis using supervised deep learning methods remains\nproblematic because of the reliance of deep learning methods on large amounts\nof labelled training data. Although medical imaging data repositories continue\nto expand there has not been a commensurate increase in the amount of annotated\ndata. Hence, we propose a new unsupervised feature learning method that learns\nfeature representations to then differentiate dissimilar medical images using\nan ensemble of different convolutional neural networks (CNNs) and K-means\nclustering. It jointly learns feature representations and clustering\nassignments in an end-to-end fashion. We tested our approach on a public\nmedical dataset and show its accuracy was better than state-of-the-art\nunsupervised feature learning methods and comparable to state-of-the-art\nsupervised CNNs. Our findings suggest that our method could be used to tackle\nthe issue of the large volume of unlabelled data in medical imaging\nrepositories.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 23:52:26 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Ahn", "Euijoon", ""], ["Kumar", "Ashnil", ""], ["Feng", "Dagan", ""], ["Fulham", "Michael", ""], ["Kim", "Jinman", ""]]}, {"id": "1906.03363", "submitter": "Tom\\'a\\v{s} Sou\\v{c}ek", "authors": "Tom\\'a\\v{s} Sou\\v{c}ek, Jaroslav Moravec, Jakub Loko\\v{c}", "title": "TransNet: A deep network for fast detection of common shot transitions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shot boundary detection (SBD) is an important first step in many video\nprocessing applications. This paper presents a simple modular convolutional\nneural network architecture that achieves state-of-the-art results on the RAI\ndataset with well above real-time inference speed even on a single mediocre\nGPU. The network employs dilated convolutions and operates just on small\nresized frames. The training process employed randomly generated transitions\nusing selected shots from the TRECVID IACC.3 dataset. The code and a selected\ntrained network will be available at https://github.com/soCzech/TransNet.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 00:45:18 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Sou\u010dek", "Tom\u00e1\u0161", ""], ["Moravec", "Jaroslav", ""], ["Loko\u010d", "Jakub", ""]]}, {"id": "1906.03365", "submitter": "Omar Vidal Pino", "authors": "Omar Vidal Pino, Erickson Rangel Nascimento and Mario Fernando\n  Montenegro Campos", "title": "Global Semantic Description of Objects based on Prototype Theory", "comments": "Content: 24 pages (22 + 2 reference) with 15 Figures and 3 Tables. In\n  the future, a new version will be updated with other experiments and results\n  (and a journal reference if applicable)", "journal-ref": null, "doi": "10.1016/j.imavis.2021.104249", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel semantic description approach inspired on\nPrototype Theory foundations. We propose a Computational Prototype Model (CPM)\nthat encodes and stores the central semantic meaning of objects category: the\nsemantic prototype. Also, we introduce a Prototype-based Description Model that\nencodes the semantic meaning of an object while describing its features using\nour CPM model. Our description method uses semantic prototypes computed by\nCNN-classifications models to create discriminative signatures that describe an\nobject highlighting its most distinctive features within the category. Our\nexperiments show that: i) our CPM model (semantic prototype + distance metric)\nis able to describe the internal semantic structure of objects categories; ii)\nour semantic distance metric can be understood as the object visual typicality\nscore within a category; iii) our descriptor encoding is semantically\ninterpretable and significantly outperforms other image global encodings in\nclustering and classification tasks.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 01:02:02 GMT"}, {"version": "v2", "created": "Sat, 29 Feb 2020 03:23:20 GMT"}, {"version": "v3", "created": "Sun, 2 Aug 2020 20:32:38 GMT"}, {"version": "v4", "created": "Sun, 20 Jun 2021 01:22:02 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Pino", "Omar Vidal", ""], ["Nascimento", "Erickson Rangel", ""], ["Campos", "Mario Fernando Montenegro", ""]]}, {"id": "1906.03404", "submitter": "Chawowei Shan", "authors": "Chaowei Shan, Zhizheng Zhang, Zhibo Chen", "title": "A Coarse-to-Fine Framework for Learned Color Enhancement with Non-Local\n  Attention", "comments": "To appear in ICIP19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic color enhancement is aimed to adaptively adjust photos to expected\nstyles and tones. For current learned methods in this field, global harmonious\nperception and local details are hard to be well-considered in a single model\nsimultaneously. To address this problem, we propose a coarse-to-fine framework\nwith non-local attention for color enhancement in this paper. Within our\nframework, we propose to divide enhancement process into channel-wise\nenhancement and pixel-wise refinement performed by two cascaded Convolutional\nNeural Networks (CNNs). In channel-wise enhancement, our model predicts a\nglobal linear mapping for RGB channels of input images to perform global style\nadjustment. In pixel-wise refinement, we learn a refining mapping using\nresidual learning for local adjustment. Further, we adopt a non-local attention\nblock to capture the long-range dependencies from global information for\nsubsequent fine-grained local refinement. We evaluate our proposed framework on\nthe commonly using benchmark and conduct sufficient experiments to demonstrate\neach technical component within it.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 07:16:01 GMT"}, {"version": "v2", "created": "Sat, 20 Jul 2019 08:15:10 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Shan", "Chaowei", ""], ["Zhang", "Zhizheng", ""], ["Chen", "Zhibo", ""]]}, {"id": "1906.03444", "submitter": "Tejas Borkar", "authors": "Tejas Borkar, Felix Heide and Lina Karam", "title": "Defending Against Universal Attacks Through Selective Feature\n  Regeneration", "comments": "CVPR 2020. Code:\n  https://github.com/tsborkar/Selective-feature-regeneration Webpage:\n  https://www.cs.princeton.edu/~fheide/SelectiveFeatureRegeneration/", "journal-ref": "CVPR 2020", "doi": "10.1109/CVPR42600.2020.00079", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network (DNN) predictions have been shown to be vulnerable to\ncarefully crafted adversarial perturbations. Specifically, image-agnostic\n(universal adversarial) perturbations added to any image can fool a target\nnetwork into making erroneous predictions. Departing from existing defense\nstrategies that work mostly in the image domain, we present a novel defense\nwhich operates in the DNN feature domain and effectively defends against such\nuniversal perturbations. Our approach identifies pre-trained convolutional\nfeatures that are most vulnerable to adversarial noise and deploys trainable\nfeature regeneration units which transform these DNN filter activations into\nresilient features that are robust to universal perturbations. Regenerating\nonly the top 50% adversarially susceptible activations in at most 6 DNN layers\nand leaving all remaining DNN activations unchanged, we outperform existing\ndefense strategies across different network architectures by more than 10% in\nrestored accuracy. We show that without any additional modification, our\ndefense trained on ImageNet with one type of universal attack examples\neffectively defends against other types of unseen universal attacks.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 12:18:13 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 09:53:52 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2020 06:41:51 GMT"}, {"version": "v4", "created": "Thu, 11 Jun 2020 02:40:33 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Borkar", "Tejas", ""], ["Heide", "Felix", ""], ["Karam", "Lina", ""]]}, {"id": "1906.03466", "submitter": "Rajagopal A", "authors": "Rajagopal. A, Nirmala. V", "title": "Strategies to architect AI Safety: Defense to guard AI from Adversaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impact of designing for security of AI is critical for humanity in the AI\nera. With humans increasingly becoming dependent upon AI, there is a need for\nneural networks that work reliably, inspite of Adversarial attacks. The vision\nfor Safe and secure AI for popular use is achievable. To achieve safety of AI,\nthis paper explores strategies and a novel deep learning architecture. To guard\nAI from adversaries, paper explores combination of 3 strategies:\n  1. Introduce randomness at inference time to hide the representation learning\nfrom adversaries.\n  2. Detect presence of adversaries by analyzing the sequence of inferences.\n  3. Exploit visual similarity.\n  To realize these strategies, this paper designs a novel architecture, Dynamic\nNeural Defense, DND. This defense has 3 deep learning architectural features:\n  1. By hiding the way a neural network learns from exploratory attacks using a\nrandom computation graph, DND evades attack.\n  2. By analyzing input sequence to cloud AI inference engine with LSTM, DND\ndetects attack sequence.\n  3. By inferring with visual similar inputs generated by VAE, any AI defended\nby DND approach does not succumb to hackers.\n  Thus, a roadmap to develop reliable, safe and secure AI is presented.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 14:34:47 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["A", "Rajagopal.", ""], ["V", "Nirmala.", ""]]}, {"id": "1906.03467", "submitter": "Jingya Liu", "authors": "Jingya Liu, Liangliang Cao, Oguz Akin and Yingli Tian", "title": "3DFPN-HS$^2$: 3D Feature Pyramid Network Based High Sensitivity and\n  Specificity Pulmonary Nodule Detection", "comments": "8 pages, 3 figures. Accepted to MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate detection of pulmonary nodules with high sensitivity and specificity\nis essential for automatic lung cancer diagnosis from CT scans. Although many\ndeep learning-based algorithms make great progress for improving the accuracy\nof nodule detection, the high false positive rate is still a challenging\nproblem which limited the automatic diagnosis in routine clinical practice. In\nthis paper, we propose a novel pulmonary nodule detection framework based on a\n3D Feature Pyramid Network (3DFPN) to improve the sensitivity of nodule\ndetection by employing multi-scale features to increase the resolution of\nnodules, as well as a parallel top-down path to transit the high-level semantic\nfeatures to complement low-level general features. Furthermore, a High\nSensitivity and Specificity (HS$^2$) network is introduced to eliminate the\nfalsely detected nodule candidates by tracking the appearance changes in\ncontinuous CT slices of each nodule candidate. The proposed framework is\nevaluated on the public Lung Nodule Analysis (LUNA16) challenge dataset. Our\nmethod is able to accurately detect lung nodules at high sensitivity and\nspecificity and achieves $90.4\\%$ sensitivity with 1/8 false positive per scan\nwhich outperforms the state-of-the-art results $15.6\\%$.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 14:35:33 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 04:05:12 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Liu", "Jingya", ""], ["Cao", "Liangliang", ""], ["Akin", "Oguz", ""], ["Tian", "Yingli", ""]]}, {"id": "1906.03495", "submitter": "Giovanna Citti", "authors": "Giovanna Citti, Alessandro Sarti", "title": "Neurogeometry of perception: isotropic and anisotropic aspects", "comments": null, "journal-ref": null, "doi": "10.1007/s10516-019-09426-1", "report-no": null, "categories": "cs.CV math.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we first recall the definition of geometical model of the\nvisual cortex, focusing in particular on the geometrical properties of\nhorizontal cortical connectivity. Then we recognize that histograms of edges -\nco-occurrences are not isotropic distributed, and are strongly biased in\nhorizontal and vertical directions of the stimulus. Finally we introduce a new\nmodel of non isotropic cortical connectivity modeled on the histogram of edges\n- co-occurrences. Using this kernel we are able to justify oblique phenomena\ncomparable with experimental findings.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 18:19:16 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Citti", "Giovanna", ""], ["Sarti", "Alessandro", ""]]}, {"id": "1906.03502", "submitter": "Vinod Kumar Kurmi", "authors": "Vinod Kumar Kurmi, Shanu Kumar and Vinay P Namboodiri", "title": "Attending to Discriminative Certainty for Domain Adaptation", "comments": "CVPR 2019 Accepted, Project: https://delta-lab-iitk.github.io/CADA/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we aim to solve for unsupervised domain adaptation of\nclassifiers where we have access to label information for the source domain\nwhile these are not available for a target domain. While various methods have\nbeen proposed for solving these including adversarial discriminator based\nmethods, most approaches have focused on the entire image based domain\nadaptation. In an image, there would be regions that can be adapted better, for\ninstance, the foreground object may be similar in nature. To obtain such\nregions, we propose methods that consider the probabilistic certainty estimate\nof various regions and specify focus on these during classification for\nadaptation. We observe that just by incorporating the probabilistic certainty\nof the discriminator while training the classifier, we are able to obtain state\nof the art results on various datasets as compared against all the recent\nmethods. We provide a thorough empirical analysis of the method by providing\nablation analysis, statistical significance test, and visualization of the\nattention maps and t-SNE embeddings. These evaluations convincingly demonstrate\nthe effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 19:04:38 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 14:51:38 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Kurmi", "Vinod Kumar", ""], ["Kumar", "Shanu", ""], ["Namboodiri", "Vinay P", ""]]}, {"id": "1906.03509", "submitter": "Aristotelis Papadopoulos", "authors": "Aristotelis-Angelos Papadopoulos, Mohammad Reza Rajati, Nazim Shaikh,\n  Jiamian Wang", "title": "Outlier Exposure with Confidence Control for Out-of-Distribution\n  Detection", "comments": "Accepted as a Journal paper at Neurocomputing. PyTorch code available\n  at https://github.com/nazim1021/OOD-detection-using-OECC", "journal-ref": "Neurocomputing 441 (2021) 138-150", "doi": "10.1016/j.neucom.2021.02.007", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved great success in classification tasks\nduring the last years. However, one major problem to the path towards\nartificial intelligence is the inability of neural networks to accurately\ndetect samples from novel class distributions and therefore, most of the\nexistent classification algorithms assume that all classes are known prior to\nthe training stage. In this work, we propose a methodology for training a\nneural network that allows it to efficiently detect out-of-distribution (OOD)\nexamples without compromising much of its classification accuracy on the test\nexamples from known classes. We propose a novel loss function that gives rise\nto a novel method, Outlier Exposure with Confidence Control (OECC), which\nachieves superior results in OOD detection with OE both on image and text\nclassification tasks without requiring access to OOD samples. Additionally, we\nexperimentally show that the combination of OECC with state-of-the-art\npost-training OOD detection methods, like the Mahalanobis Detector (MD) and the\nGramian Matrices (GM) methods, further improves their performance in the OOD\ndetection task, demonstrating the potential of combining training and\npost-training methods for OOD detection.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 19:30:24 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 19:55:12 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2020 01:52:26 GMT"}, {"version": "v4", "created": "Tue, 2 Feb 2021 22:31:14 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Papadopoulos", "Aristotelis-Angelos", ""], ["Rajati", "Mohammad Reza", ""], ["Shaikh", "Nazim", ""], ["Wang", "Jiamian", ""]]}, {"id": "1906.03516", "submitter": "Sachin Mehta", "authors": "Sachin Mehta and Hannaneh Hajishirzi and Mohammad Rastegari", "title": "DiCENet: Dimension-wise Convolutions for Efficient Networks", "comments": "Accepted at IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel and generic convolutional unit, DiCE unit, that is built\nusing dimension-wise convolutions and dimension-wise fusion. The dimension-wise\nconvolutions apply light-weight convolutional filtering across each dimension\nof the input tensor while dimension-wise fusion efficiently combines these\ndimension-wise representations; allowing the DiCE unit to efficiently encode\nspatial and channel-wise information contained in the input tensor. The DiCE\nunit is simple and can be seamlessly integrated with any architecture to\nimprove its efficiency and performance. Compared to depth-wise separable\nconvolutions, the DiCE unit shows significant improvements across different\narchitectures. When DiCE units are stacked to build the DiCENet model, we\nobserve significant improvements over state-of-the-art models across various\ncomputer vision tasks including image classification, object detection, and\nsemantic segmentation. On the ImageNet dataset, the DiCENet delivers 2-4%\nhigher accuracy than state-of-the-art manually designed models (e.g.,\nMobileNetv2 and ShuffleNetv2). Also, DiCENet generalizes better to tasks (e.g.,\nobject detection) that are often used in resource-constrained devices in\ncomparison to state-of-the-art separable convolution-based efficient networks,\nincluding neural search-based methods (e.g., MobileNetv3 and MixNet. Our source\ncode in PyTorch is open-source and is available at\nhttps://github.com/sacmehta/EdgeNets/\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 20:17:06 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 20:16:21 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2020 06:27:08 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Mehta", "Sachin", ""], ["Hajishirzi", "Hannaneh", ""], ["Rastegari", "Mohammad", ""]]}, {"id": "1906.03525", "submitter": "Zhenyu Zhang Dr.", "authors": "Zhenyu Zhang, Zhen Cui, Chunyan Xu, Yan Yan, Nicu Sebe and Jian Yang", "title": "Pattern-Affinitive Propagation across Depth, Surface Normal and Semantic\n  Segmentation", "comments": "10 pages, 9 figures, CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel Pattern-Affinitive Propagation (PAP)\nframework to jointly predict depth, surface normal and semantic segmentation.\nThe motivation behind it comes from the statistic observation that\npattern-affinitive pairs recur much frequently across different tasks as well\nas within a task. Thus, we can conduct two types of propagations, cross-task\npropagation and task-specific propagation, to adaptively diffuse those similar\npatterns. The former integrates cross-task affinity patterns to adapt to each\ntask therein through the calculation on non-local relationships. Next the\nlatter performs an iterative diffusion in the feature space so that the\ncross-task affinity patterns can be widely-spread within the task. Accordingly,\nthe learning of each task can be regularized and boosted by the complementary\ntask-level affinities. Extensive experiments demonstrate the effectiveness and\nthe superiority of our method on the joint three tasks. Meanwhile, we achieve\nthe state-of-the-art or competitive results on the three related datasets,\nNYUD-v2, SUN-RGBD and KITTI.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 21:30:02 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Zhang", "Zhenyu", ""], ["Cui", "Zhen", ""], ["Xu", "Chunyan", ""], ["Yan", "Yan", ""], ["Sebe", "Nicu", ""], ["Yang", "Jian", ""]]}, {"id": "1906.03539", "submitter": "Chris Sweeney", "authors": "Chris Sweeney, Aleksander Holynski, Brian Curless, Steve M Seitz", "title": "Structure from Motion for Panorama-Style Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel Structure from Motion pipeline that is capable of\nreconstructing accurate camera poses for panorama-style video capture without\nprior camera intrinsic calibration. While panorama-style capture is common and\nconvenient, previous reconstruction methods fail to obtain accurate\nreconstructions due to the rotation-dominant motion and small baseline between\nviews. Our method is built on the assumption that the camera motion\napproximately corresponds to motion on a sphere, and we introduce three novel\nrelative pose methods to estimate the fundamental matrix and camera distortion\nfor spherical motion. These solvers are efficient and robust, and provide an\nexcellent initialization for bundle adjustment. A soft prior on the camera\nposes is used to discourage large deviations from the spherical motion\nassumption when performing bundle adjustment, which allows cameras to remain\nproperly constrained for optimization in the absence of well-triangulated 3D\npoints. To validate the effectiveness of the proposed method we evaluate our\napproach on both synthetic and real-world data, and demonstrate that camera\nposes are accurate enough for multiview stereo.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 23:29:45 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Sweeney", "Chris", ""], ["Holynski", "Aleksander", ""], ["Curless", "Brian", ""], ["Seitz", "Steve M", ""]]}, {"id": "1906.03547", "submitter": "Dmitry Konovalov", "authors": "Dmitry A. Konovalov, Simindokht Jahangard, Lin Schwarzkopf", "title": "In Situ Cane Toad Recognition", "comments": "Accepted for DICTA2018 https://doi.org/10.1109/DICTA.2018.8615780", "journal-ref": "2018 Digital Image Computing: Techniques and Applications (DICTA),\n  Canberra, Australia, 2018, pp. 1-7", "doi": "10.1109/DICTA.2018.8615780", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cane toads are invasive, toxic to native predators, compete with native\ninsectivores, and have a devastating impact on Australian ecosystems, prompting\nthe Australian government to list toads as a key threatening process under the\nEnvironment Protection and Biodiversity Conservation Act 1999. Mechanical cane\ntoad traps could be made more native-fauna friendly if they could distinguish\ninvasive cane toads from native species. Here we designed and trained a\nConvolution Neural Network (CNN) starting from the Xception CNN. The XToadGmp\ntoad-recognition CNN we developed was trained end-to-end using heat-map\nGaussian targets. After training, XToadGmp required minimum image\npre/post-processing and when tested on 720x1280 shaped images, it achieved\n97.1% classification accuracy on 1863 toad and 2892 not-toad test images, which\nwere not used in training.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 01:10:19 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 05:11:15 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Konovalov", "Dmitry A.", ""], ["Jahangard", "Simindokht", ""], ["Schwarzkopf", "Lin", ""]]}, {"id": "1906.03548", "submitter": "Cecilia Summers", "authors": "Cecilia Summers, Michael J. Dinneen", "title": "Four Things Everyone Should Know to Improve Batch Normalization", "comments": "ICLR 2020, 18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key component of most neural network architectures is the use of\nnormalization layers, such as Batch Normalization. Despite its common use and\nlarge utility in optimizing deep architectures, it has been challenging both to\ngenerically improve upon Batch Normalization and to understand the\ncircumstances that lend themselves to other enhancements. In this paper, we\nidentify four improvements to the generic form of Batch Normalization and the\ncircumstances under which they work, yielding performance gains across all\nbatch sizes while requiring no additional computation during training. These\ncontributions include proposing a method for reasoning about the current\nexample in inference normalization statistics, fixing a training vs. inference\ndiscrepancy; recognizing and validating the powerful regularization effect of\nGhost Batch Normalization for small and medium batch sizes; examining the\neffect of weight decay regularization on the scaling and shifting parameters\ngamma and beta; and identifying a new normalization algorithm for very small\nbatch sizes by combining the strengths of Batch and Group Normalization. We\nvalidate our results empirically on six datasets: CIFAR-100, SVHN, Caltech-256,\nOxford Flowers-102, CUB-2011, and ImageNet.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 01:14:48 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 05:20:53 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Summers", "Cecilia", ""], ["Dinneen", "Michael J.", ""]]}, {"id": "1906.03560", "submitter": "Jiankai Sun", "authors": "Bowen Pan, Jiankai Sun, Ho Yin Tiga Leung, Alex Andonian, Bolei Zhou", "title": "Cross-view Semantic Segmentation for Sensing Surroundings", "comments": null, "journal-ref": "IEEE Robotics and Automation Letters ( Volume: 5 , Issue: 3 , July\n  2020 )", "doi": "10.1109/LRA.2020.3004325", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensing surroundings plays a crucial role in human spatial perception, as it\nextracts the spatial configuration of objects as well as the free space from\nthe observations. To facilitate the robot perception with such a surrounding\nsensing capability, we introduce a novel visual task called Cross-view Semantic\nSegmentation as well as a framework named View Parsing Network (VPN) to address\nit. In the cross-view semantic segmentation task, the agent is trained to parse\nthe first-view observations into a top-down-view semantic map indicating the\nspatial location of all the objects at pixel-level. The main issue of this task\nis that we lack the real-world annotations of top-down-view data. To mitigate\nthis, we train the VPN in 3D graphics environment and utilize the domain\nadaptation technique to transfer it to handle real-world data. We evaluate our\nVPN on both synthetic and real-world agents. The experimental results show that\nour model can effectively make use of the information from different views and\nmulti-modalities to understanding spatial information. Our further experiment\non a LoCoBot robot shows that our model enables the surrounding sensing\ncapability from 2D image input. Code and demo videos can be found at\n\\url{https://view-parsing-network.github.io}.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 04:18:03 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 09:27:07 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 06:56:18 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Pan", "Bowen", ""], ["Sun", "Jiankai", ""], ["Leung", "Ho Yin Tiga", ""], ["Andonian", "Alex", ""], ["Zhou", "Bolei", ""]]}, {"id": "1906.03561", "submitter": "Daqing Liu", "authors": "Daqing Liu, Hanwang Zhang, Zheng-Jun Zha, Meng Wang, Qianru Sun", "title": "Joint Visual Grounding with Language Scene Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual grounding is a task to ground referring expressions in images, e.g.,\nlocalize \"the white truck in front of the yellow one\". To resolve this task\nfundamentally, the model should first find out the contextual objects (e.g.,\nthe \"yellow\" truck) and then exploit them to disambiguate the referent from\nother similar objects by using the attributes and relationships (e.g., \"white\",\n\"yellow\", \"in front of\"). However, due to the lack of annotations on contextual\nobjects and their relationships, existing methods degenerate the above joint\ngrounding process into a holistic association between the expression and\nregions, thus suffering from unsatisfactory performance and limited\ninterpretability. In this paper, we alleviate the missing-annotation problem\nand enable the joint reasoning by leveraging the language scene graph which\ncovers both labeled referent and unlabeled contexts (other objects, attributes,\nand relationships). Specifically, the language scene graph is a graphical\nrepresentation where the nodes are objects with attributes and the edges are\nrelationships. We construct a factor graph based on it and then perform\nmarginalization over the graph, such that we can ground both referent and\ncontexts on corresponding image regions to achieve the joint visual grounding\n(JVG). Experimental results demonstrate that the proposed approach is effective\nand interpretable, e.g., on three benchmarks, it outperforms the\nstate-of-the-art methods while offers a complete grounding of all the objects\nmentioned in the referring expression.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 04:29:06 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 16:05:29 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Liu", "Daqing", ""], ["Zhang", "Hanwang", ""], ["Zha", "Zheng-Jun", ""], ["Wang", "Meng", ""], ["Sun", "Qianru", ""]]}, {"id": "1906.03563", "submitter": "Jingkang Wang", "authors": "Jingkang Wang, Tianyun Zhang, Sijia Liu, Pin-Yu Chen, Jiacen Xu, Makan\n  Fardad, Bo Li", "title": "Towards A Unified Min-Max Framework for Adversarial Exploration and\n  Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The worst-case training principle that minimizes the maximal adversarial\nloss, also known as adversarial training (AT), has shown to be a\nstate-of-the-art approach for enhancing adversarial robustness against\nnorm-ball bounded input perturbations. Nonetheless, min-max optimization beyond\nthe purpose of AT has not been rigorously explored in the research of\nadversarial attack and defense. In particular, given a set of risk sources\n(domains), minimizing the maximal loss induced from the domain set can be\nreformulated as a general min-max problem that is different from AT. Examples\nof this general formulation include attacking model ensembles, devising\nuniversal perturbation under multiple inputs or data transformations, and\ngeneralized AT over different types of attack models. We show that these\nproblems can be solved under a unified and theoretically principled min-max\noptimization framework. We also show that the self-adjusted domain weights\nlearned from our method provides a means to explain the difficulty level of\nattack and defense over multiple domains. Extensive experiments show that our\napproach leads to substantial performance improvement over the conventional\naveraging strategy.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 04:32:13 GMT"}, {"version": "v2", "created": "Sun, 29 Sep 2019 15:49:55 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Wang", "Jingkang", ""], ["Zhang", "Tianyun", ""], ["Liu", "Sijia", ""], ["Chen", "Pin-Yu", ""], ["Xu", "Jiacen", ""], ["Fardad", "Makan", ""], ["Li", "Bo", ""]]}, {"id": "1906.03568", "submitter": "Zhenyu He", "authors": "Qiao Liu, Xin Li, Zhenyu He, Nana Fan, Di Yuan, Hongpeng Wang", "title": "Learning Deep Multi-Level Similarity for Thermal Infrared Object\n  Tracking", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep Thermal InfraRed (TIR) trackers only use semantic features to\ndescribe the TIR object, which lack the sufficient discriminative capacity for\nhandling distractors. This becomes worse when the feature extraction network is\nonly trained on RGB images.To address this issue, we propose a multi-level\nsimilarity model under a Siamese framework for robust TIR object tracking.\nSpecifically, we compute different pattern similarities on two convolutional\nlayers using the proposed multi-level similarity network. One of them focuses\non the global semantic similarity and the other computes the local structural\nsimilarity of the TIR object. These two similarities complement each other and\nhence enhance the discriminative capacity of the network for handling\ndistractors. In addition, we design a simple while effective relative entropy\nbased ensemble subnetwork to integrate the semantic and structural\nsimilarities. This subnetwork can adaptive learn the weights of the semantic\nand structural similarities at the training stage. To further enhance the\ndiscriminative capacity of the tracker, we construct the first large scale TIR\nvideo sequence dataset for training the proposed model. The proposed TIR\ndataset not only benefits the training for TIR tracking but also can be applied\nto numerous TIR vision tasks. Extensive experimental results on the VOT-TIR2015\nand VOT-TIR2017 benchmarks demonstrate that the proposed algorithm performs\nfavorably against the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 05:09:05 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Liu", "Qiao", ""], ["Li", "Xin", ""], ["He", "Zhenyu", ""], ["Fan", "Nana", ""], ["Yuan", "Di", ""], ["Wang", "Hongpeng", ""]]}, {"id": "1906.03584", "submitter": "Brojeshwar Bhowmick", "authors": "Sriram N. N., Gourav Kumar, Abhay Singh, M. Siva Karthik, Saket Saurav\n  Brojeshwar Bhowmick and K. Madhava Krishna", "title": "A Hierarchical Network for Diverse Trajectory Proposals", "comments": "Accepted in IV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous explorative robots frequently encounter scenarios where multiple\nfuture trajectories can be pursued. Often these are cases with multiple paths\naround an obstacle or trajectory options towards various frontiers. Humans in\nsuch situations can inherently perceive and reason about the surrounding\nenvironment to identify several possibilities of either manoeuvring around the\nobstacles or moving towards various frontiers. In this work, we propose a 2\nstage Convolutional Neural Network architecture which mimics such an ability to\nmap the perceived surroundings to multiple trajectories that a robot can choose\nto traverse. The first stage is a Trajectory Proposal Network which suggests\ndiverse regions in the environment which can be occupied in the future. The\nsecond stage is a Trajectory Sampling network which provides a finegrained\ntrajectory over the regions proposed by Trajectory Proposal Network. We\nevaluate our framework in diverse and complicated real life settings. For the\noutdoor case, we use the KITTI dataset and our own outdoor driving dataset. In\nthe indoor setting, we use an autonomous drone to navigate various scenarios\nand also a ground robot which can explore the environment using the\ntrajectories proposed by our framework. Our experiments suggest that the\nframework is able to develop a semantic understanding of the obstacles, open\nregions and identify diverse trajectories that a robot can traverse. Our\ncomparisons portray the performance gain of the proposed architecture over a\ndiverse set of methods against which it is compared.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 07:26:50 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["N.", "Sriram N.", ""], ["Kumar", "Gourav", ""], ["Singh", "Abhay", ""], ["Karthik", "M. Siva", ""], ["Bhowmick", "Saket Saurav Brojeshwar", ""], ["Krishna", "K. Madhava", ""]]}, {"id": "1906.03598", "submitter": "Wonwoong Cho", "authors": "Wonwoong Cho, Seunghwan Choi, Junwoo Park, David Keetae Park, Tao Qin,\n  Jaegul Choo", "title": "What and Where to Translate: Local Mask-based Image-to-Image Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, image-to-image translation has obtained significant attention.\nAmong many, those approaches based on an exemplar image that contains the\ntarget style information has been actively studied, due to its capability to\nhandle multimodality as well as its applicability in practical use. However,\ntwo intrinsic problems exist in the existing methods: what and where to\ntransfer. First, those methods extract style from an entire exemplar which\nincludes noisy information, which impedes a translation model from properly\nextracting the intended style of the exemplar. That is, we need to carefully\ndetermine what to transfer from the exemplar. Second, the extracted style is\napplied to the entire input image, which causes unnecessary distortion in\nirrelevant image regions. In response, we need to decide where to transfer the\nextracted style. In this paper, we propose a novel approach that extracts out a\nlocal mask from the exemplar that determines what style to transfer, and\nanother local mask from the input image that determines where to transfer the\nextracted style. The main novelty of this paper lies in (1) the highway\nadaptive instance normalization technique and (2) an end-to-end translation\nframework which achieves an outstanding performance in reflecting a style of an\nexemplar. We demonstrate the quantitative and qualitative evaluation results to\nconfirm the advantages of our proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 09:07:01 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 14:04:11 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Cho", "Wonwoong", ""], ["Choi", "Seunghwan", ""], ["Park", "Junwoo", ""], ["Park", "David Keetae", ""], ["Qin", "Tao", ""], ["Choo", "Jaegul", ""]]}, {"id": "1906.03605", "submitter": "Qigong Sun", "authors": "Qigong Sun, Xiufang Li, Lingling Li, Xu Liu, Fang Liu, Licheng Jiao", "title": "Semi-supervised Complex-valued GAN for Polarimetric SAR Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polarimetric synthetic aperture radar (PolSAR) images are widely used in\ndisaster detection and military reconnaissance and so on. However, their\ninterpretation faces some challenges, e.g., deficiency of labeled data,\ninadequate utilization of data information and so on. In this paper, a\ncomplex-valued generative adversarial network (GAN) is proposed for the first\ntime to address these issues. The complex number form of model complies with\nthe physical mechanism of PolSAR data and in favor of utilizing and retaining\namplitude and phase information of PolSAR data. GAN architecture and\nsemi-supervised learning are combined to handle deficiency of labeled data. GAN\nexpands training data and semi-supervised learning is used to train network\nwith generated, labeled and unlabeled data. Experimental results on two\nbenchmark data sets show that our model outperforms existing state-of-the-art\nmodels, especially for conditions with fewer labeled data.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 09:30:36 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Sun", "Qigong", ""], ["Li", "Xiufang", ""], ["Li", "Lingling", ""], ["Liu", "Xu", ""], ["Liu", "Fang", ""], ["Jiao", "Licheng", ""]]}, {"id": "1906.03607", "submitter": "Qigong Sun", "authors": "Xiufang Li, Qigong Sun, Lingling Li, Zhongle Ren, Fang Liu, Licheng\n  Jiao", "title": "Pixel DAG-Recurrent Neural Network for Spectral-Spatial Hyperspectral\n  Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting rich spatial and spectral features contributes to improve the\nclassification accuracy of hyperspectral images (HSIs). In this paper, based on\nthe mechanism of the population receptive field (pRF) in human visual cortex,\nwe further utilize the spatial correlation of pixels in images and propose\npixel directed acyclic graph recurrent neural network (Pixel DAG-RNN) to\nextract and apply spectral-spatial features for HSIs classification. In our\nmodel, an undirected cyclic graph (UCG) is used to represent the relevance\nconnectivity of pixels in an image patch, and four DAGs are used to approximate\nthe spatial relationship of UCGs. In order to avoid overfitting, weight sharing\nand dropout are adopted. The higher classification performance of our model on\nHSIs classification has been verified by experiments on three benchmark data\nsets.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 09:40:55 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Li", "Xiufang", ""], ["Sun", "Qigong", ""], ["Li", "Lingling", ""], ["Ren", "Zhongle", ""], ["Liu", "Fang", ""], ["Jiao", "Licheng", ""]]}, {"id": "1906.03609", "submitter": "Tao Wang", "authors": "Tao Wang and Li Yuan and Xiaopeng Zhang and Jiashi Feng", "title": "Distilling Object Detectors with Fine-grained Feature Imitation", "comments": "accepted at CVPR2019\n  code:https://github.com/twangnh/Distilling-Object-Detectors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art CNN based recognition models are often computationally\nprohibitive to deploy on low-end devices. A promising high level approach\ntackling this limitation is knowledge distillation, which let small student\nmodel mimic cumbersome teacher model's output to get improved generalization.\nHowever, related methods mainly focus on simple task of classification while do\nnot consider complex tasks like object detection. We show applying the vanilla\nknowledge distillation to detection model gets minor gain. To address the\nchallenge of distilling knowledge in detection model, we propose a fine-grained\nfeature imitation method exploiting the cross-location discrepancy of feature\nresponse. Our intuition is that detectors care more about local near object\nregions. Thus the discrepancy of feature response on the near object anchor\nlocations reveals important information of how teacher model tends to\ngeneralize. We design a novel mechanism to estimate those locations and let\nstudent model imitate the teacher on them to get enhanced performance. We first\nvalidate the idea on a developed lightweight toy detector which carries\nsimplest notion of current state-of-the-art anchor based detection models on\nchallenging KITTI dataset, our method generates up to 15% boost of mAP for the\nstudent model compared to the non-imitated counterpart. We then extensively\nevaluate the method with Faster R-CNN model under various scenarios with common\nobject detection benchmark of Pascal VOC and COCO, imitation alleviates up to\n74% performance drop of student model compared to teacher. Codes released at\nhttps://github.com/twangnh/Distilling-Object-Detectors\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 09:57:26 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Wang", "Tao", ""], ["Yuan", "Li", ""], ["Zhang", "Xiaopeng", ""], ["Feng", "Jiashi", ""]]}, {"id": "1906.03625", "submitter": "Changxing Ding", "authors": "Xusheng Zeng, Changxing Ding, Yonggang Wen, and Dacheng Tao", "title": "Soft-ranking Label Encoding for Robust Facial Age Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic facial age estimation can be used in a wide range of real-world\napplications. However, this process is challenging due to the randomness and\nslowness of the aging process. Accordingly, in this paper, we propose a\ncomprehensive framework aimed at overcoming the challenges associated with\nfacial age estimation. First, we propose a novel age encoding method, referred\nto as 'Soft-ranking', which encodes two important properties of facial age,\ni.e., the ordinal property and the correlation between adjacent ages.\nTherefore, Soft-ranking provides a richer supervision signal for training deep\nmodels. Moreover, we also carefully analyze existing evaluation protocols for\nage estimation, finding that the overlap in identity between the training and\ntesting sets affects the relative performance of different age encoding\nmethods. Finally, since existing face databases for age estimation are\ngenerally small, deep models tend to suffer from an overfitting problem. To\naddress this issue, we propose a novel regularization strategy to encourage\ndeep models to learn more robust features from facial parts for age estimation\npurposes. Extensive experiments indicate that the proposed techniques improve\nthe age estimation performance; moreover, we achieve state-of-the-art\nperformance on the three most popular age databases, $i.e.$, Morph II,\nCLAP2015, and CLAP2016.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 12:16:57 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Zeng", "Xusheng", ""], ["Ding", "Changxing", ""], ["Wen", "Yonggang", ""], ["Tao", "Dacheng", ""]]}, {"id": "1906.03629", "submitter": "Ting Sun", "authors": "Ting Sun, Yuxiang Sun, Ming Liu, Dit-Yan Yeung", "title": "Movable-Object-Aware Visual SLAM via Weakly Supervised Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moving objects can greatly jeopardize the performance of a visual\nsimultaneous localization and mapping (vSLAM) system which relies on the\nstatic-world assumption. Motion removal have seen successful on solving this\nproblem. Two main streams of solutions are based on either geometry constraints\nor deep semantic segmentation neural network. The former rely on static\nmajority assumption, and the latter require labor-intensive pixel-wise\nannotations. In this paper we propose to adopt a novel weakly-supervised\nsemantic segmentation method. The segmentation mask is obtained from a CNN\npre-trained with image-level class labels only. Thus, we leverage the power of\ndeep semantic segmentation CNNs, while avoid requiring expensive annotations\nfor training. We integrate our motion removal approach with the ORB-SLAM2\nsystem. Experimental results on the TUM RGB-D and the KITTI stereo datasets\ndemonstrate our superiority over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 12:50:10 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 09:00:52 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Sun", "Ting", ""], ["Sun", "Yuxiang", ""], ["Liu", "Ming", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1906.03631", "submitter": "Osama Makansi", "authors": "Osama Makansi, Eddy Ilg, \\\"Ozg\\\"un Cicek and Thomas Brox", "title": "Overcoming Limitations of Mixture Density Networks: A Sampling and\n  Fitting Framework for Multimodal Future Prediction", "comments": "In CVPR 2019", "journal-ref": null, "doi": "10.1109/CVPR.2019.00731", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future prediction is a fundamental principle of intelligence that helps plan\nactions and avoid possible dangers. As the future is uncertain to a large\nextent, modeling the uncertainty and multimodality of the future states is of\ngreat relevance. Existing approaches are rather limited in this regard and\nmostly yield a single hypothesis of the future or, at the best, strongly\nconstrained mixture components that suffer from instabilities in training and\nmode collapse. In this work, we present an approach that involves the\nprediction of several samples of the future with a winner-takes-all loss and\niterative grouping of samples to multiple modes. Moreover, we discuss how to\nevaluate predicted multimodal distributions, including the common real\nscenario, where only a single sample from the ground-truth distribution is\navailable for evaluation. We show on synthetic and real data that the proposed\napproach triggers good estimates of multimodal distributions and avoids mode\ncollapse. Source code is available at\n$\\href{https://github.com/lmb-freiburg/Multimodal-Future-Prediction}{\\text{this\nhttps URL.}}$\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 13:02:26 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 16:19:30 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Makansi", "Osama", ""], ["Ilg", "Eddy", ""], ["Cicek", "\u00d6zg\u00fcn", ""], ["Brox", "Thomas", ""]]}, {"id": "1906.03639", "submitter": "Dufan Wu", "authors": "Dufan Wu, Kuang Gong, Kyungsang Kim, Quanzheng Li", "title": "Consensus Neural Network for Medical Imaging Denoising with Only Noisy\n  Training Samples", "comments": "9 pages, 2 figures, accepted by MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been proved efficient for medical image denoising.\nCurrent training methods require both noisy and clean images. However, clean\nimages cannot be acquired for many practical medical applications due to\nnaturally noisy signal, such as dynamic imaging, spectral computed tomography,\narterial spin labeling magnetic resonance imaging, etc. In this paper we\nproposed a training method which learned denoising neural networks from noisy\ntraining samples only. Training data in the acquisition domain was split to two\nsubsets and the network was trained to map one noisy set to the other. A\nconsensus loss function was further proposed to efficiently combine the outputs\nfrom both subsets. A mathematical proof was provided that the proposed training\nscheme was equivalent to training with noisy and clean samples when the noise\nin the two subsets was uncorrelated and zero-mean. The method was validated on\nLow-dose CT Challenge dataset and NYU MRI dataset and achieved improved\nperformance compared to existing unsupervised methods.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 13:37:34 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Wu", "Dufan", ""], ["Gong", "Kuang", ""], ["Kim", "Kyungsang", ""], ["Li", "Quanzheng", ""]]}, {"id": "1906.03650", "submitter": "Salman Khan Dr.", "authors": "Salman H. Khan, Yulan Guo, Munawar Hayat, Nick Barnes", "title": "Unsupervised Primitive Discovery for Improved 3D Generative Modeling", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D shape generation is a challenging problem due to the high-dimensional\noutput space and complex part configurations of real-world objects. As a\nresult, existing algorithms experience difficulties in accurate generative\nmodeling of 3D shapes. Here, we propose a novel factorized generative model for\n3D shape generation that sequentially transitions from coarse to fine scale\nshape generation. To this end, we introduce an unsupervised primitive discovery\nalgorithm based on a higher-order conditional random field model. Using the\nprimitive parts for shapes as attributes, a parameterized 3D representation is\nmodeled in the first stage. This representation is further refined in the next\nstage by adding fine scale details to shape. Our results demonstrate improved\nrepresentation ability of the generative model and better quality samples of\nnewly generated 3D shapes. Further, our primitive generation approach can\naccurately parse common objects into a simplified representation.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 14:32:09 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Khan", "Salman H.", ""], ["Guo", "Yulan", ""], ["Hayat", "Munawar", ""], ["Barnes", "Nick", ""]]}, {"id": "1906.03657", "submitter": "Xukai Xie", "authors": "Xukai Xie, Yuan Zhou and Sun-Yuan Kung", "title": "HGC: Hierarchical Group Convolution for Highly Efficient Neural Network", "comments": "arXiv admin note: text overlap with arXiv:1711.09224,\n  arXiv:1904.00346, arXiv:1811.07083 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group convolution works well with many deep convolutional neural networks\n(CNNs) that can effectively compress the model by reducing the number of\nparameters and computational cost. Using this operation, feature maps of\ndifferent group cannot communicate, which restricts their representation\ncapability. To address this issue, in this work, we propose a novel operation\nnamed Hierarchical Group Convolution (HGC) for creating computationally\nefficient neural networks. Different from standard group convolution which\nblocks the inter-group information exchange and induces the severe performance\ndegradation, HGC can hierarchically fuse the feature maps from each group and\nleverage the inter-group information effectively. Taking advantage of the\nproposed method, we introduce a family of compact networks called HGCNets.\nCompared to networks using standard group convolution, HGCNets have a huge\nimprovement in accuracy at the same model size and complexity level. Extensive\nexperimental results on the CIFAR dataset demonstrate that HGCNets obtain\nsignificant reduction of parameters and computational cost to achieve\ncomparable performance over the prior CNN architectures designed for mobile\ndevices such as MobileNet and ShuffleNet.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 15:24:42 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Xie", "Xukai", ""], ["Zhou", "Yuan", ""], ["Kung", "Sun-Yuan", ""]]}, {"id": "1906.03683", "submitter": "Kuan-Hui Lee", "authors": "Kuan-Hui Lee, Takaaki Tagawa, Jia-En M. Pan, Adrien Gaidon, Bertrand\n  Douillard", "title": "An Attention-based Recurrent Convolutional Network for Vehicle Taillight\n  Recognition", "comments": "Accepted by IV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle taillight recognition is an important application for automated\ndriving, especially for intent prediction of ado vehicles and trajectory\nplanning of the ego vehicle. In this work, we propose an end-to-end deep\nlearning framework to recognize taillights, i.e. rear turn and brake signals,\nfrom a sequence of images. The proposed method starts with a Convolutional\nNeural Network (CNN) to extract spatial features, and then applies a Long\nShort-Term Memory network (LSTM) to learn temporal dependencies. Furthermore,\nwe integrate attention models in both spatial and temporal domains, where the\nattention models learn to selectively focus on both spatial and temporal\nfeatures. Our method is able to outperform the state of the art in terms of\naccuracy on the UC Merced Vehicle Rear Signal Dataset, demonstrating the\neffectiveness of attention models for vehicle taillight recognition.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 18:08:49 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Lee", "Kuan-Hui", ""], ["Tagawa", "Takaaki", ""], ["Pan", "Jia-En M.", ""], ["Gaidon", "Adrien", ""], ["Douillard", "Bertrand", ""]]}, {"id": "1906.03685", "submitter": "Valerie Chen", "authors": "Valerie Chen, Man-Ki Yoon, Zhong Shao", "title": "Novelty Detection via Network Saliency in Visual-based Deep Learning", "comments": "To be published in Dependable and Secure Machine Learning (DSML)\n  workshop co-located with the IEEE Conference on Dependable Systems and\n  Networks 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine-learning driven safety-critical autonomous systems, such as\nself-driving cars, must be able to detect situations where its trained model is\nnot able to make a trustworthy prediction. Often viewed as a black-box, it is\nnon-obvious to determine when a model will make a safe decision and when it\nwill make an erroneous, perhaps life-threatening one. Prior work on novelty\ndetection deal with highly structured data and do not translate well to\ndynamic, real-world situations. This paper proposes a multi-step framework for\nthe detection of novel scenarios in vision-based autonomous systems by\nleveraging information learned by the trained prediction model and a new image\nsimilarity metric. We demonstrate the efficacy of this method through\nexperiments on a real-world driving dataset as well as on our in-house indoor\nracing environment.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 18:23:57 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Chen", "Valerie", ""], ["Yoon", "Man-Ki", ""], ["Shao", "Zhong", ""]]}, {"id": "1906.03720", "submitter": "Mateusz Buda", "authors": "Mateusz Buda and Ashirbani Saha and Maciej A Mazurowski", "title": "Association of genomic subtypes of lower-grade gliomas with shape\n  features automatically extracted by a deep learning algorithm", "comments": null, "journal-ref": "Computers in Biology and Medicine, 109, 2019, 218-225", "doi": "10.1016/j.compbiomed.2019.05.002", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent analysis identified distinct genomic subtypes of lower-grade glioma\ntumors which are associated with shape features. In this study, we propose a\nfully automatic way to quantify tumor imaging characteristics using deep\nlearning-based segmentation and test whether these characteristics are\npredictive of tumor genomic subtypes. We used preoperative imaging and genomic\ndata of 110 patients from 5 institutions with lower-grade gliomas from The\nCancer Genome Atlas. Based on automatic deep learning segmentations, we\nextracted three features which quantify two-dimensional and three-dimensional\ncharacteristics of the tumors. Genomic data for the analyzed cohort of patients\nconsisted of previously identified genomic clusters based on IDH mutation and\n1p/19q co-deletion, DNA methylation, gene expression, DNA copy number, and\nmicroRNA expression. To analyze the relationship between the imaging features\nand genomic clusters, we conducted the Fisher exact test for 10 hypotheses for\neach pair of imaging feature and genomic subtype. To account for multiple\nhypothesis testing, we applied a Bonferroni correction. P-values lower than\n0.005 were considered statistically significant. We found the strongest\nassociation between RNASeq clusters and the bounding ellipsoid volume ratio\n($p<0.0002$) and between RNASeq clusters and margin fluctuation ($p<0.005$). In\naddition, we identified associations between bounding ellipsoid volume ratio\nand all tested molecular subtypes ($p<0.02$) as well as between angular\nstandard deviation and RNASeq cluster ($p<0.02$). In terms of automatic tumor\nsegmentation that was used to generate the quantitative image characteristics,\nour deep learning algorithm achieved a mean Dice coefficient of 82% which is\ncomparable to human performance.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 21:45:57 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Buda", "Mateusz", ""], ["Saha", "Ashirbani", ""], ["Mazurowski", "Maciej A", ""]]}, {"id": "1906.03764", "submitter": "Adam Harley", "authors": "Adam W. Harley and Shrinidhi K. Lakshmikanth and Fangyu Li and Xian\n  Zhou and Hsiao-Yu Fish Tung and Katerina Fragkiadaki", "title": "Learning from Unlabelled Videos Using Contrastive Predictive Neural 3D\n  Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive coding theories suggest that the brain learns by predicting\nobservations at various levels of abstraction. One of the most basic prediction\ntasks is view prediction: how would a given scene look from an alternative\nviewpoint? Humans excel at this task. Our ability to imagine and fill in\nmissing information is tightly coupled with perception: we feel as if we see\nthe world in 3 dimensions, while in fact, information from only the front\nsurface of the world hits our retinas. This paper explores the role of view\nprediction in the development of 3D visual recognition. We propose neural 3D\nmapping networks, which take as input 2.5D (color and depth) video streams\ncaptured by a moving camera, and lift them to stable 3D feature maps of the\nscene, by disentangling the scene content from the motion of the camera. The\nmodel also projects its 3D feature maps to novel viewpoints, to predict and\nmatch against target views. We propose contrastive prediction losses to replace\nthe standard color regression loss, and show that this leads to better\nperformance on complex photorealistic data. We show that the proposed model\nlearns visual representations useful for (1) semi-supervised learning of 3D\nobject detectors, and (2) unsupervised learning of 3D moving object detectors,\nby estimating the motion of the inferred 3D feature maps in videos of dynamic\nscenes. To the best of our knowledge, this is the first work that empirically\nshows view prediction to be a scalable self-supervised task beneficial to 3D\nobject detection.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 01:53:42 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 02:02:58 GMT"}, {"version": "v3", "created": "Wed, 10 Jul 2019 23:02:29 GMT"}, {"version": "v4", "created": "Mon, 30 Sep 2019 18:52:19 GMT"}, {"version": "v5", "created": "Mon, 17 Feb 2020 17:09:42 GMT"}, {"version": "v6", "created": "Sun, 17 May 2020 02:16:28 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Harley", "Adam W.", ""], ["Lakshmikanth", "Shrinidhi K.", ""], ["Li", "Fangyu", ""], ["Zhou", "Xian", ""], ["Tung", "Hsiao-Yu Fish", ""], ["Fragkiadaki", "Katerina", ""]]}, {"id": "1906.03767", "submitter": "Gang Xu", "authors": "Xiaoshuo Li, Tiezhu Yue, Xuanping Huang, Zhe Yang, Gang Xu", "title": "BAGS: An automatic homework grading system using the pictures taken by\n  smart phones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Homework grading is critical to evaluate teaching quality and effect.\nHowever, it is usually time-consuming to grade the homework manually. In\nautomatic homework grading scenario, many optical mark reader (OMR)-based\nsolutions which require specific equipments have been proposed. Although many\nof them can achieve relatively high accuracy, they are less convenient for\nusers. In contrast, with the popularity of smart phones, the automatic grading\nsystem which depends on the image photographed by phones becomes more\navailable. In practice, due to different photographing angles or uneven papers,\nimages may be distorted. Moreover, most of images are photographed under\ncomplex backgrounds, making answer areas detection more difficult. To solve\nthese problems, we propose BAGS, an automatic homework grading system which can\neffectively locate and recognize handwritten answers. In BAGS, all the answers\nwould be written above the answer area underlines (AAU), and we use two\nsegmentation networks based on DeepLabv3+ to locate the answer areas. Then, we\nuse the characters recognition part to recognize students' answers. Finally,\nthe grading part is designed for the comparison between the recognized answers\nand the standard ones. In our test, BAGS correctly locates and recognizes the\nhandwritten answers in 91% of total answer areas.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 02:21:28 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Li", "Xiaoshuo", ""], ["Yue", "Tiezhu", ""], ["Huang", "Xuanping", ""], ["Yang", "Zhe", ""], ["Xu", "Gang", ""]]}, {"id": "1906.03786", "submitter": "Abu Sufian", "authors": "A. Sufian (1), Anirudha Ghosh (1), Avijit Naskar (1), Farhana Sultana\n  (1), Jaya Sil (2) and M M Hafizur Rahman (3) ((1) University of Gour Banga,\n  India, (2) IIEST Shibpur, India, (3) King Faisal University, Saudi Arabia)", "title": "BDNet: Bengali Handwritten Numeral Digit Recognition based on Densely\n  connected Convolutional Neural Networks", "comments": "23 pages, 11 figures, 7 tables, Accepted Manuscript. Journal of King\n  Saud University - Computer and Information Sciences (2019)", "journal-ref": "Journal of King Saud University - Computer and Information\n  Sciences, Elsevier, Online, 2020", "doi": "10.1016/j.jksuci.2020.03.002", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images of handwritten digits are different from natural images as the\norientation of a digit, as well as similarity of features of different digits,\nmakes confusion. On the other hand, deep convolutional neural networks are\nachieving huge success in computer vision problems, especially in image\nclassification. BDNet is a densely connected deep convolutional neural network\nmodel used to classify (recognize) Bengali handwritten numeral digits. It is\nend-to-end trained using ISI Bengali handwritten numeral dataset. During\ntraining, untraditional data preprocessing and augmentation techniques are used\nso that the trained model works on a different dataset. The model has achieved\nthe test accuracy of 99.775%(baseline was 99.40%) on the test dataset of ISI\nBengali handwritten numerals. So, the BDNet model gives 62.5% error reduction\ncompared to previous state-of-the-art models. Here we have also created a\ndataset of 1000 images of Bengali handwritten numerals to test the trained\nmodel, and it giving promising results. Codes, trained model and our own\ndataset are available at: {https://github.com/Sufianlab/BDNet}.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 03:31:58 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 14:12:37 GMT"}, {"version": "v3", "created": "Fri, 25 Oct 2019 19:32:36 GMT"}, {"version": "v4", "created": "Tue, 31 Dec 2019 13:35:43 GMT"}, {"version": "v5", "created": "Thu, 12 Mar 2020 13:15:44 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Sufian", "A.", ""], ["Ghosh", "Anirudha", ""], ["Naskar", "Avijit", ""], ["Sultana", "Farhana", ""], ["Sil", "Jaya", ""], ["Rahman", "M M Hafizur", ""]]}, {"id": "1906.03787", "submitter": "Cihang Xie", "authors": "Cihang Xie, Alan Yuille", "title": "Intriguing properties of adversarial training at scale", "comments": "To appear in ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training is one of the main defenses against adversarial attacks.\nIn this paper, we provide the first rigorous study on diagnosing elements of\nadversarial training, which reveals two intriguing properties.\n  First, we study the role of normalization. Batch normalization (BN) is a\ncrucial element for achieving state-of-the-art performance on many vision\ntasks, but we show it may prevent networks from obtaining strong robustness in\nadversarial training. One unexpected observation is that, for models trained\nwith BN, simply removing clean images from training data largely boosts\nadversarial robustness, i.e., 18.3%. We relate this phenomenon to the\nhypothesis that clean images and adversarial images are drawn from two\ndifferent domains. This two-domain hypothesis may explain the issue of BN when\ntraining with a mixture of clean and adversarial images, as estimating\nnormalization statistics of this mixture distribution is challenging. Guided by\nthis two-domain hypothesis, we show disentangling the mixture distribution for\nnormalization, i.e., applying separate BNs to clean and adversarial images for\nstatistics estimation, achieves much stronger robustness. Additionally, we find\nthat enforcing BNs to behave consistently at training and testing can further\nenhance robustness.\n  Second, we study the role of network capacity. We find our so-called \"deep\"\nnetworks are still shallow for the task of adversarial learning. Unlike\ntraditional classification tasks where accuracy is only marginally improved by\nadding more layers to \"deep\" networks (e.g., ResNet-152), adversarial training\nexhibits a much stronger demand on deeper networks to achieve higher\nadversarial robustness. This robustness improvement can be observed\nsubstantially and consistently even by pushing the network capacity to an\nunprecedented scale, i.e., ResNet-638.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 03:41:52 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2019 20:48:24 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Xie", "Cihang", ""], ["Yuille", "Alan", ""]]}, {"id": "1906.03799", "submitter": "Mathieu Garon", "authors": "Mathieu Garon, Kalyan Sunkavalli, Sunil Hadap, Nathan Carr,\n  Jean-Fran\\c{c}ois Lalonde", "title": "Fast Spatially-Varying Indoor Lighting Estimation", "comments": "CVPR19", "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2019, pp. 6908-6917", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a real-time method to estimate spatiallyvarying indoor lighting\nfrom a single RGB image. Given an image and a 2D location in that image, our\nCNN estimates a 5th order spherical harmonic representation of the lighting at\nthe given location in less than 20ms on a laptop mobile graphics card. While\nexisting approaches estimate a single, global lighting representation or\nrequire depth as input, our method reasons about local lighting without\nrequiring any geometry information. We demonstrate, through quantitative\nexperiments including a user study, that our results achieve lower lighting\nestimation errors and are preferred by users over the state-of-the-art. Our\napproach can be used directly for augmented reality applications, where a\nvirtual object is relit realistically at any position in the scene in\nreal-time.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 05:25:58 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Garon", "Mathieu", ""], ["Sunkavalli", "Kalyan", ""], ["Hadap", "Sunil", ""], ["Carr", "Nathan", ""], ["Lalonde", "Jean-Fran\u00e7ois", ""]]}, {"id": "1906.03815", "submitter": "Zahra Mirikharaji", "authors": "Zahra Mirikharaji, Yiqi Yan, and Ghassan Hamarneh", "title": "Learning to Segment Skin Lesions from Noisy Annotations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have driven substantial advancements in\nthe automatic understanding of images. Requiring a large collection of images\nand their associated annotations is one of the main bottlenecks limiting the\nadoption of deep networks. In the task of medical image segmentation, requiring\npixel-level semantic annotations performed by human experts exacerbate this\ndifficulty. This paper proposes a new framework to train a fully convolutional\nsegmentation network from a large set of cheap unreliable annotations and a\nsmall set of expert-level clean annotations. We propose a spatially adaptive\nreweighting approach to treat clean and noisy pixel-level annotations\ncommensurately in the loss function. We deploy a meta-learning approach to\nassign higher importance to pixels whose loss gradient direction is closer to\nthose of clean data. Our experiments on training the network using segmentation\nground truth corrupted with different levels of annotation noise show how\nspatial reweighting improves the robustness of deep networks to noisy\nannotations.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 06:51:21 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 21:39:44 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Mirikharaji", "Zahra", ""], ["Yan", "Yiqi", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "1906.03826", "submitter": "Yasutoshi Ida", "authors": "Yasutoshi Ida and Yasuhiro Fujiwara", "title": "Network Implosion: Effective Model Compression for ResNets via Static\n  Layer Pruning and Retraining", "comments": "Preprint of International Joint Conference on Neural Networks (IJCNN)\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual Networks with convolutional layers are widely used in the field of\nmachine learning. Since they effectively extract features from input data by\nstacking multiple layers, they can achieve high accuracy in many applications.\nHowever, the stacking of many layers raises their computation costs. To address\nthis problem, we propose Network Implosion, it erases multiple layers from\nResidual Networks without degrading accuracy. Our key idea is to introduce a\npriority term that identifies the importance of a layer; we can select\nunimportant layers according to the priority and erase them after the training.\nIn addition, we retrain the networks to avoid critical drops in accuracy after\nlayer erasure. A theoretical assessment reveals that our erasure and retraining\nscheme can erase layers without accuracy drop, and achieve higher accuracy than\nis possible with training from scratch. Our experiments show that Network\nImplosion can, for classification on Cifar-10/100 and ImageNet, reduce the\nnumber of layers by 24.00 to 42.86 percent without any drop in accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 07:53:18 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Ida", "Yasutoshi", ""], ["Fujiwara", "Yasuhiro", ""]]}, {"id": "1906.03841", "submitter": "Xiao Li", "authors": "Xiao Li, Yue Dong, Pieter Peers, Xin Tong", "title": "Synthesizing 3D Shapes from Silhouette Image Collections using\n  Multi-projection Generative Adversarial Networks", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new weakly supervised learning-based method for generating novel\ncategory-specific 3D shapes from unoccluded image collections. Our method is\nweakly supervised and only requires silhouette annotations from unoccluded,\ncategory-specific objects. Our method does not require access to the object's\n3D shape, multiple observations per object from different views, intra-image\npixel-correspondences, or any view annotations. Key to our method is a novel\nmulti-projection generative adversarial network (MP-GAN) that trains a 3D shape\ngenerator to be consistent with multiple 2D projections of the 3D shapes, and\nwithout direct access to these 3D shapes. This is achieved through multiple\ndiscriminators that encode the distribution of 2D projections of the 3D shapes\nseen from a different views. Additionally, to determine the view information\nfor each silhouette image, we also train a view prediction network on\nvisualizations of 3D shapes synthesized by the generator. We iteratively\nalternate between training the generator and training the view prediction\nnetwork. We validate our multi-projection GAN on both synthetic and real image\ndatasets. Furthermore, we also show that multi-projection GANs can aid in\nlearning other high-dimensional distributions from lower dimensional training\ndatasets, such as material-class specific spatially varying reflectance\nproperties from images.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 08:45:35 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Li", "Xiao", ""], ["Dong", "Yue", ""], ["Peers", "Pieter", ""], ["Tong", "Xin", ""]]}, {"id": "1906.03847", "submitter": "Chenyang Si", "authors": "Chenyang Si, Wentao Chen, Wei Wang, Liang Wang, Tieniu Tan", "title": "Progressive Cluster Purification for Transductive Few-shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning aims to learn to generalize a classifier to novel classes\nwith limited labeled data. Transductive inference that utilizes unlabeled test\nset to deal with low-data problem has been employed for few-shot learning in\nrecent literature. Yet, these methods do not explicitly exploit the manifold\nstructures of semantic clusters, which is inefficient for transductive\ninference. In this paper, we propose a novel Progressive Cluster Purification\n(PCP) method for transductive few-shot learning. The PCP can progressively\npurify the cluster by exploring the semantic interdependency in the individual\ncluster space. Specifically, the PCP consists of two-level operations:\ninter-class classification and intra-class transduction. The inter-class\nclassification partitions all the test samples into several clusters by\ncomparing the test samples with the prototypes. The intra-class transduction\neffectively explores trustworthy test samples for each cluster by modeling data\nrelations within a cluster as well as among different clusters. Then, it\nrefines the prototypes to better represent the real distribution of semantic\nclusters. The refined prototypes are used to remeasure all the test instances\nand purify each cluster. Furthermore, the inter-class classification and the\nintra-class transduction are extremely flexible to be repeated several times to\nprogressively purify the clusters. Experimental results are provided on two\ndatasets: miniImageNet dataset and tieredImageNet dataset. The comparison\nresults demonstrate the effectiveness of our approach and show that our\napproach outperforms the state-of-the-art methods on both datasets.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 08:55:37 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Si", "Chenyang", ""], ["Chen", "Wentao", ""], ["Wang", "Wei", ""], ["Wang", "Liang", ""], ["Tan", "Tieniu", ""]]}, {"id": "1906.03853", "submitter": "Jiajun Wu", "authors": "Zhenjia Xu, Jiajun Wu, Andy Zeng, Joshua B. Tenenbaum, and Shuran Song", "title": "DensePhysNet: Learning Dense Physical Object Representations via\n  Multi-step Dynamic Interactions", "comments": "RSS 2019. Project page: http://zhenjiaxu.com/DensePhysNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning physical object representations for robot\nmanipulation. Understanding object physics is critical for successful object\nmanipulation, but also challenging because physical object properties can\nrarely be inferred from the object's static appearance. In this paper, we\npropose DensePhysNet, a system that actively executes a sequence of dynamic\ninteractions (e.g., sliding and colliding), and uses a deep predictive model\nover its visual observations to learn dense, pixel-wise representations that\nreflect the physical properties of observed objects. Our experiments in both\nsimulation and real settings demonstrate that the learned representations carry\nrich physical information, and can directly be used to decode physical object\nproperties such as friction and mass. The use of dense representation enables\nDensePhysNet to generalize well to novel scenes with more objects than in\ntraining. With knowledge of object physics, the learned representation also\nleads to more accurate and efficient manipulation in downstream tasks than the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 09:13:02 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 18:10:45 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Xu", "Zhenjia", ""], ["Wu", "Jiajun", ""], ["Zeng", "Andy", ""], ["Tenenbaum", "Joshua B.", ""], ["Song", "Shuran", ""]]}, {"id": "1906.03857", "submitter": "Yufei Wang", "authors": "Yufei Wang, Du Tran, Lorenzo Torresani", "title": "UniDual: A Unified Model for Image and Video Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although a video is effectively a sequence of images, visual perception\nsystems typically model images and videos separately, thus failing to exploit\nthe correlation and the synergy provided by these two media. While a few prior\nresearch efforts have explored the benefits of leveraging still-image datasets\nfor video analysis, or vice-versa, most of these attempts have been limited to\npretraining a model on one type of visual modality and then adapting it via\nfinetuning on the other modality. In contrast, in this paper we introduce a\nframework that enables joint training of a unified model on mixed collections\nof image and video examples spanning different tasks. The key ingredient in our\narchitecture design is a new network block, which we name UniDual. It consists\nof a shared 2D spatial convolution followed by two parallel point-wise\nconvolutional layers, one devoted to images and the other one used for videos.\nFor video input, the point-wise filtering implements a temporal convolution.\nFor image input, it performs a pixel-wise nonlinear transformation. Repeated\nstacking of such blocks gives rise to a network where images and videos undergo\npartially distinct execution pathways, unified by spatial convolutions\n(capturing commonalities in visual appearance) but separated by point-wise\noperations (modeling patterns specific to each modality). Extensive experiments\non Kinetics and ImageNet demonstrate that our UniDual model jointly trained on\nthese datasets yields substantial accuracy gains for both tasks, compared to 1)\ntraining separate models, 2) traditional multi-task learning and 3) the\nconventional framework of pretraining-followed-by-finetuning. On Kinetics, the\nUniDual architecture applied to a state-of-the-art video backbone model\n(R(2+1)D-152) yields an additional video@1 accuracy gain of 1.5%.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 09:15:52 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 07:30:33 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Wang", "Yufei", ""], ["Tran", "Du", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "1906.03859", "submitter": "Yuval Atzmon", "authors": "Roman Visotsky, Yuval Atzmon, Gal Chechik", "title": "Few-Shot Learning with Per-Sample Rich Supervision", "comments": "Accepted to AGI 2019 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning with few samples is a major challenge for parameter-rich models like\ndeep networks. In contrast, people learn complex new concepts even from very\nfew examples, suggesting that the sample complexity of learning can often be\nreduced. Many approaches to few-shot learning build on transferring a\nrepresentation from well-sampled classes, or using meta learning to favor\narchitectures that can learn with few samples. Unfortunately, such approaches\noften struggle when learning in an online way or with non-stationary data\nstreams. Here we describe a new approach to learn with fewer samples, by using\nadditional information that is provided per sample. Specifically, we show how\nthe sample complexity can be reduced by providing semantic information about\nthe relevance of features per sample, like information about the presence of\nobjects in a scene or confidence of detecting attributes in an image. We\nprovide an improved generalization error bound for this case. We cast the\nproblem of using per-sample feature relevance by using a new ellipsoid-margin\nloss, and develop an online algorithm that minimizes this loss effectively.\nEmpirical evaluation on two machine vision benchmarks for scene classification\nand fine-grain bird classification demonstrate the benefits of this approach\nfor few-shot learning.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 09:17:30 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Visotsky", "Roman", ""], ["Atzmon", "Yuval", ""], ["Chechik", "Gal", ""]]}, {"id": "1906.03861", "submitter": "Rohan Ghosh", "authors": "Rohan Ghosh and Anupam K. Gupta", "title": "Scale Steerable Filters for Locally Scale-Invariant Convolutional Neural\n  Networks", "comments": "Accepted as a Spotlight talk to ICML Workshop on Theoretical Physics\n  for Deep Learning, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmenting transformation knowledge onto a convolutional neural network's\nweights has often yielded significant improvements in performance. For\nrotational transformation augmentation, an important element to recent\napproaches has been the use of a steerable basis i.e. the circular harmonics.\nHere, we propose a scale-steerable filter basis for the locally scale-invariant\nCNN, denoted as log-radial harmonics. By replacing the kernels in the locally\nscale-invariant CNN \\cite{lsi_cnn} with scale-steered kernels, significant\nimprovements in performance can be observed on the MNIST-Scale and FMNIST-Scale\ndatasets. Training with a scale-steerable basis results in filters which show\nmeaningful structure, and feature maps demonstrate which demonstrate visibly\nhigher spatial-structure preservation of input. Furthermore, the proposed\nscale-steerable CNN shows on-par generalization to global affine transformation\nestimation methods such as Spatial Transformers, in response to test-time data\ndistortions.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 09:25:38 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Ghosh", "Rohan", ""], ["Gupta", "Anupam K.", ""]]}, {"id": "1906.03905", "submitter": "Wang Zhengyong", "authors": "Qiuyu Zhu, Zhengyong Wang", "title": "An Image Clustering Auto-Encoder Based on Predefined Evenly-Distributed\n  Class Centroids and MMD Distance", "comments": "Accepted by Neural Processing Letters", "journal-ref": null, "doi": "10.1007/s11063-020-10194-y", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel, effective and simpler end-to-end image\nclustering auto-encoder algorithm: ICAE. The algorithm uses PEDCC (Predefined\nEvenly-Distributed Class Centroids) as the clustering centers, which ensures\nthe inter-class distance of latent features is maximal, and adds data\ndistribution constraint, data augmentation constraint, auto-encoder\nreconstruction constraint and Sobel smooth constraint to improve the clustering\nperformance. Specifically, we perform one-to-one data augmentation to learn the\nmore effective features. The data and the augmented data are simultaneously\ninput into the autoencoder to obtain latent features and the augmented latent\nfeatures whose similarity are constrained by an augmentation loss. Then, making\nuse of the maximum mean discrepancy distance (MMD), we combine the latent\nfeatures and augmented latent features to make their distribution close to the\nPEDCC distribution (uniform distribution between classes, Dirac distribution\nwithin the class) to further learn clustering-oriented features. At the same\ntime, the MSE of the original input image and reconstructed image is used as\nreconstruction constraint, and the Sobel smooth loss to build generalization\nconstraint to improve the generalization ability. Finally, extensive\nexperiments on three common datasets MNIST, Fashion-MNIST, COIL20 are\nconducted. The experimental results show that the algorithm has achieved the\nbest clustering results so far. In addition, we can use the predefined PEDCC\nclass centers, and the decoder to clearly generate the samples of each class.\nThe code can be downloaded at https://github.com/zyWang-Power/Clustering!\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 11:28:15 GMT"}, {"version": "v2", "created": "Sat, 4 Jan 2020 07:29:28 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Zhu", "Qiuyu", ""], ["Wang", "Zhengyong", ""]]}, {"id": "1906.03906", "submitter": "Guotai Wang", "authors": "Guotai Wang, Jonathan Shapey, Wenqi Li, Reuben Dorent, Alex\n  Demitriadis, Sotirios Bisdas, Ian Paddick, Robert Bradford, Sebastien\n  Ourselin, Tom Vercauteren", "title": "Automatic Segmentation of Vestibular Schwannoma from T2-Weighted MRI by\n  Deep Spatial Attention with Hardness-Weighted Loss", "comments": "9 pages, 4 figures, submitted to MICCAI", "journal-ref": null, "doi": "10.1007/978-3-030-32245-8_30", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of vestibular schwannoma (VS) tumors from magnetic\nresonance imaging (MRI) would facilitate efficient and accurate volume\nmeasurement to guide patient management and improve clinical workflow. The\naccuracy and robustness is challenged by low contrast, small target region and\nlow through-plane resolution. We introduce a 2.5D convolutional neural network\n(CNN) able to exploit the different in-plane and through-plane resolutions\nencountered in standard of care imaging protocols. We use an attention module\nto enable the CNN to focus on the small target and propose a supervision on the\nlearning of attention maps for more accurate segmentation. Additionally, we\npropose a hardness-weighted Dice loss function that gives higher weights to\nharder voxels to boost the training of CNNs. Experiments with ablation studies\non the VS tumor segmentation task show that: 1) the proposed 2.5D CNN\noutperforms its 2D and 3D counterparts, 2) our supervised attention mechanism\noutperforms unsupervised attention, 3) the voxel-level hardness-weighted Dice\nloss can improve the performance of CNNs. Our method achieved an average Dice\nscore and ASSD of 0.87 and 0.43~mm respectively. This will facilitate patient\nmanagement decisions in clinical practice.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 11:28:22 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Wang", "Guotai", ""], ["Shapey", "Jonathan", ""], ["Li", "Wenqi", ""], ["Dorent", "Reuben", ""], ["Demitriadis", "Alex", ""], ["Bisdas", "Sotirios", ""], ["Paddick", "Ian", ""], ["Bradford", "Robert", ""], ["Ourselin", "Sebastien", ""], ["Vercauteren", "Tom", ""]]}, {"id": "1906.03918", "submitter": "Gaurvi Goyal", "authors": "Gaurvi Goyal, Nicoletta Noceti, Francesca Odone, Alessandra Sciutti", "title": "The role of ego vision in view-invariant action recognition", "comments": "Accepted for presentation at EPIC@CVPR2019 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis and interpretation of egocentric video data is becoming more and\nmore important with the increasing availability and use of wearable cameras.\nExploring and fully understanding affinities and differences between ego and\nallo (or third-person) vision is paramount for the design of effective methods\nto process, analyse and interpret egocentric data. In addition, a deeper\nunderstanding of ego-vision and its peculiarities may enable new research\nperspectives in which first person viewpoints can act either as a mean for\neasily acquiring large amounts of data to be employed in general-purpose\nrecognition systems, and as a challenging test-bed to assess the usability of\ntechniques specifically tailored to deal with allocentric vision on more\nchallenging settings. Our work, with an eye to cognitive science findings,\nleverages transfer learning in Convolutional Neural Networks to demonstrate\ncapabilities and limitations of an implicitly learnt view-invariant\nrepresentation in the specific case of action recognition.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 11:49:52 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Goyal", "Gaurvi", ""], ["Noceti", "Nicoletta", ""], ["Odone", "Francesca", ""], ["Sciutti", "Alessandra", ""]]}, {"id": "1906.03922", "submitter": "Yong Man Ro", "authors": "Hyebin Lee, Seong Tae Kim, Yong Man Ro", "title": "Generation of Multimodal Justification Using Visual Word Constraint\n  Model for Explainable Computer-Aided Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ambiguity of the decision-making process has been pointed out as the main\nobstacle to applying the deep learning-based method in a practical way in spite\nof its outstanding performance. Interpretability could guarantee the confidence\nof deep learning system, therefore it is particularly important in the medical\nfield. In this study, a novel deep network is proposed to explain the\ndiagnostic decision with visual pointing map and diagnostic sentence justifying\nresult simultaneously. For the purpose of increasing the accuracy of sentence\ngeneration, a visual word constraint model is devised in training justification\ngenerator. To verify the proposed method, comparative experiments were\nconducted on the problem of the diagnosis of breast masses. Experimental\nresults demonstrated that the proposed deep network could explain diagnosis\nmore accurately with various textual justifications.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 12:07:58 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Lee", "Hyebin", ""], ["Kim", "Seong Tae", ""], ["Ro", "Yong Man", ""]]}, {"id": "1906.03951", "submitter": "Linfeng Zhang", "authors": "Linfeng Zhang, Zhanhong Tan, Jiebo Song, Jingwei Chen, Chenglong Bao,\n  Kaisheng Ma", "title": "SCAN: A Scalable Neural Networks Framework Towards Compact and Efficient\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remarkable achievements have been attained by deep neural networks in various\napplications. However, the increasing depth and width of such models also lead\nto explosive growth in both storage and computation, which has restricted the\ndeployment of deep neural networks on resource-limited edge devices. To address\nthis problem, we propose the so-called SCAN framework for networks training and\ninference, which is orthogonal and complementary to existing acceleration and\ncompression methods. The proposed SCAN firstly divides neural networks into\nmultiple sections according to their depth and constructs shallow classifiers\nupon the intermediate features of different sections. Moreover, attention\nmodules and knowledge distillation are utilized to enhance the accuracy of\nshallow classifiers. Based on this architecture, we further propose a threshold\ncontrolled scalable inference mechanism to approach human-like sample-specific\ninference. Experimental results show that SCAN can be easily equipped on\nvarious neural networks without any adjustment on hyper-parameters or neural\nnetworks architectures, yielding significant performance gain on CIFAR100 and\nImageNet. Codes will be released on github soon.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 11:11:01 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Zhang", "Linfeng", ""], ["Tan", "Zhanhong", ""], ["Song", "Jiebo", ""], ["Chen", "Jingwei", ""], ["Bao", "Chenglong", ""], ["Ma", "Kaisheng", ""]]}, {"id": "1906.03973", "submitter": "Jaakko Lehtinen", "authors": "Markus Kettunen, Erik H\\\"ark\\\"onen, Jaakko Lehtinen", "title": "E-LPIPS: Robust Perceptual Image Similarity via Random Transformation\n  Ensembles", "comments": "Code and supplemental material available at\n  https://github.com/mkettune/elpips/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  It has been recently shown that the hidden variables of convolutional neural\nnetworks make for an efficient perceptual similarity metric that accurately\npredicts human judgment on relative image similarity assessment. First, we show\nthat such learned perceptual similarity metrics (LPIPS) are susceptible to\nadversarial attacks that dramatically contradict human visual similarity\njudgment. While this is not surprising in light of neural networks' well-known\nweakness to adversarial perturbations, we proceed to show that self-ensembling\nwith an infinite family of random transformations of the input --- a technique\nknown not to render classification networks robust --- is enough to turn the\nmetric robust against attack, while retaining predictive power on human\njudgments. Finally, we study the geometry imposed by our our novel\nself-ensembled metric (E-LPIPS) on the space of natural images. We find\nevidence of \"perceptual convexity\" by showing that convex combinations of\nsimilar-looking images retain appearance, and that discrete geodesics yield\nmeaningful frame interpolation and texture morphing, all without explicit\ncorrespondences.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 13:40:37 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 08:58:35 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Kettunen", "Markus", ""], ["H\u00e4rk\u00f6nen", "Erik", ""], ["Lehtinen", "Jaakko", ""]]}, {"id": "1906.03990", "submitter": "Kaibing Chen", "authors": "Kaibing Chen, Cheng Cui, Yuning Du, Xianglong Meng, Hui Ren", "title": "2nd Place and 2nd Place Solution to Kaggle Landmark Recognition\n  andRetrieval Competition 2019", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a retrieval based system for landmark retrieval and recognition\nchallenge.There are five parts in retrieval competition system, including\nfeature extraction and matching to get candidates queue; database augmentation\nand query extension searching; reranking from recognition results and local\nfeature matching. In recognition challenge including: landmark and non-landmark\nrecognition, multiple recognition results voting and reranking using\ncombination of recognition and retrieval results. All of models trained and\npredicted by PaddlePaddle framework. Using our method, we achieved 2nd place in\nthe Google Landmark Recognition 2019 and 2nd place in the Google Landmark\nRetrieval 2019 on kaggle. The source code is available at here.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 14:04:37 GMT"}, {"version": "v2", "created": "Sun, 16 Jun 2019 12:42:03 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Chen", "Kaibing", ""], ["Cui", "Cheng", ""], ["Du", "Yuning", ""], ["Meng", "Xianglong", ""], ["Ren", "Hui", ""]]}, {"id": "1906.03994", "submitter": "Vasileios Argyriou", "authors": "Antoine Rimboux, Rob Dupre, Thomas Lagkas, Panagiotis Sarigiannidis,\n  Paolo Remagnino, Vasileios Argyriou", "title": "Smart IoT Cameras for Crowd Analysis based on augmentation for automatic\n  pedestrian detection, simulation and annotation", "comments": "IoTI4 Workshop 2019. arXiv admin note: text overlap with\n  arXiv:1707.02655", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart video sensors for applications related to surveillance and security are\nIOT-based as they use Internet for various purposes. Such applications include\ncrowd behaviour monitoring and advanced decision support systems operating and\ntransmitting information over internet. The analysis of crowd and pedestrian\nbehaviour is an important task for smart IoT cameras and in particular video\nprocessing. In order to provide related behavioural models, simulation and\ntracking approaches have been considered in the literature. In both cases\nground truth is essential to train deep models and provide a meaningful\nquantitative evaluation. We propose a framework for crowd simulation and\nautomatic data generation and annotation that supports multiple cameras and\nmultiple targets. The proposed approach is based on synthetically generated\nhuman agents, augmented frames and compositing techniques combined with path\nfinding and planning methods. A number of popular crowd and pedestrian data\nsets were used to validate the model, and scenarios related to annotation and\nsimulation were considered.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 20:43:21 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Rimboux", "Antoine", ""], ["Dupre", "Rob", ""], ["Lagkas", "Thomas", ""], ["Sarigiannidis", "Panagiotis", ""], ["Remagnino", "Paolo", ""], ["Argyriou", "Vasileios", ""]]}, {"id": "1906.04002", "submitter": "Long-Fei Chen", "authors": "Longfei Chen, Yuichi Nakamura, Kazuaki Kondo", "title": "Detecting Clues for Skill Levels and Machine Operation Difficulty from\n  Egocentric Vision", "comments": "Accepted for presentation at EPIC@CVPR2019 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With respect to machine operation tasks, the experiences from different skill\nlevel operators, especially novices, can provide worthy understanding about the\nmanner in which they perceive the operational environment and formulate\nknowledge to deal with various operation situations. In this study, we describe\nthe operator's behaviors by utilizing the relations among their head, hand, and\noperation location (hotspot) during the operation. A total of 40 experiences\nassociated with a sewing machine operation task performed by amateur operators\nwas recorded via a head-mounted RGB-D camera. We examined important features of\noperational behaviors in different skill level operators and confirmed their\ncorrelation to the difficulties of the operation steps. The result shows that\nthe pure-gazing behavior is significantly reduced when the operator's skill\nimproved. Moreover, the hand-approaching duration and the frequency of\nattention movement before operation are strongly correlated to the operational\ndifficulty in such machine operating environments.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 14:18:34 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Chen", "Longfei", ""], ["Nakamura", "Yuichi", ""], ["Kondo", "Kazuaki", ""]]}, {"id": "1906.04016", "submitter": "Gedas Bertasius", "authors": "Gedas Bertasius, Christoph Feichtenhofer, Du Tran, Jianbo Shi, Lorenzo\n  Torresani", "title": "Learning Temporal Pose Estimation from Sparsely-Labeled Videos", "comments": "Accepted to NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern approaches for multi-person pose estimation in video require large\namounts of dense annotations. However, labeling every frame in a video is\ncostly and labor intensive. To reduce the need for dense annotations, we\npropose a PoseWarper network that leverages training videos with sparse\nannotations (every k frames) to learn to perform dense temporal pose\npropagation and estimation. Given a pair of video frames---a labeled Frame A\nand an unlabeled Frame B---we train our model to predict human pose in Frame A\nusing the features from Frame B by means of deformable convolutions to\nimplicitly learn the pose warping between A and B. We demonstrate that we can\nleverage our trained PoseWarper for several applications. First, at inference\ntime we can reverse the application direction of our network in order to\npropagate pose information from manually annotated frames to unlabeled frames.\nThis makes it possible to generate pose annotations for the entire video given\nonly a few manually-labeled frames. Compared to modern label propagation\nmethods based on optical flow, our warping mechanism is much more compact (6M\nvs 39M parameters), and also more accurate (88.7% mAP vs 83.8% mAP). We also\nshow that we can improve the accuracy of a pose estimator by training it on an\naugmented dataset obtained by adding our propagated poses to the original\nmanual labels. Lastly, we can use our PoseWarper to aggregate temporal pose\ninformation from neighboring frames during inference. This allows our system to\nachieve state-of-the-art pose detection results on the PoseTrack2017 and\nPoseTrack2018 datasets. Code has been made available at:\nhttps://github.com/facebookresearch/PoseWarper.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 21:24:52 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2019 19:30:14 GMT"}, {"version": "v3", "created": "Wed, 11 Dec 2019 07:39:26 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Bertasius", "Gedas", ""], ["Feichtenhofer", "Christoph", ""], ["Tran", "Du", ""], ["Shi", "Jianbo", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "1906.04087", "submitter": "Shuhei Yokoo", "authors": "Kohei Ozaki and Shuhei Yokoo", "title": "Large-scale Landmark Retrieval/Recognition under a Noisy and Diverse\n  Dataset", "comments": "Technical report for Second Landmark Recognition Workshop to be held\n  at CVPR 2019 (Long Beach, CA), June 16th, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Google-Landmarks-v2 dataset is the biggest worldwide landmarks dataset\ncharacterized by a large magnitude of noisiness and diversity. We present a\nnovel landmark retrieval/recognition system, robust to a noisy and diverse\ndataset, by our team, smlyaka. Our approach is based on deep convolutional\nneural networks with metric learning, trained by cosine-softmax based losses.\nDeep metric learning methods are usually sensitive to noise, and it could\nhinder to learn a reliable metric. To address this issue, we develop an\nautomated data cleaning system. Besides, we devise a discriminative re-ranking\nmethod to address the diversity of the dataset for landmark retrieval. Using\nour methods, we achieved 1st place in the Google Landmark Retrieval 2019\nchallenge and 3rd place in the Google Landmark Recognition 2019 challenge on\nKaggle.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 16:00:06 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 08:32:16 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Ozaki", "Kohei", ""], ["Yokoo", "Shuhei", ""]]}, {"id": "1906.04104", "submitter": "Daniil Osokin", "authors": "Daniil Osokin", "title": "Global Context for Convolutional Pose Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Pose Machine is a popular neural network architecture for\narticulated pose estimation. In this work we explore its empirical receptive\nfield and realize, that it can be enhanced with integration of a global\ncontext. To do so U-shaped context module is proposed and compared with the\npyramid pooling and atrous spatial pyramid pooling modules, which are often\nused in semantic segmentation domain. The proposed neural network achieves\nstate-of-the-art accuracy with 87.9% PCKh for single-person pose estimation on\nthe Look Into Person dataset. A smaller version of this network runs more than\n160 frames per second while being just 2.9% less accurate. Generalization of\nthe proposed approach is tested on the MPII benchmark and shown, that it faster\nthan hourglass-based networks, while provides similar accuracy. The code is\navailable at\nhttps://github.com/opencv/openvino_training_extensions/tree/develop/pytorch_toolkit/human_pose_estimation .\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 16:24:04 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Osokin", "Daniil", ""]]}, {"id": "1906.04109", "submitter": "Quanshi Zhang", "authors": "Haotian Ma, Yinqing Zhang, Fan Zhou, Quanshi Zhang", "title": "Quantifying Layerwise Information Discarding of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method to explain how input information is discarded\nthrough intermediate layers of a neural network during the forward propagation,\nin order to quantify and diagnose knowledge representations of pre-trained deep\nneural networks. We define two types of entropy-based metrics, i.e., the strict\ninformation discarding and the reconstruction uncertainty, which measure input\ninformation of a specific layer from two perspectives. We develop a method to\nenable efficient computation of such entropy-based metrics. Our method can be\nbroadly applied to various neural networks and enable comprehensive comparisons\nbetween different layers of different networks. Preliminary experiments have\nshown the effectiveness of our metrics in analyzing benchmark networks and\nexplaining existing deep-learning techniques.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 16:33:03 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Ma", "Haotian", ""], ["Zhang", "Yinqing", ""], ["Zhou", "Fan", ""], ["Zhang", "Quanshi", ""]]}, {"id": "1906.04111", "submitter": "Sergio Vitale", "authors": "Sergio Vitale, Giampaolo Ferraioli and Vito Pascazio", "title": "A New Ratio Image Based CNN Algorithm For SAR Despeckling", "comments": null, "journal-ref": "IGARSS 2019 - 2019 IEEE International Geoscience and Remote\n  Sensing Symposium, Yokohama, Japan, 2019, pp. 9494-9497", "doi": "10.1109/IGARSS.2019.8899245", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In SAR domain many application like classification, detection and\nsegmentation are impaired by speckle. Hence, despeckling of SAR images is the\nkey for scene understanding. Usually despeckling filters face the trade-off of\nspeckle suppression and information preservation. In the last years deep\nlearning solutions for speckle reduction have been proposed. One the biggest\nissue for these methods is how to train a network given the lack of a\nreference. In this work we proposed a convolutional neural network based\nsolution trained on simulated data. We propose the use of a cost function\ntaking into account both spatial and statistical properties. The aim is two\nfold: overcome the trade-off between speckle suppression and details\nsuppression; find a suitable cost function for despeckling in unsupervised\nlearning. The algorithm is validated on both real and simulated data, showing\ninteresting performances.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 16:38:57 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Vitale", "Sergio", ""], ["Ferraioli", "Giampaolo", ""], ["Pascazio", "Vito", ""]]}, {"id": "1906.04117", "submitter": "Can Chen", "authors": "Can Chen, Luca Zanotti Fragonara, Antonios Tsourdos", "title": "Fast Hierarchical Neural Network for Feature Learning on Point Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analyses relying on 3D point clouds are an utterly complex task, often\ninvolving million of points, but also requiring computationally efficient\nalgorithms because of many real-time applications; e.g. autonomous vehicle.\nHowever, point clouds are intrinsically irregular and the points are sparsely\ndistributed in a non-Euclidean space, which normally requires point-wise\nprocessing to achieve high performances. Although shared filter matrices and\npooling layers in convolutional neural networks (CNNs) are capable of reducing\nthe dimensionality of the problem and extracting high-level information\nsimultaneously, grids and highly regular data format are required as input. In\norder to balance model performance and complexity, we introduce a novel neural\nnetwork architecture exploiting local features from a manually subsampled point\nset. In our network, a recursive farthest point sampling method is firstly\napplied to efficiently cover the entire point set. Successively, we employ the\nk-nearest neighbours (knn) algorithm to gather local neighbourhood for each\ngroup of the subsampled points. Finally, a multiple layer perceptron (MLP) is\napplied on the subsampled points and edges that connect corresponding point and\nneighbours to extract local features. The architecture has been tested for both\nshape classification and segmentation using the ModelNet40 and ShapeNet part\ndatasets, in order to show that the network achieves the best trade-off in\nterms of competitive performance when compared to other state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 16:49:40 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Chen", "Can", ""], ["Fragonara", "Luca Zanotti", ""], ["Tsourdos", "Antonios", ""]]}, {"id": "1906.04151", "submitter": "Weijian Li", "authors": "Weijian Li, Viet-Duy Nguyen, Haofu Liao, Matt Wilder, Ke Cheng, Jiebo\n  Luo", "title": "Patch Transformer for Multi-tagging Whole Slide Histopathology Images", "comments": "To appear at the International Conference on Medical Image Computing\n  and Computer Assisted Intervention (MICCAI-19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated whole slide image (WSI) tagging has become a growing demand due to\nthe increasing volume and diversity of WSIs collected nowadays in\nhistopathology. Various methods have been studied to classify WSIs with single\ntags but none of them focuses on labeling WSIs with multiple tags. To this end,\nwe propose a novel end-to-end trainable deep neural network named Patch\nTransformer which can effectively predict multiple slide-level tags from WSI\npatches based on both the correlations and the uniqueness between the tags.\nSpecifically, the proposed method learns patch characteristics considering 1)\npatch-wise relations through a patch transformation module and 2) tag-wise\nuniqueness for each tagging task through a multi-tag attention module.\nExtensive experiments on a large and diverse dataset consisting of 4,920 WSIs\nprove the effectiveness of the proposed model.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 17:43:52 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 00:50:00 GMT"}, {"version": "v3", "created": "Thu, 4 Jul 2019 13:29:54 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Li", "Weijian", ""], ["Nguyen", "Viet-Duy", ""], ["Liao", "Haofu", ""], ["Wilder", "Matt", ""], ["Cheng", "Ke", ""], ["Luo", "Jiebo", ""]]}, {"id": "1906.04158", "submitter": "Hanbyul Joo", "authors": "Hanbyul Joo, Tomas Simon, Mina Cikara, Yaser Sheikh", "title": "Towards Social Artificial Intelligence: Nonverbal Social Signal\n  Prediction in A Triadic Interaction", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new research task and a dataset to understand human social\ninteractions via computational methods, to ultimately endow machines with the\nability to encode and decode a broad channel of social signals humans use. This\nresearch direction is essential to make a machine that genuinely communicates\nwith humans, which we call Social Artificial Intelligence. We first formulate\nthe \"social signal prediction\" problem as a way to model the dynamics of social\nsignals exchanged among interacting individuals in a data-driven way. We then\npresent a new 3D motion capture dataset to explore this problem, where the\nbroad spectrum of social signals (3D body, face, and hand motions) are captured\nin a triadic social interaction scenario. Baseline approaches to predict\nspeaking status, social formation, and body gestures of interacting individuals\nare presented in the defined social prediction framework.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 17:56:03 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Joo", "Hanbyul", ""], ["Simon", "Tomas", ""], ["Cikara", "Mina", ""], ["Sheikh", "Yaser", ""]]}, {"id": "1906.04160", "submitter": "Shiry Ginosar", "authors": "Shiry Ginosar, Amir Bar, Gefen Kohavi, Caroline Chan, Andrew Owens,\n  Jitendra Malik", "title": "Learning Individual Styles of Conversational Gesture", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human speech is often accompanied by hand and arm gestures. Given audio\nspeech input, we generate plausible gestures to go along with the sound.\nSpecifically, we perform cross-modal translation from \"in-the-wild'' monologue\nspeech of a single speaker to their hand and arm motion. We train on unlabeled\nvideos for which we only have noisy pseudo ground truth from an automatic pose\ndetection system. Our proposed model significantly outperforms baseline methods\nin a quantitative comparison. To support research toward obtaining a\ncomputational understanding of the relationship between gesture and speech, we\nrelease a large video dataset of person-specific gestures. The project website\nwith video, code and data can be found at\nhttp://people.eecs.berkeley.edu/~shiry/speech2gesture .\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 17:58:08 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Ginosar", "Shiry", ""], ["Bar", "Amir", ""], ["Kohavi", "Gefen", ""], ["Chan", "Caroline", ""], ["Owens", "Andrew", ""], ["Malik", "Jitendra", ""]]}, {"id": "1906.04161", "submitter": "Deepak Pathak", "authors": "Deepak Pathak, Dhiraj Gandhi, Abhinav Gupta", "title": "Self-Supervised Exploration via Disagreement", "comments": "Accepted at ICML 2019. Website at\n  https://pathak22.github.io/exploration-by-disagreement/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient exploration is a long-standing problem in sensorimotor learning.\nMajor advances have been demonstrated in noise-free, non-stochastic domains\nsuch as video games and simulation. However, most of these formulations either\nget stuck in environments with stochastic dynamics or are too inefficient to be\nscalable to real robotics setups. In this paper, we propose a formulation for\nexploration inspired by the work in active learning literature. Specifically,\nwe train an ensemble of dynamics models and incentivize the agent to explore\nsuch that the disagreement of those ensembles is maximized. This allows the\nagent to learn skills by exploring in a self-supervised manner without any\nexternal reward. Notably, we further leverage the disagreement objective to\noptimize the agent's policy in a differentiable manner, without using\nreinforcement learning, which results in a sample-efficient exploration. We\ndemonstrate the efficacy of this formulation across a variety of benchmark\nenvironments including stochastic-Atari, Mujoco and Unity. Finally, we\nimplement our differentiable exploration on a real robot which learns to\ninteract with objects completely from scratch. Project videos and code are at\nhttps://pathak22.github.io/exploration-by-disagreement/\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 17:58:32 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Pathak", "Deepak", ""], ["Gandhi", "Dhiraj", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1906.04176", "submitter": "Caleb Robinson", "authors": "Caleb Robinson, Anthony Ortiz, Kolya Malkin, Blake Elias, Andi Peng,\n  Dan Morris, Bistra Dilkina, Nebojsa Jojic", "title": "Human-Machine Collaboration for Fast Land Cover Mapping", "comments": "To appear in AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose incorporating human labelers in a model fine-tuning system that\nprovides immediate user feedback. In our framework, human labelers can\ninteractively query model predictions on unlabeled data, choose which data to\nlabel, and see the resulting effect on the model's predictions. This\nbi-directional feedback loop allows humans to learn how the model responds to\nnew data. Our hypothesis is that this rich feedback allows human labelers to\ncreate mental models that enable them to better choose which biases to\nintroduce to the model. We compare human-selected points to points selected\nusing standard active learning methods. We further investigate how the\nfine-tuning methodology impacts the human labelers' performance. We implement\nthis framework for fine-tuning high-resolution land cover segmentation models.\nSpecifically, we fine-tune a deep neural network -- trained to segment\nhigh-resolution aerial imagery into different land cover classes in Maryland,\nUSA -- to a new spatial area in New York, USA. The tight loop turns the\nalgorithm and the human operator into a hybrid system that can produce land\ncover maps of a large area much more efficiently than the traditional\nworkflows. Our framework has applications in geospatial machine learning\nsettings where there is a practically limitless supply of unlabeled data, of\nwhich only a small fraction can feasibly be labeled through human efforts.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 16:04:57 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 01:31:45 GMT"}, {"version": "v3", "created": "Thu, 14 Nov 2019 19:30:03 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Robinson", "Caleb", ""], ["Ortiz", "Anthony", ""], ["Malkin", "Kolya", ""], ["Elias", "Blake", ""], ["Peng", "Andi", ""], ["Morris", "Dan", ""], ["Dilkina", "Bistra", ""], ["Jojic", "Nebojsa", ""]]}, {"id": "1906.04201", "submitter": "Armen Avetisyan", "authors": "Armen Avetisyan, Angela Dai, Matthias Nie{\\ss}ner", "title": "End-to-End CAD Model Retrieval and 9DoF Alignment in 3D Scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel, end-to-end approach to align CAD models to an 3D scan of\na scene, enabling transformation of a noisy, incomplete 3D scan to a compact,\nCAD reconstruction with clean, complete object geometry. Our main contribution\nlies in formulating a differentiable Procrustes alignment that is paired with a\nsymmetry-aware dense object correspondence prediction. To simultaneously align\nCAD models to all the objects of a scanned scene, our approach detects object\nlocations, then predicts symmetry-aware dense object correspondences between\nscan and CAD geometry in a unified object space, as well as a nearest neighbor\nCAD model, both of which are then used to inform a differentiable Procrustes\nalignment. Our approach operates in a fully-convolutional fashion, enabling\nalignment of CAD models to the objects of a scan in a single forward pass. This\nenables our method to outperform state-of-the-art approaches by $19.04\\%$ for\nCAD model alignment to scans, with $\\approx 250\\times$ faster runtime than\nprevious data-driven approaches.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 18:01:42 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Avetisyan", "Armen", ""], ["Dai", "Angela", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1906.04226", "submitter": "Heng Wang", "authors": "Linchao Zhu, Laura Sevilla-Lara, Du Tran, Matt Feiszli, Yi Yang, Heng\n  Wang", "title": "FASTER Recurrent Networks for Efficient Video Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical video classification methods often divide a video into short clips,\ndo inference on each clip independently, then aggregate the clip-level\npredictions to generate the video-level results. However, processing visually\nsimilar clips independently ignores the temporal structure of the video\nsequence, and increases the computational cost at inference time. In this\npaper, we propose a novel framework named FASTER, i.e., Feature Aggregation for\nSpatio-TEmporal Redundancy. FASTER aims to leverage the redundancy between\nneighboring clips and reduce the computational cost by learning to aggregate\nthe predictions from models of different complexities. The FASTER framework can\nintegrate high quality representations from expensive models to capture subtle\nmotion information and lightweight representations from cheap models to cover\nscene changes in the video. A new recurrent network (i.e., FAST-GRU) is\ndesigned to aggregate the mixture of different representations. Compared with\nexisting approaches, FASTER can reduce the FLOPs by over 10x? while maintaining\nthe state-of-the-art accuracy across popular datasets, such as Kinetics,\nUCF-101 and HMDB-51.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 18:54:00 GMT"}, {"version": "v2", "created": "Sun, 8 Sep 2019 17:10:01 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Zhu", "Linchao", ""], ["Sevilla-Lara", "Laura", ""], ["Tran", "Du", ""], ["Feiszli", "Matt", ""], ["Yang", "Yi", ""], ["Wang", "Heng", ""]]}, {"id": "1906.04231", "submitter": "Yi Fung", "authors": "Yi Ren Fung, Ziqiang Guan, Ritesh Kumar, Joie Yeahuay Wu, Madalina\n  Fiterau", "title": "Alzheimer's Disease Brain MRI Classification: Challenges and Insights", "comments": "5 pages, 2 figures, IJCAI ARIAL workshop paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, many papers have reported state-of-the-art performance on\nAlzheimer's Disease classification with MRI scans from the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) dataset using convolutional neural networks.\nHowever, we discover that when we split that data into training and testing\nsets at the subject level, we are not able to obtain similar performance,\nbringing the validity of many of the previous studies into question.\nFurthermore, we point out that previous works use different subsets of the ADNI\ndata, making comparison across similar works tricky. In this study, we present\nthe results of three splitting methods, discuss the motivations behind their\nvalidity, and report our results using all of the available subjects.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 19:04:00 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Fung", "Yi Ren", ""], ["Guan", "Ziqiang", ""], ["Kumar", "Ritesh", ""], ["Wu", "Joie Yeahuay", ""], ["Fiterau", "Madalina", ""]]}, {"id": "1906.04232", "submitter": "Mohammad Hamed Mozaffari", "authors": "M. Hamed Mozaffari and Won-Sook Lee", "title": "BowNet: Dilated Convolution Neural Network for Ultrasound Tongue Contour\n  Extraction", "comments": "23 pages, 15 figures, 10 tables", "journal-ref": "BowNet: Dilated convolutional neural network for ultrasound tongue\n  contour extraction, 2019, The Journal of the Acoustical Society of America,\n  pages 2940-2941, volume 146, number 4", "doi": "10.1121/1.5137212", "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.NE cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound imaging is safe, relatively affordable, and capable of real-time\nperformance. One application of this technology is to visualize and to\ncharacterize human tongue shape and motion during a real-time speech to study\nhealthy or impaired speech production. Due to the noisy nature of ultrasound\nimages with low-contrast characteristic, it might require expertise for\nnon-expert users to recognize organ shape such as tongue surface (dorsum). To\nalleviate this difficulty for quantitative analysis of tongue shape and motion,\ntongue surface can be extracted, tracked, and visualized instead of the whole\ntongue region. Delineating the tongue surface from each frame is a cumbersome,\nsubjective, and error-prone task. Furthermore, the rapidity and complexity of\ntongue gestures have made it a challenging task, and manual segmentation is not\na feasible solution for real-time applications. Employing the power of\nstate-of-the-art deep neural network models and training techniques, it is\nfeasible to implement new fully-automatic, accurate, and robust segmentation\nmethods with the capability of real-time performance, applicable for tracking\nof the tongue contours during the speech. This paper presents two novel deep\nneural network models named BowNet and wBowNet benefits from the ability of\nglobal prediction of decoding-encoding models, with integrated multi-scale\ncontextual information, and capability of full-resolution (local) extraction of\ndilated convolutions. Experimental results using several ultrasound tongue\nimage datasets revealed that the combination of both localization and\nglobalization searching could improve prediction result significantly.\nAssessment of BowNet models using both qualitatively and quantitatively studies\nshowed them outstanding achievements in terms of accuracy and robustness in\ncomparison with similar techniques.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 19:04:09 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Mozaffari", "M. Hamed", ""], ["Lee", "Won-Sook", ""]]}, {"id": "1906.04236", "submitter": "Oana Ignat", "authors": "Oana Ignat, Laura Burdick, Jia Deng, Rada Mihalcea", "title": "Identifying Visible Actions in Lifestyle Vlogs", "comments": "Accepted at ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of identifying human actions visible in online videos.\nWe focus on the widely spread genre of lifestyle vlogs, which consist of videos\nof people performing actions while verbally describing them. Our goal is to\nidentify if actions mentioned in the speech description of a video are visually\npresent. We construct a dataset with crowdsourced manual annotations of visible\nactions, and introduce a multimodal algorithm that leverages information\nderived from visual and linguistic clues to automatically infer which actions\nare visible in a video. We demonstrate that our multimodal algorithm\noutperforms algorithms based only on one modality at a time.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 19:11:01 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Ignat", "Oana", ""], ["Burdick", "Laura", ""], ["Deng", "Jia", ""], ["Mihalcea", "Rada", ""]]}, {"id": "1906.04252", "submitter": "Hubert Cecotti", "authors": "Gregory Dzhezyan and Hubert Cecotti", "title": "SymNet: Symmetrical Filters in Convolutional Neural Networks", "comments": "Copyright 2019 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetry is present in nature and science. In image processing, kernels for\nspatial filtering possess some symmetry (e.g. Sobel operators, Gaussian,\nLaplacian). Convolutional layers in artificial feed-forward neural networks\nhave typically considered the kernel weights without any constraint. In this\npaper, we propose to investigate the impact of a symmetry constraint in\nconvolutional layers for image classification tasks, taking our inspiration\nfrom the processes involved in the primary visual cortex and common image\nprocessing techniques. The goal is to assess the extent to which it is possible\nto enforce symmetrical constraints on the filters throughout the training\nprocess of a convolutional neural network (CNN) by modifying the weight update\npreformed during the backpropagation algorithm and to evaluate the change in\nperformance. The main hypothesis of this paper is that the symmetrical\nconstraint reduces the number of free parameters in the network, and it is able\nto achieve near identical performance to the modern methodology of training. In\nparticular, we address the following cases: x/y-axis symmetry, point\nreflection, and anti-point reflection. The performance has been evaluated on\nfour databases of images. The results support the conclusion that while random\nweights offer more freedom to the model, the symmetry constraint provides a\nsimilar level of performance while decreasing substantially the number of free\nparameters in the model. Such an approach can be valuable in phase-sensitive\napplications that require a linear phase property throughout the feature\nextraction process.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 19:55:03 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Dzhezyan", "Gregory", ""], ["Cecotti", "Hubert", ""]]}, {"id": "1906.04258", "submitter": "Mehdi Safarpour", "authors": "Mehdi Safarpour, Ilkka Hautala, Miguel Bordallo Lopez, Olli Silven", "title": "Transport Triggered Array Processor for Vision Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-level sensory data processing in many Internet-of-Things (IoT) devices\npursue energy efficiency by utilizing sleep modes or slowing the clocking to\nthe minimum. To curb the share of stand-by power dissipation in those designs,\nnear-threshold/sub-threshold operational points or ultra-low-leakage processes\nin fabrication are employed. Those limit the clocking rates significantly,\nreducing the computing throughputs of individual processing cores. In this\ncontribution we explore compensating for the performance loss of operating in\nnear-threshold region (Vdd =0.6V) through massive parallelization. Benefits of\nnear-threshold operation and massive parallelism are optimum energy consumption\nper instruction operation and minimized memory roundtrips, respectively. The\nProcessing Elements (PE) of the design are based on Transport Triggered\nArchitecture. The fine grained programmable parallel solution allows for fast\nand efficient computation of learnable low-level features (e.g. local binary\ndescriptors and convolutions). Other operations, including Max-pooling have\nalso been implemented. The programmable design achieves excellent energy\nefficiency for Local Binary Patterns computations.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 20:07:16 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Safarpour", "Mehdi", ""], ["Hautala", "Ilkka", ""], ["Lopez", "Miguel Bordallo", ""], ["Silven", "Olli", ""]]}, {"id": "1906.04306", "submitter": "Dong Nie", "authors": "Dong Nie and Dinggang Shen", "title": "Semantic-guided Encoder Feature Learning for Blurry Boundary Delineation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encoder-decoder architectures are widely adopted for medical image\nsegmentation tasks. With the lateral skip connection, the models can obtain and\nfuse both semantic and resolution information in deep layers to achieve more\naccurate segmentation performance. However, in many applications (e.g., blurry\nboundary images), these models often cannot precisely locate complex boundaries\nand segment tiny isolated parts. To solve this challenging problem, we firstly\nanalyze why simple skip connections are not enough to help accurately locate\nindistinct boundaries and argue that it is due to the fuzzy information in the\nskip connection provided in the encoder layers. Then we propose a\nsemantic-guided encoder feature learning strategy to learn both high resolution\nand rich semantic encoder features so that we can more accurately locate the\nblurry boundaries, which can also enhance the network by selectively learning\ndiscriminative features. Besides, we further propose a soft contour constraint\nmechanism to model the blurry boundary detection. Experimental results on real\nclinical datasets show that our proposed method can achieve state-of-the-art\nsegmentation accuracy, especially for the blurry regions. Further analysis also\nindicates that our proposed network components indeed contribute to the\nimprovement of performance. Experiments on additional datasets validate the\ngeneralization ability of our proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 22:31:08 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Nie", "Dong", ""], ["Shen", "Dinggang", ""]]}, {"id": "1906.04312", "submitter": "Soren Pirk", "authors": "S\\\"oren Pirk, Mohi Khansari, Yunfei Bai, Corey Lynch, Pierre Sermanet", "title": "Online Object Representations with Contrastive Learning", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a self-supervised approach for learning representations of objects\nfrom monocular videos and demonstrate it is particularly useful in situated\nsettings such as robotics. The main contributions of this paper are: 1) a\nself-supervising objective trained with contrastive learning that can discover\nand disentangle object attributes from video without using any labels; 2) we\nleverage object self-supervision for online adaptation: the longer our online\nmodel looks at objects in a video, the lower the object identification error,\nwhile the offline baseline remains with a large fixed error; 3) to explore the\npossibilities of a system entirely free of human supervision, we let a robot\ncollect its own data, train on this data with our self-supervise scheme, and\nthen show the robot can point to objects similar to the one presented in front\nof it, demonstrating generalization of object attributes. An interesting and\nperhaps surprising finding of this approach is that given a limited set of\nobjects, object correspondences will naturally emerge when using contrastive\nlearning without requiring explicit positive pairs. Videos illustrating online\nobject adaptation and robotic pointing are available at:\nhttps://online-objects.github.io/.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 22:43:20 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Pirk", "S\u00f6ren", ""], ["Khansari", "Mohi", ""], ["Bai", "Yunfei", ""], ["Lynch", "Corey", ""], ["Sermanet", "Pierre", ""]]}, {"id": "1906.04324", "submitter": "Chandrasekaran Anirudh Bhardwaj", "authors": "Chandrasekaran Anirudh Bhardwaj", "title": "Adaptively Preconditioned Stochastic Gradient Langevin Dynamics", "comments": "International Conference on Machine Learning (ICML) 2019 Workshop on\n  Understanding and Improving Generalization in Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Langevin Dynamics infuses isotropic gradient noise to SGD\nto help navigate pathological curvature in the loss landscape for deep\nnetworks. Isotropic nature of the noise leads to poor scaling, and adaptive\nmethods based on higher order curvature information such as Fisher Scoring have\nbeen proposed to precondition the noise in order to achieve better convergence.\nIn this paper, we describe an adaptive method to estimate the parameters of the\nnoise and conduct experiments on well-known model architectures to show that\nthe adaptively preconditioned SGLD method achieves convergence with the speed\nof adaptive first order methods such as Adam, AdaGrad etc. and achieves\ngeneralization equivalent of SGD in the test set.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 23:38:54 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 11:50:58 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Bhardwaj", "Chandrasekaran Anirudh", ""]]}, {"id": "1906.04333", "submitter": "Omar Al-Kadi", "authors": "Omar S. Al-Kadi", "title": "Multiscale Nakagami parametric imaging for improved liver tumor\n  localization", "comments": "IEEE International Conference on Image Processing (ICIP), USA, pp.\n  3384-3388, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.QM q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective ultrasound tissue characterization is usually hindered by complex\ntissue structures. The interlacing of speckle patterns complicates the correct\nestimation of backscatter distribution parameters. Nakagami parametric imaging\nbased on localized shape parameter mapping can model different backscattering\nconditions. However, performance of the constructed Nakagami image depends on\nthe sensitivity of the estimation method to the backscattered statistics and\nscale of analysis. Using a fixed focal region of interest in estimating the\nNakagami parametric image would increase estimation variance. In this work,\nlocalized Nakagami parameters are estimated adaptively by means of maximum\nlikelihood estimation on a multiscale basis. The varying size kernel integrates\nthe goodness-of-fit of the backscattering distribution parameters at multiple\nscales for more stable parameter estimation. Results show improved quantitative\nvisualization of changes in tissue specular reflections, suggesting a potential\napproach for improving tumor localization in low contrast ultrasound images.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 00:48:45 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Al-Kadi", "Omar S.", ""]]}, {"id": "1906.04334", "submitter": "Jing Zhang", "authors": "Jing Zhang and Dacheng Tao", "title": "FAMED-Net: A Fast and Accurate Multi-scale End-to-end Dehazing Network", "comments": "13 pages, 9 figures, To appear in IEEE Transactions on Image\n  Processing. The code is available at https://github.com/chaimi2013/FAMED-Net", "journal-ref": null, "doi": "10.1109/TIP.2019.2922837", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image dehazing is a critical image pre-processing step for subsequent\nhigh-level computer vision tasks. However, it remains challenging due to its\nill-posed nature. Existing dehazing models tend to suffer from model\novercomplexity and computational inefficiency or have limited representation\ncapacity. To tackle these challenges, here we propose a fast and accurate\nmulti-scale end-to-end dehazing network called FAMED-Net, which comprises\nencoders at three scales and a fusion module to efficiently and directly learn\nthe haze-free image. Each encoder consists of cascaded and densely connected\npoint-wise convolutional layers and pooling layers. Since no larger\nconvolutional kernels are used and features are reused layer-by-layer,\nFAMED-Net is lightweight and computationally efficient. Thorough empirical\nstudies on public synthetic datasets (including RESIDE) and real-world hazy\nimages demonstrate the superiority of FAMED-Net over other representative\nstate-of-the-art models with respect to model complexity, computational\nefficiency, restoration accuracy, and cross-set generalization. The code will\nbe made publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 00:51:40 GMT"}, {"version": "v2", "created": "Sun, 7 Jul 2019 06:22:31 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Zhang", "Jing", ""], ["Tao", "Dacheng", ""]]}, {"id": "1906.04338", "submitter": "Jayaraman J. Thiagarajan", "authors": "Kowshik Thopalli, Jayaraman J. Thiagarajan, Rushil Anirudh, Pavan\n  Turaga", "title": "SALT: Subspace Alignment as an Auxiliary Learning Task for Domain\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation aims to transfer and adapt knowledge learned\nfrom a labeled source domain to an unlabeled target domain. Key components of\nunsupervised domain adaptation include: (a) maximizing performance on the\ntarget, and (b) aligning the source and target domains. Traditionally, these\ntasks have either been considered as separate, or assumed to be implicitly\naddressed together with high-capacity feature extractors. When considered\nseparately, alignment is usually viewed as a problem of aligning data\ndistributions, either through geometric approaches such as subspace alignment\nor through distributional alignment such as optimal transport. This paper\nrepresents a hybrid approach, where we assume simplified data geometry in the\nform of subspaces, and consider alignment as an auxiliary task to the primary\ntask of maximizing performance on the source. The alignment is made rather\nsimple by leveraging tractable data geometry in the form of subspaces. We\nsynergistically allow certain parameters derived from the closed-form auxiliary\nsolution, to be affected by gradients from the primary task. The proposed\napproach represents a unique fusion of geometric and model-based alignment with\ngradients from a data-driven primary task. Our approach termed SALT, is a\nsimple framework that achieves comparable or sometimes outperforms\nstate-of-the-art on multiple standard benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 01:20:12 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 02:10:38 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Thopalli", "Kowshik", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Anirudh", "Rushil", ""], ["Turaga", "Pavan", ""]]}, {"id": "1906.04363", "submitter": "Junyi Bian", "authors": "Junyi Bian and Baojun Lin and Ke Zhang", "title": "Hybrid Function Sparse Representation towards Image Super Resolution", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representation with training-based dictionary has been shown\nsuccessful on super resolution(SR) but still have some limitations. Based on\nthe idea of making the magnification of function curve without losing its\nfidelity, we proposed a function based dictionary on sparse representation for\nsuper resolution, called hybrid function sparse representation (HFSR). The\ndictionary we designed is directly generated by preset hybrid functions without\nadditional training, which can be scaled to any size as is required due to its\nscalable property. We mixed approximated Heaviside function (AHF), sine\nfunction and DCT function as the dictionary. Multi-scale refinement is then\nproposed to utilize the scalable property of the dictionary to improve the\nresults. In addition, a reconstruct strategy is adopted to deal with the\noverlaps. The experiments on Set14 SR dataset show that our method has an\nexcellent performance particularly with regards to images containing rich\ndetails and contexts compared with non-learning based state-of-the art methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 03:04:45 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Bian", "Junyi", ""], ["Lin", "Baojun", ""], ["Zhang", "Ke", ""]]}, {"id": "1906.04375", "submitter": "Yuxin Peng", "authors": "Junchao Zhang and Yuxin Peng", "title": "Object-aware Aggregation with Bidirectional Temporal Graph for Video\n  Captioning", "comments": "10 pages, accepted by IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video captioning aims to automatically generate natural language descriptions\nof video content, which has drawn a lot of attention recent years. Generating\naccurate and fine-grained captions needs to not only understand the global\ncontent of video, but also capture the detailed object information. Meanwhile,\nvideo representations have great impact on the quality of generated captions.\nThus, it is important for video captioning to capture salient objects with\ntheir detailed temporal dynamics, and represent them using discriminative\nspatio-temporal representations. In this paper, we propose a new video\ncaptioning approach based on object-aware aggregation with bidirectional\ntemporal graph (OA-BTG), which captures detailed temporal dynamics for salient\nobjects in video, and learns discriminative spatio-temporal representations by\nperforming object-aware local feature aggregation on detected object regions.\nThe main novelties and advantages are: (1) Bidirectional temporal graph: A\nbidirectional temporal graph is constructed along and reversely along the\ntemporal order, which provides complementary ways to capture the temporal\ntrajectories for each salient object. (2) Object-aware aggregation: Learnable\nVLAD (Vector of Locally Aggregated Descriptors) models are constructed on\nobject temporal trajectories and global frame sequence, which performs\nobject-aware aggregation to learn discriminative representations. A\nhierarchical attention mechanism is also developed to distinguish different\ncontributions of multiple objects. Experiments on two widely-used datasets\ndemonstrate our OA-BTG achieves state-of-the-art performance in terms of\nBLEU@4, METEOR and CIDEr metrics.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 03:35:25 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Zhang", "Junchao", ""], ["Peng", "Yuxin", ""]]}, {"id": "1906.04376", "submitter": "Xuewen Yang", "authors": "Xuewen Yang, Xin Wang", "title": "Recognizing License Plates in Real-Time", "comments": "License Plate Detection and Recognition, Computer Vision, Supervised\n  Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  License plate detection and recognition (LPDR) is of growing importance for\nenabling intelligent transportation and ensuring the security and safety of the\ncities. However, LPDR faces a big challenge in a practical environment. The\nlicense plates can have extremely diverse sizes, fonts and colors, and the\nplate images are usually of poor quality caused by skewed capturing angles,\nuneven lighting, occlusion, and blurring. In applications such as surveillance,\nit often requires fast processing. To enable real-time and accurate license\nplate recognition, in this work, we propose a set of techniques: 1) a contour\nreconstruction method along with edge-detection to quickly detect the candidate\nplates; 2) a simple zero-one-alternation scheme to effectively remove the fake\ntop and bottom borders around plates to facilitate more accurate segmentation\nof characters on plates; 3) a set of techniques to augment the training data,\nincorporate SIFT features into the CNN network, and exploit transfer learning\nto obtain the initial parameters for more effective training; and 4) a\ntwo-phase verification procedure to determine the correct plate at low cost, a\nstatistical filtering in the plate detection stage to quickly remove unwanted\ncandidates, and the accurate CR results after the CR process to perform further\nplate verification without additional processing. We implement a complete LPDR\nsystem based on our algorithms. The experimental results demonstrate that our\nsystem can accurately recognize license plate in real-time. Additionally, it\nworks robustly under various levels of illumination and noise, and in the\npresence of car movement. Compared to peer schemes, our system is not only\namong the most accurate ones but is also the fastest, and can be easily applied\nto other scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 03:45:49 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 15:44:44 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Yang", "Xuewen", ""], ["Wang", "Xin", ""]]}, {"id": "1906.04378", "submitter": "Naji Khosravan", "authors": "Naji Khosravan, Aliasghar Mortazi, Michael Wallace, Ulas Bagci", "title": "PAN: Projective Adversarial Network for Medical Image Segmentation", "comments": "Accepted for presentation in MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial learning has been proven to be effective for capturing long-range\nand high-level label consistencies in semantic segmentation. Unique to medical\nimaging, capturing 3D semantics in an effective yet computationally efficient\nway remains an open problem. In this study, we address this computational\nburden by proposing a novel projective adversarial network, called PAN, which\nincorporates high-level 3D information through 2D projections. Furthermore, we\nintroduce an attention module into our framework that helps for a selective\nintegration of global information directly from our segmentor to our\nadversarial network. For the clinical application we chose pancreas\nsegmentation from CT scans. Our proposed framework achieved state-of-the-art\nperformance without adding to the complexity of the segmentor.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 03:51:09 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Khosravan", "Naji", ""], ["Mortazi", "Aliasghar", ""], ["Wallace", "Michael", ""], ["Bagci", "Ulas", ""]]}, {"id": "1906.04379", "submitter": "Hongwei Dong", "authors": "Hongwei Dong and Lamei Zhang and Bin Zou", "title": "Band Attention Convolutional Networks For Hyperspectral Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Redundancy and noise exist in the bands of hyperspectral images (HSIs). Thus,\nit is a good property to be able to select suitable parts from hundreds of\ninput bands for HSIs classification methods. In this letter, a band attention\nmodule (BAM) is proposed to implement the deep learning based HSIs\nclassification with the capacity of band selection or weighting. The proposed\nBAM can be seen as a plug-and-play complementary component of the existing\nclassification networks which fully considers the adverse effects caused by the\nredundancy of the bands when using convolutional neural networks (CNNs) for\nHSIs classification. Unlike most of deep learning methods used in HSIs, the\nband attention module which is customized according to the characteristics of\nhyperspectral images is embedded in the ordinary CNNs for better performance.\nAt the same time, unlike classical band selection or weighting methods, the\nproposed method achieves the end-to-end training instead of the separated\nstages. Experiments are carried out on two HSI benchmark datasets. Compared to\nsome classical and advanced deep learning methods, numerical simulations under\ndifferent evaluation criteria show that the proposed method have good\nperformance. Last but not least, some advanced CNNs are combined with the\nproposed BAM for better performance.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 03:56:20 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Dong", "Hongwei", ""], ["Zhang", "Lamei", ""], ["Zou", "Bin", ""]]}, {"id": "1906.04392", "submitter": "Ziang Yan", "authors": "Ziang Yan, Yiwen Guo, Changshui Zhang", "title": "Subspace Attack: Exploiting Promising Subspaces for Query-Efficient\n  Black-box Attacks", "comments": "10 pages + 3 pages supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike the white-box counterparts that are widely studied and readily\naccessible, adversarial examples in black-box settings are generally more\nHerculean on account of the difficulty of estimating gradients. Many methods\nachieve the task by issuing numerous queries to target classification systems,\nwhich makes the whole procedure costly and suspicious to the systems. In this\npaper, we aim at reducing the query complexity of black-box attacks in this\ncategory. We propose to exploit gradients of a few reference models which\narguably span some promising search subspaces. Experimental results show that,\nin comparison with the state-of-the-arts, our method can gain up to 2x and 4x\nreductions in the requisite mean and medium numbers of queries with much lower\nfailure rates even if the reference models are trained on a small and\ninadequate dataset disjoint to the one for training the victim model. Code and\nmodels for reproducing our results will be made publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 04:55:18 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Yan", "Ziang", ""], ["Guo", "Yiwen", ""], ["Zhang", "Changshui", ""]]}, {"id": "1906.04402", "submitter": "Yale Song", "authors": "Yale Song, Mohammad Soleymani", "title": "Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval", "comments": "CVPR 2019. Includes supplementary material. Have updated results on\n  TGIF and MRW", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual-semantic embedding aims to find a shared latent space where related\nvisual and textual instances are close to each other. Most current methods\nlearn injective embedding functions that map an instance to a single point in\nthe shared space. Unfortunately, injective embedding cannot effectively handle\npolysemous instances with multiple possible meanings; at best, it would find an\naverage representation of different meanings. This hinders its use in\nreal-world scenarios where individual instances and their cross-modal\nassociations are often ambiguous. In this work, we introduce Polysemous\nInstance Embedding Networks (PIE-Nets) that compute multiple and diverse\nrepresentations of an instance by combining global context with locally-guided\nfeatures via multi-head self-attention and residual learning. To learn\nvisual-semantic embedding, we tie-up two PIE-Nets and optimize them jointly in\nthe multiple instance learning framework. Most existing work on cross-modal\nretrieval focuses on image-text data. Here, we also tackle a more challenging\ncase of video-text retrieval. To facilitate further research in video-text\nretrieval, we release a new dataset of 50K video-sentence pairs collected from\nsocial media, dubbed MRW (my reaction when). We demonstrate our approach on\nboth image-text and video-text retrieval scenarios using MS-COCO, TGIF, and our\nnew MRW dataset.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 05:48:32 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 04:49:18 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Song", "Yale", ""], ["Soleymani", "Mohammad", ""]]}, {"id": "1906.04407", "submitter": "Sheryl Brahnam", "authors": "Loris Nanni, Alessandra Lumini, Federica Pasquali, Sheryl Brahnam", "title": "iProStruct2D: Identifying protein structural classes by deep learning\n  via 2D representations", "comments": "9 pages, 3 figures, 4 tables", "journal-ref": "Expert Systems With Applications 2020, 142, (March), 113019", "doi": "10.1016/j.eswa.2019.113019", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of protein classification starting from\na multi-view 2D representation of proteins. From each 3D protein structure, a\nlarge set of 2D projections is generated using the protein visualization\nsoftware Jmol. This set of multi-view 2D representations includes 13 different\ntypes of protein visualizations that emphasize specific properties of protein\nstructure (e.g., a backbone visualization that displays the backbone structure\nof the protein as a trace of the C{\\alpha} atom). Each type of representation\nis used to train a different Convolutional Neural Network (CNN), and the fusion\nof these CNNs is shown to be able to exploit the diversity of different types\nof representations to improve classification performance. In addition, several\nmulti-view projections are obtained by uniformly rotating the protein structure\naround its central X, Y, and Z viewing axes to produce 125 images. This\napproach can be considered a data augmentation method for improving the\nperformance of the classifier and can be used in both the training and the\ntesting phases. Experimental evaluation of the proposed approach on two\ndatasets demonstrates the strength of the proposed method with respect to the\nother state-of-the-art approaches. The MATLAB code used in this paper is\navailable at https://github.com/LorisNanni.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 06:26:13 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Nanni", "Loris", ""], ["Lumini", "Alessandra", ""], ["Pasquali", "Federica", ""], ["Brahnam", "Sheryl", ""]]}, {"id": "1906.04409", "submitter": "Sowmya Munukutla", "authors": "Siddhant Jain, Sowmya Munukutla, David Held", "title": "Few-Shot Point Cloud Region Annotation with Human in the Loop", "comments": "presented at 2019 ICML Workshop on Human in the Loop Learning (HILL\n  2019), Long Beach, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a point cloud annotation framework that employs human-in-loop\nlearning to enable the creation of large point cloud datasets with per-point\nannotations. Sparse labels from a human annotator are iteratively propagated to\ngenerate a full segmentation of the network by fine-tuning a pre-trained model\nof an allied task via a few-shot learning paradigm. We show that the proposed\nframework significantly reduces the amount of human interaction needed in\nannotating point clouds, without sacrificing on the quality of the annotations.\nOur experiments also suggest the suitability of the framework in annotating\nlarge datasets by noting a reduction in human interaction as the number of full\nannotations completed by the system increases. Finally, we demonstrate the\nflexibility of the framework to support multiple different annotations of the\nsame point cloud enabling the creation of datasets with different granularities\nof annotation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 06:34:13 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Jain", "Siddhant", ""], ["Munukutla", "Sowmya", ""], ["Held", "David", ""]]}, {"id": "1906.04419", "submitter": "Majd Zreik", "authors": "Majd Zreik, Robbert W. van Hamersvelt, Nadieh Khalili, Jelmer M.\n  Wolterink, Michiel Voskuil, Max A. Viergever, Tim Leiner, Ivana I\\v{s}gum", "title": "Deep learning analysis of coronary arteries in cardiac CT angiography\n  for detection of patients requiring invasive coronary angiography", "comments": "This work has been accepted to IEEE TMI for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In patients with obstructive coronary artery disease, the functional\nsignificance of a coronary artery stenosis needs to be determined to guide\ntreatment. This is typically established through fractional flow reserve (FFR)\nmeasurement, performed during invasive coronary angiography (ICA). We present a\nmethod for automatic and non-invasive detection of patients requiring ICA,\nemploying deep unsupervised analysis of complete coronary arteries in cardiac\nCT angiography (CCTA) images. We retrospectively collected CCTA scans of 187\npatients, 137 of them underwent invasive FFR measurement in 192 different\ncoronary arteries. These FFR measurements served as a reference standard for\nthe functional significance of the coronary stenosis. The centerlines of the\ncoronary arteries were extracted and used to reconstruct straightened\nmulti-planar reformatted (MPR) volumes. To automatically identify arteries with\nfunctionally significant stenosis that require ICA, each MPR volume was encoded\ninto a fixed number of encodings using two disjoint 3D and 1D convolutional\nautoencoders performing spatial and sequential encodings, respectively.\nThereafter, these encodings were employed to classify arteries using a support\nvector machine classifier. The detection of coronary arteries requiring\ninvasive evaluation, evaluated using repeated cross-validation experiments,\nresulted in an area under the receiver operating characteristic curve of $0.81\n\\pm 0.02$ on the artery-level, and $0.87 \\pm 0.02$ on the patient-level. The\nresults demonstrate the feasibility of automatic non-invasive detection of\npatients that require ICA and possibly subsequent coronary artery intervention.\nThis could potentially reduce the number of patients that unnecessarily undergo\nICA.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 07:38:26 GMT"}, {"version": "v2", "created": "Sun, 10 Nov 2019 13:23:46 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Zreik", "Majd", ""], ["van Hamersvelt", "Robbert W.", ""], ["Khalili", "Nadieh", ""], ["Wolterink", "Jelmer M.", ""], ["Voskuil", "Michiel", ""], ["Viergever", "Max A.", ""], ["Leiner", "Tim", ""], ["I\u0161gum", "Ivana", ""]]}, {"id": "1906.04423", "submitter": "Peng Wang", "authors": "Ning Wang, Yang Gao, Hao Chen, Peng Wang, Zhi Tian, Chunhua Shen,\n  Yanning Zhang", "title": "NAS-FCOS: Fast Neural Architecture Search for Object Detection", "comments": "9 pages, 9 figures, accepted by CVPR-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep neural networks relies on significant architecture\nengineering. Recently neural architecture search (NAS) has emerged as a promise\nto greatly reduce manual effort in network design by automatically searching\nfor optimal architectures, although typically such algorithms need an excessive\namount of computational resources, e.g., a few thousand GPU-days. To date, on\nchallenging vision tasks such as object detection, NAS, especially fast\nversions of NAS, is less studied. Here we propose to search for the decoder\nstructure of object detectors with search efficiency being taken into\nconsideration. To be more specific, we aim to efficiently search for the\nfeature pyramid network (FPN) as well as the prediction head of a simple\nanchor-free object detector, namely FCOS, using a tailored reinforcement\nlearning paradigm. With carefully designed search space, search algorithms and\nstrategies for evaluating network quality, we are able to efficiently search a\ntop-performing detection architecture within 4 days using 8 V100 GPUs. The\ndiscovered architecture surpasses state-of-the-art object detection models\n(such as Faster R-CNN, RetinaNet and FCOS) by 1.5 to 3.5 points in AP on the\nCOCO dataset, with comparable computation complexity and memory footprint,\ndemonstrating the efficacy of the proposed NAS for object detection.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 07:55:41 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 07:44:55 GMT"}, {"version": "v3", "created": "Sat, 7 Sep 2019 03:53:59 GMT"}, {"version": "v4", "created": "Tue, 25 Feb 2020 08:05:47 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Wang", "Ning", ""], ["Gao", "Yang", ""], ["Chen", "Hao", ""], ["Wang", "Peng", ""], ["Tian", "Zhi", ""], ["Shen", "Chunhua", ""], ["Zhang", "Yanning", ""]]}, {"id": "1906.04431", "submitter": "Christian S. Pilz", "authors": "Christian S. Pilz, Vladimir Blazek, Steffen Leonhardt", "title": "On the Vector Space in Photoplethysmography Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the vector space of visible wavelength intensities from face videos\nwidely used as input features in Photoplethysmography Imaging (PPGI). Based\nupon theoretical principles of Group invariance in the Euclidean space we\nderive a change of the topology where the corresponding distance between\nsuccessive measurements is defined as geodesic on a Riemannian manifold. This\nlower dimensional embedding of the sensor signal unifies the invariance\nproperties with respect to translation of the features as discussed by several\nformer approaches. The resulting operator acts implicit on the feature space\nwithout requiring any kind of prior knowledge and does not need parameter\ntuning. The resulting feature's time varying quasi-periodic shaping naturally\noccurs in form of the canonical state space representation according to the\nknown Diffusion process of blood volume changes. The computational complexity\nis low and the implementation becomes fairly simple. During experiments the\noperator achieved robust and competitive estimation performance of heart rate\nfrom face videos on two public databases.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 08:08:10 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Pilz", "Christian S.", ""], ["Blazek", "Vladimir", ""], ["Leonhardt", "Steffen", ""]]}, {"id": "1906.04441", "submitter": "Sergio Vitale", "authors": "Giampaolo Ferraioli, Vito Pascazio and Sergio Vitale", "title": "A Novel Cost Function for Despeckling using Convolutional Neural\n  Networks", "comments": "Accepted on JURSE 2019 - Joint Urban Remote Sensing Event", "journal-ref": "2019 Joint Urban Remote Sensing Event (JURSE), Vannes, France,\n  2019, pp. 1-4", "doi": "10.1109/JURSE.2019.8809042", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Removing speckle noise from SAR images is still an open issue. It is well\nknow that the interpretation of SAR images is very challenging and despeckling\nalgorithms are necessary to improve the ability of extracting information. An\nurban environment makes this task more heavy due to different structures and to\ndifferent objects scale. Following the recent spread of deep learning methods\nrelated to several remote sensing applications, in this work a convolutional\nneural networks based algorithm for despeckling is proposed. The network is\ntrained on simulated SAR data. The paper is mainly focused on the\nimplementation of a cost function that takes account of both spatial\nconsistency of image and statistical properties of noise.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 08:42:58 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Ferraioli", "Giampaolo", ""], ["Pascazio", "Vito", ""], ["Vitale", "Sergio", ""]]}, {"id": "1906.04442", "submitter": "Yuanchao Bai", "authors": "Yuanchao Bai, Huizhu Jia, Ming Jiang, Xianming Liu, Xiaodong Xie, Wen\n  Gao", "title": "Single Image Blind Deblurring Using Multi-Scale Latent Structure Prior", "comments": "To appear in IEEE Transactions on Circuits and Systems for Video\n  Technology, 2019; Image downsampling makes a good prior for fast blind image\n  deblurring", "journal-ref": null, "doi": "10.1109/TCSVT.2019.2919159", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind image deblurring is a challenging problem in computer vision, which\naims to restore both the blur kernel and the latent sharp image from only a\nblurry observation. Inspired by the prevalent self-example prior in image\nsuper-resolution, in this paper, we observe that a coarse enough image\ndown-sampled from a blurry observation is approximately a low-resolution\nversion of the latent sharp image. We prove this phenomenon theoretically and\ndefine the coarse enough image as a latent structure prior of the unknown sharp\nimage. Starting from this prior, we propose to restore sharp images from the\ncoarsest scale to the finest scale on a blurry image pyramid, and progressively\nupdate the prior image using the newly restored sharp image. These\ncoarse-to-fine priors are referred to as \\textit{Multi-Scale Latent Structures}\n(MSLS). Leveraging the MSLS prior, our algorithm comprises two phases: 1) we\nfirst preliminarily restore sharp images in the coarse scales; 2) we then apply\na refinement process in the finest scale to obtain the final deblurred image.\nIn each scale, to achieve lower computational complexity, we alternately\nperform a sharp image reconstruction with fast local self-example matching, an\naccelerated kernel estimation with error compensation, and a fast non-blind\nimage deblurring, instead of computing any computationally expensive non-convex\npriors. We further extend the proposed algorithm to solve more challenging\nnon-uniform blind image deblurring problem. Extensive experiments demonstrate\nthat our algorithm achieves competitive results against the state-of-the-art\nmethods with much faster running speed.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 08:43:47 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Bai", "Yuanchao", ""], ["Jia", "Huizhu", ""], ["Jiang", "Ming", ""], ["Liu", "Xianming", ""], ["Xie", "Xiaodong", ""], ["Gao", "Wen", ""]]}, {"id": "1906.04445", "submitter": "Firas Laakom", "authors": "Firas Laakom, Nikolaos Passalis, Jenni Raitoharju, Jarno Nikkanen,\n  Anastasios Tefas, Alexandros Iosifidis, Moncef Gabbouj", "title": "Bag of Color Features For Color Constancy", "comments": "12 pages, 5 figures, 6 tables", "journal-ref": "IEEE Transactions on Image Processing ( Volume: 29 ) 2020", "doi": "10.1109/TIP.2020.3004921", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel color constancy approach, called Bag of\nColor Features (BoCF), building upon Bag-of-Features pooling. The proposed\nmethod substantially reduces the number of parameters needed for illumination\nestimation. At the same time, the proposed method is consistent with the color\nconstancy assumption stating that global spatial information is not relevant\nfor illumination estimation and local information ( edges, etc.) is sufficient.\nFurthermore, BoCF is consistent with color constancy statistical approaches and\ncan be interpreted as a learning-based generalization of many statistical\napproaches. To further improve the illumination estimation accuracy, we propose\na novel attention mechanism for the BoCF model with two variants based on\nself-attention. BoCF approach and its variants achieve competitive, compared to\nthe state of the art, results while requiring much fewer parameters on three\nbenchmark datasets: ColorChecker RECommended, INTEL-TUT version 2, and NUS8.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 08:47:49 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Laakom", "Firas", ""], ["Passalis", "Nikolaos", ""], ["Raitoharju", "Jenni", ""], ["Nikkanen", "Jarno", ""], ["Tefas", "Anastasios", ""], ["Iosifidis", "Alexandros", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "1906.04463", "submitter": "Mostafa El-Khamy", "authors": "Mostafa El-Khamy, Haoyu Ren, Xianzhi Du, and Jungwon Lee", "title": "TW-SMNet: Deep Multitask Learning of Tele-Wide Stereo Matching", "comments": null, "journal-ref": "Multitask Deep Neural Networks for Tele-Wide Stereo Matching, IEEE\n  Access, 2020", "doi": "10.1109/ACCESS.2020.3029085", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the problem of estimating the real world depth of\nelements in a scene captured by two cameras with different field of views,\nwhere the first field of view (FOV) is a Wide FOV (WFOV) captured by a wide\nangle lens, and the second FOV is contained in the first FOV and is captured by\na tele zoom lens. We refer to the problem of estimating the inverse depth for\nthe union of FOVs, while leveraging the stereo information in the overlapping\nFOV, as Tele-Wide Stereo Matching (TW-SM). We propose different deep learning\nsolutions to the TW-SM problem. Since the disparity is proportional to the\ninverse depth, we train stereo matching disparity estimation (SMDE) networks to\nestimate the disparity for the union WFOV. We further propose an end-to-end\ndeep multitask tele-wide stereo matching neural network (MT-TW-SMNet), which\nsimultaneously learns the SMDE task for the overlapped Tele FOV and the single\nimage inverse depth estimation (SIDE) task for the WFOV. Moreover, we design\nmultiple methods for the fusion of the SMDE and SIDE networks. We evaluate the\nperformance of TW-SM on the popular KITTI and SceneFlow stereo datasets, and\ndemonstrate its practicality by synthesizing the Bokeh effect on the WFOV from\na tele-wide stereo image pair.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 09:46:26 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["El-Khamy", "Mostafa", ""], ["Ren", "Haoyu", ""], ["Du", "Xianzhi", ""], ["Lee", "Jungwon", ""]]}, {"id": "1906.04464", "submitter": "Sibei Yang", "authors": "Sibei Yang, Guanbin Li, Yizhou Yu", "title": "Relationship-Embedded Representation Learning for Grounding Referring\n  Expressions", "comments": "This paper is going to appear in TPAMI. Code is available at\n  https://github.com/sibeiyang/sgmn/tree/master/lib/cmrin_models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grounding referring expressions in images aims to locate the object instance\nin an image described by a referring expression. It involves a joint\nunderstanding of natural language and image content, and is essential for a\nrange of visual tasks related to human-computer interaction. As a\nlanguage-to-vision matching task, the core of this problem is to not only\nextract all the necessary information (i.e., objects and the relationships\namong them) in both the image and referring expression, but also make full use\nof context information to align cross-modal semantic concepts in the extracted\ninformation. Unfortunately, existing work on grounding referring expressions\nfails to accurately extract multi-order relationships from the referring\nexpression and associate them with the objects and their related contexts in\nthe image. In this paper, we propose a Cross-Modal Relationship Extractor\n(CMRE) to adaptively highlight objects and relationships (spatial and semantic\nrelations) related to the given expression with a cross-modal attention\nmechanism, and represent the extracted information as a language-guided visual\nrelation graph. In addition, we propose a Gated Graph Convolutional Network\n(GGCN) to compute multimodal semantic contexts by fusing information from\ndifferent modes and propagating multimodal information in the structured\nrelation graph. Experimental results on three common benchmark datasets show\nthat our Cross-Modal Relationship Inference Network, which consists of CMRE and\nGGCN, significantly surpasses all existing state-of-the-art methods. Code is\navailable at https://github.com/sibeiyang/sgmn/tree/master/lib/cmrin_models\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 09:47:26 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 03:45:41 GMT"}, {"version": "v3", "created": "Sun, 19 Apr 2020 11:04:35 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Yang", "Sibei", ""], ["Li", "Guanbin", ""], ["Yu", "Yizhou", ""]]}, {"id": "1906.04505", "submitter": "Tinghuai Wang", "authors": "Tinghuai Wang, Lixin Fan, Huiling Wang", "title": "Simultaneously Learning Architectures and Features of Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method which simultaneously learns the number of\nfilters and network features repeatedly over multiple epochs. We propose a\nnovel pruning loss to explicitly enforces the optimizer to focus on promising\ncandidate filters while suppressing contributions of less relevant ones. In the\nmeanwhile, we further propose to enforce the diversities between filters and\nthis diversity-based regularization term improves the trade-off between model\nsizes and accuracies. It turns out the interplay between architecture and\nfeature optimizations improves the final compressed models, and the proposed\nmethod is compared favorably to existing methods, in terms of both models sizes\nand accuracies for a wide range of applications including image classification,\nimage compression and audio classification.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 11:49:10 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Wang", "Tinghuai", ""], ["Fan", "Lixin", ""], ["Wang", "Huiling", ""]]}, {"id": "1906.04509", "submitter": "Muhammad Tayyab", "authors": "Muhammad Tayyab and Abhijit Mahalanobis", "title": "BasisConv: A method for compressed representation and learning in CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that Convolutional Neural Networks (CNNs) have significant\nredundancy in their filter weights. Various methods have been proposed in the\nliterature to compress trained CNNs. These include techniques like pruning\nweights, filter quantization and representing filters in terms of a basis\nfunctions. Our approach falls in this latter class of strategies, but is\ndistinct in that that we show both compressed learning and representation can\nbe achieved without significant modifications of popular CNN architectures.\nSpecifically, any convolution layer of the CNN is easily replaced by two\nsuccessive convolution layers: the first is a set of fixed filters (that\nrepresent the knowledge space of the entire layer and do not change), which is\nfollowed by a layer of one-dimensional filters (that represent the learned\nknowledge in this space). For the pre-trained networks, the fixed layer is just\nthe truncated eigen-decompositions of the original filters. The 1D filters are\ninitialized as the weights of linear combination, but are fine-tuned to recover\nany performance loss due to the truncation. For training networks from scratch,\nwe use a set of random orthogonal fixed filters (that never change), and learn\nthe 1D weight vector directly from the labeled data. Our method substantially\nreduces i) the number of learnable parameters during training, and ii) the\nnumber of multiplication operations and filter storage requirements during\nimplementation. It does so without requiring any special operators in the\nconvolution layer, and extends to all known popular CNN architectures. We apply\nour method to four well known network architectures trained with three\ndifferent data sets. Results show a consistent reduction in i) the number of\noperations by up to a factor of 5, and ii) number of learnable parameters by up\nto a factor of 18, with less than 3% drop in performance on the CIFAR100\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 12:07:48 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Tayyab", "Muhammad", ""], ["Mahalanobis", "Abhijit", ""]]}, {"id": "1906.04547", "submitter": "Alex Hern\\'andez Garc\\'ia", "authors": "Alex Hern\\'andez-Garc\\'ia, Peter K\\\"onig, Tim C. Kietzmann", "title": "Learning robust visual representations using data augmentation\n  invariance", "comments": "6 pages, 2 figures, work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks trained for image object categorization\nhave shown remarkable similarities with representations found across the\nprimate ventral visual stream. Yet, artificial and biological networks still\nexhibit important differences. Here we investigate one such property:\nincreasing invariance to identity-preserving image transformations found along\nthe ventral stream. Despite theoretical evidence that invariance should emerge\nnaturally from the optimization process, we present empirical evidence that the\nactivations of convolutional neural networks trained for object categorization\nare not robust to identity-preserving image transformations commonly used in\ndata augmentation. As a solution, we propose data augmentation invariance, an\nunsupervised learning objective which improves the robustness of the learned\nrepresentations by promoting the similarity between the activations of\naugmented image samples. Our results show that this approach is a simple, yet\neffective and efficient (10 % increase in training time) way of increasing the\ninvariance of the models while obtaining similar categorization performance.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 13:03:19 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Hern\u00e1ndez-Garc\u00eda", "Alex", ""], ["K\u00f6nig", "Peter", ""], ["Kietzmann", "Tim C.", ""]]}, {"id": "1906.04550", "submitter": "Siavash Ghiasvand", "authors": "Siavash Ghiasvand and Florina M. Ciorba", "title": "Anomaly Detection in High Performance Computers: A Vicinity Perspective", "comments": "9 pages, Submitted to the 18th IEEE International Symposium on\n  Parallel and Distributed Computing", "journal-ref": null, "doi": "10.1109/ispdc.2019.00024", "report-no": null, "categories": "cs.DC cs.CV cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In response to the demand for higher computational power, the number of\ncomputing nodes in high performance computers (HPC) increases rapidly. Exascale\nHPC systems are expected to arrive by 2020. With drastic increase in the number\nof HPC system components, it is expected to observe a sudden increase in the\nnumber of failures which, consequently, poses a threat to the continuous\noperation of the HPC systems. Detecting failures as early as possible and,\nideally, predicting them, is a necessary step to avoid interruptions in HPC\nsystems operation. Anomaly detection is a well-known general purpose approach\nfor failure detection, in computing systems. The majority of existing methods\nare designed for specific architectures, require adjustments on the computing\nsystems hardware and software, need excessive information, or pose a threat to\nusers' and systems' privacy. This work proposes a node failure detection\nmechanism based on a vicinity-based statistical anomaly detection approach\nusing passively collected and anonymized system log entries. Application of the\nproposed approach on system logs collected over 8 months indicates an anomaly\ndetection precision between 62% to 81%.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 13:06:02 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Ghiasvand", "Siavash", ""], ["Ciorba", "Florina M.", ""]]}, {"id": "1906.04567", "submitter": "Laura Leal-Taix\\'e", "authors": "Patrick Dendorfer and Hamid Rezatofighi and Anton Milan and Javen Shi\n  and Daniel Cremers and Ian Reid and Stefan Roth and Konrad Schindler and\n  Laura Leal-Taixe", "title": "CVPR19 Tracking and Detection Challenge: How crowded can it get?", "comments": "arXiv admin note: substantial text overlap with arXiv:1603.00831,\n  arXiv:1504.01942", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standardized benchmarks are crucial for the majority of computer vision\napplications. Although leaderboards and ranking tables should not be\nover-claimed, benchmarks often provide the most objective measure of\nperformance and are therefore important guides for research.\n  The benchmark for Multiple Object Tracking, MOTChallenge, was launched with\nthe goal to establish a standardized evaluation of multiple object tracking\nmethods. The challenge focuses on multiple people tracking, since pedestrians\nare well studied in the tracking community, and precise tracking and detection\nhas high practical relevance. Since the first release, MOT15, MOT16 and MOT17\nhave tremendously contributed to the community by introducing a clean dataset\nand precise framework to benchmark multi-object trackers. In this paper, we\npresent our CVPR19 benchmark, consisting of 8 new sequences depicting very\ncrowded challenging scenes. The benchmark will be presented at the 4th BMTT MOT\nChallenge Workshop at the Computer Vision and Pattern Recognition Conference\n(CVPR) 2019, and will evaluate the state-of-the-art in multiple object tracking\nwhend handling extremely crowded scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 12:35:01 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Dendorfer", "Patrick", ""], ["Rezatofighi", "Hamid", ""], ["Milan", "Anton", ""], ["Shi", "Javen", ""], ["Cremers", "Daniel", ""], ["Reid", "Ian", ""], ["Roth", "Stefan", ""], ["Schindler", "Konrad", ""], ["Leal-Taixe", "Laura", ""]]}, {"id": "1906.04569", "submitter": "Hien Nguyen", "authors": "Aryan Mobiny, Hien V. Nguyen, Supratik Moulik, Naveen Garg, Carol C.\n  Wu", "title": "DropConnect Is Effective in Modeling Uncertainty of Bayesian Deep\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have achieved state-of-the-art performances in\nmany important domains, including medical diagnosis, security, and autonomous\ndriving. In these domains where safety is highly critical, an erroneous\ndecision can result in serious consequences. While a perfect prediction\naccuracy is not always achievable, recent work on Bayesian deep networks shows\nthat it is possible to know when DNNs are more likely to make mistakes. Knowing\nwhat DNNs do not know is desirable to increase the safety of deep learning\ntechnology in sensitive applications. Bayesian neural networks attempt to\naddress this challenge. However, traditional approaches are computationally\nintractable and do not scale well to large, complex neural network\narchitectures. In this paper, we develop a theoretical framework to approximate\nBayesian inference for DNNs by imposing a Bernoulli distribution on the model\nweights. This method, called MC-DropConnect, gives us a tool to represent the\nmodel uncertainty with little change in the overall model structure or\ncomputational cost. We extensively validate the proposed algorithm on multiple\nnetwork architectures and datasets for classification and semantic segmentation\ntasks. We also propose new metrics to quantify the uncertainty estimates. This\nenables an objective comparison between MC-DropConnect and prior approaches.\nOur empirical results demonstrate that the proposed framework yields\nsignificant improvement in both prediction accuracy and uncertainty estimation\nquality compared to the state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 20:51:52 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Mobiny", "Aryan", ""], ["Nguyen", "Hien V.", ""], ["Moulik", "Supratik", ""], ["Garg", "Naveen", ""], ["Wu", "Carol C.", ""]]}, {"id": "1906.04574", "submitter": "Santosh Vipparthi Kumar", "authors": "Kuldeep Marotirao Biradar, Ayushi Gupta, Murari Mandal, Santosh Kumar\n  Vipparthi", "title": "Challenges in Time-Stamp Aware Anomaly Detection in Traffic Videos", "comments": "IEEE Computer Vision and Pattern Recognition Workshops (CVPRW-2019)", "journal-ref": "IEEE Computer Vision and Pattern Recognition Workshops\n  (CVPRW-2019)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-stamp aware anomaly detection in traffic videos is an essential task for\nthe advancement of the intelligent transportation system. Anomaly detection in\nvideos is a challenging problem due to sparse occurrence of anomalous events,\ninconsistent behavior of a different type of anomalies and imbalanced available\ndata for normal and abnormal scenarios. In this paper, we present a three-stage\npipeline to learn the motion patterns in videos to detect a visual anomaly.\nFirst, the background is estimated from recent history frames to identify the\nmotionless objects. This background image is used to localize the\nnormal/abnormal behavior within the frame. Further, we detect an object of\ninterest in the estimated background and categorize it into anomaly based on a\ntime-stamp aware anomaly detection algorithm. We also discuss the challenges\nfaced in improving performance over the unseen test data for traffic anomaly\ndetection. Experiments are conducted over Track 3 of NVIDIA AI city challenge\n2019. The results show the effectiveness of the proposed method in detecting\ntime-stamp aware anomalies in traffic/road videos.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 13:23:04 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Biradar", "Kuldeep Marotirao", ""], ["Gupta", "Ayushi", ""], ["Mandal", "Murari", ""], ["Vipparthi", "Santosh Kumar", ""]]}, {"id": "1906.04598", "submitter": "Zhao Zhang", "authors": "Zhao Zhang, Jiahuan Ren, Weiming Jiang, Zheng Zhang, Richang Hong,\n  Shuicheng Yan and Meng Wang", "title": "Joint Subspace Recovery and Enhanced Locality Driven Robust Flexible\n  Discriminative Dictionary Learning", "comments": "Accepted by IEEE TCSVT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a joint subspace recovery and enhanced locality based robust\nflexible label consistent dictionary learning method called Robust Flexible\nDiscriminative Dictionary Learning (RFDDL). RFDDL mainly improves the data\nrepresentation and classification abilities by enhancing the robust property to\nsparse errors and encoding the locality, reconstruction error and label\nconsistency more accurately. First, for the robustness to noise and sparse\nerrors in data and atoms, RFDDL aims at recovering the underlying clean data\nand clean atom subspaces jointly, and then performs DL and encodes the locality\nin the recovered subspaces. Second, to enable the data sampled from a nonlinear\nmanifold to be handled potentially and obtain the accurate reconstruction by\navoiding the overfitting, RFDDL minimizes the reconstruction error in a\nflexible manner. Third, to encode the label consistency accurately, RFDDL\ninvolves a discriminative flexible sparse code error to encourage the\ncoefficients to be soft. Fourth, to encode the locality well, RFDDL defines the\nLaplacian matrix over recovered atoms, includes label information of atoms in\nterms of intra-class compactness and inter-class separation, and associates\nwith group sparse codes and classifier to obtain the accurate discriminative\nlocality-constrained coefficients and classifier. Extensive results on public\ndatabases show the effectiveness of our RFDDL.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 13:49:06 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Zhang", "Zhao", ""], ["Ren", "Jiahuan", ""], ["Jiang", "Weiming", ""], ["Zhang", "Zheng", ""], ["Hong", "Richang", ""], ["Yan", "Shuicheng", ""], ["Wang", "Meng", ""]]}, {"id": "1906.04606", "submitter": "Utpal Garain", "authors": "Akshay Chaturvedi and Utpal Garain", "title": "Mimic and Fool: A Task Agnostic Adversarial Attack", "comments": null, "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems (2020)", "doi": "10.1109/TNNLS.2020.2984972", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At present, adversarial attacks are designed in a task-specific fashion.\nHowever, for downstream computer vision tasks such as image captioning, image\nsegmentation etc., the current deep learning systems use an image classifier\nlike VGG16, ResNet50, Inception-v3 etc. as a feature extractor. Keeping this in\nmind, we propose Mimic and Fool, a task agnostic adversarial attack. Given a\nfeature extractor, the proposed attack finds an adversarial image which can\nmimic the image feature of the original image. This ensures that the two images\ngive the same (or similar) output regardless of the task. We randomly select\n1000 MSCOCO validation images for experimentation. We perform experiments on\ntwo image captioning models, Show and Tell, Show Attend and Tell and one VQA\nmodel, namely, end-to-end neural module network (N2NMN). The proposed attack\nachieves success rate of 74.0%, 81.0% and 87.1% for Show and Tell, Show Attend\nand Tell and N2NMN respectively. We also propose a slight modification to our\nattack to generate natural-looking adversarial images. In addition, we also\nshow the applicability of the proposed attack for invertible architecture.\nSince Mimic and Fool only requires information about the feature extractor of\nthe model, it can be considered as a gray-box attack.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 13:56:12 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2020 18:37:42 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Chaturvedi", "Akshay", ""], ["Garain", "Utpal", ""]]}, {"id": "1906.04612", "submitter": "Simon Jenni", "authors": "Simon Jenni, Paolo Favaro", "title": "On Stabilizing Generative Adversarial Training with Noise", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method and analysis to train generative adversarial\nnetworks (GAN) in a stable manner. As shown in recent analysis, training is\noften undermined by the probability distribution of the data being zero on\nneighborhoods of the data space. We notice that the distributions of real and\ngenerated data should match even when they undergo the same filtering.\nTherefore, to address the limited support problem we propose to train GANs by\nusing different filtered versions of the real and generated data distributions.\nIn this way, filtering does not prevent the exact matching of the data\ndistribution, while helping training by extending the support of both\ndistributions. As filtering we consider adding samples from an arbitrary\ndistribution to the data, which corresponds to a convolution of the data\ndistribution with the arbitrary one. We also propose to learn the generation of\nthese samples so as to challenge the discriminator in the adversarial training.\nWe show that our approach results in a stable and well-behaved training of even\nthe original minimax GAN formulation. Moreover, our technique can be\nincorporated in most modern GAN formulations and leads to a consistent\nimprovement on several common datasets.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 14:16:04 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 14:30:26 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Jenni", "Simon", ""], ["Favaro", "Paolo", ""]]}, {"id": "1906.04634", "submitter": "Dan Liu", "authors": "Dan Liu, Dawei Du, Libo Zhang, Tiejian Luo, Yanjun Wu, Feiyue Huang,\n  Siwei Lyu", "title": "Scale Invariant Fully Convolutional Network: Detecting Hands Efficiently", "comments": "Accepted to AAAI2019", "journal-ref": null, "doi": "10.1609/aaai.v33i01.33014344", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing hand detection methods usually follow the pipeline of multiple\nstages with high computation cost, i.e., feature extraction, region proposal,\nbounding box regression, and additional layers for rotated region detection. In\nthis paper, we propose a new Scale Invariant Fully Convolutional Network\n(SIFCN) trained in an end-to-end fashion to detect hands efficiently.\nSpecifically, we merge the feature maps from high to low layers in an iterative\nway, which handles different scales of hands better with less time overhead\ncomparing to concatenating them simply. Moreover, we develop the Complementary\nWeighted Fusion (CWF) block to make full use of the distinctive features among\nmultiple layers to achieve scale invariance. To deal with rotated hand\ndetection, we present the rotation map to get rid of complex rotation and\nderotation layers. Besides, we design the multi-scale loss scheme to accelerate\nthe training process significantly by adding supervision to the intermediate\nlayers of the network. Compared with the state-of-the-art methods, our\nalgorithm shows comparable accuracy and runs a 4.23 times faster speed on the\nVIVA dataset and achieves better average precision on Oxford hand detection\ndataset at a speed of 62.5 fps.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 14:52:08 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Liu", "Dan", ""], ["Du", "Dawei", ""], ["Zhang", "Libo", ""], ["Luo", "Tiejian", ""], ["Wu", "Yanjun", ""], ["Huang", "Feiyue", ""], ["Lyu", "Siwei", ""]]}, {"id": "1906.04649", "submitter": "Anne-Marie Rickmann", "authors": "Anne-Marie Rickmann, Abhijit Guha Roy, Ignacio Sarasua, Nassir Navab,\n  Christian Wachinger", "title": "`Project & Excite' Modules for Segmentation of Volumetric Medical Scans", "comments": "Accepted for International Conference on Medical Image Computing and\n  Computer Assisted Intervention (MICCAI) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully Convolutional Neural Networks (F-CNNs) achieve state-of-the-art\nperformance for image segmentation in medical imaging. Recently, squeeze and\nexcitation (SE) modules and variations thereof have been introduced to\nrecalibrate feature maps channel- and spatial-wise, which can boost performance\nwhile only minimally increasing model complexity. So far, the development of SE\nhas focused on 2D images. In this paper, we propose `Project & Excite' (PE)\nmodules that base upon the ideas of SE and extend them to operating on 3D\nvolumetric images. `Project & Excite' does not perform global average pooling,\nbut squeezes feature maps along different slices of a tensor separately to\nretain more spatial information that is subsequently used in the excitation\nstep. We demonstrate that PE modules can be easily integrated in 3D U-Net,\nboosting performance by 5% Dice points, while only increasing the model\ncomplexity by 2%. We evaluate the PE module on two challenging tasks,\nwhole-brain segmentation of MRI scans and whole-body segmentation of CT scans.\nCode: https://github.com/ai-med/squeeze_and_excitation\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 15:21:33 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 09:21:34 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Rickmann", "Anne-Marie", ""], ["Roy", "Abhijit Guha", ""], ["Sarasua", "Ignacio", ""], ["Navab", "Nassir", ""], ["Wachinger", "Christian", ""]]}, {"id": "1906.04651", "submitter": "Anton Obukhov", "authors": "Anton Obukhov, Stamatios Georgoulis, Dengxin Dai, Luc Van Gool", "title": "Gated CRF Loss for Weakly Supervised Semantic Image Segmentation", "comments": "A portion of the reported numbers are incorrect, along with a few\n  statements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art approaches for semantic segmentation rely on deep\nconvolutional neural networks trained on fully annotated datasets, that have\nbeen shown to be notoriously expensive to collect, both in terms of time and\nmoney. To remedy this situation, weakly supervised methods leverage other forms\nof supervision that require substantially less annotation effort, but they\ntypically present an inability to predict precise object boundaries due to\napproximate nature of the supervisory signals in those regions. While great\nprogress has been made in improving the performance, many of these weakly\nsupervised methods are highly tailored to their own specific settings. This\nraises challenges in reusing algorithms and making steady progress. In this\npaper, we intentionally avoid such practices when tackling weakly supervised\nsemantic segmentation. In particular, we train standard neural networks with\npartial cross-entropy loss function for the labeled pixels and our proposed\nGated CRF loss for the unlabeled pixels. The Gated CRF loss is designed to\ndeliver several important assets: 1) it enables flexibility in the kernel\nconstruction to mask out influence from undesired pixel positions; 2) it\noffloads learning contextual relations to CNN and concentrates on semantic\nboundaries; 3) it does not rely on high-dimensional filtering and thus has a\nsimple implementation. Throughout the paper we present the advantages of the\nloss function, analyze several aspects of weakly supervised training, and show\nthat our `purist' approach achieves state-of-the-art performance for both\nclick-based and scribble-based annotations.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 15:23:01 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 16:45:43 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Obukhov", "Anton", ""], ["Georgoulis", "Stamatios", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "1906.04691", "submitter": "Taewan Kim", "authors": "Taewan Kim, Joydeep Ghosh", "title": "On Single Source Robustness in Deep Fusion Models", "comments": "Accepted to NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms that fuse multiple input sources benefit from both complementary\nand shared information. Shared information may provide robustness against\nfaulty or noisy inputs, which is indispensable for safety-critical applications\nlike self-driving cars. We investigate learning fusion algorithms that are\nrobust against noise added to a single source. We first demonstrate that\nrobustness against single source noise is not guaranteed in a linear fusion\nmodel. Motivated by this discovery, two possible approaches are proposed to\nincrease robustness: a carefully designed loss with corresponding training\nalgorithms for deep fusion models, and a simple convolutional fusion layer that\nhas a structural advantage in dealing with noise. Experimental results show\nthat both training algorithms and our fusion layer make a deep fusion-based 3D\nobject detector robust against noise applied to a single source, while\npreserving the original performance on clean data.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 16:47:56 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 05:32:13 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Kim", "Taewan", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1906.04692", "submitter": "George Adaimi", "authors": "George Adaimi, Sven Kreiss, Alexandre Alahi", "title": "Deep Visual Re-Identification with Confidence", "comments": "Show improvements on vehicle Re-ID datasets; Methods Clarified", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transportation systems often rely on understanding the flow of vehicles or\npedestrian. From traffic monitoring at the city scale, to commuters in train\nterminals, recent progress in sensing technology make it possible to use\ncameras to better understand the demand, i.e., better track moving agents\n(e.g., vehicles and pedestrians). Whether the cameras are mounted on drones,\nvehicles, or fixed in the built environments, they inevitably remain scatter.\nWe need to develop the technology to re-identify the same agents across images\ncaptured from non-overlapping field-of-views, referred to as the visual\nre-identification task. State-of-the-art methods learn a neural network based\nrepresentation trained with the cross-entropy loss function. We argue that such\nloss function is not suited for the visual re-identification task hence propose\nto model confidence in the representation learning framework. We show the\nimpact of our confidence-based learning framework with three methods: label\nsmoothing, confidence penalty, and deep variational information bottleneck.\nThey all show a boost in performance validating our claim. Our contribution is\ngeneric to any agent of interest, i.e., vehicles or pedestrians, and outperform\nhighly specialized state-of-the-art methods across 5 datasets. The source code\nand models are shared towards an open science mission.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 16:49:27 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 08:53:43 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Adaimi", "George", ""], ["Kreiss", "Sven", ""], ["Alahi", "Alexandre", ""]]}, {"id": "1906.04704", "submitter": "Nadieh Khalili", "authors": "N. Khalili, E. Turk, M. Zreik, M.A. Viergever, M.J.N.L. Benders, I.\n  Isgum", "title": "Generative adversarial network for segmentation of motion affected\n  neonatal brain MRI", "comments": "Accepted in Medical Image Computing and Computer Assisted\n  Intervention 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic neonatal brain tissue segmentation in preterm born infants is a\nprerequisite for evaluation of brain development. However, automatic\nsegmentation is often hampered by motion artifacts caused by infant head\nmovements during image acquisition. Methods have been developed to remove or\nminimize these artifacts during image reconstruction using frequency domain\ndata. However, frequency domain data might not always be available. Hence, in\nthis study we propose a method for removing motion artifacts from the already\nreconstructed MR scans. The method employs a generative adversarial network\ntrained with a cycle consistency loss to transform slices affected by motion\ninto slices without motion artifacts, and vice versa. In the experiments 40\nT2-weighted coronal MR scans of preterm born infants imaged at 30 weeks\npostmenstrual age were used. All images contained slices affected by motion\nartifacts hampering automatic tissue segmentation. To evaluate whether\ncorrection allows more accurate image segmentation, the images were segmented\ninto 8 tissue classes: cerebellum, myelinated white matter, basal ganglia and\nthalami, ventricular cerebrospinal fluid, white matter, brain stem, cortical\ngray matter, and extracerebral cerebrospinal fluid. Images corrected for motion\nand corresponding segmentations were qualitatively evaluated using 5-point\nLikert scale. Before the correction of motion artifacts, median image quality\nand quality of corresponding automatic segmentations were assigned grade 2\n(poor) and 3 (moderate), respectively. After correction of motion artifacts,\nboth improved to grades 3 and 4, respectively. The results indicate that\ncorrection of motion artifacts in the image space using the proposed approach\nallows accurate segmentation of brain tissue classes in slices affected by\nmotion artifacts.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 17:11:14 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Khalili", "N.", ""], ["Turk", "E.", ""], ["Zreik", "M.", ""], ["Viergever", "M. A.", ""], ["Benders", "M. J. N. L.", ""], ["Isgum", "I.", ""]]}, {"id": "1906.04713", "submitter": "Nadieh Khalili", "authors": "N. Khalili, N. Lessmann, E. Turk, N. Claessens, R. de Heus, T. Kolk,\n  M.A. Viergever, M.J.N.L. Benders, I. Isgum", "title": "Automatic brain tissue segmentation in fetal MRI using convolutional\n  neural networks", "comments": "Published in Magnetic Resonance Imaging, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MR images of fetuses allow clinicians to detect brain abnormalities in an\nearly stage of development. The cornerstone of volumetric and morphologic\nanalysis in fetal MRI is segmentation of the fetal brain into different tissue\nclasses. Manual segmentation is cumbersome and time consuming, hence automatic\nsegmentation could substantially simplify the procedure. However, automatic\nbrain tissue segmentation in these scans is challenging owing to artifacts\nincluding intensity inhomogeneity, caused in particular by spontaneous fetal\nmovements during the scan. Unlike methods that estimate the bias field to\nremove intensity inhomogeneity as a preprocessing step to segmentation, we\npropose to perform segmentation using a convolutional neural network that\nexploits images with synthetically introduced intensity inhomogeneity as data\naugmentation. The method first uses a CNN to extract the intracranial volume.\nThereafter, another CNN with the same architecture is employed to segment the\nextracted volume into seven brain tissue classes: cerebellum, basal ganglia and\nthalami, ventricular cerebrospinal fluid, white matter, brain stem, cortical\ngray matter and extracerebral cerebrospinal fluid. To make the method\napplicable to slices showing intensity inhomogeneity artifacts, the training\ndata was augmented by applying a combination of linear gradients with random\noffsets and orientations to image slices without artifacts.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 17:28:25 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Khalili", "N.", ""], ["Lessmann", "N.", ""], ["Turk", "E.", ""], ["Claessens", "N.", ""], ["de Heus", "R.", ""], ["Kolk", "T.", ""], ["Viergever", "M. A.", ""], ["Benders", "M. J. N. L.", ""], ["Isgum", "I.", ""]]}, {"id": "1906.04714", "submitter": "Leixin Zhou", "authors": "Leixin Zhou, Zisha Zhong, Abhay Shah, Bensheng Qiu, John Buatti, and\n  Xiaodong Wu", "title": "Deep Neural Networks for Surface Segmentation Meet Conditional Random\n  Fields", "comments": "10 pages. Submitted to IEEE TMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automated surface segmentation is important and challenging in many medical\nimage analysis applications. Recent deep learning based methods have been\ndeveloped for various object segmentation tasks. Most of them are a\nclassification based approach (e.g., U-net), which predicts the probability of\nbeing target object or background for each voxel. One problem of those methods\nis lacking of topology guarantee for segmented objects, and usually post\nprocessing is needed to infer the boundary surface of the object. In this\npaper, a novel model based on 3-D convolutional neural networks (CNNs) and\nConditional Random Fields (CRFs) is proposed to tackle the surface segmentation\nproblem with end-to-end training. To the best of our knowledge, this is the\nfirst study to apply a 3-D neural network with a CRFs model for direct surface\nsegmentation. Experiments carried out on NCI-ISBI 2013 MR prostate dataset and\nMedical Segmentation Decathlon Spleen dataset demonstrated promising\nsegmentation results.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 17:30:27 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 20:15:41 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Zhou", "Leixin", ""], ["Zhong", "Zisha", ""], ["Shah", "Abhay", ""], ["Qiu", "Bensheng", ""], ["Buatti", "John", ""], ["Wu", "Xiaodong", ""]]}, {"id": "1906.04721", "submitter": "Markus Nagel", "authors": "Markus Nagel, Mart van Baalen, Tijmen Blankevoort, Max Welling", "title": "Data-Free Quantization Through Weight Equalization and Bias Correction", "comments": "ICCV 2019", "journal-ref": "The IEEE International Conference on Computer Vision (ICCV), 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a data-free quantization method for deep neural networks that\ndoes not require fine-tuning or hyperparameter selection. It achieves\nnear-original model performance on common computer vision architectures and\ntasks. 8-bit fixed-point quantization is essential for efficient inference on\nmodern deep learning hardware. However, quantizing models to run in 8-bit is a\nnon-trivial task, frequently leading to either significant performance\nreduction or engineering time spent on training a network to be amenable to\nquantization. Our approach relies on equalizing the weight ranges in the\nnetwork by making use of a scale-equivariance property of activation functions.\nIn addition the method corrects biases in the error that are introduced during\nquantization. This improves quantization accuracy performance, and can be\napplied to many common computer vision architectures with a straight forward\nAPI call. For common architectures, such as the MobileNet family, we achieve\nstate-of-the-art quantized model performance. We further show that the method\nalso extends to other computer vision architectures and tasks such as semantic\nsegmentation and object detection.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 17:47:51 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 09:56:06 GMT"}, {"version": "v3", "created": "Mon, 25 Nov 2019 15:00:11 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Nagel", "Markus", ""], ["van Baalen", "Mart", ""], ["Blankevoort", "Tijmen", ""], ["Welling", "Max", ""]]}, {"id": "1906.04725", "submitter": "Zhile Ren", "authors": "Zhile Ren, Erik B. Sudderth", "title": "Clouds of Oriented Gradients for 3D Detection of Objects, Surfaces, and\n  Indoor Scene Layouts", "comments": "Accepted in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (T-PAMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop new representations and algorithms for three-dimensional (3D)\nobject detection and spatial layout prediction in cluttered indoor scenes. We\nfirst propose a clouds of oriented gradient (COG) descriptor that links the 2D\nappearance and 3D pose of object categories, and thus accurately models how\nperspective projection affects perceived image gradients. To better represent\nthe 3D visual styles of large objects and provide contextual cues to improve\nthe detection of small objects, we introduce latent support surfaces. We then\npropose a \"Manhattan voxel\" representation which better captures the 3D room\nlayout geometry of common indoor environments. Effective classification rules\nare learned via a latent structured prediction framework. Contextual\nrelationships among categories and layout are captured via a cascade of\nclassifiers, leading to holistic scene hypotheses that exceed the\nstate-of-the-art on the SUN RGB-D database.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 17:55:37 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Ren", "Zhile", ""], ["Sudderth", "Erik B.", ""]]}, {"id": "1906.04728", "submitter": "Aayush Bansal", "authors": "Aayush Bansal, Yaser Sheikh, Deva Ramanan", "title": "Shapes and Context: In-the-Wild Image Synthesis & Manipulation", "comments": "Project Page: http://www.cs.cmu.edu/~aayushb/OpenShapes/", "journal-ref": "CVPR 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a data-driven approach for interactively synthesizing\nin-the-wild images from semantic label maps. Our approach is dramatically\ndifferent from recent work in this space, in that we make use of no learning.\nInstead, our approach uses simple but classic tools for matching scene context,\nshapes, and parts to a stored library of exemplars. Though simple, this\napproach has several notable advantages over recent work: (1) because nothing\nis learned, it is not limited to specific training data distributions (such as\ncityscapes, facades, or faces); (2) it can synthesize arbitrarily\nhigh-resolution images, limited only by the resolution of the exemplar library;\n(3) by appropriately composing shapes and parts, it can generate an\nexponentially large set of viable candidate output images (that can say, be\ninteractively searched by a user). We present results on the diverse COCO\ndataset, significantly outperforming learning-based approaches on standard\nimage synthesis metrics. Finally, we explore user-interaction and\nuser-controllability, demonstrating that our system can be used as a platform\nfor user-driven content creation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 17:56:26 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Bansal", "Aayush", ""], ["Sheikh", "Yaser", ""], ["Ramanan", "Deva", ""]]}, {"id": "1906.04749", "submitter": "Chao Wang", "authors": "Chao Wang, Grey Ballard, Robert Plemmons, Sudhakar Prasad", "title": "Joint 3D Localization and Classification of Space Debris using a\n  Multispectral Rotating Point Spread Function", "comments": "25 pages", "journal-ref": null, "doi": "10.1364/AO.58.008598", "report-no": null, "categories": "eess.IV cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of joint three-dimensional (3D) localization and\nmaterial classification of unresolved space debris using a multispectral\nrotating point spread function (RPSF). The use of RPSF allows one to estimate\nthe 3D locations of point sources from their rotated images acquired by a\nsingle 2D sensor array, since the amount of rotation of each source image about\nits x, y location depends on its axial distance z. Using multi-spectral images,\nwith one RPSF per spectral band, we are able not only to localize the 3D\npositions of the space debris but also classify their material composition. We\npropose a three-stage method for achieving joint localization and\nclassification. In Stage 1, we adopt an optimization scheme for localization in\nwhich the spectral signature of each material is assumed to be uniform, which\nsignificantly improves efficiency and yields better localization results than\npossible with a single spectral band. In Stage 2, we estimate the spectral\nsignature and refine the localization result via an alternating approach. We\nprocess classification in the final stage. Both Poisson noise and Gaussian\nnoise models are considered, and the implementation of each is discussed.\nNumerical tests using multispectral data from NASA show the efficiency of our\nthree-stage approach and illustrate the improvement of point source\nlocalization and spectral classification from using multiple bands over a\nsingle band.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 18:00:42 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Wang", "Chao", ""], ["Ballard", "Grey", ""], ["Plemmons", "Robert", ""], ["Prasad", "Sudhakar", ""]]}, {"id": "1906.04809", "submitter": "Jinjin Gu", "authors": "Ruicheng Feng, Jinjin Gu, Yu Qiao, Chao Dong", "title": "Suppressing Model Overfitting for Image Super-Resolution Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large deep networks have demonstrated competitive performance in single image\nsuper-resolution (SISR), with a huge volume of data involved. However, in\nreal-world scenarios, due to the limited accessible training pairs, large\nmodels exhibit undesirable behaviors such as overfitting and memorization. To\nsuppress model overfitting and further enjoy the merits of large model\ncapacity, we thoroughly investigate generic approaches for supplying additional\ntraining data pairs. In particular, we introduce a simple learning principle\nMixUp to train networks on interpolations of sample pairs, which encourages\nnetworks to support linear behavior in-between training samples. In addition,\nwe propose a data synthesis method with learned degradation, enabling models to\nuse extra high-quality images with higher content diversity. This strategy\nproves to be successful in reducing biases of data. By combining these\ncomponents -- MixUp and synthetic training data, large models can be trained\nwithout overfitting under very limited data samples and achieve satisfactory\ngeneralization performance. Our method won the second place in NTIRE2019 Real\nSR Challenge.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 20:41:49 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Feng", "Ruicheng", ""], ["Gu", "Jinjin", ""], ["Qiao", "Yu", ""], ["Dong", "Chao", ""]]}, {"id": "1906.04833", "submitter": "Ping Hu", "authors": "Ping Hu, Ximeng Sun, Kate Saenko, Stan Sclaroff", "title": "Weakly-supervised Compositional FeatureAggregation for Few-shot\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from a few examples is a challenging task for machine learning.\nWhile recent progress has been made for this problem, most of the existing\nmethods ignore the compositionality in visual concept representation (e.g.\nobjects are built from parts or composed of semantic attributes), which is key\nto the human ability to easily learn from a small number of examples. To\nenhance the few-shot learning models with compositionality, in this paper we\npresent the simple yet powerful Compositional Feature Aggregation (CFA) module\nas a weakly-supervised regularization for deep networks. Given the deep feature\nmaps extracted from the input, our CFA module first disentangles the feature\nspace into disjoint semantic subspaces that model different attributes, and\nthen bilinearly aggregates the local features within each of these subspaces.\nCFA explicitly regularizes the representation with both semantic and spatial\ncompositionality to produce discriminative representations for few-shot\nrecognition tasks. Moreover, our method does not need any supervision for\nattributes and object parts during training, thus can be conveniently plugged\ninto existing models for end-to-end optimization while keeping the model size\nand computation cost nearly the same. Extensive experiments on few-shot image\nclassification and action recognition tasks demonstrate that our method\nprovides substantial improvements over recent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 21:34:09 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Hu", "Ping", ""], ["Sun", "Ximeng", ""], ["Saenko", "Kate", ""], ["Sclaroff", "Stan", ""]]}, {"id": "1906.04838", "submitter": "Kevin Christensen", "authors": "Kevin Christensen, Martial Hebert", "title": "Edge-Direct Visual Odometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an edge-direct visual odometry algorithm that\nefficiently utilizes edge pixels to find the relative pose that minimizes the\nphotometric error between images. Prior work on exploiting edge pixels instead\ntreats edges as features and employ various techniques to match edge lines or\npixels, which adds unnecessary complexity. Direct methods typically operate on\nall pixel intensities, which proves to be highly redundant. In contrast our\nmethod builds on direct visual odometry methods naturally with minimal added\ncomputation. It is not only more efficient than direct dense methods since we\niterate with a fraction of the pixels, but also more accurate. We achieve high\naccuracy and efficiency by extracting edges from only one image, and utilize\nrobust Gauss-Newton to minimize the photometric error of these edge pixels.\nThis simultaneously finds the edge pixels in the reference image, as well as\nthe relative camera pose that minimizes the photometric error. We test various\nedge detectors, including learned edges, and determine that the optimal edge\ndetector for this method is the Canny edge detection algorithm using automatic\nthresholding. We highlight key differences between our edge direct method and\ndirect dense methods, in particular how higher levels of image pyramids can\nlead to significant aliasing effects and result in incorrect solution\nconvergence. We show experimentally that reducing the photometric error of edge\npixels also reduces the photometric error of all pixels, and we show through an\nablation study the increase in accuracy obtained by optimizing edge pixels\nonly. We evaluate our method on the RGB-D TUM benchmark on which we achieve\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 21:53:49 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Christensen", "Kevin", ""], ["Hebert", "Martial", ""]]}, {"id": "1906.04854", "submitter": "Xin Wang", "authors": "Xin Wang, Fisher Yu, Trevor Darrell, Joseph E. Gonzalez", "title": "Task-Aware Feature Generation for Zero-Shot Compositional Learning", "comments": "17 pages, 9 figures; substantial content updates with additional\n  experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual concepts (e.g., red apple, big elephant) are often semantically\ncompositional and each element of the compositions can be reused to construct\nnovel concepts (e.g., red elephant). Compositional feature synthesis, which\ngenerates image feature distributions exploiting the semantic compositionality,\nis a promising approach to sample-efficient model generalization. In this work,\nwe propose a task-aware feature generation (TFG) framework for compositional\nlearning, which generates features of novel visual concepts by transferring\nknowledge from previously seen concepts. These synthetic features are then used\nto train a classifier to recognize novel concepts in a zero-shot manner. Our\nnovel TFG design injects task-conditioned noise layer-by-layer, producing\ntask-relevant variation at each level. We find the proposed generator design\nimproves classification accuracy and sample efficiency. Our model establishes a\nnew state of the art on three zero-shot compositional learning (ZSCL)\nbenchmarks, outperforming the previous discriminative models by a large margin.\nOur model improves the performance of the prior arts by over 2x in the\ngeneralized ZSCL setting.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 23:00:43 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 23:57:41 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Wang", "Xin", ""], ["Yu", "Fisher", ""], ["Darrell", "Trevor", ""], ["Gonzalez", "Joseph E.", ""]]}, {"id": "1906.04876", "submitter": "Apoorva Dornadula", "authors": "Apoorva Dornadula, Austin Narcomey, Ranjay Krishna, Michael Bernstein,\n  Li Fei-Fei", "title": "Learning Predicates as Functions to Enable Few-shot Scene Graph\n  Prediction", "comments": "14 pages, 10 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene graph prediction --- classifying the set of objects and predicates in a\nvisual scene --- requires substantial training data. However, most predicates\nonly occur a handful of times making them difficult to learn. We introduce the\nfirst scene graph prediction model that supports few-shot learning of\npredicates. Existing scene graph generation models represent objects using\npretrained object detectors or word embeddings that capture semantic object\ninformation at the cost of encoding information about which relationships they\nafford. So, these object representations are unable to generalize to new\nfew-shot relationships. We introduce a framework that induces object\nrepresentations that are structured according to their visual relationships.\nUnlike past methods, our framework embeds objects that afford similar\nrelationships closer together. This property allows our model to perform well\nin the few-shot setting. For example, applying the 'riding' predicate\ntransformation to 'person' modifies the representation towards objects like\n'skateboard' and 'horse' that enable riding. We generate object representations\nby learning predicates trained as message passing functions within a new graph\nconvolution framework. The object representations are used to build few-shot\npredicate classifiers for rare predicates with as few as 1 labeled example. We\nachieve a 5-shot performance of 22.70 recall@50, a 3.7 increase when compared\nto strong transfer learning baselines.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 01:27:15 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 02:44:37 GMT"}, {"version": "v3", "created": "Mon, 2 Dec 2019 18:14:45 GMT"}, {"version": "v4", "created": "Thu, 5 Dec 2019 19:35:36 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Dornadula", "Apoorva", ""], ["Narcomey", "Austin", ""], ["Krishna", "Ranjay", ""], ["Bernstein", "Michael", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1906.04887", "submitter": "Sam Shleifer", "authors": "Sam Shleifer and Eric Prokop", "title": "Using Small Proxy Datasets to Accelerate Hyperparameter Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the biggest bottlenecks in a machine learning workflow is waiting for\nmodels to train. Depending on the available computing resources, it can take\ndays to weeks to train a neural network on a large dataset with many classes\nsuch as ImageNet. For researchers experimenting with new algorithmic\napproaches, this is impractically time consuming and costly. We aim to generate\nsmaller \"proxy datasets\" where experiments are cheaper to run but results are\nhighly correlated with experimental results on the full dataset. We generate\nthese proxy datasets using by randomly sampling from examples or classes,\ntraining on only the easiest or hardest examples and training on synthetic\nexamples generated by \"data distillation\". We compare these techniques to the\nmore widely used baseline of training on the full dataset for fewer epochs. For\neach proxying strategy, we estimate three measures of \"proxy quality\": how much\nof the variance in experimental results on the full dataset can be explained by\nexperimental results on the proxy dataset.\n  Experiments on Imagenette and Imagewoof (Howard, 2019) show that running\nhyperparameter search on the easiest 10% of examples explains 81% of the\nvariance in experiment results on the target task, and using the easiest 50% of\nexamples can explain 95% of the variance, significantly more than training on\nall the data for fewer epochs, a more widely used baseline. These \"easy\"\nproxies are higher quality than training on the full dataset for a reduced\nnumber of epochs (but equivalent computational cost), and, unexpectedly, higher\nquality than proxies constructed from the hardest examples. Without access to a\ntrained model, researchers can improve proxy quality by restricting the subset\nto fewer classes; proxies built on half the classes are higher quality than\nthose with an equivalent number of examples spread across all classes.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 02:01:30 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Shleifer", "Sam", ""], ["Prokop", "Eric", ""]]}, {"id": "1906.04888", "submitter": "Arturo Gomez Chavez", "authors": "Arturo Gomez Chavez, Qingwen Xu, Christian A. Mueller, S\\\"oren\n  Schwertfeger, Andreas Birk", "title": "Adaptive Navigation Scheme for Optimal Deep-Sea Localization Using\n  Multimodal Perception Cues", "comments": "Submitted to IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underwater robot interventions require a high level of safety and\nreliability. A major challenge to address is a robust and accurate acquisition\nof localization estimates, as it is a prerequisite to enable more complex\ntasks, e.g. floating manipulation and mapping. State-of-the-art navigation in\ncommercial operations, such as oil & gas production (OGP), rely on costly\ninstrumentation. These can be partially replaced or assisted by visual\nnavigation methods, especially in deep-sea scenarios where equipment deployment\nhas high costs and risks. Our work presents a multimodal approach that adapts\nstate-of-the-art methods from on-land robotics, i.e., dense point cloud\ngeneration in combination with plane representation and registration, to boost\nunderwater localization performance. A two-stage navigation scheme is proposed\nthat initially generates a coarse probabilistic map of the workspace, which is\nused to filter noise from computed point clouds and planes in the second stage.\nFurthermore, an adaptive decision-making approach is introduced that determines\nwhich perception cues to incorporate into the localization filter to optimize\naccuracy and computation performance. Our approach is investigated first in\nsimulation and then validated with data from field trials in OGP monitoring and\nmaintenance scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 02:03:20 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Chavez", "Arturo Gomez", ""], ["Xu", "Qingwen", ""], ["Mueller", "Christian A.", ""], ["Schwertfeger", "S\u00f6ren", ""], ["Birk", "Andreas", ""]]}, {"id": "1906.04892", "submitter": "Weiyang Liu", "authors": "Rongmei Lin, Weiyang Liu, Zhen Liu, Chen Feng, Zhiding Yu, James M.\n  Rehg, Li Xiong, Le Song", "title": "Regularizing Neural Networks via Minimizing Hyperspherical Energy", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the Thomson problem in physics where the distribution of multiple\npropelling electrons on a unit sphere can be modeled via minimizing some\npotential energy, hyperspherical energy minimization has demonstrated its\npotential in regularizing neural networks and improving their generalization\npower. In this paper, we first study the important role that hyperspherical\nenergy plays in neural network training by analyzing its training dynamics.\nThen we show that naively minimizing hyperspherical energy suffers from some\ndifficulties due to highly non-linear and non-convex optimization as the space\ndimensionality becomes higher, therefore limiting the potential to further\nimprove the generalization. To address these problems, we propose the\ncompressive minimum hyperspherical energy (CoMHE) as a more effective\nregularization for neural networks. Specifically, CoMHE utilizes projection\nmappings to reduce the dimensionality of neurons and minimizes their\nhyperspherical energy. According to different designs for the projection\nmapping, we propose several distinct yet well-performing variants and provide\nsome theoretical guarantees to justify their effectiveness. Our experiments\nshow that CoMHE consistently outperforms existing regularization methods, and\ncan be easily applied to different neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 02:12:28 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 16:04:06 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Lin", "Rongmei", ""], ["Liu", "Weiyang", ""], ["Liu", "Zhen", ""], ["Feng", "Chen", ""], ["Yu", "Zhiding", ""], ["Rehg", "James M.", ""], ["Xiong", "Li", ""], ["Song", "Le", ""]]}, {"id": "1906.04909", "submitter": "Jinsong Zhang", "authors": "Jinsong Zhang and Kalyan Sunkavalli and Yannick Hold-Geoffroy and\n  Sunil Hadap and Jonathan Eisenmann and Jean-Fran\\c{c}ois Lalonde", "title": "All-Weather Deep Outdoor Lighting Estimation", "comments": "8 pages, CVPR 19. Project page: http://lvsn.github.io/allweather", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural network that predicts HDR outdoor illumination from a\nsingle LDR image. At the heart of our work is a method to accurately learn HDR\nlighting from LDR panoramas under any weather condition. We achieve this by\ntraining another CNN (on a combination of synthetic and real images) to take as\ninput an LDR panorama, and regress the parameters of the Lalonde-Matthews\noutdoor illumination model. This model is trained such that it a) reconstructs\nthe appearance of the sky, and b) renders the appearance of objects lit by this\nillumination. We use this network to label a large-scale dataset of LDR\npanoramas with lighting parameters and use them to train our single image\noutdoor lighting estimation network. We demonstrate, via extensive experiments,\nthat both our panorama and single image networks outperform the state of the\nart, and unlike prior work, are able to handle weather conditions ranging from\nfully sunny to overcast skies.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 03:19:08 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Zhang", "Jinsong", ""], ["Sunkavalli", "Kalyan", ""], ["Hold-Geoffroy", "Yannick", ""], ["Hadap", "Sunil", ""], ["Eisenmann", "Jonathan", ""], ["Lalonde", "Jean-Fran\u00e7ois", ""]]}, {"id": "1906.04910", "submitter": "Matheus Gadelha", "authors": "Matheus Gadelha, Aartika Rai, Subhransu Maji, Rui Wang", "title": "Inferring 3D Shapes from Image Collections using Adversarial Networks", "comments": "Source code: https://github.com/matheusgadelha/PrGAN . arXiv admin\n  note: substantial text overlap with arXiv:1612.05872", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the problem of learning a probabilistic distribution over\nthree-dimensional shapes given two-dimensional views of multiple objects taken\nfrom unknown viewpoints. Our approach called projective generative adversarial\nnetwork (PrGAN) trains a deep generative model of 3D shapes whose projections\n(or renderings) match the distributions of the provided 2D distribution. The\naddition of a differentiable projection module allows us to infer the\nunderlying 3D shape distribution without access to any explicit 3D or viewpoint\nannotation during the learning phase. We show that our approach produces 3D\nshapes of comparable quality to GANs trained directly on 3D data. %for a number\nof shape categoriesincluding chairs, airplanes, and cars. Experiments also show\nthat the disentangled representation of 2D shapes into geometry and viewpoint\nleads to a good generative model of 2D shapes. The key advantage of our model\nis that it estimates 3D shape, viewpoint, and generates novel views from an\ninput image in a completely unsupervised manner. We further investigate how the\ngenerative models can be improved if additional information such as depth,\nviewpoint or part segmentations is available at training time. To this end, we\npresent new differentiable projection operators that can be used by PrGAN to\nlearn better 3D generative models. Our experiments show that our method can\nsuccessfully leverage extra visual cues to create more diverse and accurate\nshapes.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 00:28:19 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Gadelha", "Matheus", ""], ["Rai", "Aartika", ""], ["Maji", "Subhransu", ""], ["Wang", "Rui", ""]]}, {"id": "1906.04913", "submitter": "Wei Wang", "authors": "Wei Wang, Kaicheng Yu, Joachim Hugonot, Pascal Fua and Mathieu\n  Salzmann", "title": "Recurrent U-Net for Resource-Constrained Segmentation", "comments": "arXiv admin note: substantial text overlap with arXiv:1811.10914", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art segmentation methods rely on very deep networks that are not\nalways easy to train without very large training datasets and tend to be\nrelatively slow to run on standard GPUs. In this paper, we introduce a novel\nrecurrent U-Net architecture that preserves the compactness of the original\nU-Net, while substantially increasing its performance to the point where it\noutperforms the state of the art on several benchmarks. We will demonstrate its\neffectiveness for several tasks, including hand segmentation, retina vessel\nsegmentation, and road segmentation. We also introduce a large-scale dataset\nfor hand segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 17:52:30 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Wang", "Wei", ""], ["Yu", "Kaicheng", ""], ["Hugonot", "Joachim", ""], ["Fua", "Pascal", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "1906.04933", "submitter": "Jonathan Wenger", "authors": "Jonathan Wenger and Hedvig Kjellstr\\\"om and Rudolph Triebel", "title": "Non-Parametric Calibration for Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications of classification methods not only require high accuracy\nbut also reliable estimation of predictive uncertainty. However, while many\ncurrent classification frameworks, in particular deep neural networks, achieve\nhigh accuracy, they tend to incorrectly estimate uncertainty. In this paper, we\npropose a method that adjusts the confidence estimates of a general classifier\nsuch that they approach the probability of classifying correctly. In contrast\nto existing approaches, our calibration method employs a non-parametric\nrepresentation using a latent Gaussian process, and is specifically designed\nfor multi-class classification. It can be applied to any classifier that\noutputs confidence estimates and is not limited to neural networks. We also\nprovide a theoretical analysis regarding the over- and underconfidence of a\nclassifier and its relationship to calibration, as well as an empirical outlook\nfor calibrated active learning. In experiments we show the universally strong\nperformance of our method across different classifiers and benchmark data sets,\nin particular for state-of-the art neural network architectures.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 04:11:48 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 11:41:47 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2020 14:20:33 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Wenger", "Jonathan", ""], ["Kjellstr\u00f6m", "Hedvig", ""], ["Triebel", "Rudolph", ""]]}, {"id": "1906.04944", "submitter": "Himanshu Rai", "authors": "Cheng Chang, Himanshu Rai, Satya Krishna Gorti, Junwei Ma, Chundi Liu,\n  Guangwei Yu, Maksims Volkovs", "title": "Semi-Supervised Exploration in Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our solution to Landmark Image Retrieval Challenge 2019. This\nchallenge was based on the large Google Landmarks Dataset V2[9]. The goal was\nto retrieve all database images containing the same landmark for every provided\nquery image. Our solution is a combination of global and local models to form\nan initial KNN graph. We then use a novel extension of the recently proposed\ngraph traversal method EGT [1] referred to as semi-supervised EGT to refine the\ngraph and retrieve better candidates.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 05:12:32 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Chang", "Cheng", ""], ["Rai", "Himanshu", ""], ["Gorti", "Satya Krishna", ""], ["Ma", "Junwei", ""], ["Liu", "Chundi", ""], ["Yu", "Guangwei", ""], ["Volkovs", "Maksims", ""]]}, {"id": "1906.04950", "submitter": "Xiangxi Mo", "authors": "Xiangxi Mo, Ruizhe Cheng, Tianyi Fang", "title": "Pay Attention to Convolution Filters: Towards Fast and Accurate\n  Fine-Grained Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an efficient transfer learning method for adapting ImageNet\npre-trained Convolutional Neural Network (CNN) to fine-grained image\nclassification task. Conventional transfer learning methods typically face the\ntrade-off between training time and accuracy. By adding \"attention module\" to\neach convolutional filters of the pre-trained network, we are able to rank and\nadjust the importance of each convolutional signal in an end-to-end pipeline.\nIn this report, we show our method can adapt a pre-trianed ResNet50 for a\nfine-grained transfer learning task within few epochs and achieve accuracy\nabove conventional transfer learning methods and close to models trained from\nscratch. Our model also offer interpretable result because the rank of the\nconvolutional signal shows which convolution channels are utilized and\namplified to achieve better classification result, as well as which signal\nshould be treated as noise for the specific transfer learning task, which could\nbe pruned to lower model size.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 05:38:24 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Mo", "Xiangxi", ""], ["Cheng", "Ruizhe", ""], ["Fang", "Tianyi", ""]]}, {"id": "1906.04952", "submitter": "Kazuaki Kondo Mr.", "authors": "Kazuaki Kondo, Daisuke Deguchi, Atsushi Shimada", "title": "Hand Orientation Estimation in Probability Density Form", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand orientation is an essential feature required to understand hand\nbehaviors and subsequently support human activities. In this paper, we present\na new method for estimating hand orientation in probability density form. It\ncan solve the cyclicity problem in direct angular representation and enables\nthe integration of multiple predictions based on different features. We\nvalidated the performance of the proposed method and an integration example\nusing our dataset, which captured cooperative group work.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 05:42:38 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Kondo", "Kazuaki", ""], ["Deguchi", "Daisuke", ""], ["Shimada", "Atsushi", ""]]}, {"id": "1906.04962", "submitter": "Changhee Han", "authors": "Changhee Han, Yoshiro Kitamura, Akira Kudo, Akimichi Ichinose,\n  Leonardo Rundo, Yujiro Furukawa, Kazuki Umemoto, Yuanzhong Li, Hideki\n  Nakayama", "title": "Synthesizing Diverse Lung Nodules Wherever Massively: 3D\n  Multi-Conditional GAN-based CT Image Augmentation for Object Detection", "comments": "9 pages, 6 figures, accepted to 3DV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate Computer-Assisted Diagnosis, relying on large-scale annotated\npathological images, can alleviate the risk of overlooking the diagnosis.\nUnfortunately, in medical imaging, most available datasets are\nsmall/fragmented. To tackle this, as a Data Augmentation (DA) method, 3D\nconditional Generative Adversarial Networks (GANs) can synthesize desired\nrealistic/diverse 3D images as additional training data. However, no 3D\nconditional GAN-based DA approach exists for general bounding box-based 3D\nobject detection, while it can locate disease areas with physicians' minimum\nannotation cost, unlike rigorous 3D segmentation. Moreover, since lesions vary\nin position/size/attenuation, further GAN-based DA performance requires\nmultiple conditions. Therefore, we propose 3D Multi-Conditional GAN (MCGAN) to\ngenerate realistic/diverse 32 X 32 X 32 nodules placed naturally on lung\nComputed Tomography images to boost sensitivity in 3D object detection. Our\nMCGAN adopts two discriminators for conditioning: the context discriminator\nlearns to classify real vs synthetic nodule/surrounding pairs with noise\nbox-centered surroundings; the nodule discriminator attempts to classify real\nvs synthetic nodules with size/attenuation conditions. The results show that 3D\nConvolutional Neural Network-based detection can achieve higher sensitivity\nunder any nodule size/attenuation at fixed False Positive rates and overcome\nthe medical data paucity with the MCGAN-generated realistic nodules---even\nexpert physicians fail to distinguish them from the real ones in Visual Turing\nTest.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 06:31:39 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 18:54:57 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Han", "Changhee", ""], ["Kitamura", "Yoshiro", ""], ["Kudo", "Akira", ""], ["Ichinose", "Akimichi", ""], ["Rundo", "Leonardo", ""], ["Furukawa", "Yujiro", ""], ["Umemoto", "Kazuki", ""], ["Li", "Yuanzhong", ""], ["Nakayama", "Hideki", ""]]}, {"id": "1906.04976", "submitter": "Changxing Ding", "authors": "Kan Wang, Changxing Ding, Stephen J. Maybank, and Dacheng Tao", "title": "CDPM: Convolutional Deformable Part Models for Semantically Aligned\n  Person Re-identification", "comments": "13 pages, 13 figures, To appear in IEEE Transactions on Image\n  Processing", "journal-ref": null, "doi": "10.1109/TIP.2019.2959923", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Part-level representations are essential for robust person re-identification.\nHowever, common errors that arise during pedestrian detection frequently result\nin severe misalignment problems for body parts, which degrade the quality of\npart representations. Accordingly, to deal with this problem, we propose a\nnovel model named Convolutional Deformable Part Models (CDPM). CDPM works by\ndecoupling the complex part alignment procedure into two easier steps: first, a\nvertical alignment step detects each body part in the vertical direction, with\nthe help of a multi-task learning model; second, a horizontal refinement step\nbased on attention suppresses the background information around each detected\nbody part. Since these two steps are performed orthogonally and sequentially,\nthe difficulty of part alignment is significantly reduced. In the testing\nstage, CDPM is able to accurately align flexible body parts without any need\nfor outside information. Extensive experimental results demonstrate the\neffectiveness of the proposed CDPM for part alignment. Most impressively, CDPM\nachieves state-of-the-art performance on three large-scale datasets:\nMarket-1501, DukeMTMC-ReID,and CUHK03.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 07:20:08 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 01:07:12 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Wang", "Kan", ""], ["Ding", "Changxing", ""], ["Maybank", "Stephen J.", ""], ["Tao", "Dacheng", ""]]}, {"id": "1906.04979", "submitter": "Sheng Chen", "authors": "Sheng Chen, Xu Wang, Chao Chen, Yifan Lu, Xijin Zhang, Linfu Wen", "title": "DeepSquare: Boosting the Learning Power of Deep Convolutional Neural\n  Networks with Elementwise Square Operators", "comments": "We has submitted this paper to a conference before March 22, 2019. We\n  will improve this paper according to the received reviews. The code will be\n  released if the paper is accepted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern neural network modules which can significantly enhance the learning\npower usually add too much computational complexity to the original neural\nnetworks. In this paper, we pursue very efficient neural network modules which\ncan significantly boost the learning power of deep convolutional neural\nnetworks with negligible extra computational cost. We first present both\ntheoretically and experimentally that elementwise square operator has a\npotential to enhance the learning power of neural networks. Then, we design\nfour types of lightweight modules with elementwise square operators, named as\nSquare-Pooling, Square-Softmin, Square-Excitation, and Square-Encoding. We add\nour four lightweight modules to Resnet18, Resnet50, and ShuffleNetV2 for better\nperformance in the experiment on ImageNet 2012 dataset. The experimental\nresults show that our modules can bring significant accuracy improvements to\nthe base convolutional neural network models. The performance of our\nlightweight modules is even comparable to many complicated modules such as\nbilinear pooling, Squeeze-and-Excitation, and Gather-Excite. Our highly\nefficient modules are particularly suitable for mobile models. For example,\nwhen equipped with a single Square-Pooling module, the top-1 classification\naccuracy of ShuffleNetV2-0.5x on ImageNet 2012 is absolutely improved by 1.45%\nwith no additional parameters and negligible inference time overhead.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 07:27:44 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Chen", "Sheng", ""], ["Wang", "Xu", ""], ["Chen", "Chao", ""], ["Lu", "Yifan", ""], ["Zhang", "Xijin", ""], ["Wen", "Linfu", ""]]}, {"id": "1906.04987", "submitter": "Chiranjibi Sitaula", "authors": "Chiranjibi Sitaula, Yong Xiang, Yushu Zhang, Xuequan Lu, and Sunil\n  Aryal", "title": "Indoor image representation by high-level semantic features", "comments": "This paper has been accepted in IEEE Access", "journal-ref": "IEEE Access 7 ,2019", "doi": "10.1109/ACCESS.2019.2925002", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indoor image features extraction is a fundamental problem in multiple fields\nsuch as image processing, pattern recognition, robotics and so on.\nNevertheless, most of the existing feature extraction methods, which extract\nfeatures based on pixels, color, shape/object parts or objects on images,\nsuffer from limited capabilities in describing semantic information (e.g.,\nobject association). These techniques, therefore, involve undesired\nclassification performance. To tackle this issue, we propose the notion of\nhigh-level semantic features and design four steps to extract them.\nSpecifically, we first construct the objects pattern dictionary through\nextracting raw objects in the images, and then retrieve and extract semantic\nobjects from the objects pattern dictionary. We finally extract our high-level\nsemantic features based on the calculated probability and delta parameter.\nExperiments on three publicly available datasets (MIT-67, Scene15 and NYU V1)\nshow that our feature extraction approach outperforms state-of-the-art feature\nextraction methods for indoor image classification, given a lower dimension of\nour features than those methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 07:53:26 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 05:10:12 GMT"}, {"version": "v3", "created": "Fri, 12 Jul 2019 00:48:45 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Sitaula", "Chiranjibi", ""], ["Xiang", "Yong", ""], ["Zhang", "Yushu", ""], ["Lu", "Xuequan", ""], ["Aryal", "Sunil", ""]]}, {"id": "1906.05074", "submitter": "Zawar Hussain Mr", "authors": "Zawar Hussain, Michael Sheng, Wei Emma Zhang", "title": "Different Approaches for Human Activity Recognition: A Survey", "comments": "28", "journal-ref": null, "doi": "10.1016/j.jnca.2020.102738", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human activity recognition has gained importance in recent years due to its\napplications in various fields such as health, security and surveillance,\nentertainment, and intelligent environments. A significant amount of work has\nbeen done on human activity recognition and researchers have leveraged\ndifferent approaches, such as wearable, object-tagged, and device-free, to\nrecognize human activities. In this article, we present a comprehensive survey\nof the work conducted over the period 2010-2018 in various areas of human\nactivity recognition with main focus on device-free solutions. The device-free\napproach is becoming very popular due to the fact that the subject is not\nrequired to carry anything, instead, the environment is tagged with devices to\ncapture the required information. We propose a new taxonomy for categorizing\nthe research work conducted in the field of activity recognition and divide the\nexisting literature into three sub-areas: action-based, motion-based, and\ninteraction-based. We further divide these areas into ten different sub-topics\nand present the latest research work in these sub-topics. Unlike previous\nsurveys which focus only on one type of activities, to the best of our\nknowledge, we cover all the sub-areas in activity recognition and provide a\ncomparison of the latest research work in these sub-areas. Specifically, we\ndiscuss the key attributes and design approaches for the work presented. Then\nwe provide extensive analysis based on 10 important metrics, to give the\nreader, a complete overview of the state-of-the-art techniques and trends in\ndifferent sub-areas of human activity recognition. In the end, we discuss open\nresearch issues and provide future research directions in the field of human\nactivity recognition.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 04:12:15 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Hussain", "Zawar", ""], ["Sheng", "Michael", ""], ["Zhang", "Wei Emma", ""]]}, {"id": "1906.05096", "submitter": "Jianlei Yang", "authors": "Runze Liu, Jianlei Yang, Yiran Chen, Weisheng Zhao", "title": "eSLAM: An Energy-Efficient Accelerator for Real-Time ORB-SLAM on FPGA\n  Platform", "comments": "to appear in DAC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.RO eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Simultaneous Localization and Mapping (SLAM) is a critical task for\nautonomous navigation. However, due to the computational complexity of SLAM\nalgorithms, it is very difficult to achieve real-time implementation on\nlow-power platforms.We propose an energy efficient architecture for real-time\nORB (Oriented-FAST and Rotated- BRIEF) based visual SLAM system by accelerating\nthe most time consuming stages of feature extraction and matching on FPGA\nplatform.Moreover, the original ORB descriptor pattern is reformed as a\nrotational symmetric manner which is much more hardware friendly. Optimizations\nincluding rescheduling and parallelizing are further utilized to improve the\nthroughput and reduce the memory footprint. Compared with Intel i7 and ARM\nCortex-A9 CPUs on TUM dataset, our FPGA realization achieves up to 3X and 31X\nframe rate improvement, as well as up to 71X and 25X energy efficiency\nimprovement, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 16:30:06 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Liu", "Runze", ""], ["Yang", "Jianlei", ""], ["Chen", "Yiran", ""], ["Zhao", "Weisheng", ""]]}, {"id": "1906.05105", "submitter": "Yang Xiao", "authors": "Yang Xiao, Xuchong Qiu, Pierre-Alain Langlois, Mathieu Aubry, Renaud\n  Marlet", "title": "Pose from Shape: Deep Pose Estimation for Arbitrary 3D Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most deep pose estimation methods need to be trained for specific object\ninstances or categories. In this work we propose a completely generic deep pose\nestimation approach, which does not require the network to have been trained on\nrelevant categories, nor objects in a category to have a canonical pose. We\nbelieve this is a crucial step to design robotic systems that can interact with\nnew objects in the wild not belonging to a predefined category. Our main\ninsight is to dynamically condition pose estimation with a representation of\nthe 3D shape of the target object. More precisely, we train a Convolutional\nNeural Network that takes as input both a test image and a 3D model, and\noutputs the relative 3D pose of the object in the input image with respect to\nthe 3D model. We demonstrate that our method boosts performances for supervised\ncategory pose estimation on standard benchmarks, namely Pascal3D+, ObjectNet3D\nand Pix3D, on which we provide results superior to the state of the art. More\nimportantly, we show that our network trained on everyday man-made objects from\nShapeNet generalizes without any additional training to completely new types of\n3D objects by providing results on the LINEMOD dataset as well as on natural\nentities such as animals from ImageNet.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 12:58:21 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 14:43:03 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Xiao", "Yang", ""], ["Qiu", "Xuchong", ""], ["Langlois", "Pierre-Alain", ""], ["Aubry", "Mathieu", ""], ["Marlet", "Renaud", ""]]}, {"id": "1906.05119", "submitter": "Guisik Kim", "authors": "Guisik Kim and Junseok Kwon", "title": "LED2Net: Deep Illumination-aware Dehazing with Low-light and Detail\n  Enhancement", "comments": "we will upload new version of this paper after August", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel dehazing and low-light enhancement method based on an\nillumination map that is accurately estimated by a convolutional neural network\n(CNN). In this paper, the illumination map is used as a component for three\ndifferent tasks, namely, atmospheric light estimation, transmission map\nestimation, and low-light enhancement. To train CNNs for dehazing and low-light\nenhancement simultaneously based on the retinex theory, we synthesize numerous\nlow-light and hazy images from normal hazy images from the FADE data set. In\naddition, we further improve the network using detail enhancement. Experimental\nresults demonstrate that our method surpasses recent state-of-theart algorithms\nquantitatively and qualitatively. In particular, our haze-free images present\nvivid colors and enhance visibility without a halo effect or color distortion.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 13:09:28 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 04:38:27 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Kim", "Guisik", ""], ["Kwon", "Junseok", ""]]}, {"id": "1906.05131", "submitter": "Hamed Talebi", "authors": "Hamed Talebi, Amin Ranjbar, Alireza Davoudi, Hamed Gholami, Mohammad\n  Bagher Menhaj", "title": "High Accuracy Classification of White Blood Cells using TSLDA Classifier\n  and Covariance Features", "comments": "7 pages, 4 tables, 3 figures, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  creating automated processes in different areas of medical science with the\napplication of engineering tools is a highly growing field over recent decades.\nIn this context, many medical image processing and analyzing researchers use\nworthwhile methods in artificial intelligence, which can reduce necessary human\npower while increases accuracy of results. Among various medical images, blood\nmicroscopic images play a vital role in heart failure diagnosis, e.g., blood\ncancers. The prominent component in blood cancer diagnosis is white blood cells\n(WBCs) which due to its general characteristics in microscopic images sometimes\nmake difficulties in recognition and classification tasks such as non-uniform\ncolors/illuminances, different shapes, sizes, and textures. Moreover,\noverlapped WBCs in bone marrow images and neighboring to red blood cells are\nidentified as reasons for errors in the classification task. In this paper, we\nhave endeavored to segment various parts in medical images via Na\\\"ive Bayes\nclustering method and in next stage via TSLDA classifier, which is supplied by\nfeatures acquired from covariance descriptor results in the accuracy of 98.02%.\nIt seems that this result is delightful in WBCs recognition.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 13:36:41 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 15:23:52 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Talebi", "Hamed", ""], ["Ranjbar", "Amin", ""], ["Davoudi", "Alireza", ""], ["Gholami", "Hamed", ""], ["Menhaj", "Mohammad Bagher", ""]]}, {"id": "1906.05147", "submitter": "Nachwa Abou Bakr", "authors": "Nachwa Aboubakr, James L. Crowley, Remi Ronfard", "title": "Recognizing Manipulation Actions from State-Transformations", "comments": "Accepted for presentation at EPIC@CVPR2019 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manipulation actions transform objects from an initial state into a final\nstate. In this paper, we report on the use of object state transitions as a\nmean for recognizing manipulation actions. Our method is inspired by the\nintuition that object states are visually more apparent than actions from a\nstill frame and thus provide information that is complementary to\nspatio-temporal action recognition. We start by defining a state transition\nmatrix that maps action labels into a pre-state and a post-state. From each\nkeyframe, we learn appearance models of objects and their states. Manipulation\nactions can then be recognized from the state transition matrix. We report\nresults on the EPIC kitchen action recognition challenge.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 14:02:17 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Aboubakr", "Nachwa", ""], ["Crowley", "James L.", ""], ["Ronfard", "Remi", ""]]}, {"id": "1906.05156", "submitter": "Ahmad Kalhor", "authors": "Ahmad Kalhor, Mohsen Saffar, Melika Kheirieh, Somayyeh Hoseinipoor and\n  Babak N. Araabi (University of Tehran, College of Engineering, School of\n  Electrical and Computer Engineering, Tehran, Iran)", "title": "Evaluation of Dataflow through layers of Deep Neural Networks in\n  Classification and Regression Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces two straightforward, effective indices to evaluate the\ninput data and the data flowing through layers of a feedforward deep neural\nnetwork. For classification problems, the separation rate of target labels in\nthe space of dataflow is explained as a key factor indicating the performance\nof designed layers in improving the generalization of the network. According to\nthe explained concept, a shapeless distance-based evaluation index is proposed.\nSimilarly, for regression problems, the smoothness rate of target outputs in\nthe space of dataflow is explained as a key factor indicating the performance\nof designed layers in improving the generalization of the network. According to\nthe explained smoothness concept, a shapeless distance-based smoothness index\nis proposed for regression problems. To consider more strictly concepts of\nseparation and smoothness, their extended versions are introduced, and by\ninterpreting a regression problem as a classification problem, it is shown that\nthe separation and smoothness indices are related together. Through four case\nstudies, the profits of using the introduced indices are shown. In the first\ncase study, for classification and regression problems , the challenging of\nsome known input datasets are compared respectively by the proposed separation\nand smoothness indices. In the second case study, the quality of dataflow is\nevaluated through layers of two pre-trained VGG 16 networks in classification\nof Cifar10 and Cifar100. In the third case study, it is shown that the correct\nclassification rate and the separation index are almost equivalent through\nlayers particularly while the serration index is increased. In the last case\nstudy, two multi-layer neural networks, which are designed for the prediction\nof Boston Housing price, are compared layer by layer by using the proposed\nsmoothness index.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 14:16:10 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Kalhor", "Ahmad", "", "University of Tehran, College of Engineering, School of\n  Electrical and Computer Engineering, Tehran, Iran"], ["Saffar", "Mohsen", "", "University of Tehran, College of Engineering, School of\n  Electrical and Computer Engineering, Tehran, Iran"], ["Kheirieh", "Melika", "", "University of Tehran, College of Engineering, School of\n  Electrical and Computer Engineering, Tehran, Iran"], ["Hoseinipoor", "Somayyeh", "", "University of Tehran, College of Engineering, School of\n  Electrical and Computer Engineering, Tehran, Iran"], ["Araabi", "Babak N.", "", "University of Tehran, College of Engineering, School of\n  Electrical and Computer Engineering, Tehran, Iran"]]}, {"id": "1906.05165", "submitter": "Jiahua Xu", "authors": "Zhibo Chen, Jiahua Xu, Chaoyi Lin and Wei Zhou", "title": "Stereoscopic Omnidirectional Image Quality Assessment Based on\n  Predictive Coding Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective quality assessment of stereoscopic omnidirectional images is a\nchallenging problem since it is influenced by multiple aspects such as\nprojection deformation, field of view (FoV) range, binocular vision, visual\ncomfort, etc. Existing studies show that classic 2D or 3D image quality\nassessment (IQA) metrics are not able to perform well for stereoscopic\nomnidirectional images. However, very few research works have focused on\nevaluating the perceptual visual quality of omnidirectional images, especially\nfor stereoscopic omnidirectional images. In this paper, based on the predictive\ncoding theory of the human vision system (HVS), we propose a stereoscopic\nomnidirectional image quality evaluator (SOIQE) to cope with the\ncharacteristics of 3D 360-degree images. Two modules are involved in SOIQE:\npredictive coding theory based binocular rivalry module and multi-view fusion\nmodule. In the binocular rivalry module, we introduce predictive coding theory\nto simulate the competition between high-level patterns and calculate the\nsimilarity and rivalry dominance to obtain the quality scores of viewport\nimages. Moreover, we develop the multi-view fusion module to aggregate the\nquality scores of viewport images with the help of both content weight and\nlocation weight. The proposed SOIQE is a parametric model without necessary of\nregression learning, which ensures its interpretability and generalization\nperformance. Experimental results on our published stereoscopic omnidirectional\nimage quality assessment database (SOLID) demonstrate that our proposed SOIQE\nmethod outperforms state-of-the-art metrics. Furthermore, we also verify the\neffectiveness of each proposed module on both public stereoscopic image\ndatasets and panoramic image datasets.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 14:25:28 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Chen", "Zhibo", ""], ["Xu", "Jiahua", ""], ["Lin", "Chaoyi", ""], ["Zhou", "Wei", ""]]}, {"id": "1906.05186", "submitter": "Spyros Gidaris", "authors": "Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick P\\'erez,\n  Matthieu Cord", "title": "Boosting Few-Shot Visual Learning with Self-Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning and self-supervised learning address different facets of\nthe same problem: how to train a model with little or no labeled data. Few-shot\nlearning aims for optimization methods and models that can learn efficiently to\nrecognize patterns in the low data regime. Self-supervised learning focuses\ninstead on unlabeled data and looks into it for the supervisory signal to feed\nhigh capacity deep neural networks. In this work we exploit the complementarity\nof these two domains and propose an approach for improving few-shot learning\nthrough self-supervision. We use self-supervision as an auxiliary task in a\nfew-shot learning pipeline, enabling feature extractors to learn richer and\nmore transferable visual representations while still using few annotated\nsamples. Through self-supervision, our approach can be naturally extended\ntowards using diverse unlabeled data from other datasets in the few-shot\nsetting. We report consistent improvements across an array of architectures,\ndatasets and self-supervision techniques.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 14:57:21 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Gidaris", "Spyros", ""], ["Bursuc", "Andrei", ""], ["Komodakis", "Nikos", ""], ["P\u00e9rez", "Patrick", ""], ["Cord", "Matthieu", ""]]}, {"id": "1906.05190", "submitter": "Xin Li", "authors": "Xin Li, Rui Cao, Dongxiao Zhu", "title": "Vispi: Automatic Visual Perception and Interpretation of Chest X-rays", "comments": "In the proceeding of Medical Imaging with Deep Learning (MIDL-20)", "journal-ref": null, "doi": null, "report-no": "MIDL/2020/ExtendedAbstract/otswIbmgYA", "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical imaging contains the essential information for rendering diagnostic\nand treatment decisions. Inspecting (visual perception) and interpreting image\nto generate a report are tedious clinical routines for a radiologist where\nautomation is expected to greatly reduce the workload. Despite rapid\ndevelopment of natural image captioning, computer-aided medical image visual\nperception and interpretation remain a challenging task, largely due to the\nlack of high-quality annotated image-report pairs and tailor-made generative\nmodels for sufficient extraction and exploitation of localized semantic\nfeatures, particularly those associated with abnormalities. To tackle these\nchallenges, we present Vispi, an automatic medical image interpretation system,\nwhich first annotates an image via classifying and localizing common thoracic\ndiseases with visual support and then followed by report generation from an\nattentive LSTM model. Analyzing an open IU X-ray dataset, we demonstrate a\nsuperior performance of Vispi in disease classification, localization and\nreport generation using automatic performance evaluation metrics ROUGE and\nCIDEr.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 15:01:31 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 07:26:27 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 15:53:06 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Li", "Xin", ""], ["Cao", "Rui", ""], ["Zhu", "Dongxiao", ""]]}, {"id": "1906.05199", "submitter": "Tatiana Tommasi", "authors": "Silvia Bucci, Antonio D'Innocente, Tatiana Tommasi", "title": "Tackling Partial Domain Adaptation with Self-Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation approaches have shown promising results in reducing the\nmarginal distribution difference among visual domains. They allow to train\nreliable models that work over datasets of different nature (photos, paintings\netc), but they still struggle when the domains do not share an identical label\nspace. In the partial domain adaptation setting, where the target covers only a\nsubset of the source classes, it is challenging to reduce the domain gap\nwithout incurring in negative transfer. Many solutions just keep the standard\ndomain adaptation techniques by adding heuristic sample weighting strategies.\nIn this work we show how the self-supervisory signal obtained from the spatial\nco-location of patches can be used to define a side task that supports\nadaptation regardless of the exact label sharing condition across domains. We\nbuild over a recent work that introduced a jigsaw puzzle task for domain\ngeneralization: we describe how to reformulate this approach for partial domain\nadaptation and we show how it boosts existing adaptive solutions when combined\nwith them. The obtained experimental results on three datasets supports the\neffectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 15:16:31 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Bucci", "Silvia", ""], ["D'Innocente", "Antonio", ""], ["Tommasi", "Tatiana", ""]]}, {"id": "1906.05202", "submitter": "Chia-Wen Kuo", "authors": "Chia-Wen Kuo, Chih-Yao Ma, Jia-Bin Huang, Zsolt Kira", "title": "Manifold Graph with Learned Prototypes for Semi-Supervised Image\n  Classification", "comments": "Project site:\n  https://sites.google.com/view/manifold-graph-with-prototypes/home", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in semi-supervised learning methods rely on estimating the\ncategories of unlabeled data using a model trained on the labeled data\n(pseudo-labeling) and using the unlabeled data for various consistency-based\nregularization. In this work, we propose to explicitly leverage the structure\nof the data manifold based on a Manifold Graph constructed over the image\ninstances within the feature space. Specifically, we propose an architecture\nbased on graph networks that jointly optimizes feature extraction, graph\nconnectivity, and feature propagation and aggregation to unlabeled data in an\nend-to-end manner. Further, we present a novel Prototype Generator for\nproducing a diverse set of prototypes that compactly represent each category,\nwhich supports feature propagation. To evaluate our method, we first contribute\na strong baseline that combines two consistency-based regularizers that already\nachieves state-of-the-art results especially with fewer labels. We then show\nthat when combined with these regularizers, the proposed method facilitates the\npropagation of information from generated prototypes to image data to further\nimprove results. We provide extensive qualitative and quantitative experimental\nresults on semi-supervised benchmarks demonstrating the improvements arising\nfrom our design and show that our method achieves state-of-the-art performance\nwhen compared with existing methods using a single model and comparable with\nensemble methods. Specifically, we achieve error rates of 3.35% on SVHN, 8.27%\non CIFAR-10, and 33.83% on CIFAR-100. With much fewer labels, we surpass the\nstate of the arts by significant margins of 41% relative error decrease on\naverage.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 15:18:36 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 02:46:21 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Kuo", "Chia-Wen", ""], ["Ma", "Chih-Yao", ""], ["Huang", "Jia-Bin", ""], ["Kira", "Zsolt", ""]]}, {"id": "1906.05203", "submitter": "Ines Rieger", "authors": "Ines Rieger, Thomas Hauenstein, Sebastian Hettenkofer, and Jens-Uwe\n  Garbas", "title": "Towards Real-Time Head Pose Estimation: Exploring Parameter-Reduced\n  Residual Networks on In-the-wild Datasets", "comments": "32nd International Conference on Industrial, Engineering & Other\n  Applications of Applied Intelligent Systems (IEA/AIE 2019)", "journal-ref": "Lecture Notes in Computer Science 11606 (2019) 123-134", "doi": "10.1007/978-3-030-22999-3_12", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Head poses are a key component of human bodily communication and thus a\ndecisive element of human-computer interaction. Real-time head pose estimation\nis crucial in the context of human-robot interaction or driver assistance\nsystems. The most promising approaches for head pose estimation are based on\nConvolutional Neural Networks (CNNs). However, CNN models are often too complex\nto achieve real-time performance. To face this challenge, we explore a popular\nsubgroup of CNNs, the Residual Networks (ResNets) and modify them in order to\nreduce their number of parameters. The ResNets are modifed for different image\nsizes including low-resolution images and combined with a varying number of\nlayers. They are trained on in-the-wild datasets to ensure real-world\napplicability. As a result, we demonstrate that the performance of the ResNets\ncan be maintained while reducing the number of parameters. The modified ResNets\nachieve state-of-the-art accuracy and provide fast inference for real-time\napplicability.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 15:19:12 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 07:38:41 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Rieger", "Ines", ""], ["Hauenstein", "Thomas", ""], ["Hettenkofer", "Sebastian", ""], ["Garbas", "Jens-Uwe", ""]]}, {"id": "1906.05226", "submitter": "Ramakanth Pasunuru", "authors": "Ramakanth Pasunuru, Mohit Bansal", "title": "Continual and Multi-Task Architecture Search", "comments": "ACL 2019 (12 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Architecture search is the process of automatically learning the neural model\nor cell structure that best suits the given task. Recently, this approach has\nshown promising performance improvements (on language modeling and image\nclassification) with reasonable training speed, using a weight sharing strategy\ncalled Efficient Neural Architecture Search (ENAS). In our work, we first\nintroduce a novel continual architecture search (CAS) approach, so as to\ncontinually evolve the model parameters during the sequential training of\nseveral tasks, without losing performance on previously learned tasks (via\nblock-sparsity and orthogonality constraints), thus enabling life-long\nlearning. Next, we explore a multi-task architecture search (MAS) approach over\nENAS for finding a unified, single cell structure that performs well across\nmultiple tasks (via joint controller rewards), and hence allows more\ngeneralizable transfer of the cell structure knowledge to an unseen new task.\nWe empirically show the effectiveness of our sequential continual learning and\nparallel multi-task learning based architecture search approaches on diverse\nsentence-pair classification tasks (GLUE) and multimodal-generation based video\ncaptioning tasks. Further, we present several ablations and analyses on the\nlearned cell structures.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 16:01:35 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Pasunuru", "Ramakanth", ""], ["Bansal", "Mohit", ""]]}, {"id": "1906.05229", "submitter": "Junho Jo", "authors": "Junho Jo, Hyung Il Koo, Jae Woong Soh, Nam Ik Cho", "title": "Handwritten Text Segmentation via End-to-End Learning of Convolutional\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new handwritten text segmentation method by training a\nconvolutional neural network (CNN) in an end-to-end manner. Many conventional\nmethods addressed this problem by extracting connected components and then\nclassifying them. However, this two-step approach has limitations when\nhandwritten components and machine-printed parts are overlapping. Unlike\nconventional methods, we develop an end-to-end deep CNN for this problem, which\ndoes not need any preprocessing steps. Since there is no publicly available\ndataset for this goal and pixel-wise annotations are time-consuming and costly,\nwe also propose a data synthesis algorithm that generates realistic training\nsamples. For training our network, we develop a cross-entropy based loss\nfunction that addresses the imbalance problems. Experimental results on\nsynthetic and real images show the effectiveness of the proposed method.\nSpecifically, the proposed network has been trained solely on synthetic images,\nnevertheless the removal of handwritten text in real documents improves OCR\nperformance from 71.13% to 92.50%, showing the generalization performance of\nour network and synthesized images.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 16:03:57 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Jo", "Junho", ""], ["Koo", "Hyung Il", ""], ["Soh", "Jae Woong", ""], ["Cho", "Nam Ik", ""]]}, {"id": "1906.05251", "submitter": "Yuxiang Dai", "authors": "Yuxiang Dai and Peixian Zhuang", "title": "Compressed Sensing MRI via a Multi-scale Dilated Residual Convolution\n  Network", "comments": "27 pages and 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging (MRI) reconstruction is an active inverse problem\nwhich can be addressed by conventional compressed sensing (CS) MRI algorithms\nthat exploit the sparse nature of MRI in an iterative optimization-based\nmanner. However, two main drawbacks of iterative optimization-based CSMRI\nmethods are time-consuming and are limited in model capacity. Meanwhile, one\nmain challenge for recent deep learning-based CSMRI is the trade-off between\nmodel performance and network size. To address the above issues, we develop a\nnew multi-scale dilated network for MRI reconstruction with high speed and\noutstanding performance. Comparing to convolutional kernels with same receptive\nfields, dilated convolutions reduce network parameters with smaller kernels and\nexpand receptive fields of kernels to obtain almost same information. To\nmaintain the abundance of features, we present global and local residual\nlearnings to extract more image edges and details. Then we utilize\nconcatenation layers to fuse multi-scale features and residual learnings for\nbetter reconstruction. Compared with several non-deep and deep learning CSMRI\nalgorithms, the proposed method yields better reconstruction accuracy and\nnoticeable visual improvements. In addition, we perform the noisy setting to\nverify the model stability, and then extend the proposed model on a MRI\nsuper-resolution task.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 09:05:57 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Dai", "Yuxiang", ""], ["Zhuang", "Peixian", ""]]}, {"id": "1906.05261", "submitter": "Vicky Kalogeiton", "authors": "Manuel J. Marin-Jimenez, Vicky Kalogeiton, Pablo Medina-Suarez, Andrew\n  Zisserman", "title": "LAEO-Net: revisiting people Looking At Each Other in videos", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing the `mutual gaze' of people is essential for understanding and\ninterpreting the social interactions between them. To this end, this paper\naddresses the problem of detecting people Looking At Each Other (LAEO) in video\nsequences. For this purpose, we propose LAEO-Net, a new deep CNN for\ndetermining LAEO in videos. In contrast to previous works, LAEO-Net takes\nspatio-temporal tracks as input and reasons about the whole track. It consists\nof three branches, one for each character's tracked head and one for their\nrelative position. Moreover, we introduce two new LAEO datasets: UCO-LAEO and\nAVA-LAEO. A thorough experimental evaluation demonstrates the ability of\nLAEONet to successfully determine if two people are LAEO and the temporal\nwindow where it happens. Our model achieves state-of-the-art results on the\nexisting TVHID-LAEO video dataset, significantly outperforming previous\napproaches. Finally, we apply LAEO-Net to social network analysis, where we\nautomatically infer the social relationship between pairs of people based on\nthe frequency and duration that they LAEO.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 17:40:38 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Marin-Jimenez", "Manuel J.", ""], ["Kalogeiton", "Vicky", ""], ["Medina-Suarez", "Pablo", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1906.05268", "submitter": "Jeff Yan", "authors": "Aur\\'elien Bourquard, Jeff Yan", "title": "Differential Imaging Forensics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce some new forensics based on differential imaging, where a novel\ncategory of visual evidence created via subtle interactions of light with a\nscene, such as dim reflections, can be computationally extracted and amplified\nfrom an image of interest through a comparative analysis with an additional\nreference baseline image acquired under similar conditions. This paradigm of\ndifferential imaging forensics (DIF) enables forensic examiners for the first\ntime to retrieve the said visual evidence that is readily available in an image\nor video footage but would otherwise remain faint or even invisible to a human\nobserver. We demonstrate the relevance and effectiveness of our approach\nthrough practical experiments. We also show that DIF provides a novel method\nfor detecting forged images and video clips, including deep fakes.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 17:48:47 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Bourquard", "Aur\u00e9lien", ""], ["Yan", "Jeff", ""]]}, {"id": "1906.05272", "submitter": "Oisin Mac Aodha", "authors": "Oisin Mac Aodha, Elijah Cole, Pietro Perona", "title": "Presence-Only Geographical Priors for Fine-Grained Image Classification", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Appearance information alone is often not sufficient to accurately\ndifferentiate between fine-grained visual categories. Human experts make use of\nadditional cues such as where, and when, a given image was taken in order to\ninform their final decision. This contextual information is readily available\nin many online image collections but has been underutilized by existing image\nclassifiers that focus solely on making predictions based on the image\ncontents.\n  We propose an efficient spatio-temporal prior, that when conditioned on a\ngeographical location and time, estimates the probability that a given object\ncategory occurs at that location. Our prior is trained from presence-only\nobservation data and jointly models object categories, their spatio-temporal\ndistributions, and photographer biases. Experiments performed on multiple\nchallenging image classification datasets show that combining our prior with\nthe predictions from image classifiers results in a large improvement in final\nclassification performance.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 17:54:23 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 23:46:21 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 06:46:10 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Mac Aodha", "Oisin", ""], ["Cole", "Elijah", ""], ["Perona", "Pietro", ""]]}, {"id": "1906.05284", "submitter": "Shady Abu Hussein", "authors": "Shady Abu Hussein, Tom Tirer and Raja Giryes", "title": "Image-Adaptive GAN based Reconstruction", "comments": "Accepted to AAAI 2020. Code available at\n  https://github.com/shadyabh/IAGAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years, there has been a significant improvement in the quality\nof samples produced by (deep) generative models such as variational\nauto-encoders and generative adversarial networks. However, the representation\ncapabilities of these methods still do not capture the full distribution for\ncomplex classes of images, such as human faces. This deficiency has been\nclearly observed in previous works that use pre-trained generative models to\nsolve imaging inverse problems. In this paper, we suggest to mitigate the\nlimited representation capabilities of generators by making them image-adaptive\nand enforcing compliance of the restoration with the observations via\nback-projections. We empirically demonstrate the advantages of our proposed\napproach for image super-resolution and compressed sensing.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 17:39:43 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 16:42:36 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Hussein", "Shady Abu", ""], ["Tirer", "Tom", ""], ["Giryes", "Raja", ""]]}, {"id": "1906.05332", "submitter": "Xiuye Gu", "authors": "Xiuye Gu and Yijie Wang and Chongruo wu and Yong-Jae lee and Panqu\n  Wang", "title": "HPLFlowNet: Hierarchical Permutohedral Lattice FlowNet for Scene Flow\n  Estimation on Large-scale Point Clouds", "comments": null, "journal-ref": "CVPR 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel deep neural network architecture for end-to-end scene flow\nestimation that directly operates on large-scale 3D point clouds. Inspired by\nBilateral Convolutional Layers (BCL), we propose novel DownBCL, UpBCL, and\nCorrBCL operations that restore structural information from unstructured point\nclouds, and fuse information from two consecutive point clouds. Operating on\ndiscrete and sparse permutohedral lattice points, our architectural design is\nparsimonious in computational cost. Our model can efficiently process a pair of\npoint cloud frames at once with a maximum of 86K points per frame. Our approach\nachieves state-of-the-art performance on the FlyingThings3D and KITTI Scene\nFlow 2015 datasets. Moreover, trained on synthetic data, our approach shows\ngreat generalization ability on real-world data and on different point\ndensities without fine-tuning.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 19:12:58 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Gu", "Xiuye", ""], ["Wang", "Yijie", ""], ["wu", "Chongruo", ""], ["lee", "Yong-Jae", ""], ["Wang", "Panqu", ""]]}, {"id": "1906.05352", "submitter": "Zhoutong Wang", "authors": "Qianhui Liang, Zhoutong Wang", "title": "Uncovering Dominant Social Class in Neighborhoods through Building\n  Footprints: A Case Study of Residential Zones in Massachusetts using Computer\n  Vision", "comments": "Publishing as conference proceeding of 16th International Conference\n  on Computers in Urban Planning and Urban Management", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In urban theory, urban form is related to social and economic status. This\npaper explores to uncover zip-code level income through urban form by analyzing\nfigure-ground map, a simple, prevailing and precise representation of urban\nform in the field of urban study. Deep learning in computer vision enables such\nrepresentation maps to be studied at a large scale. We propose to train a DCNN\nmodel to identify and uncover the internal bridge between social class and\nurban form. Further, using hand-crafted informative visual features related\nwith urban form properties (building size, building density, etc.), we apply a\nrandom forest classifier to interpret how morphological properties are related\nwith social class.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 19:47:09 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Liang", "Qianhui", ""], ["Wang", "Zhoutong", ""]]}, {"id": "1906.05360", "submitter": "Mason Chen", "authors": "Mason T. Chen, Faisal Mahmood, Jordan A. Sweer, and Nicholas J. Durr", "title": "GANPOP: Generative Adversarial Network Prediction of Optical Properties\n  from Single Snapshot Wide-field Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep learning framework for wide-field, content-aware estimation\nof absorption and scattering coefficients of tissues, called Generative\nAdversarial Network Prediction of Optical Properties (GANPOP). Spatial\nfrequency domain imaging is used to obtain ground-truth optical properties from\nin vivo human hands, freshly resected human esophagectomy samples and\nhomogeneous tissue phantoms. Images of objects with either flat-field or\nstructured illumination are paired with registered optical property maps and\nare used to train conditional generative adversarial networks that estimate\noptical properties from a single input image. We benchmark this approach by\ncomparing GANPOP to a single-snapshot optical property (SSOP) technique, using\na normalized mean absolute error (NMAE) metric. In human gastrointestinal\nspecimens, GANPOP estimates both reduced scattering and absorption coefficients\nat 660 nm from a single 0.2/mm spatial frequency illumination image with 58%\nhigher accuracy than SSOP. When applied to both in vivo and ex vivo swine\ntissues, a GANPOP model trained solely on human specimens and phantoms\nestimates optical properties with approximately 43% improvement over SSOP,\nindicating adaptability to sample variety. Moreover, we demonstrate that GANPOP\nestimates optical properties from flat-field illumination images with similar\nerror to SSOP, which requires structured-illumination. Given a training set\nthat appropriately spans the target domain, GANPOP has the potential to enable\nrapid and accurate wide-field measurements of optical properties, even from\nconventional imaging systems with flat-field illumination.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 19:55:49 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 18:04:02 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Chen", "Mason T.", ""], ["Mahmood", "Faisal", ""], ["Sweer", "Jordan A.", ""], ["Durr", "Nicholas J.", ""]]}, {"id": "1906.05372", "submitter": "Kiat Chuan Tan", "authors": "Kiat Chuan Tan, Yulong Liu, Barbara Ambrose, Melissa Tulig and Serge\n  Belongie", "title": "The Herbarium Challenge 2019 Dataset", "comments": "Part of the 6th Fine-Grained Visual Categorization Workshop (FGVC6)\n  at CVPR 2019. Dataset available at\n  https://github.com/visipedia/herbarium_comp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Herbarium sheets are invaluable for botanical research, and considerable time\nand effort is spent by experts to label and identify specimens on them. In view\nof recent advances in computer vision and deep learning, developing an\nautomated approach to help experts identify specimens could significantly\naccelerate research in this area. Whereas most existing botanical datasets\ncomprise photos of specimens in the wild, herbarium sheets exhibit dried\nspecimens, which poses new challenges. We present a challenge dataset of\nherbarium sheet images labeled by experts, with the intent of facilitating the\ndevelopment of automated identification techniques for this challenging\nscenario.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 20:42:24 GMT"}, {"version": "v2", "created": "Sat, 15 Jun 2019 06:16:28 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Tan", "Kiat Chuan", ""], ["Liu", "Yulong", ""], ["Ambrose", "Barbara", ""], ["Tulig", "Melissa", ""], ["Belongie", "Serge", ""]]}, {"id": "1906.05378", "submitter": "Leo Furkan Isikdogan", "authors": "Leo F. Isikdogan, Timo Gerasimow, Gilad Michael", "title": "Eye Contact Correction using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a typical video conferencing setup, it is hard to maintain eye contact\nduring a call since it requires looking into the camera rather than the\ndisplay. We propose an eye contact correction model that restores the eye\ncontact regardless of the relative position of the camera and display. Unlike\nprevious solutions, our model redirects the gaze from an arbitrary direction to\nthe center without requiring a redirection angle or camera/display/user\ngeometry as inputs. We use a deep convolutional neural network that inputs a\nmonocular image and produces a vector field and a brightness map to correct the\ngaze. We train this model in a bi-directional way on a large set of\nsynthetically generated photorealistic images with perfect labels. The learned\nmodel is a robust eye contact corrector which also predicts the input gaze\nimplicitly at no additional cost. Our system is primarily designed to improve\nthe quality of video conferencing experience. Therefore, we use a set of\ncontrol mechanisms to prevent creepy results and to ensure a smooth and natural\nvideo conferencing experience. The entire eye contact correction system runs\nend-to-end in real-time on a commodity CPU and does not require any dedicated\nhardware, making our solution feasible for a variety of devices.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 21:15:12 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2019 17:59:44 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Isikdogan", "Leo F.", ""], ["Gerasimow", "Timo", ""], ["Michael", "Gilad", ""]]}, {"id": "1906.05388", "submitter": "Mohammad Rastegari", "authors": "Mohammad Mahdi Derakhshani, Saeed Masoudnia, Amir Hossein Shaker, Omid\n  Mersa, Mohammad Amin Sadeghi, Mohammad Rastegari, Babak N. Araabi", "title": "Assisted Excitation of Activations: A Learning Technique to Improve\n  Object Detectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple and effective learning technique that significantly\nimproves mAP of YOLO object detectors without compromising their speed. During\nnetwork training, we carefully feed in localization information. We excite\ncertain activations in order to help the network learn to better localize. In\nthe later stages of training, we gradually reduce our assisted excitation to\nzero. We reached a new state-of-the-art in the speed-accuracy trade-off. Our\ntechnique improves the mAP of YOLOv2 by 3.8% and mAP of YOLOv3 by 2.2% on\nMSCOCO dataset.This technique is inspired from curriculum learning. It is\nsimple and effective and it is applicable to most single-stage object\ndetectors.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 21:32:07 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Derakhshani", "Mohammad Mahdi", ""], ["Masoudnia", "Saeed", ""], ["Shaker", "Amir Hossein", ""], ["Mersa", "Omid", ""], ["Sadeghi", "Mohammad Amin", ""], ["Rastegari", "Mohammad", ""], ["Araabi", "Babak N.", ""]]}, {"id": "1906.05404", "submitter": "Xiaoling Hu Mr", "authors": "Xiaoling Hu, Li Fuxin, Dimitris Samaras and Chao Chen", "title": "Topology-Preserving Deep Image Segmentation", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation algorithms are prone to make topological errors on fine-scale\nstructures, e.g., broken connections. We propose a novel method that learns to\nsegment with correct topology. In particular, we design a continuous-valued\nloss function that enforces a segmentation to have the same topology as the\nground truth, i.e., having the same Betti number. The proposed\ntopology-preserving loss function is differentiable and we incorporate it into\nend-to-end training of a deep neural network. Our method achieves much better\nperformance on the Betti number error, which directly accounts for the\ntopological correctness. It also performs superiorly on other topology-relevant\nmetrics, e.g., the Adjusted Rand Index and the Variation of Information. We\nillustrate the effectiveness of the proposed method on a broad spectrum of\nnatural and biomedical datasets.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 22:12:44 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Hu", "Xiaoling", ""], ["Fuxin", "Li", ""], ["Samaras", "Dimitris", ""], ["Chen", "Chao", ""]]}, {"id": "1906.05423", "submitter": "Natasa Tagasovska", "authors": "Natasa Tagasovska, Damien Ackerer, Thibault Vatter", "title": "Copulas as High-Dimensional Generative Models: Vine Copula Autoencoders", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 32, pages:\n  6525--6537, year: 2019", "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the vine copula autoencoder (VCAE), a flexible generative model\nfor high-dimensional distributions built in a straightforward three-step\nprocedure.\n  First, an autoencoder (AE) compresses the data into a lower dimensional\nrepresentation. Second, the multivariate distribution of the encoded data is\nestimated with vine copulas. Third, a generative model is obtained by combining\nthe estimated distribution with the decoder part of the AE. As such, the\nproposed approach can transform any already trained AE into a flexible\ngenerative model at a low computational cost. This is an advantage over\nexisting generative models such as adversarial networks and variational AEs\nwhich can be difficult to train and can impose strong assumptions on the latent\nspace. Experiments on MNIST, Street View House Numbers and Large-Scale\nCelebFaces Attributes datasets show that VCAEs can achieve competitive results\nto standard baselines.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 23:29:17 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 12:12:33 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Tagasovska", "Natasa", ""], ["Ackerer", "Damien", ""], ["Vatter", "Thibault", ""]]}, {"id": "1906.05441", "submitter": "Riddhish Bhalodia", "authors": "Riddhish Bhalodia, Shireen Elhabian, Ladislav Kavan, Ross Whitaker", "title": "CoopSubNet: Cooperating Subnetwork for Data-Driven Regularization of\n  Deep Networks under Limited Training Budgets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks are an integral part of the current machine learning paradigm.\nTheir inherent ability to learn complex functional mappings between data and\nvarious target variables, while discovering hidden, task-driven features, makes\nthem a powerful technology in a wide variety of applications. Nonetheless, the\nsuccess of these networks typically relies on the availability of sufficient\ntraining data to optimize a large number of free parameters while avoiding\noverfitting, especially for networks with large capacity. In scenarios with\nlimited training budgets, e.g., supervised tasks with limited labeled samples,\nseveral generic and/or task-specific regularization techniques, including data\naugmentation, have been applied to improve the generalization of deep\nnetworks.Typically such regularizations are introduced independently of that\ndata or training scenario, and must therefore be tuned, tested, and modified to\nmeet the needs of a particular network. In this paper, we propose a novel\nregularization framework that is driven by the population-level statistics of\nthe feature space to be learned. The regularization is in the form of a\n\\textbf{cooperating subnetwork}, which is an auto-encoder architecture attached\nto the feature space and trained in conjunction with the primary network. We\nintroduce the architecture and training methodology and demonstrate the\neffectiveness of the proposed cooperative network-based regularization in a\nvariety of tasks and architectures from the literature. Our code is freely\navailable at \\url{https://github.com/riddhishb/CoopSubNet\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 01:13:53 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Bhalodia", "Riddhish", ""], ["Elhabian", "Shireen", ""], ["Kavan", "Ladislav", ""], ["Whitaker", "Ross", ""]]}, {"id": "1906.05478", "submitter": "Sreyas Mohan", "authors": "Sreyas Mohan, Zahra Kadkhodaie, Eero P. Simoncelli and Carlos\n  Fernandez-Granda", "title": "Robust and interpretable blind image denoising via bias-free\n  convolutional neural networks", "comments": "Published as conference paper in ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks often append additive constant (\"bias\") terms to\ntheir convolution operations, enabling a richer repertoire of functional\nmappings. Biases are also used to facilitate training, by subtracting mean\nresponse over batches of training images (a component of \"batch\nnormalization\"). Recent state-of-the-art blind denoising methods (e.g., DnCNN)\nseem to require these terms for their success. Here, however, we show that\nthese networks systematically overfit the noise levels for which they are\ntrained: when deployed at noise levels outside the training range, performance\ndegrades dramatically. In contrast, a bias-free architecture -- obtained by\nremoving the constant terms in every layer of the network, including those used\nfor batch normalization-- generalizes robustly across noise levels, while\npreserving state-of-the-art performance within the training range. Locally, the\nbias-free network acts linearly on the noisy image, enabling direct analysis of\nnetwork behavior via standard linear-algebraic tools. These analyses provide\ninterpretations of network functionality in terms of nonlinear adaptive\nfiltering, and projection onto a union of low-dimensional subspaces, connecting\nthe learning-based method to more traditional denoising methodology.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 04:48:21 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 03:18:09 GMT"}, {"version": "v3", "created": "Sat, 8 Feb 2020 05:55:05 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Mohan", "Sreyas", ""], ["Kadkhodaie", "Zahra", ""], ["Simoncelli", "Eero P.", ""], ["Fernandez-Granda", "Carlos", ""]]}, {"id": "1906.05480", "submitter": "Jae-Seok Choi", "authors": "Jae-Seok Choi, Yongwoo Kim and Munchurl Kim", "title": "S3: A Spectral-Spatial Structure Loss for Pan-Sharpening Networks", "comments": "Accepted for publication in IEEE Geoscience and Remote Sensing\n  Letters", "journal-ref": null, "doi": "10.1109/LGRS.2019.2934493", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many deep-learning-based pan-sharpening methods have been proposed\nfor generating high-quality pan-sharpened (PS) satellite images. These methods\nfocused on various types of convolutional neural network (CNN) structures,\nwhich were trained by simply minimizing a spectral loss between network outputs\nand the corresponding high-resolution multi-spectral (MS) target images.\nHowever, due to different sensor characteristics and acquisition times,\nhigh-resolution panchromatic (PAN) and low-resolution MS image pairs tend to\nhave large pixel misalignments, especially for moving objects in the images.\nConventional CNNs trained with only the spectral loss with these satellite\nimage datasets often produce PS images of low visual quality including\ndouble-edge artifacts along strong edges and ghosting artifacts on moving\nobjects. In this letter, we propose a novel loss function, called a\nspectral-spatial structure (S3) loss, based on the correlation maps between MS\ntargets and PAN inputs. Our proposed S3 loss can be very effectively utilized\nfor pan-sharpening with various types of CNN structures, resulting in\nsignificant visual improvements on PS images with suppressed artifacts.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 05:02:26 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 01:57:24 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Choi", "Jae-Seok", ""], ["Kim", "Yongwoo", ""], ["Kim", "Munchurl", ""]]}, {"id": "1906.05496", "submitter": "Wufei Ma", "authors": "Elizabeth Kautz, Wufei Ma, Saumyadeep Jana, Arun Devaraj, Vineet\n  Joshi, B\\\"ulent Yener, Daniel Lewis", "title": "An image-driven machine learning approach to kinetic modeling of a\n  discontinuous precipitation reaction", "comments": "30 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.app-ph cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Micrograph quantification is an essential component of several materials\nscience studies. Machine learning methods, in particular convolutional neural\nnetworks, have previously demonstrated performance in image recognition tasks\nacross several disciplines (e.g. materials science, medical imaging, facial\nrecognition). Here, we apply these well-established methods to develop an\napproach to microstructure quantification for kinetic modeling of a\ndiscontinuous precipitation reaction in a case study on the uranium-molybdenum\nsystem. Prediction of material processing history based on image data\n(classification), calculation of area fraction of phases present in the\nmicrographs (segmentation), and kinetic modeling from segmentation results were\nperformed. Results indicate that convolutional neural networks represent\nmicrostructure image data well, and segmentation using the k-means clustering\nalgorithm yields results that agree well with manually annotated images.\nClassification accuracies of original and segmented images are both 94\\% for a\n5-class classification problem. Kinetic modeling results agree well with\npreviously reported data using manual thresholding. The image quantification\nand kinetic modeling approach developed and presented here aims to reduce\nresearcher bias introduced into the characterization process, and allows for\nleveraging information in limited image data sets.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 06:07:57 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Kautz", "Elizabeth", ""], ["Ma", "Wufei", ""], ["Jana", "Saumyadeep", ""], ["Devaraj", "Arun", ""], ["Joshi", "Vineet", ""], ["Yener", "B\u00fclent", ""], ["Lewis", "Daniel", ""]]}, {"id": "1906.05498", "submitter": "Lasitha Piyathilaka Dr", "authors": "Lasitha Piyathilaka, Sarath Kodagoda", "title": "Understanding Human Context in 3D Scenes by Learning Spatial Affordances\n  with Virtual Skeleton Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots are often required to operate in environments where humans are not\npresent, but yet require the human context information for better human-robot\ninteraction. Even when humans are present in the environment, detecting their\npresence in cluttered environments could be challenging. As a solution to this\nproblem, this paper presents the concept of spatial affordance map which learns\nhuman context by looking at geometric features of the environment. Instead of\nobserving real humans to learn human context, it uses virtual human models and\ntheir relationships with the environment to map hidden human affordances in 3D\nscenes by placing virtual skeleton models in 3D scenes with their confidence\nvalues. The spatial affordance map learning problem is formulated as a\nmulti-label classification problem that can be learned using Support Vector\nMachine (SVM) based learners. Experiments carried out in a real 3D scene\ndataset recorded promising results and proved the applicability of\naffordance-map for mapping human context.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 06:21:14 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Piyathilaka", "Lasitha", ""], ["Kodagoda", "Sarath", ""]]}, {"id": "1906.05512", "submitter": "Jingliang Hu", "authors": "Jingliang Hu and Danfeng Hong and Xiao Xiang Zhu", "title": "MIMA: MAPPER-Induced Manifold Alignment for Semi-Supervised Fusion of\n  Optical Image and Polarimetric SAR Data", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2019.2924113", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modal data fusion has recently been shown promise in classification\ntasks in remote sensing. Optical data and radar data, two important yet\nintrinsically different data sources, are attracting more and more attention\nfor potential data fusion. It is already widely known that, a machine learning\nbased methodology often yields excellent performance. However, the methodology\nrelies on a large training set, which is very expensive to achieve in remote\nsensing. The semi-supervised manifold alignment (SSMA), a multi-modal data\nfusion algorithm, has been designed to amplify the impact of an existing\ntraining set by linking labeled data to unlabeled data via unsupervised\ntechniques. In this paper, we explore the potential of SSMA in fusing optical\ndata and polarimetric SAR data, which are multi-sensory data sources.\nFurthermore, we propose a MAPPER-induced manifold alignment (MIMA) for\nsemi-supervised fusion of multi-sensory data sources. Our proposed method\nunites SSMA with MAPPER, which is developed from the emerging topological data\nanalysis (TDA) field. To our best knowledge, this is the first time that SSMA\nhas been applied on fusing optical data and SAR data, and also the first time\nthat TDA has been applied in remote sensing. The conventional SSMA derives a\ntopological structure using k-nearest-neighbor (kNN), while MIMA employs\nMAPPER, which considers the field knowledge and derives a novel topological\nstructure through the spectral clustering in a data-driven fashion. Experiment\nresults on data fusion with respect to land cover land use classification and\nlocal climate zone classification suggest superior performance of MIMA.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 07:24:33 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Hu", "Jingliang", ""], ["Hong", "Danfeng", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "1906.05526", "submitter": "Eytan Lifshitz", "authors": "Eytan Lifshitz, Dani Lischinski", "title": "Illuminant Chromaticity Estimation from Interreflections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable estimation of illuminant chromaticity is crucial for simulating\ncolor constancy and for white balancing digital images. However, estimating\nilluminant chromaticity from a single image is an ill-posed task, in general,\nand existing solutions typically employ a variety of assumptions and\nheuristics. In this paper, we present a new, physically-based, approach for\nestimating illuminant chromaticity from interreflections of light between\ndiffuse surfaces. Our approach assumes that all of the direct illumination in\nthe scene has the same chromaticity, and that at least two areas where\ninterreflections between Lambertian surfaces occur may be detected in the\nimage. No further assumptions or restrictions on the illuminant chromaticty or\nthe shading in the scene are necessary. Our approach is based on representing\ninterreflections as lines in a special 2D color space, and the chromaticity of\nthe illuminant is estimated from the approximate intersection between two or\nmore such lines. Experimental results are reported on a dataset of illumination\nand surface reflectance spectra, as well as on real images we captured. The\nresults indicate that our approach can yield state-of-the-art results when the\ninterreflections are significant enough to be captured by the camera.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 07:50:28 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Lifshitz", "Eytan", ""], ["Lischinski", "Dani", ""]]}, {"id": "1906.05528", "submitter": "Valery Vishnevskiy", "authors": "Valery Vishnevskiy, Richard Rau, Orcun Goksel", "title": "Deep Variational Networks with Exponential Weighting for Learning\n  Computed Tomography", "comments": "Accepted to MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tomographic image reconstruction is relevant for many medical imaging\nmodalities including X-ray, ultrasound (US) computed tomography (CT) and\nphotoacoustics, for which the access to full angular range tomographic\nprojections might be not available in clinical practice due to physical or time\nconstraints. Reconstruction from incomplete data in low signal-to-noise ratio\nregime is a challenging and ill-posed inverse problem that usually leads to\nunsatisfactory image quality. While informative image priors may be learned\nusing generic deep neural network architectures, the artefacts caused by an\nill-conditioned design matrix often have global spatial support and cannot be\nefficiently filtered out by means of convolutions. In this paper we propose to\nlearn an inverse mapping in an end-to-end fashion via unrolling optimization\niterations of a prototypical reconstruction algorithm. We herein introduce a\nnetwork architecture that performs filtering jointly in both sinogram and\nspatial domains. To efficiently train such deep network we propose a novel\nregularization approach based on deep exponential weighting. Experiments on US\nand X-ray CT data show that our proposed method is qualitatively and\nquantitatively superior to conventional non-linear reconstruction methods as\nwell as state-of-the-art deep networks for image reconstruction. Fast inference\ntime of the proposed algorithm allows for sophisticated reconstructions in\nreal-time critical settings, demonstrated with US SoS imaging of an ex vivo\nbovine phantom.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 07:54:51 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Vishnevskiy", "Valery", ""], ["Rau", "Richard", ""], ["Goksel", "Orcun", ""]]}, {"id": "1906.05571", "submitter": "Ting Yao", "authors": "Zhaofan Qiu and Ting Yao and Chong-Wah Ngo and Xinmei Tian and Tao Mei", "title": "Learning Spatio-Temporal Representation with Local and Global Diffusion", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) have been regarded as a powerful class of\nmodels for visual recognition problems. Nevertheless, the convolutional filters\nin these networks are local operations while ignoring the large-range\ndependency. Such drawback becomes even worse particularly for video\nrecognition, since video is an information-intensive media with complex\ntemporal variations. In this paper, we present a novel framework to boost the\nspatio-temporal representation learning by Local and Global Diffusion (LGD).\nSpecifically, we construct a novel neural network architecture that learns the\nlocal and global representations in parallel. The architecture is composed of\nLGD blocks, where each block updates local and global features by modeling the\ndiffusions between these two representations. Diffusions effectively interact\ntwo aspects of information, i.e., localized and holistic, for more powerful way\nof representation learning. Furthermore, a kernelized classifier is introduced\nto combine the representations from two aspects for video recognition. Our LGD\nnetworks achieve clear improvements on the large-scale Kinetics-400 and\nKinetics-600 video classification datasets against the best competitors by 3.5%\nand 0.7%. We further examine the generalization of both the global and local\nrepresentations produced by our pre-trained LGD networks on four different\nbenchmarks for video action recognition and spatio-temporal action detection\ntasks. Superior performances over several state-of-the-art techniques on these\nbenchmarks are reported. Code is available at:\nhttps://github.com/ZhaofanQiu/local-and-global-diffusion-networks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 09:41:00 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Qiu", "Zhaofan", ""], ["Yao", "Ting", ""], ["Ngo", "Chong-Wah", ""], ["Tian", "Xinmei", ""], ["Mei", "Tao", ""]]}, {"id": "1906.05586", "submitter": "Shuyuan Li", "authors": "Shuyuan Li, Jianguo Li, Hanlin Tang, Rui Qian, Weiyao Lin", "title": "ATRW: A Benchmark for Amur Tiger Re-identification in the Wild", "comments": "ACM Multimedia (MM) 2020", "journal-ref": null, "doi": "10.1145/3394171.3413569", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Monitoring the population and movements of endangered species is an important\ntask to wildlife conversation. Traditional tagging methods do not scale to\nlarge populations, while applying computer vision methods to camera sensor data\nrequires re-identification (re-ID) algorithms to obtain accurate counts and\nmoving trajectory of wildlife. However, existing re-ID methods are largely\ntargeted at persons and cars, which have limited pose variations and\nconstrained capture environments. This paper tries to fill the gap by\nintroducing a novel large-scale dataset, the Amur Tiger Re-identification in\nthe Wild (ATRW) dataset. ATRW contains over 8,000 video clips from 92 Amur\ntigers, with bounding box, pose keypoint, and tiger identity annotations. In\ncontrast to typical re-ID datasets, the tigers are captured in a diverse set of\nunconstrained poses and lighting conditions. We demonstrate with a set of\nbaseline algorithms that ATRW is a challenging dataset for re-ID. Lastly, we\npropose a novel method for tiger re-identification, which introduces precise\npose parts modeling in deep neural networks to handle large pose variation of\ntigers, and reaches notable performance improvement over existing re-ID\nmethods. The dataset is public available at https://cvwc2019.github.io/ .\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 10:16:30 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 08:25:54 GMT"}, {"version": "v3", "created": "Tue, 27 Oct 2020 10:20:17 GMT"}, {"version": "v4", "created": "Thu, 29 Oct 2020 11:04:38 GMT"}, {"version": "v5", "created": "Sat, 31 Oct 2020 17:46:09 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Li", "Shuyuan", ""], ["Li", "Jianguo", ""], ["Tang", "Hanlin", ""], ["Qian", "Rui", ""], ["Lin", "Weiyao", ""]]}, {"id": "1906.05596", "submitter": "Mithun Das Gupta", "authors": "Sudhir Kumar and Mithun Das Gupta", "title": "$c^+$GAN: Complementary Fashion Item Recommendation", "comments": "KDD'19: Workshop on AI for Fashion. arXiv admin note: text overlap\n  with arXiv:1806.08977, arXiv:1411.1784, arXiv:1609.04802 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a conditional generative adversarial model to draw realistic\nsamples from paired fashion clothing distribution and provide real samples to\npair with arbitrary fashion units. More concretely, given an image of a shirt,\nobtained from a fashion magazine, a brochure or even any random click on ones\nphone, we draw realistic samples from a parameterized conditional distribution\nlearned as a conditional generative adversarial network ($c^+$GAN) to generate\nthe possible pants which can go with the shirt. We start with a classical cGAN\nmodel as proposed by Mirza and Osindero [arXiv:1411.1784] and modify both the\ngenerator and discriminator to work on captured-in-the-wild data with no human\nalignment. We gather a dataset from web crawled data, systematically develop a\nmethod which counters the problems inherent to such data, and finally present\nplausible results based on our technique. We propose simple ideas to evaluate\nhow these techniques can conquer the cognitive gap that exists when arbitrary\nclothing articles need to be paired with another relevant article, based on\nsimilarity of search results.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 10:47:56 GMT"}], "update_date": "2019-07-06", "authors_parsed": [["Kumar", "Sudhir", ""], ["Gupta", "Mithun Das", ""]]}, {"id": "1906.05599", "submitter": "Aly El Gamal", "authors": "Rajeev Sahay, Rehana Mahfuz, Aly El Gamal", "title": "A Computationally Efficient Method for Defending Adversarial Deep\n  Learning Attacks", "comments": "6 pages, 6 figures, submitted to IEEE Signal Processing Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reliance on deep learning algorithms has grown significantly in recent\nyears. Yet, these models are highly vulnerable to adversarial attacks, which\nintroduce visually imperceptible perturbations into testing data to induce\nmisclassifications. The literature has proposed several methods to combat such\nadversarial attacks, but each method either fails at high perturbation values,\nrequires excessive computing power, or both. This letter proposes a\ncomputationally efficient method for defending the Fast Gradient Sign (FGS)\nadversarial attack by simultaneously denoising and compressing data.\nSpecifically, our proposed defense relies on training a fully connected\nmulti-layer Denoising Autoencoder (DAE) and using its encoder as a defense\nagainst the adversarial attack. Our results show that using this dimensionality\nreduction scheme is not only highly effective in mitigating the effect of the\nFGS attack in multiple threat models, but it also provides a 2.43x speedup in\ncomparison to defense strategies providing similar robustness against the same\nattack.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 10:56:47 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Sahay", "Rajeev", ""], ["Mahfuz", "Rehana", ""], ["Gamal", "Aly El", ""]]}, {"id": "1906.05675", "submitter": "Haotao Wang", "authors": "Zhenyu Wu, Haotao Wang, Zhaowen Wang, Hailin Jin, Zhangyang Wang", "title": "Privacy-Preserving Deep Action Recognition: An Adversarial Learning\n  Framework and A New Dataset", "comments": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI). arXiv admin note: text overlap with arXiv:1807.08379", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate privacy-preserving, video-based action recognition in deep\nlearning, a problem with growing importance in smart camera applications. A\nnovel adversarial training framework is formulated to learn an anonymization\ntransform for input videos such that the trade-off between target utility task\nperformance and the associated privacy budgets is explicitly optimized on the\nanonymized videos. Notably, the privacy budget, often defined and measured in\ntask-driven contexts, cannot be reliably indicated using any single model\nperformance because strong protection of privacy should sustain against any\nmalicious model that tries to steal private information. To tackle this\nproblem, we propose two new optimization strategies of model restarting and\nmodel ensemble to achieve stronger universal privacy protection against any\nattacker models. Extensive experiments have been carried out and analyzed. On\nthe other hand, given few public datasets available with both utility and\nprivacy labels, the data-driven (supervised) learning cannot exert its full\npower on this task. We first discuss an innovative heuristic of cross-dataset\ntraining and evaluation, enabling the use of multiple single-task datasets (one\nwith target task labels and the other with privacy labels) in our problem. To\nfurther address this dataset challenge, we have constructed a new dataset,\ntermed PA-HMDB51, with both target task labels (action) and selected privacy\nattributes (skin color, face, gender, nudity, and relationship) annotated on a\nper-frame basis. This first-of-its-kind video dataset and evaluation protocol\ncan greatly facilitate visual privacy research and open up other opportunities.\nOur codes, models, and the PA-HMDB51 dataset are available at\nhttps://github.com/VITA-Group/PA-HMDB51.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 02:23:32 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 02:53:15 GMT"}, {"version": "v3", "created": "Sun, 27 Sep 2020 05:56:36 GMT"}, {"version": "v4", "created": "Fri, 2 Oct 2020 18:28:13 GMT"}, {"version": "v5", "created": "Thu, 21 Jan 2021 02:51:13 GMT"}, {"version": "v6", "created": "Sun, 21 Mar 2021 21:34:49 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Wu", "Zhenyu", ""], ["Wang", "Haotao", ""], ["Wang", "Zhaowen", ""], ["Jin", "Hailin", ""], ["Wang", "Zhangyang", ""]]}, {"id": "1906.05688", "submitter": "Buyu Li", "authors": "Xin Lu, Buyu Li, Yuxin Yue, Quanquan Li, Junjie Yan", "title": "Grid R-CNN Plus: Faster and Better", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grid R-CNN is a well-performed objection detection framework. It transforms\nthe traditional box offset regression problem into a grid point estimation\nproblem. With the guidance of the grid points, it can obtain high-quality\nlocalization results. However, the speed of Grid R-CNN is not so satisfactory.\nIn this technical report we present Grid R-CNN Plus, a better and faster\nversion of Grid R-CNN. We have made several updates that significantly speed up\nthe framework and simultaneously improve the accuracy. On COCO dataset, the\nRes50-FPN based Grid R-CNN Plus detector achieves an mAP of 40.4%,\noutperforming the baseline on the same model by 3.0 points with similar\ninference time. Code is available at https://github.com/STVIR/Grid-R-CNN .\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 13:52:28 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Lu", "Xin", ""], ["Li", "Buyu", ""], ["Yue", "Yuxin", ""], ["Li", "Quanquan", ""], ["Yan", "Junjie", ""]]}, {"id": "1906.05695", "submitter": "Ilkay Oksuz", "authors": "lkay Oksuz, James Clough, Bram Ruijsink, Esther Puyol-Anton, Aurelien\n  Bustin, Gastao Cruz, Claudia Prieto, Daniel Rueckert, Andrew P. King, Julia\n  A. Schnabel", "title": "Detection and Correction of Cardiac MR Motion Artefacts during\n  Reconstruction from K-space", "comments": "Accepted to MICCAI 2019. arXiv admin note: text overlap with\n  arXiv:1808.05130", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In fully sampled cardiac MR (CMR) acquisitions, motion can lead to corruption\nof k-space lines, which can result in artefacts in the reconstructed images. In\nthis paper, we propose a method to automatically detect and correct\nmotion-related artefacts in CMR acquisitions during reconstruction from k-space\ndata. Our correction method is inspired by work on undersampled CMR\nreconstruction, and uses deep learning to optimize a data-consistency term for\nunder-sampled k-space reconstruction. Our main methodological contribution is\nthe addition of a detection network to classify motion-corrupted k-space lines\nto convert the problem of artefact correction to a problem of reconstruction\nusing the data consistency term. We train our network to automatically correct\nfor motion-related artefacts using synthetically corrupted cine CMR k-space\ndata as well as uncorrupted CMR images. Using a test set of 50 2D+time cine CMR\ndatasets from the UK Biobank, we achieve good image quality in the presence of\nsynthetic motion artefacts. We quantitatively compare our method with a variety\nof techniques for recovering good image quality and showcase better performance\ncompared to state of the art denoising techniques with a PSNR of 37.1.\nMoreover, we show that our method preserves the quality of uncorrupted images\nand therefore can be also utilized as a general image reconstruction algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 14:58:31 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Oksuz", "lkay", ""], ["Clough", "James", ""], ["Ruijsink", "Bram", ""], ["Puyol-Anton", "Esther", ""], ["Bustin", "Aurelien", ""], ["Cruz", "Gastao", ""], ["Prieto", "Claudia", ""], ["Rueckert", "Daniel", ""], ["King", "Andrew P.", ""], ["Schnabel", "Julia A.", ""]]}, {"id": "1906.05706", "submitter": "Natalia Neverova", "authors": "Natalia Neverova, James Thewlis, R{\\i}za Alp G\\\"uler, Iasonas\n  Kokkinos, Andrea Vedaldi", "title": "Slim DensePose: Thrifty Learning from Sparse Annotations and Motion Cues", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  DensePose supersedes traditional landmark detectors by densely mapping image\npixels to body surface coordinates. This power, however, comes at a greatly\nincreased annotation time, as supervising the model requires to manually label\nhundreds of points per pose instance. In this work, we thus seek methods to\nsignificantly slim down the DensePose annotations, proposing more efficient\ndata collection strategies. In particular, we demonstrate that if annotations\nare collected in video frames, their efficacy can be multiplied for free by\nusing motion cues. To explore this idea, we introduce DensePose-Track, a\ndataset of videos where selected frames are annotated in the traditional\nDensePose manner. Then, building on geometric properties of the DensePose\nmapping, we use the video dynamic to propagate ground-truth annotations in time\nas well as to learn from Siamese equivariance constraints. Having performed\nexhaustive empirical evaluation of various data annotation and learning\nstrategies, we demonstrate that doing so can deliver significantly improved\npose estimation results over strong baselines. However, despite what is\nsuggested by some recent works, we show that merely synthesizing motion\npatterns by applying geometric transformations to isolated frames is\nsignificantly less effective, and that motion cues help much more when they are\nextracted from videos.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 14:07:40 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Neverova", "Natalia", ""], ["Thewlis", "James", ""], ["G\u00fcler", "R\u0131za Alp", ""], ["Kokkinos", "Iasonas", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1906.05708", "submitter": "Pengyuan Lyu", "authors": "Pengyuan Lyu, Zhicheng Yang, Xinhang Leng, Xiaojun Wu, Ruiyu Li,\n  Xiaoyong Shen", "title": "2D Attentional Irregular Scene Text Recognizer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Irregular scene text, which has complex layout in 2D space, is challenging to\nmost previous scene text recognizers. Recently, some irregular scene text\nrecognizers either rectify the irregular text to regular text image with\napproximate 1D layout or transform the 2D image feature map to 1D feature\nsequence. Though these methods have achieved good performance, the robustness\nand accuracy are still limited due to the loss of spatial information in the\nprocess of 2D to 1D transformation. Different from all of previous, we in this\npaper propose a framework which transforms the irregular text with 2D layout to\ncharacter sequence directly via 2D attentional scheme. We utilize a relation\nattention module to capture the dependencies of feature maps and a parallel\nattention module to decode all characters in parallel, which make our method\nmore effective and efficient. Extensive experiments on several public\nbenchmarks as well as our collected multi-line text dataset show that our\napproach is effective to recognize regular and irregular scene text and\noutperforms previous methods both in accuracy and speed.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 14:10:12 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Lyu", "Pengyuan", ""], ["Yang", "Zhicheng", ""], ["Leng", "Xinhang", ""], ["Wu", "Xiaojun", ""], ["Li", "Ruiyu", ""], ["Shen", "Xiaoyong", ""]]}, {"id": "1906.05717", "submitter": "Vincent Casser", "authors": "Vincent Casser, Soeren Pirk, Reza Mahjourian, Anelia Angelova", "title": "Unsupervised Monocular Depth and Ego-motion Learning with Structure and\n  Semantics", "comments": "CVPR Workshop on Visual Odometry & Computer Vision Applications Based\n  on Location Clues (VOCVALC), 2019. This is an extension of arXiv:1811.06152:\n  Depth Prediction Without the Sensors: Leveraging Structure for Unsupervised\n  Learning from Monocular Videos. Thirty-Third AAAI Conference on Artificial\n  Intelligence (AAAI'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach which takes advantage of both structure and semantics\nfor unsupervised monocular learning of depth and ego-motion. More specifically,\nwe model the motion of individual objects and learn their 3D motion vector\njointly with depth and ego-motion. We obtain more accurate results, especially\nfor challenging dynamic scenes not addressed by previous approaches. This is an\nextended version of Casser et al. [AAAI'19]. Code and models have been open\nsourced at https://sites.google.com/corp/view/struct2depth.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 15:28:57 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Casser", "Vincent", ""], ["Pirk", "Soeren", ""], ["Mahjourian", "Reza", ""], ["Angelova", "Anelia", ""]]}, {"id": "1906.05721", "submitter": "Aakanksha Chowdhery", "authors": "Aakanksha Chowdhery, Pete Warden, Jonathon Shlens, Andrew Howard,\n  Rocky Rhodes", "title": "Visual Wake Words Dataset", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The emergence of Internet of Things (IoT) applications requires intelligence\non the edge. Microcontrollers provide a low-cost compute platform to deploy\nintelligent IoT applications using machine learning at scale, but have\nextremely limited on-chip memory and compute capability. To deploy computer\nvision on such devices, we need tiny vision models that fit within a few\nhundred kilobytes of memory footprint in terms of peak usage and model size on\ndevice storage. To facilitate the development of microcontroller friendly\nmodels, we present a new dataset, Visual Wake Words, that represents a common\nmicrocontroller vision use-case of identifying whether a person is present in\nthe image or not, and provides a realistic benchmark for tiny vision models.\nWithin a limited memory footprint of 250 KB, several state-of-the-art mobile\nmodels achieve accuracy of 85-90% on the Visual Wake Words dataset. We\nanticipate the proposed dataset will advance the research on tiny vision models\nthat can push the pareto-optimal boundary in terms of accuracy versus memory\nusage for microcontroller applications.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 17:47:21 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Chowdhery", "Aakanksha", ""], ["Warden", "Pete", ""], ["Shlens", "Jonathon", ""], ["Howard", "Andrew", ""], ["Rhodes", "Rocky", ""]]}, {"id": "1906.05739", "submitter": "Ayan Chakrabarti", "authors": "Zhihao Xia, Patrick Sullivan, Ayan Chakrabarti", "title": "Generating and Exploiting Probabilistic Monocular Depth Estimates", "comments": "Project page at https://projects.ayanc.org/prdepth/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beyond depth estimation from a single image, the monocular cue is useful in a\nbroader range of depth inference applications and settings---such as when one\ncan leverage other available depth cues for improved accuracy. Currently,\ndifferent applications, with different inference tasks and combinations of\ndepth cues, are solved via different specialized networks---trained separately\nfor each application. Instead, we propose a versatile task-agnostic monocular\nmodel that outputs a probability distribution over scene depth given an input\ncolor image, as a sample approximation of outputs from a patch-wise conditional\nVAE. We show that this distributional output can be used to enable a variety of\ninference tasks in different settings, without needing to retrain for each\napplication. Across a diverse set of applications (depth completion, user\nguided estimation, etc.), our common model yields results with high\naccuracy---comparable to or surpassing that of state-of-the-art methods\ndependent on application-specific networks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 14:59:54 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 16:40:53 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Xia", "Zhihao", ""], ["Sullivan", "Patrick", ""], ["Chakrabarti", "Ayan", ""]]}, {"id": "1906.05743", "submitter": "Chen Sun", "authors": "Chen Sun, Fabien Baradel, Kevin Murphy, Cordelia Schmid", "title": "Learning Video Representations using Contrastive Bidirectional\n  Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a self-supervised learning approach for video features\nthat results in significantly improved performance on downstream tasks (such as\nvideo classification, captioning and segmentation) compared to existing\nmethods. Our method extends the BERT model for text sequences to the case of\nsequences of real-valued feature vectors, by replacing the softmax loss with\nnoise contrastive estimation (NCE). We also show how to learn representations\nfrom sequences of visual features and sequences of words derived from ASR\n(automatic speech recognition), and show that such cross-modal training (when\npossible) helps even more.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 15:03:52 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 21:59:59 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Sun", "Chen", ""], ["Baradel", "Fabien", ""], ["Murphy", "Kevin", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1906.05750", "submitter": "Sheng Guo", "authors": "Sheng Guo, Weilin Huang, Xiao Zhang, Prasanna Srikhanta, Yin Cui, Yuan\n  Li, Matthew R. Scott, Hartwig Adam and Serge Belongie", "title": "The iMaterialist Fashion Attribute Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale image databases such as ImageNet have significantly advanced\nimage classification and other visual recognition tasks. However much of these\ndatasets are constructed only for single-label and coarse object-level\nclassification. For real-world applications, multiple labels and fine-grained\ncategories are often needed, yet very few such datasets exist publicly,\nespecially those of large-scale and high quality. In this work, we contribute\nto the community a new dataset called iMaterialist Fashion Attribute\n(iFashion-Attribute) to address this problem in the fashion domain. The dataset\nis constructed from over one million fashion images with a label space that\nincludes 8 groups of 228 fine-grained attributes in total. Each image is\nannotated by experts with multiple, high-quality fashion attributes. The result\nis the first known million-scale multi-label and fine-grained image dataset. We\nconduct extensive experiments and provide baseline results with modern deep\nConvolutional Neural Networks (CNNs). Additionally, we demonstrate models\npre-trained on iFashion-Attribute achieve superior transfer learning\nperformance on fashion related tasks compared with pre-training from ImageNet\nor other fashion datasets. Data is available at:\nhttps://github.com/visipedia/imat_fashion_comp\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 15:25:04 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 17:36:43 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Guo", "Sheng", ""], ["Huang", "Weilin", ""], ["Zhang", "Xiao", ""], ["Srikhanta", "Prasanna", ""], ["Cui", "Yin", ""], ["Li", "Yuan", ""], ["Scott", "Matthew R.", ""], ["Adam", "Hartwig", ""], ["Belongie", "Serge", ""]]}, {"id": "1906.05762", "submitter": "Hanshu Yan", "authors": "Hanshu Yan, Xuan Chen, Vincent Y. F. Tan, Wenhan Yang, Joe Wu, Jiashi\n  Feng", "title": "Unsupervised Image Noise Modeling with Self-Consistent GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noise modeling lies in the heart of many image processing tasks. However,\nexisting deep learning methods for noise modeling generally require clean and\nnoisy image pairs for model training; these image pairs are difficult to obtain\nin many realistic scenarios. To ameliorate this problem, we propose a\nself-consistent GAN (SCGAN), that can directly extract noise maps from noisy\nimages, thus enabling unsupervised noise modeling. In particular, the SCGAN\nintroduces three novel self-consistent constraints that are complementary to\none another, viz.: the noise model should produce a zero response over a clean\ninput; the noise model should return the same output when fed with a specific\npure noise input; and the noise model also should re-extract a pure noise map\nif the map is added to a clean image. These three constraints are simple yet\neffective. They jointly facilitate unsupervised learning of a noise model for\nvarious noise types. To demonstrate its wide applicability, we deploy the SCGAN\non three image processing tasks including blind image denoising, rain streak\nremoval, and noisy image super-resolution. The results demonstrate the\neffectiveness and superiority of our method over the state-of-the-art methods\non a variety of benchmark datasets, even though the noise types vary\nsignificantly and paired clean images are not available.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 15:45:46 GMT"}, {"version": "v2", "created": "Sat, 12 Oct 2019 07:15:25 GMT"}, {"version": "v3", "created": "Thu, 4 Jun 2020 12:12:52 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Yan", "Hanshu", ""], ["Chen", "Xuan", ""], ["Tan", "Vincent Y. F.", ""], ["Yang", "Wenhan", ""], ["Wu", "Joe", ""], ["Feng", "Jiashi", ""]]}, {"id": "1906.05775", "submitter": "Ayan Chakrabarti", "authors": "Zhihao Xia, Ayan Chakrabarti", "title": "Training Image Estimators without Image Ground-Truth", "comments": "Project page at https://projects.ayanc.org/unsupimg/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been very successful in image estimation\napplications such as compressive-sensing and image restoration, as a means to\nestimate images from partial, blurry, or otherwise degraded measurements. These\nnetworks are trained on a large number of corresponding pairs of measurements\nand ground-truth images, and thus implicitly learn to exploit domain-specific\nimage statistics. But unlike measurement data, it is often expensive or\nimpractical to collect a large training set of ground-truth images in many\napplication settings. In this paper, we introduce an unsupervised framework for\ntraining image estimation networks, from a training set that contains only\nmeasurements---with two varied measurements per image---but no ground-truth for\nthe full images desired as output. We demonstrate that our framework can be\napplied for both regular and blind image estimation tasks, where in the latter\ncase parameters of the measurement model (e.g., the blur kernel) are unknown:\nduring inference, and potentially, also during training. We evaluate our method\nfor training networks for compressive-sensing and blind deconvolution,\nconsidering both non-blind and blind training for the latter. Our unsupervised\nframework yields models that are nearly as accurate as those from fully\nsupervised training, despite not having access to any ground-truth images.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 16:04:03 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 19:40:44 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Xia", "Zhihao", ""], ["Chakrabarti", "Ayan", ""]]}, {"id": "1906.05794", "submitter": "Eduardo Ruiz", "authors": "Eduardo Ruiz and Walterio Mayol-Cuevas", "title": "Egocentric affordance detection with the one-shot geometry-driven\n  Interaction Tensor", "comments": "Accepted for presentation at EPIC@CVPR2019 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this abstract we describe recent [4,7] and latest work on the\ndetermination of affordances in visually perceived 3D scenes. Our method builds\non the hypothesis that geometry on its own provides enough information to\nenable the detection of significant interaction possibilities in the\nenvironment. The motivation behind this is that geometric information is\nintimately related to the physical interactions afforded by objects in the\nworld. The approach uses a generic representation for the interaction between\neveryday objects such as a mug or an umbrella with the environment, and also\nfor more complex affordances such as humans Sitting or Riding a motorcycle.\nExperiments with synthetic and real RGB-D scenes show that the representation\nenables the prediction of affordance candidate locations in novel environments\nat fast rates and from a single (one-shot) training example. The determination\nof affordances is a crucial step towards systems that need to perceive and\ninteract with their surroundings. We here illustrate output on two cases for a\nsimulated robot and for an Augmented Reality setting, both perceiving in an\negocentric manner.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 16:28:54 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Ruiz", "Eduardo", ""], ["Mayol-Cuevas", "Walterio", ""]]}, {"id": "1906.05797", "submitter": "Julian Straub", "authors": "Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans,\n  Simon Green, Jakob J. Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, Anton\n  Clarkson, Mingfei Yan, Brian Budge, Yajie Yan, Xiaqing Pan, June Yon, Yuyang\n  Zou, Kimberly Leon, Nigel Carter, Jesus Briales, Tyler Gillingham, Elias\n  Mueggler, Luis Pesqueira, Manolis Savva, Dhruv Batra, Hauke M. Strasdat,\n  Renzo De Nardi, Michael Goesele, Steven Lovegrove, Richard Newcombe", "title": "The Replica Dataset: A Digital Replica of Indoor Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce Replica, a dataset of 18 highly photo-realistic 3D indoor scene\nreconstructions at room and building scale. Each scene consists of a dense\nmesh, high-resolution high-dynamic-range (HDR) textures, per-primitive semantic\nclass and instance information, and planar mirror and glass reflectors. The\ngoal of Replica is to enable machine learning (ML) research that relies on\nvisually, geometrically, and semantically realistic generative models of the\nworld - for instance, egocentric computer vision, semantic segmentation in 2D\nand 3D, geometric inference, and the development of embodied agents (virtual\nrobots) performing navigation, instruction following, and question answering.\nDue to the high level of realism of the renderings from Replica, there is hope\nthat ML systems trained on Replica may transfer directly to real world image\nand video data. Together with the data, we are releasing a minimal C++ SDK as a\nstarting point for working with the Replica dataset. In addition, Replica is\n`Habitat-compatible', i.e. can be natively used with AI Habitat for training\nand testing embodied agents.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 16:29:58 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Straub", "Julian", ""], ["Whelan", "Thomas", ""], ["Ma", "Lingni", ""], ["Chen", "Yufan", ""], ["Wijmans", "Erik", ""], ["Green", "Simon", ""], ["Engel", "Jakob J.", ""], ["Mur-Artal", "Raul", ""], ["Ren", "Carl", ""], ["Verma", "Shobhit", ""], ["Clarkson", "Anton", ""], ["Yan", "Mingfei", ""], ["Budge", "Brian", ""], ["Yan", "Yajie", ""], ["Pan", "Xiaqing", ""], ["Yon", "June", ""], ["Zou", "Yuyang", ""], ["Leon", "Kimberly", ""], ["Carter", "Nigel", ""], ["Briales", "Jesus", ""], ["Gillingham", "Tyler", ""], ["Mueggler", "Elias", ""], ["Pesqueira", "Luis", ""], ["Savva", "Manolis", ""], ["Batra", "Dhruv", ""], ["Strasdat", "Hauke M.", ""], ["De Nardi", "Renzo", ""], ["Goesele", "Michael", ""], ["Lovegrove", "Steven", ""], ["Newcombe", "Richard", ""]]}, {"id": "1906.05823", "submitter": "Nikolas Tapia", "authors": "Joscha Diehl, Kurusch Ebrahimi-Fard and Nikolas Tapia", "title": "Time-warping invariants of multidimensional time series", "comments": "18 pages, 1 figure", "journal-ref": "Acta Applicandae Mathematicae 170(1) (2020), 265--290", "doi": "10.1007/s10440-020-00333-x", "report-no": null, "categories": "math.RA cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In data science, one is often confronted with a time series representing\nmeasurements of some quantity of interest. Usually, as a first step, features\nof the time series need to be extracted. These are numerical quantities that\naim to succinctly describe the data and to dampen the influence of noise. In\nsome applications, these features are also required to satisfy some invariance\nproperties. In this paper, we concentrate on time-warping invariants. We show\nthat these correspond to a certain family of iterated sums of the increments of\nthe time series, known as quasisymmetric functions in the mathematics\nliterature. We present these invariant features in an algebraic framework, and\nwe develop some of their basic properties.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 17:09:19 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 10:55:59 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Diehl", "Joscha", ""], ["Ebrahimi-Fard", "Kurusch", ""], ["Tapia", "Nikolas", ""]]}, {"id": "1906.05841", "submitter": "Ashvin Nair", "authors": "Gerrit Schoettler, Ashvin Nair, Jianlan Luo, Shikhar Bahl, Juan\n  Aparicio Ojea, Eugen Solowjow, Sergey Levine", "title": "Deep Reinforcement Learning for Industrial Insertion Tasks with Visual\n  Inputs and Natural Rewards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connector insertion and many other tasks commonly found in modern\nmanufacturing settings involve complex contact dynamics and friction. Since it\nis difficult to capture related physical effects with first-order modeling,\ntraditional control methods often result in brittle and inaccurate controllers,\nwhich have to be manually tuned. Reinforcement learning (RL) methods have been\ndemonstrated to be capable of learning controllers in such environments from\nautonomous interaction with the environment, but running RL algorithms in the\nreal world poses sample efficiency and safety challenges. Moreover, in\npractical real-world settings we cannot assume access to perfect state\ninformation or dense reward signals. In this paper, we consider a variety of\ndifficult industrial insertion tasks with visual inputs and different natural\nreward specifications, namely sparse rewards and goal images. We show that\nmethods that combine RL with prior information, such as classical controllers\nor demonstrations, can solve these tasks from a reasonable amount of real-world\ninteraction.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 17:43:07 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 16:34:16 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Schoettler", "Gerrit", ""], ["Nair", "Ashvin", ""], ["Luo", "Jianlan", ""], ["Bahl", "Shikhar", ""], ["Ojea", "Juan Aparicio", ""], ["Solowjow", "Eugen", ""], ["Levine", "Sergey", ""]]}, {"id": "1906.05845", "submitter": "Kumar Abhishek", "authors": "Kumar Abhishek, Ghassan Hamarneh", "title": "Mask2Lesion: Mask-Constrained Adversarial Skin Lesion Image Synthesis", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin lesion segmentation is a vital task in skin cancer diagnosis and further\ntreatment. Although deep learning based approaches have significantly improved\nthe segmentation accuracy, these algorithms are still reliant on having a large\nenough dataset in order to achieve adequate results. Inspired by the immense\nsuccess of generative adversarial networks (GANs), we propose a GAN-based\naugmentation of the original dataset in order to improve the segmentation\nperformance. In particular, we use the segmentation masks available in the\ntraining dataset to train the Mask2Lesion model, and use the model to generate\nnew lesion images given any arbitrary mask, which are then used to augment the\noriginal training dataset. We test Mask2Lesion augmentation on the ISBI ISIC\n2017 Skin Lesion Segmentation Challenge dataset and achieve an improvement of\n5.17% in the mean Dice score as compared to a model trained with only classical\ndata augmentation techniques.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 17:47:09 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 11:24:37 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Abhishek", "Kumar", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "1906.05849", "submitter": "Yonglong Tian", "authors": "Yonglong Tian, Dilip Krishnan, Phillip Isola", "title": "Contrastive Multiview Coding", "comments": "Code: http://github.com/HobbitLong/CMC/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Humans view the world through many sensory channels, e.g., the\nlong-wavelength light channel, viewed by the left eye, or the high-frequency\nvibrations channel, heard by the right ear. Each view is noisy and incomplete,\nbut important factors, such as physics, geometry, and semantics, tend to be\nshared between all views (e.g., a \"dog\" can be seen, heard, and felt). We\ninvestigate the classic hypothesis that a powerful representation is one that\nmodels view-invariant factors. We study this hypothesis under the framework of\nmultiview contrastive learning, where we learn a representation that aims to\nmaximize mutual information between different views of the same scene but is\notherwise compact. Our approach scales to any number of views, and is\nview-agnostic. We analyze key properties of the approach that make it work,\nfinding that the contrastive loss outperforms a popular alternative based on\ncross-view prediction, and that the more views we learn from, the better the\nresulting representation captures underlying scene semantics. Our approach\nachieves state-of-the-art results on image and video unsupervised learning\nbenchmarks. Code is released at: http://github.com/HobbitLong/CMC/.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 17:49:20 GMT"}, {"version": "v2", "created": "Sun, 23 Jun 2019 21:58:11 GMT"}, {"version": "v3", "created": "Mon, 21 Oct 2019 02:22:14 GMT"}, {"version": "v4", "created": "Wed, 11 Mar 2020 07:50:52 GMT"}, {"version": "v5", "created": "Fri, 18 Dec 2020 10:00:51 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Tian", "Yonglong", ""], ["Krishnan", "Dilip", ""], ["Isola", "Phillip", ""]]}, {"id": "1906.05856", "submitter": "Sheng-Yu Wang", "authors": "Sheng-Yu Wang, Oliver Wang, Andrew Owens, Richard Zhang, Alexei A.\n  Efros", "title": "Detecting Photoshopped Faces by Scripting Photoshop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most malicious photo manipulations are created using standard image editing\ntools, such as Adobe Photoshop. We present a method for detecting one very\npopular Photoshop manipulation -- image warping applied to human faces -- using\na model trained entirely using fake images that were automatically generated by\nscripting Photoshop itself. We show that our model outperforms humans at the\ntask of recognizing manipulated images, can predict the specific location of\nedits, and in some cases can be used to \"undo\" a manipulation to reconstruct\nthe original, unedited image. We demonstrate that the system can be\nsuccessfully applied to real, artist-created image manipulations.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 17:59:02 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 17:59:09 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Wang", "Sheng-Yu", ""], ["Wang", "Oliver", ""], ["Owens", "Andrew", ""], ["Zhang", "Richard", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1906.05857", "submitter": "Jia-Bin Huang", "authors": "Yun-Chun Chen, Yen-Yu Lin, Ming-Hsuan Yang, Jia-Bin Huang", "title": "Show, Match and Segment: Joint Weakly Supervised Learning of Semantic\n  Matching and Object Co-segmentation", "comments": "PAMI 2020. Project: https://yunchunchen.github.io/MaCoSNet-web/ Code:\n  https://github.com/YunChunChen/MaCoSNet-pytorch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for jointly matching and segmenting object instances\nof the same category within a collection of images. In contrast to existing\nalgorithms that tackle the tasks of semantic matching and object\nco-segmentation in isolation, our method exploits the complementary nature of\nthe two tasks. The key insights of our method are two-fold. First, the\nestimated dense correspondence fields from semantic matching provide\nsupervision for object co-segmentation by enforcing consistency between the\npredicted masks from a pair of images. Second, the predicted object masks from\nobject co-segmentation in turn allow us to reduce the adverse effects due to\nbackground clutters for improving semantic matching. Our model is end-to-end\ntrainable and does not require supervision from manually annotated\ncorrespondences and object masks. We validate the efficacy of our approach on\nfive benchmark datasets: TSS, Internet, PF-PASCAL, PF-WILLOW, and SPair-71k,\nand show that our algorithm performs favorably against the state-of-the-art\nmethods on both semantic matching and object co-segmentation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 17:59:19 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2020 08:58:13 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Chen", "Yun-Chun", ""], ["Lin", "Yen-Yu", ""], ["Yang", "Ming-Hsuan", ""], ["Huang", "Jia-Bin", ""]]}, {"id": "1906.05879", "submitter": "Wen Tang", "authors": "Wen Tang, Ashkan Panahi, and Hamid Krim", "title": "Joint Concept Matching based Learning for Zero-Shot Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) which aims to recognize unseen object classes by\nonly training on seen object classes, has increasingly been of great interest\nin Machine Learning, and has registered with some successes. Most existing ZSL\nmethods typically learn a projection map between the visual feature space and\nthe semantic space and mainly suffer which is prone to a projection domain\nshift primarily due to a large domain gap between seen and unseen classes. In\nthis paper, we propose a novel inductive ZSL model based on projecting both\nvisual and semantic features into a common distinct latent space with\nclass-specific knowledge, and on reconstructing both visual and semantic\nfeatures by such a distinct common space to narrow the domain shift gap. We\nshow that all these constraints on the latent space, class-specific knowledge,\nreconstruction of features and their combinations enhance the robustness\nagainst the projection domain shift problem, and improve the generalization\nability to unseen object classes. Comprehensive experiments on four benchmark\ndatasets demonstrate that our proposed method is superior to state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 18:15:29 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 21:11:15 GMT"}, {"version": "v3", "created": "Wed, 3 Jul 2019 01:32:39 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Tang", "Wen", ""], ["Panahi", "Ashkan", ""], ["Krim", "Hamid", ""]]}, {"id": "1906.05888", "submitter": "Xin Yu", "authors": "Siddhant Ranade, Xin Yu, Shantnu Kakkar, Pedro Miraldo, Srikumar\n  Ramalingam", "title": "Can generalised relative pose estimation solve sparse 3D registration?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular 3D scan registration projects, such as Stanford digital Michelangelo\nor KinectFusion, exploit the high-resolution sensor data for scan alignment. It\nis particularly challenging to solve the registration of sparse 3D scans in the\nabsence of RGB components. In this case, we can not establish point\ncorrespondences since the same 3D point cannot be captured in two successive\nscans. In contrast to correspondence based methods, we take a different\nviewpoint and formulate the sparse 3D registration problem based on the\nconstraints from the intersection of line segments from adjacent scans. We\nobtain the line segments by modeling every horizontal and vertical scan-line as\npiece-wise linear segments. We propose a new alternating projection algorithm\nfor solving the scan alignment problem using line intersection constraints. We\ndevelop two new minimal solvers for scan alignment in the presence of plane\ncorrespondences: 1) 3 line intersections and 1 plane correspondence, and 2) 1\nline intersection and 2 plane correspondences. We outperform other competing\nmethods on Kinect and LiDAR datasets.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 18:41:27 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Ranade", "Siddhant", ""], ["Yu", "Xin", ""], ["Kakkar", "Shantnu", ""], ["Miraldo", "Pedro", ""], ["Ramalingam", "Srikumar", ""]]}, {"id": "1906.05893", "submitter": "Gr\\'egoire Nieto", "authors": "Gr\\'egoire Nieto, Mohammad Rouhani, Philippe Robert", "title": "IntrinSeqNet: Learning to Estimate the Reflectance from Varying\n  Illumination", "comments": "This article has been removed by arXiv administrators because the\n  submitter did not have the rights to agree to the license at the time of\n  submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article has been removed by arXiv administrators because the submitter\ndid not have the rights to agree to the license at the time of submission\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 18:59:29 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Nieto", "Gr\u00e9goire", ""], ["Rouhani", "Mohammad", ""], ["Robert", "Philippe", ""]]}, {"id": "1906.05894", "submitter": "Sungmin Eum", "authors": "Sungmin Eum and Heesung Kwon", "title": "Semantics to Space(S2S): Embedding semantics into spatial space for\n  zero-shot verb-object query inferencing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel deep zero-shot learning (ZSL) model for inferencing\nhuman-object-interaction with verb-object (VO) query. While the previous\ntwo-stream ZSL approaches only use the semantic/textual information to be fed\ninto the query stream, we seek to incorporate and embed the semantics into the\nvisual representation stream as well. Our approach is powered by\nSemantics-to-Space (S2S) architecture where semantics derived from the residing\nobjects are embedded into a spatial space of the visual stream. This\narchitecture allows the co-capturing of the semantic attributes of the human\nand the objects along with their location/size/silhouette information. To\nvalidate, we have constructed a new dataset, Verb-Transferability 60 (VT60).\nVT60 provides 60 different VO pairs with overlapping verbs tailored for testing\ntwo-stream ZSL approaches with VO query. Experimental evaluations show that our\napproach not only outperforms the state-of-the-art, but also shows the\ncapability of consistently improving performance regardless of which ZSL\nbaseline architecture is used.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 19:01:05 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 22:39:48 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Eum", "Sungmin", ""], ["Kwon", "Heesung", ""]]}, {"id": "1906.05895", "submitter": "Sungyong Baik", "authors": "Sungyong Baik, Seokil Hong, Kyoung Mu Lee", "title": "Learning to Forget for Meta-Learning", "comments": "CVPR 2020. Code at https://github.com/baiksung/L2F", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning is a challenging problem where the goal is to achieve\ngeneralization from only few examples. Model-agnostic meta-learning (MAML)\ntackles the problem by formulating prior knowledge as a common initialization\nacross tasks, which is then used to quickly adapt to unseen tasks. However,\nforcibly sharing an initialization can lead to conflicts among tasks and the\ncompromised (undesired by tasks) location on optimization landscape, thereby\nhindering the task adaptation. Further, we observe that the degree of conflict\ndiffers among not only tasks but also layers of a neural network. Thus, we\npropose task-and-layer-wise attenuation on the compromised initialization to\nreduce its influence. As the attenuation dynamically controls (or selectively\nforgets) the influence of prior knowledge for a given task and each layer, we\nname our method as L2F (Learn to Forget). The experimental results demonstrate\nthat the proposed method provides faster adaptation and greatly improves the\nperformance. Furthermore, L2F can be easily applied and improve other\nstate-of-the-art MAML-based frameworks, illustrating its simplicity and\ngeneralizability.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 19:03:27 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 03:24:44 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Baik", "Sungyong", ""], ["Hong", "Seokil", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "1906.05896", "submitter": "Kwonjoon Lee", "authors": "Justin Lazarow, Kwonjoon Lee, Kunyu Shi, Zhuowen Tu", "title": "Learning Instance Occlusion for Panoptic Segmentation", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Panoptic segmentation requires segments of both \"things\" (countable object\ninstances) and \"stuff\" (uncountable and amorphous regions) within a single\noutput. A common approach involves the fusion of instance segmentation (for\n\"things\") and semantic segmentation (for \"stuff\") into a non-overlapping\nplacement of segments, and resolves overlaps. However, instance ordering with\ndetection confidence do not correlate well with natural occlusion relationship.\nTo resolve this issue, we propose a branch that is tasked with modeling how two\ninstance masks should overlap one another as a binary relation. Our method,\nnamed OCFusion, is lightweight but particularly effective in the instance\nfusion process. OCFusion is trained with the ground truth relation derived\nautomatically from the existing dataset annotations. We obtain state-of-the-art\nresults on COCO and show competitive results on the Cityscapes panoptic\nsegmentation benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 19:04:58 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 16:04:32 GMT"}, {"version": "v3", "created": "Thu, 21 Nov 2019 18:58:21 GMT"}, {"version": "v4", "created": "Wed, 8 Apr 2020 06:24:27 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Lazarow", "Justin", ""], ["Lee", "Kwonjoon", ""], ["Shi", "Kunyu", ""], ["Tu", "Zhuowen", ""]]}, {"id": "1906.05897", "submitter": "Ida H\\\"aggstr\\\"om", "authors": "Ida H\\\"aggstr\\\"om, Yizun Lin, Si Li, Andrzej Krol, Yuesheng Xu, and C.\n  Ross Schmidtlein", "title": "Dynamic PET cardiac and parametric image reconstruction: a fixed-point\n  proximity gradient approach using patch-based DCT and tensor SVD\n  regularization", "comments": "11 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our aim was to enhance visual quality and quantitative accuracy of dynamic\npositron emission tomography (PET)uptake images by improved image\nreconstruction, using sophisticated sparse penalty models that incorporate both\n2D spatial+1D temporal (3DT) information. We developed two new 3DT PET\nreconstruction algorithms, incorporating different temporal and spatial\npenalties based on discrete cosine transform (DCT)w/ patches, and tensor\nnuclear norm (TNN) w/ patches, and compared to frame-by-frame methods;\nconventional 2D ordered subsets expectation maximization (OSEM) w/\npost-filtering and 2D-DCT and 2D-TNN. A 3DT brain phantom with kinetic uptake\n(2-tissue model), and a moving 3DT cardiac/lung phantom was simulated and\nreconstructed. For the cardiac/lung phantom, an additional cardiac gated\n2D-OSEM set was reconstructed. The structural similarity index (SSIM) and\nrelative root mean squared error (rRMSE) relative ground truth was\ninvestigated. The image derived left ventricular (LV) volume for the\ncardiac/lung images was found by region growing and parametric images of the\nbrain phantom were calculated. For the cardiac/lung phantom, 3DT-TNN yielded\noptimal images, and 3DT-DCT was best for the brain phantom. The optimal LV\nvolume from the 3DT-TNN images was on average 11 and 55 percentage points\ncloser to the true value compared to cardiac gated 2D-OSEM and 2D-OSEM\nrespectively. Compared to 2D-OSEM, parametric images based on 3DT-DCT images\ngenerally had smaller bias and higher SSIM. Our novel methods that incorporate\nboth 2D spatial and 1D temporal penalties produced dynamic PET images of higher\nquality than conventional 2D methods, w/o need for post-filtering. Breathing\nand cardiac motion were simultaneously captured w/o need for respiratory or\ncardiac gating. LV volumes were better recovered, and subsequently fitted\nparametric images were generally less biased and of higher quality.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 19:06:11 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["H\u00e4ggstr\u00f6m", "Ida", ""], ["Lin", "Yizun", ""], ["Li", "Si", ""], ["Krol", "Andrzej", ""], ["Xu", "Yuesheng", ""], ["Schmidtlein", "C. Ross", ""]]}, {"id": "1906.05909", "submitter": "Niki Parmar", "authors": "Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm\n  Levskaya, Jonathon Shlens", "title": "Stand-Alone Self-Attention in Vision Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Convolutions are a fundamental building block of modern computer vision\nsystems. Recent approaches have argued for going beyond convolutions in order\nto capture long-range dependencies. These efforts focus on augmenting\nconvolutional models with content-based interactions, such as self-attention\nand non-local means, to achieve gains on a number of vision tasks. The natural\nquestion that arises is whether attention can be a stand-alone primitive for\nvision models instead of serving as just an augmentation on top of\nconvolutions. In developing and testing a pure self-attention vision model, we\nverify that self-attention can indeed be an effective stand-alone layer. A\nsimple procedure of replacing all instances of spatial convolutions with a form\nof self-attention applied to ResNet model produces a fully self-attentional\nmodel that outperforms the baseline on ImageNet classification with 12% fewer\nFLOPS and 29% fewer parameters. On COCO object detection, a pure self-attention\nmodel matches the mAP of a baseline RetinaNet while having 39% fewer FLOPS and\n34% fewer parameters. Detailed ablation studies demonstrate that self-attention\nis especially impactful when used in later layers. These results establish that\nstand-alone self-attention is an important addition to the vision\npractitioner's toolbox.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 19:43:01 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Ramachandran", "Prajit", ""], ["Parmar", "Niki", ""], ["Vaswani", "Ashish", ""], ["Bello", "Irwan", ""], ["Levskaya", "Anselm", ""], ["Shlens", "Jonathon", ""]]}, {"id": "1906.05910", "submitter": "Piotr Koniusz", "authors": "Lei Wang, Piotr Koniusz, Du Q. Huynh", "title": "Hallucinating IDT Descriptors and I3D Optical Flow Features for Action\n  Recognition with CNNs", "comments": "First two authors contributed equally. This paper is accepted by\n  ICCV'19", "journal-ref": "ICCV 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we revive the use of old-fashioned handcrafted video\nrepresentations for action recognition and put new life into these techniques\nvia a CNN-based hallucination step. Despite of the use of RGB and optical flow\nframes, the I3D model (amongst others) thrives on combining its output with the\nImproved Dense Trajectory (IDT) and extracted with its low-level video\ndescriptors encoded via Bag-of-Words (BoW) and Fisher Vectors (FV). Such a\nfusion of CNNs and handcrafted representations is time-consuming due to\npre-processing, descriptor extraction, encoding and tuning parameters. Thus, we\npropose an end-to-end trainable network with streams which learn the IDT-based\nBoW/FV representations at the training stage and are simple to integrate with\nthe I3D model. Specifically, each stream takes I3D feature maps ahead of the\nlast 1D conv. layer and learns to `translate' these maps to BoW/FV\nrepresentations. Thus, our model can hallucinate and use such synthesized\nBoW/FV representations at the testing stage. We show that even features of the\nentire I3D optical flow stream can be hallucinated thus simplifying the\npipeline. Our model saves 20-55h of computations and yields state-of-the-art\nresults on four publicly available datasets.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 19:44:17 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2019 15:37:40 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Wang", "Lei", ""], ["Koniusz", "Piotr", ""], ["Huynh", "Du Q.", ""]]}, {"id": "1906.05928", "submitter": "Fitsum Reda", "authors": "Fitsum A. Reda, Deqing Sun, Aysegul Dundar, Mohammad Shoeybi, Guilin\n  Liu, Kevin J. Shih, Andrew Tao, Jan Kautz, Bryan Catanzaro", "title": "Unsupervised Video Interpolation Using Cycle Consistency", "comments": "Published in ICCV 2019. Codes are available at\n  https://github.com/NVIDIA/unsupervised-video-interpolation. Project website\n  https://nv-adlr.github.io/publication/2019-UnsupervisedVideoInterpolation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to synthesize high frame rate videos via interpolation requires\nlarge quantities of high frame rate training videos, which, however, are\nscarce, especially at high resolutions. Here, we propose unsupervised\ntechniques to synthesize high frame rate videos directly from low frame rate\nvideos using cycle consistency. For a triplet of consecutive frames, we\noptimize models to minimize the discrepancy between the center frame and its\ncycle reconstruction, obtained by interpolating back from interpolated\nintermediate frames. This simple unsupervised constraint alone achieves results\ncomparable with supervision using the ground truth intermediate frames. We\nfurther introduce a pseudo supervised loss term that enforces the interpolated\nframes to be consistent with predictions of a pre-trained interpolation model.\nThe pseudo supervised loss term, used together with cycle consistency, can\neffectively adapt a pre-trained model to a new target domain. With no\nadditional data and in a completely unsupervised fashion, our techniques\nsignificantly improve pre-trained models on new target domains, increasing PSNR\nvalues from 32.84dB to 33.05dB on the Slowflow and from 31.82dB to 32.53dB on\nthe Sintel evaluation datasets.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 21:04:10 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 04:59:58 GMT"}, {"version": "v3", "created": "Sun, 28 Mar 2021 00:43:10 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Reda", "Fitsum A.", ""], ["Sun", "Deqing", ""], ["Dundar", "Aysegul", ""], ["Shoeybi", "Mohammad", ""], ["Liu", "Guilin", ""], ["Shih", "Kevin J.", ""], ["Tao", "Andrew", ""], ["Kautz", "Jan", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "1906.05929", "submitter": "Noseong Park", "authors": "Duanshun Li, Jing Liu, Noseong Park, Dongeun Lee, Giridhar\n  Ramachandran, Ali Seyedmazloom, Kookjin Lee, Chen Feng, Vadim Sokolov, Rajesh\n  Ganesan", "title": "Solving Large-Scale 0-1 Knapsack Problems and its Application to Point\n  Cloud Resampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  0-1 knapsack is of fundamental importance in computer science, business,\noperations research, etc. In this paper, we present a deep learning\ntechnique-based method to solve large-scale 0-1 knapsack problems where the\nnumber of products (items) is large and/or the values of products are not\nnecessarily predetermined but decided by an external value assignment function\nduring the optimization process. Our solution is greatly inspired by the method\nof Lagrange multiplier and some recent adoptions of game theory to deep\nlearning. After formally defining our proposed method based on them, we develop\nan adaptive gradient ascent method to stabilize its optimization process. In\nour experiments, the presented method solves all the large-scale benchmark KP\ninstances in a minute whereas existing methods show fluctuating runtime. We\nalso show that our method can be used for other applications, including but not\nlimited to the point cloud resampling.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 10:56:18 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Li", "Duanshun", ""], ["Liu", "Jing", ""], ["Park", "Noseong", ""], ["Lee", "Dongeun", ""], ["Ramachandran", "Giridhar", ""], ["Seyedmazloom", "Ali", ""], ["Lee", "Kookjin", ""], ["Feng", "Chen", ""], ["Sokolov", "Vadim", ""], ["Ganesan", "Rajesh", ""]]}, {"id": "1906.05930", "submitter": "Ang Li", "authors": "Ang Li, Huiyi Hu, Piotr Mirowski, Mehrdad Farajtabar", "title": "Cross-View Policy Learning for Street Navigation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to navigate from visual observations in unfamiliar environments\nis a core component of intelligent agents and an ongoing challenge for Deep\nReinforcement Learning (RL). Street View can be a sensible testbed for such RL\nagents, because it provides real-world photographic imagery at ground level,\nwith diverse street appearances; it has been made into an interactive\nenvironment called StreetLearn and used for research on navigation. However,\ngoal-driven street navigation agents have not so far been able to transfer to\nunseen areas without extensive retraining, and relying on simulation is not a\nscalable solution. Since aerial images are easily and globally accessible, we\npropose instead to train a multi-modal policy on ground and aerial views, then\ntransfer the ground view policy to unseen (target) parts of the city by\nutilizing aerial view observations. Our core idea is to pair the ground view\nwith an aerial view and to learn a joint policy that is transferable across\nviews. We achieve this by learning a similar embedding space for both views,\ndistilling the policy across views and dropping out visual modalities. We\nfurther reformulate the transfer learning paradigm into three stages: 1)\ncross-modal training, when the agent is initially trained on multiple city\nregions, 2) aerial view-only adaptation to a new area, when the agent is\nadapted to a held-out region using only the easily obtainable aerial view, and\n3) ground view-only transfer, when the agent is tested on navigation tasks on\nunseen ground views, without aerial imagery. Experimental results suggest that\nthe proposed cross-view policy learning enables better generalization of the\nagent and allows for more effective transfer to unseen environments.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 21:07:31 GMT"}, {"version": "v2", "created": "Sun, 22 Sep 2019 07:33:44 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Li", "Ang", ""], ["Hu", "Huiyi", ""], ["Mirowski", "Piotr", ""], ["Farajtabar", "Mehrdad", ""]]}, {"id": "1906.05947", "submitter": "Suhas Lohit", "authors": "Suhas Lohit, Qiao Wang and Pavan Turaga", "title": "Temporal Transformer Networks: Joint Learning of Invariant and\n  Discriminative Time Warping", "comments": "Published in CVPR 2019, Codes available at\n  https://github.com/suhaslohit/TTN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many time-series classification problems involve developing metrics that are\ninvariant to temporal misalignment. In human activity analysis, temporal\nmisalignment arises due to various reasons including differing initial phase,\nsensor sampling rates, and elastic time-warps due to subject-specific\nbiomechanics. Past work in this area has only looked at reducing intra-class\nvariability by elastic temporal alignment. In this paper, we propose a hybrid\nmodel-based and data-driven approach to learn warping functions that not just\nreduce intra-class variability, but also increase inter-class separation. We\ncall this a temporal transformer network (TTN). TTN is an interpretable\ndifferentiable module, which can be easily integrated at the front end of a\nclassification network. The module is capable of reducing intra-class variance\nby generating input-dependent warping functions which lead to rate-robust\nrepresentations. At the same time, it increases inter-class variance by\nlearning warping functions that are more discriminative. We show improvements\nover strong baselines in 3D action recognition on challenging datasets using\nthe proposed framework. The improvements are especially pronounced when\ntraining sets are smaller.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 22:05:15 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Lohit", "Suhas", ""], ["Wang", "Qiao", ""], ["Turaga", "Pavan", ""]]}, {"id": "1906.05948", "submitter": "Tri Huynh", "authors": "Tri Huynh, Michael Maire, Matthew R. Walter", "title": "Multigrid Neural Memory", "comments": "ICML 2020; Project Website:\n  http://people.cs.uchicago.edu/~trihuynh/multigrid_mem", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel approach to endowing neural networks with emergent,\nlong-term, large-scale memory. Distinct from strategies that connect neural\nnetworks to external memory banks via intricately crafted controllers and\nhand-designed attentional mechanisms, our memory is internal, distributed,\nco-located alongside computation, and implicitly addressed, while being\ndrastically simpler than prior efforts. Architecting networks with multigrid\nstructure and connectivity, while distributing memory cells alongside\ncomputation throughout this topology, we observe the emergence of coherent\nmemory subsystems. Our hierarchical spatial organization, parameterized\nconvolutionally, permits efficient instantiation of large-capacity memories,\nwhile multigrid topology provides short internal routing pathways, allowing\nconvolutional networks to efficiently approximate the behavior of fully\nconnected networks. Such networks have an implicit capacity for internal\nattention; augmented with memory, they learn to read and write specific memory\nlocations in a dynamic data-dependent manner. We demonstrate these capabilities\non exploration and mapping tasks, where our network is able to self-organize\nand retain long-term memory for trajectories of thousands of time steps. On\ntasks decoupled from any notion of spatial geometry: sorting, associative\nrecall, and question answering, our design functions as a truly generic memory\nand yields excellent results.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 22:10:01 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 21:18:45 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2020 22:26:05 GMT"}, {"version": "v4", "created": "Sat, 15 Aug 2020 21:12:15 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Huynh", "Tri", ""], ["Maire", "Michael", ""], ["Walter", "Matthew R.", ""]]}, {"id": "1906.05956", "submitter": "Sungbin Lim", "authors": "Sungwoong Kim, Ildoo Kim, Sungbin Lim, Woonhyuk Baek, Chiheon Kim,\n  Hyungjoo Cho, Boogeon Yoon, Taesup Kim", "title": "Scalable Neural Architecture Search for 3D Medical Image Segmentation", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a neural architecture search (NAS) framework is proposed for\n3D medical image segmentation, to automatically optimize a neural architecture\nfrom a large design space. Our NAS framework searches the structure of each\nlayer including neural connectivities and operation types in both of the\nencoder and decoder. Since optimizing over a large discrete architecture space\nis difficult due to high-resolution 3D medical images, a novel stochastic\nsampling algorithm based on a continuous relaxation is also proposed for\nscalable gradient based optimization. On the 3D medical image segmentation\ntasks with a benchmark dataset, an automatically designed architecture by the\nproposed NAS framework outperforms the human-designed 3D U-Net, and moreover\nthis optimized architecture is well suited to be transferred for different\ntasks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 22:39:42 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Kim", "Sungwoong", ""], ["Kim", "Ildoo", ""], ["Lim", "Sungbin", ""], ["Baek", "Woonhyuk", ""], ["Kim", "Chiheon", ""], ["Cho", "Hyungjoo", ""], ["Yoon", "Boogeon", ""], ["Kim", "Taesup", ""]]}, {"id": "1906.05962", "submitter": "Guan-Lin Chao", "authors": "Guan-Lin Chao, William Chan, Ian Lane", "title": "Speaker-Targeted Audio-Visual Models for Speech Recognition in\n  Cocktail-Party Environments", "comments": "Published in INTERSPEECH 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech recognition in cocktail-party environments remains a significant\nchallenge for state-of-the-art speech recognition systems, as it is extremely\ndifficult to extract an acoustic signal of an individual speaker from a\nbackground of overlapping speech with similar frequency and temporal\ncharacteristics. We propose the use of speaker-targeted acoustic and\naudio-visual models for this task. We complement the acoustic features in a\nhybrid DNN-HMM model with information of the target speaker's identity as well\nas visual features from the mouth region of the target speaker. Experimentation\nwas performed using simulated cocktail-party data generated from the GRID\naudio-visual corpus by overlapping two speakers's speech on a single acoustic\nchannel. Our audio-only baseline achieved a WER of 26.3%. The audio-visual\nmodel improved the WER to 4.4%. Introducing speaker identity information had an\neven more pronounced effect, improving the WER to 3.6%. Combining both\napproaches, however, did not significantly improve performance further. Our\nwork demonstrates that speaker-targeted models can significantly improve the\nspeech recognition in cocktail party environments.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 23:52:16 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Chao", "Guan-Lin", ""], ["Chan", "William", ""], ["Lane", "Ian", ""]]}, {"id": "1906.05963", "submitter": "Armin Kappeler", "authors": "Simao Herdade, Armin Kappeler, Kofi Boakye, Joao Soares", "title": "Image Captioning: Transforming Objects into Words", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning models typically follow an encoder-decoder architecture\nwhich uses abstract image feature vectors as input to the encoder. One of the\nmost successful algorithms uses feature vectors extracted from the region\nproposals obtained from an object detector. In this work we introduce the\nObject Relation Transformer, that builds upon this approach by explicitly\nincorporating information about the spatial relationship between input detected\nobjects through geometric attention. Quantitative and qualitative results\ndemonstrate the importance of such geometric attention for image captioning,\nleading to improvements on all common captioning metrics on the MS-COCO\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 00:00:29 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2020 11:03:05 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Herdade", "Simao", ""], ["Kappeler", "Armin", ""], ["Boakye", "Kofi", ""], ["Soares", "Joao", ""]]}, {"id": "1906.05990", "submitter": "Artsiom Sanakoyeu", "authors": "Artsiom Sanakoyeu, Vadim Tschernezki, Uta B\\\"uchler and Bj\\\"orn Ommer", "title": "Divide and Conquer the Embedding Space for Metric Learning", "comments": "Source code:\n  https://github.com/CompVis/metric-learning-divide-and-conquer", "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2019, pp. 471-480", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the embedding space, where semantically similar objects are located\nclose together and dissimilar objects far apart, is a cornerstone of many\ncomputer vision applications. Existing approaches usually learn a single metric\nin the embedding space for all available data points, which may have a very\ncomplex non-uniform distribution with different notions of similarity between\nobjects, e.g. appearance, shape, color or semantic meaning. Approaches for\nlearning a single distance metric often struggle to encode all different types\nof relationships and do not generalize well. In this work, we propose a novel\neasy-to-implement divide and conquer approach for deep metric learning, which\nsignificantly improves the state-of-the-art performance of metric learning. Our\napproach utilizes the embedding space more efficiently by jointly splitting the\nembedding space and data into $K$ smaller sub-problems. It divides both, the\ndata and the embedding space into $K$ subsets and learns $K$ separate distance\nmetrics in the non-overlapping subspaces of the embedding space, defined by\ngroups of neurons in the embedding layer of the neural network. The proposed\napproach increases the convergence speed and improves generalization since the\ncomplexity of each sub-problem is reduced compared to the original one. We show\nthat our approach outperforms the state-of-the-art by a large margin in\nretrieval, clustering and re-identification tasks on CUB200-2011, CARS196,\nStanford Online Products, In-shop Clothes and PKU VehicleID datasets.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 02:43:01 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Sanakoyeu", "Artsiom", ""], ["Tschernezki", "Vadim", ""], ["B\u00fcchler", "Uta", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "1906.06011", "submitter": "Icaro Dourado", "authors": "Icaro Cavalcante Dourado, Ricardo da Silva Torres", "title": "Fusion vectors: Embedding Graph Fusions for Efficient Unsupervised Rank\n  Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast increase in amount and complexity of digital content led to a wide\ninterest in ad-hoc retrieval systems in recent years. Complementary, the\nexistence of heterogeneous data sources and retrieval models stimulated the\nproliferation of increasingly ingenious and effective rank aggregation\nfunctions. Although recently proposed rank aggregation functions are promising\nwith respect to effectiveness, existing proposals in the area usually overlook\nefficiency aspects. We propose an innovative rank aggregation function that is\nunsupervised, intrinsically multimodal, and targeted for fast retrieval and top\neffectiveness performance. We introduce the concepts of embedding and indexing\nof graph-based rank-aggregation representation models, and their application\nfor search tasks. Embedding formulations are also proposed for graph-based rank\nrepresentations. We introduce the concept of fusion vectors, a late-fusion\nrepresentation of objects based on ranks, from which an intrinsically\nrank-aggregation retrieval model is defined. Next, we present an approach for\nfast retrieval based on fusion vectors, thus promoting an efficient rank\naggregation system. Our method presents top effectiveness performance among\nstate-of-the-art related work, while bringing novel aspects of multimodality\nand effectiveness. Consistent speedups are achieved against the recent\nbaselines in all datasets considered.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 04:04:07 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 01:48:06 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Dourado", "Icaro Cavalcante", ""], ["Torres", "Ricardo da Silva", ""]]}, {"id": "1906.06013", "submitter": "Chunhua Shen", "authors": "Peng Wang, Hui Li, Chunhua Shen", "title": "Towards End-to-End Text Spotting in Natural Scenes", "comments": "Accepted to IEEE Trans. Pattern Analysis and Machine Intelligence\n  (TPAMI). 16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Text spotting in natural scene images is of great importance for many image\nunderstanding tasks. It includes two sub-tasks: text detection and recognition.\nIn this work, we propose a unified network that simultaneously localizes and\nrecognizes text with a single forward pass, avoiding intermediate processes\nsuch as image cropping and feature re-calculation, word separation, and\ncharacter grouping.\n  In contrast to existing approaches that consider text detection and\nrecognition as two distinct tasks and tackle them one by one, the proposed\nframework settles these two tasks concurrently. The whole framework can be\ntrained end-to-end and is able to handle text of arbitrary shapes. The\nconvolutional features are calculated only once and shared by both detection\nand recognition modules. Through multi-task training, the learned features\nbecome more discriminate and improve the overall performance. By employing the\n$2$D attention model in word recognition, the irregularity of text can be\nrobustly addressed. It provides the spatial location for each character, which\nnot only helps local feature extraction in word recognition, but also indicates\nan orientation angle to refine text localization. Our proposed method has\nachieved state-of-the-art performance on several standard text spotting\nbenchmarks, including both regular and irregular ones.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 04:20:55 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 01:11:48 GMT"}, {"version": "v3", "created": "Thu, 19 Dec 2019 03:01:25 GMT"}, {"version": "v4", "created": "Sat, 30 May 2020 03:53:02 GMT"}, {"version": "v5", "created": "Sun, 2 May 2021 03:50:13 GMT"}, {"version": "v6", "created": "Sat, 26 Jun 2021 03:36:25 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Wang", "Peng", ""], ["Li", "Hui", ""], ["Shen", "Chunhua", ""]]}, {"id": "1906.06023", "submitter": "Boxiao Liu", "authors": "Yan Gao, Boxiao Liu, Nan Guo, Xiaochun Ye, Fang Wan, Haihang You,\n  Dongrui Fan", "title": "Utilizing the Instability in Weakly Supervised Object Detection", "comments": "Accepted by CVPR 2019 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised object detection (WSOD) focuses on training object detector\nwith only image-level annotations, and is challenging due to the gap between\nthe supervision and the objective. Most of existing approaches model WSOD as a\nmultiple instance learning (MIL) problem. However, we observe that the result\nof MIL based detector is unstable, i.e., the most confident bounding boxes\nchange significantly when using different initializations. We quantitatively\ndemonstrate the instability by introducing a metric to measure it, and\nempirically analyze the reason of instability. Although the instability seems\nharmful for detection task, we argue that it can be utilized to improve the\nperformance by fusing the results of differently initialized detectors. To\nimplement this idea, we propose an end-to-end framework with multiple detection\nbranches, and introduce a simple fusion strategy. We further propose an\northogonal initialization method to increase the difference between detection\nbranches. By utilizing the instability, we achieve 52.6% and 48.0% mAP on the\nchallenging PASCAL VOC 2007 and 2012 datasets, which are both the new\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 05:05:27 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Gao", "Yan", ""], ["Liu", "Boxiao", ""], ["Guo", "Nan", ""], ["Ye", "Xiaochun", ""], ["Wan", "Fang", ""], ["You", "Haihang", ""], ["Fan", "Dongrui", ""]]}, {"id": "1906.06026", "submitter": "Shashank Kotyan", "authors": "Shashank Kotyan, Danilo Vasconcellos Vargas", "title": "Adversarial Robustness Assessment: Why both $L_0$ and $L_\\infty$ Attacks\n  Are Necessary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exists a vast number of adversarial attacks and defences for machine\nlearning algorithms of various types which makes assessing the robustness of\nalgorithms a daunting task. To make matters worse, there is an intrinsic bias\nin these adversarial algorithms. Here, we organise the problems faced: a) Model\nDependence, b) Insufficient Evaluation, c) False Adversarial Samples, and d)\nPerturbation Dependent Results). Based on this, we propose a model agnostic\ndual quality assessment method, together with the concept of robustness levels\nto tackle them. We validate the dual quality assessment on state-of-the-art\nneural networks (WideResNet, ResNet, AllConv, DenseNet, NIN, LeNet and CapsNet)\nas well as adversarial defences for image classification problem. We further\nshow that current networks and defences are vulnerable at all levels of\nrobustness. The proposed robustness assessment reveals that depending on the\nmetric used (i.e., $L_0$ or $L_\\infty$), the robustness may vary significantly.\nHence, the duality should be taken into account for a correct evaluation.\nMoreover, a mathematical derivation, as well as a counter-example, suggest that\n$L_1$ and $L_2$ metrics alone are not sufficient to avoid spurious adversarial\nsamples. Interestingly, the threshold attack of the proposed assessment is a\nnovel $L_\\infty$ black-box adversarial method which requires even less\nperturbation than the One-Pixel Attack (only $12\\%$ of One-Pixel Attack's\namount of perturbation) to achieve similar results.\n  Code is available at http://bit.ly/DualQualityAssessment.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 05:11:12 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 08:30:06 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 13:44:38 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Kotyan", "Shashank", ""], ["Vargas", "Danilo Vasconcellos", ""]]}, {"id": "1906.06027", "submitter": "Yangming Shi", "authors": "Yangming Shi, Xiaopo Wu, Ming Zhu", "title": "Low-light Image Enhancement Algorithm Based on Retinex and Generative\n  Adversarial Network", "comments": "9 pages,10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-light image enhancement is generally regarded as a challenging task in\nimage processing, especially for the complex visual tasks at night or weakly\nilluminated. In order to reduce the blurs or noises on the low-light images, a\nlarge number of papers have contributed to applying different technologies.\nRegretfully, most of them had served little purposes in coping with the\nextremely poor illumination parts of images or test in practice. In this work,\nthe authors propose a novel approach for processing low-light images based on\nthe Retinex theory and generative adversarial network (GAN), which is composed\nof the decomposition part for splitting the image into illumination image and\nreflected image, and the enhancement part for generating high-quality image.\nSuch a discriminative network is expected to make the generated image clearer.\nCouples of experiments have been implemented under the circumstance of\ndifferent lighting strength on the basis of Converted See-In-the-Dark (CSID)\ndatasets, and the satisfactory results have been achieved with exceeding\nexpectation that much encourages the authors. In a word, the proposed GAN-based\nnetwork and employed Retinex theory in this work have proven to be effective in\ndealing with the low-light image enhancement problems, which will benefit the\nimage processing with no doubt.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 05:16:54 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Shi", "Yangming", ""], ["Wu", "Xiaopo", ""], ["Zhu", "Ming", ""]]}, {"id": "1906.06058", "submitter": "Christoph Haarburger", "authors": "Christoph Haarburger, Michael Baumgartner, Daniel Truhn, Mirjam\n  Broeckmann, Hannah Schneider, Simone Schrading, Christiane Kuhl, Dorit Merhof", "title": "Multi Scale Curriculum CNN for Context-Aware Breast MRI Malignancy\n  Classification", "comments": "Accepted to MICCAI 2019", "journal-ref": null, "doi": "10.1007/978-3-030-32251-9_54", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification of malignancy for breast cancer and other cancer types is\nusually tackled as an object detection problem: Individual lesions are first\nlocalized and then classified with respect to malignancy. However, the drawback\nof this approach is that abstract features incorporating several lesions and\nareas that are not labelled as a lesion but contain global medically relevant\ninformation are thus disregarded: especially for dynamic contrast-enhanced\nbreast MRI, criteria such as background parenchymal enhancement and location\nwithin the breast are important for diagnosis and cannot be captured by object\ndetection approaches properly.\n  In this work, we propose a 3D CNN and a multi scale curriculum learning\nstrategy to classify malignancy globally based on an MRI of the whole breast.\nThus, the global context of the whole breast rather than individual lesions is\ntaken into account. Our proposed approach does not rely on lesion\nsegmentations, which renders the annotation of training data much more\neffective than in current object detection approaches.\n  Achieving an AUROC of 0.89, we compare the performance of our approach to\nMask R-CNN and Retina U-Net as well as a radiologist. Our performance is on par\nwith approaches that, in contrast to our method, rely on pixelwise\nsegmentations of lesions.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 07:33:18 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 06:16:35 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Haarburger", "Christoph", ""], ["Baumgartner", "Michael", ""], ["Truhn", "Daniel", ""], ["Broeckmann", "Mirjam", ""], ["Schneider", "Hannah", ""], ["Schrading", "Simone", ""], ["Kuhl", "Christiane", ""], ["Merhof", "Dorit", ""]]}, {"id": "1906.06059", "submitter": "Lorenzo Bertoni", "authors": "Lorenzo Bertoni, Sven Kreiss, Alexandre Alahi", "title": "MonoLoco: Monocular 3D Pedestrian Localization and Uncertainty\n  Estimation", "comments": "International Conference on Computer Vision (ICCV) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the fundamentally ill-posed problem of 3D human localization from\nmonocular RGB images. Driven by the limitation of neural networks outputting\npoint estimates, we address the ambiguity in the task by predicting confidence\nintervals through a loss function based on the Laplace distribution. Our\narchitecture is a light-weight feed-forward neural network that predicts 3D\nlocations and corresponding confidence intervals given 2D human poses. The\ndesign is particularly well suited for small training data, cross-dataset\ngeneralization, and real-time applications. Our experiments show that we (i)\noutperform state-of-the-art results on KITTI and nuScenes datasets, (ii) even\noutperform a stereo-based method for far-away pedestrians, and (iii) estimate\nmeaningful confidence intervals. We further share insights on our model of\nuncertainty in cases of limited observations and out-of-distribution samples.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 07:39:03 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 15:43:44 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Bertoni", "Lorenzo", ""], ["Kreiss", "Sven", ""], ["Alahi", "Alexandre", ""]]}, {"id": "1906.06064", "submitter": "Uzair Nadeem", "authors": "Uzair Nadeem, Mohammad A. A. K. Jalwana, Mohammed Bennamoun, Roberto\n  Togneri, Ferdous Sohel", "title": "Direct Image to Point Cloud Descriptors Matching for 6-DOF Camera\n  Localization in Dense 3D Point Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel concept to directly match feature descriptors extracted\nfrom RGB images, with feature descriptors extracted from 3D point clouds. We\nuse this concept to localize the position and orientation (pose) of the camera\nof a query image in dense point clouds. We generate a dataset of matching 2D\nand 3D descriptors, and use it to train a proposed Descriptor-Matcher\nalgorithm. To localize a query image in a point cloud, we extract 2D keypoints\nand descriptors from the query image. Then the Descriptor-Matcher is used to\nfind the corresponding pairs 2D and 3D keypoints by matching the 2D descriptors\nwith the pre-extracted 3D descriptors of the point cloud. This information is\nused in a robust pose estimation algorithm to localize the query image in the\n3D point cloud. Experiments demonstrate that directly matching 2D and 3D\ndescriptors is not only a viable idea but also achieves competitive accuracy\ncompared to other state-of-the-art approaches for camera pose localization.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 08:01:19 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Nadeem", "Uzair", ""], ["Jalwana", "Mohammad A. A. K.", ""], ["Bennamoun", "Mohammed", ""], ["Togneri", "Roberto", ""], ["Sohel", "Ferdous", ""]]}, {"id": "1906.06086", "submitter": "Thomas Brunner", "authors": "Thomas Brunner, Frederik Diehl, Alois Knoll", "title": "Copy and Paste: A Simple But Effective Initialization Method for\n  Black-Box Adversarial Attacks", "comments": "Presented at CVPR 2019 Workshop on Adversarial Machine Learning in\n  Real-World Computer Vision Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many optimization methods for generating black-box adversarial examples have\nbeen proposed, but the aspect of initializing said optimizers has not been\nconsidered in much detail. We show that the choice of starting points is indeed\ncrucial, and that the performance of state-of-the-art attacks depends on it.\nFirst, we discuss desirable properties of starting points for attacking image\nclassifiers, and how they can be chosen to increase query efficiency. Notably,\nwe find that simply copying small patches from other images is a valid\nstrategy. We then present an evaluation on ImageNet that clearly demonstrates\nthe effectiveness of this method: Our initialization scheme reduces the number\nof queries required for a state-of-the-art Boundary Attack by 81%,\nsignificantly outperforming previous results reported for targeted black-box\nadversarial examples.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 09:17:19 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 16:58:39 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Brunner", "Thomas", ""], ["Diehl", "Frederik", ""], ["Knoll", "Alois", ""]]}, {"id": "1906.06110", "submitter": "Vikash Sehwag", "authors": "Vikash Sehwag, Shiqi Wang, Prateek Mittal, Suman Jana", "title": "Towards Compact and Robust Deep Neural Networks", "comments": "14 pages, 9 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved impressive performance in many\napplications but their large number of parameters lead to significant\ncomputational and storage overheads. Several recent works attempt to mitigate\nthese overheads by designing compact networks using pruning of connections.\nHowever, we observe that most of the existing strategies to design compact\nnetworks fail to preserve network robustness against adversarial examples. In\nthis work, we rigorously study the extension of network pruning strategies to\npreserve both benign accuracy and robustness of a network. Starting with a\nformal definition of the pruning procedure, including pre-training, weights\npruning, and fine-tuning, we propose a new pruning method that can create\ncompact networks while preserving both benign accuracy and robustness. Our\nmethod is based on two main insights: (1) we ensure that the training\nobjectives of the pre-training and fine-tuning steps match the training\nobjective of the desired robust model (e.g., adversarial robustness/verifiable\nrobustness), and (2) we keep the pruning strategy agnostic to pre-training and\nfine-tuning objectives. We evaluate our method on four different networks on\nthe CIFAR-10 dataset and measure benign accuracy, empirical robust accuracy,\nand verifiable robust accuracy. We demonstrate that our pruning method can\npreserve on average 93\\% benign accuracy, 92.5\\% empirical robust accuracy, and\n85.0\\% verifiable robust accuracy while compressing the tested network by\n10$\\times$.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 10:12:02 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Sehwag", "Vikash", ""], ["Wang", "Shiqi", ""], ["Mittal", "Prateek", ""], ["Jana", "Suman", ""]]}, {"id": "1906.06113", "submitter": "Hamid Laga", "authors": "Hamid Laga", "title": "A Survey on Deep Learning Architectures for Image-based Depth\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating depth from RGB images is a long-standing ill-posed problem, which\nhas been explored for decades by the computer vision, graphics, and machine\nlearning communities. In this article, we provide a comprehensive survey of the\nrecent developments in this field. We will focus on the works which use deep\nlearning techniques to estimate depth from one or multiple images. Deep\nlearning, coupled with the availability of large training datasets, have\nrevolutionized the way the depth reconstruction problem is being approached by\nthe research community. In this article, we survey more than 100 key\ncontributions that appeared in the past five years, summarize the most commonly\nused pipelines, and discuss their benefits and limitations. In retrospect of\nwhat has been achieved so far, we also conjecture what the future may hold for\nlearning-based depth reconstruction research.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 10:22:14 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Laga", "Hamid", ""]]}, {"id": "1906.06114", "submitter": "Changhee Han", "authors": "Changhee Han, Leonardo Rundo, Kohei Murao, Zolt\\'an \\'Ad\\'am Milacski,\n  Kazuki Umemoto, Evis Sala, Hideki Nakayama, Shin'ichi Satoh", "title": "GAN-based Multiple Adjacent Brain MRI Slice Reconstruction for\n  Unsupervised Alzheimer's Disease Diagnosis", "comments": "10 pages, 4 figures, Accepted to Lecture Notes in Bioinformatics\n  (LNBI) as a volume in the Springer series", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning can discover various unseen diseases, relying on\nlarge-scale unannotated medical images of healthy subjects. Towards this,\nunsupervised methods reconstruct a single medical image to detect outliers\neither in the learned feature space or from high reconstruction loss. However,\nwithout considering continuity between multiple adjacent slices, they cannot\ndirectly discriminate diseases composed of the accumulation of subtle\nanatomical anomalies, such as Alzheimer's Disease (AD). Moreover, no study has\nshown how unsupervised anomaly detection is associated with disease stages.\nTherefore, we propose a two-step method using Generative Adversarial\nNetwork-based multiple adjacent brain MRI slice reconstruction to detect AD at\nvarious stages: (Reconstruction) Wasserstein loss with Gradient Penalty + L1\nloss---trained on 3 healthy slices to reconstruct the next 3\nones---reconstructs unseen healthy/AD cases; (Diagnosis) Average/Maximum loss\n(e.g., L2 loss) per scan discriminates them, comparing the reconstructed/ground\ntruth images. The results show that we can reliably detect AD at a very early\nstage with Area Under the Curve (AUC) 0.780 while also detecting AD at a late\nstage much more accurately with AUC 0.917; since our method is fully\nunsupervised, it should also discover and alert any anomalies including rare\ndisease.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 10:26:18 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 14:01:19 GMT"}, {"version": "v3", "created": "Fri, 5 Jul 2019 10:04:28 GMT"}, {"version": "v4", "created": "Mon, 8 Jul 2019 08:16:48 GMT"}, {"version": "v5", "created": "Mon, 16 Mar 2020 21:19:13 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Han", "Changhee", ""], ["Rundo", "Leonardo", ""], ["Murao", "Kohei", ""], ["Milacski", "Zolt\u00e1n \u00c1d\u00e1m", ""], ["Umemoto", "Kazuki", ""], ["Sala", "Evis", ""], ["Nakayama", "Hideki", ""], ["Satoh", "Shin'ichi", ""]]}, {"id": "1906.06142", "submitter": "Brian Kenji Iwana", "authors": "Taichi Sumi, Brian Kenji Iwana, Hideaki Hayashi, Seiichi Uchida", "title": "Modality Conversion of Handwritten Patterns by Cross Variational\n  Autoencoders", "comments": "to appear at the International Conference on Document Analysis and\n  Recognition (ICDAR) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research attempts to construct a network that can convert online and\noffline handwritten characters to each other. The proposed network consists of\ntwo Variational Auto-Encoders (VAEs) with a shared latent space. The VAEs are\ntrained to generate online and offline handwritten Latin characters\nsimultaneously. In this way, we create a cross-modal VAE (Cross-VAE). During\ntraining, the proposed Cross-VAE is trained to minimize the reconstruction loss\nof the two modalities, the distribution loss of the two VAEs, and a novel third\nloss called the space sharing loss. This third, space sharing loss is used to\nencourage the modalities to share the same latent space by calculating the\ndistance between the latent variables. Through the proposed method mutual\nconversion of online and offline handwritten characters is possible. In this\npaper, we demonstrate the performance of the Cross-VAE through qualitative and\nquantitative analysis.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 11:53:39 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Sumi", "Taichi", ""], ["Iwana", "Brian Kenji", ""], ["Hayashi", "Hideaki", ""], ["Uchida", "Seiichi", ""]]}, {"id": "1906.06148", "submitter": "Robin Br\\\"ugger", "authors": "Robin Br\\\"ugger, Christian F. Baumgartner, Ender Konukoglu", "title": "A Partially Reversible U-Net for Memory-Efficient Volumetric Image\n  Segmentation", "comments": "Accepted to MICCAI 2019; Edit v2: Added reference to related work of\n  Blumberg et al", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key drawbacks of 3D convolutional neural networks for segmentation\nis their memory footprint, which necessitates compromises in the network\narchitecture in order to fit into a given memory budget. Motivated by the\nRevNet for image classification, we propose a partially reversible U-Net\narchitecture that reduces memory consumption substantially. The reversible\narchitecture allows us to exactly recover each layer's outputs from the\nsubsequent layer's ones, eliminating the need to store activations for\nbackpropagation. This alleviates the biggest memory bottleneck and enables very\ndeep (theoretically infinitely deep) 3D architectures. On the BraTS challenge\ndataset, we demonstrate substantial memory savings. We further show that the\nfreed memory can be used for processing the whole field-of-view (FOV) instead\nof patches. Increasing network depth led to higher segmentation accuracy while\ngrowing the memory footprint only by a very small fraction, thanks to the\npartially reversible architecture.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 12:04:46 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 15:02:02 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Br\u00fcgger", "Robin", ""], ["Baumgartner", "Christian F.", ""], ["Konukoglu", "Ender", ""]]}, {"id": "1906.06151", "submitter": "Silvia Liberata Ullo", "authors": "Silvia L. Ullo and Maximillian S. Langenkamp and Tuomas P. Oikarinen\n  and Maria P. Del Rosso and Alessandro Sebastianelli and Federica Piccirillo\n  and Stefania Sica", "title": "Landslide Geohazard Assessment With Convolutional Neural Networks Using\n  Sentinel-2 Imagery Data", "comments": "4 pages, 3 figures, 1 table, accepted to 2019 IEEE IGARSS Conference\n  that will be held in Japan next July", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the authors aim to combine the latest state of the art models\nin image recognition with the best publicly available satellite images to\ncreate a system for landslide risk mitigation. We focus first on landslide\ndetection and further propose a similar system to be used for prediction. Such\nmodels are valuable as they could easily be scaled up to provide data for\nhazard evaluation, as satellite imagery becomes increasingly available. The\ngoal is to use satellite images and correlated data to enrich the public\nrepository of data and guide disaster relief efforts for locating precise areas\nwhere landslides have occurred. Different image augmentation methods are used\nto increase diversity in the chosen dataset and create more robust\nclassification. The resulting outputs are then fed into variants of 3-D\nconvolutional neural networks. A review of the current literature indicates\nthere is no research using CNNs (Convolutional Neural Networks) and freely\navailable satellite imagery for classifying landslide risk. The model has shown\nto be ultimately able to achieve a significantly better than baseline accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 15:22:54 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Ullo", "Silvia L.", ""], ["Langenkamp", "Maximillian S.", ""], ["Oikarinen", "Tuomas P.", ""], ["Del Rosso", "Maria P.", ""], ["Sebastianelli", "Alessandro", ""], ["Piccirillo", "Federica", ""], ["Sica", "Stefania", ""]]}, {"id": "1906.06180", "submitter": "Abdullah Nazib", "authors": "Abdullah Nazib, Clinton Fookes, Dimitri Perrin", "title": "Dense Deformation Network for High Resolution Tissue Cleared Image\n  Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent application of deep learning in various areas of medical image\nanalysis has brought excellent performance gains. In particular, technologies\nbased on deep learning in medical image registration can outperform traditional\noptimisation-based registration algorithms both in registration time and\naccuracy. However, the U-net based architectures used in most of the image\nregistration frameworks downscale the data, which removes global information\nand affects the deformation. In this paper, we present a densely connected\nconvolutional architecture for deformable image registration. Our proposed\ndense network downsizes data only in one stage and have dense connections\ninstead of the skip connections in U-net architecture. The training of the\nnetwork is unsupervised and does not require ground-truth deformation or any\nsynthetic deformation as a label. The proposed architecture is trained and\ntested on two different versions of tissue-cleared data, at 10\\% and 25\\%\nresolution of the original single-cell-resolution dataset. We demonstrate\ncomparable registration performance to state-of-the-art registration methods\nand superior performance to the deep-learning based VoxelMorph method in terms\nof accuracy and increased resolution handling ability. In both resolutions, the\nproposed DenseDeformation network outperforms VoxelMorph in registration\naccuracy. Importantly, it can register brains in one minute where conventional\nmethods can take hours at 25\\% resolution.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 11:54:19 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 00:56:27 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Nazib", "Abdullah", ""], ["Fookes", "Clinton", ""], ["Perrin", "Dimitri", ""]]}, {"id": "1906.06188", "submitter": "James Clough", "authors": "James R. Clough, Ilkay Oksuz, Esther Puyol-Anton, Bram Ruijsink,\n  Andrew P. King, Julia A. Schnabel", "title": "Global and Local Interpretability for Cardiac MRI Classification", "comments": "Accepted at MICCAI 2019, 9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods for classifying medical images have demonstrated\nimpressive accuracy in a wide range of tasks but often these models are hard to\ninterpret, limiting their applicability in clinical practice. In this work we\nintroduce a convolutional neural network model for identifying disease in\ntemporal sequences of cardiac MR segmentations which is interpretable in terms\nof clinically familiar measurements. The model is based around a variational\nautoencoder, reducing the input into a low-dimensional latent space in which\nclassification occurs. We then use the recently developed `concept activation\nvector' technique to associate concepts which are diagnostically meaningful\n(eg. clinical biomarkers such as `low left-ventricular ejection fraction') to\ncertain vectors in the latent space. These concepts are then qualitatively\ninspected by observing the change in the image domain resulting from\ninterpolations in the latent space in the direction of these vectors. As a\nresult, when the model classifies images it is also capable of providing\nnaturally interpretable concepts relevant to that classification and\ndemonstrating the meaning of those concepts in the image domain. Our approach\nis demonstrated on the UK Biobank cardiac MRI dataset where we detect the\npresence of coronary artery disease.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 13:06:24 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 11:19:43 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Clough", "James R.", ""], ["Oksuz", "Ilkay", ""], ["Puyol-Anton", "Esther", ""], ["Ruijsink", "Bram", ""], ["King", "Andrew P.", ""], ["Schnabel", "Julia A.", ""]]}, {"id": "1906.06195", "submitter": "Jerome Revaud", "authors": "Jerome Revaud, Philippe Weinzaepfel, C\\'esar De Souza, Noe Pion,\n  Gabriela Csurka, Yohann Cabon, Martin Humenberger", "title": "R2D2: Repeatable and Reliable Detector and Descriptor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interest point detection and local feature description are fundamental steps\nin many computer vision applications. Classical methods for these tasks are\nbased on a detect-then-describe paradigm where separate handcrafted methods are\nused to first identify repeatable keypoints and then represent them with a\nlocal descriptor. Neural networks trained with metric learning losses have\nrecently caught up with these techniques, focusing on learning repeatable\nsaliency maps for keypoint detection and learning descriptors at the detected\nkeypoint locations. In this work, we argue that salient regions are not\nnecessarily discriminative, and therefore can harm the performance of the\ndescription. Furthermore, we claim that descriptors should be learned only in\nregions for which matching can be performed with high confidence. We thus\npropose to jointly learn keypoint detection and description together with a\npredictor of the local descriptor discriminativeness. This allows us to avoid\nambiguous areas and leads to reliable keypoint detections and descriptions. Our\ndetection-and-description approach, trained with self-supervision, can\nsimultaneously output sparse, repeatable and reliable keypoints that\noutperforms state-of-the-art detectors and descriptors on the HPatches dataset.\nIt also establishes a record on the recently released Aachen Day-Night\nlocalization dataset.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 13:30:40 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 16:07:03 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Revaud", "Jerome", ""], ["Weinzaepfel", "Philippe", ""], ["De Souza", "C\u00e9sar", ""], ["Pion", "Noe", ""], ["Csurka", "Gabriela", ""], ["Cabon", "Yohann", ""], ["Humenberger", "Martin", ""]]}, {"id": "1906.06196", "submitter": "Jean Kossaifi", "authors": "Jean Kossaifi, Antoine Toisoul, Adrian Bulat, Yannis Panagakis,\n  Timothy Hospedales and Maja Pantic", "title": "Factorized Higher-Order CNNs with an Application to Spatio-Temporal\n  Emotion Estimation", "comments": "IEEE CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks with spatio-temporal (i.e., 3D) or\nmultidimensional convolutions of higher-order is computationally challenging\ndue to millions of unknown parameters across dozens of layers. To alleviate\nthis, one approach is to apply low-rank tensor decompositions to convolution\nkernels in order to compress the network and reduce its number of parameters.\nAlternatively, new convolutional blocks, such as MobileNet, can be directly\ndesigned for efficiency. In this paper, we unify these two approaches by\nproposing a tensor factorization framework for efficient multidimensional\n(separable) convolutions of higher-order. Interestingly, the proposed framework\nenables a novel higher-order transduction, allowing to train a network on a\ngiven domain (e.g., 2D images or N-dimensional data in general) and using\ntransduction to generalize to higher-order data such as videos (or\n(N+K)-dimensional data in general), capturing for instance temporal dynamics\nwhile preserving the learnt spatial information.\n  We apply the proposed methodology, coined CP-Higher-Order Convolution\n(HO-CPConv), to spatio-temporal facial emotion analysis. Most existing facial\naffect models focus on static imagery and discard all temporal information.\nThis is due to the above-mentioned burden of training 3D convolutional nets and\nthe lack of large bodies of video data annotated by experts. We address both\nissues with our proposed framework. Initial training is first done on static\nimagery before using transduction to generalize to the temporal domain. We\ndemonstrate superior performance on three challenging large scale affect\nestimation datasets, AffectNet, SEWA, and AFEW-VA.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 13:30:57 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 23:57:47 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Kossaifi", "Jean", ""], ["Toisoul", "Antoine", ""], ["Bulat", "Adrian", ""], ["Panagakis", "Yannis", ""], ["Hospedales", "Timothy", ""], ["Pantic", "Maja", ""]]}, {"id": "1906.06224", "submitter": "Alan Reyes-Figueroa", "authors": "Alan Reyes-Figueroa, Mariano Rivera", "title": "Deep neural network for fringe pattern filtering and normalisation", "comments": "19 pages (including references), 21 figures Comments: Full\n  mathematical details of the arquitecture were replaced by tables in a more\n  readable form (Tables 1-3). Added new series of experiments with high-levels\n  of noise in Subsection 4.4. References added", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV eess.SP physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for processing Fringe Patterns (FP). Our novel\napproach builds upon the hypothesis that the denoising and normalisation of FPs\ncan be learned by a deep neural network if enough pairs of corrupted and ideal\nFPs are provided. The main contributions of this paper are the following: (1)\nWe propose the use of the U-net neural network architecture for FP\nnormalisation tasks; (2) we propose a modification for the distribution of\nweights in the U-net, called here the V-net model, which is more convenient for\nreconstruction tasks, and we conduct extensive experimental evidence in which\nthe V-net produces high-quality results for FP filtering and normalisation. (3)\nWe also propose two modifications of the V-net scheme, namely, a residual\nversion called ResV-net and a fast operating version of the V-net, to evaluate\nthe potential improvements when modify our proposal. We evaluate the\nperformance of our methods in various scenarios: FPs corrupted with different\ndegrees of noise, and corrupted with different noise distributions. We compare\nour methodology versus other state-of-the-art methods. The experimental results\n(on both synthetic and real data) demonstrate the capabilities and potential of\nthis new paradigm for processing interferograms.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 14:36:12 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 01:53:59 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Reyes-Figueroa", "Alan", ""], ["Rivera", "Mariano", ""]]}, {"id": "1906.06281", "submitter": "Andrey Zharkov", "authors": "Andrey Zharkov, Ivan Zagaynov", "title": "Universal Barcode Detector via Semantic Segmentation", "comments": null, "journal-ref": "2019 International Conference on Document Analysis and Recognition\n  (ICDAR)", "doi": "10.1109/ICDAR.2019.00139", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Barcodes are used in many commercial applications, thus fast and robust\nreading is important. There are many different types of barcodes, some of them\nlook similar while others are completely different. In this paper we introduce\nnew fast and robust deep learning detector based on semantic segmentation\napproach. It is capable of detecting barcodes of any type simultaneously both\nin the document scans and in the wild by means of a single model. The detector\nachieves state-of-the-art results on the ArTe-Lab 1D Medium Barcode Dataset\nwith detection rate 0.995. Moreover, developed detector can deal with more\ncomplicated object shapes like very long but narrow or very small barcodes. The\nproposed approach can also identify types of detected barcodes and performs at\nreal-time speed on CPU environment being much faster than previous\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 16:44:10 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 07:07:41 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Zharkov", "Andrey", ""], ["Zagaynov", "Ivan", ""]]}, {"id": "1906.06301", "submitter": "Konstantinos Vougioukas", "authors": "Konstantinos Vougioukas, Pingchuan Ma, Stavros Petridis, Maja Pantic", "title": "Video-Driven Speech Reconstruction using Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech is a means of communication which relies on both audio and visual\ninformation. The absence of one modality can often lead to confusion or\nmisinterpretation of information. In this paper we present an end-to-end\ntemporal model capable of directly synthesising audio from silent video,\nwithout needing to transform to-and-from intermediate features. Our proposed\napproach, based on GANs is capable of producing natural sounding, intelligible\nspeech which is synchronised with the video. The performance of our model is\nevaluated on the GRID dataset for both speaker dependent and speaker\nindependent scenarios. To the best of our knowledge this is the first method\nthat maps video directly to raw audio and the first to produce intelligible\nspeech when tested on previously unseen speakers. We evaluate the synthesised\naudio not only based on the sound quality but also on the accuracy of the\nspoken words.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 17:15:27 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Vougioukas", "Konstantinos", ""], ["Ma", "Pingchuan", ""], ["Petridis", "Stavros", ""], ["Pantic", "Maja", ""]]}, {"id": "1906.06307", "submitter": "Namhoon Lee", "authors": "Namhoon Lee, Thalaiyasingam Ajanthan, Stephen Gould, Philip H. S. Torr", "title": "A Signal Propagation Perspective for Pruning Neural Networks at\n  Initialization", "comments": "ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network pruning is a promising avenue for compressing deep neural networks. A\ntypical approach to pruning starts by training a model and then removing\nredundant parameters while minimizing the impact on what is learned.\nAlternatively, a recent approach shows that pruning can be done at\ninitialization prior to training, based on a saliency criterion called\nconnection sensitivity. However, it remains unclear exactly why pruning an\nuntrained, randomly initialized neural network is effective. In this work, by\nnoting connection sensitivity as a form of gradient, we formally characterize\ninitialization conditions to ensure reliable connection sensitivity\nmeasurements, which in turn yields effective pruning results. Moreover, we\nanalyze the signal propagation properties of the resulting pruned networks and\nintroduce a simple, data-free method to improve their trainability. Our\nmodifications to the existing pruning at initialization method lead to improved\nresults on all tested network models for image classification tasks.\nFurthermore, we empirically study the effect of supervision for pruning and\ndemonstrate that our signal propagation perspective, combined with unsupervised\npruning, can be useful in various scenarios where pruning is applied to\nnon-standard arbitrarily-designed architectures.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 17:26:29 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2020 18:23:41 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Lee", "Namhoon", ""], ["Ajanthan", "Thalaiyasingam", ""], ["Gould", "Stephen", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1906.06310", "submitter": "Wei-Lun Chao", "authors": "Yurong You, Yan Wang, Wei-Lun Chao, Divyansh Garg, Geoff Pleiss,\n  Bharath Hariharan, Mark Campbell, Kilian Q. Weinberger", "title": "Pseudo-LiDAR++: Accurate Depth for 3D Object Detection in Autonomous\n  Driving", "comments": "Accepted to International Conference on Learning Representations\n  (ICLR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Detecting objects such as cars and pedestrians in 3D plays an indispensable\nrole in autonomous driving. Existing approaches largely rely on expensive LiDAR\nsensors for accurate depth information. While recently pseudo-LiDAR has been\nintroduced as a promising alternative, at a much lower cost based solely on\nstereo images, there is still a notable performance gap. In this paper we\nprovide substantial advances to the pseudo-LiDAR framework through improvements\nin stereo depth estimation. Concretely, we adapt the stereo network\narchitecture and loss function to be more aligned with accurate depth\nestimation of faraway objects --- currently the primary weakness of\npseudo-LiDAR. Further, we explore the idea to leverage cheaper but extremely\nsparse LiDAR sensors, which alone provide insufficient information for 3D\ndetection, to de-bias our depth estimation. We propose a depth-propagation\nalgorithm, guided by the initial depth estimates, to diffuse these few exact\nmeasurements across the entire depth map. We show on the KITTI object detection\nbenchmark that our combined approach yields substantial improvements in depth\nestimation and stereo-based 3D object detection --- outperforming the previous\nstate-of-the-art detection accuracy for faraway objects by 40%. Our code is\navailable at https://github.com/mileyan/Pseudo_Lidar_V2.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 17:36:11 GMT"}, {"version": "v2", "created": "Sun, 25 Aug 2019 22:09:40 GMT"}, {"version": "v3", "created": "Sat, 15 Feb 2020 07:29:02 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["You", "Yurong", ""], ["Wang", "Yan", ""], ["Chao", "Wei-Lun", ""], ["Garg", "Divyansh", ""], ["Pleiss", "Geoff", ""], ["Hariharan", "Bharath", ""], ["Campbell", "Mark", ""], ["Weinberger", "Kilian Q.", ""]]}, {"id": "1906.06322", "submitter": "Yunzhu Li", "authors": "Yunzhu Li, Jun-Yan Zhu, Russ Tedrake, Antonio Torralba", "title": "Connecting Touch and Vision via Cross-Modal Prediction", "comments": "Accepted to CVPR 2019. Project Page: http://visgel.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans perceive the world using multi-modal sensory inputs such as vision,\naudition, and touch. In this work, we investigate the cross-modal connection\nbetween vision and touch. The main challenge in this cross-domain modeling task\nlies in the significant scale discrepancy between the two: while our eyes\nperceive an entire visual scene at once, humans can only feel a small region of\nan object at any given moment. To connect vision and touch, we introduce new\ntasks of synthesizing plausible tactile signals from visual inputs as well as\nimagining how we interact with objects given tactile data as input. To\naccomplish our goals, we first equip robots with both visual and tactile\nsensors and collect a large-scale dataset of corresponding vision and tactile\nimage sequences. To close the scale gap, we present a new conditional\nadversarial model that incorporates the scale and location information of the\ntouch. Human perceptual studies demonstrate that our model can produce\nrealistic visual images from tactile data and vice versa. Finally, we present\nboth qualitative and quantitative experimental results regarding different\nsystem designs, as well as visualizing the learned representations of our\nmodel.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 17:55:54 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Li", "Yunzhu", ""], ["Zhu", "Jun-Yan", ""], ["Tedrake", "Russ", ""], ["Torralba", "Antonio", ""]]}, {"id": "1906.06329", "submitter": "Stefan Leichenauer", "authors": "Stavros Efthymiou, Jack Hidary, Stefan Leichenauer", "title": "TensorNetwork for Machine Learning", "comments": "9 pages, 8 figures. All code can be found at\n  https://github.com/google/tensornetwork", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.str-el cs.CV physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate the use of tensor networks for image classification with the\nTensorNetwork open source library. We explain in detail the encoding of image\ndata into a matrix product state form, and describe how to contract the network\nin a way that is parallelizable and well-suited to automatic gradients for\noptimization. Applying the technique to the MNIST and Fashion-MNIST datasets we\nfind out-of-the-box performance of 98% and 88% accuracy, respectively, using\nthe same tensor network architecture. The TensorNetwork library allows us to\nseamlessly move from CPU to GPU hardware, and we see a factor of more than 10\nimprovement in computational speed using a GPU.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 17:58:31 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Efthymiou", "Stavros", ""], ["Hidary", "Jack", ""], ["Leichenauer", "Stefan", ""]]}, {"id": "1906.06337", "submitter": "Konstantinos Vougioukas", "authors": "Konstantinos Vougioukas, Stavros Petridis, Maja Pantic", "title": "Realistic Speech-Driven Facial Animation with GANs", "comments": "arXiv admin note: text overlap with arXiv:1805.09313", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech-driven facial animation is the process that automatically synthesizes\ntalking characters based on speech signals. The majority of work in this domain\ncreates a mapping from audio features to visual features. This approach often\nrequires post-processing using computer graphics techniques to produce\nrealistic albeit subject dependent results. We present an end-to-end system\nthat generates videos of a talking head, using only a still image of a person\nand an audio clip containing speech, without relying on handcrafted\nintermediate features. Our method generates videos which have (a) lip movements\nthat are in sync with the audio and (b) natural facial expressions such as\nblinks and eyebrow movements. Our temporal GAN uses 3 discriminators focused on\nachieving detailed frames, audio-visual synchronization, and realistic\nexpressions. We quantify the contribution of each component in our model using\nan ablation study and we provide insights into the latent representation of the\nmodel. The generated videos are evaluated based on sharpness, reconstruction\nquality, lip-reading accuracy, synchronization as well as their ability to\ngenerate natural blinks.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 16:52:27 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Vougioukas", "Konstantinos", ""], ["Petridis", "Stavros", ""], ["Pantic", "Maja", ""]]}, {"id": "1906.06392", "submitter": "Issam Hadj Laradji", "authors": "Issam H. Laradji, Negar Rostamzadeh, Pedro O. Pinheiro, David Vazquez,\n  Mark Schmidt", "title": "Instance Segmentation with Point Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance segmentation methods often require costly per-pixel labels. We\npropose a method that only requires point-level annotations. During training,\nthe model only has access to a single pixel label per object, yet the task is\nto output full segmentation masks. To address this challenge, we construct a\nnetwork with two branches: (1) a localization network (L-Net) that predicts the\nlocation of each object; and (2) an embedding network (E-Net) that learns an\nembedding space where pixels of the same object are close. The segmentation\nmasks for the located objects are obtained by grouping pixels with similar\nembeddings. At training time, while L-Net only requires point-level\nannotations, E-Net uses pseudo-labels generated by a class-agnostic object\nproposal method. We evaluate our approach on PASCAL VOC, COCO, KITTI and\nCityScapes datasets. The experiments show that our method (1) obtains\ncompetitive results compared to fully-supervised methods in certain scenarios;\n(2) outperforms fully- and weakly- supervised methods with a fixed annotation\nbudget; and (3) is a first strong baseline for instance segmentation with\npoint-level supervision.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 20:29:38 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Laradji", "Issam H.", ""], ["Rostamzadeh", "Negar", ""], ["Pinheiro", "Pedro O.", ""], ["Vazquez", "David", ""], ["Schmidt", "Mark", ""]]}, {"id": "1906.06406", "submitter": "Nikolas Tapia", "authors": "Elena Celledoni, P{\\aa}l Erik Lystad and Nikolas Tapia", "title": "Signatures in Shape Analysis: an Efficient Approach to Motion\n  Identification", "comments": "7 pages, 3 figures. Conference paper for Geometric Science of\n  Information 2019", "journal-ref": null, "doi": "10.1007/978-3-030-26980-7_3", "report-no": "Geometric Science of Information. GSI 2019. Lecture Notes in\n  Computer Science, vol 11712", "categories": "math.DG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signatures provide a succinct description of certain features of paths in a\nreparametrization invariant way. We propose a method for classifying shapes\nbased on signatures, and compare it to current approaches based on the SRV\ntransform and dynamic programming.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 21:14:34 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Celledoni", "Elena", ""], ["Lystad", "P\u00e5l Erik", ""], ["Tapia", "Nikolas", ""]]}, {"id": "1906.06423", "submitter": "Hugo Touvron", "authors": "Hugo Touvron and Andrea Vedaldi and Matthijs Douze and Herv\\'e J\\'egou", "title": "Fixing the train-test resolution discrepancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-augmentation is key to the training of neural networks for image\nclassification. This paper first shows that existing augmentations induce a\nsignificant discrepancy between the typical size of the objects seen by the\nclassifier at train and test time. We experimentally validate that, for a\ntarget test resolution, using a lower train resolution offers better\nclassification at test time.\n  We then propose a simple yet effective and efficient strategy to optimize the\nclassifier performance when the train and test resolutions differ. It involves\nonly a computationally cheap fine-tuning of the network at the test resolution.\nThis enables training strong classifiers using small training images. For\ninstance, we obtain 77.1% top-1 accuracy on ImageNet with a ResNet-50 trained\non 128x128 images, and 79.8% with one trained on 224x224 image. In addition, if\nwe use extra training data we get 82.5% with the ResNet-50 train with 224x224\nimages.\n  Conversely, when training a ResNeXt-101 32x48d pre-trained in\nweakly-supervised fashion on 940 million public images at resolution 224x224\nand further optimizing for test resolution 320x320, we obtain a test top-1\naccuracy of 86.4% (top-5: 98.0%) (single-crop). To the best of our knowledge\nthis is the highest ImageNet single-crop, top-1 and top-5 accuracy to date.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 22:27:30 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 09:01:21 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 07:52:15 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Touvron", "Hugo", ""], ["Vedaldi", "Andrea", ""], ["Douze", "Matthijs", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "1906.06430", "submitter": "Abdullah-Al-Zubaer Imran", "authors": "Abdullah-Al-Zubaer Imran and Demetri Terzopoulos", "title": "Multi-Adversarial Variational Autoencoder Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unsupervised training of GANs and VAEs has enabled them to generate\nrealistic images mimicking real-world distributions and perform image-based\nunsupervised clustering or semi-supervised classification. Combining the power\nof these two generative models, we introduce Multi-Adversarial Variational\nautoEncoder Networks (MAVENs), a novel network architecture that incorporates\nan ensemble of discriminators in a VAE-GAN network, with simultaneous\nadversarial learning and variational inference. We apply MAVENs to the\ngeneration of synthetic images and propose a new distribution measure to\nquantify the quality of the generated images. Our experimental results using\ndatasets from the computer vision and medical imaging domains---Street View\nHouse Numbers, CIFAR-10, and Chest X-Ray datasets---demonstrate competitive\nperformance against state-of-the-art semi-supervised models both in image\ngeneration and classification tasks.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 23:04:44 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Imran", "Abdullah-Al-Zubaer", ""], ["Terzopoulos", "Demetri", ""]]}, {"id": "1906.06439", "submitter": "Emily Denton", "authors": "Emily Denton, Ben Hutchinson, Margaret Mitchell, Timnit Gebru, Andrew\n  Zaldivar", "title": "Image Counterfactual Sensitivity Analysis for Detecting Unintended Bias", "comments": "Presented at CVPR 2019 Workshop on Fairness Accountability\n  Transparency and Ethics in Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial analysis models are increasingly used in applications that have\nserious impacts on people's lives, ranging from authentication to surveillance\ntracking. It is therefore critical to develop techniques that can reveal\nunintended biases in facial classifiers to help guide the ethical use of facial\nanalysis technology. This work proposes a framework called \\textit{image\ncounterfactual sensitivity analysis}, which we explore as a proof-of-concept in\nanalyzing a smiling attribute classifier trained on faces of celebrities. The\nframework utilizes counterfactuals to examine how a classifier's prediction\nchanges if a face characteristic slightly changes. We leverage recent advances\nin generative adversarial networks to build a realistic generative model of\nface images that affords controlled manipulation of specific image\ncharacteristics. We then introduce a set of metrics that measure the effect of\nmanipulating a specific property on the output of the trained classifier.\nEmpirically, we find several different factors of variation that affect the\npredictions of the smiling classifier. This proof-of-concept demonstrates\npotential ways generative models can be leveraged for fine-grained analysis of\nbias and fairness.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 23:50:04 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 18:45:47 GMT"}, {"version": "v3", "created": "Sat, 3 Oct 2020 21:33:55 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Denton", "Emily", ""], ["Hutchinson", "Ben", ""], ["Mitchell", "Margaret", ""], ["Gebru", "Timnit", ""], ["Zaldivar", "Andrew", ""]]}, {"id": "1906.06446", "submitter": "Sze Teng Liong", "authors": "Sze-Teng Liong and Y.S. Gan, Kun-Hong Liu and Tran Quang Binh and Cong\n  Tue Le and Chien An Wu and Cheng-Yan Yang and Yen-Chang Huang", "title": "Efficient Neural Network Approaches for Leather Defect Classification", "comments": "15 pages, 10 Tables, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genuine leather, such as the hides of cows, crocodiles, lizards and goats\nusually contain natural and artificial defects, like holes, fly bites, tick\nmarks, veining, cuts, wrinkles and others. A traditional solution to identify\nthe defects is by manual defect inspection, which involves skilled experts. It\nis time consuming and may incur a high error rate and results in low\nproductivity. This paper presents a series of automatic image processing\nprocesses to perform the classification of leather defects by adopting deep\nlearning approaches. Particularly, the leather images are first partitioned\ninto small patches,then it undergoes a pre-processing technique, namely the\nCanny edge detection to enhance defect visualization. Next, artificial neural\nnetwork (ANN) and convolutional neural network (CNN) are employed to extract\nthe rich image features. The best classification result achieved is 80.3 %,\nevaluated on a data set that consists of 2000 samples. In addition, the\nperformance metrics such as confusion matrix and Receiver Operating\nCharacteristic (ROC) are reported to demonstrate the efficiency of the method\nproposed.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 01:12:02 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Liong", "Sze-Teng", ""], ["Gan", "Y. S.", ""], ["Liu", "Kun-Hong", ""], ["Binh", "Tran Quang", ""], ["Le", "Cong Tue", ""], ["Wu", "Chien An", ""], ["Yang", "Cheng-Yan", ""], ["Huang", "Yen-Chang", ""]]}, {"id": "1906.06476", "submitter": "Somdyuti Paul", "authors": "Somdyuti Paul, Andrey Norkin, and Alan C. Bovik", "title": "Speeding up VP9 Intra Encoder with Hierarchical Deep Learning Based\n  Partition Prediction", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2020.3011270", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In VP9 video codec, the sizes of blocks are decided during encoding by\nrecursively partitioning 64$\\times$64 superblocks using rate-distortion\noptimization (RDO). This process is computationally intensive because of the\ncombinatorial search space of possible partitions of a superblock. Here, we\npropose a deep learning based alternative framework to predict the intra-mode\nsuperblock partitions in the form of a four-level partition tree, using a\nhierarchical fully convolutional network (H-FCN). We created a large database\nof VP9 superblocks and the corresponding partitions to train an H-FCN model,\nwhich was subsequently integrated with the VP9 encoder to reduce the intra-mode\nencoding time. The experimental results establish that our approach speeds up\nintra-mode encoding by 69.7% on average, at the expense of a 1.71% increase in\nthe Bjontegaard-Delta bitrate (BD-rate). While VP9 provides several built-in\nspeed levels which are designed to provide faster encoding at the expense of\ndecreased rate-distortion performance, we find that our model is able to\noutperform the fastest recommended speed level of the reference VP9 encoder for\nthe good quality intra encoding configuration, in terms of both speedup and\nBD-rate.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 05:50:25 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 21:38:51 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Paul", "Somdyuti", ""], ["Norkin", "Andrey", ""], ["Bovik", "Alan C.", ""]]}, {"id": "1906.06480", "submitter": "Jayasree Saha", "authors": "Jayasree Saha and Jayanta Mukhopadhyay", "title": "RECAL: Reuse of Established CNN classifer Apropos unsupervised Learning\n  paradigm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, clustering with deep network framework has attracted attention of\nseveral researchers in the computer vision community. Deep framework gains\nextensive attention due to its efficiency and scalability towards large-scale\nand high-dimensional data. In this paper, we transform supervised CNN\nclassifier architecture into an unsupervised clustering model, called RECAL,\nwhich jointly learns discriminative embedding subspace and cluster labels.\nRECAL is made up of feature extraction layers which are convolutional, followed\nby unsupervised classifier layers which is fully connected. A multinomial\nlogistic regression function (softmax) stacked on top of classifier layers. We\ntrain this network using stochastic gradient descent (SGD) optimizer. However,\nthe successful implementation of our model is revolved around the design of\nloss function. Our loss function uses the heuristics that true partitioning\nentails lower entropy given that the class distribution is not heavily skewed.\nThis is a trade-off between the situations of \"skewed distribution\" and\n\"low-entropy\". To handle this, we have proposed classification entropy and\nclass entropy which are the two components of our loss function. In this\napproach, size of the mini-batch should be kept high. Experimental results\nindicate the consistent and competitive behavior of our model for clustering\nwell-known digit, multi-viewed object and face datasets. Morever, we use this\nmodel to generate unsupervised patch segmentation for multi-spectral LISS-IV\nimages. We observe that it is able to distinguish built-up area, wet land,\nvegetation and waterbody from the underlying scene.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 06:46:04 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Saha", "Jayasree", ""], ["Mukhopadhyay", "Jayanta", ""]]}, {"id": "1906.06496", "submitter": "Shiye Lei", "authors": "Tian Wang, Shiye Lei, Youyou Jiang, Choi Chang, Hichem Snoussi,\n  Guangcun Shan", "title": "Accelerating temporal action proposal generation via high performance\n  computing", "comments": "11 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Temporal action recognition always depends on temporal action proposal\ngeneration to hypothesize actions and algorithms usually need to process very\nlong video sequences and output the starting and ending times of each potential\naction in each video suffering from high computation cost. To address this,\nbased on boundary sensitive network we propose a new temporal convolution\nnetwork called Multipath Temporal ConvNet (MTN), which consists of two parts\ni.e. Multipath DenseNet and SE-ConvNet. In this work, one novel high\nperformance ring parallel architecture based on Message Passing Interface (MPI)\nis further introduced into temporal action proposal generation, which is a\nreliable communication protocol, in order to respond to the requirements of\nlarge memory occupation and a large number of videos. Remarkably, the total\ndata transmission is reduced by adding a connection between multiple computing\nload in the newly developed architecture. It is found that, compared to the\ntraditional Parameter Server architecture, our parallel architecture has higher\nefficiency on temporal action detection task with multiple GPUs, which is\nsuitable for dealing with the tasks of temporal action proposal generation,\nespecially for large datasets of millions of videos. We conduct experiments on\nActivityNet-1.3 and THUMOS14, where our method outperforms other state-of-art\ntemporal action detection methods with high recall and high temporal precision.\nIn addition, a time metric is further proposed here to evaluate the speed\nperformance in the distributed training process.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 08:35:34 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 05:12:50 GMT"}, {"version": "v3", "created": "Thu, 12 Sep 2019 10:08:11 GMT"}, {"version": "v4", "created": "Fri, 24 Apr 2020 06:35:10 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Wang", "Tian", ""], ["Lei", "Shiye", ""], ["Jiang", "Youyou", ""], ["Chang", "Choi", ""], ["Snoussi", "Hichem", ""], ["Shan", "Guangcun", ""]]}, {"id": "1906.06514", "submitter": "Hongsong Wang", "authors": "Hongsong Wang, Jian Dong, Bin Cheng, and Jiashi Feng", "title": "PVRED: A Position-Velocity Recurrent Encoder-Decoder for Human Motion\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion prediction, which aims to predict future human poses given past\nposes, has recently seen increased interest. Many recent approaches are based\non Recurrent Neural Networks (RNN) which model human poses with exponential\nmaps. These approaches neglect the pose velocity as well as temporal relation\nof different poses, and tend to converge to the mean pose or fail to generate\nnatural-looking poses. We therefore propose a novel Position-Velocity Recurrent\nEncoder-Decoder (PVRED) for human motion prediction, which makes full use of\npose velocities and temporal positional information. A temporal position\nembedding method is presented and a Position-Velocity RNN (PVRNN) is proposed.\nWe also emphasize the benefits of quaternion parameterization of poses and\ndesign a novel trainable Quaternion Transformation (QT) layer, which is\ncombined with a robust loss function during training. We provide quantitative\nresults for both short-term prediction in the future 0.5 seconds and long-term\nprediction in the future 0.5 to 1 seconds. Experiments on several benchmarks\nshow that our approach considerably outperforms the state-of-the-art methods.\nIn addition, qualitative visualizations in the future 4 seconds show that our\napproach could predict future human-like and meaningful poses in very long time\nhorizons. Code is publicly available on GitHub:\n\\textcolor{red}{https://github.com/hongsong-wang/PVRNN}.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 09:59:30 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 01:42:05 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Wang", "Hongsong", ""], ["Dong", "Jian", ""], ["Cheng", "Bin", ""], ["Feng", "Jiashi", ""]]}, {"id": "1906.06521", "submitter": "Hongsong Wang", "authors": "Hongsong Wang and Jiashi Feng", "title": "Delving into 3D Action Anticipation from Streaming Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action anticipation, which aims to recognize the action with a partial\nobservation, becomes increasingly popular due to a wide range of applications.\nIn this paper, we investigate the problem of 3D action anticipation from\nstreaming videos with the target of understanding best practices for solving\nthis problem. We first introduce several complementary evaluation metrics and\npresent a basic model based on frame-wise action classification. To achieve\nbetter performance, we then investigate two important factors, i.e., the length\nof the training clip and clip sampling method. We also explore multi-task\nlearning strategies by incorporating auxiliary information from two aspects:\nthe full action representation and the class-agnostic action label. Our\ncomprehensive experiments uncover the best practices for 3D action\nanticipation, and accordingly we propose a novel method with a multi-task loss.\nThe proposed method considerably outperforms the recent methods and exhibits\nthe state-of-the-art performance on standard benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 10:30:29 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Wang", "Hongsong", ""], ["Feng", "Jiashi", ""]]}, {"id": "1906.06538", "submitter": "Fuxian Li", "authors": "Qi Xuan, Fuxian Li, Yi Liu, Yun Xiang", "title": "MV-C3D: A Spatial Correlated Multi-View 3D Convolutional Neural Networks", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the development of deep neural networks, 3D object recognition is becoming\nincreasingly popular in computer vision community. Many multi-view based\nmethods are proposed to improve the category recognition accuracy. These\napproaches mainly rely on multi-view images which are rendered with the whole\ncircumference. In real-world applications, however, 3D objects are mostly\nobserved from partial viewpoints in a less range. Therefore, we propose a\nmulti-view based 3D convolutional neural network, which takes only part of\ncontiguous multi-view images as input and can still maintain high accuracy.\nMoreover, our model takes these view images as a joint variable to better learn\nspatially correlated features using 3D convolution and 3D max-pooling layers.\nExperimental results on ModelNet10 and ModelNet40 datasets show that our MV-C3D\ntechnique can achieve outstanding performance with multi-view images which are\ncaptured from partial angles with less range. The results on 3D rotated real\nimage dataset MIRO further demonstrate that MV-C3D is more adaptable in\nreal-world scenarios. The classification accuracy can be further improved with\nthe increasing number of view images.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 12:00:05 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Xuan", "Qi", ""], ["Li", "Fuxian", ""], ["Liu", "Yi", ""], ["Xiang", "Yun", ""]]}, {"id": "1906.06543", "submitter": "Hamid Laga", "authors": "Xian-Feng Han, Hamid Laga, Mohammed Bennamoun", "title": "Image-based 3D Object Reconstruction: State-of-the-Art and Trends in the\n  Deep Learning Era", "comments": "arXiv admin note: text overlap with arXiv:1806.06098,\n  arXiv:1712.06584, arXiv:1804.10975 by other authors", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  Nov. 2019", "doi": "10.1109/TPAMI.2019.2954885", "report-no": null, "categories": "cs.CV cs.CG cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D reconstruction is a longstanding ill-posed problem, which has been\nexplored for decades by the computer vision, computer graphics, and machine\nlearning communities. Since 2015, image-based 3D reconstruction using\nconvolutional neural networks (CNN) has attracted increasing interest and\ndemonstrated an impressive performance. Given this new era of rapid evolution,\nthis article provides a comprehensive survey of the recent developments in this\nfield. We focus on the works which use deep learning techniques to estimate the\n3D shape of generic objects either from a single or multiple RGB images. We\norganize the literature based on the shape representations, the network\narchitectures, and the training mechanisms they use. While this survey is\nintended for methods which reconstruct generic objects, we also review some of\nthe recent works which focus on specific object classes such as human body\nshapes and faces. We provide an analysis and comparison of the performance of\nsome key papers, summarize some of the open problems in this field, and discuss\npromising directions for future research.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 12:35:05 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 15:51:43 GMT"}, {"version": "v3", "created": "Fri, 1 Nov 2019 14:01:31 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Han", "Xian-Feng", ""], ["Laga", "Hamid", ""], ["Bennamoun", "Mohammed", ""]]}, {"id": "1906.06558", "submitter": "Ron Mokady", "authors": "Ron Mokady, Sagie Benaim, Lior Wolf, Amit Bermano", "title": "Mask Based Unsupervised Content Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of translating, in an unsupervised manner, between\ntwo domains where one contains some additional information compared to the\nother. The proposed method disentangles the common and separate parts of these\ndomains and, through the generation of a mask, focuses the attention of the\nunderlying network to the desired augmentation alone, without wastefully\nreconstructing the entire target. This enables state-of-the-art quality and\nvariety of content translation, as demonstrated through extensive quantitative\nand qualitative evaluation. Our method is also capable of adding the separate\ncontent of different guide images and domains as well as remove existing\nseparate content. Furthermore, our method enables weakly-supervised semantic\nsegmentation of the separate part of each domain, where only class labels are\nprovided. Our code is publicly available at\nhttps://github.com/rmokady/mbu-content-tansfer.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 13:15:51 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 12:04:01 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Mokady", "Ron", ""], ["Benaim", "Sagie", ""], ["Wolf", "Lior", ""], ["Bermano", "Amit", ""]]}, {"id": "1906.06575", "submitter": "Yuan Ma", "authors": "Kewen Liu, Yuan Ma, Hongxia Xiong, Zejun Yan, Zhijun Zhou, Chaoyang\n  Liu, Panpan Fang, Xiaojun Li, Yalei Chen", "title": "Single Image Super-resolution via Dense Blended Attention Generative\n  Adversarial Network for Clinical Diagnosis", "comments": "We abandoned this paper due to its limitation only applied on medical\n  images, please view our lastest work at arXiv:1911.03464", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  During training phase, more connections (e.g. channel concatenation in last\nlayer of DenseNet) means more occupied GPU memory and lower GPU utilization,\nrequiring more training time. The increase of training time is also not\nconducive to launch application of SR algorithms. This's why we abandoned\nDenseNet as basic network. Futhermore, we abandoned this paper due to its\nlimitation only applied on medical images. Please view our lastest work applied\non general images at arXiv:1911.03464.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 15:13:41 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 06:37:17 GMT"}, {"version": "v3", "created": "Fri, 21 Feb 2020 03:03:14 GMT"}, {"version": "v4", "created": "Mon, 24 Feb 2020 03:55:33 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Liu", "Kewen", ""], ["Ma", "Yuan", ""], ["Xiong", "Hongxia", ""], ["Yan", "Zejun", ""], ["Zhou", "Zhijun", ""], ["Liu", "Chaoyang", ""], ["Fang", "Panpan", ""], ["Li", "Xiaojun", ""], ["Chen", "Yalei", ""]]}, {"id": "1906.06579", "submitter": "YoungJoon Yoo", "authors": "YoungJoon Yoo, Dongyoon Han, Sangdoo Yun", "title": "EXTD: Extremely Tiny Face Detector via Iterative Filter Reuse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new multi-scale face detector having an extremely\ntiny number of parameters (EXTD),less than 0.1 million, as well as achieving\ncomparable performance to deep heavy detectors. While existing multi-scale face\ndetectors extract feature maps with different scales from a single backbone\nnetwork, our method generates the feature maps by iteratively reusing a shared\nlightweight and shallow backbone network. This iterative sharing of the\nbackbone network significantly reduces the number of parameters, and also\nprovides the abstract image semantics captured from the higher stage of the\nnetwork layers to the lower-level feature map. The proposed idea is employed by\nvarious model architectures and evaluated by extensive experiments. From the\nexperiments from WIDER FACE dataset, we show that the proposed face detector\ncan handle faces with various scale and conditions, and achieved comparable\nperformance to the more massive face detectors that few hundreds and tens times\nheavier in model size and floating point operations.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 15:53:41 GMT"}, {"version": "v2", "created": "Sun, 23 Jun 2019 12:41:13 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Yoo", "YoungJoon", ""], ["Han", "Dongyoon", ""], ["Yun", "Sangdoo", ""]]}, {"id": "1906.06597", "submitter": "Cheng-Yang Fu", "authors": "Cheng-Yang Fu, Tamara L. Berg, Alexander C. Berg", "title": "IMP: Instance Mask Projection for High Accuracy Semantic Segmentation of\n  Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a new operator, called Instance Mask Projection\n(IMP), which projects a predicted Instance Segmentation as a new feature for\nsemantic segmentation. It also supports back propagation so is trainable\nend-to-end. Our experiments show the effectiveness of IMP on both Clothing\nParsing (with complex layering, large deformations, and non-convex objects),\nand on Street Scene Segmentation (with many overlapping instances and small\nobjects). On the Varied Clothing Parsing dataset (VCP), we show instance mask\nprojection can improve 3 points on mIOU from a state-of-the-art Panoptic FPN\nsegmentation approach. On the ModaNet clothing parsing dataset, we show a\ndramatic improvement of 20.4% absolutely compared to existing baseline semantic\nsegmentation results. In addition, the instance mask projection operator works\nwell on other (non-clothing) datasets, providing an improvement of 3 points in\nmIOU on Thing classes of Cityscapes, a self-driving dataset, on top of a\nstate-of-the-art approach.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 17:58:07 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Fu", "Cheng-Yang", ""], ["Berg", "Tamara L.", ""], ["Berg", "Alexander C.", ""]]}, {"id": "1906.06600", "submitter": "Oliver Bimber", "authors": "Indrajit Kurmi and David C. Schedl and Oliver Bimber", "title": "A Statistical View on Synthetic Aperture Imaging for Occlusion Removal", "comments": "10 pages, 11 figures, IEEE Sensors Jounral (accepted)", "journal-ref": null, "doi": "10.1109/JSEN.2019.2922731", "report-no": "upload03", "categories": "cs.GR cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic apertures find applications in many fields, such as radar, radio\ntelescopes, microscopy, sonar, ultrasound, LiDAR, and optical imaging. They\napproximate the signal of a single hypothetical wide aperture sensor with\neither an array of static small aperture sensors or a single moving small\naperture sensor. Common sense in synthetic aperture sampling is that a dense\nsampling pattern within a wide aperture is required to reconstruct a clear\nsignal. In this article we show that there exists practical limits to both,\nsynthetic aperture size and number of samples for the application of occlusion\nremoval. This leads to an understanding on how to design synthetic aperture\nsampling patterns and sensors in a most optimal and practically efficient way.\nWe apply our findings to airborne optical sectioning which uses camera drones\nand synthetic aperture imaging to computationally remove occluding vegetation\nor trees for inspecting ground surfaces.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 18:28:23 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Kurmi", "Indrajit", ""], ["Schedl", "David C.", ""], ["Bimber", "Oliver", ""]]}, {"id": "1906.06601", "submitter": "Soumendu Majee", "authors": "Soumendu Majee, Thilo Balke, Craig A. J. Kemp, Gregery T. Buzzard,\n  Charles A. Bouman", "title": "4D X-Ray CT Reconstruction using Multi-Slice Fusion", "comments": "8 pages, 8 figures, IEEE International Conference on Computational\n  Photography 2019, Tokyo", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing need to reconstruct objects in four or more dimensions\ncorresponding to space, time and other independent parameters. The best 4D\nreconstruction algorithms use regularized iterative reconstruction approaches\nsuch as model based iterative reconstruction (MBIR), which depends critically\non the quality of the prior modeling. Recently, Plug-and-Play methods have been\nshown to be an effective way to incorporate advanced prior models using\nstate-of-the-art denoising algorithms designed to remove additive white\nGaussian noise (AWGN). However, state-of-the-art denoising algorithms such as\nBM4D and deep convolutional neural networks (CNNs) are primarily available for\n2D and sometimes 3D images. In particular, CNNs are difficult and\ncomputationally expensive to implement in four or more dimensions, and training\nmay be impossible if there is no associated high-dimensional training data.\n  In this paper, we present Multi-Slice Fusion, a novel algorithm for 4D and\nhigher-dimensional reconstruction, based on the fusion of multiple\nlow-dimensional denoisers. Our approach uses multi-agent consensus equilibrium\n(MACE), an extension of Plug-and-Play, as a framework for integrating the\nmultiple lower-dimensional prior models. We apply our method to the problem of\n4D cone-beam X-ray CT reconstruction for Non Destructive Evaluation (NDE) of\nmoving parts. This is done by solving the MACE equations using\nlower-dimensional CNN denoisers implemented in parallel on a heterogeneous\ncluster. Results on experimental CT data demonstrate that Multi-Slice Fusion\ncan substantially improve the quality of reconstructions relative to\ntraditional 4D priors, while also being practical to implement and train.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 18:37:56 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Majee", "Soumendu", ""], ["Balke", "Thilo", ""], ["Kemp", "Craig A. J.", ""], ["Buzzard", "Gregery T.", ""], ["Bouman", "Charles A.", ""]]}, {"id": "1906.06618", "submitter": "Yihong Xu", "authors": "Yihong Xu, Aljosa Osep, Yutong Ban, Radu Horaud, Laura Leal-Taixe,\n  Xavier Alameda-Pineda", "title": "How To Train Your Deep Multi-Object Tracker", "comments": "14 pages, 9 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent trend in vision-based multi-object tracking (MOT) is heading\ntowards leveraging the representational power of deep learning to jointly learn\nto detect and track objects. However, existing methods train only certain\nsub-modules using loss functions that often do not correlate with established\ntracking evaluation measures such as Multi-Object Tracking Accuracy (MOTA) and\nPrecision (MOTP). As these measures are not differentiable, the choice of\nappropriate loss functions for end-to-end training of multi-object tracking\nmethods is still an open research problem. In this paper, we bridge this gap by\nproposing a differentiable proxy of MOTA and MOTP, which we combine in a loss\nfunction suitable for end-to-end training of deep multi-object trackers. As a\nkey ingredient, we propose a Deep Hungarian Net (DHN) module that approximates\nthe Hungarian matching algorithm. DHN allows estimating the correspondence\nbetween object tracks and ground truth objects to compute differentiable\nproxies of MOTA and MOTP, which are in turn used to optimize deep trackers\ndirectly. We experimentally demonstrate that the proposed differentiable\nframework improves the performance of existing multi-object trackers, and we\nestablish a new state of the art on the MOTChallenge benchmark. Our code is\npublicly available from https://github.com/yihongXU/deepMOT.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 21:34:30 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 08:53:00 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2020 14:00:36 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Xu", "Yihong", ""], ["Osep", "Aljosa", ""], ["Ban", "Yutong", ""], ["Horaud", "Radu", ""], ["Leal-Taixe", "Laura", ""], ["Alameda-Pineda", "Xavier", ""]]}, {"id": "1906.06619", "submitter": "Gil Sadeh", "authors": "Gil Sadeh, Lior Fritz, Gabi Shalev and Eduard Oks", "title": "Generating Diverse and Informative Natural Language Fashion Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in multi-modal vision and language tasks enable a new set of\napplications. In this paper, we consider the task of generating natural\nlanguage fashion feedback on outfit images. We collect a unique dataset, which\ncontains outfit images and corresponding positive and constructive fashion\nfeedback. We treat each feedback type separately, and train deep generative\nencoder-decoder models with visual attention, similar to the standard image\ncaptioning pipeline. Following this approach, the generated sentences tend to\nbe too general and non-informative. We propose an alternative decoding\ntechnique based on the Maximum Mutual Information objective function, which\nleads to more diverse and detailed responses. We evaluate our model with common\nlanguage metrics, and also show human evaluation results. This technology is\napplied within the ``Alexa, how do I look?'' feature, publicly available in\nEcho Look devices.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 21:39:34 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Sadeh", "Gil", ""], ["Fritz", "Lior", ""], ["Shalev", "Gabi", ""], ["Oks", "Eduard", ""]]}, {"id": "1906.06620", "submitter": "Gil Sadeh", "authors": "Gil Sadeh, Lior Fritz, Gabi Shalev and Eduard Oks", "title": "Joint Visual-Textual Embedding for Multimodal Style Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a multimodal visual-textual search refinement method for fashion\ngarments. Existing search engines do not enable intuitive, interactive,\nrefinement of retrieved results based on the properties of a particular\nproduct. We propose a method to retrieve similar items, based on a query item\nimage and textual refinement properties. We believe this method can be\nleveraged to solve many real-life customer scenarios, in which a similar item\nin a different color, pattern, length or style is desired. We employ a joint\nembedding training scheme in which product images and their catalog textual\nmetadata are mapped closely in a shared space. This joint visual-textual\nembedding space enables manipulating catalog images semantically, based on\ntextual refinement requirements. We propose a new training objective function,\nMini-Batch Match Retrieval, and demonstrate its superiority over the commonly\nused triplet loss. Additionally, we demonstrate the feasibility of adding an\nattribute extraction module, trained on the same catalog data, and demonstrate\nhow to integrate it within the multimodal search to boost its performance. We\nintroduce an evaluation protocol with an associated benchmark, and compare\nseveral approaches.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 21:50:31 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Sadeh", "Gil", ""], ["Fritz", "Lior", ""], ["Shalev", "Gabi", ""], ["Oks", "Eduard", ""]]}, {"id": "1906.06624", "submitter": "Deniz Oktay", "authors": "Deniz Oktay, Johannes Ball\\'e, Saurabh Singh, Abhinav Shrivastava", "title": "Scalable Model Compression by Entropy Penalized Reparameterization", "comments": "Published in ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a simple and general neural network weight compression approach,\nin which the network parameters (weights and biases) are represented in a\n\"latent\" space, amounting to a reparameterization. This space is equipped with\na learned probability model, which is used to impose an entropy penalty on the\nparameter representation during training, and to compress the representation\nusing a simple arithmetic coder after training. Classification accuracy and\nmodel compressibility is maximized jointly, with the bitrate--accuracy\ntrade-off specified by a hyperparameter. We evaluate the method on the MNIST,\nCIFAR-10 and ImageNet classification benchmarks using six distinct model\narchitectures. Our results show that state-of-the-art model compression can be\nachieved in a scalable and general way without requiring complex procedures\nsuch as multi-stage training.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 22:46:33 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 19:52:15 GMT"}, {"version": "v3", "created": "Sun, 16 Feb 2020 17:51:13 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Oktay", "Deniz", ""], ["Ball\u00e9", "Johannes", ""], ["Singh", "Saurabh", ""], ["Shrivastava", "Abhinav", ""]]}, {"id": "1906.06626", "submitter": "Miroslaw Bober", "authors": "Syed Sameed Husain, Miroslaw Bober", "title": "REMAP: Multi-layer entropy-guided pooling of dense CNN features for\n  image retrieval", "comments": "Submitted to IEEE Trans. Image Processing on 24 May 2018, published\n  22 May 2019", "journal-ref": "IEEE Transactions on Image Processing, Early Access 22 May 2019", "doi": "10.1109/TIP.2019.2917234", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of very large-scale image retrieval,\nfocusing on improving its accuracy and robustness. We target enhanced\nrobustness of search to factors such as variations in illumination, object\nappearance and scale, partial occlusions, and cluttered backgrounds -\nparticularly important when search is performed across very large datasets with\nsignificant variability. We propose a novel CNN-based global descriptor, called\nREMAP, which learns and aggregates a hierarchy of deep features from multiple\nCNN layers, and is trained end-to-end with a triplet loss. REMAP explicitly\nlearns discriminative features which are mutually-supportive and complementary\nat various semantic levels of visual abstraction. These dense local features\nare max-pooled spatially at each layer, within multi-scale overlapping regions,\nbefore aggregation into a single image-level descriptor. To identify the\nsemantically useful regions and layers for retrieval, we propose to measure the\ninformation gain of each region and layer using KL-divergence. Our system\neffectively learns during training how useful various regions and layers are\nand weights them accordingly. We show that such relative entropy-guided\naggregation outperforms classical CNN-based aggregation controlled by SGD. The\nentire framework is trained in an end-to-end fashion, outperforming the latest\nstate-of-the-art results. On image retrieval datasets Holidays, Oxford and\nMPEG, the REMAP descriptor achieves mAP of 95.5%, 91.5%, and 80.1%\nrespectively, outperforming any results published to date. REMAP also formed\nthe core of the winning submission to the Google Landmark Retrieval Challenge\non Kaggle.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 23:02:49 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Husain", "Syed Sameed", ""], ["Bober", "Miroslaw", ""]]}, {"id": "1906.06627", "submitter": "Shashank Kotyan", "authors": "Shashank Kotyan, Danilo Vasconcellos Vargas, and Moe Matsuki", "title": "Representation Quality Of Neural Networks Links To Adversarial Attacks\n  and Defences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have been shown vulnerable to a variety of adversarial\nalgorithms. A crucial step to understanding the rationale for this lack of\nrobustness is to assess the potential of the neural networks' representation to\nencode the existing features. Here, we propose a method to understand the\nrepresentation quality of the neural networks using a novel test based on\nZero-Shot Learning, entitled Raw Zero-Shot. The principal idea is that, if an\nalgorithm learns rich features, such features should be able to interpret\n\"unknown\" classes as an aggregate of previously learned features. This is\nbecause unknown classes usually share several regular features with recognised\nclasses, given the features learned are general enough. We further introduce\ntwo metrics to assess these learned features to interpret unknown classes. One\nis based on inter-cluster validation technique (Davies-Bouldin Index), and the\nother is based on the distance to an approximated ground-truth. Experiments\nsuggest that adversarial defences improve the representation of the\nclassifiers, further suggesting that to improve the robustness of the\nclassifiers, one has to improve the representation quality also. Experiments\nalso reveal a strong association (a high Pearson Correlation and low p-value)\nbetween the metrics and adversarial attacks. Interestingly, the results\nindicate that dynamic routing networks such as CapsNet have better\nrepresentation while current deeper neural networks are trading off\nrepresentation quality for accuracy.\n  Code available at http://bit.ly/RepresentationMetrics.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 23:32:33 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 05:27:20 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2019 09:39:43 GMT"}, {"version": "v4", "created": "Tue, 18 Feb 2020 20:56:36 GMT"}, {"version": "v5", "created": "Thu, 16 Jul 2020 14:49:14 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Kotyan", "Shashank", ""], ["Vargas", "Danilo Vasconcellos", ""], ["Matsuki", "Moe", ""]]}, {"id": "1906.06632", "submitter": "Jian Zheng", "authors": "Jian Zheng, Sudha Krishnamurthy, Ruxin Chen, Min-Hung Chen, Zhenhao\n  Ge, Xiaohua Li", "title": "Image Captioning with Integrated Bottom-Up and Multi-level Residual\n  Top-Down Attention for Game Scene Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning has attracted considerable attention in recent years.\nHowever, little work has been done for game image captioning which has some\nunique characteristics and requirements. In this work we propose a novel game\nimage captioning model which integrates bottom-up attention with a new\nmulti-level residual top-down attention mechanism. Firstly, a lower-level\nresidual top-down attention network is added to the Faster R-CNN based\nbottom-up attention network to address the problem that the latter may lose\nimportant spatial information when extracting regional features. Secondly, an\nupper-level residual top-down attention network is implemented in the caption\ngeneration network to better fuse the extracted regional features for\nsubsequent caption prediction. We create two game datasets to evaluate the\nproposed model. Extensive experiments show that our proposed model outperforms\nexisting baseline models.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 01:45:15 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Zheng", "Jian", ""], ["Krishnamurthy", "Sudha", ""], ["Chen", "Ruxin", ""], ["Chen", "Min-Hung", ""], ["Ge", "Zhenhao", ""], ["Li", "Xiaohua", ""]]}, {"id": "1906.06633", "submitter": "Trung Dung Do", "authors": "Trung Dung Do, Cheng-Bin Jin, Hakil Kim, Van Huan Nguyen", "title": "Mixture separability loss in a deep convolutional network for image\n  classification", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, the cost function is crucial because it measures how\ngood or bad a system is. In image classification, well-known networks only\nconsider modifying the network structures and applying cross-entropy loss at\nthe end of the network. However, using only cross-entropy loss causes a network\nto stop updating weights when all training images are correctly classified.\nThis is the problem of the early saturation. This paper proposes a novel cost\nfunction, called mixture separability loss (MSL), which updates the weights of\nthe network even when most of the training images are accurately predicted. MSL\nconsists of between-class and within-class loss. Between-class loss maximizes\nthe differences between inter-class images, whereas within-class loss minimizes\nthe similarities between intra-class images. We designed the proposed loss\nfunction to attach to different convolutional layers in the network in order to\nutilize intermediate feature maps. Experiments show that a network with MSL\ndeepens the learning process and obtains promising results with some public\ndatasets, such as Street View House Number (SVHN), Canadian Institute for\nAdvanced Research (CIFAR), and our self-collected Inha Computer Vision Lab\n(ICVL) gender dataset.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 01:45:57 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Do", "Trung Dung", ""], ["Jin", "Cheng-Bin", ""], ["Kim", "Hakil", ""], ["Nguyen", "Van Huan", ""]]}, {"id": "1906.06690", "submitter": "Jun Xu", "authors": "Jun Xu, Yingkun Hou, Dongwei Ren, Li Liu, Fan Zhu, Mengyang Yu,\n  Haoqian Wang, Ling Shao", "title": "STAR: A Structure and Texture Aware Retinex Model", "comments": "16 pages, 13 figures, 3 tables, accepted by TIP", "journal-ref": null, "doi": "10.1109/TIP.2020.2974060", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinex theory is developed mainly to decompose an image into the\nillumination and reflectance components by analyzing local image derivatives.\nIn this theory, larger derivatives are attributed to the changes in\nreflectance, while smaller derivatives are emerged in the smooth illumination.\nIn this paper, we utilize exponentiated local derivatives (with an exponent\n{\\gamma}) of an observed image to generate its structure map and texture map.\nThe structure map is produced by been amplified with {\\gamma} > 1, while the\ntexture map is generated by been shrank with {\\gamma} < 1. To this end, we\ndesign exponential filters for the local derivatives, and present their\ncapability on extracting accurate structure and texture maps, influenced by the\nchoices of exponents {\\gamma}. The extracted structure and texture maps are\nemployed to regularize the illumination and reflectance components in Retinex\ndecomposition. A novel Structure and Texture Aware Retinex (STAR) model is\nfurther proposed for illumination and reflectance decomposition of a single\nimage. We solve the STAR model by an alternating optimization algorithm. Each\nsub-problem is transformed into a vectorized least squares regression, with\nclosed-form solutions. Comprehensive experiments on commonly tested datasets\ndemonstrate that, the proposed STAR model produce better quantitative and\nqualitative performance than previous competing methods, on illumination and\nreflectance decomposition, low-light image enhancement, and color correction.\nThe code is publicly available at https://github.com/csjunxu/STAR.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 13:58:52 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 06:36:28 GMT"}, {"version": "v3", "created": "Sun, 30 Jun 2019 07:28:57 GMT"}, {"version": "v4", "created": "Wed, 12 Feb 2020 00:05:38 GMT"}, {"version": "v5", "created": "Wed, 11 Mar 2020 06:41:12 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Xu", "Jun", ""], ["Hou", "Yingkun", ""], ["Ren", "Dongwei", ""], ["Liu", "Li", ""], ["Zhu", "Fan", ""], ["Yu", "Mengyang", ""], ["Wang", "Haoqian", ""], ["Shao", "Ling", ""]]}, {"id": "1906.06693", "submitter": "Jun Li", "authors": "Jun Li and Chengjie Niu and Kai Xu", "title": "Learning Part Generation and Assembly for Structure-aware Shape\n  Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning powerful deep generative models for 3D shape synthesis is largely\nhindered by the difficulty in ensuring plausibility encompassing correct\ntopology and reasonable geometry. Indeed, learning the distribution of\nplausible 3D shapes seems a daunting task for the holistic approaches, given\nthe significant topological variations of 3D objects even within the same\ncategory. Enlightened by the fact that 3D shape structure is characterized as\npart composition and placement, we propose to model 3D shape variations with a\npart-aware deep generative network, coined as PAGENet. The network is composed\nof an array of per-part VAE-GANs, generating semantic parts composing a\ncomplete shape, followed by a part assembly module that estimates a\ntransformation for each part to correlate and assemble them into a plausible\nstructure. Through delegating the learning of part composition and part\nplacement into separate networks, the difficulty of modeling structural\nvariations of 3D shapes is greatly reduced. We demonstrate through both\nqualitative and quantitative evaluations that PAGENet generates 3D shapes with\nplausible, diverse and detailed structure, and show two applications, i.e.,\nsemantic shape segmentation and part-based shape editing.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 14:14:33 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 15:41:30 GMT"}, {"version": "v3", "created": "Tue, 30 Jul 2019 09:05:53 GMT"}, {"version": "v4", "created": "Sat, 25 Jan 2020 14:23:11 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Li", "Jun", ""], ["Niu", "Chengjie", ""], ["Xu", "Kai", ""]]}, {"id": "1906.06697", "submitter": "Michal Kawulok", "authors": "Michal Kawulok and Szymon Piechaczek and Krzysztof Hrynczenko and\n  Pawel Benecki and Daniel Kostrzewa and Jakub Nalepa", "title": "On training deep networks for satellite image super-resolution", "comments": "IGARSS 2019 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The capabilities of super-resolution reconstruction (SRR)---techniques for\nenhancing image spatial resolution---have been recently improved significantly\nby the use of deep convolutional neural networks. Commonly, such networks are\nlearned using huge training sets composed of original images alongside their\nlow-resolution counterparts, obtained with bicubic downsampling. In this paper,\nwe investigate how the SRR performance is influenced by the way such\nlow-resolution training data are obtained, which has not been explored up to\ndate. Our extensive experimental study indicates that the training data\ncharacteristics have a large impact on the reconstruction accuracy, and the\nwidely-adopted approach is not the most effective for dealing with satellite\nimages. Overall, we argue that developing better training data preparation\nroutines may be pivotal in making SRR suitable for real-world applications.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 14:21:23 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Kawulok", "Michal", ""], ["Piechaczek", "Szymon", ""], ["Hrynczenko", "Krzysztof", ""], ["Benecki", "Pawel", ""], ["Kostrzewa", "Daniel", ""], ["Nalepa", "Jakub", ""]]}, {"id": "1906.06698", "submitter": "Xiaosu Zhu", "authors": "Lianli Gao, Xiaosu Zhu, Jingkuan Song, Zhou Zhao and Heng Tao Shen", "title": "Beyond Product Quantization: Deep Progressive Quantization for Image\n  Retrieval", "comments": null, "journal-ref": "Proceedings of the Twenty-Eighth International Joint Conference on\n  Artificial Intelligence 1 (2019) 723-729", "doi": "10.24963/ijcai.2019", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product Quantization (PQ) has long been a mainstream for generating an\nexponentially large codebook at very low memory/time cost. Despite its success,\nPQ is still tricky for the decomposition of high-dimensional vector space, and\nthe retraining of model is usually unavoidable when the code length changes. In\nthis work, we propose a deep progressive quantization (DPQ) model, as an\nalternative to PQ, for large scale image retrieval. DPQ learns the quantization\ncodes sequentially and approximates the original feature space progressively.\nTherefore, we can train the quantization codes with different code lengths\nsimultaneously. Specifically, we first utilize the label information for\nguiding the learning of visual features, and then apply several quantization\nblocks to progressively approach the visual features. Each quantization block\nis designed to be a layer of a convolutional neural network, and the whole\nframework can be trained in an end-to-end manner. Experimental results on the\nbenchmark datasets show that our model significantly outperforms the\nstate-of-the-art for image retrieval. Our model is trained once for different\ncode lengths and therefore requires less computation time. Additional ablation\nstudy demonstrates the effect of each component of our proposed model. Our code\nis released at https://github.com/cfm-uestc/DPQ.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 14:23:01 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 08:11:09 GMT"}, {"version": "v3", "created": "Sat, 5 Dec 2020 03:22:03 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Gao", "Lianli", ""], ["Zhu", "Xiaosu", ""], ["Song", "Jingkuan", ""], ["Zhao", "Zhou", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1906.06699", "submitter": "Xiaosu Zhu", "authors": "Jingkuan Song, Xiaosu Zhu, Lianli Gao, Xin-Shun Xu, Wu Liu, Heng Tao\n  Shen", "title": "Deep Recurrent Quantization for Generating Sequential Binary Codes", "comments": null, "journal-ref": "Proceedings of the Twenty-Eighth International Joint Conference on\n  Artificial Intelligence 1 (2019) 912-918", "doi": "10.24963/ijcai.2019", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantization has been an effective technology in ANN (approximate nearest\nneighbour) search due to its high accuracy and fast search speed. To meet the\nrequirement of different applications, there is always a trade-off between\nretrieval accuracy and speed, reflected by variable code lengths. However, to\nencode the dataset into different code lengths, existing methods need to train\nseveral models, where each model can only produce a specific code length. This\nincurs a considerable training time cost, and largely reduces the flexibility\nof quantization methods to be deployed in real applications. To address this\nissue, we propose a Deep Recurrent Quantization (DRQ) architecture which can\ngenerate sequential binary codes. To the end, when the model is trained, a\nsequence of binary codes can be generated and the code length can be easily\ncontrolled by adjusting the number of recurrent iterations. A shared codebook\nand a scalar factor is designed to be the learnable weights in the deep\nrecurrent quantization block, and the whole framework can be trained in an\nend-to-end manner. As far as we know, this is the first quantization method\nthat can be trained once and generate sequential binary codes. Experimental\nresults on the benchmark datasets show that our model achieves comparable or\neven better performance compared with the state-of-the-art for image retrieval.\nBut it requires significantly less number of parameters and training times. Our\ncode is published online: https://github.com/cfm-uestc/DRQ.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 14:28:25 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 07:57:43 GMT"}, {"version": "v3", "created": "Sat, 5 Dec 2020 03:18:56 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Song", "Jingkuan", ""], ["Zhu", "Xiaosu", ""], ["Gao", "Lianli", ""], ["Xu", "Xin-Shun", ""], ["Liu", "Wu", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1906.06722", "submitter": "Gregor Robinson", "authors": "Gregor Robinson and Ian Grooms", "title": "A fast tunable blurring algorithm for scattered data", "comments": "19 pages, 5 figures", "journal-ref": null, "doi": "10.1137/19M1268781", "report-no": null, "categories": "stat.CO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A blurring algorithm with linear time complexity can reduce the small-scale\ncontent of data observed at scattered locations in a spatially extended domain\nof arbitrary dimension. The method works by forming a Gaussian interpolant of\nthe input data, and then convolving the interpolant with a multiresolution\nGaussian approximation of the Green's function to a differential operator whose\nspectrum can be tuned for problem-specific considerations. Like conventional\nblurring algorithms, which the new algorithm generalizes to data measured at\nlocations other than a uniform grid, applications include deblurring and\nseparation of spatial scales. An example illustrates a possible application\ntoward enabling importance sampling approaches to data assimilation of\ngeophysical observations, which are often scattered over a spatial domain,\nsince blurring observations can make particle filters more effective at state\nestimation of large scales. Another example, motivated by data analysis of\ndynamics like ocean eddies that have strong separation of spatial scales, uses\nthe algorithm to decompose scattered oceanographic float measurements into\nlarge-scale and small-scale components.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 16:27:52 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 17:11:21 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Robinson", "Gregor", ""], ["Grooms", "Ian", ""]]}, {"id": "1906.06765", "submitter": "Yifan Ding", "authors": "Yifan Ding, Liqiang Wang, Huan Zhang, Jinfeng Yi, Deliang Fan and\n  Boqing Gong", "title": "Defending Against Adversarial Attacks Using Random Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep neural networks (DNNs) have become increasingly important and\npopular, the robustness of DNNs is the key to the safety of both the Internet\nand the physical world. Unfortunately, some recent studies show that\nadversarial examples, which are hard to be distinguished from real examples,\ncan easily fool DNNs and manipulate their predictions. Upon observing that\nadversarial examples are mostly generated by gradient-based methods, in this\npaper, we first propose to use a simple yet very effective non-differentiable\nhybrid model that combines DNNs and random forests, rather than hide gradients\nfrom attackers, to defend against the attacks. Our experiments show that our\nmodel can successfully and completely defend the white-box attacks, has a lower\ntransferability, and is quite resistant to three representative types of\nblack-box attacks; while at the same time, our model achieves similar\nclassification accuracy as the original DNNs. Finally, we investigate and\nsuggest a criterion to define where to grow random forests in DNNs.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 20:46:44 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Ding", "Yifan", ""], ["Wang", "Liqiang", ""], ["Zhang", "Huan", ""], ["Yi", "Jinfeng", ""], ["Fan", "Deliang", ""], ["Gong", "Boqing", ""]]}, {"id": "1906.06768", "submitter": "Samah Khawaled", "authors": "Samah Khawaled and Yehoshua Y. Zeevi", "title": "On the Self-Similarity of Natural Stochastic Textures", "comments": "5 pages , 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-similarity is the essence of fractal images and, as such, characterizes\nnatural stochastic textures. This paper is concerned with the property of\nself-similarity in the statistical sense in the case of fully-textured images\nthat contain both stochastic texture and structural (mostly deterministic)\ninformation. We firstly decompose a textured image into two layers\ncorresponding to its texture and structure, and show that the layer\nrepresenting the stochastic texture is characterized by random phase of uniform\ndistribution, unlike the phase of the structured information which is coherent.\nThe uniform distribution of the the random phase is verified by using a\nsuitable hypothesis testing framework. We proceed by proposing two approaches\nto assessment of self-similarity. The first is based on patch-wise calculation\nof the mutual information, while the second measures the mutual information\nthat exists across scales. Quantifying the extent of self-similarity by means\nof mutual information is of paramount importance in the analysis of natural\nstochastic textures that are encountered in medical imaging, geology,\nagriculture and in computer vision algorithms that are designed for application\non fully-textures images.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 21:01:20 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Khawaled", "Samah", ""], ["Zeevi", "Yehoshua Y.", ""]]}, {"id": "1906.06789", "submitter": "Christoph Schoeller", "authors": "Annkathrin Kr\\\"ammer, Christoph Sch\\\"oller, Dhiraj Gulati,\n  Venkatnarayanan Lakshminarasimhan, Franz Kurz, Dominik Rosenbaum, Claus Lenz,\n  Alois Knoll", "title": "Providentia - A Large-Scale Sensor System for the Assistance of\n  Autonomous Vehicles and Its Evaluation", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The environmental perception of an autonomous vehicle is limited by its\nphysical sensor ranges and algorithmic performance, as well as by occlusions\nthat degrade its understanding of an ongoing traffic situation. This not only\nposes a significant threat to safety and limits driving speeds, but it can also\nlead to inconvenient maneuvers. Intelligent Infrastructure Systems can help to\nalleviate these problems. An Intelligent Infrastructure System can fill in the\ngaps in a vehicle's perception and extend its field of view by providing\nadditional detailed information about its surroundings, in the form of a\ndigital model of the current traffic situation, i.e. a digital twin. However,\ndetailed descriptions of such systems and working prototypes demonstrating\ntheir feasibility are scarce. In this paper, we propose a hardware and software\narchitecture that enables such a reliable Intelligent Infrastructure System to\nbe built. We have implemented this system in the real world and demonstrate its\nability to create an accurate digital twin of an extended highway stretch, thus\nenhancing an autonomous vehicle's perception beyond the limits of its on-board\nsensors. Furthermore, we evaluate the digital twin with respect to its spatial\naccuracy, precision, and recall by using aerial images and earth observation\nmethods for generating ground truth data.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 22:57:54 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 07:41:54 GMT"}, {"version": "v3", "created": "Wed, 19 Jun 2019 09:04:38 GMT"}, {"version": "v4", "created": "Mon, 20 Jul 2020 15:54:39 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Kr\u00e4mmer", "Annkathrin", ""], ["Sch\u00f6ller", "Christoph", ""], ["Gulati", "Dhiraj", ""], ["Lakshminarasimhan", "Venkatnarayanan", ""], ["Kurz", "Franz", ""], ["Rosenbaum", "Dominik", ""], ["Lenz", "Claus", ""], ["Knoll", "Alois", ""]]}, {"id": "1906.06792", "submitter": "Steven Hickson", "authors": "Steven Hickson, Karthik Raveendran, Alireza Fathi, Kevin Murphy, Irfan\n  Essa", "title": "Floors are Flat: Leveraging Semantics for Real-Time Surface Normal\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We propose 4 insights that help to significantly improve the performance of\ndeep learning models that predict surface normals and semantic labels from a\nsingle RGB image. These insights are: (1) denoise the \"ground truth\" surface\nnormals in the training set to ensure consistency with the semantic labels; (2)\nconcurrently train on a mix of real and synthetic data, instead of pretraining\non synthetic and finetuning on real; (3) jointly predict normals and semantics\nusing a shared model, but only backpropagate errors on pixels that have valid\ntraining labels; (4) slim down the model and use grayscale instead of color\ninputs. Despite the simplicity of these steps, we demonstrate consistently\nimproved results on several datasets, using a model that runs at 12 fps on a\nstandard mobile phone.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 23:01:32 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Hickson", "Steven", ""], ["Raveendran", "Karthik", ""], ["Fathi", "Alireza", ""], ["Murphy", "Kevin", ""], ["Essa", "Irfan", ""]]}, {"id": "1906.06794", "submitter": "Tom Tirer", "authors": "Tom Tirer, Raja Giryes", "title": "Back-Projection based Fidelity Term for Ill-Posed Linear Inverse\n  Problems", "comments": null, "journal-ref": "IEEE Transactions on Image Processing, 2020", "doi": "10.1109/TIP.2020.2988779", "report-no": null, "categories": "cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ill-posed linear inverse problems appear in many image processing\napplications, such as deblurring, super-resolution and compressed sensing. Many\nrestoration strategies involve minimizing a cost function, which is composed of\nfidelity and prior terms, balanced by a regularization parameter. While a vast\namount of research has been focused on different prior models, the fidelity\nterm is almost always chosen to be the least squares (LS) objective, that\nencourages fitting the linearly transformed optimization variable to the\nobservations. In this paper, we examine a different fidelity term, which has\nbeen implicitly used by the recently proposed iterative denoising and backward\nprojections (IDBP) framework. This term encourages agreement between the\nprojection of the optimization variable onto the row space of the linear\noperator and the pseudo-inverse of the linear operator (\"back-projection\")\napplied on the observations. We analytically examine the difference between the\ntwo fidelity terms for Tikhonov regularization and identify cases (such as a\nbadly conditioned linear operator) where the new term has an advantage over the\nstandard LS one. Moreover, we demonstrate empirically that the behavior of the\ntwo induced cost functions for sophisticated convex and non-convex priors, such\nas total-variation, BM3D, and deep generative models, correlates with the\nobtained theoretical analysis.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 23:27:37 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 12:11:19 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Tirer", "Tom", ""], ["Giryes", "Raja", ""]]}, {"id": "1906.06798", "submitter": "Jasper Uijlings", "authors": "Jasper R. R. Uijlings, Mykhaylo Andriluka, Vittorio Ferrari", "title": "Panoptic Image Annotation with a Collaborative Assistant", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to reduce the time to annotate images for panoptic\nsegmentation, which requires annotating segmentation masks and class labels for\nall object instances and stuff regions. We formulate our approach as a\ncollaborative process between an annotator and an automated assistant who take\nturns to jointly annotate an image using a predefined pool of segments. Actions\nperformed by the annotator serve as a strong contextual signal. The assistant\nintelligently reacts to this signal by annotating other parts of the image on\nits own, which reduces the amount of work required by the annotator. We perform\nthorough experiments on the COCO panoptic dataset, both in simulation and with\nhuman annotators. These demonstrate that our approach is significantly faster\nthan the recent machine-assisted interface of [4], and 2.4x to 5x faster than\nmanual polygon drawing. Finally, we show on ADE20k that our method can be used\nto efficiently annotate new datasets, bootstrapping from a very small amount of\nannotated data.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 00:03:05 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 16:28:43 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2020 14:19:26 GMT"}, {"version": "v4", "created": "Tue, 15 Dec 2020 17:57:37 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Uijlings", "Jasper R. R.", ""], ["Andriluka", "Mykhaylo", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1906.06804", "submitter": "Ilya Kavalerov", "authors": "Ilya Kavalerov and Weilin Li and Wojciech Czaja and Rama Chellappa", "title": "Three-Dimensional Fourier Scattering Transform and Classification of\n  Hyperspectral Images", "comments": "Accepted to IEEE Transactions On Geoscience And Remote Sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in machine learning and signal processing have resulted\nin many new techniques that are able to effectively capture the intrinsic yet\ncomplex properties of hyperspectral imagery. Tasks ranging from anomaly\ndetection to classification can now be solved by taking advantage of very\nefficient algorithms which have their roots in representation theory and in\ncomputational approximation. Time-frequency methods are one example of such\ntechniques. They provide means to analyze and extract the spectral content from\ndata. On the other hand, hierarchical methods such as neural networks\nincorporate spatial information across scales and model multiple levels of\ndependencies between spectral features. Both of these approaches have recently\nbeen proven to provide significant advances in the spectral-spatial\nclassification of hyperspectral imagery. The 3D Fourier scattering transform,\nwhich is introduced in this paper, is an amalgamation of time-frequency\nrepresentations with neural network architectures. It leverages the benefits\nprovided by the Short-Time Fourier Transform with the numerical efficiency of\ndeep learning network structures. We test the proposed method on several\nstandard hyperspectral datasets, and we present results that indicate that the\n3D Fourier scattering transform is highly effective at representing spectral\ncontent when compared with other state-of-the-art spectral-spatial\nclassification methods.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 00:47:31 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 20:12:20 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Kavalerov", "Ilya", ""], ["Li", "Weilin", ""], ["Czaja", "Wojciech", ""], ["Chellappa", "Rama", ""]]}, {"id": "1906.06813", "submitter": "Sangwoo Cho", "authors": "Sangwoo Cho, Hassan Foroosh", "title": "A Temporal Sequence Learning for Action Recognition and Prediction", "comments": "10 pages, 8 figures, 2018 IEEE Winter Conference on Applications of\n  Computer Vision (WACV)", "journal-ref": "{IEEE} Winter Conference on Applications of Computer Vision, 2018,\n  352-361", "doi": "10.1109/WACV.2018.00045", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work\\footnote {This work was supported in part by the National\nScience Foundation under grant IIS-1212948.}, we present a method to represent\na video with a sequence of words, and learn the temporal sequencing of such\nwords as the key information for predicting and recognizing human actions. We\nleverage core concepts from the Natural Language Processing (NLP) literature\nused in sentence classification to solve the problems of action prediction and\naction recognition. Each frame is converted into a word that is represented as\na vector using the Bag of Visual Words (BoW) encoding method. The words are\nthen combined into a sentence to represent the video, as a sentence. The\nsequence of words in different actions are learned with a simple but effective\nTemporal Convolutional Neural Network (T-CNN) that captures the temporal\nsequencing of information in a video sentence. We demonstrate that a key\ncharacteristic of the proposed method is its low-latency, i.e. its ability to\npredict an action accurately with a partial sequence (sentence). Experiments on\ntwo datasets, \\textit{UCF101} and \\textit{HMDB51} show that the method on\naverage reaches 95\\% of its accuracy within half the video frames. Results,\nalso demonstrate that our method achieves compatible state-of-the-art\nperformance in action recognition (i.e. at the completion of the sentence) in\naddition to action prediction.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 01:33:21 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Cho", "Sangwoo", ""], ["Foroosh", "Hassan", ""]]}, {"id": "1906.06818", "submitter": "Adam Kosiorek", "authors": "Adam R. Kosiorek, Sara Sabour, Yee Whye Teh, Geoffrey E. Hinton", "title": "Stacked Capsule Autoencoders", "comments": "NeurIPS 2019; 14 pages, 7 figures, 4 tables, code is available at\n  https://github.com/google-research/google-research/tree/master/stacked_capsule_autoencoders", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objects are composed of a set of geometrically organized parts. We introduce\nan unsupervised capsule autoencoder (SCAE), which explicitly uses geometric\nrelationships between parts to reason about objects. Since these relationships\ndo not depend on the viewpoint, our model is robust to viewpoint changes. SCAE\nconsists of two stages. In the first stage, the model predicts presences and\nposes of part templates directly from the image and tries to reconstruct the\nimage by appropriately arranging the templates. In the second stage, SCAE\npredicts parameters of a few object capsules, which are then used to\nreconstruct part poses. Inference in this model is amortized and performed by\noff-the-shelf neural encoders, unlike in previous capsule networks. We find\nthat object capsule presences are highly informative of the object class, which\nleads to state-of-the-art results for unsupervised classification on SVHN (55%)\nand MNIST (98.7%). The code is available at\nhttps://github.com/google-research/google-research/tree/master/stacked_capsule_autoencoders\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 02:31:37 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 16:29:43 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Kosiorek", "Adam R.", ""], ["Sabour", "Sara", ""], ["Teh", "Yee Whye", ""], ["Hinton", "Geoffrey E.", ""]]}, {"id": "1906.06819", "submitter": "Hanyu Li", "authors": "Hanyu Li, Jingjing Li, and Wei Wang", "title": "A Fusion Adversarial Underwater Image Enhancement Network with a Public\n  Test Dataset", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underwater image enhancement algorithms have attracted much attention in\nunderwater vision task. However, these algorithms are mainly evaluated on\ndifferent data sets and different metrics. In this paper, we set up an\neffective and pubic underwater test dataset named U45 including the color\ncasts, low contrast and haze-like effects of underwater degradation and propose\na fusion adversarial network for enhancing underwater images. Meanwhile, the\nwell-designed the adversarial loss including Lgt loss and Lfe loss is presented\nto focus on image features of ground truth, and image features of the image\nenhanced by fusion enhance method, respectively. The proposed network corrects\ncolor casts effectively and owns faster testing time with fewer parameters.\nExperiment results on U45 dataset demonstrate that the proposed method achieves\nbetter or comparable performance than the other state-of-the-art methods in\nterms of qualitative and quantitative evaluations. Moreover, an ablation study\ndemonstrates the contributions of each component, and the application test\nfurther shows the effectiveness of the enhanced images.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 02:41:42 GMT"}, {"version": "v2", "created": "Sun, 30 Jun 2019 02:25:13 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Li", "Hanyu", ""], ["Li", "Jingjing", ""], ["Wang", "Wei", ""]]}, {"id": "1906.06822", "submitter": "Sangwoo Cho", "authors": "Sangwoo Cho, Hassan Foroosh", "title": "Spatio-Temporal Fusion Networks for Action Recognition", "comments": null, "journal-ref": "Asian Conference on Computer Vision (2018) 347-364", "doi": "10.1007/978-3-030-20887-5_22", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The video based CNN works have focused on effective ways to fuse appearance\nand motion networks, but they typically lack utilizing temporal information\nover video frames. In this work, we present a novel spatio-temporal fusion\nnetwork (STFN) that integrates temporal dynamics of appearance and motion\ninformation from entire videos. The captured temporal dynamic information is\nthen aggregated for a better video level representation and learned via\nend-to-end training. The spatio-temporal fusion network consists of two set of\nResidual Inception blocks that extract temporal dynamics and a fusion\nconnection for appearance and motion features. The benefits of STFN are: (a) it\ncaptures local and global temporal dynamics of complementary data to learn\nvideo-wide information; and (b) it is applicable to any network for video\nclassification to boost performance. We explore a variety of design choices for\nSTFN and verify how the network performance is varied with the ablation\nstudies. We perform experiments on two challenging human activity datasets,\nUCF101 and HMDB51, and achieve the state-of-the-art results with the best\nnetwork.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 02:59:11 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Cho", "Sangwoo", ""], ["Foroosh", "Hassan", ""]]}, {"id": "1906.06832", "submitter": "Linnan Wang", "authors": "Linnan Wang, Saining Xie, Teng Li, Rodrigo Fonseca, Yuandong Tian", "title": "Sample-Efficient Neural Architecture Search by Learning Action Space", "comments": "Accepted at TPAMI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) has emerged as a promising technique for\nautomatic neural network design. However, existing MCTS based NAS approaches\noften utilize manually designed action space, which is not directly related to\nthe performance metric to be optimized (e.g., accuracy), leading to\nsample-inefficient explorations of architectures. To improve the sample\nefficiency, this paper proposes Latent Action Neural Architecture Search\n(LaNAS), which learns actions to recursively partition the search space into\ngood or bad regions that contain networks with similar performance metrics.\nDuring the search phase, as different action sequences lead to regions with\ndifferent performance, the search efficiency can be significantly improved by\nbiasing towards the good regions. On three NAS tasks, empirical results\ndemonstrate that LaNAS is at least an order more sample efficient than baseline\nmethods including evolutionary algorithms, Bayesian optimizations, and random\nsearch. When applied in practice, both one-shot and regular LaNAS consistently\noutperform existing results. Particularly, LaNAS achieves 99.0% accuracy on\nCIFAR-10 and 80.8% top1 accuracy at 600 MFLOPS on ImageNet in only 800 samples,\nsignificantly outperforming AmoebaNet with 33x fewer samples. Our code is\npublicly available at https://github.com/facebookresearch/LaMCTS.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 03:50:25 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 19:13:16 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Wang", "Linnan", ""], ["Xie", "Saining", ""], ["Li", "Teng", ""], ["Fonseca", "Rodrigo", ""], ["Tian", "Yuandong", ""]]}, {"id": "1906.06834", "submitter": "Jun Xu", "authors": "Yingkun Hou, Jun Xu, Mingxia Liu, Guanghai Liu, Li Liu, Fan Zhu, Ling\n  Shao", "title": "NLH: A Blind Pixel-level Non-local Method for Real-world Image Denoising", "comments": "14 pages, 9 figures, 10 tables, accept by IEEE TIP", "journal-ref": null, "doi": "10.1109/TIP.2020.2980116", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-local self similarity (NSS) is a powerful prior of natural images for\nimage denoising. Most of existing denoising methods employ similar patches,\nwhich is a patch-level NSS prior. In this paper, we take one step forward by\nintroducing a pixel-level NSS prior, i.e., searching similar pixels across a\nnon-local region. This is motivated by the fact that finding closely similar\npixels is more feasible than similar patches in natural images, which can be\nused to enhance image denoising performance. With the introduced pixel-level\nNSS prior, we propose an accurate noise level estimation method, and then\ndevelop a blind image denoising method based on the lifting Haar transform and\nWiener filtering techniques. Experiments on benchmark datasets demonstrate\nthat, the proposed method achieves much better performance than previous\nnon-deep methods, and is still competitive with existing state-of-the-art deep\nlearning based methods on real-world image denoising. The code is publicly\navailable at https://github.com/njusthyk1972/NLH.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 04:08:42 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 07:34:22 GMT"}, {"version": "v3", "created": "Sun, 4 Aug 2019 05:27:15 GMT"}, {"version": "v4", "created": "Sat, 14 Sep 2019 05:34:51 GMT"}, {"version": "v5", "created": "Sun, 8 Mar 2020 15:25:48 GMT"}, {"version": "v6", "created": "Wed, 11 Mar 2020 06:37:58 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Hou", "Yingkun", ""], ["Xu", "Jun", ""], ["Liu", "Mingxia", ""], ["Liu", "Guanghai", ""], ["Liu", "Li", ""], ["Zhu", "Fan", ""], ["Shao", "Ling", ""]]}, {"id": "1906.06841", "submitter": "Biao Jia", "authors": "Biao Jia, Jonathan Brandt, Radomir Mech, Byungmoon Kim, Dinesh Manocha", "title": "LPaintB: Learning to Paint from Self-Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel reinforcement learning-based natural media painting\nalgorithm. Our goal is to reproduce a reference image using brush strokes and\nwe encode the objective through observations. Our formulation takes into\naccount that the distribution of the reward in the action space is sparse and\ntraining a reinforcement learning algorithm from scratch can be difficult. We\npresent an approach that combines self-supervised learning and reinforcement\nlearning to effectively transfer negative samples into positive ones and change\nthe reward distribution. We demonstrate the benefits of our painting agent to\nreproduce reference images with brush strokes. The training phase takes about\none hour and the runtime algorithm takes about 30 seconds on a GTX1080 GPU\nreproducing a 1000x800 image with 20,000 strokes.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 04:52:15 GMT"}, {"version": "v2", "created": "Sat, 21 Sep 2019 15:14:21 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Jia", "Biao", ""], ["Brandt", "Jonathan", ""], ["Mech", "Radomir", ""], ["Kim", "Byungmoon", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1906.06854", "submitter": "Jong Chul Ye", "authors": "Yoseob Han, Junyoung Kim, and Jong Chul Ye", "title": "Differentiated Backprojection Domain Deep Learning for Conebeam Artifact\n  Removal", "comments": "This paper is accepted for IEEE Trans. Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conebeam CT using a circular trajectory is quite often used for various\napplications due to its relative simple geometry. For conebeam geometry,\nFeldkamp, Davis and Kress algorithm is regarded as the standard reconstruction\nmethod, but this algorithm suffers from so-called conebeam artifacts as the\ncone angle increases. Various model-based iterative reconstruction methods have\nbeen developed to reduce the cone-beam artifacts, but these algorithms usually\nrequire multiple applications of computational expensive forward and\nbackprojections. In this paper, we develop a novel deep learning approach for\naccurate conebeam artifact removal. In particular, our deep network, designed\non the differentiated backprojection domain, performs a data-driven inversion\nof an ill-posed deconvolution problem associated with the Hilbert transform.\nThe reconstruction results along the coronal and sagittal directions are then\ncombined using a spectral blending technique to minimize the spectral leakage.\nExperimental results show that our method outperforms the existing iterative\nmethods despite significantly reduced runtime complexity.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 05:59:33 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 13:05:48 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Han", "Yoseob", ""], ["Kim", "Junyoung", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1906.06874", "submitter": "Zhi-Song Liu", "authors": "Zhi-Song Liu, Li-Wen Wang, Chu-Tak Li and Wan-Chi Siu", "title": "Hierarchical Back Projection Network for Image Super-Resolution", "comments": null, "journal-ref": "2019 IEEE Computer Society Conference on Computer Vision and\n  Pattern Recognition", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based single image super-resolution methods use a large number\nof training datasets and have recently achieved great quality progress both\nquantitatively and qualitatively. Most deep networks focus on nonlinear mapping\nfrom low-resolution inputs to high-resolution outputs via residual learning\nwithout exploring the feature abstraction and analysis. We propose a\nHierarchical Back Projection Network (HBPN), that cascades multiple HourGlass\n(HG) modules to bottom-up and top-down process features across all scales to\ncapture various spatial correlations and then consolidates the best\nrepresentation for reconstruction. We adopt the back projection blocks in our\nproposed network to provide the error correlated up and down-sampling process\nto replace simple deconvolution and pooling process for better estimation. A\nnew Softmax based Weighted Reconstruction (WR) process is used to combine the\noutputs of HG modules to further improve super-resolution. Experimental results\non various datasets (including the validation dataset, NTIRE2019, of the Real\nImage Super-resolution Challenge) show that our proposed approach can achieve\nand improve the performance of the state-of-the-art methods for different\nscaling factors.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 07:23:01 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 11:08:50 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Liu", "Zhi-Song", ""], ["Wang", "Li-Wen", ""], ["Li", "Chu-Tak", ""], ["Siu", "Wan-Chi", ""]]}, {"id": "1906.06876", "submitter": "Hong Huy Nguyen", "authors": "Huy H. Nguyen, Fuming Fang, Junichi Yamagishi, Isao Echizen", "title": "Multi-task Learning For Detecting and Segmenting Manipulated Facial\n  Images and Videos", "comments": "Accepted to be Published in Proceedings of the IEEE International\n  Conference on Biometrics: Theory, Applications and Systems (BTAS) 2019,\n  Florida, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting manipulated images and videos is an important topic in digital\nmedia forensics. Most detection methods use binary classification to determine\nthe probability of a query being manipulated. Another important topic is\nlocating manipulated regions (i.e., performing segmentation), which are mostly\ncreated by three commonly used attacks: removal, copy-move, and splicing. We\nhave designed a convolutional neural network that uses the multi-task learning\napproach to simultaneously detect manipulated images and videos and locate the\nmanipulated regions for each query. Information gained by performing one task\nis shared with the other task and thereby enhance the performance of both\ntasks. A semi-supervised learning approach is used to improve the network's\ngenerability. The network includes an encoder and a Y-shaped decoder.\nActivation of the encoded features is used for the binary classification. The\noutput of one branch of the decoder is used for segmenting the manipulated\nregions while that of the other branch is used for reconstructing the input,\nwhich helps improve overall performance. Experiments using the FaceForensics\nand FaceForensics++ databases demonstrated the network's effectiveness against\nfacial reenactment attacks and face swapping attacks as well as its ability to\ndeal with the mismatch condition for previously seen attacks. Moreover,\nfine-tuning using just a small amount of data enables the network to deal with\nunseen attacks.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 07:27:54 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Nguyen", "Huy H.", ""], ["Fang", "Fuming", ""], ["Yamagishi", "Junichi", ""], ["Echizen", "Isao", ""]]}, {"id": "1906.06878", "submitter": "Jun Xu", "authors": "Jun Xu, Yuan Huang, Ming-Ming Cheng, Li Liu, Fan Zhu, Zhou Xu, Ling\n  Shao", "title": "Noisy-As-Clean: Learning Self-supervised Denoising from the Corrupted\n  Image", "comments": "12 pages, 9 figures, 6 tables, the first two authors contribute\n  equally", "journal-ref": null, "doi": "10.1109/TIP.2020.3026622", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised deep networks have achieved promisingperformance on image\ndenoising, by learning image priors andnoise statistics on plenty pairs of\nnoisy and clean images. Unsupervised denoising networks are trained with only\nnoisy images. However, for an unseen corrupted image, both supervised\nandunsupervised networks ignore either its particular image prior, the noise\nstatistics, or both. That is, the networks learned from external images\ninherently suffer from a domain gap problem: the image priors and noise\nstatistics are very different between the training and test images. This\nproblem becomes more clear when dealing with the signal dependent realistic\nnoise. To circumvent this problem, in this work, we propose a novel\n\"Noisy-As-Clean\" (NAC) strategy of training self-supervised denoising networks.\nSpecifically, the corrupted test image is directly taken as the \"clean\" target,\nwhile the inputs are synthetic images consisted of this corrupted image and a\nsecond and similar corruption. A simple but useful observation on our NAC is:\nas long as the noise is weak, it is feasible to learn a self-supervised network\nonly with the corrupted image, approximating the optimal parameters of a\nsupervised network learned with pairs of noisy and clean images. Experiments on\nsynthetic and realistic noise removal demonstrate that, the DnCNN and ResNet\nnetworks trained with our self-supervised NAC strategy achieve comparable or\nbetter performance than the original ones and previous\nsupervised/unsupervised/self-supervised networks. The code is publicly\navailable at https://github.com/csjunxu/Noisy-As-Clean.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 07:39:00 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 06:41:07 GMT"}, {"version": "v3", "created": "Thu, 4 Jul 2019 07:20:28 GMT"}, {"version": "v4", "created": "Sat, 9 May 2020 09:04:17 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Xu", "Jun", ""], ["Huang", "Yuan", ""], ["Cheng", "Ming-Ming", ""], ["Liu", "Li", ""], ["Zhu", "Fan", ""], ["Xu", "Zhou", ""], ["Shao", "Ling", ""]]}, {"id": "1906.06892", "submitter": "Yaxian Xia", "authors": "Yaxian Xia and Lun Huang and Wenmin Wang and Xiaoyong Wei and Wenmin\n  Wang", "title": "ParNet: Position-aware Aggregated Relation Network for Image-Text\n  matching", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploring fine-grained relationship between entities(e.g. objects in image or\nwords in sentence) has great contribution to understand multimedia content\nprecisely. Previous attention mechanism employed in image-text matching either\ntakes multiple self attention steps to gather correspondences or uses image\nobjects (or words) as context to infer image-text similarity. However, they\nonly take advantage of semantic information without considering that objects'\nrelative position also contributes to image understanding. To this end, we\nintroduce a novel position-aware relation module to model both the semantic and\nspatial relationship simultaneously for image-text matching in this paper.\nGiven an image, our method utilizes the location of different objects to\ncapture spatial relationship innovatively. With the combination of semantic and\nspatial relationship, it's easier to understand the content of different\nmodalities (images and sentences) and capture fine-grained latent\ncorrespondences of image-text pairs. Besides, we employ a two-step aggregated\nrelation module to capture interpretable alignment of image-text pairs. The\nfirst step, we call it intra-modal relation mechanism, in which we computes\nresponses between different objects in an image or different words in a\nsentence separately; The second step, we call it inter-modal relation\nmechanism, in which the query plays a role of textual context to refine the\nrelationship among object proposals in an image. In this way, our\nposition-aware aggregated relation network (ParNet) not only knows which\nentities are relevant by attending on different objects (words) adaptively, but\nalso adjust the inter-modal correspondence according to the latent alignments\naccording to query's content. Our approach achieves the state-of-the-art\nresults on MS-COCO dataset.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 08:26:43 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Xia", "Yaxian", ""], ["Huang", "Lun", ""], ["Wang", "Wenmin", ""], ["Wei", "Xiaoyong", ""], ["Wang", "Wenmin", ""]]}, {"id": "1906.06919", "submitter": "Shuyu Cheng", "authors": "Shuyu Cheng, Yinpeng Dong, Tianyu Pang, Hang Su, Jun Zhu", "title": "Improving Black-box Adversarial Attacks with a Transfer-based Prior", "comments": "NeurIPS 2019; Code available at\n  https://github.com/thu-ml/Prior-Guided-RGF", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the black-box adversarial setting, where the adversary has to\ngenerate adversarial perturbations without access to the target models to\ncompute gradients. Previous methods tried to approximate the gradient either by\nusing a transfer gradient of a surrogate white-box model, or based on the query\nfeedback. However, these methods often suffer from low attack success rates or\npoor query efficiency since it is non-trivial to estimate the gradient in a\nhigh-dimensional space with limited information. To address these problems, we\npropose a prior-guided random gradient-free (P-RGF) method to improve black-box\nadversarial attacks, which takes the advantage of a transfer-based prior and\nthe query information simultaneously. The transfer-based prior given by the\ngradient of a surrogate model is appropriately integrated into our algorithm by\nan optimal coefficient derived by a theoretical analysis. Extensive experiments\ndemonstrate that our method requires much fewer queries to attack black-box\nmodels with higher success rates compared with the alternative state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 09:40:32 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 07:56:53 GMT"}, {"version": "v3", "created": "Sun, 26 Jul 2020 14:00:51 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Cheng", "Shuyu", ""], ["Dong", "Yinpeng", ""], ["Pang", "Tianyu", ""], ["Su", "Hang", ""], ["Zhu", "Jun", ""]]}, {"id": "1906.06962", "submitter": "Ayush Dewan", "authors": "Ayush Dewan and Wolfram Burgard", "title": "DeepTemporalSeg: Temporally Consistent Semantic Segmentation of 3D LiDAR\n  Scans", "comments": "Accepted for ICRA-2020. Code and dataset available at\n  https://github.com/ayushais/DBLiDARNet. Added results for Semantic Kitti\n  Dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the semantic characteristics of the environment is a key\nenabler for autonomous robot operation. In this paper, we propose a deep\nconvolutional neural network (DCNN) for the semantic segmentation of a LiDAR\nscan into the classes car, pedestrian or bicyclist. This architecture is based\non dense blocks and efficiently utilizes depth separable convolutions to limit\nthe number of parameters while still maintaining state-of-the-art performance.\nTo make the predictions from the DCNN temporally consistent, we propose a Bayes\nfilter based method. This method uses the predictions from the neural network\nto recursively estimate the current semantic state of a point in a scan. This\nrecursive estimation uses the knowledge gained from previous scans, thereby\nmaking the predictions temporally consistent and robust towards isolated\nerroneous predictions. We compare the performance of our proposed architecture\nwith other state-of-the-art neural network architectures and report substantial\nimprovement. For the proposed Bayes filter approach, we show results on various\nsequences in the KITTI tracking benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 11:35:17 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 18:56:16 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2020 09:02:23 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Dewan", "Ayush", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1906.06972", "submitter": "Yifan Jiang", "authors": "Yifan Jiang, Xinyu Gong, Ding Liu, Yu Cheng, Chen Fang, Xiaohui Shen,\n  Jianchao Yang, Pan Zhou, Zhangyang Wang", "title": "EnlightenGAN: Deep Light Enhancement without Paired Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based methods have achieved remarkable success in image\nrestoration and enhancement, but are they still competitive when there is a\nlack of paired training data? As one such example, this paper explores the\nlow-light image enhancement problem, where in practice it is extremely\nchallenging to simultaneously take a low-light and a normal-light photo of the\nsame visual scene. We propose a highly effective unsupervised generative\nadversarial network, dubbed EnlightenGAN, that can be trained without\nlow/normal-light image pairs, yet proves to generalize very well on various\nreal-world test images. Instead of supervising the learning using ground truth\ndata, we propose to regularize the unpaired training using the information\nextracted from the input itself, and benchmark a series of innovations for the\nlow-light image enhancement problem, including a global-local discriminator\nstructure, a self-regularized perceptual loss fusion, and attention mechanism.\nThrough extensive experiments, our proposed approach outperforms recent methods\nunder a variety of metrics in terms of visual quality and subjective user\nstudy. Thanks to the great flexibility brought by unpaired training,\nEnlightenGAN is demonstrated to be easily adaptable to enhancing real-world\nimages from various domains. The code is available at\n\\url{https://github.com/yueruchen/EnlightenGAN}\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 11:54:20 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2021 18:22:22 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Jiang", "Yifan", ""], ["Gong", "Xinyu", ""], ["Liu", "Ding", ""], ["Cheng", "Yu", ""], ["Fang", "Chen", ""], ["Shen", "Xiaohui", ""], ["Yang", "Jianchao", ""], ["Zhou", "Pan", ""], ["Wang", "Zhangyang", ""]]}, {"id": "1906.06978", "submitter": "Nikolai Ufer", "authors": "Nikolai Ufer, Kam To Lui, Katja Schwarz, Paul Warkentin, Bj\\\"orn Ommer", "title": "Multi-Scale Convolutions for Learning Context Aware Feature\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding semantic correspondences is a challenging problem. With the\nbreakthrough of CNNs stronger features are available for tasks like\nclassification but not specifically for the requirements of semantic matching.\nIn the following we present a weakly supervised metric learning approach which\ngenerates stronger features by encoding far more context than previous methods.\nFirst, we generate more suitable training data using a geometrically informed\ncorrespondence mining method which is less prone to spurious matches and\nrequires only image category labels as supervision. Second, we introduce a new\nconvolutional layer which is a learned mixture of differently strided\nconvolutions and allows the network to encode implicitly more context while\npreserving matching accuracy at the same time. The strong geometric encoding on\nthe feature side enables us to learn a semantic flow network, which generates\nmore natural deformations than parametric transformation based models and is\nable to jointly predict foreground regions at the same time. Our semantic flow\nnetwork outperforms current state-of-the-art on several semantic matching\nbenchmarks and the learned features show astonishing performance regarding\nsimple nearest neighbor matching.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 12:03:59 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Ufer", "Nikolai", ""], ["Lui", "Kam To", ""], ["Schwarz", "Katja", ""], ["Warkentin", "Paul", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "1906.07008", "submitter": "Qiangqiang Wu", "authors": "Qiangqiang Wu, Zhihui Chen, Lin Cheng, Yan Yan, Bo Li, Hanzi Wang", "title": "Hallucinated Adversarial Learning for Robust Visual Tracking", "comments": "Visual object tracking, data hallucination", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can easily learn new concepts from just a single exemplar, mainly due\nto their remarkable ability to imagine or hallucinate what the unseen exemplar\nmay look like in different settings. Incorporating such an ability to\nhallucinate diverse new samples of the tracked instance can help the trackers\nalleviate the over-fitting problem in the low-data tracking regime. To achieve\nthis, we propose an effective adversarial approach, denoted as adversarial\n\"hallucinator\" (AH), for robust visual tracking. The proposed AH is designed to\nfirstly learn transferable non-linear deformations between a pair of\nsame-identity instances, and then apply these deformations to an unseen tracked\ninstance in order to generate diverse positive training samples. By\nincorporating AH into an online tracking-by-detection framework, we propose the\nhallucinated adversarial tracker (HAT), which jointly optimizes AH with an\nonline classifier (e.g., MDNet) in an end-to-end manner. In addition, a novel\nselective deformation transfer (SDT) method is presented to better select the\ndeformations which are more suitable for transfer. Extensive experiments on 3\npopular benchmarks demonstrate that our HAT achieves the state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 13:02:23 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Wu", "Qiangqiang", ""], ["Chen", "Zhihui", ""], ["Cheng", "Lin", ""], ["Yan", "Yan", ""], ["Li", "Bo", ""], ["Wang", "Hanzi", ""]]}, {"id": "1906.07016", "submitter": "Ting Yao", "authors": "Zhaofan Qiu, Dong Li, Yehao Li, Qi Cai, Yingwei Pan, Ting Yao", "title": "Trimmed Action Recognition, Dense-Captioning Events in Videos, and\n  Spatio-temporal Action Localization with Focus on ActivityNet Challenge 2019", "comments": "arXiv admin note: substantial text overlap with arXiv:1807.00686,\n  arXiv:1710.08011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This notebook paper presents an overview and comparative analysis of our\nsystems designed for the following three tasks in ActivityNet Challenge 2019:\ntrimmed action recognition, dense-captioning events in videos, and\nspatio-temporal action localization.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 08:39:52 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Qiu", "Zhaofan", ""], ["Li", "Dong", ""], ["Li", "Yehao", ""], ["Cai", "Qi", ""], ["Pan", "Yingwei", ""], ["Yao", "Ting", ""]]}, {"id": "1906.07029", "submitter": "Jan Quenzel", "authors": "Radu Alexandru Rosu and Jan Quenzel and Sven Behnke", "title": "Semi-Supervised Semantic Mapping through Label Propagation with Semantic\n  Texture Meshes", "comments": "This is a pre-print of an article published in International Journal\n  of Computer Vision (IJCV, 2019)", "journal-ref": null, "doi": "10.1007/s11263-019-01187-z", "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene understanding is an important capability for robots acting in\nunstructured environments. While most SLAM approaches provide a geometrical\nrepresentation of the scene, a semantic map is necessary for more complex\ninteractions with the surroundings. Current methods treat the semantic map as\npart of the geometry which limits scalability and accuracy. We propose to\nrepresent the semantic map as a geometrical mesh and a semantic texture coupled\nat independent resolution. The key idea is that in many environments the\ngeometry can be greatly simplified without loosing fidelity, while semantic\ninformation can be stored at a higher resolution, independent of the mesh. We\nconstruct a mesh from depth sensors to represent the scene geometry and fuse\ninformation into the semantic texture from segmentations of individual RGB\nviews of the scene. Making the semantics persistent in a global mesh enables us\nto enforce temporal and spatial consistency of the individual view predictions.\nFor this, we propose an efficient method of establishing consensus between\nindividual segmentations by iteratively retraining semantic segmentation with\nthe information stored within the map and using the retrained segmentation to\nre-fuse the semantics. We demonstrate the accuracy and scalability of our\napproach by reconstructing semantic maps of scenes from NYUv2 and a scene\nspanning large buildings.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 13:36:21 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Rosu", "Radu Alexandru", ""], ["Quenzel", "Jan", ""], ["Behnke", "Sven", ""]]}, {"id": "1906.07052", "submitter": "Chen-Lin Zhang", "authors": "Chen-Lin Zhang, Xin-Xin Liu, Jianxin Wu", "title": "Towards Real-Time Action Recognition on Mobile Devices Using Deep Models", "comments": "work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition is a vital task in computer vision, and many methods are\ndeveloped to push it to the limit. However, current action recognition models\nhave huge computational costs, which cannot be deployed to real-world tasks on\nmobile devices. In this paper, we first illustrate the setting of real-time\naction recognition, which is different from current action recognition\ninference settings. Under the new inference setting, we investigate\nstate-of-the-art action recognition models on the Kinetics dataset empirically.\nOur results show that designing efficient real-time action recognition models\nis different from designing efficient ImageNet models, especially in weight\ninitialization. We show that pre-trained weights on ImageNet improve the\naccuracy under the real-time action recognition setting. Finally, we use the\nhand gesture recognition task as a case study to evaluate our compact real-time\naction recognition models in real-world applications on mobile phones. Results\nshow that our action recognition models, being 6x faster and with similar\naccuracy as state-of-the-art, can roughly meet the real-time requirements on\nmobile devices. To our best knowledge, this is the first paper that deploys\ncurrent deep learning action recognition models on mobile devices.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 14:27:24 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Zhang", "Chen-Lin", ""], ["Liu", "Xin-Xin", ""], ["Wu", "Jianxin", ""]]}, {"id": "1906.07078", "submitter": "Berk Dogan", "authors": "Berk Dogan, Shuhang Gu, Radu Timofte", "title": "Exemplar Guided Face Image Super-Resolution without Facial Landmarks", "comments": "Published in 2019 IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR) Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, due to the ubiquitous visual media there are vast amounts of\nalready available high-resolution (HR) face images. Therefore, for\nsuper-resolving a given very low-resolution (LR) face image of a person it is\nvery likely to find another HR face image of the same person which can be used\nto guide the process. In this paper, we propose a convolutional neural network\n(CNN)-based solution, namely GWAInet, which applies super-resolution (SR) by a\nfactor 8x on face images guided by another unconstrained HR face image of the\nsame person with possible differences in age, expression, pose or size. GWAInet\nis trained in an adversarial generative manner to produce the desired high\nquality perceptual image results. The utilization of the HR guiding image is\nrealized via the use of a warper subnetwork that aligns its contents to the\ninput image and the use of a feature fusion chain for the extracted features\nfrom the warped guiding image and the input image. In training, the identity\nloss further helps in preserving the identity related features by minimizing\nthe distance between the embedding vectors of SR and HR ground truth images.\nContrary to the current state-of-the-art in face super-resolution, our method\ndoes not require facial landmark points for its training, which helps its\nrobustness and allows it to produce fine details also for the surrounding face\nregion in a uniform manner. Our method GWAInet produces photo-realistic images\nin upscaling factor 8x and outperforms state-of-the-art in quantitative terms\nand perceptual quality.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 15:08:09 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Dogan", "Berk", ""], ["Gu", "Shuhang", ""], ["Timofte", "Radu", ""]]}, {"id": "1906.07079", "submitter": "Jong-Chyi Su", "authors": "Jong-Chyi Su, Subhransu Maji, Bharath Hariharan", "title": "Boosting Supervision with Self-Supervision for Few-shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique to improve the transferability of deep representations\nlearned on small labeled datasets by introducing self-supervised tasks as\nauxiliary loss functions. While recent approaches for self-supervised learning\nhave shown the benefits of training on large unlabeled datasets, we find\nimprovements in generalization even on small datasets and when combined with\nstrong supervision. Learning representations with self-supervised losses\nreduces the relative error rate of a state-of-the-art meta-learner by 5-25% on\nseveral few-shot learning benchmarks, as well as off-the-shelf deep networks on\nstandard classification tasks when training from scratch. We find the benefits\nof self-supervision increase with the difficulty of the task. Our approach\nutilizes the images within the dataset to construct self-supervised losses and\nhence is an effective way of learning transferable representations without\nrelying on any external training data.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 15:17:40 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Su", "Jong-Chyi", ""], ["Maji", "Subhransu", ""], ["Hariharan", "Bharath", ""]]}, {"id": "1906.07084", "submitter": "Qiang Huo", "authors": "Qiang Huo", "title": "Particle Swarm Optimization for Great Enhancement in Semi-Supervised\n  Retinal Vessel Segmentation with Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinal vessel segmentation based on deep learning requires a lot of manual\nlabeled data. That is time-consuming, laborious and professional. What is\nworse, the acquisition of abundant fundus images is difficult. These problems\nare more serious due to the presence of abnormalities, varying size and shape\nof the vessels, non-uniform illumination and anatomical changes. In this paper,\nwe propose a data-efficient semi-supervised learning framework, which\neffectively combines the existing deep learning network with GAN and\nself-training ideas. In view of the difficulty of tuning hyper-parameters of\nsemi-supervised learning, we propose a method for hyper-parameters selection\nbased on particle swarm optimization algorithm. To the best of our knowledge,\nthis work is the first demonstration that combines intelligent optimization\nwith semi-supervised learning for achieving the best performance. Under the\ncollaboration of adversarial learning, self-training and PSO to select optimal\nhyper-parameters, we obtain the performance of retinal vessel segmentation\napproximate to or even better than representative supervised learning using\nonly one tenth of the labeled data from DRIVE.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 15:28:29 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 14:38:01 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Huo", "Qiang", ""]]}, {"id": "1906.07133", "submitter": "Quan Kong", "authors": "Quan Kong, Bin Tong, Martin Klinkigt, Yuki Watanabe, Naoto Akira,\n  Tomokazu Murakami", "title": "Active Generative Adversarial Network for Image Classification", "comments": "AAAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Sufficient supervised information is crucial for any machine learning models\nto boost performance. However, labeling data is expensive and sometimes\ndifficult to obtain. Active learning is an approach to acquire annotations for\ndata from a human oracle by selecting informative samples with a high\nprobability to enhance performance. In recent emerging studies, a generative\nadversarial network (GAN) has been integrated with active learning to generate\ngood candidates to be presented to the oracle. In this paper, we propose a\nnovel model that is able to obtain labels for data in a cheaper manner without\nthe need to query an oracle. In the model, a novel reward for each sample is\ndevised to measure the degree of uncertainty, which is obtained from a\nclassifier trained with existing labeled data. This reward is used to guide a\nconditional GAN to generate informative samples with a higher probability for a\ncertain label. With extensive evaluations, we have confirmed the effectiveness\nof the model, showing that the generated samples are capable of improving the\nclassification performance in popular image classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 17:11:07 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Kong", "Quan", ""], ["Tong", "Bin", ""], ["Klinkigt", "Martin", ""], ["Watanabe", "Yuki", ""], ["Akira", "Naoto", ""], ["Murakami", "Tomokazu", ""]]}, {"id": "1906.07138", "submitter": "Favyen Bastani", "authors": "Favyen Bastani, Songtao He, Sofiane Abbar, Mohammad Alizadeh, Hari\n  Balakrishnan, Sanjay Chawla, Sam Madden", "title": "Machine-Assisted Map Editing", "comments": null, "journal-ref": "Proceedings of the 26th ACM SIGSPATIAL International Conference on\n  Advances in Geographic Information Systems, pg 23-32, 2018", "doi": "10.1145/3274895.3274927", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping road networks today is labor-intensive. As a result, road maps have\npoor coverage outside urban centers in many countries. Systems to automatically\ninfer road network graphs from aerial imagery and GPS trajectories have been\nproposed to improve coverage of road maps. However, because of high error\nrates, these systems have not been adopted by mapping communities. We propose\nmachine-assisted map editing, where automatic map inference is integrated into\nexisting, human-centric map editing workflows. To realize this, we build\nMachine-Assisted iD (MAiD), where we extend the web-based OpenStreetMap editor,\niD, with machine-assistance functionality. We complement MAiD with a novel\napproach for inferring road topology from aerial imagery that combines the\nspeed of prior segmentation approaches with the accuracy of prior iterative\ngraph construction methods. We design MAiD to tackle the addition of major,\narterial roads in regions where existing maps have poor coverage, and the\nincremental improvement of coverage in regions where major roads are already\nmapped. We conduct two user studies and find that, when participants are given\na fixed time to map roads, they are able to add as much as 3.5x more roads with\nMAiD.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 17:21:10 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Bastani", "Favyen", ""], ["He", "Songtao", ""], ["Abbar", "Sofiane", ""], ["Alizadeh", "Mohammad", ""], ["Balakrishnan", "Hari", ""], ["Chawla", "Sanjay", ""], ["Madden", "Sam", ""]]}, {"id": "1906.07155", "submitter": "Kai Chen", "authors": "Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao\n  Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng,\n  Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu,\n  Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, Dahua\n  Lin", "title": "MMDetection: Open MMLab Detection Toolbox and Benchmark", "comments": "Technical report of MMDetection. 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MMDetection, an object detection toolbox that contains a rich set\nof object detection and instance segmentation methods as well as related\ncomponents and modules. The toolbox started from a codebase of MMDet team who\nwon the detection track of COCO Challenge 2018. It gradually evolves into a\nunified platform that covers many popular detection methods and contemporary\nmodules. It not only includes training and inference codes, but also provides\nweights for more than 200 network models. We believe this toolbox is by far the\nmost complete detection toolbox. In this paper, we introduce the various\nfeatures of this toolbox. In addition, we also conduct a benchmarking study on\ndifferent methods, components, and their hyper-parameters. We wish that the\ntoolbox and benchmark could serve the growing research community by providing a\nflexible toolkit to reimplement existing methods and develop their own new\ndetectors. Code and models are available at\nhttps://github.com/open-mmlab/mmdetection. The project is under active\ndevelopment and we will keep this document updated.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 17:58:12 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Chen", "Kai", ""], ["Wang", "Jiaqi", ""], ["Pang", "Jiangmiao", ""], ["Cao", "Yuhang", ""], ["Xiong", "Yu", ""], ["Li", "Xiaoxiao", ""], ["Sun", "Shuyang", ""], ["Feng", "Wansen", ""], ["Liu", "Ziwei", ""], ["Xu", "Jiarui", ""], ["Zhang", "Zheng", ""], ["Cheng", "Dazhi", ""], ["Zhu", "Chenchen", ""], ["Cheng", "Tianheng", ""], ["Zhao", "Qijie", ""], ["Li", "Buyu", ""], ["Lu", "Xin", ""], ["Zhu", "Rui", ""], ["Wu", "Yue", ""], ["Dai", "Jifeng", ""], ["Wang", "Jingdong", ""], ["Shi", "Jianping", ""], ["Ouyang", "Wanli", ""], ["Loy", "Chen Change", ""], ["Lin", "Dahua", ""]]}, {"id": "1906.07160", "submitter": "Malav Bateriwala", "authors": "Malav Bateriwala and Pierrick Bourgeat", "title": "Enforcing temporal consistency in Deep Learning segmentation of brain MR\n  images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longitudinal analysis has great potential to reveal developmental\ntrajectories and monitor disease progression in medical imaging. This process\nrelies on consistent and robust joint 4D segmentation. Traditional techniques\nare dependent on the similarity of images over time and the use of\nsubject-specific priors to reduce random variation and improve the robustness\nand sensitivity of the overall longitudinal analysis. This is however slow and\ncomputationally intensive as subject-specific templates need to be rebuilt\nevery time. The focus of this work to accelerate this analysis with the use of\ndeep learning. The proposed approach is based on deep CNNs and incorporates\nsemantic segmentation and provides a longitudinal relationship for the same\nsubject. The proposed approach is based on deep CNNs and incorporates semantic\nsegmentation and provides a longitudinal relationship for the same subject. The\nstate of art using 3D patches as inputs to modified Unet provides results\naround ${0.91 \\pm 0.5}$ Dice and using multi-view atlas in CNNs provide around\nthe same results. In this work, different models are explored, each offers\nbetter accuracy and fast results while increasing the segmentation quality.\nThese methods are evaluated on 135 scans from the EADC-ADNI Harmonized\nHippocampus Protocol. Proposed CNN based segmentation approaches demonstrate\nhow 2D segmentation using prior slices can provide similar results to 3D\nsegmentation while maintaining good continuity in the 3D dimension and improved\nspeed. Just using 2D modified sagittal slices provide us a better Dice and\nlongitudinal analysis for a given subject. For the ADNI dataset, using the\nsimple UNet CNN technique gives us ${0.84 \\pm 0.5}$ and while using modified\nCNN techniques on the same input yields ${0.89 \\pm 0.5}$. Rate of atrophy and\nRMS error are calculated for several test cases using various methods and\nanalyzed.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 08:33:23 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Bateriwala", "Malav", ""], ["Bourgeat", "Pierrick", ""]]}, {"id": "1906.07165", "submitter": "Henri Rebecq", "authors": "Henri Rebecq, Ren\\'e Ranftl, Vladlen Koltun, Davide Scaramuzza", "title": "High Speed and High Dynamic Range Video with an Event Camera", "comments": "arXiv admin note: substantial text overlap with arXiv:1904.08298", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are novel sensors that report brightness changes in the form of\na stream of asynchronous \"events\" instead of intensity frames. They offer\nsignificant advantages with respect to conventional cameras: high temporal\nresolution, high dynamic range, and no motion blur. While the stream of events\nencodes in principle the complete visual signal, the reconstruction of an\nintensity image from a stream of events is an ill-posed problem in practice.\nExisting reconstruction approaches are based on hand-crafted priors and strong\nassumptions about the imaging process as well as the statistics of natural\nimages. In this work we propose to learn to reconstruct intensity images from\nevent streams directly from data instead of relying on any hand-crafted priors.\nWe propose a novel recurrent network to reconstruct videos from a stream of\nevents, and train it on a large amount of simulated event data. During training\nwe propose to use a perceptual loss to encourage reconstructions to follow\nnatural image statistics. We further extend our approach to synthesize color\nimages from color event streams. Our network surpasses state-of-the-art\nreconstruction methods by a large margin in terms of image quality (> 20%),\nwhile comfortably running in real-time. We show that the network is able to\nsynthesize high framerate videos (> 5,000 frames per second) of high-speed\nphenomena (e.g. a bullet hitting an object) and is able to provide high dynamic\nrange reconstructions in challenging lighting conditions. We also demonstrate\nthe effectiveness of our reconstructions as an intermediate representation for\nevent data. We show that off-the-shelf computer vision algorithms can be\napplied to our reconstructions for tasks such as object classification and\nvisual-inertial odometry and that this strategy consistently outperforms\nalgorithms that were specifically designed for event data.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 04:56:14 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Rebecq", "Henri", ""], ["Ranftl", "Ren\u00e9", ""], ["Koltun", "Vladlen", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "1906.07172", "submitter": "Erkao Bao", "authors": "Erkao Bao and Linqi Song", "title": "Equivariant neural networks and equivarification", "comments": "More explanations added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a process to modify a neural network to an equivariant one, which\nwe call equivarification. As an illustration, we build an equivariant neural\nnetwork for image classification by equivarifying a convolutional neural\nnetwork.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 23:26:03 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 03:40:06 GMT"}, {"version": "v3", "created": "Sun, 15 Sep 2019 06:51:29 GMT"}, {"version": "v4", "created": "Sun, 22 Mar 2020 04:28:55 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Bao", "Erkao", ""], ["Song", "Linqi", ""]]}, {"id": "1906.07176", "submitter": "Xu Liu", "authors": "Xu Liu, Licheng Jiao, Dan Zhang, Fang Liu", "title": "PolSAR Image Classification based on Polarimetric Scattering Coding and\n  Sparse Support Matrix Machine", "comments": "arXiv admin note: text overlap with arXiv:1807.02975", "journal-ref": "IGARSS2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  POLSAR image has an advantage over optical image because it can be acquired\nindependently of cloud cover and solar illumination. PolSAR image\nclassification is a hot and valuable topic for the interpretation of POLSAR\nimage. In this paper, a novel POLSAR image classification method is proposed\nbased on polarimetric scattering coding and sparse support matrix machine.\nFirst, we transform the original POLSAR data to get a real value matrix by the\npolarimetric scattering coding, which is called polarimetric scattering matrix\nand is a sparse matrix. Second, the sparse support matrix machine is used to\nclassify the sparse polarimetric scattering matrix and get the classification\nmap. The combination of these two steps takes full account of the\ncharacteristics of POLSAR. The experimental results show that the proposed\nmethod can get better results and is an effective classification method.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 03:17:13 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Liu", "Xu", ""], ["Jiao", "Licheng", ""], ["Zhang", "Dan", ""], ["Liu", "Fang", ""]]}, {"id": "1906.07207", "submitter": "Qiaoyun Wu", "authors": "Qiaoyun Wu, Dinesh Manocha, Jun Wang and Kai Xu", "title": "NeoNav: Improving the Generalization of Visual Navigation via Generating\n  Next Expected Observations", "comments": "added experiments, accepted by AAAI, Corresponding author: Kai Xu\n  (kevin.kai.xu@gmail.com)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We propose improving the cross-target and cross-scene generalization of\nvisual navigation through learning an agent that is guided by conceiving the\nnext observations it expects to see. This is achieved by learning a variational\nBayesian model, called NeoNav, which generates the next expected observations\n(NEO) conditioned on the current observations of the agent and the target view.\nOur generative model is learned through optimizing a variational objective\nencompassing two key designs. First, the latent distribution is conditioned on\ncurrent observations and the target view, leading to a model-based,\ntarget-driven navigation. Second, the latent space is modeled with a Mixture of\nGaussians conditioned on the current observation and the next best action. Our\nuse of mixture-of-posteriors prior effectively alleviates the issue of\nover-regularized latent space, thus significantly boosting the model\ngeneralization for new targets and in novel scenes. Moreover, the NEO\ngeneration models the forward dynamics of agent-environment interaction, which\nimproves the quality of approximate inference and hence benefits data\nefficiency. We have conducted extensive evaluations on both real-world and\nsynthetic benchmarks, and show that our model consistently outperforms the\nstate-of-the-art models in terms of success rate, data efficiency, and\ngeneralization.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 18:14:03 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 15:23:16 GMT"}, {"version": "v3", "created": "Fri, 22 Nov 2019 19:34:58 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Wu", "Qiaoyun", ""], ["Manocha", "Dinesh", ""], ["Wang", "Jun", ""], ["Xu", "Kai", ""]]}, {"id": "1906.07214", "submitter": "Sai Vineeth Kalluru Srinivas", "authors": "Sai Vineeth Kalluru Srinivas, Harideep Nair, Vinay Vidyasagar", "title": "Hardware Aware Neural Network Architectures using FbNet", "comments": "8 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We implement a differentiable Neural Architecture Search (NAS) method\ninspired by FBNet for discovering neural networks that are heavily optimized\nfor a particular target device. The FBNet NAS method discovers a neural network\nfrom a given search space by optimizing over a loss function which accounts for\naccuracy and target device latency. We extend this loss function by adding an\nenergy term. This will potentially enhance the ``hardware awareness\" and help\nus find a neural network architecture that is optimal in terms of accuracy,\nlatency and energy consumption, given a target device (Raspberry Pi in our\ncase). We name our trained child architecture obtained at the end of search\nprocess as Hardware Aware Neural Network Architecture (HANNA). We prove the\nefficacy of our approach by benchmarking HANNA against two other\nstate-of-the-art neural networks designed for mobile/embedded applications,\nnamely MobileNetv2 and CondenseNet for CIFAR-10 dataset. Our results show that\nHANNA provides a speedup of about 2.5x and 1.7x, and reduces energy consumption\nby 3.8x and 2x compared to MobileNetv2 and CondenseNet respectively. HANNA is\nable to provide such significant speedup and energy efficiency benefits over\nthe state-of-the-art baselines at the cost of a tolerable 4-5% drop in\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 18:34:01 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Srinivas", "Sai Vineeth Kalluru", ""], ["Nair", "Harideep", ""], ["Vidyasagar", "Vinay", ""]]}, {"id": "1906.07247", "submitter": "Ashwin Geet D'Sa", "authors": "Ashwin Geet D'Sa and B. G. Prasad", "title": "An IoT Based Framework For Activity Recognition Using Deep Learning\n  Technique", "comments": "Intended to submit to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activity recognition is the ability to identify and recognize the action or\ngoals of the agent. The agent can be any object or entity that performs action\nthat has end goals. The agents can be a single agent performing the action or\ngroup of agents performing the actions or having some interaction. Human\nactivity recognition has gained popularity due to its demands in many practical\napplications such as entertainment, healthcare, simulations and surveillance\nsystems. Vision based activity recognition is gaining advantage as it does not\nrequire any human intervention or physical contact with humans. Moreover, there\nare set of cameras that are networked with the intention to track and recognize\nthe activities of the agent. Traditional applications that were required to\ntrack or recognize human activities made use of wearable devices. However, such\napplications require physical contact of the person. To overcome such\nchallenges, vision based activity recognition system can be used, which uses a\ncamera to record the video and a processor that performs the task of\nrecognition. The work is implemented in two stages. In the first stage, an\napproach for the Implementation of Activity recognition is proposed using\nbackground subtraction of images, followed by 3D- Convolutional Neural\nNetworks. The impact of using Background subtraction prior to 3D-Convolutional\nNeural Networks has been reported. In the second stage, the work is further\nextended and implemented on Raspberry Pi, that can be used to record a stream\nof video, followed by recognizing the activity that was involved in the video.\nThus, a proof-of-concept for activity recognition using small, IoT based\ndevice, is provided, which can enhance the system and extend its applications\nin various forms like, increase in portability, networking, and other\ncapabilities of the device.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 20:09:41 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["D'Sa", "Ashwin Geet", ""], ["Prasad", "B. G.", ""]]}, {"id": "1906.07251", "submitter": "Wei Sun", "authors": "Wei Sun, Jawadul H. Bappy, Shanglin Yang, Yi Xu, Tianfu Wu, Hui Zhou", "title": "Pose Guided Fashion Image Synthesis Using Deep Generative Model", "comments": "KDD 2019 Workshop AI for Fashion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating a photorealistic image with intended human pose is a promising yet\nchallenging research topic for many applications such as smart photo editing,\nmovie making, virtual try-on, and fashion display. In this paper, we present a\nnovel deep generative model to transfer an image of a person from a given pose\nto a new pose while keeping fashion item consistent. In order to formulate the\nframework, we employ one generator and two discriminators for image synthesis.\nThe generator includes an image encoder, a pose encoder and a decoder. The two\nencoders provide good representation of visual and geometrical context which\nwill be utilized by the decoder in order to generate a photorealistic image.\nUnlike existing pose-guided image generation models, we exploit two\ndiscriminators to guide the synthesis process where one discriminator\ndifferentiates between generated image and real images (training samples), and\nanother discriminator verifies the consistency of appearance between a target\npose and a generated image. We perform end-to-end training of the network to\nlearn the parameters through back-propagation given ground-truth images. The\nproposed generative model is capable of synthesizing a photorealistic image of\na person given a target pose. We have demonstrated our results by conducting\nrigorous experiments on two data sets, both quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 20:26:59 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 18:37:42 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Sun", "Wei", ""], ["Bappy", "Jawadul H.", ""], ["Yang", "Shanglin", ""], ["Xu", "Yi", ""], ["Wu", "Tianfu", ""], ["Zhou", "Hui", ""]]}, {"id": "1906.07258", "submitter": "Mahdi Maktab Dar Oghaz", "authors": "Mahdi Maktabdar Oghaz, Anish R Khadka, Vasileios Argyriou, Paolo\n  Remagnino", "title": "Content-aware Density Map for Crowd Counting and Density Estimation", "comments": null, "journal-ref": "32nd International Conference on Computer Animation and Social\n  Agents (CASA 2019)", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise knowledge about the size of a crowd, its density and flow can provide\nvaluable information for safety and security applications, event planning,\narchitectural design and to analyze consumer behavior. Creating a powerful\nmachine learning model, to employ for such applications requires a large and\nhighly accurate and reliable dataset. Unfortunately the existing crowd counting\nand density estimation benchmark datasets are not only limited in terms of\ntheir size, but also lack annotation, in general too time consuming to\nimplement. This paper attempts to address this very issue through a content\naware technique, uses combinations of Chan-Vese segmentation algorithm,\ntwo-dimensional Gaussian filter and brute-force nearest neighbor search. The\nresults shows that by simply replacing the commonly used density map generators\nwith the proposed method, higher level of accuracy can be achieved using the\nexisting state of the art models.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 20:45:29 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Oghaz", "Mahdi Maktabdar", ""], ["Khadka", "Anish R", ""], ["Argyriou", "Vasileios", ""], ["Remagnino", "Paolo", ""]]}, {"id": "1906.07273", "submitter": "Kedan Li", "authors": "Kedan Li, Chen Liu and David Forsyth", "title": "Coherent and Controllable Outfit Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  When thinking about dressing oneself, people often have a theme in mind\nwhether they're going to a tropical getaway or wish to appear attractive at a\ncocktail party. A useful outfit generation system should come up with clothing\nitems that are compatible while matching a theme specified by the user.\nExisting methods use item-wise compatibility between products but lack an\neffective way to enforce a global constraint (e.g., style, occasion).\n  We introduce a method that generates outfits whose items match a theme\ndescribed by a text query. Our method uses text and image embeddings to\nrepresent fashion items. We learn a multimodal embedding where the image\nrepresentation for an item is close to its text representation, and use this\nembedding to measure item-query coherence. We then use a discriminator to\ncompute compatibility between fashion items. This strategy yields a\ncompatibility prediction method that meets or exceeds the state of the art.\n  Our method combines item-item compatibility and item-query coherence to\nconstruct an outfit whose items are (a) close to the query and (b) compatible\nwith one another. Quantitative evaluation shows that the items in our outfits\nare tightly clustered compared to standard outfits. Furthermore, outfits\nproduced by similar queries are close to one another, and outfits produced by\nvery different queries are far apart. Qualitative evaluation shows that our\nmethod responds well to queries. A user study suggests that people understand\nthe match between the queries and the outfits produced by our method.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 21:12:36 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 21:26:04 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Li", "Kedan", ""], ["Liu", "Chen", ""], ["Forsyth", "David", ""]]}, {"id": "1906.07282", "submitter": "Alex Lu", "authors": "Alex X. Lu, Amy X. Lu, Wiebke Schormann, Marzyeh Ghassemi, David W.\n  Andrews, Alan M. Moses", "title": "The Cells Out of Sample (COOS) dataset and benchmarks for measuring\n  out-of-sample generalization of image classifiers", "comments": null, "journal-ref": "In Advances in Neural Information Processing Systems 32, pages\n  1852-1860. NeurIPS 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding if classifiers generalize to out-of-sample datasets is a\ncentral problem in machine learning. Microscopy images provide a standardized\nway to measure the generalization capacity of image classifiers, as we can\nimage the same classes of objects under increasingly divergent, but controlled\nfactors of variation. We created a public dataset of 132,209 images of mouse\ncells, COOS-7 (Cells Out Of Sample 7-Class). COOS-7 provides a classification\nsetting where four test datasets have increasing degrees of covariate shift:\nsome images are random subsets of the training data, while others are from\nexperiments reproduced months later and imaged by different instruments. We\nbenchmarked a range of classification models using different representations,\nincluding transferred neural network features, end-to-end classification with a\nsupervised deep CNN, and features from a self-supervised CNN. While most\nclassifiers perform well on test datasets similar to the training dataset, all\nclassifiers failed to generalize their performance to datasets with greater\ncovariate shifts. These baselines highlight the challenges of covariate shifts\nin image data, and establish metrics for improving the generalization capacity\nof image classifiers.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 21:35:00 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 15:05:54 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2020 15:46:56 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Lu", "Alex X.", ""], ["Lu", "Amy X.", ""], ["Schormann", "Wiebke", ""], ["Ghassemi", "Marzyeh", ""], ["Andrews", "David W.", ""], ["Moses", "Alan M.", ""]]}, {"id": "1906.07295", "submitter": "Andriy Myronenko", "authors": "Andriy Myronenko, Dong Yang, Varun Buch, Daguang Xu, Alvin Ihsani,\n  Sean Doyle, Mark Michalski, Neil Tenenholtz, Holger Roth", "title": "4D CNN for semantic segmentation of cardiac volumetric sequences", "comments": "MICCAI, STACOM, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a 4D convolutional neural network (CNN) for the segmentation of\nretrospective ECG-gated cardiac CT, a series of single-channel volumetric data\nover time. While only a small subset of volumes in the temporal sequence is\nannotated, we define a sparse loss function on available labels to allow the\nnetwork to leverage unlabeled images during training and generate a fully\nsegmented sequence. We investigate the accuracy of the proposed 4D network to\npredict temporally consistent segmentations and compare with traditional 3D\nsegmentation approaches. We demonstrate the feasibility of the 4D CNN and\nestablish its performance on cardiac 4D CCTA.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 22:43:06 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 00:32:59 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Myronenko", "Andriy", ""], ["Yang", "Dong", ""], ["Buch", "Varun", ""], ["Xu", "Daguang", ""], ["Ihsani", "Alvin", ""], ["Doyle", "Sean", ""], ["Michalski", "Mark", ""], ["Tenenholtz", "Neil", ""], ["Roth", "Holger", ""]]}, {"id": "1906.07316", "submitter": "John Flynn", "authors": "John Flynn, Michael Broxton, Paul Debevec, Matthew DuVall, Graham\n  Fyffe, Ryan Overbeck, Noah Snavely, Richard Tucker", "title": "DeepView: View Synthesis with Learned Gradient Descent", "comments": "See https://augmentedperception.github.io/deepview/ for more results,\n  video and an interactive viewer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to view synthesis using multiplane images (MPIs).\nBuilding on recent advances in learned gradient descent, our algorithm\ngenerates an MPI from a set of sparse camera viewpoints. The resulting method\nincorporates occlusion reasoning, improving performance on challenging scene\nfeatures such as object boundaries, lighting reflections, thin structures, and\nscenes with high depth complexity. We show that our method achieves\nhigh-quality, state-of-the-art results on two datasets: the Kalantari light\nfield dataset, and a new camera array dataset, Spaces, which we make publicly\navailable.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 00:29:27 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Flynn", "John", ""], ["Broxton", "Michael", ""], ["Debevec", "Paul", ""], ["DuVall", "Matthew", ""], ["Fyffe", "Graham", ""], ["Overbeck", "Ryan", ""], ["Snavely", "Noah", ""], ["Tucker", "Richard", ""]]}, {"id": "1906.07330", "submitter": "Jong Chul Ye", "authors": "Eunju Cha, Jaeduck Jang, Junho Lee, Eunha Lee and Jong Chul Ye", "title": "Boosting CNN beyond Label in Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) have been extensively used for inverse\nproblems. However, their prediction error for unseen test data is difficult to\nestimate a priori since the neural networks are trained using only selected\ndata and their architecture are largely considered a blackbox. This poses a\nfundamental challenge to neural networks for unsupervised learning or\nimprovement beyond the label. In this paper, we show that the recent\nunsupervised learning methods such as Noise2Noise, Stein's unbiased risk\nestimator (SURE)-based denoiser, and Noise2Void are closely related to each\nother in their formulation of an unbiased estimator of the prediction error,\nbut each of them are associated with its own limitations. Based on these\nobservations, we provide a novel boosting estimator for the prediction error.\nIn particular, by employing combinatorial convolutional frame representation of\nencoder-decoder CNN and synergistically combining it with the batch\nnormalization, we provide a close form formulation for the unbiased estimator\nof the prediction error that can be minimized for neural network training\nbeyond the label. Experimental results show that the resulting algorithm, what\nwe call Noise2Boosting, provides consistent improvement in various inverse\nproblems under both supervised and unsupervised learning setting.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 01:21:40 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Cha", "Eunju", ""], ["Jang", "Jaeduck", ""], ["Lee", "Junho", ""], ["Lee", "Eunha", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1906.07341", "submitter": "Zhiyong Yang", "authors": "Zhiyong Yang, Qianqian Xu, Xiaochun Cao, Qingming Huang", "title": "Learning Personalized Attribute Preference via Multi-task AUC\n  Optimization", "comments": "AAAI2019 oral", "journal-ref": "AAAI2019 oral", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, most of the existing attribute learning methods are trained\nbased on the consensus of annotations aggregated from a limited number of\nannotators. However, the consensus might fail in settings, especially when a\nwide spectrum of annotators with different interests and comprehension about\nthe attribute words are involved. In this paper, we develop a novel multi-task\nmethod to understand and predict personalized attribute annotations. Regarding\nthe attribute preference learning for each annotator as a specific task, we\nfirst propose a multi-level task parameter decomposition to capture the\nevolution from a highly popular opinion of the mass to highly personalized\nchoices that are special for each person. Meanwhile, for personalized learning\nmethods, ranking prediction is much more important than accurate\nclassification. This motivates us to employ an Area Under ROC Curve (AUC) based\nloss function to improve our model. On top of the AUC-based loss, we propose an\nefficient method to evaluate the loss and gradients. Theoretically, we propose\na novel closed-form solution for one of our non-convex subproblem, which leads\nto provable convergence behaviors. Furthermore, we also provide a\ngeneralization bound to guarantee a reasonable performance. Finally, empirical\nanalysis consistently speaks to the efficacy of our proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 02:14:36 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Yang", "Zhiyong", ""], ["Xu", "Qianqian", ""], ["Cao", "Xiaochun", ""], ["Huang", "Qingming", ""]]}, {"id": "1906.07347", "submitter": "Xinzhe Luo", "authors": "Qian Yue, Xinzhe Luo, Qing Ye, Lingchao Xu, Xiahai Zhuang", "title": "Cardiac Segmentation from LGE MRI Using Deep Neural Network\n  Incorporating Shape and Spatial Priors", "comments": "Accepted for publication in MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiac segmentation from late gadolinium enhancement MRI is an important\ntask in clinics to identify and evaluate the infarction of myocardium. The\nautomatic segmentation is however still challenging, due to the heterogeneous\nintensity distributions and indistinct boundaries in the images. In this paper,\nwe propose a new method, based on deep neural networks (DNN), for fully\nautomatic segmentation. The proposed network, referred to as SRSCN, comprises a\nshape reconstruction neural network (SRNN) and a spatial constraint network\n(SCN). SRNN aims to maintain a realistic shape of the resulting segmentation.\nIt can be pre-trained by a set of label images, and then be embedded into a\nunified loss function as a regularization term. Hence, no manually designed\nfeature is needed. Furthermore, SCN incorporates the spatial information of the\n2D slices. It is formulated and trained with the segmentation network via the\nmulti-task learning strategy. We evaluated the proposed method using 45\npatients and compared with two state-of-the-art regularization schemes, i.e.,\nthe anatomically constraint neural network and the adversarial neural network.\nThe results show that the proposed SRSCN outperformed the conventional schemes,\nand obtained a Dice score of 0.758(std=0.227) for myocardial segmentation,\nwhich compares with 0.757(std=0.083) from the inter-observer variations.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 02:36:34 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 08:25:38 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Yue", "Qian", ""], ["Luo", "Xinzhe", ""], ["Ye", "Qing", ""], ["Xu", "Lingchao", ""], ["Zhuang", "Xiahai", ""]]}, {"id": "1906.07357", "submitter": "Wentao Zhu", "authors": "Wentao Zhu, Yufang Huang, Mani A Vannan, Shizhen Liu, Daguang Xu, Wei\n  Fan, Zhen Qian, Xiaohui Xie", "title": "Neural Multi-Scale Self-Supervised Registration for Echocardiogram Dense\n  Tracking", "comments": "Blood tracking video: https://youtu.be/pEA6ZmtTNuQ Muscle tracking\n  video: https://youtu.be/NvLrCaqCiAE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Echocardiography has become routinely used in the diagnosis of cardiomyopathy\nand abnormal cardiac blood flow. However, manually measuring myocardial motion\nand cardiac blood flow from echocardiogram is time-consuming and error-prone.\nComputer algorithms that can automatically track and quantify myocardial motion\nand cardiac blood flow are highly sought after, but have not been very\nsuccessful due to noise and high variability of echocardiography. In this work,\nwe propose a neural multi-scale self-supervised registration (NMSR) method for\nautomated myocardial and cardiac blood flow dense tracking. NMSR incorporates\ntwo novel components: 1) utilizing a deep neural net to parameterize the\nvelocity field between two image frames, and 2) optimizing the parameters of\nthe neural net in a sequential multi-scale fashion to account for large\nvariations within the velocity field. Experiments demonstrate that NMSR yields\nsignificantly better registration accuracy than state-of-the-art methods, such\nas advanced normalization tools (ANTs) and VoxelMorph, for both myocardial and\ncardiac blood flow dense tracking. Our approach promises to provide a fully\nautomated method for fast and accurate analyses of echocardiograms.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 03:05:47 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Zhu", "Wentao", ""], ["Huang", "Yufang", ""], ["Vannan", "Mani A", ""], ["Liu", "Shizhen", ""], ["Xu", "Daguang", ""], ["Fan", "Wei", ""], ["Qian", "Zhen", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1906.07367", "submitter": "Zhenxi Zhang", "authors": "Zhenxi Zhang, Jie Li, Zhusi Zhong, Zhicheng Jiao and Xinbo Gao", "title": "A sparse annotation strategy based on attention-guided active learning\n  for 3D medical image segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D image segmentation is one of the most important and ubiquitous problems in\nmedical image processing. It provides detailed quantitative analysis for\naccurate disease diagnosis, abnormal detection, and classification. Currently\ndeep learning algorithms are widely used in medical image segmentation, most\nalgorithms trained models with full annotated datasets. However, obtaining\nmedical image datasets is very difficult and expensive, and full annotation of\n3D medical image is a monotonous and time-consuming work. Partially labelling\ninformative slices in 3D images will be a great relief of manual annotation.\nSample selection strategies based on active learning have been proposed in the\nfield of 2D image, but few strategies focus on 3D images. In this paper, we\npropose a sparse annotation strategy based on attention-guided active learning\nfor 3D medical image segmentation. Attention mechanism is used to improve\nsegmentation accuracy and estimate the segmentation accuracy of each slice. The\ncomparative experiments with three different strategies using datasets from the\ndeveloping human connectome project (dHCP) show that, our strategy only needs\n15% to 20% annotated slices in brain extraction task and 30% to 35% annotated\nslices in tissue segmentation task to achieve comparative results as full\nannotation.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 04:09:46 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Zhang", "Zhenxi", ""], ["Li", "Jie", ""], ["Zhong", "Zhusi", ""], ["Jiao", "Zhicheng", ""], ["Gao", "Xinbo", ""]]}, {"id": "1906.07370", "submitter": "Shuran Song", "authors": "Shuran Song, Thomas Funkhouser", "title": "Neural Illumination: Lighting Prediction for Indoor Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the task of estimating the light arriving from all\ndirections to a 3D point observed at a selected pixel in an RGB image. This\ntask is challenging because it requires predicting a mapping from a partial\nscene observation by a camera to a complete illumination map for a selected\nposition, which depends on the 3D location of the selection, the distribution\nof unobserved light sources, the occlusions caused by scene geometry, etc.\nPrevious methods attempt to learn this complex mapping directly using a single\nblack-box neural network, which often fails to estimate high-frequency lighting\ndetails for scenes with complicated 3D geometry. Instead, we propose \"Neural\nIllumination\" a new approach that decomposes illumination prediction into\nseveral simpler differentiable sub-tasks: 1) geometry estimation, 2) scene\ncompletion, and 3) LDR-to-HDR estimation. The advantage of this approach is\nthat the sub-tasks are relatively easy to learn and can be trained with direct\nsupervision, while the whole pipeline is fully differentiable and can be\nfine-tuned with end-to-end supervision. Experiments show that our approach\nperforms significantly better quantitatively and qualitatively than prior work.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 04:17:16 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Song", "Shuran", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "1906.07383", "submitter": "Vijai Thottathil Jayadevan", "authors": "Vijai T. Jayadevan, Jeffrey J. Rodriguez, Alexander D. Cronin", "title": "A Conditional Random Field Model for Context Aware Cloud Detection in\n  Sky Images", "comments": "This was part of Vijai Jayadevan's MS thesis which was completed in\n  Dec 2013. This particular chapter remained unpublished till now", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A conditional random field (CRF) model for cloud detection in ground based\nsky images is presented. We show that very high cloud detection accuracy can be\nachieved by combining a discriminative classifier and a higher order clique\npotential in a CRF framework. The image is first divided into homogeneous\nregions using a mean shift clustering algorithm and then a CRF model is defined\nover these regions. The various parameters involved are estimated using\ntraining data and the inference is performed using Iterated Conditional Modes\n(ICM) algorithm. We demonstrate how taking spatial context into account can\nboost the accuracy. We present qualitative and quantitative results to prove\nthe superior performance of this framework in comparison with other state of\nthe art methods applied for cloud detection.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 05:26:06 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Jayadevan", "Vijai T.", ""], ["Rodriguez", "Jeffrey J.", ""], ["Cronin", "Alexander D.", ""]]}, {"id": "1906.07409", "submitter": "Chenyang Zhu", "authors": "Lintao Zheng, Chenyang Zhu, Jiazhao Zhang, Hang Zhao, Hui Huang,\n  Matthias Niessner and Kai Xu", "title": "Active Scene Understanding via Online Semantic Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to robot-operated active understanding of unknown\nindoor scenes, based on online RGBD reconstruction with semantic segmentation.\nIn our method, the exploratory robot scanning is both driven by and targeting\nat the recognition and segmentation of semantic objects from the scene. Our\nalgorithm is built on top of the volumetric depth fusion framework (e.g.,\nKinectFusion) and performs real-time voxel-based semantic labeling over the\nonline reconstructed volume. The robot is guided by an online estimated\ndiscrete viewing score field (VSF) parameterized over the 3D space of 2D\nlocation and azimuth rotation. VSF stores for each grid the score of the\ncorresponding view, which measures how much it reduces the uncertainty\n(entropy) of both geometric reconstruction and semantic labeling. Based on VSF,\nwe select the next best views (NBV) as the target for each time step. We then\njointly optimize the traverse path and camera trajectory between two adjacent\nNBVs, through maximizing the integral viewing score (information gain) along\npath and trajectory. Through extensive evaluation, we show that our method\nachieves efficient and accurate online scene parsing during exploratory\nscanning.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 07:15:27 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Zheng", "Lintao", ""], ["Zhu", "Chenyang", ""], ["Zhang", "Jiazhao", ""], ["Zhao", "Hang", ""], ["Huang", "Hui", ""], ["Niessner", "Matthias", ""], ["Xu", "Kai", ""]]}, {"id": "1906.07413", "submitter": "Kaidi Cao", "authors": "Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, Tengyu Ma", "title": "Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss", "comments": "to appear in NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning algorithms can fare poorly when the training dataset suffers\nfrom heavy class-imbalance but the testing criterion requires good\ngeneralization on less frequent classes. We design two novel methods to improve\nperformance in such scenarios. First, we propose a theoretically-principled\nlabel-distribution-aware margin (LDAM) loss motivated by minimizing a\nmargin-based generalization bound. This loss replaces the standard\ncross-entropy objective during training and can be applied with prior\nstrategies for training with class-imbalance such as re-weighting or\nre-sampling. Second, we propose a simple, yet effective, training schedule that\ndefers re-weighting until after the initial stage, allowing the model to learn\nan initial representation while avoiding some of the complications associated\nwith re-weighting or re-sampling. We test our methods on several benchmark\nvision tasks including the real-world imbalanced dataset iNaturalist 2018. Our\nexperiments show that either of these methods alone can already improve over\nexisting techniques and their combination achieves even better performance\ngains.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 07:21:18 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2019 23:31:52 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Cao", "Kaidi", ""], ["Wei", "Colin", ""], ["Gaidon", "Adrien", ""], ["Arechiga", "Nikos", ""], ["Ma", "Tengyu", ""]]}, {"id": "1906.07421", "submitter": "Shreyank N Gowda", "authors": "Shreyank Narayana Gowda", "title": "Using colorization as a tool for automatic makeup suggestion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colorization is the method of converting an image in grayscale to a fully\ncolor image. There are multiple methods to do the same. Old school methods used\nmachine learning algorithms and optimization techniques to suggest possible\ncolors to use. With advances in the field of deep learning, colorization\nresults have improved consistently with improvements in deep learning\narchitectures. The latest development in the field of deep learning is the\nemergence of generative adversarial networks (GANs) which is used to generate\ninformation and not just predict or classify. As part of this report, 2\narchitectures of recent papers are reproduced along with a novel architecture\nbeing suggested for general colorization. Following this, we propose the use of\ncolorization by generating makeup suggestions automatically on a face. To do\nthis, a dataset consisting of 1000 images has been created. When an image of a\nperson without makeup is sent to the model, the model first converts the image\nto grayscale and then passes it through the suggested GAN model. The output is\na generated makeup suggestion. To develop this model, we need to tweak the\ngeneral colorization model to deal only with faces of people.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 07:45:43 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Gowda", "Shreyank Narayana", ""]]}, {"id": "1906.07441", "submitter": "Jingjing Li", "authors": "Li Jingjing and Jing Mengmeng and Lu Ke and Zhu Lei and Shen Heng Tao", "title": "Locality Preserving Joint Transfer for Domain Adaptation", "comments": "Accepted to IEEE TIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation aims to leverage knowledge from a well-labeled source\ndomain to a poorly-labeled target domain. A majority of existing works transfer\nthe knowledge at either feature level or sample level. Recent researches reveal\nthat both of the paradigms are essentially important, and optimizing one of\nthem can reinforce the other. Inspired by this, we propose a novel approach to\njointly exploit feature adaptation with distribution matching and sample\nadaptation with landmark selection. During the knowledge transfer, we also take\nthe local consistency between samples into consideration, so that the manifold\nstructures of samples can be preserved. At last, we deploy label propagation to\npredict the categories of new instances. Notably, our approach is suitable for\nboth homogeneous and heterogeneous domain adaptation by learning\ndomain-specific projections. Extensive experiments on five open benchmarks,\nwhich consist of both standard and large-scale datasets, verify that our\napproach can significantly outperform not only conventional approaches but also\nend-to-end deep models. The experiments also demonstrate that we can leverage\nhandcrafted features to promote the accuracy on deep features by heterogeneous\nadaptation.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 08:34:40 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Jingjing", "Li", ""], ["Mengmeng", "Jing", ""], ["Ke", "Lu", ""], ["Lei", "Zhu", ""], ["Tao", "Shen Heng", ""]]}, {"id": "1906.07480", "submitter": "Matthieu Grard", "authors": "Matthieu Grard (imagine), Emmanuel Dellandr\\'ea (imagine), Liming Chen\n  (imagine)", "title": "Deep Multicameral Decoding for Localizing Unoccluded Object Instances\n  from a Single RGB Image", "comments": "International Journal of Computer Vision, Springer Verlag, 2020,\n  Special Issue on Deep Learning for Robotic Vision", "journal-ref": null, "doi": "10.1007/s11263-020-01323-0", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occlusion-aware instance-sensitive segmentation is a complex task generally\nsplit into region-based segmentations, by approximating instances as their\nbounding box. We address the showcase scenario of dense homogeneous layouts in\nwhich this approximation does not hold. In this scenario, outlining unoccluded\ninstances by decoding a deep encoder becomes difficult, due to the translation\ninvariance of convolutional layers and the lack of complexity in the decoder.\nWe therefore propose a multicameral design composed of subtask-specific\nlightweight decoder and encoder-decoder units, coupled in cascade to encourage\nsubtask-specific feature reuse and enforce a learning path within the decoding\nprocess. Furthermore, the state-of-the-art datasets for occlusion-aware\ninstance segmentation contain real images with few instances and occlusions\nmostly due to objects occluding the background, unlike dense object layouts. We\nthus also introduce a synthetic dataset of dense homogeneous object layouts,\nnamely Mikado, which extensibly contains more instances and inter-instance\nocclusions per image than these public datasets. Our extensive experiments on\nMikado and public datasets show that ordinal multiscale units within the\ndecoding process prove more effective than state-of-the-art design patterns for\ncapturing position-sensitive representations. We also show that Mikado is\nplausible with respect to real-world problems, in the sense that it enables the\nlearning of performance-enhancing representations transferable to real images,\nwhile drastically reducing the need of hand-made annotations for finetuning.\nThe proposed dataset will be made publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 10:20:17 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 16:08:17 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2020 08:46:37 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Grard", "Matthieu", "", "imagine"], ["Dellandr\u00e9a", "Emmanuel", "", "imagine"], ["Chen", "Liming", "", "imagine"]]}, {"id": "1906.07488", "submitter": "Dong Wang", "authors": "Dong Wang, Lei Zhou, Xiao Bai and Jun Zhou", "title": "A One-step Pruning-recovery Framework for Acceleration of Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acceleration of convolutional neural network has received increasing\nattention during the past several years. Among various acceleration techniques,\nfilter pruning has its inherent merit by effectively reducing the number of\nconvolution filters. However, most filter pruning methods resort to tedious and\ntime-consuming layer-by-layer pruning-recovery strategy to avoid a significant\ndrop of accuracy. In this paper, we present an efficient filter pruning\nframework to solve this problem. Our method accelerates the network in one-step\npruning-recovery manner with a novel optimization objective function, which\nachieves higher accuracy with much less cost compared with existing pruning\nmethods. Furthermore, our method allows network compression with global filter\npruning. Given a global pruning rate, it can adaptively determine the pruning\nrate for each single convolutional layer, while these rates are often set as\nhyper-parameters in previous approaches. Evaluated on VGG-16 and ResNet-50\nusing ImageNet, our approach outperforms several state-of-the-art methods with\nless accuracy drop under the same and even much fewer floating-point operations\n(FLOPs).\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 10:40:55 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Wang", "Dong", ""], ["Zhou", "Lei", ""], ["Bai", "Xiao", ""], ["Zhou", "Jun", ""]]}, {"id": "1906.07496", "submitter": "Petru Manescu", "authors": "Petru Manescu, Lydia Neary- Zajiczek, Michael J. Shaw, Muna Elmi, Remy\n  Claveau, Vijay Pawar, John Shawe-Taylor, Iasonas Kokkinos, Mandayam A.\n  Srinivasan, Ikeoluwa Lagunju, Olugbemiro Sodeinde, Biobele J. Brown, Delmiro\n  Fernandez-Reyes", "title": "Deep Learning Enhanced Extended Depth-of-Field for Thick Blood-Film\n  Malaria High-Throughput Microscopy", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast accurate diagnosis of malaria is still a global health challenge for\nwhich automated digital-pathology approaches could provide scalable solutions\namenable to be deployed in low-to-middle income countries. Here we address the\nproblem of Extended Depth-of-Field (EDoF) in thick blood film microscopy for\nrapid automated malaria diagnosis. High magnification oil-objectives (100x)\nwith large numerical aperture are usually preferred to resolve the fine\nstructural details that help separate true parasites from distractors. However,\nsuch objectives have a very limited depth-of-field requiring the acquisition of\na series of images at different focal planes per field of view (FOV). Current\nEDoF techniques based on multi-scale decompositions are time consuming and\ntherefore not suited for high-throughput analysis of specimens. To overcome\nthis challenge, we developed a new deep learning method based on Convolutional\nNeural Networks (EDoF-CNN) that is able to rapidly perform the extended\ndepth-of-field while also enhancing the spatial resolution of the resulting\nfused image. We evaluated our approach using simulated low-resolution z-stacks\nfrom Giemsa-stained thick blood smears from patients presenting with Plasmodium\nfalciparum malaria. The EDoF-CNN allows speed-up of our digital-pathology\nacquisition platform and significantly improves the quality of the EDoF\ncompared to the traditional multi-scaled approaches when applied to lower\nresolution stacks corresponding to acquisitions with fewer focal planes, large\ncamera pixel binning or lower magnification objectives (larger FOV). We use the\nparasite detection accuracy of a deep learning model on the EDoFs as a\nconcrete, task-specific measure of performance of this approach.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 11:12:44 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Manescu", "Petru", ""], ["Zajiczek", "Lydia Neary-", ""], ["Shaw", "Michael J.", ""], ["Elmi", "Muna", ""], ["Claveau", "Remy", ""], ["Pawar", "Vijay", ""], ["Shawe-Taylor", "John", ""], ["Kokkinos", "Iasonas", ""], ["Srinivasan", "Mandayam A.", ""], ["Lagunju", "Ikeoluwa", ""], ["Sodeinde", "Olugbemiro", ""], ["Brown", "Biobele J.", ""], ["Fernandez-Reyes", "Delmiro", ""]]}, {"id": "1906.07527", "submitter": "Mingjie Li", "authors": "Mingjie Li, Youqian Feng, Zhonghai Yin, Cheng Zhou, Fanghao Dong", "title": "Impoved RPN for Single Targets Detection based on the Anchor Mask Net", "comments": "10 pages,9 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common target detection is usually based on single frame images, which is\nvulnerable to affected by the similar targets in the image and not applicable\nto video images. In this paper , anchor mask is proposed to add the prior\nknowledge for target detection and an anchor mask net is designed to impove the\nRPN performance for single target detection. Tested in the VOT2016, the model\nperform better.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 12:32:17 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Li", "Mingjie", ""], ["Feng", "Youqian", ""], ["Yin", "Zhonghai", ""], ["Zhou", "Cheng", ""], ["Dong", "Fanghao", ""]]}, {"id": "1906.07538", "submitter": "Skand Vishwanath Peri", "authors": "Deepak Babu Sam, Skand Vishwanath Peri, Mukuntha Narayanan\n  Sundararaman, Amogh Kamath, R. Venkatesh Babu", "title": "Locate, Size and Count: Accurately Resolving People in Dense Crowds via\n  Detection", "comments": "Accepted in T-PAMI, 2020. Code available at :\n  https://github.com/val-iisc/lsc-cnn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a detection framework for dense crowd counting and eliminate the\nneed for the prevalent density regression paradigm. Typical counting models\npredict crowd density for an image as opposed to detecting every person. These\nregression methods, in general, fail to localize persons accurate enough for\nmost applications other than counting. Hence, we adopt an architecture that\nlocates every person in the crowd, sizes the spotted heads with bounding box\nand then counts them. Compared to normal object or face detectors, there exist\ncertain unique challenges in designing such a detection system. Some of them\nare direct consequences of the huge diversity in dense crowds along with the\nneed to predict boxes contiguously. We solve these issues and develop our\nLSC-CNN model, which can reliably detect heads of people across sparse to dense\ncrowds. LSC-CNN employs a multi-column architecture with top-down feedback\nprocessing to better resolve persons and produce refined predictions at\nmultiple resolutions. Interestingly, the proposed training regime requires only\npoint head annotation, but can estimate approximate size information of heads.\nWe show that LSC-CNN not only has superior localization than existing density\nregressors, but outperforms in counting as well. The code for our approach is\navailable at https://github.com/val-iisc/lsc-cnn.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 12:58:07 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 17:25:20 GMT"}, {"version": "v3", "created": "Sat, 15 Feb 2020 16:44:11 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Sam", "Deepak Babu", ""], ["Peri", "Skand Vishwanath", ""], ["Sundararaman", "Mukuntha Narayanan", ""], ["Kamath", "Amogh", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1906.07549", "submitter": "Zhusi Zhong", "authors": "Zhusi Zhong, Jie Li, Zhenxi Zhang, Zhicheng Jiao, Xinbo Gao", "title": "An Attention-Guided Deep Regression Model for Landmark Detection in\n  Cephalograms", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-32226-7_60", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cephalometric tracing method is usually used in orthodontic diagnosis and\ntreatment planning. In this paper, we propose a deep learning based framework\nto automatically detect anatomical landmarks in cephalometric X-ray images. We\ntrain the deep encoder-decoder for landmark detection, and combine global\nlandmark configuration with local high-resolution feature responses. The\nproposed frame-work is based on 2-stage u-net, regressing the multi-channel\nheatmaps for land-mark detection. In this framework, we embed attention\nmechanism with global stage heatmaps, guiding the local stage inferring, to\nregress the local heatmap patches in a high resolution. Besides, the Expansive\nExploration strategy improves robustness while inferring, expanding the\nsearching scope without increasing model complexity. We have evaluated our\nframework in the most widely-used public dataset of landmark detection in\ncephalometric X-ray images. With less computation and manually tuning, our\nframework achieves state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 15:11:31 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 03:24:20 GMT"}, {"version": "v3", "created": "Sun, 27 Sep 2020 07:44:02 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Zhong", "Zhusi", ""], ["Li", "Jie", ""], ["Zhang", "Zhenxi", ""], ["Jiao", "Zhicheng", ""], ["Gao", "Xinbo", ""]]}, {"id": "1906.07582", "submitter": "Karen Ullrich", "authors": "Karen Ullrich, Rianne van den Berg, Marcus Brubaker, David Fleet, Max\n  Welling", "title": "Differentiable probabilistic models of scientific imaging with the\n  Fourier slice theorem", "comments": "accepted to UAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scientific imaging techniques such as optical and electron microscopy and\ncomputed tomography (CT) scanning are used to study the 3D structure of an\nobject through 2D observations. These observations are related to the original\n3D object through orthogonal integral projections. For common 3D reconstruction\nalgorithms, computational efficiency requires the modeling of the 3D structures\nto take place in Fourier space by applying the Fourier slice theorem. At\npresent, it is unclear how to differentiate through the projection operator,\nand hence current learning algorithms can not rely on gradient based methods to\noptimize 3D structure models. In this paper we show how back-propagation\nthrough the projection operator in Fourier space can be achieved. We\ndemonstrate the validity of the approach with experiments on 3D reconstruction\nof proteins. We further extend our approach to learning probabilistic models of\n3D objects. This allows us to predict regions of low sampling rates or estimate\nnoise. A higher sample efficiency can be reached by utilizing the learned\nuncertainties of the 3D structure as an unsupervised estimate of the model fit.\nFinally, we demonstrate how the reconstruction algorithm can be extended with\nan amortized inference scheme on unknown attributes such as object pose.\nThrough empirical studies we show that joint inference of the 3D structure and\nthe object pose becomes more difficult when the ground truth object contains\nmore symmetries. Due to the presence of for instance (approximate) rotational\nsymmetries, the pose estimation can easily get stuck in local optima,\ninhibiting a fine-grained high-quality estimate of the 3D structure.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 13:49:06 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 15:20:54 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Ullrich", "Karen", ""], ["Berg", "Rianne van den", ""], ["Brubaker", "Marcus", ""], ["Fleet", "David", ""], ["Welling", "Max", ""]]}, {"id": "1906.07589", "submitter": "Jerome Revaud", "authors": "Jerome Revaud, Jon Almazan, Rafael Sampaio de Rezende, Cesar Roberto\n  de Souza", "title": "Learning with Average Precision: Training Image Retrieval with a\n  Listwise Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image retrieval can be formulated as a ranking problem where the goal is to\norder database images by decreasing similarity to the query. Recent deep models\nfor image retrieval have outperformed traditional methods by leveraging\nranking-tailored loss functions, but important theoretical and practical\nproblems remain. First, rather than directly optimizing the global ranking,\nthey minimize an upper-bound on the essential loss, which does not necessarily\nresult in an optimal mean average precision (mAP). Second, these methods\nrequire significant engineering efforts to work well, e.g. special pre-training\nand hard-negative mining. In this paper we propose instead to directly optimize\nthe global mAP by leveraging recent advances in listwise loss formulations.\nUsing a histogram binning approximation, the AP can be differentiated and thus\nemployed to end-to-end learning. Compared to existing losses, the proposed\nmethod considers thousands of images simultaneously at each iteration and\neliminates the need for ad hoc tricks. It also establishes a new state of the\nart on many standard retrieval benchmarks. Models and evaluation scripts have\nbeen made available at https://europe.naverlabs.com/Deep-Image-Retrieval/\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 13:59:34 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Revaud", "Jerome", ""], ["Almazan", "Jon", ""], ["de Rezende", "Rafael Sampaio", ""], ["de Souza", "Cesar Roberto", ""]]}, {"id": "1906.07645", "submitter": "Alice Othmani", "authors": "Alice Othmani, Fakhri Torkhani, Jean-Marie Favreau", "title": "3D Geometric salient patterns analysis on 3D meshes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Pattern analysis is a wide domain that has wide applicability in many fields.\nIn fact, texture analysis is one of those fields, since the texture is defined\nas a set of repetitive or quasi-repetitive patterns. Despite its importance in\nanalyzing 3D meshes, geometric texture analysis is less studied by geometry\nprocessing community. This paper presents a new efficient approach for\ngeometric texture analysis on 3D triangular meshes. The proposed method is a\nscale-aware approach that takes as input a 3D mesh and a user-scale. It\nprovides, as a result, a similarity-based clustering of texels in meaningful\nclasses. Experimental results of the proposed algorithm are presented for both\nreal-world and synthetic meshes within various textures. Furthermore, the\nefficiency of the proposed approach was experimentally demonstrated under mesh\nsimplification and noise addition on the mesh surface. In this paper, we\npresent a practical application for semantic annotation of 3D geometric salient\ntexels.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 15:43:14 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Othmani", "Alice", ""], ["Torkhani", "Fakhri", ""], ["Favreau", "Jean-Marie", ""]]}, {"id": "1906.07647", "submitter": "Mustafa Umit Oner", "authors": "Mustafa Umit Oner, Hwee Kuan Lee, Wing-Kin Sung", "title": "Weakly Supervised Clustering by Exploiting Unique Class Count", "comments": "Published as a conference paper at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A weakly supervised learning based clustering framework is proposed in this\npaper. As the core of this framework, we introduce a novel multiple instance\nlearning task based on a bag level label called unique class count ($ucc$),\nwhich is the number of unique classes among all instances inside the bag. In\nthis task, no annotations on individual instances inside the bag are needed\nduring training of the models. We mathematically prove that with a perfect\n$ucc$ classifier, perfect clustering of individual instances inside the bags is\npossible even when no annotations on individual instances are given during\ntraining. We have constructed a neural network based $ucc$ classifier and\nexperimentally shown that the clustering performance of our framework with our\nweakly supervised $ucc$ classifier is comparable to that of fully supervised\nlearning models where labels for all instances are known. Furthermore, we have\ntested the applicability of our framework to a real world task of semantic\nsegmentation of breast cancer metastases in histological lymph node sections\nand shown that the performance of our weakly supervised framework is comparable\nto the performance of a fully supervised Unet model.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 15:44:54 GMT"}, {"version": "v2", "created": "Sat, 25 Jan 2020 08:47:04 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Oner", "Mustafa Umit", ""], ["Lee", "Hwee Kuan", ""], ["Sung", "Wing-Kin", ""]]}, {"id": "1906.07671", "submitter": "Zhisheng Zhong", "authors": "Zhisheng Zhong, Fangyin Wei, Zhouchen Lin and Chao Zhang", "title": "ADA-Tucker: Compressing Deep Neural Networks via Adaptive Dimension\n  Adjustment Tucker Decomposition", "comments": "25 pages, 12 figures", "journal-ref": "Elsevier, Neural Networks, Volume 110, Feb. 2019, Pages 104-115", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent success of deep learning models in numerous applications,\ntheir widespread use on mobile devices is seriously impeded by storage and\ncomputational requirements. In this paper, we propose a novel network\ncompression method called Adaptive Dimension Adjustment Tucker decomposition\n(ADA-Tucker). With learnable core tensors and transformation matrices,\nADA-Tucker performs Tucker decomposition of arbitrary-order tensors.\nFurthermore, we propose that weight tensors in networks with proper order and\nbalanced dimension are easier to be compressed. Therefore, the high flexibility\nin decomposition choice distinguishes ADA-Tucker from all previous low-rank\nmodels. To compress more, we further extend the model to Shared Core ADA-Tucker\n(SCADA-Tucker) by defining a shared core tensor for all layers. Our methods\nrequire no overhead of recording indices of non-zero elements. Without loss of\naccuracy, our methods reduce the storage of LeNet-5 and LeNet-300 by ratios of\n691 times and 233 times, respectively, significantly outperforming state of the\nart. The effectiveness of our methods is also evaluated on other three\nbenchmarks (CIFAR-10, SVHN, ILSVRC12) and modern newly deep networks (ResNet,\nWide-ResNet).\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 16:19:04 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Zhong", "Zhisheng", ""], ["Wei", "Fangyin", ""], ["Lin", "Zhouchen", ""], ["Zhang", "Chao", ""]]}, {"id": "1906.07675", "submitter": "Robin Heinzler", "authors": "Robin Heinzler, Philipp Schindler, J\\\"urgen Seekircher, Werner Ritter,\n  Wilhelm Stork", "title": "Weather Influence and Classification with Automotive Lidar Sensors", "comments": "8 pages, will be published in the IEEE IV 2019 Proceedings", "journal-ref": "2019 IEEE Intelligent Vehicles Symposium (IV)", "doi": "10.1109/IVS.2019.8814205", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lidar sensors are often used in mobile robots and autonomous vehicles to\ncomplement camera, radar and ultrasonic sensors for environment perception.\nTypically, perception algorithms are trained to only detect moving and static\nobjects as well as ground estimation, but intentionally ignore weather effects\nto reduce false detections. In this work, we present an in-depth analysis of\nautomotive lidar performance under harsh weather conditions, i.e. heavy rain\nand dense fog. An extensive data set has been recorded for various fog and rain\nconditions, which is the basis for the conducted in-depth analysis of the point\ncloud under changing environmental conditions. In addition, we introduce a\nnovel approach to detect and classify rain or fog with lidar sensors only and\nachieve an mean union over intersection of 97.14 % for a data set in controlled\nenvironments. The analysis of weather influences on the performance of lidar\nsensors and the weather detection are important steps towards improving safety\nlevels for autonomous driving in adverse weather conditions by providing\nreliable information to adapt vehicle behavior.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 16:29:03 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Heinzler", "Robin", ""], ["Schindler", "Philipp", ""], ["Seekircher", "J\u00fcrgen", ""], ["Ritter", "Werner", ""], ["Stork", "Wilhelm", ""]]}, {"id": "1906.07679", "submitter": "Rhona Asgari", "authors": "Rhona Asgari and Jos\\'e Ignacio Orlando and Sebastian Waldstein and\n  Ferdinand Schlanitz and Magdalena Baratsits and Ursula Schmidt-Erfurth and\n  Hrvoje Bogunovi\\'c", "title": "Multiclass segmentation as multitask learning for drusen segmentation in\n  retinal optical coherence tomography", "comments": "Accepted for publication in MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated drusen segmentation in retinal optical coherence tomography (OCT)\nscans is relevant for understanding age-related macular degeneration (AMD) risk\nand progression. This task is usually performed by segmenting the top/bottom\nanatomical interfaces that define drusen, the outer boundary of the retinal\npigment epithelium (OBRPE) and the Bruch's membrane (BM), respectively. In this\npaper we propose a novel multi-decoder architecture that tackles drusen\nsegmentation as a multitask problem. Instead of training a multiclass model for\nOBRPE/BM segmentation, we use one decoder per target class and an extra one\naiming for the area between the layers. We also introduce connections between\neach class-specific branch and the additional decoder to increase the\nregularization effect of this surrogate task. We validated our approach on\nprivate/public data sets with 166 early/intermediate AMD Spectralis, and 200\nAMD and control Bioptigen OCT volumes, respectively. Our method consistently\noutperformed several baselines in both layer and drusen segmentation\nevaluations.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 16:45:46 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 13:41:06 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Asgari", "Rhona", ""], ["Orlando", "Jos\u00e9 Ignacio", ""], ["Waldstein", "Sebastian", ""], ["Schlanitz", "Ferdinand", ""], ["Baratsits", "Magdalena", ""], ["Schmidt-Erfurth", "Ursula", ""], ["Bogunovi\u0107", "Hrvoje", ""]]}, {"id": "1906.07689", "submitter": "Hao Tan", "authors": "Hao Tan, Franck Dernoncourt, Zhe Lin, Trung Bui, Mohit Bansal", "title": "Expressing Visual Relationships via Language", "comments": "ACL 2019 (11 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Describing images with text is a fundamental problem in vision-language\nresearch. Current studies in this domain mostly focus on single image\ncaptioning. However, in various real applications (e.g., image editing,\ndifference interpretation, and retrieval), generating relational captions for\ntwo images, can also be very useful. This important problem has not been\nexplored mostly due to lack of datasets and effective models. To push forward\nthe research in this direction, we first introduce a new language-guided image\nediting dataset that contains a large number of real image pairs with\ncorresponding editing instructions. We then propose a new relational speaker\nmodel based on an encoder-decoder architecture with static relational attention\nand sequential multi-head attention. We also extend the model with dynamic\nrelational attention, which calculates visual alignment while decoding. Our\nmodels are evaluated on our newly collected and two public datasets consisting\nof image pairs annotated with relationship sentences. Experimental results,\nbased on both automatic and human evaluation, demonstrate that our model\noutperforms all baselines and existing methods on all the datasets.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 17:01:21 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 02:49:11 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Tan", "Hao", ""], ["Dernoncourt", "Franck", ""], ["Lin", "Zhe", ""], ["Bui", "Trung", ""], ["Bansal", "Mohit", ""]]}, {"id": "1906.07751", "submitter": "Stephen Lombardi", "authors": "Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz,\n  Andreas Lehrmann, Yaser Sheikh", "title": "Neural Volumes: Learning Dynamic Renderable Volumes from Images", "comments": "Accepted to SIGGRAPH 2019", "journal-ref": "ACM Transactions on Graphics (SIGGRAPH 2019) 38, 4, Article 65", "doi": "10.1145/3306346.3323020", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling and rendering of dynamic scenes is challenging, as natural scenes\noften contain complex phenomena such as thin structures, evolving topology,\ntranslucency, scattering, occlusion, and biological motion. Mesh-based\nreconstruction and tracking often fail in these cases, and other approaches\n(e.g., light field video) typically rely on constrained viewing conditions,\nwhich limit interactivity. We circumvent these difficulties by presenting a\nlearning-based approach to representing dynamic objects inspired by the\nintegral projection model used in tomographic imaging. The approach is\nsupervised directly from 2D images in a multi-view capture setting and does not\nrequire explicit reconstruction or tracking of the object. Our method has two\nprimary components: an encoder-decoder network that transforms input images\ninto a 3D volume representation, and a differentiable ray-marching operation\nthat enables end-to-end training. By virtue of its 3D representation, our\nconstruction extrapolates better to novel viewpoints compared to screen-space\nrendering techniques. The encoder-decoder architecture learns a latent\nrepresentation of a dynamic scene that enables us to produce novel content\nsequences not seen during training. To overcome memory limitations of\nvoxel-based representations, we learn a dynamic irregular grid structure\nimplemented with a warp field during ray-marching. This structure greatly\nimproves the apparent resolution and reduces grid-like artifacts and jagged\nmotion. Finally, we demonstrate how to incorporate surface-based\nrepresentations into our volumetric-learning framework for applications where\nthe highest resolution is required, using facial performance capture as a case\nin point.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 18:21:46 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Lombardi", "Stephen", ""], ["Simon", "Tomas", ""], ["Saragih", "Jason", ""], ["Schwartz", "Gabriel", ""], ["Lehrmann", "Andreas", ""], ["Sheikh", "Yaser", ""]]}, {"id": "1906.07760", "submitter": "Fei Xu", "authors": "Fei Xu, Yingtao Zhang, Min Xian, H. D. Cheng, Boyu Zhang, Jianrui\n  Ding, Chunping Ning, Ying Wang", "title": "Tumor Saliency Estimation for Breast Ultrasound Images via Breast\n  Anatomy Modeling", "comments": "submitted a journal paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tumor saliency estimation aims to localize tumors by modeling the visual\nstimuli in medical images. However, it is a challenging task for breast\nultrasound due to the complicated anatomic structure of the breast and poor\nimage quality; and existing saliency estimation approaches only model generic\nvisual stimuli, e.g., local and global contrast, location, and feature\ncorrelation, and achieve poor performance for tumor saliency estimation. In\nthis paper, we propose a novel optimization model to estimate tumor saliency by\nutilizing breast anatomy. First, we model breast anatomy and decompose breast\nultrasound image into layers using Neutro-Connectedness; then utilize the\nlayers to generate the foreground and background maps; and finally propose a\nnovel objective function to estimate the tumor saliency by integrating the\nforeground map, background map, adaptive center bias, and region-based\ncorrelation cues. The extensive experiments demonstrate that the proposed\napproach obtains more accurate foreground and background maps with the\nassistance of the breast anatomy; especially, for the images having large or\nsmall tumors; meanwhile, the new objective function can handle the images\nwithout tumors. The newly proposed method achieves state-of-the-art performance\nwhen compared to eight tumor saliency estimation approaches using two breast\nultrasound datasets.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 18:35:01 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Xu", "Fei", ""], ["Zhang", "Yingtao", ""], ["Xian", "Min", ""], ["Cheng", "H. D.", ""], ["Zhang", "Boyu", ""], ["Ding", "Jianrui", ""], ["Ning", "Chunping", ""], ["Wang", "Ying", ""]]}, {"id": "1906.07771", "submitter": "Sara Mardanisamani", "authors": "Sara Mardanisamani, Farhad Maleki, Sara Hosseinzadeh Kassani, Sajith\n  Rajapaksa, Hema Duddu, Menglu Wang, Steve Shirtliffe, Seungbum Ryu, Anique\n  Josuttes, Ti Zhang, Sally Vail, Curtis Pozniak, Isobel Parkin, Ian Stavness\n  and Mark Eramian", "title": "Crop Lodging Prediction from UAV-Acquired Images of Wheat and Canola\n  using a DCNN Augmented with Handcrafted Texture Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lodging, the permanent bending over of food crops, leads to poor plant growth\nand development. Consequently, lodging results in reduced crop quality, lowers\ncrop yield, and makes harvesting difficult. Plant breeders routinely evaluate\nseveral thousand breeding lines, and therefore, automatic lodging detection and\nprediction is of great value aid in selection. In this paper, we propose a deep\nconvolutional neural network (DCNN) architecture for lodging classification\nusing five spectral channel orthomosaic images from canola and wheat breeding\ntrials. Also, using transfer learning, we trained 10 lodging detection models\nusing well-established deep convolutional neural network architectures. Our\nproposed model outperforms the state-of-the-art lodging detection methods in\nthe literature that use only handcrafted features. In comparison to 10 DCNN\nlodging detection models, our proposed model achieves comparable results while\nhaving a substantially lower number of parameters. This makes the proposed\nmodel suitable for applications such as real-time classification using\ninexpensive hardware for high-throughput phenotyping pipelines. The GitHub\nrepository at https://github.com/FarhadMaleki/LodgedNet contains code and\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 19:11:38 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Mardanisamani", "Sara", ""], ["Maleki", "Farhad", ""], ["Kassani", "Sara Hosseinzadeh", ""], ["Rajapaksa", "Sajith", ""], ["Duddu", "Hema", ""], ["Wang", "Menglu", ""], ["Shirtliffe", "Steve", ""], ["Ryu", "Seungbum", ""], ["Josuttes", "Anique", ""], ["Zhang", "Ti", ""], ["Vail", "Sally", ""], ["Pozniak", "Curtis", ""], ["Parkin", "Isobel", ""], ["Stavness", "Ian", ""], ["Eramian", "Mark", ""]]}, {"id": "1906.07775", "submitter": "Sasa Grbic", "authors": "Florin C. Ghesu, Bogdan Georgescu, Eli Gibson, Sebastian Guendel,\n  Mannudeep K. Kalra, Ramandeep Singh, Subba R. Digumarthy, Sasa Grbic, Dorin\n  Comaniciu", "title": "Quantifying and Leveraging Classification Uncertainty for Chest\n  Radiograph Assessment", "comments": "Accepted for presentation at MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interpretation of chest radiographs is an essential task for the\ndetection of thoracic diseases and abnormalities. However, it is a challenging\nproblem with high inter-rater variability and inherent ambiguity due to\ninconclusive evidence in the data, limited data quality or subjective\ndefinitions of disease appearance. Current deep learning solutions for chest\nradiograph abnormality classification are typically limited to providing\nprobabilistic predictions, relying on the capacity of learning models to adapt\nto the high degree of label noise and become robust to the enumerated causal\nfactors. In practice, however, this leads to overconfident systems with poor\ngeneralization on unseen data. To account for this, we propose an automatic\nsystem that learns not only the probabilistic estimate on the presence of an\nabnormality, but also an explicit uncertainty measure which captures the\nconfidence of the system in the predicted output. We argue that explicitly\nlearning the classification uncertainty as an orthogonal measure to the\npredicted output, is essential to account for the inherent variability\ncharacteristic of this data. Experiments were conducted on two datasets of\nchest radiographs of over 85,000 patients. Sample rejection based on the\npredicted uncertainty can significantly improve the ROC-AUC, e.g., by 8% to\n0.91 with an expected rejection rate of under 25%. Eliminating training samples\nusing uncertainty-driven bootstrapping, enables a significant increase in\nrobustness and accuracy. In addition, we present a multi-reader study showing\nthat the predictive uncertainty is indicative of reader errors.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 19:21:44 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Ghesu", "Florin C.", ""], ["Georgescu", "Bogdan", ""], ["Gibson", "Eli", ""], ["Guendel", "Sebastian", ""], ["Kalra", "Mannudeep K.", ""], ["Singh", "Ramandeep", ""], ["Digumarthy", "Subba R.", ""], ["Grbic", "Sasa", ""], ["Comaniciu", "Dorin", ""]]}, {"id": "1906.07789", "submitter": "Michael Schmitt", "authors": "Michael Schmitt and Lloyd Haydn Hughes and Chunping Qiu and Xiao Xiang\n  Zhu", "title": "SEN12MS -- A Curated Dataset of Georeferenced Multi-Spectral\n  Sentinel-1/2 Imagery for Deep Learning and Data Fusion", "comments": "accepted for publication in the ISPRS Annals of the Photogrammetry,\n  Remote Sensing and Spatial Information Sciences (online from September 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of curated large-scale training data is a crucial factor for\nthe development of well-generalizing deep learning methods for the extraction\nof geoinformation from multi-sensor remote sensing imagery. While quite some\ndatasets have already been published by the community, most of them suffer from\nrather strong limitations, e.g. regarding spatial coverage, diversity or simply\nnumber of available samples. Exploiting the freely available data acquired by\nthe Sentinel satellites of the Copernicus program implemented by the European\nSpace Agency, as well as the cloud computing facilities of Google Earth Engine,\nwe provide a dataset consisting of 180,662 triplets of dual-pol synthetic\naperture radar (SAR) image patches, multi-spectral Sentinel-2 image patches,\nand MODIS land cover maps. With all patches being fully georeferenced at a 10 m\nground sampling distance and covering all inhabited continents during all\nmeteorological seasons, we expect the dataset to support the community in\ndeveloping sophisticated deep learning-based approaches for common tasks such\nas scene classification or semantic segmentation for land cover mapping.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 20:21:41 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Schmitt", "Michael", ""], ["Hughes", "Lloyd Haydn", ""], ["Qiu", "Chunping", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "1906.07802", "submitter": "Saeed Izadi", "authors": "Saeed Izadi, Darren Sutton, Ghassan Hamarneh", "title": "Image Super Resolution via Bilinear Pooling: Application to Confocal\n  Endomicroscopy", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in image acquisition literature have miniaturized the\nconfocal laser endomicroscopes to improve usability and flexibility of the\napparatus in actual clinical settings. However, miniaturized devices collect\nless light and have fewer optical components, resulting in pixelation artifacts\nand low resolution images. Owing to the strength of deep networks, many\nsupervised methods known as super resolution have achieved considerable success\nin restoring low resolution images by generating the missing high frequency\ndetails. In this work, we propose a novel attention mechanism that, for the\nfirst time, combines 1st- and 2nd-order statistics for pooling operation, in\nthe spatial and channel-wise dimensions. We compare the efficacy of our method\nto 11 other existing single image super resolution techniques that compensate\nfor the reduction in image quality caused by the necessity of endomicroscope\nminiaturization. All evaluations are carried out on three publicly available\ndatasets. Experimental results show that our method can produce competitive\nresults against state-of-the-art in terms of PSNR, SSIM, and IFC metrics.\nAdditionally, our proposed method contains small number of parameters, which\nmakes it lightweight and fast for real-time applications.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 20:39:37 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 17:29:46 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Izadi", "Saeed", ""], ["Sutton", "Darren", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "1906.07851", "submitter": "Sungeun Hong", "authors": "Donghyeon Cho, Sungeun Hong, Sungil Kang, Jiwon Kim", "title": "Key Instance Selection for Unsupervised Video Object Segmentation", "comments": "Ranked 3rd in 'Unsupervised DAVIS Challenge' (CVPR 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes key instance selection based on video saliency covering\nobjectness and dynamics for unsupervised video object segmentation (UVOS). Our\nmethod takes frames sequentially and extracts object proposals with\ncorresponding masks for each frame. We link objects according to their\nsimilarity until the M-th frame and then assign them unique IDs (i.e.,\ninstances). Similarity measure takes into account multiple properties such as\nReID descriptor, expected trajectory, and semantic co-segmentation result.\nAfter M-th frame, we select K IDs based on video saliency and frequency of\nappearance; then only these key IDs are tracked through the remaining frames.\nThanks to these technical contributions, our results are ranked third on the\nleaderboard of UVOS DAVIS challenge.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 23:49:22 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 07:39:17 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Cho", "Donghyeon", ""], ["Hong", "Sungeun", ""], ["Kang", "Sungil", ""], ["Kim", "Jiwon", ""]]}, {"id": "1906.07866", "submitter": "Tat-Jun Chin Dr", "authors": "Samya Bagchi and Tat-Jun Chin", "title": "Event-based Star Tracking via Multiresolution Progressive Hough\n  Transforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Star trackers are state-of-the-art attitude estimation devices which function\nby recognising and tracking star patterns. Most commercial star trackers use\nconventional optical sensors. A recent alternative is to use event sensors,\nwhich could enable more energy efficient and faster star trackers. However,\nthis demands new algorithms that can efficiently cope with high-speed\nasynchronous data, and are feasible on resource-constrained computing\nplatforms. To this end, we propose an event-based processing approach for star\ntracking. Our technique operates on the event stream from a star field, by\nusing multiresolution Hough Transforms to time-progressively integrate event\ndata and produce accurate relative rotations. Optimisation via rotation\naveraging is then used to fuse the relative rotations and jointly refine the\nabsolute orientations. Our technique is designed to be feasible for\nasynchronous operation on standard hardware. Moreover, compared to\nstate-of-the-art event-based motion estimation schemes, our technique is much\nmore efficient and accurate.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 01:17:05 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 10:34:55 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Bagchi", "Samya", ""], ["Chin", "Tat-Jun", ""]]}, {"id": "1906.07870", "submitter": "Zaiqiang Wu", "authors": "Zaiqiang Wu, Wei Jiang", "title": "Analytical Derivatives for Differentiable Renderer: 3D Pose Estimation\n  by Silhouette Consistency", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentiable render is widely used in optimization-based 3D reconstruction\nwhich requires gradients from differentiable operations for gradient-based\noptimization. The existing differentiable renderers obtain the gradients of\nrendering via numerical technique which is of low accuracy and efficiency.\nMotivated by this fact, a differentiable mesh renderer with analytical\ngradients is proposed. The main obstacle of rasterization based rendering being\ndifferentiable is the discrete sampling operation. To make the rasterization\ndifferentiable, the pixel intensity is defined as a double integral over the\npixel area and the integral is approximated by anti-aliasing with an average\nfilter. Then the analytical gradients with respect to the vertices coordinates\ncan be derived from the continuous definition of pixel intensity. To\ndemonstrate the effectiveness and efficiency of the proposed differentiable\nrenderer, experiments of 3D pose estimation by only multi-viewpoint silhouettes\nwere conducted. The experimental results show that 3D pose estimation without\n3D and 2D joints supervision is capable of producing competitive results both\nqualitatively and quantitatively. The experimental results also show that the\nproposed differentiable renderer is of higher accuracy and efficiency compared\nwith previous method of differentiable renderer.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 01:34:12 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Wu", "Zaiqiang", ""], ["Jiang", "Wei", ""]]}, {"id": "1906.07889", "submitter": "Matthias Minderer", "authors": "Matthias Minderer, Chen Sun, Ruben Villegas, Forrester Cole, Kevin\n  Murphy, Honglak Lee", "title": "Unsupervised Learning of Object Structure and Dynamics from Videos", "comments": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Extracting and predicting object structure and dynamics from videos without\nsupervision is a major challenge in machine learning. To address this\nchallenge, we adopt a keypoint-based image representation and learn a\nstochastic dynamics model of the keypoints. Future frames are reconstructed\nfrom the keypoints and a reference frame. By modeling dynamics in the keypoint\ncoordinate space, we achieve stable learning and avoid compounding of errors in\npixel space. Our method improves upon unstructured representations both for\npixel-level video prediction and for downstream tasks requiring object-level\nunderstanding of motion dynamics. We evaluate our model on diverse datasets: a\nmulti-agent sports dataset, the Human3.6M dataset, and datasets based on\ncontinuous control tasks from the DeepMind Control Suite. The spatially\nstructured representation outperforms unstructured representations on a range\nof motion-related tasks such as object tracking, action recognition and reward\nprediction.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 02:56:51 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 16:49:53 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2020 17:32:13 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Minderer", "Matthias", ""], ["Sun", "Chen", ""], ["Villegas", "Ruben", ""], ["Cole", "Forrester", ""], ["Murphy", "Kevin", ""], ["Lee", "Honglak", ""]]}, {"id": "1906.07892", "submitter": "Kun Li", "authors": "Jingyu Yang, Ji Xu, Kun Li, Yu-Kun Lai, Huanjing Yue, Jianzhi Lu, Hao\n  Wu and Yebin Liu", "title": "Learning to Reconstruct and Understand Indoor Scenes from Sparse Views", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method for simultaneous 3D reconstruction and\nsemantic segmentation of indoor scenes. Unlike existing methods that require\nrecording a video using a color camera and/or a depth camera, our method only\nneeds a small number of (e.g., 3-5) color images from uncalibrated sparse views\nas input, which greatly simplifies data acquisition and extends applicable\nscenarios. Since different views have limited overlaps, our method allows a\nsingle image as input to discern the depth and semantic information of the\nscene. The key issue is how to recover relatively accurate depth from single\nimages and reconstruct a 3D scene by fusing very few depth maps. To address\nthis problem, we first design an iterative deep architecture, IterNet, that\nestimates depth and semantic segmentation alternately, so that they benefit\neach other. To deal with the little overlap and non-rigid transformation\nbetween views, we further propose a joint global and local registration method\nto reconstruct a 3D scene with semantic information from sparse views. We also\nmake available a new indoor synthetic dataset simultaneously providing\nphotorealistic high-resolution RGB images, accurate depth maps and pixel-level\nsemantic labels for thousands of complex layouts, useful for training and\nevaluation. Experimental results on public datasets and our dataset demonstrate\nthat our method achieves more accurate depth estimation, smaller semantic\nsegmentation errors and better 3D reconstruction results, compared with\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 03:10:06 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Yang", "Jingyu", ""], ["Xu", "Ji", ""], ["Li", "Kun", ""], ["Lai", "Yu-Kun", ""], ["Yue", "Huanjing", ""], ["Lu", "Jianzhi", ""], ["Wu", "Hao", ""], ["Liu", "Yebin", ""]]}, {"id": "1906.07893", "submitter": "Hisashi Shimodaira", "authors": "Hisashi Shimodaira", "title": "Extended probabilistic Rand index and the adjustable moving window-based\n  pixel-pair sampling method", "comments": "9 pages, 9 figures and 9tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The probabilistic Rand (PR) index has the following three problems: It lacks\nvariations in its value over images; the normalized probabilistic Rand (NPR)\nindex to address this is theoretically unclear, and the sampling method of\npixel-pairs was not proposed concretely. In this paper, we propose methods for\nsolving these problems. First, we propose extended probabilistic Rand (EPR)\nindex that considers not only similarity but also dissimilarity between\nsegmentations. The EPR index provides twice as wide effective range as the PR\nindex does. Second, we propose an adjustable moving window-based pixel-pair\nsampling (AWPS) method in which each pixel-pair is sampled adjustably by\nconsidering granularities of ground truth segmentations. Results of experiments\nshow that the proposed methods work effectively and efficiently.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 03:14:55 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Shimodaira", "Hisashi", ""]]}, {"id": "1906.07901", "submitter": "Shruti Palaskar", "authors": "Shruti Palaskar, Jindrich Libovick\\'y, Spandana Gella and Florian\n  Metze", "title": "Multimodal Abstractive Summarization for How2 Videos", "comments": "To appear in ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study abstractive summarization for open-domain videos.\nUnlike the traditional text news summarization, the goal is less to \"compress\"\ntext information but rather to provide a fluent textual summary of information\nthat has been collected and fused from different source modalities, in our case\nvideo and audio transcripts (or text). We show how a multi-source\nsequence-to-sequence model with hierarchical attention can integrate\ninformation from different modalities into a coherent output, compare various\nmodels trained with different modalities and present pilot experiments on the\nHow2 corpus of instructional videos. We also propose a new evaluation metric\n(Content F1) for abstractive summarization task that measures semantic adequacy\nrather than fluency of the summaries, which is covered by metrics like ROUGE\nand BLEU.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 03:52:42 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Palaskar", "Shruti", ""], ["Libovick\u00fd", "Jindrich", ""], ["Gella", "Spandana", ""], ["Metze", "Florian", ""]]}, {"id": "1906.07912", "submitter": "Zhuo Chen", "authors": "Zhuo Chen, Jiyuan Zhang, Ruizhou Ding, Diana Marculescu", "title": "ViP: Virtual Pooling for Accelerating CNN-based Image Classification and\n  Object Detection", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Convolutional Neural Networks (CNNs) have shown superior\ncapability in visual learning tasks. While accuracy-wise CNNs provide\nunprecedented performance, they are also known to be computationally intensive\nand energy demanding for modern computer systems. In this paper, we propose\nVirtual Pooling (ViP), a model-level approach to improve speed and energy\nconsumption of CNN-based image classification and object detection tasks, with\na provable error bound. We show the efficacy of ViP through experiments on four\nCNN models, three representative datasets, both desktop and mobile platforms,\nand two visual learning tasks, i.e., image classification and object detection.\nFor example, ViP delivers 2.1x speedup with less than 1.5% accuracy degradation\nin ImageNet classification on VGG-16, and 1.8x speedup with 0.025 mAP\ndegradation in PASCAL VOC object detection with Faster-RCNN. ViP also reduces\nmobile GPU and CPU energy consumption by up to 55% and 70%, respectively. As a\ncomplementary method to existing acceleration approaches, ViP achieves 1.9x\nspeedup on ThiNet leading to a combined speedup of 5.23x on VGG-16.\nFurthermore, ViP provides a knob for machine learning practitioners to generate\na set of CNN models with varying trade-offs between system speed/energy\nconsumption and accuracy to better accommodate the requirements of their tasks.\nCode is available at https://github.com/cmu-enyac/VirtualPooling.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 04:44:54 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 06:25:43 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Chen", "Zhuo", ""], ["Zhang", "Jiyuan", ""], ["Ding", "Ruizhou", ""], ["Marculescu", "Diana", ""]]}, {"id": "1906.07917", "submitter": "Vikas Desai", "authors": "Sai Vikas Desai, Vineeth N Balasubramanian, Tokihiro Fukatsu, Seishi\n  Ninomiya and Wei Guo", "title": "Automatic estimation of heading date of paddy rice using deep learning", "comments": null, "journal-ref": null, "doi": "10.1186/s13007-019-0457-1", "report-no": null, "categories": "q-bio.QM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate estimation of heading date of paddy rice greatly helps the breeders\nto understand the adaptability of different crop varieties in a given location.\nThe heading date also plays a vital role in determining grain yield for\nresearch experiments. Visual examination of the crop is laborious and time\nconsuming. Therefore, quick and precise estimation of heading date of paddy\nrice is highly essential. In this work, we propose a simple pipeline to detect\nregions containing flowering panicles from ground level RGB images of paddy\nrice. Given a fixed region size for an image, the number of regions containing\nflowering panicles is directly proportional to the number of flowering panicles\npresent. Consequently, we use the flowering panicle region counts to estimate\nthe heading date of the crop. The method is based on image classification using\nConvolutional Neural Networks (CNNs). We evaluated the performance of our\nalgorithm on five time series image sequences of three different varieties of\nrice crops. When compared to the previous work on this dataset, the accuracy\nand general versatility of the method has been improved and heading date has\nbeen estimated with a mean absolute error of less than 1 day.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 05:02:43 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Desai", "Sai Vikas", ""], ["Balasubramanian", "Vineeth N", ""], ["Fukatsu", "Tokihiro", ""], ["Ninomiya", "Seishi", ""], ["Guo", "Wei", ""]]}, {"id": "1906.07923", "submitter": "Jia-Wei Chen", "authors": "Rongfang Wang, Jie Zhang, Jia-Wei Chen, Licheng Jiao, Mi Wang", "title": "Imbalanced Learning-based Automatic SAR Images Change Detection by\n  Morphologically Supervised PCA-Net", "comments": null, "journal-ref": null, "doi": "10.1109/LGRS.2018.2878420", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection is a quite challenging task due to the imbalance between\nunchanged and changed class. In addition, the traditional difference map\ngenerated by log-ratio is subject to the speckle, which will reduce the\naccuracy. In this letter, an imbalanced learning-based change detection is\nproposed based on PCA network (PCA-Net), where a supervised PCA-Net is designed\nto obtain the robust features directly from given multitemporal SAR images\ninstead of a difference map. Furthermore, to tackle with the imbalance between\nchanged and unchanged classes, we propose a morphologically supervised learning\nmethod, where the knowledge in the pixels near the boundary between two classes\nare exploited to guide network training. Finally, our proposed PCA-Net can be\ntrained by the datasets with available reference maps and applied to a new\ndataset, which is quite practical in change detection projects. Our proposed\nmethod is verified on five sets of multiple temporal SAR images. It is\ndemonstrated from the experiment results that with the knowledge in training\nsamples from the boundary, the learned features benefit for change detection\nand make the proposed method outperforms than supervised methods trained by\nrandomly drawing samples.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 05:34:50 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Wang", "Rongfang", ""], ["Zhang", "Jie", ""], ["Chen", "Jia-Wei", ""], ["Jiao", "Licheng", ""], ["Wang", "Mi", ""]]}, {"id": "1906.07927", "submitter": "Xinchen Yan", "authors": "Haonan Qiu, Chaowei Xiao, Lei Yang, Xinchen Yan, Honglak Lee, Bo Li", "title": "SemanticAdv: Generating Adversarial Examples via Attribute-conditional\n  Image Editing", "comments": "To appear at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have achieved great success in various\napplications due to their strong expressive power. However, recent studies have\nshown that DNNs are vulnerable to adversarial examples which are manipulated\ninstances targeting to mislead DNNs to make incorrect predictions. Currently,\nmost such adversarial examples try to guarantee \"subtle perturbation\" by\nlimiting the $L_p$ norm of the perturbation. In this paper, we aim to explore\nthe impact of semantic manipulation on DNNs predictions by manipulating the\nsemantic attributes of images and generate \"unrestricted adversarial examples\".\n  In particular, we propose an algorithm \\emph{SemanticAdv} which leverages\ndisentangled semantic factors to generate adversarial perturbation by altering\ncontrolled semantic attributes to fool the learner towards various\n\"adversarial\" targets. We conduct extensive experiments to show that the\nsemantic based adversarial examples can not only fool different learning tasks\nsuch as face verification and landmark detection, but also achieve high\ntargeted attack success rate against \\emph{real-world black-box} services such\nas Azure face verification service based on transferability.\n  To further demonstrate the applicability of \\emph{SemanticAdv} beyond face\nrecognition domain, we also generate semantic perturbations on street-view\nimages. Such adversarial examples with controlled semantic manipulation can\nshed light on further understanding about vulnerabilities of DNNs as well as\npotential defensive approaches.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 05:55:16 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 07:32:08 GMT"}, {"version": "v3", "created": "Mon, 6 Apr 2020 17:34:07 GMT"}, {"version": "v4", "created": "Thu, 2 Jul 2020 19:47:47 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Qiu", "Haonan", ""], ["Xiao", "Chaowei", ""], ["Yang", "Lei", ""], ["Yan", "Xinchen", ""], ["Lee", "Honglak", ""], ["Li", "Bo", ""]]}, {"id": "1906.07930", "submitter": "Jia-Wei Chen", "authors": "Rongfang Wang, Jia-Wei Chen, Yule Wang, Licheng Jiao, Mi Wang", "title": "SAR Image Change Detection via Spatial Metric Learning with an Improved\n  Mahalanobis Distance", "comments": null, "journal-ref": null, "doi": "10.1109/LGRS.2019.2915251", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The log-ratio (LR) operator has been widely employed to generate the\ndifference image for synthetic aperture radar (SAR) image change detection.\nHowever, the difference image generated by this pixel-wise operator can be\nsubject to SAR images speckle and unavoidable registration errors between\nbitemporal SAR images. In this letter, we proposed a spatial metric learning\nmethod to obtain a difference image more robust to the speckle by learning a\nmetric from a set of constraint pairs. In the proposed method, spatial context\nis considered in constructing constraint pairs, each of which consists of\npatches in the same location of bitemporal SAR images. Then, a semi-definite\npositive metric matrix $\\bf M$ can be obtained by the optimization with the\nmax-margin criterion. Finally, we verify our proposed method on four\nchallenging datasets of bitemporal SAR images. Experimental results demonstrate\nthat the difference map obtained by our proposed method outperforms than other\nstate-of-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 06:10:58 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Wang", "Rongfang", ""], ["Chen", "Jia-Wei", ""], ["Wang", "Yule", ""], ["Jiao", "Licheng", ""], ["Wang", "Mi", ""]]}, {"id": "1906.07934", "submitter": "Yan Liu", "authors": "Yan Liu, Yun Li, Yunhao Yuan and jipeng qiang", "title": "A simple and effective postprocessing method for image classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whether it is computer vision, natural language processing or speech\nrecognition, the essence of these applications is to obtain powerful feature\nrepresentations that make downstream applications completion more efficient.\nTaking image recognition as an example, whether it is hand-crafted low-level\nfeature representation or feature representation extracted by a convolutional\nneural networks(CNNs), the goal is to extract features that better represent\nimage features, thereby improving classification accuracy. However, we observed\nthat image feature representations share a large common vector and a few top\ndominating directions. To address this problems, we propose a simple but\neffective postprocessing method to render off-the-shelf feature representations\neven stronger by eliminating the common mean vector from off-the-shelf feature\nrepresentations. The postprocessing is empirically validated on a variety of\ndatasets and feature extraction methods.such as VGG, LBP, and HOG. Some\nexperiments show that the features that have been post-processed by\npostprocessing algorithm can get better results than original ones.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 06:28:33 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Liu", "Yan", ""], ["Li", "Yun", ""], ["Yuan", "Yunhao", ""], ["qiang", "jipeng", ""]]}, {"id": "1906.07944", "submitter": "Mingjie Li", "authors": "Mingjie Li, Youqian Feng, Zhonghai Yin, Cheng Zhou, Fanghao Dong, Yuan\n  Lin, Yuhao Dong", "title": "An Action Recognition network for specific target based on rMC and RPN", "comments": "10 pages, 14 figures", "journal-ref": null, "doi": "10.1088/1742-6596/1325/1/012073", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional methods of action recognition are not specific for the\noperator, thus results are easy to be disturbed when other actions are operated\nin videos. The network based on mixed convolutional resnet and RPN is proposed\nin this paper. The rMC is tested in the data set of UCF-101 to compare with the\nmethod of R3D. The result shows that its correct rate reaches 71.07%.\nMeanwhile, the action recognition network is tested in our gesture and body\nposture data sets for specific target. The simulation achieves a good\nperformance in which the running speed reaches 200 FPS. Finally, our model is\nimproved by introducing the regression block and performs better, which shows\nthe great potential of this model.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 07:09:41 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Li", "Mingjie", ""], ["Feng", "Youqian", ""], ["Yin", "Zhonghai", ""], ["Zhou", "Cheng", ""], ["Dong", "Fanghao", ""], ["Lin", "Yuan", ""], ["Dong", "Yuhao", ""]]}, {"id": "1906.07947", "submitter": "Kun Zhan", "authors": "Changlu Chen, Chaoxi Niu, Xia Zhan, Kun Zhan", "title": "Generative approach to unsupervised deep local learning", "comments": null, "journal-ref": "Journal of Electronic Imaging, 2019", "doi": "10.1117/1.JEI.28.4.043005", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing feature learning methods optimize inflexible handcrafted\nfeatures and the affinity matrix is constructed by shallow linear embedding\nmethods. Different from these conventional methods, we pretrain a generative\nneural network by stacking convolutional autoencoders to learn the latent data\nrepresentation and then construct an affinity graph with them as a prior. Based\non the pretrained model and the constructed graph, we add a self-expressive\nlayer to complete the generative model and then fine-tune it with a new loss\nfunction, including the reconstruction loss and a deliberately defined\nlocality-preserving loss. The locality-preserving loss designed by the\nconstructed affinity graph serves as prior to preserve the local structure\nduring the fine-tuning stage, which in turn improves the quality of feature\nrepresentation effectively. Furthermore, the self-expressive layer between the\nencoder and decoder is based on the assumption that each latent feature is a\nlinear combination of other latent features, so the weighted combination\ncoefficients of the self-expressive layer are used to construct a new refined\naffinity graph for representing the data structure. We conduct experiments on\nfour datasets to demonstrate the superiority of the representation ability of\nour proposed model over the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 07:19:06 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 07:54:52 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Chen", "Changlu", ""], ["Niu", "Chaoxi", ""], ["Zhan", "Xia", ""], ["Zhan", "Kun", ""]]}, {"id": "1906.07981", "submitter": "Shouvik Mani", "authors": "Shouvik Mani", "title": "Artistic Enhancement and Style Transfer of Image Edges using Directional\n  Pseudo-coloring", "comments": null, "journal-ref": "2nd Workshop on Humanizing AI (HAI), IJCAI 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing the gradient of an image is a common step in computer vision\npipelines. The image gradient quantifies the magnitude and direction of edges\nin an image and is used in creating features for downstream machine learning\ntasks. Typically, the image gradient is represented as a grayscale image. This\npaper introduces directional pseudo-coloring, an approach to color the image\ngradient in a deliberate and coherent manner. By pseudo-coloring the image\ngradient magnitude with the image gradient direction, we can enhance the visual\nquality of image edges and achieve an artistic transformation of the original\nimage. Additionally, we present a simple style transfer pipeline which learns a\ncolor map from a style image and then applies that color map to color the edges\nof a content image through the directional pseudo-coloring technique. Code for\nthe algorithms and experiments is available at\nhttps://github.com/shouvikmani/edge-colorizer.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 09:12:06 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Mani", "Shouvik", ""]]}, {"id": "1906.07997", "submitter": "Dou Goodman", "authors": "Dou Goodman and Tao Wei", "title": "Cloud-based Image Classification Service Is Not Robust To Simple\n  Transformations: A Forgotten Battlefield", "comments": "arXiv admin note: text overlap with arXiv:1901.01223,\n  arXiv:1704.05051, arXiv:1801.02612 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent works demonstrated that Deep Learning models are vulnerable to\nadversarial examples.Fortunately, generating adversarial examples usually\nrequires white-box access to the victim model, and the attacker can only access\nthe APIs opened by cloud platforms. Thus, keeping models in the cloud can\nusually give a (false) sense of security.Unfortunately, cloud-based image\nclassification service is not robust to simple transformations such as Gaussian\nNoise, Salt-and-Pepper Noise, Rotation and Monochromatization. In this\npaper,(1) we propose one novel attack method called Image Fusion(IF) attack,\nwhich achieve a high bypass rate,can be implemented only with OpenCV and is\ndifficult to defend; and (2) we make the first attempt to conduct an extensive\nempirical study of Simple Transformation (ST) attacks against real-world\ncloud-based classification services. Through evaluations on four popular cloud\nplatforms including Amazon, Google, Microsoft, Clarifai, we demonstrate that ST\nattack has a success rate of approximately 100% except Amazon approximately\n50%, IF attack have a success rate over 98% among different classification\nservices. (3) We discuss the possible defenses to address these security\nchallenges.Experiments show that our defense technology can effectively defend\nknown ST attacks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 09:36:17 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 06:03:09 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Goodman", "Dou", ""], ["Wei", "Tao", ""]]}, {"id": "1906.08019", "submitter": "Klemen Isteni\\v{c}", "authors": "Klemen Istenic, Nuno Gracias, Aurelien Arnaubec, Javier Escartin and\n  Rafael Garcia", "title": "Automatic Scale Estimation of Structure from Motion based 3D Models\n  using Laser Scalers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in structure-from-motion techniques are enabling many\nscientific fields to benefit from the routine creation of detailed 3D models.\nHowever, for a large number of applications, only a single camera is available,\ndue to cost or space constraints in the survey platforms. Monocular\nstructure-from-motion raises the issue of properly estimating the scale of the\n3D models, in order to later use those models for metrology. The scale can be\ndetermined from the presence of visible objects of known dimensions, or from\ninformation on the magnitude of the camera motion provided by other sensors,\nsuch as GPS.\n  This paper addresses the problem of accurately scaling 3D models created from\nmonocular cameras in GPS-denied environments, such as in underwater\napplications. Motivated by the common availability of underwater laser scalers,\nwe present two novel approaches. A fully-calibrated method enables the use of\narbitrary laser setups, while a partially-calibrated method reduces the need\nfor calibration by only assuming parallelism on the laser beams, with no\nconstraints on the camera. The proposed methods have several advantages with\nrespect to the existing methods. The need for laser alignment with the optical\naxis of the camera is removed, together with the extremely error-prone manual\nidentification of image points on the 3D model.\n  The performance of the methods and their applicability was evaluated on both\ndata generated from a realistic 3D model and data collected during an\noceanographic cruise in 2017. Three separate laser configurations have been\ntested, encompassing nearly all possible laser setups, to evaluate the effects\nof terrain roughness, noise, camera perspective angle and camera-scene\ndistance. In the real scenario, the computation of 6 independent model scale\nestimates using our fully-calibrated approach, produced values with standard\ndeviation of 0.3%.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 10:52:54 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Istenic", "Klemen", ""], ["Gracias", "Nuno", ""], ["Arnaubec", "Aurelien", ""], ["Escartin", "Javier", ""], ["Garcia", "Rafael", ""]]}, {"id": "1906.08031", "submitter": "Niv Nayman", "authors": "Niv Nayman, Asaf Noy, Tal Ridnik, Itamar Friedman, Rong Jin, Lihi\n  Zelnik-Manor", "title": "XNAS: Neural Architecture Search with Expert Advice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel optimization method for differential neural\narchitecture search, based on the theory of prediction with expert advice. Its\noptimization criterion is well fitted for an architecture-selection, i.e., it\nminimizes the regret incurred by a sub-optimal selection of operations. Unlike\nprevious search relaxations, that require hard pruning of architectures, our\nmethod is designed to dynamically wipe out inferior architectures and enhance\nsuperior ones. It achieves an optimal worst-case regret bound and suggests the\nuse of multiple learning-rates, based on the amount of information carried by\nthe backward gradients. Experiments show that our algorithm achieves a strong\nperformance over several image classification datasets. Specifically, it\nobtains an error rate of 1.6% for CIFAR-10, 24% for ImageNet under mobile\nsettings, and achieves state-of-the-art results on three additional datasets.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 12:00:00 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Nayman", "Niv", ""], ["Noy", "Asaf", ""], ["Ridnik", "Tal", ""], ["Friedman", "Itamar", ""], ["Jin", "Rong", ""], ["Zelnik-Manor", "Lihi", ""]]}, {"id": "1906.08070", "submitter": "Eskil J\\\"orgensen", "authors": "Eskil J\\\"orgensen, Christopher Zach, Fredrik Kahl", "title": "Monocular 3D Object Detection and Box Fitting Trained End-to-End Using\n  Intersection-over-Union Loss", "comments": "For associated videos file, see http://tinyurl.com/SS3D-YouTube ;\n  without supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional object detection from a single view is a challenging task\nwhich, if performed with good accuracy, is an important enabler of low-cost\nmobile robot perception. Previous approaches to this problem suffer either from\nan overly complex inference engine or from an insufficient detection accuracy.\nTo deal with these issues, we present SS3D, a single-stage monocular 3D object\ndetector. The framework consists of (i) a CNN, which outputs a redundant\nrepresentation of each relevant object in the image with corresponding\nuncertainty estimates, and (ii) a 3D bounding box optimizer. We show how\nmodeling heteroscedastic uncertainty improves performance upon our baseline,\nand furthermore, how back-propagation can be done through the optimizer in\norder to train the pipeline end-to-end for additional accuracy. Our method\nachieves SOTA accuracy on monocular 3D object detection, while running at 20\nfps in a straightforward implementation. We argue that the SS3D architecture\nprovides a solid framework upon which high performing detection systems can be\nbuilt, with autonomous driving being the main application in mind.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 12:39:16 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 09:26:37 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["J\u00f6rgensen", "Eskil", ""], ["Zach", "Christopher", ""], ["Kahl", "Fredrik", ""]]}, {"id": "1906.08095", "submitter": "Guangyao Zhai", "authors": "Guangyao Zhai, Liang Liu, Linjian Zhang, Yong Liu", "title": "PoseConvGRU: A Monocular Approach for Visual Ego-motion Estimation by\n  Learning", "comments": "33 pages,12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many visual ego-motion algorithm variants have been proposed in the\npast decade, learning based ego-motion estimation methods have seen an\nincreasing attention because of its desirable properties of robustness to image\nnoise and camera calibration independence. In this work, we propose a\ndata-driven approach of fully trainable visual ego-motion estimation for a\nmonocular camera. We use an end-to-end learning approach in allowing the model\nto map directly from input image pairs to an estimate of ego-motion\n(parameterized as 6-DoF transformation matrices). We introduce a novel\ntwo-module Long-term Recurrent Convolutional Neural Networks called\nPoseConvGRU, with an explicit sequence pose estimation loss to achieve this.\nThe feature-encoding module encodes the short-term motion feature in an image\npair, while the memory-propagating module captures the long-term motion feature\nin the consecutive image pairs. The visual memory is implemented with\nconvolutional gated recurrent units, which allows propagating information over\ntime. At each time step, two consecutive RGB images are stacked together to\nform a 6 channels tensor for module-1 to learn how to extract motion\ninformation and estimate poses. The sequence of output maps is then passed\nthrough a stacked ConvGRU module to generate the relative transformation pose\nof each image pair. We also augment the training data by randomly skipping\nframes to simulate the velocity variation which results in a better performance\nin turning and high-velocity situations. We evaluate the performance of our\nproposed approach on the KITTI Visual Odometry benchmark. The experiments show\na competitive performance of the proposed method to the geometric method and\nencourage further exploration of learning based methods for the purpose of\nestimating camera ego-motion even though geometrical methods demonstrate\npromising results.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 13:42:35 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Zhai", "Guangyao", ""], ["Liu", "Liang", ""], ["Zhang", "Linjian", ""], ["Liu", "Yong", ""]]}, {"id": "1906.08143", "submitter": "Jing Cheng", "authors": "Jing Cheng, Haifeng Wang, Yanjie Zhu, Qiegen Liu, Qiyang Zhang, Ting\n  Su, Jianwei Chen, Yongshuai Ge, Zhanli Hu, Xin Liu, Hairong Zheng, Leslie\n  Ying, Dong Liang", "title": "Model-based Deep Medical Imaging: the roadmap of generalizing iterative\n  reconstruction model using deep learning", "comments": "part of the preliminary work will be presented at MICCAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP math.OC physics.med-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical imaging is playing a more and more important role in clinics.\nHowever, there are several issues in different imaging modalities such as slow\nimaging speed in MRI, radiation injury in CT and PET. Therefore, accelerating\nMRI, reducing radiation dose in CT and PET have been ongoing research topics\nsince their invention. Usually, acquiring less data is a direct but important\nstrategy to address these issues. However, less acquisition usually results in\naliasing artifacts in reconstructions. Recently, deep learning (DL) has been\nintroduced in medical image reconstruction and shown potential on significantly\nspeeding up MR reconstruction and reducing radiation dose. In this paper, we\npropose a general framework on combining the reconstruction model with deep\nlearning to maximize the potential of deep learning and model-based\nreconstruction, and give the examples to demonstrate the performance and\nrequirements of unrolling different algorithms using deep learning.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 15:10:59 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 01:33:27 GMT"}, {"version": "v3", "created": "Sun, 23 Jun 2019 08:56:26 GMT"}, {"version": "v4", "created": "Thu, 5 Sep 2019 15:42:47 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Cheng", "Jing", ""], ["Wang", "Haifeng", ""], ["Zhu", "Yanjie", ""], ["Liu", "Qiegen", ""], ["Zhang", "Qiyang", ""], ["Su", "Ting", ""], ["Chen", "Jianwei", ""], ["Ge", "Yongshuai", ""], ["Hu", "Zhanli", ""], ["Liu", "Xin", ""], ["Zheng", "Hairong", ""], ["Ying", "Leslie", ""], ["Liang", "Dong", ""]]}, {"id": "1906.08200", "submitter": "Michele Piana", "authors": "Francesco Fiz, Helmut Dittmann, Cristina Campi, Matthias Weissinger,\n  Samine Sahbai, Matthias Reimold, Arnulf Stenzl, Michele Piana, Gianmario\n  Sambuceti, Christian la Foug\\`ere", "title": "Automated Definition of Skeletal Disease Burden in Metastatic Prostate\n  Carcinoma: a 3D analysis of SPECT/CT images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.TO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To meet the current need for skeletal tumor-load estimation in prostate\ncancer (mCRPC), we developed a novel approach, based on adaptive bone\nsegmentation. In this study, we compared the program output with existing\nestimates and with the radiological outcome. Seventy-six whole-body\n99mTc-DPD-SPECT/CT from mCRPC patients were analyzed. The software identified\nthe whole skeletal volume (SVol) and classified it voxels metastases (MVol) or\nnormal bone (BVol). SVol was compared with the estimation of a commercial\nsoftware. MVol was compared with manual assessment and with PSA-level.\nCounts/voxel were extracted from MVol and BVol. After six cycles of\n223RaCl2-therapy every patient was re-evaluated as progressing (PD), stabilized\n(SD) or responsive (PR). SVol correlated with the one of the commercial\nsoftware (R=0,99, p<0,001). MVol correlated with manually-counted lesions\n(R=0,61, p<0,001) and PSA (R=0,46, p<0.01). PD had a lower counts/voxel in MVol\nthan PR/SD (715 \\pm 190 Vs. 975 \\pm 215 and 1058 \\pm 255, p<0,05 and p<0,01)\nand in BVol (PD 275 \\pm 60, PR 515 \\pm 188 and SD 528 \\pm 162 counts/voxel,\np<0,001). Segmentation-based tumor load correlated with radiological/laboratory\nindices. Uptake was linked with the clinical outcome, suggesting that\nmetastases in PD patients have a lower affinity for bone-seeking radionuclides\nand might benefit less from bone-targeted radioisotope therapies.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 16:23:09 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Fiz", "Francesco", ""], ["Dittmann", "Helmut", ""], ["Campi", "Cristina", ""], ["Weissinger", "Matthias", ""], ["Sahbai", "Samine", ""], ["Reimold", "Matthias", ""], ["Stenzl", "Arnulf", ""], ["Piana", "Michele", ""], ["Sambuceti", "Gianmario", ""], ["la Foug\u00e8re", "Christian", ""]]}, {"id": "1906.08236", "submitter": "Lerrel Pinto Mr", "authors": "Adithyavairavan Murali, Tao Chen, Kalyan Vasudev Alwala, Dhiraj\n  Gandhi, Lerrel Pinto, Saurabh Gupta, Abhinav Gupta", "title": "PyRobot: An Open-source Robotics Framework for Research and Benchmarking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces PyRobot, an open-source robotics framework for research\nand benchmarking. PyRobot is a light-weight, high-level interface on top of ROS\nthat provides a consistent set of hardware independent mid-level APIs to\ncontrol different robots. PyRobot abstracts away details about low-level\ncontrollers and inter-process communication, and allows non-robotics\nresearchers (ML, CV researchers) to focus on building high-level AI\napplications. PyRobot aims to provide a research ecosystem with convenient\naccess to robotics datasets, algorithm implementations and models that can be\nused to quickly create a state-of-the-art baseline. We believe PyRobot, when\npaired up with low-cost robot platforms such as LoCoBot, will reduce the entry\nbarrier into robotics, and democratize robotics. PyRobot is open-source, and\ncan be accessed via https://pyrobot.org.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 17:35:43 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Murali", "Adithyavairavan", ""], ["Chen", "Tao", ""], ["Alwala", "Kalyan Vasudev", ""], ["Gandhi", "Dhiraj", ""], ["Pinto", "Lerrel", ""], ["Gupta", "Saurabh", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1906.08240", "submitter": "Kara-Ali Aliev", "authors": "Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry Ulyanov,\n  Victor Lempitsky", "title": "Neural Point-Based Graphics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new point-based approach for modeling the appearance of real\nscenes. The approach uses a raw point cloud as the geometric representation of\na scene, and augments each point with a learnable neural descriptor that\nencodes local geometry and appearance. A deep rendering network is learned in\nparallel with the descriptors, so that new views of the scene can be obtained\nby passing the rasterizations of a point cloud from new viewpoints through this\nnetwork. The input rasterizations use the learned descriptors as point\npseudo-colors. We show that the proposed approach can be used for modeling\ncomplex scenes and obtaining their photorealistic views, while avoiding\nexplicit surface estimation and meshing. In particular, compelling results are\nobtained for scene scanned using hand-held commodity RGB-D sensors as well as\nstandard RGB cameras even in the presence of objects that are challenging for\nstandard mesh-based modeling.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 17:38:45 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 19:07:00 GMT"}, {"version": "v3", "created": "Sun, 5 Apr 2020 21:38:47 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Aliev", "Kara-Ali", ""], ["Sevastopolsky", "Artem", ""], ["Kolos", "Maria", ""], ["Ulyanov", "Dmitry", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1906.08255", "submitter": "Christopher Geier", "authors": "Christopher Geier", "title": "Training on test data: Removing near duplicates in Fashion-MNIST", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  MNIST and Fashion MNIST are extremely popular for testing in the machine\nlearning space. Fashion MNIST improves on MNIST by introducing a harder\nproblem, increasing the diversity of testing sets, and more accurately\nrepresenting a modern computer vision task. In order to increase the data\nquality of FashionMNIST, this paper investigates near duplicate images between\ntraining and testing sets. Near-duplicates between testing and training sets\nartificially increase the testing accuracy of machine learning models. This\npaper identifies near-duplicate images in Fashion MNIST and proposes a dataset\nwith near-duplicates removed.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 22:09:47 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Geier", "Christopher", ""]]}, {"id": "1906.08305", "submitter": "Hsin-Pai Cheng", "authors": "Hsin-Pai Cheng, Tunhou Zhang, Yukun Yang, Feng Yan, Shiyu Li, Harris\n  Teague, Hai Li, and Yiran Chen", "title": "SwiftNet: Using Graph Propagation as Meta-knowledge to Search Highly\n  Representative Neural Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Designing neural architectures for edge devices is subject to constraints of\naccuracy, inference latency, and computational cost. Traditionally, researchers\nmanually craft deep neural networks to meet the needs of mobile devices. Neural\nArchitecture Search (NAS) was proposed to automate the neural architecture\ndesign without requiring extensive domain expertise and significant manual\nefforts. Recent works utilized NAS to design mobile models by taking into\naccount hardware constraints and achieved state-of-the-art accuracy with fewer\nparameters and less computational cost measured in Multiply-accumulates (MACs).\nTo find highly compact neural architectures, existing works relies on\npredefined cells and directly applying width multiplier, which may potentially\nlimit the model flexibility, reduce the useful feature map information, and\ncause accuracy drop. To conquer this issue, we propose GRAM(GRAph propagation\nas Meta-knowledge) that adopts fine-grained (node-wise) search method and\naccumulates the knowledge learned in updates into a meta-graph. As a result,\nGRAM can enable more flexible search space and achieve higher search\nefficiency. Without the constraints of predefined cell or blocks, we propose a\nnew structure-level pruning method to remove redundant operations in neural\narchitectures. SwiftNet, which is a set of models discovered by GRAM,\noutperforms MobileNet-V2 by 2.15x higher accuracy density and 2.42x faster with\nsimilar accuracy. Compared with FBNet, SwiftNet reduces the search cost by 26x\nand achieves 2.35x higher accuracy density and 1.47x speedup while preserving\nsimilar accuracy. SwiftNetcan obtain 63.28% top-1 accuracy on ImageNet-1K with\nonly 53M MACs and 2.07M parameters. The corresponding inference latency is only\n19.09 ms on Google Pixel 1.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 19:00:12 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 23:33:24 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Cheng", "Hsin-Pai", ""], ["Zhang", "Tunhou", ""], ["Yang", "Yukun", ""], ["Yan", "Feng", ""], ["Li", "Shiyu", ""], ["Teague", "Harris", ""], ["Li", "Hai", ""], ["Chen", "Yiran", ""]]}, {"id": "1906.08331", "submitter": "Jun Zhang", "authors": "Jun Zhang, Yamei Liu, Shengping Zhang, Ronald Poppe, Meng Wang", "title": "Light Field Saliency Detection with Deep Convolutional Networks", "comments": "14 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field imaging presents an attractive alternative to RGB imaging because\nof the recording of the direction of the incoming light. The detection of\nsalient regions in a light field image benefits from the additional modeling of\nangular patterns. For RGB imaging, methods using CNNs have achieved excellent\nresults on a range of tasks, including saliency detection. However, it is not\ntrivial to use CNN-based methods for saliency detection on light field images\nbecause these methods are not specifically designed for processing light field\ninputs. In addition, current light field datasets are not sufficiently large to\ntrain CNNs. To overcome these issues, we present a new Lytro Illum dataset,\nwhich contains 640 light fields and their corresponding ground-truth saliency\nmaps. Compared to current light field saliency datasets [1], [2], our new\ndataset is larger, of higher quality, contains more variation and more types of\nlight field inputs. This makes our dataset suitable for training deeper\nnetworks and benchmarking. Furthermore, we propose a novel end-to-end CNN-based\nframework for light field saliency detection. Specifically, we propose three\nnovel MAC (Model Angular Changes) blocks to process light field micro-lens\nimages. We systematically study the impact of different architecture variants\nand compare light field saliency with regular 2D saliency. Our extensive\ncomparisons indicate that our novel network significantly outperforms\nstate-of-the-art methods on the proposed dataset and has desired generalization\nabilities on other existing datasets.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 20:03:49 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 09:17:08 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Zhang", "Jun", ""], ["Liu", "Yamei", ""], ["Zhang", "Shengping", ""], ["Poppe", "Ronald", ""], ["Wang", "Meng", ""]]}, {"id": "1906.08332", "submitter": "Hao Luo", "authors": "Hao Luo, Wei Jiang, Youzhi Gu, Fuxu Liu, Xingyu Liao, Shenqi Lai,\n  Jianyang Gu", "title": "A Strong Baseline and Batch Normalization Neck for Deep Person\n  Re-identification", "comments": "Accepted by IEEE Transactions on Multimedia. This is the submitted\n  journal version of the oral paper [arXiv:1903.07071] in CVPRW'19. Code are\n  avaliable at: https://github.com/michuanhaohao/reid-strong-baseline", "journal-ref": null, "doi": "10.1109/TMM.2019.2958756", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study explores a simple but strong baseline for person re-identification\n(ReID). Person ReID with deep neural networks has progressed and achieved high\nperformance in recent years. However, many state-of-the-art methods design\ncomplex network structures and concatenate multi-branch features. In the\nliterature, some effective training tricks briefly appear in several papers or\nsource codes. The present study collects and evaluates these effective training\ntricks in person ReID. By combining these tricks, the model achieves 94.5%\nrank-1 and 85.9% mean average precision on Market1501 with only using the\nglobal features of ResNet50. The performance surpasses all existing global- and\npart-based baselines in person ReID. We propose a novel neck structure named as\nbatch normalization neck (BNNeck). BNNeck adds a batch normalization layer\nafter global pooling layer to separate metric and classification losses into\ntwo different feature spaces because we observe they are inconsistent in one\nembedding space. Extended experiments show that BNNeck can boost the baseline,\nand our baseline can improve the performance of existing state-of-the-art\nmethods. Our codes and models are available at:\nhttps://github.com/michuanhaohao/reid-strong-baseline.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 20:12:20 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 10:25:19 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Luo", "Hao", ""], ["Jiang", "Wei", ""], ["Gu", "Youzhi", ""], ["Liu", "Fuxu", ""], ["Liao", "Xingyu", ""], ["Lai", "Shenqi", ""], ["Gu", "Jianyang", ""]]}, {"id": "1906.08380", "submitter": "Claudio Zito", "authors": "Claudio Zito, Tomasz Deregowski and Rustam Stolkin", "title": "2D Linear Time-Variant Controller for Human's Intention Detection for\n  Reach-to-Grasp Trajectories in Novel Scenes", "comments": null, "journal-ref": "In Proc. of the Workshop on Task-Informed Grasping (TIG-II): From\n  Perception to Physical Interaction, Robotics: Science and Systems (RSS), 2019", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Designing robotic assistance devices for manipulation tasks is challenging.\nThis work is concerned with improving accuracy and usability of semi-autonomous\nrobots, such as human operated manipulators or exoskeletons. The key insight is\nto develop a system that takes into account context- and user-awareness to take\nbetter decisions in how to assist the user. The context-awareness is\nimplemented by enabling the system to automatically generate a set of candidate\ngrasps and reach-to-grasp trajectories in novel, cluttered scenes. The\nuser-awareness is implemented as a linear time-variant feedback controller to\nfacilitate the motion towards the most promising grasp. Our approach is\ndemonstrated in a simple 2D example in which participants are asked to grasp a\nspecific object in a clutter scene. Our approach also reduce the number of\ncontrollable dimensions for the user by providing only control on x- and\ny-axis, while orientation of the end-effector and the pose of its fingers are\ninferred by the system. The experimental results show the benefits of our\napproach in terms of accuracy and execution time with respect to a pure manual\ncontrol.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 22:01:36 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 19:04:15 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Zito", "Claudio", ""], ["Deregowski", "Tomasz", ""], ["Stolkin", "Rustam", ""]]}, {"id": "1906.08416", "submitter": "Emin Orhan", "authors": "A. Emin Orhan, Brenden M. Lake", "title": "Improving the robustness of ImageNet classifiers using elements of human\n  visual cognition", "comments": "v2 involves reformating and stylistic changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the robustness properties of image recognition models equipped\nwith two features inspired by human vision, an explicit episodic memory and a\nshape bias, at the ImageNet scale. As reported in previous work, we show that\nan explicit episodic memory improves the robustness of image recognition models\nagainst small-norm adversarial perturbations under some threat models. It does\nnot, however, improve the robustness against more natural, and typically\nlarger, perturbations. Learning more robust features during training appears to\nbe necessary for robustness in this second sense. We show that features derived\nfrom a model that was encouraged to learn global, shape-based representations\n(Geirhos et al., 2019) do not only improve the robustness against natural\nperturbations, but when used in conjunction with an episodic memory, they also\nprovide additional robustness against adversarial perturbations. Finally, we\naddress three important design choices for the episodic memory: memory size,\ndimensionality of the memories and the retrieval method. We show that to make\nthe episodic memory more compact, it is preferable to reduce the number of\nmemories by clustering them, instead of reducing their dimensionality.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 02:28:19 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 16:29:21 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Orhan", "A. Emin", ""], ["Lake", "Brenden M.", ""]]}, {"id": "1906.08430", "submitter": "Gabriel Grand", "authors": "Gabriel Grand and Yonatan Belinkov", "title": "Adversarial Regularization for Visual Question Answering: Strengths,\n  Shortcomings, and Side Effects", "comments": "In Proceedings of the 2nd Workshop on Shortcomings in Vision and\n  Language (SiVL) at NAACL-HLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (VQA) models have been shown to over-rely on\nlinguistic biases in VQA datasets, answering questions \"blindly\" without\nconsidering visual context. Adversarial regularization (AdvReg) aims to address\nthis issue via an adversary sub-network that encourages the main model to learn\na bias-free representation of the question. In this work, we investigate the\nstrengths and shortcomings of AdvReg with the goal of better understanding how\nit affects inference in VQA models. Despite achieving a new state-of-the-art on\nVQA-CP, we find that AdvReg yields several undesirable side-effects, including\nunstable gradients and sharply reduced performance on in-domain examples. We\ndemonstrate that gradual introduction of regularization during training helps\nto alleviate, but not completely solve, these issues. Through error analyses,\nwe observe that AdvReg improves generalization to binary questions, but impairs\nperformance on questions with heterogeneous answer distributions.\nQualitatively, we also find that regularized models tend to over-rely on visual\nfeatures, while ignoring important linguistic cues in the question. Our results\nsuggest that AdvReg requires further refinement before it can be considered a\nviable bias mitigation technique for VQA.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 03:28:09 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Grand", "Gabriel", ""], ["Belinkov", "Yonatan", ""]]}, {"id": "1906.08462", "submitter": "Chongyi Li", "authors": "Chongyi Li, Runmin Cong, Junhui Hou, Sanyi Zhang, Yue Qian, Sam Kwong", "title": "Nested Network with Two-Stream Pyramid for Salient Object Detection in\n  Optical Remote Sensing Images", "comments": "11 pages, 8 figures, has been accepted by TGRS", "journal-ref": null, "doi": "10.1109/TGRS.2019.2925070", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arising from the various object types and scales, diverse imaging\norientations, and cluttered backgrounds in optical remote sensing image (RSI),\nit is difficult to directly extend the success of salient object detection for\nnature scene image to the optical RSI. In this paper, we propose an end-to-end\ndeep network called LV-Net based on the shape of network architecture, which\ndetects salient objects from optical RSIs in a purely data-driven fashion. The\nproposed LV-Net consists of two key modules, i.e., a two-stream pyramid module\n(L-shaped module) and an encoder-decoder module with nested connections\n(V-shaped module). Specifically, the L-shaped module extracts a set of\ncomplementary information hierarchically by using a two-stream pyramid\nstructure, which is beneficial to perceiving the diverse scales and local\ndetails of salient objects. The V-shaped module gradually integrates encoder\ndetail features with decoder semantic features through nested connections,\nwhich aims at suppressing the cluttered backgrounds and highlighting the\nsalient objects. In addition, we construct the first publicly available optical\nRSI dataset for salient object detection, including 800 images with varying\nspatial resolutions, diverse saliency types, and pixel-wise ground truth.\nExperiments on this benchmark dataset demonstrate that the proposed method\noutperforms the state-of-the-art salient object detection methods both\nqualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 06:57:13 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Li", "Chongyi", ""], ["Cong", "Runmin", ""], ["Hou", "Junhui", ""], ["Zhang", "Sanyi", ""], ["Qian", "Yue", ""], ["Kwong", "Sam", ""]]}, {"id": "1906.08467", "submitter": "Wei Hong", "authors": "Wei Hong and Jin ke Yu Fan Zong", "title": "GAN-Knowledge Distillation for one-stage Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have a significant improvement in the accuracy\nof Object detection. As convolutional neural networks become deeper, the\naccuracy of detection is also obviously improved, and more floating-point\ncalculations are needed. Many researchers use the knowledge distillation method\nto improve the accuracy of student networks by transferring knowledge from a\ndeeper and larger teachers network to a small student network, in object\ndetection. Most methods of knowledge distillation need to designed complex cost\nfunctions and they are aimed at the two-stage object detection algorithm. This\npaper proposes a clean and effective knowledge distillation method for the\none-stage object detection. The feature maps generated by teacher network and\nstudent network are used as true samples and fake samples respectively, and\ngenerate adversarial training for both to improve the performance of the\nstudent network in one-stage object detection.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 07:10:06 GMT"}, {"version": "v2", "created": "Sat, 22 Jun 2019 10:15:06 GMT"}, {"version": "v3", "created": "Wed, 26 Jun 2019 12:22:00 GMT"}, {"version": "v4", "created": "Thu, 4 Jul 2019 02:44:51 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Hong", "Wei", ""], ["Zong", "Jin ke Yu Fan", ""]]}, {"id": "1906.08469", "submitter": "Nemanja Djuric", "authors": "Fang-Chieh Chou, Tsung-Han Lin, Henggang Cui, Vladan Radosavljevic,\n  Thi Nguyen, Tzu-Kuo Huang, Matthew Niedoba, Jeff Schneider, Nemanja Djuric", "title": "Predicting Motion of Vulnerable Road Users using High-Definition Maps\n  and Efficient ConvNets", "comments": "Accepted for publication at IEEE Intelligent Vehicles Symposium (IV)\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following detection and tracking of traffic actors, prediction of their\nfuture motion is the next critical component of a self-driving vehicle (SDV)\ntechnology, allowing the SDV to operate safely and efficiently in its\nenvironment. This is particularly important when it comes to vulnerable road\nusers (VRUs), such as pedestrians and bicyclists. These actors need to be\nhandled with special care due to an increased risk of injury, as well as the\nfact that their behavior is less predictable than that of motorized actors. To\naddress this issue, in the current study we present a deep learning-based\nmethod for predicting VRU movement, where we rasterize high-definition maps and\nactor's surroundings into a bird's-eye view image used as an input to deep\nconvolutional networks. In addition, we propose a fast architecture suitable\nfor real-time inference, and perform an ablation study of various rasterization\napproaches to find the optimal choice for accurate prediction. The results\nstrongly indicate benefits of using the proposed approach for motion prediction\nof VRUs, both in terms of accuracy and latency.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 07:16:16 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 06:54:12 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Chou", "Fang-Chieh", ""], ["Lin", "Tsung-Han", ""], ["Cui", "Henggang", ""], ["Radosavljevic", "Vladan", ""], ["Nguyen", "Thi", ""], ["Huang", "Tzu-Kuo", ""], ["Niedoba", "Matthew", ""], ["Schneider", "Jeff", ""], ["Djuric", "Nemanja", ""]]}, {"id": "1906.08476", "submitter": "Rongren Wu", "authors": "Jonathan Li, Rongren Wu, Yiping Chen, Qing Zhu, Zhipeng Luo, Cheng\n  Wang", "title": "PointNLM: Point Nonlocal-Means for vegetation segmentation based on\n  middle echo point clouds", "comments": "There are some syntax errors that need to be fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Middle-echo, which covers one or a few corresponding points, is a specific\ntype of 3D point cloud acquired by a multi-echo laser scanner. In this paper,\nwe propose a novel approach for automatic segmentation of trees that leverages\nmiddle-echo information from LiDAR point clouds. First, using a convolution\nclassification method, the proposed type of point clouds reflected by the\nmiddle echoes are identified from all point clouds. The middle-echo point\nclouds are distinguished from the first and last echoes. Hence, the crown\npositions of the trees are quickly detected from the huge number of point\nclouds. Second, to accurately extract trees from all point clouds, we propose a\n3D deep learning network, PointNLM, to semantically segment tree crowns.\nPointNLM captures the long-range relationship between the point clouds via a\nnon-local branch and extracts high-level features via max-pooling applied to\nunordered points. The whole framework is evaluated using the Semantic 3D\nreduced-test set. The IoU of tree point cloud segmentation reached 0.864. In\naddition, the semantic segmentation network was tested using the Paris-Lille-3D\ndataset. The average IoU outperformed several other popular methods. The\nexperimental results indicate that the proposed algorithm provides an excellent\nsolution for vegetation segmentation from LiDAR point clouds.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 07:34:05 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 04:50:13 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Li", "Jonathan", ""], ["Wu", "Rongren", ""], ["Chen", "Yiping", ""], ["Zhu", "Qing", ""], ["Luo", "Zhipeng", ""], ["Wang", "Cheng", ""]]}, {"id": "1906.08477", "submitter": "Jingwei Song", "authors": "Jingwei Song, Fang Bai, Liang Zhao, Shoudong Huang and Rong Xiong", "title": "Efficient two step optimization for large embedded deformation graph\n  based SLAM", "comments": "This work is accepted by ICRA2020 (2020 International Conference on\n  Robotics and Automation) 7 pages 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedded deformation nodes based formulation has been widely applied in\ndeformable geometry and graphical problems. Though being promising in stereo\n(or RGBD) sensor based SLAM applications, it remains challenging to keep\nconstant speed in deformation nodes parameter estimation when model grows\nlarger. In practice, the processing time grows rapidly in accordance with the\nexpansion of maps. In this paper, we propose an approach to decouple nodes of\ndeformation graph in large scale dense deformable SLAM and keep the estimation\ntime to be constant. We observe that only partial deformable nodes in the graph\nare connected to visible points. Based on this fact, sparsity of original\nHessian matrix is utilized to split parameter estimation in two independent\nsteps. With this new technique, we achieve faster parameter estimation with\namortized computation complexity reduced from O(n^2) to closing O(1). As a\nresult, the computation cost barely increases as the map keeps growing. Based\non our strategy, computational bottleneck in large scale embedded deformation\ngraph based applications will be greatly mitigated. The effectiveness is\nvalidated by experiments, featuring large scale deformation scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 07:36:16 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 03:30:20 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Song", "Jingwei", ""], ["Bai", "Fang", ""], ["Zhao", "Liang", ""], ["Huang", "Shoudong", ""], ["Xiong", "Rong", ""]]}, {"id": "1906.08501", "submitter": "Chengzhi Shi Mr", "authors": "Chengzhi Shi, Jihong Liu, Dali Chen", "title": "A Segmentation-Oriented Inter-Class Transfer Method: Application to\n  Retinal Vessel Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinal vessel segmentation, as a principal nonintrusive diagnose method for\nophthalmology diseases or diabetics, suffers from data scarcity due to\nrequiring pixel-wise labels. In this paper, we proposed a convenient\npatch-based two-stage transfer method. First, based on the information\nbottleneck theory, we insert one dimensionality-reduced layer for task-specific\nfeature space. Next, the semi-supervised clustering is conducted to select\ninstances, from different sources databases, possessing similarities in the\nfeature space. Surprisingly, we empirically demonstrate that images from\ndifferent classes possessing similarities contribute to better performance than\nsome same-class instances. The proposed framework achieved an accuracy of 97%,\n96.8%, and 96.77% on DRIVE, STARE, and HRF respectively, outperforming current\nmethods and independent human observers (DRIVE (96.37%) and STARE (93.39%)).\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 08:39:38 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Shi", "Chengzhi", ""], ["Liu", "Jihong", ""], ["Chen", "Dali", ""]]}, {"id": "1906.08507", "submitter": "Jerone Andrews", "authors": "Jerone T. A. Andrews, Thomas Tanay, Lewis D. Griffin", "title": "Multiple-Identity Image Attacks Against Face-based Identity Verification", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial verification systems are vulnerable to poisoning attacks that make use\nof multiple-identity images (MIIs)---face images stored in a database that\nresemble multiple persons, such that novel images of any of the constituent\npersons are verified as matching the identity of the MII. Research on this mode\nof attack has focused on defence by detection, with no explanation as to why\nthe vulnerability exists. New quantitative results are presented that support\nan explanation in terms of the geometry of the representations spaces used by\nthe verification systems. In the spherical geometry of those spaces, the\nangular distance distributions of matching and non-matching pairs of face\nrepresentations are only modestly separated, approximately centred at 90 and\n40-60 degrees, respectively. This is sufficient for open-set verification on\nnormal data but provides an opportunity for MII attacks. Our analysis considers\nideal MII algorithms, demonstrating that, if realisable, they would deliver\nfaces roughly 45 degrees from their constituent faces, thus classed as matching\nthem. We study the performance of three methods for MII generation---gallery\nsearch, image space morphing, and representation space inversion---and show\nthat the latter two realise the ideal well enough to produce effective attacks,\nwhile the former could succeed but only with an implausibly large gallery to\nsearch. Gallery search and inversion MIIs depend on having access to a facial\ncomparator, for optimisation, but our results show that these attacks can still\nbe effective when attacking disparate comparators, thus securing a deployed\ncomparator is an insufficient defence.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 08:58:22 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Andrews", "Jerone T. A.", ""], ["Tanay", "Thomas", ""], ["Griffin", "Lewis D.", ""]]}, {"id": "1906.08511", "submitter": "Jingjing Li", "authors": "Jingjing Li, Mengmeng Jing, Ke Lu, Lei Zhu, Yang Yang, and Zi Huang", "title": "From Zero-Shot Learning to Cold-Start Recommendation", "comments": "Accepted to AAAI 2019. Codes are available at\n  https://github.com/lijin118/LLAE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) and cold-start recommendation (CSR) are two\nchallenging problems in computer vision and recommender system, respectively.\nIn general, they are independently investigated in different communities. This\npaper, however, reveals that ZSL and CSR are two extensions of the same\nintension. Both of them, for instance, attempt to predict unseen classes and\ninvolve two spaces, one for direct feature representation and the other for\nsupplementary description. Yet there is no existing approach which addresses\nCSR from the ZSL perspective. This work, for the first time, formulates CSR as\na ZSL problem, and a tailor-made ZSL method is proposed to handle CSR.\nSpecifically, we propose a Low-rank Linear Auto-Encoder (LLAE), which\nchallenges three cruxes, i.e., domain shift, spurious correlations and\ncomputing efficiency, in this paper. LLAE consists of two parts, a low-rank\nencoder maps user behavior into user attributes and a symmetric decoder\nreconstructs user behavior from user attributes. Extensive experiments on both\nZSL and CSR tasks verify that the proposed method is a win-win formulation,\ni.e., not only can CSR be handled by ZSL models with a significant performance\nimprovement compared with several conventional state-of-the-art methods, but\nthe consideration of CSR can benefit ZSL as well.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 09:13:17 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 02:52:11 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Li", "Jingjing", ""], ["Jing", "Mengmeng", ""], ["Lu", "Ke", ""], ["Zhu", "Lei", ""], ["Yang", "Yang", ""], ["Huang", "Zi", ""]]}, {"id": "1906.08547", "submitter": "Ting Yao", "authors": "Fuchen Long, Qi Cai, Zhaofan Qiu, Zhijian Hou, Yingwei Pan, Ting Yao,\n  Chong-Wah Ngo", "title": "vireoJD-MM at Activity Detection in Extended Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This notebook paper presents an overview and comparative analysis of our\nsystem designed for activity detection in extended videos (ActEV-PC) in\nActivityNet Challenge 2019. Specifically, we exploit person/vehicle detections\nin spatial level and action localization in temporal level for action detection\nin surveillance videos. The mechanism of different tubelet generation and model\ndecomposition methods are studied as well. The detection results are finally\npredicted by late fusing the results from each component.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 10:43:00 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Long", "Fuchen", ""], ["Cai", "Qi", ""], ["Qiu", "Zhaofan", ""], ["Hou", "Zhijian", ""], ["Pan", "Yingwei", ""], ["Yao", "Ting", ""], ["Ngo", "Chong-Wah", ""]]}, {"id": "1906.08580", "submitter": "Ignacio Ubeda", "authors": "Ignacio \\'Ubeda, Jose M. Saavedra, St\\'ephane Nicolas, Caroline\n  Petitjean, Laurent Heutte", "title": "Pattern Spotting in Historical Documents Using Convolutional Models", "comments": "6 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pattern spotting consists of searching in a collection of historical document\nimages for occurrences of a graphical object using an image query. Contrary to\nobject detection, no prior information nor predefined class is given about the\nquery so training a model of the object is not feasible. In this paper, a\nconvolutional neural network approach is proposed to tackle this problem. We\nuse RetinaNet as a feature extractor to obtain multiscale embeddings of the\nregions of the documents and also for the queries. Experiments conducted on the\nDocExplore dataset show that our proposal is better at locating patterns and\nrequires less storage for indexing images than the state-of-the-art system, but\nfails at retrieving some pages containing multiple instances of the query.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 12:39:35 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["\u00dabeda", "Ignacio", ""], ["Saavedra", "Jose M.", ""], ["Nicolas", "St\u00e9phane", ""], ["Petitjean", "Caroline", ""], ["Heutte", "Laurent", ""]]}, {"id": "1906.08598", "submitter": "Caixia Zhang", "authors": "Bo wang, Hao Hu, Caixia Zhang", "title": "Companion Surface of Danger Cylinder and its Role in Solution Variation\n  of P3P Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally the danger cylinder is intimately related to the solution\nstability in P3P problem. In this work, we show that the danger cylinder is\nalso closely related to the multiple-solution phenomenon. More specifically, we\nshow when the optical center lies on the danger cylinder, of the 3 possible P3P\nsolutions, i.e., one double solution, and two other solutions, the optical\ncenter of the double solution still lies on the danger cylinder, but the\noptical centers of the other two solutions no longer lie on the danger\ncylinder. And when the optical center moves on the danger cylinder, accordingly\nthe optical centers of the two other solutions of the corresponding P3P problem\nform a new surface, characterized by a polynomial equation of degree 12 in the\noptical center coordinates, called the Companion Surface of Danger Cylinder\n(CSDC). That means the danger cylinder always has a companion surface. For the\nsignificance of CSDC, we show that when the optical center passes through the\nCSDC, the number of solutions of P3P problem must change by 2. That means CSDC\nacts as a delimitating surface of the P3P solution space. These new findings\nshed some new lights on the P3P multi-solution phenomenon, an important issue\nin PnP study.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 16:09:23 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["wang", "Bo", ""], ["Hu", "Hao", ""], ["Zhang", "Caixia", ""]]}, {"id": "1906.08620", "submitter": "Jonathan Ramos", "authors": "Jonathan S. Ramos, Carolina Y. V. Watanabe, Marcello H.\n  Nogueira-Barbosa, Agma J. M. Traina", "title": "BGrowth: an efficient approach for the segmentation of vertebral\n  compression fractures in magnetic resonance imaging", "comments": "This is a pre-print of an article published in Symposium on Applied\n  Computing. The final authenticated version is available online at\n  https://doi.org/10.1145/3297280.3299728", "journal-ref": "The 34th ACM/SIGAPP Symposium on Applied Computing (SAC2019)", "doi": "10.1145/3297280.3299728", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of medical images is a critical issue: several process of\nanalysis and classification rely on this segmentation. With the growing number\nof people presenting back pain and problems related to it, the automatic or\nsemi-automatic segmentation of fractured vertebral bodies became a challenging\ntask. In general, those fractures present several regions with non-homogeneous\nintensities and the dark regions are quite similar to the structures nearby.\nAimed at overriding this challenge, in this paper we present a semi-automatic\nsegmentation method, called Balanced Growth (BGrowth). The experimental results\non a dataset with 102 crushed and 89 normal vertebrae show that our approach\nsignificantly outperforms well-known methods from the literature. We have\nachieved an accuracy up to 95% while keeping acceptable processing time\nperformance, that is equivalent to the state-of-the-artmethods. Moreover,\nBGrowth presents the best results even with a rough (sloppy) manual annotation\n(seed points).\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 13:51:27 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 01:17:16 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Ramos", "Jonathan S.", ""], ["Watanabe", "Carolina Y. V.", ""], ["Nogueira-Barbosa", "Marcello H.", ""], ["Traina", "Agma J. M.", ""]]}, {"id": "1906.08628", "submitter": "Guo-Jun Qi", "authors": "Guo-Jun Qi, Liheng Zhang, Xiao Wang", "title": "Learning Generalized Transformation Equivariant Representations via\n  Autoencoding Transformations", "comments": "arXiv admin note: text overlap with arXiv:1903.10863", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformation Equivariant Representations (TERs) aim to capture the\nintrinsic visual structures that equivary to various transformations by\nexpanding the notion of {\\em translation} equivariance underlying the success\nof Convolutional Neural Networks (CNNs). For this purpose, we present both\ndeterministic AutoEncoding Transformations (AET) and probabilistic AutoEncoding\nVariational Transformations (AVT) models to learn visual representations from\ngeneric groups of transformations. While the AET is trained by directly\ndecoding the transformations from the learned representations, the AVT is\ntrained by maximizing the joint mutual information between the learned\nrepresentation and transformations. This results in Generalized TERs (GTERs)\nequivariant against transformations in a more general fashion by capturing\ncomplex patterns of visual structures beyond the conventional linear\nequivariance under a transformation group. The presented approach can be\nextended to (semi-)supervised models by jointly maximizing the mutual\ninformation of the learned representation with both labels and transformations.\nExperiments demonstrate the proposed models outperform the state-of-the-art\nmodels in both unsupervised and (semi-)supervised tasks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 06:17:56 GMT"}, {"version": "v2", "created": "Sat, 14 Sep 2019 04:50:28 GMT"}, {"version": "v3", "created": "Sun, 17 Nov 2019 16:27:59 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Qi", "Guo-Jun", ""], ["Zhang", "Liheng", ""], ["Wang", "Xiao", ""]]}, {"id": "1906.08637", "submitter": "Joseph Bethge", "authors": "Joseph Bethge, Haojin Yang, Marvin Bornstein, Christoph Meinel", "title": "Back to Simplicity: How to Train Accurate BNNs from Scratch?", "comments": "Supplementary Material can be found\n  https://owncloud.hpi.de/s/1jrAUnqRAfg0TXH. arXiv admin note: substantial text\n  overlap with arXiv:1812.01965", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary Neural Networks (BNNs) show promising progress in reducing\ncomputational and memory costs but suffer from substantial accuracy degradation\ncompared to their real-valued counterparts on large-scale datasets, e.g.,\nImageNet. Previous work mainly focused on reducing quantization errors of\nweights and activations, whereby a series of approximation methods and\nsophisticated training tricks have been proposed. In this work, we make several\nobservations that challenge conventional wisdom. We revisit some commonly used\ntechniques, such as scaling factors and custom gradients, and show that these\nmethods are not crucial in training well-performing BNNs. On the contrary, we\nsuggest several design principles for BNNs based on the insights learned and\ndemonstrate that highly accurate BNNs can be trained from scratch with a simple\ntraining strategy. We propose a new BNN architecture BinaryDenseNet, which\nsignificantly surpasses all existing 1-bit CNNs on ImageNet without tricks. In\nour experiments, BinaryDenseNet achieves 18.6% and 7.6% relative improvement\nover the well-known XNOR-Network and the current state-of-the-art Bi-Real Net\nin terms of top-1 accuracy on ImageNet, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 10:32:45 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Bethge", "Joseph", ""], ["Yang", "Haojin", ""], ["Bornstein", "Marvin", ""], ["Meinel", "Christoph", ""]]}, {"id": "1906.08650", "submitter": "Jean Lahoud", "authors": "Jean Lahoud, Bernard Ghanem, Marc Pollefeys, Martin R. Oswald", "title": "3D Instance Segmentation via Multi-Task Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for instance label segmentation of dense 3D voxel\ngrids. We target volumetric scene representations, which have been acquired\nwith depth sensors or multi-view stereo methods and which have been processed\nwith semantic 3D reconstruction or scene completion methods. The main task is\nto learn shape information about individual object instances in order to\naccurately separate them, including connected and incompletely scanned objects.\nWe solve the 3D instance-labeling problem with a multi-task learning strategy.\nThe first goal is to learn an abstract feature embedding, which groups voxels\nwith the same instance label close to each other while separating clusters with\ndifferent instance labels from each other. The second goal is to learn instance\ninformation by densely estimating directional information of the instance's\ncenter of mass for each voxel. This is particularly useful to find instance\nboundaries in the clustering post-processing step, as well as, for scoring the\nsegmentation quality for the first goal. Both synthetic and real-world\nexperiments demonstrate the viability and merits of our approach. In fact, it\nachieves state-of-the-art performance on the ScanNet 3D instance segmentation\nbenchmark.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 14:14:16 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 01:07:14 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Lahoud", "Jean", ""], ["Ghanem", "Bernard", ""], ["Pollefeys", "Marc", ""], ["Oswald", "Martin R.", ""]]}, {"id": "1906.08675", "submitter": "Alan Lukezic", "authors": "Alan Luke\\v{z}i\\v{c}, Luka \\v{C}ehovin Zajc, Tom\\'a\\v{s} Voj\\'i\\v{r},\n  Ji\\v{r}\\'i Matas and Matej Kristan", "title": "Performance Evaluation Methodology for Long-Term Visual Object Tracking", "comments": "Submitted to a journal on June 2018. arXiv admin note: substantial\n  text overlap with arXiv:1804.07056", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long-term visual object tracking performance evaluation methodology and a\nbenchmark are proposed. Performance measures are designed by following a\nlong-term tracking definition to maximize the analysis probing strength. The\nnew measures outperform existing ones in interpretation potential and in better\ndistinguishing between different tracking behaviors. We show that these\nmeasures generalize the short-term performance measures, thus linking the two\ntracking problems. Furthermore, the new measures are highly robust to temporal\nannotation sparsity and allow annotation of sequences hundreds of times longer\nthan in the current datasets without increasing manual annotation labor. A new\nchallenging dataset of carefully selected sequences with many target\ndisappearances is proposed. A new tracking taxonomy is proposed to position\ntrackers on the short-term/long-term spectrum. The benchmark contains an\nextensive evaluation of the largest number of long-term tackers and comparison\nto state-of-the-art short-term trackers. We analyze the influence of tracking\narchitecture implementations to long-term performance and explore various\nre-detection strategies as well as influence of visual model update strategies\nto long-term tracking drift. The methodology is integrated in the VOT toolkit\nto automate experimental analysis and benchmarking and to facilitate future\ndevelopment of long-term trackers.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 12:38:21 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Luke\u017ei\u010d", "Alan", ""], ["Zajc", "Luka \u010cehovin", ""], ["Voj\u00ed\u0159", "Tom\u00e1\u0161", ""], ["Matas", "Ji\u0159\u00ed", ""], ["Kristan", "Matej", ""]]}, {"id": "1906.08676", "submitter": "Adam Byerly", "authors": "Adam Byerly and Tatiana Kalganova", "title": "Homogeneous Vector Capsules Enable Adaptive Gradient Descent in\n  Convolutional Neural Networks", "comments": "14 pages, 5 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capsules are the name given by Geoffrey Hinton to vector-valued neurons.\nNeural networks traditionally produce a scalar value for an activated neuron.\nCapsules, on the other hand, produce a vector of values, which Hinton argues\ncorrespond to a single, composite feature wherein the values of the components\nof the vectors indicate properties of the feature such as transformation or\ncontrast. We present a new way of parameterizing and training capsules that we\nrefer to as homogeneous vector capsules (HVCs). We demonstrate, experimentally,\nthat altering a convolutional neural network (CNN) to use HVCs can achieve\nsuperior classification accuracy without increasing the number of parameters or\noperations in its architecture as compared to a CNN using a single final fully\nconnected layer. Additionally, the introduction of HVCs enables the use of\nadaptive gradient descent, reducing the dependence a model's achievable\naccuracy has on the finely tuned hyperparameters of a non-adaptive optimizer.\nWe demonstrate our method and results using two neural network architectures.\nFirst, a very simple monolithic CNN that when using HVCs achieved a 63%\nimprovement in top-1 classification accuracy and a 35% improvement in top-5\nclassification accuracy over the baseline architecture. Second, with the CNN\narchitecture referred to as Inception v3 that achieved similar accuracies both\nwith and without HVCs. Additionally, the simple monolithic CNN when using HVCs\nshowed no overfitting after more than 300 epochs whereas the baseline showed\noverfitting after 30 epochs. We use the ImageNet ILSVRC 2012 classification\nchallenge dataset with both networks.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 14:54:14 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 01:55:50 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Byerly", "Adam", ""], ["Kalganova", "Tatiana", ""]]}, {"id": "1906.08707", "submitter": "Brandon Amos", "authors": "Brandon Amos, Vladlen Koltun, J. Zico Kolter", "title": "The Limited Multi-Label Projection Layer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Limited Multi-Label (LML) projection layer as a new primitive\noperation for end-to-end learning systems. The LML layer provides a\nprobabilistic way of modeling multi-label predictions limited to having exactly\nk labels. We derive efficient forward and backward passes for this layer and\nshow how the layer can be used to optimize the top-k recall for multi-label\ntasks with incomplete label information. We evaluate LML layers on top-k\nCIFAR-100 classification and scene graph generation. We show that LML layers\nadd a negligible amount of computational overhead, strictly improve the model's\nrepresentational capacity, and improve accuracy. We also revisit the truncated\ntop-k entropy method as a competitive baseline for top-k classification.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 15:51:24 GMT"}, {"version": "v2", "created": "Sat, 22 Jun 2019 18:24:49 GMT"}, {"version": "v3", "created": "Mon, 14 Oct 2019 17:53:34 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Amos", "Brandon", ""], ["Koltun", "Vladlen", ""], ["Kolter", "J. Zico", ""]]}, {"id": "1906.08713", "submitter": "Mehmet Yamac", "authors": "Mehmet Yamac, Mete Ahishali, Nikolaos Passalis, Jenni Raitoharju,\n  Bulent Sankur, and Moncef Gabbouj", "title": "Reversible Privacy Preservation using Multi-level Encryption and\n  Compressive Sensing", "comments": "5 pages, submitted/accepted, EUSIPCO 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV eess.IV eess.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Security monitoring via ubiquitous cameras and their more extended in\nintelligent buildings stand to gain from advances in signal processing and\nmachine learning. While these innovative and ground-breaking applications can\nbe considered as a boon, at the same time they raise significant privacy\nconcerns. In fact, recent GDPR (General Data Protection Regulation) legislation\nhas highlighted and become an incentive for privacy-preserving solutions.\nTypical privacy-preserving video monitoring schemes address these concerns by\neither anonymizing the sensitive data. However, these approaches suffer from\nsome limitations, since they are usually non-reversible, do not provide\nmultiple levels of decryption and computationally costly. In this paper, we\nprovide a novel privacy-preserving method, which is reversible, supports\nde-identification at multiple privacy levels, and can efficiently perform data\nacquisition, encryption and data hiding by combining multi-level encryption\nwith compressive sensing. The effectiveness of the proposed approach in\nprotecting the identity of the users has been validated using the goodness of\nreconstruction quality and strong anonymization of the faces.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 15:58:53 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Yamac", "Mehmet", ""], ["Ahishali", "Mete", ""], ["Passalis", "Nikolaos", ""], ["Raitoharju", "Jenni", ""], ["Sankur", "Bulent", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "1906.08714", "submitter": "Jin-mo Choi", "authors": "Jin-mo Choi", "title": "Clustering and Classification Networks", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we will describe a network architecture that demonstrates high\nperformance on various sizes of datasets. To do this, we will perform an\narchitecture search by dividing the fully connected layer into three levels in\nthe existing network architecture. The first step is to learn existing CNN\nlayer and existing fully connected layer for 1 epoch. The second step is\nclustering similar classes by applying L1 distance to the result of Softmax.\nThe third step is to reclassify using clustering class masks. We accomplished\nthe result of state-of-the-art by performing the above three steps sequentially\nor recursively. The technology recorded an error of 11.56% on Cifar-100.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 15:59:22 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Choi", "Jin-mo", ""]]}, {"id": "1906.08716", "submitter": "Christos Kyrkou", "authors": "Christos Kyrkou, Theocharis Theocharides", "title": "Deep-Learning-Based Aerial Image Classification for Emergency Response\n  Applications Using Unmanned Aerial Vehicles", "comments": "CVPR International Workshop on Computer Vision for UAVs\n  (UAVision2019), 16 June 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unmanned Aerial Vehicles (UAVs), equipped with camera sensors can facilitate\nenhanced situational awareness for many emergency response and disaster\nmanagement applications since they are capable of operating in remote and\ndifficult to access areas. In addition, by utilizing an embedded platform and\ndeep learning UAVs can autonomously monitor a disaster stricken area, analyze\nthe image in real-time and alert in the presence of various calamities such as\ncollapsed buildings, flood, or fire in order to faster mitigate their effects\non the environment and on human population. To this end, this paper focuses on\nthe automated aerial scene classification of disaster events from on-board a\nUAV. Specifically, a dedicated Aerial Image Database for Emergency Response\n(AIDER) applications is introduced and a comparative analysis of existing\napproaches is performed. Through this analysis a lightweight convolutional\nneural network (CNN) architecture is developed, capable of running efficiently\non an embedded platform achieving ~3x higher performance compared to existing\nmodels with minimal memory requirements with less than 2% accuracy drop\ncompared to the state-of-the-art. These preliminary results provide a solid\nbasis for further experimentation towards real-time aerial image classification\nfor emergency response applications using UAVs.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 16:03:32 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Kyrkou", "Christos", ""], ["Theocharides", "Theocharis", ""]]}, {"id": "1906.08743", "submitter": "David G\\\"uera", "authors": "David G\\\"uera and Sriram Baireddy and Paolo Bestagini and Stefano\n  Tubaro and Edward J. Delp", "title": "We Need No Pixels: Video Manipulation Detection Using Stream Descriptors", "comments": "7 pages, 6 figures, presented at the ICML 2019 Worksop on Synthetic\n  Realities: Deep Learning for Detecting AudioVisual Fakes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manipulating video content is easier than ever. Due to the misuse potential\nof manipulated content, multiple detection techniques that analyze the pixel\ndata from the videos have been proposed. However, clever manipulators should\nalso carefully forge the metadata and auxiliary header information, which is\nharder to do for videos than images. In this paper, we propose to identify\nforged videos by analyzing their multimedia stream descriptors with simple\nbinary classifiers, completely avoiding the pixel space. Using well-known\ndatasets, our results show that this scalable approach can achieve a high\nmanipulation detection score if the manipulators have not done a careful data\nsanitization of the multimedia stream descriptors.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 16:42:06 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["G\u00fcera", "David", ""], ["Baireddy", "Sriram", ""], ["Bestagini", "Paolo", ""], ["Tubaro", "Stefano", ""], ["Delp", "Edward J.", ""]]}, {"id": "1906.08744", "submitter": "Stuart Golodetz", "authors": "Tommaso Cavallari, Luca Bertinetto, Jishnu Mukhoti, Philip Torr and\n  Stuart Golodetz", "title": "Let's Take This Online: Adapting Scene Coordinate Regression Network\n  Predictions for Online RGB-D Camera Relocalisation", "comments": "Tommaso Cavallari and Stuart Golodetz contributed equally to this\n  paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications require a camera to be relocalised online, without\nexpensive offline training on the target scene. Whilst both keyframe and sparse\nkeypoint matching methods can be used online, the former often fail away from\nthe training trajectory, and the latter can struggle in textureless regions. By\ncontrast, scene coordinate regression (SCoRe) methods generalise to novel poses\nand can leverage dense correspondences to improve robustness, and recent work\nhas shown how to adapt SCoRe forests between scenes, allowing their\nstate-of-the-art performance to be leveraged online. However, because they use\nfeatures hand-crafted for indoor use, they do not generalise well to harder\noutdoor scenes. Whilst replacing the forest with a neural network and learning\nsuitable features for outdoor use is possible, the techniques used to adapt\nforests between scenes are unfortunately harder to transfer to a network\ncontext. In this paper, we address this by proposing a novel way of leveraging\na network trained on one scene to predict points in another scene. Our approach\nreplaces the appearance clustering performed by the branching structure of a\nregression forest with a two-step process that first uses the network to\npredict points in the original scene, and then uses these predicted points to\nlook up clusters of points from the new scene. We show experimentally that our\nonline approach achieves state-of-the-art performance on both the 7-Scenes and\nCambridge Landmarks datasets, whilst running in under 300ms, making it highly\neffective in live scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 16:44:16 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Cavallari", "Tommaso", ""], ["Bertinetto", "Luca", ""], ["Mukhoti", "Jishnu", ""], ["Torr", "Philip", ""], ["Golodetz", "Stuart", ""]]}, {"id": "1906.08746", "submitter": "Le Thanh Nguyen-Meidine", "authors": "Le Thanh Nguyen-Meidine, Eric Granger, Madhu Kiran, Louis-Antoine\n  Blais-Morin, Marco Pedersoli", "title": "Progressive Gradient Pruning for Classification, Detection and\n  DomainAdaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep neural networks (NNs) have achievedstate-of-the-art accuracy in\nmany visual recognition tasks,the growing computational complexity and energy\ncon-sumption of networks remains an issue, especially for ap-plications on\nplatforms with limited resources and requir-ing real-time processing. Filter\npruning techniques haverecently shown promising results for the compression\nandacceleration of convolutional NNs (CNNs). However, thesetechniques involve\nnumerous steps and complex optimisa-tions because some only prune after\ntraining CNNs, whileothers prune from scratch during training by\nintegratingsparsity constraints or modifying the loss function.In this paper we\npropose a new Progressive GradientPruning (PGP) technique for iterative filter\npruning dur-ing training. In contrast to previous progressive\npruningtechniques, it relies on a novel filter selection criterion thatmeasures\nthe change in filter weights, uses a new hard andsoft pruning strategy and\neffectively adapts momentum ten-sors during the backward propagation pass.\nExperimentalresults obtained after training various CNNs on image datafor\nclassification, object detection and domain adaptationbenchmarks indicate that\nthe PGP technique can achievea better trade-off between classification accuracy\nand net-work (time and memory) complexity than PSFP and otherstate-of-the-art\nfilter pruning techniques.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 16:46:16 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 13:46:47 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 15:27:16 GMT"}, {"version": "v4", "created": "Tue, 25 Feb 2020 18:42:05 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Nguyen-Meidine", "Le Thanh", ""], ["Granger", "Eric", ""], ["Kiran", "Madhu", ""], ["Blais-Morin", "Louis-Antoine", ""], ["Pedersoli", "Marco", ""]]}, {"id": "1906.08754", "submitter": "Ferdia Sherry", "authors": "Ferdia Sherry, Martin Benning, Juan Carlos De los Reyes, Martin J.\n  Graves, Georg Maierhofer, Guy Williams, Carola-Bibiane Sch\\\"onlieb, Matthias\n  J. Ehrhardt", "title": "Learning the Sampling Pattern for MRI", "comments": "The main document is 12 pages, the supporting document is 2 pages and\n  attached at the end of the main document", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.NA math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discovery of the theory of compressed sensing brought the realisation\nthat many inverse problems can be solved even when measurements are\n\"incomplete\". This is particularly interesting in magnetic resonance imaging\n(MRI), where long acquisition times can limit its use. In this work, we\nconsider the problem of learning a sparse sampling pattern that can be used to\noptimally balance acquisition time versus quality of the reconstructed image.\nWe use a supervised learning approach, making the assumption that our training\ndata is representative enough of new data acquisitions. We demonstrate that\nthis is indeed the case, even if the training data consists of just 7 training\npairs of measurements and ground-truth images; with a training set of brain\nimages of size 192 by 192, for instance, one of the learned patterns samples\nonly 35% of k-space, however results in reconstructions with mean SSIM 0.914 on\na test set of similar images. The proposed framework is general enough to learn\narbitrary sampling patterns, including common patterns such as Cartesian,\nspiral and radial sampling.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 17:13:45 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 00:03:35 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Sherry", "Ferdia", ""], ["Benning", "Martin", ""], ["Reyes", "Juan Carlos De los", ""], ["Graves", "Martin J.", ""], ["Maierhofer", "Georg", ""], ["Williams", "Guy", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""], ["Ehrhardt", "Matthias J.", ""]]}, {"id": "1906.08763", "submitter": "Gauri Jagatap", "authors": "Gauri Jagatap, Chinmay Hegde", "title": "Algorithmic Guarantees for Inverse Imaging with Untrained Network Priors", "comments": "NeurIPS 2019 version with few modifications", "journal-ref": "NeurIPS 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks as image priors have been recently introduced for\nproblems such as denoising, super-resolution and inpainting with promising\nperformance gains over hand-crafted image priors such as sparsity and low-rank.\nUnlike learned generative priors they do not require any training over large\ndatasets. However, few theoretical guarantees exist in the scope of using\nuntrained neural network priors for inverse imaging problems. We explore new\napplications and theory for untrained neural network priors. Specifically, we\nconsider the problem of solving linear inverse problems, such as compressive\nsensing, as well as non-linear problems, such as compressive phase retrieval.\nWe model images to lie in the range of an untrained deep generative network\nwith a fixed seed. We further present a projected gradient descent scheme that\ncan be used for both compressive sensing and phase retrieval and provide\nrigorous theoretical guarantees for its convergence. We also show both\ntheoretically as well as empirically that with deep network priors, one can\nachieve better compression rates for the same image quality compared to hand\ncrafted priors.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 17:34:49 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 18:29:58 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Jagatap", "Gauri", ""], ["Hegde", "Chinmay", ""]]}, {"id": "1906.08764", "submitter": "Qiuxia Lai", "authors": "Qiuxia Lai, Salman Khan, Yongwei Nie, Jianbing Shen, Hanqiu Sun, Ling\n  Shao", "title": "Understanding More about Human and Machine Attention in Deep Neural\n  Networks", "comments": "Q. Lai, S. Khan, Y. Nie, J. Shen, H. Sun, L. Shao, TMM, in press,\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human visual system can selectively attend to parts of a scene for quick\nperception, a biological mechanism known as Human attention. Inspired by this,\nrecent deep learning models encode attention mechanisms to focus on the most\ntask-relevant parts of the input signal for further processing, which is called\nMachine/Neural/Artificial attention. Understanding the relation between human\nand machine attention is important for interpreting and designing neural\nnetworks. Many works claim that the attention mechanism offers an extra\ndimension of interpretability by explaining where the neural networks look.\nHowever, recent studies demonstrate that artificial attention maps do not\nalways coincide with common intuition. In view of these conflicting evidence,\nhere we make a systematic study on using artificial attention and human\nattention in neural network design. With three example computer vision tasks,\ndiverse representative backbones, and famous architectures, corresponding real\nhuman gaze data, and systematically conducted large-scale quantitative studies,\nwe quantify the consistency between artificial attention and human visual\nattention and offer novel insights into existing artificial attention\nmechanisms by giving preliminary answers to several key questions related to\nhuman and artificial attention mechanisms. Overall results demonstrate that\nhuman attention can benchmark the meaningful `ground-truth' in attention-driven\ntasks, where the more the artificial attention is close to human attention, the\nbetter the performance; for higher-level vision tasks, it is case-by-case. It\nwould be advisable for attention-driven tasks to explicitly force a better\nalignment between artificial and human attention to boost the performance; such\nalignment would also improve the network explainability for higher-level\ncomputer vision tasks.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 17:41:57 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 06:17:11 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 05:13:58 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Lai", "Qiuxia", ""], ["Khan", "Salman", ""], ["Nie", "Yongwei", ""], ["Shen", "Jianbing", ""], ["Sun", "Hanqiu", ""], ["Shao", "Ling", ""]]}, {"id": "1906.08826", "submitter": "Mohamad Ali-Dib", "authors": "Mohamad Ali-Dib, Kristen Menou, Alan P. Jackson, Chenchong Zhu, Noah\n  Hammond", "title": "Automated crater shape retrieval using weakly-supervised deep learning", "comments": "59 pages, 13 figures, Accepted for publication in Icarus", "journal-ref": null, "doi": "10.1016/j.icarus.2020.113749", "report-no": null, "categories": "astro-ph.EP astro-ph.IM cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crater ellipticity determination is a complex and time consuming task that so\nfar has evaded successful automation. We train a state of the art computer\nvision algorithm to identify craters in Lunar digital elevation maps and\nretrieve their sizes and 2D shapes. The computational backbone of the model is\nMaskRCNN, an \"instance segmentation\" general framework that detects craters in\nan image while simultaneously producing a mask for each crater that traces its\nouter rim. Our post-processing pipeline then finds the closest fitting ellipse\nto these masks, allowing us to retrieve the crater ellipticities. Our model is\nable to correctly identify 87\\% of known craters in the longitude range we hid\nfrom the network during training and validation (test set), while predicting\nthousands of additional craters not present in our training data. Manual\nvalidation of a subset of these \"new\" craters indicates that a majority of them\nare real, which we take as an indicator of the strength of our model in\nlearning to identify craters, despite incomplete training data. The crater\nsize, ellipticity, and depth distributions predicted by our model are\nconsistent with human-generated results. The model allows us to perform a large\nscale search for differences in crater diameter and shape distributions between\nthe lunar highlands and maria, and we exclude any such differences with a high\nstatistical significance. The predicted test set catalogue and trained model\nare available here: https://github.com/malidib/Craters_MaskRCNN/.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 20:04:08 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 17:38:10 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2020 16:34:45 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Ali-Dib", "Mohamad", ""], ["Menou", "Kristen", ""], ["Jackson", "Alan P.", ""], ["Zhu", "Chenchong", ""], ["Hammond", "Noah", ""]]}, {"id": "1906.08859", "submitter": "Yulia Sandamirskaya", "authors": "Bodo R\\\"uckauer, Nicolas K\\\"anzig, Shih-Chii Liu, Tobi Delbruck, and\n  Yulia Sandamirskaya", "title": "Closing the Accuracy Gap in an Event-Based Visual Recognition Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile and embedded applications require neural networks-based pattern\nrecognition systems to perform well under a tight computational budget. In\ncontrast to commonly used synchronous, frame-based vision systems and CNNs,\nasynchronous, spiking neural networks driven by event-based visual input\nrespond with low latency to sparse, salient features in the input, leading to\nhigh efficiency at run-time. The discrete nature of the event-based data\nstreams makes direct training of asynchronous neural networks challenging. This\npaper studies asynchronous spiking neural networks, obtained by conversion from\na conventional CNN trained on frame-based data. As an example, we consider a\nCNN trained to steer a robot to follow a moving target. We identify possible\npitfalls of the conversion and demonstrate how the proposed solutions bring the\nclassification accuracy of the asynchronous network to only 3\\% below the\nperformance of the original synchronous CNN, while requiring 12x fewer\ncomputations. While being applied to a simple task, this work is an important\nstep towards low-power, fast, and embedded neural networks-based vision\nsolutions for robotic applications.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 12:11:05 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["R\u00fcckauer", "Bodo", ""], ["K\u00e4nzig", "Nicolas", ""], ["Liu", "Shih-Chii", ""], ["Delbruck", "Tobi", ""], ["Sandamirskaya", "Yulia", ""]]}, {"id": "1906.08861", "submitter": "Deboleena Roy", "authors": "Deboleena Roy, Priyadarshini Panda, and Kaushik Roy", "title": "Synthesizing Images from Spatio-Temporal Representations using\n  Spike-based Backpropagation", "comments": "17 pages, 10 Figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural networks (SNNs) offer a promising alternative to current\nartificial neural networks to enable low-power event-driven neuromorphic\nhardware. Spike-based neuromorphic applications require processing and\nextracting meaningful information from spatio-temporal data, represented as\nseries of spike trains over time. In this paper, we propose a method to\nsynthesize images from multiple modalities in a spike-based environment. We use\nspiking auto-encoders to convert image and audio inputs into compact\nspatio-temporal representations that is then decoded for image synthesis. For\nthis, we use a direct training algorithm that computes loss on the membrane\npotential of the output layer and back-propagates it by using a sigmoid\napproximation of the neuron's activation function to enable differentiability.\nThe spiking autoencoders are benchmarked on MNIST and Fashion-MNIST and achieve\nvery low reconstruction loss, comparable to ANNs. Then, spiking autoencoders\nare trained to learn meaningful spatio-temporal representations of the data,\nacross the two modalities - audio and visual. We synthesize images from audio\nin a spike-based environment by first generating, and then utilizing such\nshared multi-modal spatio-temporal representations. Our audio to image\nsynthesis model is tested on the task of converting TI-46 digits audio samples\nto MNIST images. We are able to synthesize images with high fidelity and the\nmodel achieves competitive performance against ANNs.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 01:33:15 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Roy", "Deboleena", ""], ["Panda", "Priyadarshini", ""], ["Roy", "Kaushik", ""]]}, {"id": "1906.08876", "submitter": "Sanqiang Zhao", "authors": "Sanqiang Zhao, Piyush Sharma, Tomer Levinboim, Radu Soricut", "title": "Informative Image Captioning with External Sources of Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An image caption should fluently present the essential information in a given\nimage, including informative, fine-grained entity mentions and the manner in\nwhich these entities interact. However, current captioning models are usually\ntrained to generate captions that only contain common object names, thus\nfalling short on an important \"informativeness\" dimension. We present a\nmechanism for integrating image information together with fine-grained labels\n(assumed to be generated by some upstream models) into a caption that describes\nthe image in a fluent and informative manner. We introduce a multimodal,\nmulti-encoder model based on Transformer that ingests both image features and\nmultiple sources of entity labels. We demonstrate that we can learn to control\nthe appearance of these entity labels in the output, resulting in captions that\nare both fluent and informative.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 21:51:48 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Zhao", "Sanqiang", ""], ["Sharma", "Piyush", ""], ["Levinboim", "Tomer", ""], ["Soricut", "Radu", ""]]}, {"id": "1906.08889", "submitter": "Tuo Feng", "authors": "Tuo Feng, Dongbing Gu", "title": "SGANVO: Unsupervised Deep Visual Odometry and Depth Estimation with\n  Stacked Generative Adversarial Networks", "comments": "7 pages, 4 figures,", "journal-ref": null, "doi": "10.1109/LRA.2019.2925555", "report-no": "ras.ral.19-0181.628f4a7b", "categories": "cs.RO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently end-to-end unsupervised deep learning methods have achieved an\neffect beyond geometric methods for visual depth and ego-motion estimation\ntasks. These data-based learning methods perform more robustly and accurately\nin some of the challenging scenes. The encoder-decoder network has been widely\nused in the depth estimation and the RCNN has brought significant improvements\nin the ego-motion estimation. Furthermore, the latest use of Generative\nAdversarial Nets(GANs) in depth and ego-motion estimation has demonstrated that\nthe estimation could be further improved by generating pictures in the game\nlearning process. This paper proposes a novel unsupervised network system for\nvisual depth and ego-motion estimation: Stacked Generative Adversarial\nNetwork(SGANVO). It consists of a stack of GAN layers, of which the lowest\nlayer estimates the depth and ego-motion while the higher layers estimate the\nspatial features. It can also capture the temporal dynamic due to the use of a\nrecurrent representation across the layers. See Fig.1 for details. We select\nthe most commonly used KITTI [1] data set for evaluation. The evaluation\nresults show that our proposed method can produce better or comparable results\nin depth and ego-motion estimation.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 22:51:10 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Feng", "Tuo", ""], ["Gu", "Dongbing", ""]]}, {"id": "1906.08891", "submitter": "Sandipan Choudhuri", "authors": "Sandipan Choudhuri, Kaustav Basu, Kevin Thomas, Arunabha Sen", "title": "Predicting Future Opioid Incidences Today", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to the Center of Disease Control (CDC), the Opioid epidemic has\nclaimed more than 72,000 lives in the US in 2017 alone. In spite of various\nefforts at the local, state and federal level, the impact of the epidemic is\nbecoming progressively worse, as evidenced by the fact that the number of\nOpioid related deaths increased by 12.5\\% between 2016 and 2017. Predictive\nanalytics can play an important role in combating the epidemic by providing\ndecision making tools to stakeholders at multiple levels - from health care\nprofessionals to policy makers to first responders. Generating Opioid incidence\nheat maps from past data, aid these stakeholders to visualize the profound\nimpact of the Opioid epidemic. Such post-fact creation of the heat map provides\nonly retrospective information, and as a result, may not be as useful for\npreventive action in the current or future time-frames. In this paper, we\npresent a novel deep neural architecture, which learns subtle spatio-temporal\nvariations in Opioid incidences data and accurately predicts future heat maps.\nWe evaluated the efficacy of our model on two open source datasets- (i) The\nCincinnati Heroin Overdose dataset, and (ii) Connecticut Drug Related Death\nDataset.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 22:53:18 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Choudhuri", "Sandipan", ""], ["Basu", "Kaustav", ""], ["Thomas", "Kevin", ""], ["Sen", "Arunabha", ""]]}, {"id": "1906.08945", "submitter": "Joey Hong", "authors": "Joey Hong, Benjamin Sapp, James Philbin", "title": "Rules of the Road: Predicting Driving Behavior with a Convolutional\n  Model of Semantic Interactions", "comments": "Accepted at CVPR 2019", "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2019, pp. 8454-8462", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the problem of predicting future states of entities in complex,\nreal-world driving scenarios. Previous research has used low-level signals to\npredict short time horizons, and has not addressed how to leverage key assets\nrelied upon heavily by industry self-driving systems: (1) large 3D perception\nefforts which provide highly accurate 3D states of agents with rich attributes,\nand (2) detailed and accurate semantic maps of the environment (lanes, traffic\nlights, crosswalks, etc). We present a unified representation which encodes\nsuch high-level semantic information in a spatial grid, allowing the use of\ndeep convolutional models to fuse complex scene context. This enables learning\nentity-entity and entity-environment interactions with simple, feed-forward\ncomputations in each timestep within an overall temporal model of an agent's\nbehavior. We propose different ways of modelling the future as a distribution\nover future states using standard supervised learning. We introduce a novel\ndataset providing industry-grade rich perception and semantic inputs, and\nempirically show we can effectively learn fundamentals of driving behavior.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 04:35:14 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Hong", "Joey", ""], ["Sapp", "Benjamin", ""], ["Philbin", "James", ""]]}, {"id": "1906.08953", "submitter": "Gruber Tobias", "authors": "Tobias Gruber and Mario Bijelic and Felix Heide and Werner Ritter and\n  Klaus Dietmayer", "title": "Pixel-Accurate Depth Evaluation in Realistic Driving Scenarios", "comments": "3DV 2019", "journal-ref": "Published in: 2019 International Conference on 3D Vision (3DV)", "doi": "10.1109/3DV.2019.00020", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces an evaluation benchmark for depth estimation and\ncompletion using high-resolution depth measurements with angular resolution of\nup to 25\" (arcsecond), akin to a 50 megapixel camera with per-pixel depth\navailable. Existing datasets, such as the KITTI benchmark, provide only sparse\nreference measurements with an order of magnitude lower angular resolution -\nthese sparse measurements are treated as ground truth by existing depth\nestimation methods. We propose an evaluation methodology in four characteristic\nautomotive scenarios recorded in varying weather conditions (day, night, fog,\nrain). As a result, our benchmark allows us to evaluate the robustness of depth\nsensing methods in adverse weather and different driving conditions. Using the\nproposed evaluation data, we demonstrate that current stereo approaches provide\nsignificantly more stable depth estimates than monocular methods and lidar\ncompletion in adverse weather. Data and code are available at\nhttps://github.com/gruberto/PixelAccurateDepthBenchmark.git.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 05:35:36 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 16:28:04 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Gruber", "Tobias", ""], ["Bijelic", "Mario", ""], ["Heide", "Felix", ""], ["Ritter", "Werner", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "1906.08960", "submitter": "Swathikiran Sudhakaran", "authors": "Swathikiran Sudhakaran and Sergio Escalera and Oswald Lanz", "title": "FBK-HUPBA Submission to the EPIC-Kitchens 2019 Action Recognition\n  Challenge", "comments": "Ranked 3rd in the EPIC-Kitchens 2019 action recognition challenge,\n  held as part of CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report we describe the technical details of our submission to the\nEPIC-Kitchens 2019 action recognition challenge. To participate in the\nchallenge we have developed a number of CNN-LSTA [3] and HF-TSN [2] variants,\nand submitted predictions from an ensemble compiled out of these two model\nfamilies. Our submission, visible on the public leaderboard with team name\nFBK-HUPBA, achieved a top-1 action recognition accuracy of 35.54% on S1\nsetting, and 20.25% on S2 setting.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 05:59:58 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Sudhakaran", "Swathikiran", ""], ["Escalera", "Sergio", ""], ["Lanz", "Oswald", ""]]}, {"id": "1906.08967", "submitter": "Cho-Ying Wu", "authors": "Yiqi Zhong, Cho-Ying Wu, Suya You, Ulrich Neumann", "title": "Deep RGB-D Canonical Correlation Analysis For Sparse Depth Completion", "comments": "NeurIPS 2019. Code link https://github.com/choyingw/CFCNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose our Correlation For Completion Network (CFCNet), an\nend-to-end deep learning model that uses the correlation between two data\nsources to perform sparse depth completion. CFCNet learns to capture, to the\nlargest extent, the semantically correlated features between RGB and depth\ninformation. Through pairs of image pixels and the visible measurements in a\nsparse depth map, CFCNet facilitates feature-level mutual transformation of\ndifferent data sources. Such a transformation enables CFCNet to predict\nfeatures and reconstruct data of missing depth measurements according to their\ncorresponding, transformed RGB features. We extend canonical correlation\nanalysis to a 2D domain and formulate it as one of our training objectives\n(i.e. 2d deep canonical correlation, or \"2D2CCA loss\"). Extensive experiments\nvalidate the ability and flexibility of our CFCNet compared to the\nstate-of-the-art methods on both indoor and outdoor scenes with different\nreal-life sparse patterns. Codes are available at:\nhttps://github.com/choyingw/CFCNet.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 06:18:13 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 06:35:11 GMT"}, {"version": "v3", "created": "Sun, 15 Mar 2020 22:46:11 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Zhong", "Yiqi", ""], ["Wu", "Cho-Ying", ""], ["You", "Suya", ""], ["Neumann", "Ulrich", ""]]}, {"id": "1906.08988", "submitter": "Dong Yin", "authors": "Dong Yin, Raphael Gontijo Lopes, Jonathon Shlens, Ekin D. Cubuk,\n  Justin Gilmer", "title": "A Fourier Perspective on Model Robustness in Computer Vision", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving robustness to distributional shift is a longstanding and\nchallenging goal of computer vision. Data augmentation is a commonly used\napproach for improving robustness, however robustness gains are typically not\nuniform across corruption types. Indeed increasing performance in the presence\nof random noise is often met with reduced performance on other corruptions such\nas contrast change. Understanding when and why these sorts of trade-offs occur\nis a crucial step towards mitigating them. Towards this end, we investigate\nrecently observed trade-offs caused by Gaussian data augmentation and\nadversarial training. We find that both methods improve robustness to\ncorruptions that are concentrated in the high frequency domain while reducing\nrobustness to corruptions that are concentrated in the low frequency domain.\nThis suggests that one way to mitigate these trade-offs via data augmentation\nis to use a more diverse set of augmentations. Towards this end we observe that\nAutoAugment, a recently proposed data augmentation policy optimized for clean\naccuracy, achieves state-of-the-art robustness on the CIFAR-10-C benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 07:31:26 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 04:55:48 GMT"}, {"version": "v3", "created": "Thu, 17 Sep 2020 01:30:25 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Yin", "Dong", ""], ["Lopes", "Raphael Gontijo", ""], ["Shlens", "Jonathon", ""], ["Cubuk", "Ekin D.", ""], ["Gilmer", "Justin", ""]]}, {"id": "1906.08989", "submitter": "Xinchen Yan", "authors": "Xinchen Yan, Mohi Khansari, Jasmine Hsu, Yuanzheng Gong, Yunfei Bai,\n  S\\\"oren Pirk, Honglak Lee", "title": "Data-Efficient Learning for Sim-to-Real Robotic Grasping using Deep\n  Point Cloud Prediction Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Training a deep network policy for robot manipulation is notoriously costly\nand time consuming as it depends on collecting a significant amount of real\nworld data. To work well in the real world, the policy needs to see many\ninstances of the task, including various object arrangements in the scene as\nwell as variations in object geometry, texture, material, and environmental\nillumination.\n  In this paper, we propose a method that learns to perform table-top instance\ngrasping of a wide variety of objects while using no real world grasping data,\noutperforming the baseline using 2.5D shape by 10%. Our method learns 3D point\ncloud of object, and use that to train a domain-invariant grasping policy. We\nformulate the learning process as a two-step procedure: 1) Learning a\ndomain-invariant 3D shape representation of objects from about 76K episodes in\nsimulation and about 530 episodes in the real world, where each episode lasts\nless than a minute and 2) Learning a critic grasping policy in simulation only\nbased on the 3D shape representation from step 1. Our real world data\ncollection in step 1 is both cheaper and faster compared to existing approaches\nas it only requires taking multiple snapshots of the scene using a RGBD camera.\nFinally, the learned 3D representation is not specific to grasping, and can\npotentially be used in other interaction tasks.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 07:36:29 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Yan", "Xinchen", ""], ["Khansari", "Mohi", ""], ["Hsu", "Jasmine", ""], ["Gong", "Yuanzheng", ""], ["Bai", "Yunfei", ""], ["Pirk", "S\u00f6ren", ""], ["Lee", "Honglak", ""]]}, {"id": "1906.09020", "submitter": "Jonas Prellberg", "authors": "Jonas Prellberg, Oliver Kramer", "title": "Acute Lymphoblastic Leukemia Classification from Microscopic Images\n  using Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Examining blood microscopic images for leukemia is necessary when expensive\nequipment for flow cytometry is unavailable. Automated systems can ease the\nburden on medical experts for performing this examination and may be especially\nhelpful to quickly screen a large number of patients. We present a simple, yet\neffective classification approach using a ResNeXt convolutional neural network\nwith Squeeze-and-Excitation modules. The approach was evaluated in the C-NMC\nonline challenge and achieves a weighted F1-score of 88.91% on the test set.\nCode is available at https://github.com/jprellberg/isbi2019cancer\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 09:13:45 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 08:01:39 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Prellberg", "Jonas", ""], ["Kramer", "Oliver", ""]]}, {"id": "1906.09023", "submitter": "Wei Wang", "authors": "Wei Wang, Zheng Dang, Yinlin Hu, Pascal Fua and Mathieu Salzmann", "title": "Backpropagation-Friendly Eigendecomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eigendecomposition (ED) is widely used in deep networks. However, the\nbackpropagation of its results tends to be numerically unstable, whether using\nED directly or approximating it with the Power Iteration method, particularly\nwhen dealing with large matrices. While this can be mitigated by partitioning\nthe data in small and arbitrary groups, doing so has no theoretical basis and\nmakes its impossible to exploit the power of ED to the full. In this paper, we\nintroduce a numerically stable and differentiable approach to leveraging\neigenvectors in deep networks. It can handle large matrices without requiring\nto split them. We demonstrate the better robustness of our approach over\nstandard ED and PI for ZCA whitening, an alternative to batch normalization,\nand for PCA denoising, which we introduce as a new normalization strategy for\ndeep networks, aiming to further denoise the network's features.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 09:17:14 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 07:42:43 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Wang", "Wei", ""], ["Dang", "Zheng", ""], ["Hu", "Yinlin", ""], ["Fua", "Pascal", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "1906.09072", "submitter": "Ruijia Yang", "authors": "YiGui Luo, RuiJia Yang, Wei Sha, WeiYi Ding, YouTeng Sun, YiSi Wang", "title": "Evolution Attack On Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies have been done to prove the vulnerability of neural networks to\nadversarial example. A trained and well-behaved model can be fooled by a\nvisually imperceptible perturbation, i.e., an originally correctly classified\nimage could be misclassified after a slight perturbation. In this paper, we\npropose a black-box strategy to attack such networks using an evolution\nalgorithm. First, we formalize the generation of an adversarial example into\nthe optimization problem of perturbations that represent the noise added to an\noriginal image at each pixel. To solve this optimization problem in a black-box\nway, we find that an evolution algorithm perfectly meets our requirement since\nit can work without any gradient information. Therefore, we test various\nevolution algorithms, including a simple genetic algorithm, a\nparameter-exploring policy gradient, an OpenAI evolution strategy, and a\ncovariance matrix adaptive evolution strategy. Experimental results show that a\ncovariance matrix adaptive evolution Strategy performs best in this\noptimization problem. Additionally, we also perform several experiments to\nexplore the effect of different regularizations on improving the quality of an\nadversarial example.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 11:22:03 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Luo", "YiGui", ""], ["Yang", "RuiJia", ""], ["Sha", "Wei", ""], ["Ding", "WeiYi", ""], ["Sun", "YouTeng", ""], ["Wang", "YiSi", ""]]}, {"id": "1906.09108", "submitter": "Huiping Zhuang", "authors": "Huiping Zhuang, Yi Wang, Qinglai Liu, Shuai Zhang, Zhiping Lin", "title": "Fully Decoupled Neural Network Learning Using Delayed Gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training neural networks with back-propagation (BP) requires a sequential\npassing of activations and gradients, which forces the network modules to work\nin a synchronous fashion. This has been recognized as the lockings (i.e., the\nforward, backward and update lockings) inherited from the BP. In this paper, we\npropose a fully decoupled training scheme using delayed gradients (FDG) to\nbreak all these lockings. The FDG splits a neural network into multiple modules\nand trains them independently and asynchronously using different workers (e.g.,\nGPUs). We also introduce a gradient shrinking process to reduce the stale\ngradient effect caused by the delayed gradients. In addition, we prove that the\nproposed FDG algorithm guarantees a statistical convergence during training.\nExperiments are conducted by training deep convolutional neural networks to\nperform classification tasks on benchmark datasets, showing comparable or\nbetter results against the state-of-the-art methods as well as the BP in terms\nof both generalization and acceleration abilities. In particular, we show that\nthe FDG is also able to train very wide networks (e.g., WRN-28-10) and\nextremely deep networks (e.g., ResNet-1202). Code is available at\nhttps://github.com/ZHUANGHP/FDG.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 13:02:35 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 11:18:32 GMT"}, {"version": "v3", "created": "Fri, 22 Nov 2019 07:01:47 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Zhuang", "Huiping", ""], ["Wang", "Yi", ""], ["Liu", "Qinglai", ""], ["Zhang", "Shuai", ""], ["Lin", "Zhiping", ""]]}, {"id": "1906.09266", "submitter": "Keegan Hines E", "authors": "Mohammad Reza Sarshogh, Keegan E. Hines", "title": "A Multitask Network for Localization and Recognition of Text in Images", "comments": "ICDAR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end trainable multi-task network that addresses the\nproblem of lexicon-free text extraction from complex documents. This network\nsimultaneously solves the problems of text localization and text recognition\nand text segments are identified with no post-processing, cropping, or word\ngrouping. A convolutional backbone and Feature Pyramid Network are combined to\nprovide a shared representation that benefits each of three model heads: text\nlocalization, classification, and text recognition. To improve recognition\naccuracy, we describe a dynamic pooling mechanism that retains high-resolution\ninformation across all RoIs. For text recognition, we propose a convolutional\nmechanism with attention which out-performs more common recurrent\narchitectures. Our model is evaluated against benchmark datasets and comparable\nmethods and achieves high performance in challenging regimes of non-traditional\nOCR.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 17:25:45 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Sarshogh", "Mohammad Reza", ""], ["Hines", "Keegan E.", ""]]}, {"id": "1906.09288", "submitter": "Yuezun Li", "authors": "Yuezun Li, Xin Yang, Baoyuan Wu and Siwei Lyu", "title": "Hiding Faces in Plain Sight: Disrupting AI Face Synthesis with\n  Adversarial Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen fast development in synthesizing realistic human faces\nusing AI technologies. Such fake faces can be weaponized to cause negative\npersonal and social impact. In this work, we develop technologies to defend\nindividuals from becoming victims of recent AI synthesized fake videos by\nsabotaging would-be training data. This is achieved by disrupting deep neural\nnetwork (DNN) based face detection method with specially designed imperceptible\nadversarial perturbations to reduce the quality of the detected faces. We\ndescribe attacking schemes under white-box, gray-box and black-box settings,\neach with decreasing information about the DNN based face detectors. We\nempirically show the effectiveness of our methods in disrupting\nstate-of-the-art DNN based face detectors on several datasets.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 18:29:57 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Li", "Yuezun", ""], ["Yang", "Xin", ""], ["Wu", "Baoyuan", ""], ["Lyu", "Siwei", ""]]}, {"id": "1906.09300", "submitter": "Sobhan Soleymani", "authors": "Sobhan Soleymani, Ali Dabouei, Jeremy Dawson, and Nasser M. Nasrabadi", "title": "Adversarial Examples to Fool Iris Recognition Systems", "comments": "2019 International Conference on Biometrics (ICB 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples have recently proven to be able to fool deep learning\nmethods by adding carefully crafted small perturbation to the input space\nimage. In this paper, we study the possibility of generating adversarial\nexamples for code-based iris recognition systems. Since generating adversarial\nexamples requires back-propagation of the adversarial loss, conventional filter\nbank-based iris-code generation frameworks cannot be employed in such a setup.\nTherefore, to compensate for this shortcoming, we propose to train a deep\nauto-encoder surrogate network to mimic the conventional iris code generation\nprocedure. This trained surrogate network is then deployed to generate the\nadversarial examples using the iterative gradient sign method algorithm. We\nconsider non-targeted and targeted attacks through three attack scenarios.\nConsidering these attacks, we study the possibility of fooling an iris\nrecognition system in white-box and black-box frameworks.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 19:30:41 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 21:09:35 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Soleymani", "Sobhan", ""], ["Dabouei", "Ali", ""], ["Dawson", "Jeremy", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "1906.09313", "submitter": "Jiawei Chen", "authors": "Jiawei Chen, Janusz Konrad, Prakash Ishwar", "title": "A Cyclically-Trained Adversarial Network for Invariant Representation\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies show that deep neural networks are vulnerable to adversarial\nexamples which can be generated via certain types of transformations. Being\nrobust to a desired family of adversarial attacks is then equivalent to being\ninvariant to a family of transformations. Learning invariant representations\nthen naturally emerges as an important goal to achieve which we explore in this\npaper within specific application contexts. Specifically, we propose a\ncyclically-trained adversarial network to learn a mapping from image space to\nlatent representation space and back such that the latent representation is\ninvariant to a specified factor of variation (e.g., identity). The learned\nmapping assures that the synthesized image is not only realistic, but has the\nsame values for unspecified factors (e.g., pose and illumination) as the\noriginal image and a desired value of the specified factor. Unlike disentangled\nrepresentation learning, which requires two latent spaces, one for specified\nand another for unspecified factors, invariant representation learning needs\nonly one such space. We encourage invariance to a specified factor by applying\nadversarial training using a variational autoencoder in the image space as\nopposed to the latent space. We strengthen this invariance by introducing a\ncyclic training process (forward and backward cycle). We also propose a new\nmethod to evaluate conditional generative networks. It compares how well\ndifferent factors of variation can be predicted from the synthesized, as\nopposed to real, images. In quantitative terms, our approach attains\nstate-of-the-art performance in experiments spanning three datasets with\nfactors such as identity, pose, illumination or style. Our method produces\nsharp, high-quality synthetic images with little visible artefacts compared to\nprevious approaches.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 20:43:16 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 16:14:12 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Chen", "Jiawei", ""], ["Konrad", "Janusz", ""], ["Ishwar", "Prakash", ""]]}, {"id": "1906.09336", "submitter": "Ken C. L. Wong", "authors": "Tanveer Syeda-Mahmood, Hassan M. Ahmad, Nadeem Ansari, Yaniv Gur,\n  Satyananda Kashyap, Alexandros Karargyris, Mehdi Moradi, Anup Pillai, Karthik\n  Sheshadri, Weiting Wang, Ken C. L. Wong, Joy T. Wu", "title": "Building a Benchmark Dataset and Classifiers for Sentence-Level Findings\n  in AP Chest X-rays", "comments": "This paper was accepted by the IEEE International Symposium on\n  Biomedical Imaging (ISBI) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest X-rays are the most common diagnostic exams in emergency rooms and\nhospitals. There has been a surge of work on automatic interpretation of chest\nX-rays using deep learning approaches after the availability of large open\nsource chest X-ray dataset from NIH. However, the labels are not sufficiently\nrich and descriptive for training classification tools. Further, it does not\nadequately address the findings seen in Chest X-rays taken in\nanterior-posterior (AP) view which also depict the placement of devices such as\ncentral vascular lines and tubes. In this paper, we present a new chest X-ray\nbenchmark database of 73 rich sentence-level descriptors of findings seen in AP\nchest X-rays. We describe our method of obtaining these findings through a\nsemi-automated ground truth generation process from crowdsourcing of clinician\nannotations. We also present results of building classifiers for these findings\nthat show that such higher granularity labels can also be learned through the\nframework of deep learning classifiers.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 21:34:57 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Syeda-Mahmood", "Tanveer", ""], ["Ahmad", "Hassan M.", ""], ["Ansari", "Nadeem", ""], ["Gur", "Yaniv", ""], ["Kashyap", "Satyananda", ""], ["Karargyris", "Alexandros", ""], ["Moradi", "Mehdi", ""], ["Pillai", "Anup", ""], ["Sheshadri", "Karthik", ""], ["Wang", "Weiting", ""], ["Wong", "Ken C. L.", ""], ["Wu", "Joy T.", ""]]}, {"id": "1906.09354", "submitter": "Ken C. L. Wong", "authors": "Alexandros Karargyris, Ken C. L. Wong, Joy T. Wu, Mehdi Moradi,\n  Tanveer Syeda-Mahmood", "title": "Boosting the rule-out accuracy of deep disease detection using class\n  weight modifiers", "comments": "This paper was accepted by the IEEE International Symposium on\n  Biomedical Imaging (ISBI) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many screening applications, the primary goal of a radiologist or\nassisting artificial intelligence is to rule out certain findings. The\nclassifiers built for such applications are often trained on large datasets\nthat derive labels from clinical notes written for patients. While the quality\nof the positive findings described in these notes is often reliable, lack of\nthe mention of a finding does not always rule out the presence of it. This\nhappens because radiologists comment on the patient in the context of the exam,\nfor example focusing on trauma as opposed to chronic disease at emergency\nrooms. However, this disease finding ambiguity can affect the performance of\nalgorithms. Hence it is critical to model the ambiguity during training. We\npropose a scheme to apply reasonable class weight modifiers to our loss\nfunction for the no mention cases during training. We experiment with two\ndifferent deep neural network architectures and show that the proposed method\nresults in a large improvement in the performance of the classifiers, specially\non negated findings. The baseline performance of a custom-made dilated block\nnetwork proposed in this paper shows an improvement in comparison with baseline\nDenseNet-201, while both architectures benefit from the new proposed loss\nfunction weighting scheme. Over 200,000 chest X-ray images and three highly\ncommon diseases, along with their negated counterparts, are included in this\nstudy.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 23:55:36 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Karargyris", "Alexandros", ""], ["Wong", "Ken C. L.", ""], ["Wu", "Joy T.", ""], ["Moradi", "Mehdi", ""], ["Syeda-Mahmood", "Tanveer", ""]]}, {"id": "1906.09383", "submitter": "Linchao Zhu", "authors": "Xiaohan Wang, Yu Wu, Linchao Zhu, Yi Yang", "title": "Baidu-UTS Submission to the EPIC-Kitchens Action Recognition Challenge\n  2019", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we present the Baidu-UTS submission to the EPIC-Kitchens\nAction Recognition Challenge in CVPR 2019. This is the winning solution to this\nchallenge. In this task, the goal is to predict verbs, nouns, and actions from\nthe vocabulary for each video segment. The EPIC-Kitchens dataset contains\nvarious small objects, intense motion blur, and occlusions. It is challenging\nto locate and recognize the object that an actor interacts with. To address\nthese problems, we utilize object detection features to guide the training of\n3D Convolutional Neural Networks (CNN), which can significantly improve the\naccuracy of noun prediction. Specifically, we introduce a Gated Feature\nAggregator module to learn from the clip feature and the object feature. This\nmodule can strengthen the interaction between the two kinds of activations and\navoid gradient exploding. Experimental results demonstrate our approach\noutperforms other methods on both seen and unseen test set.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 03:45:41 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Wang", "Xiaohan", ""], ["Wu", "Yu", ""], ["Zhu", "Linchao", ""], ["Yang", "Yi", ""]]}, {"id": "1906.09433", "submitter": "Yinglong Wang", "authors": "Yinglong Wang, Qinfeng Shi, Ehsan Abbasnejad, Chao Ma, Xiaoping Ma,\n  Bing Zeng", "title": "Deep Single Image Deraining Via Estimating Transmission and Atmospheric\n  Light in rainy Scenes", "comments": "10 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rain removal in images/videos is still an important task in computer vision\nfield and attracting attentions of more and more people. Traditional methods\nalways utilize some incomplete priors or filters (e.g. guided filter) to remove\nrain effect. Deep learning gives more probabilities to better solve this task.\nHowever, they remove rain either by evaluating background from rainy image\ndirectly or learning a rain residual first then subtracting the residual to\nobtain a clear background. No other models are used in deep learning based\nde-raining methods to remove rain and obtain other information about rainy\nscenes. In this paper, we utilize an extensively-used image degradation model\nwhich is derived from atmospheric scattering principles to model the formation\nof rainy images and try to learn the transmission, atmospheric light in rainy\nscenes and remove rain further. To reach this goal, we propose a robust\nevaluation method of global atmospheric light in a rainy scene. Instead of\nusing the estimated atmospheric light directly to learn a network to calculate\ntransmission, we utilize it as ground truth and design a simple but novel\ntriangle-shaped network structure to learn atmospheric light for every rainy\nimage, then fine-tune the network to obtain a better estimation of atmospheric\nlight during the training of transmission network. Furthermore, more efficient\nShuffleNet Units are utilized in transmission network to learn transmission map\nand the de-raining image is then obtained by the image degradation model. By\nsubjective and objective comparisons, our method outperforms the selected\nstate-of-the-art works.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 10:58:27 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Wang", "Yinglong", ""], ["Shi", "Qinfeng", ""], ["Abbasnejad", "Ehsan", ""], ["Ma", "Chao", ""], ["Ma", "Xiaoping", ""], ["Zeng", "Bing", ""]]}, {"id": "1906.09447", "submitter": "Yixing Zhu", "authors": "Yixing Zhu, Xueqing Wu, Jun Du", "title": "Adaptive Period Embedding for Representing Oriented Objects in Aerial\n  Images", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2020.2981203", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for representing oriented objects in aerial images\nnamed Adaptive Period Embedding (APE). While traditional object detection\nmethods represent object with horizontal bounding boxes, the objects in aerial\nimages are oritented. Calculating the angle of object is an yet challenging\ntask. While almost all previous object detectors for aerial images directly\nregress the angle of objects, they use complex rules to calculate the angle,\nand their performance is limited by the rule design. In contrast, our method is\nbased on the angular periodicity of oriented objects. The angle is represented\nby two two-dimensional periodic vectors whose periods are different, the vector\nis continuous as shape changes. The label generation rule is more simple and\nreasonable compared with previous methods. The proposed method is general and\ncan be applied to other oriented detector. Besides, we propose a novel IoU\ncalculation method for long objects named length independent IoU (LIIoU). We\nintercept part of the long side of the target box to get the maximum IoU\nbetween the proposed box and the intercepted target box. Thereby, some long\nboxes will have corresponding positive samples. Our method reaches the 1st\nplace of DOAI2019 competition task1 (oriented object) held in workshop on\nDetecting Objects in Aerial Images in conjunction with IEEE CVPR 2019.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 13:40:43 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Zhu", "Yixing", ""], ["Wu", "Xueqing", ""], ["Du", "Jun", ""]]}, {"id": "1906.09449", "submitter": "Dawid Rymarczyk", "authors": "Bartosz Zieli\\'nski, Agnieszka Sroka-Oleksiak, Dawid Rymarczyk, Adam\n  Piekarczyk, Monika Brzychczy-W{\\l}och", "title": "Deep learning approach to description and classification of fungi\n  microscopic images", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0234806", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diagnosis of fungal infections can rely on microscopic examination, however,\nin many cases, it does not allow unambiguous identification of the species due\nto their visual similarity. Therefore, it is usually necessary to use\nadditional biochemical tests. That involves additional costs and extends the\nidentification process up to 10 days. Such a delay in the implementation of\ntargeted treatment is grave in consequences as the mortality rate for\nimmunosuppressed patients is high. In this paper, we apply machine learning\napproach based on deep learning and bag-of-words to classify microscopic images\nof various fungi species. Our approach makes the last stage of biochemical\nidentification redundant, shortening the identification process by 2-3 days and\nreducing the cost of the diagnostic examination.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 14:00:51 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 11:24:49 GMT"}, {"version": "v3", "created": "Tue, 28 Jan 2020 14:30:00 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Zieli\u0144ski", "Bartosz", ""], ["Sroka-Oleksiak", "Agnieszka", ""], ["Rymarczyk", "Dawid", ""], ["Piekarczyk", "Adam", ""], ["Brzychczy-W\u0142och", "Monika", ""]]}, {"id": "1906.09453", "submitter": "Dimitris Tsipras", "authors": "Shibani Santurkar, Dimitris Tsipras, Brandon Tran, Andrew Ilyas, Logan\n  Engstrom, Aleksander Madry", "title": "Image Synthesis with a Single (Robust) Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the basic classification framework alone can be used to tackle\nsome of the most challenging tasks in image synthesis. In contrast to other\nstate-of-the-art approaches, the toolkit we develop is rather minimal: it uses\na single, off-the-shelf classifier for all these tasks. The crux of our\napproach is that we train this classifier to be adversarially robust. It turns\nout that adversarial robustness is precisely what we need to directly\nmanipulate salient features of the input. Overall, our findings demonstrate the\nutility of robustness in the broader machine learning context. Code and models\nfor our experiments can be found at https://git.io/robust-apps.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 09:12:08 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 15:47:42 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Santurkar", "Shibani", ""], ["Tsipras", "Dimitris", ""], ["Tran", "Brandon", ""], ["Ilyas", "Andrew", ""], ["Engstrom", "Logan", ""], ["Madry", "Aleksander", ""]]}, {"id": "1906.09472", "submitter": "Radim Spetlik", "authors": "Radim Spetlik, Ivan Razumenic", "title": "Iris Verification with Convolutional Neural Network and Unit-Circle\n  Layer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel convolutional neural network to verify a~match between two\nnormalized images of the human iris. The network is trained end-to-end and\nvalidated on three publicly available datasets yielding state-of-the-art\nresults against four baseline methods. The network performs better by a 10%\nmargin to the state-of-the-art method on the CASIA.v4 dataset. In the network,\nwe use a novel Unit-Circle Layer layer which replaces the Gabor-filtering step\nin a common iris-verification pipeline. We show that the layer improves the\nperformance of the model up to 15% on previously-unseen data.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 17:11:15 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 14:57:36 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Spetlik", "Radim", ""], ["Razumenic", "Ivan", ""]]}, {"id": "1906.09513", "submitter": "Kelly Lais Wiggers", "authors": "Kelly L. Wiggers and Alceu S. Britto Jr. and Laurent Heutte and\n  Alessandro L. Koerich and Luiz S. Oliveira", "title": "Image Retrieval and Pattern Spotting using Siamese Neural Network", "comments": "Accepted for IJCNN 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for image retrieval and pattern spotting\nin document image collections. The manual feature engineering is avoided by\nlearning a similarity-based representation using a Siamese Neural Network\ntrained on a previously prepared subset of image pairs from the ImageNet\ndataset. The learned representation is used to provide the similarity-based\nfeature maps used to find relevant image candidates in the data collection\ngiven an image query. A robust experimental protocol based on the public\nTobacco800 document image collection shows that the proposed method compares\nfavorably against state-of-the-art document image retrieval methods, reaching\n0.94 and 0.83 of mean average precision (mAP) for retrieval and pattern\nspotting (IoU=0.7), respectively. Besides, we have evaluated the proposed\nmethod considering feature maps of different sizes, showing the impact of\nreducing the number of features in the retrieval performance and\ntime-consuming.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 22:33:44 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Wiggers", "Kelly L.", ""], ["Britto", "Alceu S.", "Jr."], ["Heutte", "Laurent", ""], ["Koerich", "Alessandro L.", ""], ["Oliveira", "Luiz S.", ""]]}, {"id": "1906.09529", "submitter": "Mohit Goyal", "authors": "Mohit Goyal, Rajan Goyal, Brejesh Lall", "title": "Learning Activation Functions: A new paradigm for understanding Neural\n  Networks", "comments": "A modified version of the article has been published in IEEE WCCI\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scope of research in the domain of activation functions remains limited\nand centered around improving the ease of optimization or generalization\nquality of neural networks (NNs). However, to develop a deeper understanding of\ndeep learning, it becomes important to look at the non linear component of NNs\nmore carefully. In this paper, we aim to provide a generic form of activation\nfunction along with appropriate mathematical grounding so as to allow for\ninsights into the working of NNs in future. We propose \"Self-Learnable\nActivation Functions\" (SLAF), which are learned during training and are capable\nof approximating most of the existing activation functions. SLAF is given as a\nweighted sum of pre-defined basis elements which can serve for a good\napproximation of the optimal activation function. The coefficients for these\nbasis elements allow a search in the entire space of continuous functions\n(consisting of all the conventional activations). We propose various training\nroutines which can be used to achieve performance with SLAF equipped neural\nnetworks (SLNNs). We prove that SLNNs can approximate any neural network with\nlipschitz continuous activations, to any arbitrary error highlighting their\ncapacity and possible equivalence with standard NNs. Also, SLNNs can be\ncompletely represented as a collections of finite degree polynomial upto the\nvery last layer obviating several hyper parameters like width and depth. Since\nthe optimization of SLNNs is still a challenge, we show that using SLAF along\nwith standard activations (like ReLU) can provide performance improvements with\nonly a small increase in number of parameters.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2019 01:54:36 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 14:56:10 GMT"}, {"version": "v3", "created": "Wed, 9 Dec 2020 04:13:25 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Goyal", "Mohit", ""], ["Goyal", "Rajan", ""], ["Lall", "Brejesh", ""]]}, {"id": "1906.09540", "submitter": "Yuyin Zhou", "authors": "Yuyin Zhou, David Dreizin, Yingwei Li, Zhishuai Zhang, Yan Wang, Alan\n  Yuille", "title": "Multi-Scale Attentional Network for Multi-Focal Segmentation of Active\n  Bleed after Pelvic Fractures", "comments": "To appear in MICCAI-MLMI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trauma is the worldwide leading cause of death and disability in those\nyounger than 45 years, and pelvic fractures are a major source of morbidity and\nmortality. Automated segmentation of multiple foci of arterial bleeding from\nabdominopelvic trauma CT could provide rapid objective measurements of the\ntotal extent of active bleeding, potentially augmenting outcome prediction at\nthe point of care, while improving patient triage, allocation of appropriate\nresources, and time to definitive intervention. In spite of the importance of\nactive bleeding in the quick tempo of trauma care, the task is still quite\nchallenging due to the variable contrast, intensity, location, size, shape, and\nmultiplicity of bleeding foci. Existing work [4] presents a heuristic\nrule-based segmentation technique which requires multiple stages and cannot be\nefficiently optimized end-to-end. To this end, we present, Multi-Scale\nAttentional Network (MSAN), the first yet reliable end-to-end network, for\nautomated segmentation of active hemorrhage from contrast-enhanced trauma CT\nscans. MSAN consists of the following components: 1) an encoder which fully\nintegrates the global contextual information from holistic 2D slices; 2) a\nmulti-scale strategy applied both in the training stage and the inference stage\nto handle the challenges induced by variation of target sizes; 3) an\nattentional module to further refine the deep features, leading to better\nsegmentation quality; and 4) a multi-view mechanism to fully leverage the 3D\ninformation. Our MSAN reports a significant improvement of more than 7%\ncompared to prior arts in terms of DSC.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2019 02:40:20 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 01:02:39 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Zhou", "Yuyin", ""], ["Dreizin", "David", ""], ["Li", "Yingwei", ""], ["Zhang", "Zhishuai", ""], ["Wang", "Yan", ""], ["Yuille", "Alan", ""]]}, {"id": "1906.09549", "submitter": "Yuankai Huo", "authors": "Yuankai Huo, James G. Terry, Jiachen Wang, Sangeeta Nair, Thomas A.\n  Lasko, Barry I. Freedman, J. Jeffery Carr, Bennett A. Landman", "title": "Fully Automatic Liver Attenuation Estimation Combing CNN Segmentation\n  and Morphological Operations", "comments": "Medical Physics", "journal-ref": null, "doi": "10.1002/mp.13675", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manually tracing regions of interest (ROIs) within the liver is the de facto\nstandard method for measuring liver attenuation on computed tomography (CT) in\ndiagnosing nonalcoholic fatty liver disease (NAFLD). However, manual tracing is\nresource intensive. To address these limitations and to expand the availability\nof a quantitative CT measure of hepatic steatosis, we propose the automatic\nliver attenuation ROI-based measurement (ALARM) method for automated liver\nattenuation estimation. The ALARM method consists of two major stages: (1) deep\nconvolutional neural network (DCNN)-based liver segmentation and (2) automated\nROI extraction. First, liver segmentation was achieved using our previously\ndeveloped SS-Net. Then, a single central ROI (center-ROI) and three circles ROI\n(periphery-ROI) were computed based on liver segmentation and morphological\noperations. The ALARM method is available as an open source Docker container\n(https://github.com/MASILab/ALARM).246 subjects with 738 abdomen CT scans from\nthe African American-Diabetes Heart Study (AA-DHS) were used for external\nvalidation (testing), independent from the training and validation cohort (100\nclinically acquired CT abdominal scans).\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2019 04:00:32 GMT"}, {"version": "v2", "created": "Sat, 29 Jun 2019 16:48:26 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Huo", "Yuankai", ""], ["Terry", "James G.", ""], ["Wang", "Jiachen", ""], ["Nair", "Sangeeta", ""], ["Lasko", "Thomas A.", ""], ["Freedman", "Barry I.", ""], ["Carr", "J. Jeffery", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1906.09551", "submitter": "Zhilu Zhang", "authors": "Zhilu Zhang, Adrian V. Dalca, Mert R. Sabuncu", "title": "Confidence Calibration for Convolutional Neural Networks Using\n  Structured Dropout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In classification applications, we often want probabilistic predictions to\nreflect confidence or uncertainty. Dropout, a commonly used training technique,\nhas recently been linked to Bayesian inference, yielding an efficient way to\nquantify uncertainty in neural network models. However, as previously\ndemonstrated, confidence estimates computed with a naive implementation of\ndropout can be poorly calibrated, particularly when using convolutional\nnetworks. In this paper, through the lens of ensemble learning, we associate\ncalibration error with the correlation between the models sampled with dropout.\nMotivated by this, we explore the use of structured dropout to promote model\ndiversity and improve confidence calibration. We use the SVHN, CIFAR-10 and\nCIFAR-100 datasets to empirically compare model diversity and confidence errors\nobtained using various dropout techniques. We also show the merit of structured\ndropout in a Bayesian active learning application.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2019 04:34:14 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Zhang", "Zhilu", ""], ["Dalca", "Adrian V.", ""], ["Sabuncu", "Mert R.", ""]]}, {"id": "1906.09587", "submitter": "Nagender Aneja", "authors": "Amit Kumar Jaiswal and Ivan Panshin and Dimitrij Shulkin and Nagender\n  Aneja and Samuel Abramov", "title": "Semi-Supervised Learning for Cancer Detection of Lymph Node Metastases", "comments": "Accepted in CVPR 2019 Workshop Towards Causal, Explainable and\n  Universal Medical Visual Diagnosis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pathologists find tedious to examine the status of the sentinel lymph node on\na large number of pathological scans. The examination process of such lymph\nnode which encompasses metastasized cancer cells is histopathologically\norganized. However, the task of finding metastatic tissues is gradual which is\noften challenging. In this work, we present our deep convolutional neural\nnetwork based model validated on PatchCamelyon (PCam) benchmark dataset for\nfundamental machine learning research in histopathology diagnosis. We find that\nour proposed model trained with a semi-supervised learning approach by using\npseudo labels on PCam-level significantly leads to better performances to\nstrong CNN baseline on the AUC metric.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2019 11:54:53 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Jaiswal", "Amit Kumar", ""], ["Panshin", "Ivan", ""], ["Shulkin", "Dimitrij", ""], ["Aneja", "Nagender", ""], ["Abramov", "Samuel", ""]]}, {"id": "1906.09607", "submitter": "Jiemin Fang", "authors": "Jiemin Fang, Yuzhu Sun, Qian Zhang, Yuan Li, Wenyu Liu, Xinggang Wang", "title": "Densely Connected Search Space for More Flexible Neural Architecture\n  Search", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) has dramatically advanced the development of\nneural network design. We revisit the search space design in most previous NAS\nmethods and find the number and widths of blocks are set manually. However,\nblock counts and block widths determine the network scale (depth and width) and\nmake a great influence on both the accuracy and the model cost (FLOPs/latency).\nIn this paper, we propose to search block counts and block widths by designing\na densely connected search space, i.e., DenseNAS. The new search space is\nrepresented as a dense super network, which is built upon our designed routing\nblocks. In the super network, routing blocks are densely connected and we\nsearch for the best path between them to derive the final architecture. We\nfurther propose a chained cost estimation algorithm to approximate the model\ncost during the search. Both the accuracy and model cost are optimized in\nDenseNAS. For experiments on the MobileNetV2-based search space, DenseNAS\nachieves 75.3% top-1 accuracy on ImageNet with only 361MB FLOPs and 17.9ms\nlatency on a single TITAN-XP. The larger model searched by DenseNAS achieves\n76.1% accuracy with only 479M FLOPs. DenseNAS further promotes the ImageNet\nclassification accuracies of ResNet-18, -34 and -50-B by 1.5%, 0.5% and 0.3%\nwith 200M, 600M and 680M FLOPs reduction respectively. The related code is\navailable at https://github.com/JaminFong/DenseNAS.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2019 16:49:40 GMT"}, {"version": "v2", "created": "Sun, 24 Nov 2019 07:17:22 GMT"}, {"version": "v3", "created": "Thu, 9 Apr 2020 07:58:17 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Fang", "Jiemin", ""], ["Sun", "Yuzhu", ""], ["Zhang", "Qian", ""], ["Li", "Yuan", ""], ["Liu", "Wenyu", ""], ["Wang", "Xinggang", ""]]}, {"id": "1906.09610", "submitter": "Kai Niu", "authors": "Kai Niu, Yan Huang, Wanli Ouyang, Liang Wang", "title": "Improving Description-based Person Re-identification by\n  Multi-granularity Image-text Alignments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Description-based person re-identification (Re-id) is an important task in\nvideo surveillance that requires discriminative cross-modal representations to\ndistinguish different people. It is difficult to directly measure the\nsimilarity between images and descriptions due to the modality heterogeneity\n(the cross-modal problem). And all samples belonging to a single category (the\nfine-grained problem) makes this task even harder than the conventional\nimage-description matching task. In this paper, we propose a Multi-granularity\nImage-text Alignments (MIA) model to alleviate the cross-modal fine-grained\nproblem for better similarity evaluation in description-based person Re-id.\nSpecifically, three different granularities, i.e., global-global, global-local\nand local-local alignments are carried out hierarchically. Firstly, the\nglobal-global alignment in the Global Contrast (GC) module is for matching the\nglobal contexts of images and descriptions. Secondly, the global-local\nalignment employs the potential relations between local components and global\ncontexts to highlight the distinguishable components while eliminating the\nuninvolved ones adaptively in the Relation-guided Global-local Alignment (RGA)\nmodule. Thirdly, as for the local-local alignment, we match visual human parts\nwith noun phrases in the Bi-directional Fine-grained Matching (BFM) module. The\nwhole network combining multiple granularities can be end-to-end trained\nwithout complex pre-processing. To address the difficulties in training the\ncombination of multiple granularities, an effective step training strategy is\nproposed to train these granularities step-by-step. Extensive experiments and\nanalysis have shown that our method obtains the state-of-the-art performance on\nthe CUHK-PEDES dataset and outperforms the previous methods by a significant\nmargin.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2019 17:06:45 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Niu", "Kai", ""], ["Huang", "Yan", ""], ["Ouyang", "Wanli", ""], ["Wang", "Liang", ""]]}, {"id": "1906.09631", "submitter": "Jakub Nalepa", "authors": "Jakub Nalepa, Michal Myller, Michal Kawulok", "title": "Transfer Learning for Segmenting Dimensionally-Reduced Hyperspectral\n  Images", "comments": "Submitted to IEEE Geoscience and Remote Sensing Letters", "journal-ref": null, "doi": "10.1109/LGRS.2019.2942832", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has established the state of the art in multiple fields,\nincluding hyperspectral image analysis. However, training large-capacity\nlearners to segment such imagery requires representative training sets.\nAcquiring such data is human-dependent and time-consuming, especially in Earth\nobservation scenarios, where the hyperspectral data transfer is very costly and\ntime-constrained. In this letter, we show how to effectively deal with a\nlimited number and size of available hyperspectral ground-truth sets, and apply\ntransfer learning for building deep feature extractors. Also, we exploit\nspectral dimensionality reduction to make our technique applicable over\nhyperspectral data acquired using different sensors, which may capture\ndifferent numbers of hyperspectral bands. The experiments, performed over\nseveral benchmarks and backed up with statistical tests, indicated that our\napproach allows us to effectively train well-generalizing deep convolutional\nneural nets even using significantly reduced data.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2019 19:11:26 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Nalepa", "Jakub", ""], ["Myller", "Michal", ""], ["Kawulok", "Michal", ""]]}, {"id": "1906.09666", "submitter": "Ali Moghimi", "authors": "Ali Moghimi, Ce Yang, James A. Anderson", "title": "Aerial hyperspectral imagery and deep neural networks for\n  high-throughput yield phenotyping in wheat", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Crop production needs to increase in a sustainable manner to meet the growing\nglobal demand for food. To identify crop varieties with high yield potential,\nplant scientists and breeders evaluate the performance of hundreds of lines in\nmultiple locations over several years. To facilitate the process of selecting\nadvanced varieties, an automated framework was developed in this study. A\nhyperspectral camera was mounted on an unmanned aerial vehicle to collect\naerial imagery with high spatial and spectral resolution. Aerial images were\ncaptured in two consecutive growing seasons from three experimental yield\nfields composed of hundreds experimental plots (1x2.4 meter), each contained a\nsingle wheat line. The grain of more than thousand wheat plots was harvested by\na combine, weighed, and recorded as the ground truth data. To leverage the high\nspatial resolution and investigate the yield variation within the plots, images\nof plots were divided into sub-plots by integrating image processing techniques\nand spectral mixture analysis with the expert domain knowledge. Afterwards, the\nsub-plot dataset was divided into train, validation, and test sets using\nstratified sampling. Subsequent to extracting features from each sub-plot, deep\nneural networks were trained for yield estimation. The coefficient of\ndetermination for predicting the yield of the test dataset at sub-plot scale\nwas 0.79 with root mean square error of 5.90 grams. In addition to providing\ninsights into yield variation at sub-plot scale, the proposed framework can\nfacilitate the process of high-throughput yield phenotyping as a valuable\ndecision support tool. It offers the possibility of (i) remote visual\ninspection of the plots, (ii) studying the effect of crop density on yield, and\n(iii) optimizing plot size to investigate more lines in a dedicated field each\nyear.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2019 22:48:08 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Moghimi", "Ali", ""], ["Yang", "Ce", ""], ["Anderson", "James A.", ""]]}, {"id": "1906.09676", "submitter": "Arnold Wiliem", "authors": "Sam Maksoud, Arnold Wiliem, Kun Zhao, Teng Zhang, Lin Wu and Brian C.\n  Lovell", "title": "CORAL8: Concurrent Object Regression for Area Localization in Medical\n  Image Panels", "comments": "Accepted for MICCAI 2019", "journal-ref": null, "doi": "10.1007/978-3-030-32239-7_48", "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work tackles the problem of generating a medical report for multi-image\npanels. We apply our solution to the Renal Direct Immunofluorescence (RDIF)\nassay which requires a pathologist to generate a report based on observations\nacross the eight different WSI in concert with existing clinical features. To\nthis end, we propose a novel attention-based multi-modal generative recurrent\nneural network (RNN) architecture capable of dynamically sampling image data\nconcurrently across the RDIF panel. The proposed methodology incorporates text\nfrom the clinical notes of the requesting physician to regulate the output of\nthe network to align with the overall clinical context. In addition, we found\nthe importance of regularizing the attention weights for word generation\nprocesses. This is because the system can ignore the attention mechanism by\nassigning equal weights for all members. Thus, we propose two regularizations\nwhich force the system to utilize the attention mechanism. Experiments on our\nnovel collection of RDIF WSIs provided by a large clinical laboratory\ndemonstrate that our framework offers significant improvements over existing\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 00:30:32 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Maksoud", "Sam", ""], ["Wiliem", "Arnold", ""], ["Zhao", "Kun", ""], ["Zhang", "Teng", ""], ["Wu", "Lin", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1906.09677", "submitter": "Lucas Jaffe", "authors": "Lucas Jaffe, Michael Zelinski, and Wesam Sakla", "title": "Remote Sensor Design for Visual Recognition with Convolutional Neural\n  Networks", "comments": "Accepted for publication in IEEE Transactions on Geoscience and\n  Remote Sensing", "journal-ref": null, "doi": "10.1109/TGRS.2019.2925813", "report-no": "LLNL-JRNL-760588", "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning technologies for computer vision have developed rapidly\nsince 2012, modeling of remote sensing systems has remained focused around\nhuman vision. In particular, remote sensing systems are usually constructed to\noptimize sensing cost-quality trade-offs with respect to human image\ninterpretability. While some recent studies have explored remote sensing system\ndesign as a function of simple computer vision algorithm performance, there has\nbeen little work relating this design to the state-of-the-art in computer\nvision: deep learning with convolutional neural networks. We develop\nexperimental systems to conduct this analysis, showing results with modern deep\nlearning algorithms and recent overhead image data. Our results are compared to\nstandard image quality measurements based on human visual perception, and we\nconclude not only that machine and human interpretability differ significantly,\nbut that computer vision performance is largely self-consistent across a range\nof disparate conditions. This research is presented as a cornerstone for a new\ngeneration of sensor design systems which focus on computer algorithm\nperformance instead of human visual perception.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 00:43:32 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Jaffe", "Lucas", ""], ["Zelinski", "Michael", ""], ["Sakla", "Wesam", ""]]}, {"id": "1906.09681", "submitter": "Meng Li", "authors": "Meng Li, Lin Wu, Arnold Wiliem, Kun Zhao, Teng Zhang and Brian C.\n  Lovell", "title": "Deep Instance-Level Hard Negative Mining Model for Histopathology Images", "comments": "Accepted by MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histopathology image analysis can be considered as a Multiple instance\nlearning (MIL) problem, where the whole slide histopathology image (WSI) is\nregarded as a bag of instances (i.e, patches) and the task is to predict a\nsingle class label to the WSI. However, in many real-life applications such as\ncomputational pathology, discovering the key instances that trigger the bag\nlabel is of great interest because it provides reasons for the decision made by\nthe system. In this paper, we propose a deep convolutional neural network (CNN)\nmodel that addresses the primary task of a bag classification on a WSI and also\nlearns to identify the response of each instance to provide interpretable\nresults to the final prediction. We incorporate the attention mechanism into\nthe proposed model to operate the transformation of instances and learn\nattention weights to allow us to find key patches. To perform a balanced\ntraining, we introduce adaptive weighing in each training bag to explicitly\nadjust the weight distribution in order to concentrate more on the contribution\nof hard samples. Based on the learned attention weights, we further develop a\nsolution to boost the classification performance by generating the bags with\nhard negative instances. We conduct extensive experiments on colon and breast\ncancer histopathology data and show that our framework achieves\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 01:00:05 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 00:35:51 GMT"}, {"version": "v3", "created": "Thu, 27 Jun 2019 02:00:32 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Li", "Meng", ""], ["Wu", "Lin", ""], ["Wiliem", "Arnold", ""], ["Zhao", "Kun", ""], ["Zhang", "Teng", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1906.09684", "submitter": "Yalong Liu", "authors": "Yalong Liu, Jie Li, Ying Wang, Miaomiao Wang, Xianjun Li, Zhicheng\n  Jiao, Jian Yang, and Xingbo Gao", "title": "Refined-Segmentation R-CNN: A Two-stage Convolutional Neural Network for\n  Punctate White Matter Lesion Segmentation in Preterm Infants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmentation of punctate white matter lesion (PWML) in infantile\nbrains by an automatic algorithm can reduce the potential risk of postnatal\ndevelopment. How to segment PWML effectively has become one of the active\ntopics in medical image segmentation in recent years. In this paper, we\nconstruct an efficient two-stage PWML semantic segmentation network based on\nthe characteristics of the lesion, called refined segmentation R-CNN (RS RCNN).\nWe propose a heuristic RPN (H-RPN) which can utilize surrounding information\naround the PWMLs for heuristic segmentation. Also, we design a lightweight\nsegmentation network to segment the lesion in a fast way. Densely connected\nconditional random field (DCRF) is used to optimize the segmentation results.\nWe only use T1w MRIs to segment PWMLs. The result shows that our model can well\nsegment the lesion of ordinary size or even pixel size. The Dice similarity\ncoefficient reaches 0.6616, the sensitivity is 0.7069, the specificity is\n0.9997, and the Hausdorff distance is 52.9130. The proposed method outperforms\nthe state-of-the-art algorithm. (The code of this paper is available on\nhttps://github.com/YalongLiu/Refined-Segmentation-R-CNN)\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 01:19:24 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 02:25:57 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Liu", "Yalong", ""], ["Li", "Jie", ""], ["Wang", "Ying", ""], ["Wang", "Miaomiao", ""], ["Li", "Xianjun", ""], ["Jiao", "Zhicheng", ""], ["Yang", "Jian", ""], ["Gao", "Xingbo", ""]]}, {"id": "1906.09693", "submitter": "Jun Wen", "authors": "Jun Wen, Nenggan Zheng, Junsong Yuan, Zhefeng Gong, Changyou Chen", "title": "Bayesian Uncertainty Matching for Unsupervised Domain Adaptation", "comments": "IJCAI-2019 Accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation is an important technique to alleviate performance\ndegradation caused by domain shift, e.g., when training and test data come from\ndifferent domains. Most existing deep adaptation methods focus on reducing\ndomain shift by matching marginal feature distributions through deep\ntransformations on the input features, due to the unavailability of target\ndomain labels. We show that domain shift may still exist via label distribution\nshift at the classifier, thus deteriorating model performances. To alleviate\nthis issue, we propose an approximate joint distribution matching scheme by\nexploiting prediction uncertainty. Specifically, we use a Bayesian neural\nnetwork to quantify prediction uncertainty of a classifier. By imposing\ndistribution matching on both features and labels (via uncertainty), label\ndistribution mismatching in source and target data is effectively alleviated,\nencouraging the classifier to produce consistent predictions across domains. We\nalso propose a few techniques to improve our method by adaptively reweighting\ndomain adaptation loss to achieve nontrivial distribution matching and stable\ntraining. Comparisons with state of the art unsupervised domain adaptation\nmethods on three popular benchmark datasets demonstrate the superiority of our\napproach, especially on the effectiveness of alleviating negative transfer.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 02:57:22 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Wen", "Jun", ""], ["Zheng", "Nenggan", ""], ["Yuan", "Junsong", ""], ["Gong", "Zhefeng", ""], ["Chen", "Changyou", ""]]}, {"id": "1906.09707", "submitter": "Hao Liu", "authors": "Feng Dai, Hao Liu, Yike Ma, Juan Cao, Qiang Zhao, Yongdong Zhang", "title": "Dense Scale Network for Crowd Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting has been widely studied by computer vision community in recent\nyears. Due to the large scale variation, it remains to be a challenging task.\nPrevious methods adopt either multi-column CNN or single-column CNN with\nmultiple branches to deal with this problem. However, restricted by the number\nof columns or branches, these methods can only capture a few different scales\nand have limited capability. In this paper, we propose a simple but effective\nnetwork called DSNet for crowd counting, which can be easily trained in an\nend-to-end fashion. The key component of our network is the dense dilated\nconvolution block, in which each dilation layer is densely connected with the\nothers to preserve information from continuously varied scales. The dilation\nrates in dilation layers are carefully selected to prevent the block from\ngridding artifacts. To further enlarge the range of scales covered by the\nnetwork, we cascade three blocks and link them with dense residual connections.\nWe also introduce a novel multi-scale density level consistency loss for\nperformance improvement. To evaluate our method, we compare it with\nstate-of-the-art algorithms on four crowd counting datasets (ShanghaiTech,\nUCF-QNRF, UCF_CC_50 and UCSD). Experimental results demonstrate that DSNet can\nachieve the best performance and make significant improvements on all the four\ndatasets (30% on the UCF-QNRF and UCF_CC_50, and 20% on the others).\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 03:33:35 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Dai", "Feng", ""], ["Liu", "Hao", ""], ["Ma", "Yike", ""], ["Cao", "Juan", ""], ["Zhao", "Qiang", ""], ["Zhang", "Yongdong", ""]]}, {"id": "1906.09726", "submitter": "Bhavya Ajani", "authors": "Bhavya Ajani", "title": "Automatic Intracranial Brain Segmentation from Computed Tomography Head\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and automatic algorithm to segment Brain (intracranial region) from\ncomputed tomography (CT) head images using combination of HU thresholding,\nidentification of intracranial voxels through ray intersection with cranium,\nspecial binary erosion and connected components per slice.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 09:29:27 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Ajani", "Bhavya", ""]]}, {"id": "1906.09739", "submitter": "Hideki Oki", "authors": "Hideki Oki and Takio Kurita", "title": "Mixup of Feature Maps in a Hidden Layer for Training of Convolutional\n  Neural Network", "comments": "11 pages, 5 figures", "journal-ref": "Neural Information Processing 25th International Conference\n  (ICONIP2018) Proceedings Part II", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep Convolutional Neural Network (CNN) became very popular as a\nfundamental technique for image classification and objects recognition. To\nimprove the recognition accuracy for the more complex tasks, deeper networks\nhave being introduced. However, the recognition accuracy of the trained deep\nCNN drastically decreases for the samples which are obtained from the outside\nregions of the training samples. To improve the generalization ability for such\nsamples, Krizhevsky et al. proposed to generate additional samples through\ntransformations from the existing samples and to make the training samples\nricher. This method is known as data augmentation. Hongyi Zhang et al.\nintroduced data augmentation method called mixup which achieves\nstate-of-the-art performance in various datasets. Mixup generates new samples\nby mixing two different training samples. Mixing of the two images is\nimplemented with simple image morphing. In this paper, we propose to apply\nmixup to the feature maps in a hidden layer. To implement the mixup in the\nhidden layer we use the Siamese network or the triplet network architecture to\nmix feature maps. From the experimental comparison, it is observed that the\nmixup of the feature maps obtained from the first convolution layer is more\neffective than the original image mixup.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 06:10:17 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Oki", "Hideki", ""], ["Kurita", "Takio", ""]]}, {"id": "1906.09744", "submitter": "Ye Zhu PhD", "authors": "Ye Zhu, Kai Ming Ting", "title": "Improving the Effectiveness and Efficiency of Stochastic Neighbour\n  Embedding with Isolation Kernel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new insight into improving the performance of\nStochastic Neighbour Embedding (t-SNE) by using Isolation kernel instead of\nGaussian kernel. Isolation kernel outperforms Gaussian kernel in two aspects.\nFirst, the use of Isolation kernel in t-SNE overcomes the drawback of\nmisrepresenting some structures in the data, which often occurs when Gaussian\nkernel is applied in t-SNE. This is because Gaussian kernel determines each\nlocal bandwidth based on one local point only, while Isolation kernel is\nderived directly from the data based on space partitioning. Second, the use of\nIsolation kernel yields a more efficient similarity computation because\ndata-dependent Isolation kernel has only one parameter that needs to be tuned.\nIn contrast, the use of data-independent Gaussian kernel increases the\ncomputational cost by determining n bandwidths for a dataset of n points. As\nthe root cause of these deficiencies in t-SNE is Gaussian kernel, we show that\nsimply replacing Gaussian kernel with Isolation kernel in t-SNE significantly\nimproves the quality of the final visualisation output (without creating\nmisrepresented structures) and removes one key obstacle that prevents t-SNE\nfrom processing large datasets. Moreover, Isolation kernel enables t-SNE to\ndeal with large-scale datasets in less runtime without trading off accuracy,\nunlike existing methods in speeding up t-SNE.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 06:49:04 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 03:34:10 GMT"}, {"version": "v3", "created": "Thu, 8 Jul 2021 04:20:20 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Zhu", "Ye", ""], ["Ting", "Kai Ming", ""]]}, {"id": "1906.09748", "submitter": "Shunan Mao", "authors": "Shunan Mao, Shiliang Zhang and Ming Yang", "title": "Resolution-invariant Person Re-Identification", "comments": "IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting resolution invariant representation is critical for person\nRe-Identification (ReID) in real applications, where the resolutions of\ncaptured person images may vary dramatically. This paper learns person\nrepresentations robust to resolution variance through jointly training a\nForeground-Focus Super-Resolution (FFSR) module and a Resolution-Invariant\nFeature Extractor (RIFE) by end-to-end CNN learning. FFSR upscales the person\nforeground using a fully convolutional auto-encoder with skip connections\nlearned with a foreground focus training loss. RIFE adopts two feature\nextraction streams weighted by a dual-attention block to learn features for low\nand high resolution images, respectively. These two complementary modules are\njointly trained, leading to a strong resolution invariant representation. We\nevaluate our methods on five datasets containing person images at a large range\nof resolutions, where our methods show substantial superiority to existing\nsolutions. For instance, we achieve Rank-1 accuracy of 36.4% and 73.3% on\nCAVIAR and MLR-CUHK03, outperforming the state-of-the art by 2.9% and 2.6%,\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 07:03:20 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 06:28:02 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Mao", "Shunan", ""], ["Zhang", "Shiliang", ""], ["Yang", "Ming", ""]]}, {"id": "1906.09756", "submitter": "Zhaowei Cai", "authors": "Zhaowei Cai, Nuno Vasconcelos", "title": "Cascade R-CNN: High Quality Object Detection and Instance Segmentation", "comments": "extension of arXiv:1712.00726 \"Cascade R-CNN: Delving into High\n  Quality Object Detection\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In object detection, the intersection over union (IoU) threshold is\nfrequently used to define positives/negatives. The threshold used to train a\ndetector defines its \\textit{quality}. While the commonly used threshold of 0.5\nleads to noisy (low-quality) detections, detection performance frequently\ndegrades for larger thresholds. This paradox of high-quality detection has two\ncauses: 1) overfitting, due to vanishing positive samples for large thresholds,\nand 2) inference-time quality mismatch between detector and test hypotheses. A\nmulti-stage object detection architecture, the Cascade R-CNN, composed of a\nsequence of detectors trained with increasing IoU thresholds, is proposed to\naddress these problems. The detectors are trained sequentially, using the\noutput of a detector as training set for the next. This resampling\nprogressively improves hypotheses quality, guaranteeing a positive training set\nof equivalent size for all detectors and minimizing overfitting. The same\ncascade is applied at inference, to eliminate quality mismatches between\nhypotheses and detectors. An implementation of the Cascade R-CNN without bells\nor whistles achieves state-of-the-art performance on the COCO dataset, and\nsignificantly improves high-quality detection on generic and specific object\ndetection datasets, including VOC, KITTI, CityPerson, and WiderFace. Finally,\nthe Cascade R-CNN is generalized to instance segmentation, with nontrivial\nimprovements over the Mask R-CNN. To facilitate future research, two\nimplementations are made available at\n\\url{https://github.com/zhaoweicai/cascade-rcnn} (Caffe) and\n\\url{https://github.com/zhaoweicai/Detectron-Cascade-RCNN} (Detectron).\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 07:22:28 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Cai", "Zhaowei", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "1906.09763", "submitter": "Moti Freiman", "authors": "Moti Freiman, Hannes Nickisch, Sven Prevrhal, Holger Schmitt, Mani\n  Vembar, P\\'al Maurovich-Horvat, Patrick Donnelly, and Liran Goshen", "title": "Improving CCTA based lesions' hemodynamic significance assessment by\n  accounting for partial volume modeling in automatic coronary lumen\n  segmentation", "comments": null, "journal-ref": "Medical Physics 44: 1040-1049, 2017", "doi": "10.1002/mp.12121", "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: The goal of this study was to assess the potential added benefit of\naccounting for partial volume effects (PVE) in an automatic coronary lumen\nsegmentation algorithm from coronary computed tomography angiography (CCTA).\nMaterials and methods: We assessed the potential added value of PVE integration\nas a part of the automatic coronary lumen segmentation algorithm by means of\nsegmentation accuracy using the MICCAI 2012 challenge framework and by means of\nflow simulation overall accuracy, sensitivity, specificity, negative and\npositive predictive values and the receiver operated characteristic (ROC) area\nunder the curve. We also evaluated the potential benefit of accounting for PVE\nin automatic segmentation for flow-simulation for lesions that were diagnosed\nas obstructive based on CCTA, which could have indicated a need for an invasive\nexam and revascularization. Results: Our segmentation algorithm improves the\nmaximal surface distance error by ~39% compared to previously published method\non the 18 datasets 50 from the MICCAI 2012 challenge with comparable Dice and\nmean surface distance. Results with and without accounting for PVE were\ncomparable. In contrast, integrating PVE analysis into an automatic coronary\nlumen segmentation algorithm improved the flow simulation specificity from 0.6\nto 0.68 with the same sensitivity of 0.83. Also, accounting for PVE improved\nthe area under the ROC curve for detecting hemodynamically significant CAD from\n0.76 to 0.8 compared to automatic segmentation without PVE analysis with\ninvasive FFR threshold of 0.8 as the reference standard. The improvement in the\nAUC was statistically significant (N=76, Delong's test, p=0.012). Conclusion:\nAccounting for the partial volume effects in automatic coronary lumen\nsegmentation algorithms has the potential to improve the accuracy of CCTA-based\nhemodynamic assessment of coronary artery lesions.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 07:45:26 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Freiman", "Moti", ""], ["Nickisch", "Hannes", ""], ["Prevrhal", "Sven", ""], ["Schmitt", "Holger", ""], ["Vembar", "Mani", ""], ["Maurovich-Horvat", "P\u00e1l", ""], ["Donnelly", "Patrick", ""], ["Goshen", "Liran", ""]]}, {"id": "1906.09806", "submitter": "Ali Mahmoudi", "authors": "Hooman Misaghi, Reza Askari Moghadam, Ali Mahmoudi, Kurosh Madani", "title": "Saliency Detection With Fully Convolutional Neural Network", "comments": null, "journal-ref": "Proceedings of 61st Research World International Conference,\n  Ottawa, Canada, 27th -28th March, 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency detection is an important task in image processing as it can solve\nmany problems and it usually is the first step in for other processes.\nConvolutional neural networks have been proved to be very effective on several\nimage processing tasks such as classification, segmentation, semantic\ncolorization and object manipulation. Besides, using the weights of a\npretrained networks is a common practice for enhancing the accuracy of a\nnetwork. In this paper a fully convolutional neural network which uses a part\nof VGG-16 is proposed for saliency detection in images.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 09:29:59 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Misaghi", "Hooman", ""], ["Moghadam", "Reza Askari", ""], ["Mahmoudi", "Ali", ""], ["Madani", "Kurosh", ""]]}, {"id": "1906.09826", "submitter": "Yu Wang", "authors": "Yu Wang, Quan Zhou, Xiaofu Wu", "title": "ESNet: An Efficient Symmetric Network for Real-time Semantic\n  Segmentation", "comments": "12 pages, 3 figures, 4 tables, submitted. Code can be found at\n  https://github.com/xiaoyufenfei/ESNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent years have witnessed great advances for semantic segmentation\nusing deep convolutional neural networks (DCNNs). However, a large number of\nconvolutional layers and feature channels lead to semantic segmentation as a\ncomputationally heavy task, which is disadvantage to the scenario with limited\nresources. In this paper, we design an efficient symmetric network, called\n(ESNet), to address this problem. The whole network has nearly symmetric\narchitecture, which is mainly composed of a series of factorized convolution\nunit (FCU) and its parallel counterparts (PFCU). On one hand, the FCU adopts a\nwidely-used 1D factorized convolution in residual layers. On the other hand,\nthe parallel version employs a transform-split-transform-merge strategy in the\ndesignment of residual module, where the split branch adopts dilated\nconvolutions with different rate to enlarge receptive field. Our model has\nnearly 1.6M parameters, and is able to be performed over 62 FPS on a single GTX\n1080Ti GPU. The experiments demonstrate that our approach achieves\nstate-of-the-art results in terms of speed and accuracy trade-off for real-time\nsemantic segmentation on CityScapes dataset.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 10:05:32 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Wang", "Yu", ""], ["Zhou", "Quan", ""], ["Wu", "Xiaofu", ""]]}, {"id": "1906.09840", "submitter": "I-Chao Shen", "authors": "Toby Chong Long Hin, I-Chao Shen, Issei Sato, Takeo Igarashi", "title": "Interactive Optimization of Generative Image Modeling using Sequential\n  Subspace Search and Content-based Guidance", "comments": "13 pages, Toby Chong Long Hin and I-Chao Shen contributed equally to\n  the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative image modeling techniques such as GAN demonstrate highly\nconvincing image generation result. However, user interaction is often\nnecessary to obtain the desired results. Existing attempts add interactivity\nbut require either tailored architectures or extra data. We present a\nhuman-in-the-optimization method that allows users to directly explore and\nsearch the latent vector space of generative image modeling. Our system\nprovides multiple candidates by sampling the latent vector space, and the user\nselects the best blending weights within the subspace using multiple sliders.\nIn addition, the user can express their intention through image editing tools.\nThe system samples latent vectors based on inputs and presents new candidates\nto the user iteratively. An advantage of our formulation is that one can apply\nour method to arbitrary pre-trained model without developing specialized\narchitecture or data. We demonstrate our method with various generative image\nmodeling applications, and show superior performance in a comparative user\nstudy with prior art iGAN.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 10:34:37 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 01:53:04 GMT"}, {"version": "v3", "created": "Sat, 29 Aug 2020 09:05:11 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Hin", "Toby Chong Long", ""], ["Shen", "I-Chao", ""], ["Sato", "Issei", ""], ["Igarashi", "Takeo", ""]]}, {"id": "1906.09844", "submitter": "Zhaoquan Yuan", "authors": "Zhaoquan Yuan, Siyuan Sun, Lixin Duan, Xiao Wu, Changsheng Xu", "title": "Adversarial Multimodal Network for Movie Question Answering", "comments": "We will revise the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering by using information from multiple modalities has\nattracted more and more attention in recent years. However, it is a very\nchallenging task, as the visual content and natural language have quite\ndifferent statistical properties. In this work, we present a method called\nAdversarial Multimodal Network (AMN) to better understand video stories for\nquestion answering. In AMN, as inspired by generative adversarial networks, we\npropose to learn multimodal feature representations by finding a more coherent\nsubspace for video clips and the corresponding texts (e.g., subtitles and\nquestions). Moreover, we introduce a self-attention mechanism to enforce the\nso-called consistency constraints in order to preserve the self-correlation of\nvisual cues of the original video clips in the learned multimodal\nrepresentations. Extensive experiments on the MovieQA dataset show the\neffectiveness of our proposed AMN over other published state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 10:44:48 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 07:13:55 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Yuan", "Zhaoquan", ""], ["Sun", "Siyuan", ""], ["Duan", "Lixin", ""], ["Wu", "Xiao", ""], ["Xu", "Changsheng", ""]]}, {"id": "1906.09868", "submitter": "Sumant Sharma", "authors": "Sumant Sharma, Simone D'Amico", "title": "Pose Estimation for Non-Cooperative Rendezvous Using Neural Networks", "comments": "Presented at AIAA/AAS Space Flight Mechanics Meeting January 2019", "journal-ref": null, "doi": null, "report-no": "AAS 19-350", "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces the Spacecraft Pose Network (SPN) for on-board\nestimation of the pose, i.e., the relative position and attitude, of a known\nnon-cooperative spacecraft using monocular vision. In contrast to other\nstate-of-the-art pose estimation approaches for spaceborne applications, the\nSPN method does not require the formulation of hand-engineered features and\nonly requires a single grayscale image to determine the pose of the spacecraft\nrelative to the camera. The SPN method uses a Convolutional Neural Network\n(CNN) with three branches to solve for the pose. The first branch of the CNN\nbootstraps a state-of-the-art object detector to detect a 2D bounding box\naround the target spacecraft. The region inside the bounding box is then used\nby the other two branches of the CNN to determine the attitude by initially\nclassifying the input region into discrete coarse attitude labels before\nregressing to a finer estimate. The SPN method then uses a novel Gauss-Newton\nalgorithm to estimate the position by using the constraints imposed by the\ndetected 2D bounding box and the estimated attitude. The secondary contribution\nof this work is the generation of the Spacecraft PosE Estimation Dataset\n(SPEED). SPEED consists of synthetic as well as actual camera images of a\nmock-up of the Tango spacecraft from the PRISMA mission. The synthetic images\nare created by fusing OpenGL-based renderings of the spacecraft's 3D model with\nactual images of the Earth captured by the Himawari-8 meteorological satellite.\nThe actual camera images are created using a 7 degrees-of-freedom robotic arm,\nwhich positions and orients a vision-based sensor with respect to a full-scale\nmock-up of the Tango spacecraft. The SPN method, trained only on synthetic\nimages, produces degree-level attitude error and cm-level position errors when\nevaluated on the actual camera images not used during training.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 11:51:57 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Sharma", "Sumant", ""], ["D'Amico", "Simone", ""]]}, {"id": "1906.09884", "submitter": "Yan Niu", "authors": "Niu Yan, Jihong Ouyang", "title": "Channel-by-Channel Demosaicking Networks with Embedded Spectral\n  Correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demosaicking is standardly the first step in today's Image Signal Processing\n(ISP) pipeline of digital cameras. It reconstructs image RGB values from the\nspatially and spectrally sparse Color Filter Array (CFA) samples, which are the\noriginal raw data digitized from electrical signals. High quality and low cost\ndemosaicking is not only necessary for photography, but also fundamental for\nmany machine vision tasks. This paper proposes an accurate and fast\ndemosaicking model based on Convolutional Neural Networks (CNN) for the Bayer\nCFA, which is the most popular color filter arrangement adopted by digital\ncamera manufacturers. Observing that each channel has different estimation\ncomplexity, we reconstruct each channel by an individual sub-network. Moreover,\ninstead of directly estimating the red and blue values, our model infers the\ngreen-red and green-blue color difference. This strategy allows recovering the\nmost complex channel by a light weight network. Although the total size of our\nmodel is significantly smaller than the state of the art demosaicking networks,\nit achieves substantially higher performance in both demosaicking quality and\ncomputational cost, as validated by extensive experiments. Source code will be\nreleased along with paper publication.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 12:33:49 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 11:55:00 GMT"}, {"version": "v3", "created": "Wed, 22 Apr 2020 07:31:17 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Yan", "Niu", ""], ["Ouyang", "Jihong", ""]]}, {"id": "1906.09908", "submitter": "Zhen Zhou", "authors": "Zhen Zhou, Xiaobo Chen, Yu Zhang, Lishan Qiao, Renping Yu, Gang Pan,\n  Han Zhang, Dinggang Shen", "title": "Brain Network Construction and Classification Toolbox (BrainNetClass)", "comments": null, "journal-ref": null, "doi": "10.1002/hbm.24979", "report-no": null, "categories": "q-bio.NC cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Brain functional network has become an increasingly used approach in\nunderstanding brain functions and diseases. Many network construction methods\nhave been developed, whereas the majority of the studies still used static\npairwise Pearson's correlation-based functional connectivity. The goal of this\nwork is to introduce a toolbox namely \"Brain Network Construction and\nClassification\" (BrainNetClass) to the field to promote more advanced brain\nnetwork construction methods. It comprises various brain network construction\nmethods, including some state-of-the-art methods that were recently developed\nto capture more complex interactions among brain regions along with connectome\nfeature extraction, reduction, parameter optimization towards network-based\nindividualized classification. BrainNetClass is a MATLAB-based, open-source,\ncross-platform toolbox with graphical user-friendly interfaces for cognitive\nand clinical neuroscientists to perform rigorous computer-aided diagnosis with\ninterpretable result presentations even though they do not possess neuroimage\ncomputing and machine learning knowledge. We demonstrate the implementations of\nthis toolbox on real resting-state functional MRI datasets. BrainNetClass\n(v1.0) can be downloaded from https://github.com/zzstefan/BrainNetClass.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 18:19:18 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Zhou", "Zhen", ""], ["Chen", "Xiaobo", ""], ["Zhang", "Yu", ""], ["Qiao", "Lishan", ""], ["Yu", "Renping", ""], ["Pan", "Gang", ""], ["Zhang", "Han", ""], ["Shen", "Dinggang", ""]]}, {"id": "1906.09909", "submitter": "Bo Zhang", "authors": "Bo Zhang, Mingming He, Jing Liao, Pedro V. Sander, Lu Yuan, Amine\n  Bermak, Dong Chen", "title": "Deep Exemplar-based Video Colorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the first end-to-end network for exemplar-based video\ncolorization. The main challenge is to achieve temporal consistency while\nremaining faithful to the reference style. To address this issue, we introduce\na recurrent framework that unifies the semantic correspondence and color\npropagation steps. Both steps allow a provided reference image to guide the\ncolorization of every frame, thus reducing accumulated propagation errors.\nVideo frames are colorized in sequence based on the colorization history, and\nits coherency is further enforced by the temporal consistency loss. All of\nthese components, learned end-to-end, help produce realistic videos with good\ntemporal stability. Experiments show our result is superior to the\nstate-of-the-art methods both quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 12:56:36 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Zhang", "Bo", ""], ["He", "Mingming", ""], ["Liao", "Jing", ""], ["Sander", "Pedro V.", ""], ["Yuan", "Lu", ""], ["Bermak", "Amine", ""], ["Chen", "Dong", ""]]}, {"id": "1906.09954", "submitter": "Somak Aditya", "authors": "Somak Aditya, Yezhou Yang and Chitta Baral", "title": "Integrating Knowledge and Reasoning in Image Understanding", "comments": "8 pages, 2 figures", "journal-ref": "IJCAI 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based data-driven approaches have been successfully applied in\nvarious image understanding applications ranging from object recognition,\nsemantic segmentation to visual question answering. However, the lack of\nknowledge integration as well as higher-level reasoning capabilities with the\nmethods still pose a hindrance. In this work, we present a brief survey of a\nfew representative reasoning mechanisms, knowledge integration methods and\ntheir corresponding image understanding applications developed by various\ngroups of researchers, approaching the problem from a variety of angles.\nFurthermore, we discuss upon key efforts on integrating external knowledge with\nneural networks. Taking cues from these efforts, we conclude by discussing\npotential pathways to improve reasoning capabilities.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 13:44:34 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Aditya", "Somak", ""], ["Yang", "Yezhou", ""], ["Baral", "Chitta", ""]]}, {"id": "1906.09955", "submitter": "Piotr Koniusz", "authors": "Lei Wang and Du Q. Huynh and Piotr Koniusz", "title": "A Comparative Review of Recent Kinect-based Action Recognition\n  Algorithms", "comments": "Accepted by the IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2019.2925285", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based human action recognition is currently one of the most active\nresearch areas in computer vision. Various research studies indicate that the\nperformance of action recognition is highly dependent on the type of features\nbeing extracted and how the actions are represented. Since the release of the\nKinect camera, a large number of Kinect-based human action recognition\ntechniques have been proposed in the literature. However, there still does not\nexist a thorough comparison of these Kinect-based techniques under the grouping\nof feature types, such as handcrafted versus deep learning features and\ndepth-based versus skeleton-based features. In this paper, we analyze and\ncompare ten recent Kinect-based algorithms for both cross-subject action\nrecognition and cross-view action recognition using six benchmark datasets. In\naddition, we have implemented and improved some of these techniques and\nincluded their variants in the comparison. Our experiments show that the\nmajority of methods perform better on cross-subject action recognition than\ncross-view action recognition, that skeleton-based features are more robust for\ncross-view recognition than depth-based features, and that deep learning\nfeatures are suitable for large datasets.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 13:45:21 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Wang", "Lei", ""], ["Huynh", "Du Q.", ""], ["Koniusz", "Piotr", ""]]}, {"id": "1906.09986", "submitter": "Suraj Tripathi", "authors": "Suraj Tripathi, Abhay Kumar, Chirag Singh", "title": "Visual Context-aware Convolution Filters for Transformation-invariant\n  Neural Network", "comments": "Under-Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel visual context-aware filter generation module which\nincorporates contextual information present in images into Convolutional Neural\nNetworks (CNNs). In contrast to traditional CNNs, we do not employ the same set\nof learned convolution filters for all input image instances. Our proposed\ninput-conditioned convolution filters when combined with techniques inspired by\nMulti-instance learning and max-pooling, results in a transformation-invariant\nneural network. We investigated the performance of our proposed framework on\nthree MNIST variations, which covers both rotation and scaling variance, and\nachieved 1.13% error on MNIST-rot-12k, 1.12% error on Half-rotated MNIST and\n0.68% error on Scaling MNIST, which is significantly better than the\nstate-of-the-art results. We make use of visualization to further prove the\neffectiveness of our visual context-aware convolution filters. Our proposed\nvisual context-aware convolution filter generation framework can also serve as\na plugin for any CNN based architecture and enhance its modeling capacity.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 05:53:16 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Tripathi", "Suraj", ""], ["Kumar", "Abhay", ""], ["Singh", "Chirag", ""]]}, {"id": "1906.09988", "submitter": "Robin Sandkuehler", "authors": "Robin Sandk\\\"uhler, Simon Andermatt, Grzegorz Bauman, Sylvia Nyilas,\n  Christoph Jud, Philippe C. Cattin", "title": "Recurrent Registration Neural Networks for Deformable Image Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric spatial transformation models have been successfully applied to\nimage registration tasks. In such models, the transformation of interest is\nparameterized by a fixed set of basis functions as for example B-splines. Each\nbasis function is located on a fixed regular grid position among the image\ndomain, because the transformation of interest is not known in advance. As a\nconsequence, not all basis functions will necessarily contribute to the final\ntransformation which results in a non-compact representation of the\ntransformation. We reformulate the pairwise registration problem as a recursive\nsequence of successive alignments. For each element in the sequence, a local\ndeformation defined by its position, shape, and weight is computed by our\nrecurrent registration neural network. The sum of all local deformations yield\nthe final spatial alignment of both images. Formulating the registration\nproblem in this way allows the network to detect non-aligned regions in the\nimages and to learn how to locally refine the registration properly. In\ncontrast to current non-sequence-based registration methods, our approach\niteratively applies local spatial deformations to the images until the desired\nregistration accuracy is achieved. We trained our network on 2D magnetic\nresonance images of the lung and compared our method to a standard parametric\nB-spline registration. The experiments show, that our method performs on par\nfor the accuracy but yields a more compact representation of the\ntransformation. Furthermore, we achieve a speedup of around 15 compared to the\nB-spline registration.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 16:44:53 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Sandk\u00fchler", "Robin", ""], ["Andermatt", "Simon", ""], ["Bauman", "Grzegorz", ""], ["Nyilas", "Sylvia", ""], ["Jud", "Christoph", ""], ["Cattin", "Philippe C.", ""]]}, {"id": "1906.10011", "submitter": "Sandy Engelhardt", "authors": "Sandy Engelhardt, Lalith Sharan, Matthias Karck, Raffaele De Simone,\n  Ivo Wolf", "title": "Cross-Domain Conditional Generative Adversarial Networks for\n  Stereoscopic Hyperrealism in Surgical Training", "comments": "accepted for MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.CY", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Phantoms for surgical training are able to mimic cutting and suturing\nproperties and patient-individual shape of organs, but lack a realistic visual\nappearance that captures the heterogeneity of surgical scenes. In order to\novercome this in endoscopic approaches, hyperrealistic concepts have been\nproposed to be used in an augmented reality-setting, which are based on deep\nimage-to-image transformation methods. Such concepts are able to generate\nrealistic representations of phantoms learned from real intraoperative\nendoscopic sequences. Conditioned on frames from the surgical training process,\nthe learned models are able to generate impressive results by transforming\nunrealistic parts of the image (e.g.\\ the uniform phantom texture is replaced\nby the more heterogeneous texture of the tissue). Image-to-image synthesis\nusually learns a mapping $G:X~\\to~Y$ such that the distribution of images from\n$G(X)$ is indistinguishable from the distribution $Y$. However, it does not\nnecessarily force the generated images to be consistent and without artifacts.\nIn the endoscopic image domain this can affect depth cues and stereo\nconsistency of a stereo image pair, which ultimately impairs surgical vision.\nWe propose a cross-domain conditional generative adversarial network approach\n(GAN) that aims to generate more consistent stereo pairs. The results show\nsubstantial improvements in depth perception and realism evaluated by 3 domain\nexperts and 3 medical students on a 3D monitor over the baseline method. In 84\nof 90 instances our proposed method was preferred or rated equal to the\nbaseline.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 15:05:07 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Engelhardt", "Sandy", ""], ["Sharan", "Lalith", ""], ["Karck", "Matthias", ""], ["De Simone", "Raffaele", ""], ["Wolf", "Ivo", ""]]}, {"id": "1906.10017", "submitter": "Wenqiang Cui", "authors": "Wenqiang Cui and Girts Strazdins and Hao Wang", "title": "Confluent-Drawing Parallel Coordinates: Web-Based Interactive Visual\n  Analytics of Large Multi-Dimensional Data", "comments": "7 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel coordinates plot is one of the most popular and widely used\nvisualization techniques for multi-dimensional data sets. Its main challenges\nfor large-scale data sets are visual clutter and overplotting which hamper the\nrecognition of patterns and trends in the data. In this paper, we propose a\nconfluent drawing approach of parallel coordinates to support the web-based\ninteractive visual analytics of large multi-dimensional data. The proposed\nmethod maps multi-dimensional data to node-link diagrams through the data\nbinning-based clustering for each dimension. It uses density-based confluent\ndrawing to visualize clusters and edges to reduce visual clutter and\noverplotting. Its rendering time is independent of the number of data items. It\nsupports interactive visualization of large data sets without hardware\nacceleration in a normal web browser. Moreover, we design interactions to\ncontrol the data binning process with this approach to support interactive\nvisual analytics of large multi-dimensional data sets. Based on the proposed\napproach, we implement a web-based visual analytics application. The efficiency\nof the proposed method is examined through experiments on several data sets.\nThe effectiveness of the proposed method is evaluated through a user study, in\nwhich two typical tasks of parallel coordinates plot are performed by\nparticipants to compare the proposed method with another parallel coordinates\nbundling technique. Results show that the proposed method significantly\nenhances the web-based interactive visual analytics of large multi-dimensional\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 12:14:50 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Cui", "Wenqiang", ""], ["Strazdins", "Girts", ""], ["Wang", "Hao", ""]]}, {"id": "1906.10042", "submitter": "Joon Son Chung", "authors": "Joon Son Chung, Bong-Jin Lee, Icksang Han", "title": "Who said that?: Audio-visual speaker diarisation of real-world meetings", "comments": "Accepted to Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this work is to determine 'who spoke when' in real-world\nmeetings. The method takes surround-view video and single or multi-channel\naudio as inputs, and generates robust diarisation outputs. To achieve this, we\npropose a novel iterative approach that first enrolls speaker models using\naudio-visual correspondence, then uses the enrolled models together with the\nvisual information to determine the active speaker. We show strong quantitative\nand qualitative performance on a dataset of real-world meetings. The method is\nalso evaluated on the public AMI meeting corpus, on which we demonstrate\nresults that exceed all comparable methods. We also show that beamforming can\nbe used together with the video to further improve the performance when\nmulti-channel audio is available.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 16:06:13 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Chung", "Joon Son", ""], ["Lee", "Bong-Jin", ""], ["Han", "Icksang", ""]]}, {"id": "1906.10044", "submitter": "Johanna Rock", "authors": "Johanna Rock, Mate Toth, Elmar Messner, Paul Meissner, Franz Pernkopf", "title": "Complex Signal Denoising and Interference Mitigation for Automotive\n  Radar Using Convolutional Neural Networks", "comments": "FUSION 2019; 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driver assistance systems as well as autonomous cars have to rely on sensors\nto perceive their environment. A heterogeneous set of sensors is used to\nperform this task robustly. Among them, radar sensors are indispensable because\nof their range resolution and the possibility to directly measure velocity.\nSince more and more radar sensors are deployed on the streets, mutual\ninterference must be dealt with. In the so far unregulated automotive radar\nfrequency band, a sensor must be capable of detecting, or even mitigating the\nharmful effects of interference, which include a decreased detection\nsensitivity. In this paper, we address this issue with Convolutional Neural\nNetworks (CNNs), which are state-of-the-art machine learning tools. We show\nthat the ability of CNNs to find structured information in data while\npreserving local information enables superior denoising performance. To achieve\nthis, CNN parameters are found using training with simulated data and\nintegrated into the automotive radar signal processing chain. The presented\nmethod is compared with the state of the art, highlighting its promising\nperformance. Hence, CNNs can be employed for interference mitigation as an\nalternative to conventional signal processing methods. Code and pre-trained\nmodels are available at https://github.com/johanna-rock/imRICnn.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 16:07:52 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 11:07:24 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Rock", "Johanna", ""], ["Toth", "Mate", ""], ["Messner", "Elmar", ""], ["Meissner", "Paul", ""], ["Pernkopf", "Franz", ""]]}, {"id": "1906.10048", "submitter": "Stella X. Yu", "authors": "Rudrasis Chakraborty and Jiayun Wang and Stella X. Yu", "title": "SurReal: Fr\\'echet Mean and Distance Transform for Complex-Valued Deep\n  Learning", "comments": "IEEE Computer Vision and Pattern Recognition Workshop on Perception\n  Beyond the Visible Spectrum, Long Beach, California, 16 June 2019 Best Paper\n  Award", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel deep learning architecture for naturally complex-valued\ndata, which is often subject to complex scaling ambiguity. We treat each sample\nas a field in the space of complex numbers. With the polar form of a\ncomplex-valued number, the general group that acts in this space is the product\nof planar rotation and non-zero scaling. This perspective allows us to develop\nnot only a novel convolution operator using weighted Fr\\'echet mean (wFM) on a\nRiemannian manifold, but also a novel fully connected layer operator using the\ndistance to the wFM, with natural equivariant properties to non-zero scaling\nand planar rotation for the former and invariance properties for the latter.\n  Compared to the baseline approach of learning real-valued neural network\nmodels on the two-channel real-valued representation of complex-valued data,\nour method achieves surreal performance on two publicly available\ncomplex-valued datasets: MSTAR on SAR images and RadioML on radio frequency\nsignals. On MSTAR, at 8% of the baseline model size and with fewer than 45,000\nparameters, our model improves the target classification accuracy from 94% to\n98% on this highly imbalanced dataset. On RadioML, our model achieves\ncomparable RF modulation classification accuracy at 10% of the baseline model\nsize.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 16:12:31 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Chakraborty", "Rudrasis", ""], ["Wang", "Jiayun", ""], ["Yu", "Stella X.", ""]]}, {"id": "1906.10057", "submitter": "Mu Li", "authors": "Mu Li, Kede Ma, Jane You, David Zhang and Wangmeng Zuo", "title": "Efficient and Effective Context-Based Convolutional Entropy Modeling for\n  Image Compression", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2020.2985225", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise estimation of the probabilistic structure of natural images plays an\nessential role in image compression. Despite the recent remarkable success of\nend-to-end optimized image compression, the latent codes are usually assumed to\nbe fully statistically factorized in order to simplify entropy modeling.\nHowever, this assumption generally does not hold true and may hinder\ncompression performance. Here we present context-based convolutional networks\n(CCNs) for efficient and effective entropy modeling. In particular, a 3D zigzag\nscanning order and a 3D code dividing technique are introduced to define proper\ncoding contexts for parallel entropy decoding, both of which boil down to place\ntranslation-invariant binary masks on convolution filters of CCNs. We\ndemonstrate the promise of CCNs for entropy modeling in both lossless and lossy\nimage compression. For the former, we directly apply a CCN to the binarized\nrepresentation of an image to compute the Bernoulli distribution of each code\nfor entropy estimation. For the latter, the categorical distribution of each\ncode is represented by a discretized mixture of Gaussian distributions, whose\nparameters are estimated by three CCNs. We then jointly optimize the CCN-based\nentropy model along with analysis and synthesis transforms for rate-distortion\nperformance. Experiments on the Kodak and Tecnick datasets show that our\nmethods powered by the proposed CCNs generally achieve comparable compression\nperformance to the state-of-the-art while being much faster.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 16:26:03 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 17:26:12 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Li", "Mu", ""], ["Ma", "Kede", ""], ["You", "Jane", ""], ["Zhang", "David", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "1906.10089", "submitter": "Solale Tabarestani", "authors": "Mohammad Eslami, Solale Tabarestani, Shadi Albarqouni, Ehsan Adeli,\n  Nassir Navab, Malek Adjouadi", "title": "Image to Images Translation for Multi-Task Organ Segmentation and Bone\n  Suppression in Chest X-Ray Radiography", "comments": null, "journal-ref": null, "doi": "10.1109/TMI.2020.2974159", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest X-ray radiography is one of the earliest medical imaging technologies\nand remains one of the most widely-used for diagnosis, screening, and treatment\nfollow up of diseases related to lungs and heart. The literature in this field\nof research reports many interesting studies dealing with the challenging tasks\nof bone suppression and organ segmentation but performed separately, limiting\nany learning that comes with the consolidation of parameters that could\noptimize both processes. This study, and for the first time, introduces a\nmultitask deep learning model that generates simultaneously the bone-suppressed\nimage and the organ-segmented image, enhancing the accuracy of tasks,\nminimizing the number of parameters needed by the model and optimizing the\nprocessing time, all by exploiting the interplay between the network parameters\nto benefit the performance of both tasks. The architectural design of this\nmodel, which relies on a conditional generative adversarial network, reveals\nthe process on how the well-established pix2pix network (image-to-image\nnetwork) is modified to fit the need for multitasking and extending it to the\nnew image-to-images architecture. The developed source code of this multitask\nmodel is shared publicly on Github as the first attempt for providing the\ntwo-task pix2pix extension, a supervised/paired/aligned/registered\nimage-to-images translation which would be useful in many multitask\napplications. Dilated convolutions are also used to improve the results through\na more effective receptive field assessment. The comparison with\nstate-of-the-art algorithms along with ablation study and a demonstration video\nare provided to evaluate efficacy and gauge the merits of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 17:12:31 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2019 18:59:21 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Eslami", "Mohammad", ""], ["Tabarestani", "Solale", ""], ["Albarqouni", "Shadi", ""], ["Adeli", "Ehsan", ""], ["Navab", "Nassir", ""], ["Adjouadi", "Malek", ""]]}, {"id": "1906.10095", "submitter": "Cun Mu", "authors": "Cun Mu, Binwei Yang, Zheng Yan", "title": "An Empirical Comparison of FAISS and FENSHSES for Nearest Neighbor\n  Search in Hamming Space", "comments": "SIGIR eCom'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we compare the performances of FAISS and FENSHSES on nearest\nneighbor search in Hamming space--a fundamental task with ubiquitous\napplications in nowadays eCommerce. Comprehensive evaluations are made in terms\nof indexing speed, search latency and RAM consumption. This comparison is\nconducted towards a better understanding on trade-offs between nearest neighbor\nsearch systems implemented in main memory and the ones implemented in secondary\nmemory, which is largely unaddressed in literature.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 17:24:11 GMT"}, {"version": "v2", "created": "Sun, 28 Jul 2019 20:46:33 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Mu", "Cun", ""], ["Yang", "Binwei", ""], ["Yan", "Zheng", ""]]}, {"id": "1906.10096", "submitter": "Xiaoting Wu", "authors": "Xiaoting Wu, Eric Granger and Xiaoyi Feng", "title": "Audio-Visual Kinship Verification", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual kinship verification entails confirming whether or not two individuals\nin a given pair of images or videos share a hypothesized kin relation. As a\ngeneralized face verification task, visual kinship verification is particularly\ndifficult with low-quality found Internet data. Due to uncontrolled variations\nin background, pose, facial expression, blur, illumination and occlusion,\nstate-of-the-art methods fail to provide high level of recognition accuracy. As\nwith many other visual recognition tasks, kinship verification may benefit from\ncombining visual and audio signals. However, voice-based kinship verification\nhas received very little prior attention. We hypothesize that the human voice\ncontains kin-related cues that are complementary to visual cues. In this paper,\nwe address, for the first time, the use of audio-visual information from face\nand voice modalities to perform kinship verification. We first propose a new\nmulti-modal kinship dataset, called TALking KINship (TALKIN), that contains\nseveral pairs of Internet-quality video sequences. Using TALKIN, we then study\nthe utility of various kinship verification methods including traditional local\nfeature based methods, statistical methods and more recent deep learning\napproaches. Then, early and late fusion methods are evaluated on the TALKIN\ndataset for the study of kinship verification with both face and voice\nmodalities. Finally, we propose a deep Siamese fusion network with contrastive\nloss for multi-modal fusion of kinship relations. Extensive experiments on the\nTALKIN dataset indicate that by combining face and voice modalities, the\nproposed Siamese network can provide a significantly higher level of accuracy\ncompared to baseline uni-modal and multi-modal fusion techniques. Experimental\nresults also indicate that audio (vocal) information is complementary (to\nfacial information) and useful for kinship verification.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 17:24:56 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Wu", "Xiaoting", ""], ["Granger", "Eric", ""], ["Feng", "Xiaoyi", ""]]}, {"id": "1906.10101", "submitter": "Dong Wang", "authors": "Dong Wang, Yitong Li, Wei Cao, Liqun Chen, Qi Wei, Lawrence Carin", "title": "LMVP: Video Predictor with Leaked Motion Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Leaked Motion Video Predictor (LMVP) to predict future frames by\ncapturing the spatial and temporal dependencies from given inputs. The motion\nis modeled by a newly proposed component, motion guider, which plays the role\nof both learner and teacher. Specifically, it {\\em learns} the temporal\nfeatures from real data and {\\em guides} the generator to predict future\nframes. The spatial consistency in video is modeled by an adaptive filtering\nnetwork. To further ensure the spatio-temporal consistency of the prediction, a\ndiscriminator is also adopted to distinguish the real and generated frames.\nFurther, the discriminator leaks information to the motion guider and the\ngenerator to help the learning of motion. The proposed LMVP can effectively\nlearn the static and temporal features in videos without the need for human\nlabeling. Experiments on synthetic and real data demonstrate that LMVP can\nyield state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 17:36:27 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Wang", "Dong", ""], ["Li", "Yitong", ""], ["Cao", "Wei", ""], ["Chen", "Liqun", ""], ["Wei", "Qi", ""], ["Carin", "Lawrence", ""]]}, {"id": "1906.10104", "submitter": "Weilian Song", "authors": "Weilian Song, Tawfiq Salem, Hunter Blanton, Nathan Jacobs", "title": "Remote Estimation of Free-Flow Speeds", "comments": "4 pages, 4 figures, IGARSS 2019 camera-ready submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an automated method to estimate a road segment's free-flow speed\nfrom overhead imagery and road metadata. The free-flow speed of a road segment\nis the average observed vehicle speed in ideal conditions, without congestion\nor adverse weather. Standard practice for estimating free-flow speeds depends\non several road attributes, including grade, curve, and width of the right of\nway. Unfortunately, many of these fine-grained labels are not always readily\navailable and are costly to manually annotate. To compensate, our model uses a\nsmall, easy to obtain subset of road features along with aerial imagery to\ndirectly estimate free-flow speed with a deep convolutional neural network\n(CNN). We evaluate our approach on a large dataset, and demonstrate that using\nimagery alone performs nearly as well as the road features and that the\ncombination of imagery with road features leads to the highest accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 17:41:46 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Song", "Weilian", ""], ["Salem", "Tawfiq", ""], ["Blanton", "Hunter", ""], ["Jacobs", "Nathan", ""]]}, {"id": "1906.10109", "submitter": "Daniele Cattaneo", "authors": "Daniele Cattaneo, Matteo Vaghi, Augusto Luis Ballardini, Simone\n  Fontana, Domenico Giorgio Sorrenti, Wolfram Burgard", "title": "CMRNet: Camera to LiDAR-Map Registration", "comments": "Accepted for presentation at IEEE ITSC2019", "journal-ref": "2019 IEEE Intelligent Transportation Systems Conference (ITSC) pp.\n  1283-1289", "doi": "10.1109/ITSC.2019.8917470", "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present CMRNet, a realtime approach based on a Convolutional\nNeural Network to localize an RGB image of a scene in a map built from LiDAR\ndata. Our network is not trained in the working area, i.e. CMRNet does not\nlearn the map. Instead it learns to match an image to the map. We validate our\napproach on the KITTI dataset, processing each frame independently without any\ntracking procedure. CMRNet achieves 0.27m and 1.07deg median localization\naccuracy on the sequence 00 of the odometry dataset, starting from a rough pose\nestimate displaced up to 3.5m and 17deg. To the best of our knowledge this is\nthe first CNN-based approach that learns to match images from a monocular\ncamera to a given, preexisting 3D LiDAR-map.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 17:53:28 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 13:06:48 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 11:50:34 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Cattaneo", "Daniele", ""], ["Vaghi", "Matteo", ""], ["Ballardini", "Augusto Luis", ""], ["Fontana", "Simone", ""], ["Sorrenti", "Domenico Giorgio", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1906.10112", "submitter": "Lore Goetschalckx", "authors": "Lore Goetschalckx (1 and 2), Alex Andonian (1), Aude Oliva (1),\n  Phillip Isola (1) ((1) MIT, (2) KU Leuven)", "title": "GANalyze: Toward Visual Definitions of Cognitive Image Properties", "comments": "17 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework that uses Generative Adversarial Networks (GANs) to\nstudy cognitive properties like memorability, aesthetics, and emotional\nvalence. These attributes are of interest because we do not have a concrete\nvisual definition of what they entail. What does it look like for a dog to be\nmore or less memorable? GANs allow us to generate a manifold of natural-looking\nimages with fine-grained differences in their visual attributes. By navigating\nthis manifold in directions that increase memorability, we can visualize what\nit looks like for a particular generated image to become more or less\nmemorable. The resulting ``visual definitions\" surface image properties (like\n``object size\") that may underlie memorability. Through behavioral experiments,\nwe verify that our method indeed discovers image manipulations that causally\naffect human memory performance. We further demonstrate that the same framework\ncan be used to analyze image aesthetics and emotional valence. Visit the\nGANalyze website at http://ganalyze.csail.mit.edu/.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 17:56:29 GMT"}], "update_date": "2019-08-10", "authors_parsed": [["Goetschalckx", "Lore", "", "1 and 2"], ["Andonian", "Alex", "", "MIT"], ["Oliva", "Aude", "", "MIT"], ["Isola", "Phillip", "", "MIT"]]}, {"id": "1906.10169", "submitter": "Corentin Dancette", "authors": "Remi Cadene and Corentin Dancette and Hedi Ben-younes and Matthieu\n  Cord and Devi Parikh", "title": "RUBi: Reducing Unimodal Biases in Visual Question Answering", "comments": "NeurIPS 2019\n  http://papers.nips.cc/paper/8371-rubi-reducing-unimodal-biases-for-visual-question-answering", "journal-ref": "Advances in Neural Information Processing Systems 2019 (pp.\n  839-850)", "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) is the task of answering questions about an\nimage. Some VQA models often exploit unimodal biases to provide the correct\nanswer without using the image information. As a result, they suffer from a\nhuge drop in performance when evaluated on data outside their training set\ndistribution. This critical issue makes them unsuitable for real-world\nsettings.\n  We propose RUBi, a new learning strategy to reduce biases in any VQA model.\nIt reduces the importance of the most biased examples, i.e. examples that can\nbe correctly classified without looking at the image. It implicitly forces the\nVQA model to use the two input modalities instead of relying on statistical\nregularities between the question and the answer. We leverage a question-only\nmodel that captures the language biases by identifying when these unwanted\nregularities are used. It prevents the base VQA model from learning them by\ninfluencing its predictions. This leads to dynamically adjusting the loss in\norder to compensate for biases. We validate our contributions by surpassing the\ncurrent state-of-the-art results on VQA-CP v2. This dataset is specifically\ndesigned to assess the robustness of VQA models when exposed to different\nquestion biases at test time than what was seen during training.\n  Our code is available: github.com/cdancette/rubi.bootstrap.pytorch\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 18:55:24 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 11:25:27 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Cadene", "Remi", ""], ["Dancette", "Corentin", ""], ["Ben-younes", "Hedi", ""], ["Cord", "Matthieu", ""], ["Parikh", "Devi", ""]]}, {"id": "1906.10182", "submitter": "Meenakshi Sarkar", "authors": "Meenakshi Sarkar, Prabhu Pradhan and Debasish Ghose", "title": "Planning Robot Motion using Deep Visual Prediction", "comments": "7th ICAPS Workshop on Planning and Robotics (PlanRob), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we introduce a novel framework that can learn to make visual\npredictions about the motion of a robotic agent from raw video frames. Our\nproposed motion prediction network (PROM-Net) can learn in a completely\nunsupervised manner and efficiently predict up to 10 frames in the future.\nMoreover, unlike any other motion prediction models, it is lightweight and once\ntrained it can be easily implemented on mobile platforms that have very limited\ncomputing capabilities. We have created a new robotic data set comprising LEGO\nMindstorms moving along various trajectories in three different environments\nunder different lighting conditions for testing and training the network.\nFinally, we introduce a framework that would use the predicted frames from the\nnetwork as an input to a model predictive controller for motion planning in\nunknown dynamic environments with moving obstacles.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 19:17:34 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Sarkar", "Meenakshi", ""], ["Pradhan", "Prabhu", ""], ["Ghose", "Debasish", ""]]}, {"id": "1906.10183", "submitter": "Yading Yuan", "authors": "Yading Yuan, Ren-Dih Sheu, Luke Fu, Yeh-Chi Lo", "title": "A Deep Regression Model for Seed Identification in Prostate\n  Brachytherapy", "comments": "Accepted for presentation in MICCAI 2019 (8 pages, 4 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Post-implant dosimetry (PID) is an essential step of prostate brachytherapy\nthat utilizes CT to image the prostate and allow the location and dose\ndistribution of the radioactive seeds to be directly related to the actual\nprostate. However, it it a very challenging task to identify these seeds in CT\nimages due to the severe metal artifacts and high-overlapped appearance when\nmultiple seeds clustered together. In this paper, we propose an automatic and\nefficient algorithm based on 3D deep fully convolutional network for\nidentifying implanted seeds in CT images. Our method models the seed\nlocalization task as a supervised regression problem that projects the input CT\nimage to a map where each element represents the probability that the\ncorresponding input voxel belongs to a seed. This deep regression model\nsignificantly suppresses image artifacts and makes the post-processing much\neasier and more controllable. The proposed method is validated on a large\nclinical database with 7820 seeds in 100 patients, in which 5534 seeds from 70\npatients were used for model training and validation. Our method correctly\ndetected 2150 of 2286 (94.1%) seeds in the 30 testing patients, yielding 16%\nimprovement as compared to a widely-used commercial seed finder software\n(VariSeed, Varian, Palo Alto, CA).\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 19:17:37 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Yuan", "Yading", ""], ["Sheu", "Ren-Dih", ""], ["Fu", "Luke", ""], ["Lo", "Yeh-Chi", ""]]}, {"id": "1906.10267", "submitter": "Yunsheng Li", "authors": "Yunsheng Li, Nuno Vasconcelos", "title": "Efficient Multi-Domain Network Learning by Covariance Normalization", "comments": "10 Pages Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of multi-domain learning of deep networks is considered. An\nadaptive layer is induced per target domain and a novel procedure, denoted\ncovariance normalization (CovNorm), proposed to reduce its parameters. CovNorm\nis a data driven method of fairly simple implementation, requiring two\nprincipal component analyzes (PCA) and fine-tuning of a mini-adaptation layer.\nNevertheless, it is shown, both theoretically and experimentally, to have\nseveral advantages over previous approaches, such as batch normalization or\ngeometric matrix approximations. Furthermore, CovNorm can be deployed both when\ntarget datasets are available sequentially or simultaneously. Experiments show\nthat, in both cases, it has performance comparable to a fully fine-tuned\nnetwork, using as few as 0.13% of the corresponding parameters per target\ndomain.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 23:42:55 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Li", "Yunsheng", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "1906.10269", "submitter": "Seiichi Uchida", "authors": "Yuto Shinahara, Takuro Karamatsu, Daisuke Harada, Kota Yamaguchi,\n  Seiichi Uchida", "title": "Serif or Sans: Visual Font Analytics on Book Covers and Online\n  Advertisements", "comments": "Accepted by ICDAR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we conduct a large-scale study of font statistics in book\ncovers and online advertisements. Through the statistical study, we try to\nunderstand how graphic designers relate fonts and content genres and identify\nthe relationship between font styles, colors, and genres. We propose an\nautomatic approach to extract font information from graphic designs by applying\na sequence of character detection, style classification, and clustering\ntechniques to the graphic designs. The extracted font information is\naccumulated together with genre information, such as romance or business, for\nfurther trend analysis. Through our unique empirical study, we show that the\ncollected font statistics reveal interesting trends in terms of how typographic\ndesign represents the impression and the atmosphere of the content genres.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 23:56:00 GMT"}, {"version": "v2", "created": "Sat, 29 Jun 2019 10:56:58 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Shinahara", "Yuto", ""], ["Karamatsu", "Takuro", ""], ["Harada", "Daisuke", ""], ["Yamaguchi", "Kota", ""], ["Uchida", "Seiichi", ""]]}, {"id": "1906.10284", "submitter": "Shohei Nobuhara", "authors": "Ryo Kawahara, Meng-Yu Jennifer Kuo, Shohei Nobuhara, Ko Nishino", "title": "Appearance and Shape from Water Reflection", "comments": "WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces single-image geometric and appearance reconstruction\nfrom water reflection photography, i.e., images capturing direct and\nwater-reflected real-world scenes. Water reflection offers an additional\nviewpoint to the direct sight, collectively forming a stereo pair. The\nwater-reflected scene, however, includes internally scattered and reflected\nenvironmental illumination in addition to the scene radiance, which precludes\ndirect stereo matching. We derive a principled iterative method that\ndisentangles this scene radiometry and geometry for reconstructing 3D scene\nstructure as well as its high-dynamic range appearance. In the presence of\nwaves, we simultaneously recover the wave geometry as surface normal\nperturbations of the water surface. Most important, we show that the water\nreflection enables calibration of the camera. In other words, for the first\ntime, we show that capturing a direct and water-reflected scene in a single\nexposure forms a self-calibrating HDR catadioptric stereo camera. We\ndemonstrate our method on a number of images taken in the wild. The results\ndemonstrate a new means for leveraging this accidental catadioptric camera.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 00:52:37 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 12:06:01 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Kawahara", "Ryo", ""], ["Kuo", "Meng-Yu Jennifer", ""], ["Nobuhara", "Shohei", ""], ["Nishino", "Ko", ""]]}, {"id": "1906.10288", "submitter": "Jonathan Ramos", "authors": "Jonathan S. Ramos, Mirela T. Cazzolato, Bruno S. Fai\\c{c}al, Marcello\n  H. Nogueira-Barbosa, Caetano Traina Jr. and Agma J. M. Traina", "title": "3DBGrowth: volumetric vertebrae segmentation and reconstruction in\n  magnetic resonance imaging", "comments": "This is a pre-print of an article published in Computer-Based Medical\n  Systems. The final authenticated version is available online at:\n  https://doi.org/10.1109/CBMS.2019.00091", "journal-ref": "Computer-Based Medical Systems, 2019", "doi": "10.1109/CBMS.2019.00091", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of medical images is critical for making several processes of\nanalysis and classification more reliable. With the growing number of people\npresenting back pain and related problems, the semi-automatic segmentation and\n3D reconstruction of vertebral bodies became even more important to support\ndecision making. A 3D reconstruction allows a fast and objective analysis of\neach vertebrae condition, which may play a major role in surgical planning and\nevaluation of suitable treatments. In this paper, we propose 3DBGrowth, which\ndevelops a 3D reconstruction over the efficient Balanced Growth method for 2D\nimages. We also take advantage of the slope coefficient from the annotation\ntime to reduce the total number of annotated slices, reducing the time spent on\nmanual annotation. We show experimental results on a representative dataset\nwith 17 MRI exams demonstrating that our approach significantly outperforms the\ncompetitors and, on average, only 37% of the total slices with vertebral body\ncontent must be annotated without losing performance/accuracy. Compared to the\nstate-of-the-art methods, we have achieved a Dice Score gain of over 5% with\ncomparable processing time. Moreover, 3DBGrowth works well with imprecise seed\npoints, which reduces the time spent on manual annotation by the specialist.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 01:28:20 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 21:44:44 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Ramos", "Jonathan S.", ""], ["Cazzolato", "Mirela T.", ""], ["Fai\u00e7al", "Bruno S.", ""], ["Nogueira-Barbosa", "Marcello H.", ""], ["Traina", "Caetano", "Jr."], ["Traina", "Agma J. M.", ""]]}, {"id": "1906.10324", "submitter": "Mohsen Soryani", "authors": "Mohammad Amin Mehralian, Mohsen Soryani", "title": "EKFPnP: Extended Kalman Filter for Camera Pose Estimation in a Sequence\n  of Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world applications the Perspective-n-Point (PnP) problem should\ngenerally be applied in a sequence of images which a set of drift-prone\nfeatures are tracked over time. In this paper, we consider both the temporal\ndependency of camera poses and the uncertainty of features for the sequential\ncamera pose estimation. Using the Extended Kalman Filter (EKF), a priori\nestimate of the camera pose is calculated from the camera motion model and then\ncorrected by minimizing the reprojection error of the reference points.\nExperimental results, using both simulated and real data, demonstrate that the\nproposed method improves the robustness of the camera pose estimation, in the\npresence of noise, compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 05:33:48 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 12:25:27 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Mehralian", "Mohammad Amin", ""], ["Soryani", "Mohsen", ""]]}, {"id": "1906.10327", "submitter": "Xiaofan Zhang", "authors": "Xiaofan Zhang, Cong Hao, Haoming Lu, Jiachen Li, Yuhong Li, Yuchen\n  Fan, Kyle Rupnow, Jinjun Xiong, Thomas Huang, Honghui Shi, Wen-mei Hwu,\n  Deming Chen", "title": "SkyNet: A Champion Model for DAC-SDC on Low Power Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing artificial intelligence (AI) at the edge is always challenging,\nsince edge devices have limited computation capability and memory resources but\nneed to meet demanding requirements, such as real-time processing, high\nthroughput performance, and high inference accuracy. To overcome these\nchallenges, we propose SkyNet, an extremely lightweight DNN with 12\nconvolutional (Conv) layers and only 1.82 megabyte (MB) of parameters following\na bottom-up DNN design approach. SkyNet is demonstrated in the 56th IEEE/ACM\nDesign Automation Conference System Design Contest (DAC-SDC), a low power\nobject detection challenge in images captured by unmanned aerial vehicles\n(UAVs). SkyNet won the first place award for both the GPU and FPGA tracks of\nthe contest: we deliver 0.731 Intersection over Union (IoU) and 67.33 frames\nper second (FPS) on a TX2 GPU and deliver 0.716 IoU and 25.05 FPS on an Ultra96\nFPGA.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 05:41:01 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 05:51:07 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Zhang", "Xiaofan", ""], ["Hao", "Cong", ""], ["Lu", "Haoming", ""], ["Li", "Jiachen", ""], ["Li", "Yuhong", ""], ["Fan", "Yuchen", ""], ["Rupnow", "Kyle", ""], ["Xiong", "Jinjun", ""], ["Huang", "Thomas", ""], ["Shi", "Honghui", ""], ["Hwu", "Wen-mei", ""], ["Chen", "Deming", ""]]}, {"id": "1906.10337", "submitter": "Wenxiao Wang", "authors": "Wenxiao Wang, Cong Fu, Jishun Guo, Deng Cai and Xiaofei He", "title": "COP: Customized Deep Model Compression via Regularized Correlation-Based\n  Filter-Level Pruning", "comments": "7 pages, 4 figures, has been accepted by IJCAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network compression empowers the effective yet unwieldy deep\nconvolutional neural networks (CNN) to be deployed in resource-constrained\nscenarios. Most state-of-the-art approaches prune the model in filter-level\naccording to the \"importance\" of filters. Despite their success, we notice they\nsuffer from at least two of the following problems: 1) The redundancy among\nfilters is not considered because the importance is evaluated independently. 2)\nCross-layer filter comparison is unachievable since the importance is defined\nlocally within each layer. Consequently, we must manually specify layer-wise\npruning ratios. 3) They are prone to generate sub-optimal solutions because\nthey neglect the inequality between reducing parameters and reducing\ncomputational cost. Reducing the same number of parameters in different\npositions in the network may reduce different computational cost. To address\nthe above problems, we develop a novel algorithm named as COP\n(correlation-based pruning), which can detect the redundant filters\nefficiently. We enable the cross-layer filter comparison through global\nnormalization. We add parameter-quantity and computational-cost regularization\nterms to the importance, which enables the users to customize the compression\naccording to their preference (smaller or faster). Extensive experiments have\nshown COP outperforms the others significantly. The code is released at\nhttps://github.com/ZJULearning/COP.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 06:15:42 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Wang", "Wenxiao", ""], ["Fu", "Cong", ""], ["Guo", "Jishun", ""], ["Cai", "Deng", ""], ["He", "Xiaofei", ""]]}, {"id": "1906.10338", "submitter": "Moti Freiman", "authors": "Moti Freiman, Hannes Nickisch, Holger Schmitt, Pal Maurovich-Horvat,\n  Patrick Donnelly, Mani Vembar and Liran Goshen", "title": "Learning a sparse database for patch-based medical image segmentation", "comments": null, "journal-ref": "Wu G., Munsell B., Zhan Y., Bai W., Sanroma G., Coup\\'e P. (eds)\n  Patch-Based Techniques in Medical Imaging. Patch-MI 2017. Lecture Notes in\n  Computer Science, vol 10530. Springer, Cham", "doi": "10.1007/978-3-319-67434-6_6", "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a functional for the learning of an optimal database for\npatch-based image segmentation with application to coronary lumen segmentation\nfrom coronary computed tomography angiography (CCTA) data. The proposed\nfunctional consists of fidelity, sparseness and robustness to small-variations\nterms and their associated weights. Existing work address database optimization\nby prototype selection aiming to optimize the database by either adding or\nremoving prototypes according to a set of predefined rules. In contrast, we\nformulate the database optimization task as an energy minimization problem that\ncan be solved using standard numerical tools. We apply the proposed database\noptimization functional to the task of optimizing a database for patch-base\ncoronary lumen segmentation. Our experiments using the publicly available\nMICCAI 2012 coronary lumen segmentation challenge data show that optimizing the\ndatabase using the proposed approach reduced database size by 96% while\nmaintaining the same level of lumen segmentation accuracy. Moreover, we show\nthat the optimized database yields an improved specificity of CCTA based\nfractional flow reserve (0.73 vs 0.7 for all lesions and 0.68 vs 0.65 for\nobstructive lesions) using a training set of 132 (76 obstructive) coronary\nlesions with invasively measured FFR as the reference.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 06:16:05 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Freiman", "Moti", ""], ["Nickisch", "Hannes", ""], ["Schmitt", "Holger", ""], ["Maurovich-Horvat", "Pal", ""], ["Donnelly", "Patrick", ""], ["Vembar", "Mani", ""], ["Goshen", "Liran", ""]]}, {"id": "1906.10343", "submitter": "Phi Vu Tran", "authors": "Phi Vu Tran", "title": "Exploring Self-Supervised Regularization for Supervised and\n  Semi-Supervised Learning", "comments": "NeurIPS'19 Workshop on Learning with Rich Experience: Integration of\n  Learning Paradigms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in semi-supervised learning have shown tremendous potential\nin overcoming a major barrier to the success of modern machine learning\nalgorithms: access to vast amounts of human-labeled training data. Previous\nalgorithms based on consistency regularization can harness the abundance of\nunlabeled data to produce impressive results on a number of semi-supervised\nbenchmarks, approaching the performance of strong supervised baselines using\nonly a fraction of the available labeled data. In this work, we challenge the\nlong-standing success of consistency regularization by introducing\nself-supervised regularization as the basis for combining semantic feature\nrepresentations from unlabeled data. We perform extensive comparative\nexperiments to demonstrate the effectiveness of self-supervised regularization\nfor supervised and semi-supervised image classification on SVHN, CIFAR-10, and\nCIFAR-100 benchmark datasets. We present two main results: (1) models augmented\nwith self-supervised regularization significantly improve upon traditional\nsupervised classifiers without the need for unlabeled data; (2) together with\nunlabeled data, our models yield semi-supervised performance competitive with,\nand in many cases exceeding, prior state-of-the-art consistency baselines.\nLastly, our models have the practical utility of being efficiently trained\nend-to-end and require no additional hyper-parameters to tune for optimal\nperformance beyond the standard set for training neural networks. Reference\ncode and data are available at https://github.com/vuptran/sesemi\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 06:42:05 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 08:30:43 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Tran", "Phi Vu", ""]]}, {"id": "1906.10399", "submitter": "Li Zhang", "authors": "Li Zhang, Quanhong Wang, Haihua Lu, Yong Zhao", "title": "End-to-End Learning of Multi-scale Convolutional Neural Network for\n  Stereo Matching", "comments": "16 pages, 4 figures,Asian Conference on Machine Learning(ACML)2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have shown excellent performance in stereo matching\ntask. Recently CNN-based methods have shown that stereo matching can be\nformulated as a supervised learning task. However, less attention is paid on\nthe fusion of contextual semantic information and details. To tackle this\nproblem, we propose a network for disparity estimation based on abundant\ncontextual details and semantic information, called Multi-scale Features\nNetwork (MSFNet). First, we design a new structure to encode rich semantic\ninformation and fine-grained details by fusing multi-scale features. And we\ncombine the advantages of element-wise addition and concatenation, which is\nconducive to merge semantic information with details. Second, a guidance\nmechanism is introduced to guide the network to automatically focus more on the\nunreliable regions. Third, we formulate the consistency check as an error map,\nobtained by the low stage features with fine-grained details. Finally, we adopt\nthe consistency checking between the left feature and the synthetic left\nfeature to refine the initial disparity. Experiments on Scene Flow and KITTI\n2015 benchmark demonstrated that the proposed method can achieve the\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 09:15:22 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Zhang", "Li", ""], ["Wang", "Quanhong", ""], ["Lu", "Haihua", ""], ["Zhao", "Yong", ""]]}, {"id": "1906.10400", "submitter": "Xuhua Ren", "authors": "Xuhua Ren, Lichi Zhang, Qian Wang and Dinggang Shen", "title": "Brain MR Image Segmentation in Small Dataset with Adversarial Defense\n  and Task Reorganization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image segmentation is challenging especially in dealing with small\ndataset of 3D MR images. Encoding the variation of brain anatomical struc-tures\nfrom individual subjects cannot be easily achieved, which is further\nchal-lenged by only a limited number of well labeled subjects for training. In\nthis study, we aim to address the issue of brain MR image segmentation in small\nda-taset. First, concerning the limited number of training images, we adopt\nadver-sarial defense to augment the training data and therefore increase the\nrobustness of the network. Second, inspired by the prior knowledge of neural\nanatomies, we reorganize the segmentation tasks of different regions into\nseveral groups in a hierarchical way. Third, the task reorganization extends to\nthe semantic level, as we incorporate an additional object-level classification\ntask to contribute high-order visual features toward the pixel-level\nsegmentation task. In experiments we validate our method by segmenting gray\nmatter, white matter, and several major regions on a challenge dataset. The\nproposed method with only seven subjects for training can achieve 84.46% of\nDice score in the onsite test set.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 09:15:24 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Ren", "Xuhua", ""], ["Zhang", "Lichi", ""], ["Wang", "Qian", ""], ["Shen", "Dinggang", ""]]}, {"id": "1906.10401", "submitter": "Paul Maergner", "authors": "Paul Maergner, Nicholas R. Howe, Kaspar Riesen, Rolf Ingold, Andreas\n  Fischer", "title": "Graph-Based Offline Signature Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs provide a powerful representation formalism that offers great promise\nto benefit tasks like handwritten signature verification. While most\nstate-of-the-art approaches to signature verification rely on fixed-size\nrepresentations, graphs are flexible in size and allow modeling local features\nas well as the global structure of the handwriting. In this article, we present\ntwo recent graph-based approaches to offline signature verification: keypoint\ngraphs with approximated graph edit distance and inkball models. We provide a\ncomprehensive description of the methods, propose improvements both in terms of\ncomputational time and accuracy, and report experimental results for four\nbenchmark datasets. The proposed methods achieve top results for several\nbenchmarks, highlighting the potential of graph-based signature verification.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 09:15:35 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Maergner", "Paul", ""], ["Howe", "Nicholas R.", ""], ["Riesen", "Kaspar", ""], ["Ingold", "Rolf", ""], ["Fischer", "Andreas", ""]]}, {"id": "1906.10411", "submitter": "Yochai Zur", "authors": "Yochai Zur, Amir Adler", "title": "Deep Learning of Compressed Sensing Operators with Structural Similarity\n  Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Compressed sensing (CS) is a signal processing framework for efficiently\nreconstructing a signal from a small number of measurements, obtained by linear\nprojections of the signal. In this paper we present an end-to-end deep learning\napproach for CS, in which a fully-connected network performs both the linear\nsensing and non-linear reconstruction stages. During the training phase, the\nsensing matrix and the non-linear reconstruction operator are jointly optimized\nusing Structural similarity index (SSIM) as loss rather than the standard Mean\nSquared Error (MSE) loss. We compare the proposed approach with\nstate-of-the-art in terms of reconstruction quality under both losses, i.e.\nSSIM score and MSE score.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 09:33:30 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Zur", "Yochai", ""], ["Adler", "Amir", ""]]}, {"id": "1906.10413", "submitter": "Massimiliano Gargiulo", "authors": "Massimiliano Gargiulo, Domenico Antonio Giuseppe Dell'Aglio, Antonio\n  Iodice, Daniele Riccio, and Giuseppe Ruello", "title": "A CNN-Based Super-Resolution Technique for Active Fire Detection on\n  Sentinel-2 Data", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote Sensing applications can benefit from a relatively fine spatial\nresolution multispectral (MS) images and a high revisit frequency ensured by\nthe twin satellites Sentinel-2. Unfortunately, only four out of thirteen bands\nare provided at the highest resolution of 10 meters, and the others at 20 or 60\nmeters. For instance the Short-Wave Infrared (SWIR) bands, provided at 20\nmeters, are very useful to detect active fires. Aiming to a more detailed\nActive Fire Detection (AFD) maps, we propose a super-resolution data fusion\nmethod based on Convolutional Neural Network (CNN) to move towards the 10-m\nspatial resolution the SWIR bands. The proposed CNN-based solution achieves\nbetter results than alternative methods in terms of some accuracy metrics.\nMoreover we test the super-resolved bands from an application point of view by\nmonitoring active fire through classic indices. Advantages and limits of our\nproposed approach are validated on specific geographical area (the mount\nVesuvius, close to Naples) that was damaged by widespread fires during the\nsummer of 2017.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 09:34:38 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Gargiulo", "Massimiliano", ""], ["Dell'Aglio", "Domenico Antonio Giuseppe", ""], ["Iodice", "Antonio", ""], ["Riccio", "Daniele", ""], ["Ruello", "Giuseppe", ""]]}, {"id": "1906.10414", "submitter": "Linyu Zheng", "authors": "Linyu Zheng, Ming Tang, Yingying Chen, Jinqiao Wang, Hanqing Lu", "title": "Learning Feature Embeddings for Discriminant Model based Tracking", "comments": "15 pages, 5 figures, accepted by ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After observing that the features used in most online discriminatively\ntrained trackers are not optimal, in this paper, we propose a novel and\neffective architecture to learn optimal feature embeddings for online\ndiscriminative tracking. Our method, called DCFST, integrates the solver of a\ndiscriminant model that is differentiable and has a closed-form solution into\nconvolutional neural networks. Then, the resulting network can be trained in an\nend-to-end way, obtaining optimal feature embeddings for the discriminant\nmodel-based tracker. As an instance, we apply the popular ridge regression\nmodel in this work to demonstrate the power of DCFST. Extensive experiments on\nsix public benchmarks, OTB2015, NFS, GOT10k, TrackingNet, VOT2018, and VOT2019,\nshow that our approach is efficient and generalizes well to class-agnostic\ntarget objects in online tracking, thus achieves state-of-the-art accuracy,\nwhile running beyond the real-time speed. Code will be made available.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 09:40:37 GMT"}, {"version": "v2", "created": "Sun, 6 Sep 2020 09:23:13 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Zheng", "Linyu", ""], ["Tang", "Ming", ""], ["Chen", "Yingying", ""], ["Wang", "Jinqiao", ""], ["Lu", "Hanqing", ""]]}, {"id": "1906.10486", "submitter": "Isaac Shiri", "authors": "Shakiba Moradi, Mostafa Ghelich-Oghli, Azin Alizadehasl, Isaac Shiri,\n  Niki Oveisi, Mehrdad Oveisi, Majid Maleki, Jan Dhooge", "title": "A Novel Deep Learning Based Approach for Left Ventricle Segmentation in\n  Echocardiography: MFP-Unet", "comments": "32 Pages, 10 Figures, 5 Tables", "journal-ref": "https://doi.org/10.1016/j.ejmp.2019.10.001", "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of the Left ventricle (LV) is a crucial step for quantitative\nmeasurements such as area, volume, and ejection fraction. However, the\nautomatic LV segmentation in 2D echocardiographic images is a challenging task\ndue to ill-defined borders, and operator dependence issues (insufficient\nreproducibility). U-net, which is a well-known architecture in medical image\nsegmentation, addressed this problem through an encoder-decoder path. Despite\noutstanding overall performance, U-net ignores the contribution of all semantic\nstrengths in the segmentation procedure. In the present study, we have proposed\na novel architecture to tackle this drawback. Feature maps in all levels of the\ndecoder path of U-net are concatenated, their depths are equalized, and\nup-sampled to a fixed dimension. This stack of feature maps would be the input\nof the semantic segmentation layer. The proposed network yielded\nstate-of-the-art results when comparing with results from U-net, dilated U-net,\nand deeplabv3, using the same dataset. An average Dice Metric (DM) of 0.945,\nHausdorff Distance (HD) of 1.62, Jaccard Coefficient (JC) of 0.97, and Mean\nAbsolute Distance (MAD) of 1.32 are achieved. The correlation graph,\nbland-altman analysis, and box plot showed a great agreement between automatic\nand manually calculated volume, area, and length.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 12:56:27 GMT"}, {"version": "v2", "created": "Sun, 22 Dec 2019 09:18:39 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Moradi", "Shakiba", ""], ["Ghelich-Oghli", "Mostafa", ""], ["Alizadehasl", "Azin", ""], ["Shiri", "Isaac", ""], ["Oveisi", "Niki", ""], ["Oveisi", "Mehrdad", ""], ["Maleki", "Majid", ""], ["Dhooge", "Jan", ""]]}, {"id": "1906.10490", "submitter": "Martim Brand\\~ao", "authors": "Martim Brandao", "title": "Age and gender bias in pedestrian detection algorithms", "comments": "Appeared at the Workshop on Fairness Accountability Transparency and\n  Ethics in Computer Vision (FATE CV) at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian detection algorithms are important components of mobile robots,\nsuch as autonomous vehicles, which directly relate to human safety. Performance\ndisparities in these algorithms could translate into disparate impact in the\nform of biased accident outcomes. To evaluate the need for such concerns, we\ncharacterize the age and gender bias in the performance of state-of-the-art\npedestrian detection algorithms. Our analysis is based on the INRIA Person\nDataset extended with child, adult, male and female labels. We show that all of\nthe 24 top-performing methods of the Caltech Pedestrian Detection Benchmark\nhave higher miss rates on children. The difference is significant and we\nanalyse how it varies with the classifier, features and training data used by\nthe methods. Algorithms were also gender-biased on average but the performance\ndifferences were not significant. We discuss the source of the bias, the\nethical implications, possible technical solutions and barriers.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 13:01:29 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Brandao", "Martim", ""]]}, {"id": "1906.10491", "submitter": "Nikolay Savinov", "authors": "Nikolay Savinov, Lubor Ladicky, Christian Haene, Marc Pollefeys", "title": "Discrete Optimization of Ray Potentials for Semantic 3D Reconstruction", "comments": "Published at CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense semantic 3D reconstruction is typically formulated as a discrete or\ncontinuous problem over label assignments in a voxel grid, combining semantic\nand depth likelihoods in a Markov Random Field framework. The depth and\nsemantic information is incorporated as a unary potential, smoothed by a\npairwise regularizer. However, modelling likelihoods as a unary potential does\nnot model the problem correctly leading to various undesirable visibility\nartifacts.\n  We propose to formulate an optimization problem that directly optimizes the\nreprojection error of the 3D model with respect to the image estimates, which\ncorresponds to the optimization over rays, where the cost function depends on\nthe semantic class and depth of the first occupied voxel along the ray. The\n2-label formulation is made feasible by transforming it into a\ngraph-representable form under QPBO relaxation, solvable using graph cut. The\nmulti-label problem is solved by applying alpha-expansion using the same\nrelaxation in each expansion move. Our method was indeed shown to be feasible\nin practice, running comparably fast to the competing methods, while not\nsuffering from ray potential approximation artifacts.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 13:04:56 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Savinov", "Nikolay", ""], ["Ladicky", "Lubor", ""], ["Haene", "Christian", ""], ["Pollefeys", "Marc", ""]]}, {"id": "1906.10509", "submitter": "Mohammad Rostami", "authors": "Mohammad Rostami, Soheil Kolouri, Zak Murez, Yuri Owekcho, Eric Eaton,\n  Kuyngnam Kim", "title": "Zero-Shot Image Classification Using Coupled Dictionary Embedding", "comments": "arXiv admin note: substantial text overlap with arXiv:1709.03688", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) is a framework to classify images belonging to\nunseen classes based on solely semantic information about these unseen classes.\nIn this paper, we propose a new ZSL algorithm using coupled dictionary\nlearning. The core idea is that the visual features and the semantic attributes\nof an image can share the same sparse representation in an intermediate space.\nWe use images from seen classes and semantic attributes from seen and unseen\nclasses to learn two dictionaries that can represent sparsely the visual and\nsemantic feature vectors of an image. In the ZSL testing stage and in the\nabsence of labeled data, images from unseen classes can be mapped into the\nattribute space by finding the joint sparse representation using solely the\nvisual data. The image is then classified in the attribute space given semantic\ndescriptions of unseen classes. We also provide an attribute-aware formulation\nto tackle domain shift and hubness problems in ZSL. Extensive experiments are\nprovided to demonstrate the superior performance of our approach against the\nstate of the art ZSL algorithms on benchmark ZSL datasets.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 01:35:36 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Rostami", "Mohammad", ""], ["Kolouri", "Soheil", ""], ["Murez", "Zak", ""], ["Owekcho", "Yuri", ""], ["Eaton", "Eric", ""], ["Kim", "Kuyngnam", ""]]}, {"id": "1906.10515", "submitter": "Luis Guillermo Roldao Jimenez", "authors": "Luis Rold\\~ao, Raoul de Charette and Anne Verroust-Blondet", "title": "3D Surface Reconstruction from Voxel-based Lidar Data", "comments": "IEEE Intelligent Transportation Systems Conference (ITSC) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To achieve fully autonomous navigation, vehicles need to compute an accurate\nmodel of their direct surrounding. In this paper, a 3D surface reconstruction\nalgorithm from heterogeneous density 3D data is presented. The proposed method\nis based on a TSDF voxel-based representation, where an adaptive neighborhood\nkernel sourced on a Gaussian confidence evaluation is introduced. This enables\nto keep a good trade-off between the density of the reconstructed mesh and its\naccuracy. Experimental evaluations carried on both synthetic (CARLA) and real\n(KITTI) 3D data show a good performance compared to a state of the art method\nused for surface reconstruction.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 13:37:26 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Rold\u00e3o", "Luis", ""], ["de Charette", "Raoul", ""], ["Verroust-Blondet", "Anne", ""]]}, {"id": "1906.10546", "submitter": "Sihui Luo", "authors": "Sihui Luo, Xinchao Wang, Gongfan Fang, Yao Hu, Dapeng Tao and Mingli\n  Song", "title": "Knowledge Amalgamation from Heterogeneous Networks by Common Feature\n  Learning", "comments": "IJCAI 2019, 7 pages, the 28th International Joint Conference on\n  Artificial Intelligence (IJCAI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing number of well-trained deep networks have been released online\nby researchers and developers, enabling the community to reuse them in a\nplug-and-play way without accessing the training annotations. However, due to\nthe large number of network variants, such public-available trained models are\noften of different architectures, each of which being tailored for a specific\ntask or dataset. In this paper, we study a deep-model reusing task, where we\nare given as input pre-trained networks of heterogeneous architectures\nspecializing in distinct tasks, as teacher models. We aim to learn a\nmultitalented and light-weight student model that is able to grasp the\nintegrated knowledge from all such heterogeneous-structure teachers, again\nwithout accessing any human annotation. To this end, we propose a common\nfeature learning scheme, in which the features of all teachers are transformed\ninto a common space and the student is enforced to imitate them all so as to\namalgamate the intact knowledge. We test the proposed approach on a list of\nbenchmarks and demonstrate that the learned student is able to achieve very\npromising performance, superior to those of the teachers in their specialized\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 12:33:24 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Luo", "Sihui", ""], ["Wang", "Xinchao", ""], ["Fang", "Gongfan", ""], ["Hu", "Yao", ""], ["Tao", "Dapeng", ""], ["Song", "Mingli", ""]]}, {"id": "1906.10555", "submitter": "Joon Son Chung", "authors": "Joon Son Chung", "title": "Naver at ActivityNet Challenge 2019 -- Task B Active Speaker Detection\n  (AVA)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes our submission to the ActivityNet Challenge at CVPR\n2019. We use a 3D convolutional neural network (CNN) based front-end and an\nensemble of temporal convolution and LSTM classifiers to predict whether a\nvisible person is speaking or not. Our results show significant improvements\nover the baseline on the AVA-ActiveSpeaker dataset.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 14:11:14 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Chung", "Joon Son", ""]]}, {"id": "1906.10643", "submitter": "Haimiao Zhang", "authors": "Haimiao Zhang and Bin Dong", "title": "A Review on Deep Learning in Medical Image Reconstruction", "comments": "31 pages, 6 figures. Survey paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical imaging is crucial in modern clinics to guide the diagnosis and\ntreatment of diseases. Medical image reconstruction is one of the most\nfundamental and important components of medical imaging, whose major objective\nis to acquire high-quality medical images for clinical usage at the minimal\ncost and risk to the patients. Mathematical models in medical image\nreconstruction or, more generally, image restoration in computer vision, have\nbeen playing a prominent role. Earlier mathematical models are mostly designed\nby human knowledge or hypothesis on the image to be reconstructed, and we shall\ncall these models handcrafted models. Later, handcrafted plus data-driven\nmodeling started to emerge which still mostly relies on human designs, while\npart of the model is learned from the observed data. More recently, as more\ndata and computation resources are made available, deep learning based models\n(or deep models) pushed the data-driven modeling to the extreme where the\nmodels are mostly based on learning with minimal human designs. Both\nhandcrafted and data-driven modeling have their own advantages and\ndisadvantages. One of the major research trends in medical imaging is to\ncombine handcrafted modeling with deep modeling so that we can enjoy benefits\nfrom both approaches. The major part of this article is to provide a conceptual\nreview of some recent works on deep modeling from the unrolling dynamics\nviewpoint. This viewpoint stimulates new designs of neural network\narchitectures with inspirations from optimization algorithms and numerical\ndifferential equations. Given the popularity of deep modeling, there are still\nvast remaining challenges in the field, as well as opportunities which we shall\ndiscuss at the end of this article.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2019 06:57:18 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Zhang", "Haimiao", ""], ["Dong", "Bin", ""]]}, {"id": "1906.10651", "submitter": "Peter Hase", "authors": "Peter Hase, Chaofan Chen, Oscar Li, Cynthia Rudin", "title": "Interpretable Image Recognition with Hierarchical Prototypes", "comments": "Published as a full paper at HCOMP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision models are interpretable when they classify objects on the basis of\nfeatures that a person can directly understand. Recently, methods relying on\nvisual feature prototypes have been developed for this purpose. However, in\ncontrast to how humans categorize objects, these approaches have not yet made\nuse of any taxonomical organization of class labels. With such an approach, for\ninstance, we may see why a chimpanzee is classified as a chimpanzee, but not\nwhy it was considered to be a primate or even an animal. In this work we\nintroduce a model that uses hierarchically organized prototypes to classify\nobjects at every level in a predefined taxonomy. Hence, we may find distinct\nexplanations for the prediction an image receives at each level of the\ntaxonomy. The hierarchical prototypes enable the model to perform another\nimportant task: interpretably classifying images from previously unseen classes\nat the level of the taxonomy to which they correctly relate, e.g. classifying a\nhand gun as a weapon, when the only weapons in the training data are rifles.\nWith a subset of ImageNet, we test our model against its counterpart black-box\nmodel on two tasks: 1) classification of data from familiar classes, and 2)\nclassification of data from previously unseen classes at the appropriate level\nin the taxonomy. We find that our model performs approximately as well as its\ncounterpart black-box model while allowing for each classification to be\ninterpreted.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 16:45:34 GMT"}, {"version": "v2", "created": "Sun, 25 Aug 2019 00:35:55 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Hase", "Peter", ""], ["Chen", "Chaofan", ""], ["Li", "Oscar", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1906.10678", "submitter": "David Arathorn", "authors": "David W. Arathorn", "title": "Fast Robot Arm Inverse Kinematics and Path Planning Under Complex Static\n  and Dynamic Obstacle Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Described here is a simple, reliable, and quite general method for rapid\ncomputation of robot arm inverse kinematic solutions and motion path plans in\nthe presence of complex obstructions. The method derived from the MSC\n(map-seeking circuit) algorithm, optimized to exploit the characteristics of\npractical arm configurations. The representation naturally incorporates both\narm and obstacle geometries. The consequent performance on modern hardware is\nsuitable for applications requiring real-time response, including smooth\ncontinuous avoidance of dynamic obstacles which impinge on the planned path\nduring the traversal of the arm. On high-end GPGPU hardware computation of both\nfinal pose for an 8 DOF arm and a smooth obstacle-avoiding motion path to that\npose takes approximately 200-300msec depending on the number of waypoints\nimplemented. The mathematics of the method is accessible to high school\nseniors, making it suitable for broad instruction. [Note: This revision\nincludes a general compute strategy for paths from arbitrary pose to arbitrary\npose and a compute strategy for continuous motion mid-course avoidance of\ndynamic obstacles.]\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 17:34:15 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2019 13:24:52 GMT"}, {"version": "v3", "created": "Tue, 23 Jul 2019 20:03:28 GMT"}, {"version": "v4", "created": "Tue, 3 Sep 2019 13:26:44 GMT"}, {"version": "v5", "created": "Tue, 14 Apr 2020 21:01:00 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Arathorn", "David W.", ""]]}, {"id": "1906.10729", "submitter": "Farzad Khalvati", "authors": "Yucheng Zhang, Edrise M. Lobo-Mueller, Paul Karanicolas, Steven\n  Gallinger, Masoom A. Haider, Farzad Khalvati", "title": "CNN-based Survival Model for Pancreatic Ductal Adenocarcinoma in Medical\n  Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cox proportional hazard model (CPH) is commonly used in clinical research for\nsurvival analysis. In quantitative medical imaging (radiomics) studies, CPH\nplays an important role in feature reduction and modeling. However, the\nunderlying linear assumption of CPH model limits the prognostic performance. In\naddition, the multicollinearity of radiomic features and multiple testing\nproblem further impedes the CPH models performance. In this work, using\ntransfer learning, a convolutional neural network (CNN) based survival model\nwas built and tested on preoperative CT images of resectable Pancreatic Ductal\nAdenocarcinoma (PDAC) patients. The proposed CNN-based survival model\noutperformed the traditional CPH-based radiomics approach in terms of\nconcordance index by 22%, providing a better fit for patients' survival\npatterns. The proposed CNN-based survival model outperforms CPH-based radiomics\npipeline in PDAC prognosis. This approach offers a better fit for survival\npatterns based on CT images and overcomes the limitations of conventional\nsurvival models.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 19:12:39 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Zhang", "Yucheng", ""], ["Lobo-Mueller", "Edrise M.", ""], ["Karanicolas", "Paul", ""], ["Gallinger", "Steven", ""], ["Haider", "Masoom A.", ""], ["Khalvati", "Farzad", ""]]}, {"id": "1906.10732", "submitter": "Utku Evci", "authors": "Utku Evci, Fabian Pedregosa, Aidan Gomez, Erich Elsen", "title": "The Difficulty of Training Sparse Neural Networks", "comments": "sparse networks, pruning, energy landscape, sparsity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the difficulties of training sparse neural networks and make\nnew observations about optimization dynamics and the energy landscape within\nthe sparse regime. Recent work of \\citep{Gale2019, Liu2018} has shown that\nsparse ResNet-50 architectures trained on ImageNet-2012 dataset converge to\nsolutions that are significantly worse than those found by pruning. We show\nthat, despite the failure of optimizers, there is a linear path with a\nmonotonically decreasing objective from the initialization to the \"good\"\nsolution. Additionally, our attempts to find a decreasing objective path from\n\"bad\" solutions to the \"good\" ones in the sparse subspace fail. However, if we\nallow the path to traverse the dense subspace, then we consistently find a path\nbetween two solutions. These findings suggest traversing extra dimensions may\nbe needed to escape stationary points found in the sparse subspace.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 19:21:15 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 19:38:25 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2020 17:38:07 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Evci", "Utku", ""], ["Pedregosa", "Fabian", ""], ["Gomez", "Aidan", ""], ["Elsen", "Erich", ""]]}, {"id": "1906.10770", "submitter": "Zhou Yu", "authors": "Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, Qi Tian", "title": "Deep Modular Co-Attention Networks for Visual Question Answering", "comments": "Accepted at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) requires a fine-grained and simultaneous\nunderstanding of both the visual content of images and the textual content of\nquestions. Therefore, designing an effective `co-attention' model to associate\nkey words in questions with key objects in images is central to VQA\nperformance. So far, most successful attempts at co-attention learning have\nbeen achieved by using shallow models, and deep co-attention models show little\nimprovement over their shallow counterparts. In this paper, we propose a deep\nModular Co-Attention Network (MCAN) that consists of Modular Co-Attention (MCA)\nlayers cascaded in depth. Each MCA layer models the self-attention of questions\nand images, as well as the guided-attention of images jointly using a modular\ncomposition of two basic attention units. We quantitatively and qualitatively\nevaluate MCAN on the benchmark VQA-v2 dataset and conduct extensive ablation\nstudies to explore the reasons behind MCAN's effectiveness. Experimental\nresults demonstrate that MCAN significantly outperforms the previous\nstate-of-the-art. Our best single model delivers 70.63$\\%$ overall accuracy on\nthe test-dev set. Code is available at https://github.com/MILVLG/mcan-vqa.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 22:16:03 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Yu", "Zhou", ""], ["Yu", "Jun", ""], ["Cui", "Yuhao", ""], ["Tao", "Dacheng", ""], ["Tian", "Qi", ""]]}, {"id": "1906.10771", "submitter": "Pavlo Molchanov", "authors": "Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, Jan Kautz", "title": "Importance Estimation for Neural Network Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural pruning of neural network parameters reduces computation, energy,\nand memory transfer costs during inference. We propose a novel method that\nestimates the contribution of a neuron (filter) to the final loss and\niteratively removes those with smaller scores. We describe two variations of\nour method using the first and second-order Taylor expansions to approximate a\nfilter's contribution. Both methods scale consistently across any network layer\nwithout requiring per-layer sensitivity analysis and can be applied to any kind\nof layer, including skip connections. For modern networks trained on ImageNet,\nwe measured experimentally a high (>93%) correlation between the contribution\ncomputed by our methods and a reliable estimate of the true importance. Pruning\nwith the proposed methods leads to an improvement over state-of-the-art in\nterms of accuracy, FLOPs, and parameter reduction. On ResNet-101, we achieve a\n40% FLOPS reduction by removing 30% of the parameters, with a loss of 0.02% in\nthe top-1 accuracy on ImageNet. Code is available at\nhttps://github.com/NVlabs/Taylor_pruning.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 22:20:16 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Molchanov", "Pavlo", ""], ["Mallya", "Arun", ""], ["Tyree", "Stephen", ""], ["Frosio", "Iuri", ""], ["Kautz", "Jan", ""]]}, {"id": "1906.10823", "submitter": "Qing Fang", "authors": "Qing Fang", "title": "Topology Maintained Structure Encoding", "comments": "7 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been used as a powerful tool for various tasks in computer\nvision, such as image segmentation, object recognition and data generation. A\nkey part of end-to-end training is designing the appropriate encoder to extract\nspecific features from the input data. However, few encoders maintain the\ntopological properties of data, such as connection structures and global\ncontours. In this paper, we introduce a Voronoi Diagram encoder based on convex\nset distance (CSVD) and apply it in edge encoding. The boundaries of Voronoi\ncells is related to detected edges of structures and contours. The CSVD model\nimproves contour extraction in CNN and structure generation in GAN. We also\nshow the experimental results and demonstrate that the proposed model has great\npotentiality in different visual problems where topology information should be\ninvolved.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 02:59:24 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Fang", "Qing", ""]]}, {"id": "1906.10842", "submitter": "Aniruddha Saha", "authors": "Soheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, Heiko Hoffmann", "title": "Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs", "comments": "CVPR 2020 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unprecedented success of deep neural networks in many applications has\nmade these networks a prime target for adversarial exploitation. In this paper,\nwe introduce a benchmark technique for detecting backdoor attacks (aka Trojan\nattacks) on deep convolutional neural networks (CNNs). We introduce the concept\nof Universal Litmus Patterns (ULPs), which enable one to reveal backdoor\nattacks by feeding these universal patterns to the network and analyzing the\noutput (i.e., classifying the network as `clean' or `corrupted'). This\ndetection is fast because it requires only a few forward passes through a CNN.\nWe demonstrate the effectiveness of ULPs for detecting backdoor attacks on\nthousands of networks with different architectures trained on four benchmark\ndatasets, namely the German Traffic Sign Recognition Benchmark (GTSRB), MNIST,\nCIFAR10, and Tiny-ImageNet. The codes and train/test models for this paper can\nbe found here https://umbcvision.github.io/Universal-Litmus-Patterns/.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 04:30:55 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 02:52:42 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Kolouri", "Soheil", ""], ["Saha", "Aniruddha", ""], ["Pirsiavash", "Hamed", ""], ["Hoffmann", "Heiko", ""]]}, {"id": "1906.10855", "submitter": "Yael Moses Prof", "authors": "Moti Kadosh, Yael Moses, Ariel Shamir", "title": "On the Role of Geometry in Geo-Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can build a mental map of a geographical area to find their way and\nrecognize places. The basic task we consider is geo-localization - finding the\npose (position & orientation) of a camera in a large 3D scene from a single\nimage. We aim to experimentally explore the role of geometry in\ngeo-localization in a convolutional neural network (CNN) solution. We do so by\nignoring the often available texture of the scene. We therefore deliberately\navoid using texture or rich geometric details and use images projected from a\nsimple 3D model of a city, which we term lean images. Lean images contain\nsolely information that relates to the geometry of the area viewed (edges,\nfaces, or relative depth). We find that the network is capable of estimating\nthe camera pose from the lean images, and it does so not by memorization but by\nsome measure of geometric learning of the geographical area. The main\ncontributions of this paper are: (i) providing insight into the role of\ngeometry in the CNN learning process; and (ii) demonstrating the power of CNNs\nfor recovering camera pose using lean images.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 05:47:21 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Kadosh", "Moti", ""], ["Moses", "Yael", ""], ["Shamir", "Ariel", ""]]}, {"id": "1906.10881", "submitter": "Ammar Mahmood Mr.", "authors": "Ammar Mahmood, Ana Giraldo Ospina, Mohammed Bennamoun, Senjian An,\n  Ferdous Sohel, Farid Boussaid, Renae Hovey, Robert B. Fisher, Gary Kendrick", "title": "Automatic Hierarchical Classification of Kelps using Deep Residual\n  Features", "comments": "MDPI Sensors", "journal-ref": "Sensors 2020, 20, 447", "doi": "10.3390/s20020447", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Across the globe, remote image data is rapidly being collected for the\nassessment of benthic communities from shallow to extremely deep waters on\ncontinental slopes to the abyssal seas. Exploiting this data is presently\nlimited by the time it takes for experts to identify organisms found in these\nimages. With this limitation in mind, a large effort has been made globally to\nintroduce automation and machine learning algorithms to accelerate both\nclassification and assessment of marine benthic biota. One major issue lies\nwith organisms that move with swell and currents, like kelps. This paper\npresents an automatic hierarchical classification method (local binary\nclassification as opposed to the conventional flat classification) to classify\nkelps in images collected by autonomous underwater vehicles. The proposed kelp\nclassification approach exploits learned feature representations extracted from\ndeep residual networks. We show that these generic features outperform the\ntraditional off-the-shelf CNN features and the conventional hand-crafted\nfeatures. Experiments also demonstrate that the hierarchical classification\nmethod outperforms the traditional parallel multi-class classifications by a\nsignificant margin (90.0% vs 57.6% and 77.2% vs 59.0%) on Benthoz15 and\nRottnest datasets respectively. Furthermore, we compare different hierarchical\nclassification approaches and experimentally show that the sibling hierarchical\ntraining approach outperforms the inclusive hierarchical approach by a\nsignificant margin. We also report an application of our proposed method to\nstudy the change in kelp cover over time for annually repeated AUV surveys.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 07:29:18 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 06:45:18 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Mahmood", "Ammar", ""], ["Ospina", "Ana Giraldo", ""], ["Bennamoun", "Mohammed", ""], ["An", "Senjian", ""], ["Sohel", "Ferdous", ""], ["Boussaid", "Farid", ""], ["Hovey", "Renae", ""], ["Fisher", "Robert B.", ""], ["Kendrick", "Gary", ""]]}, {"id": "1906.10882", "submitter": "Boitumelo Ruf", "authors": "Sylvia Schmitz, Martin Weinmann, Boitumelo Ruf", "title": "Automatic Co-Registration of Aerial Imagery and Untextured Model Data\n  Utilizing Average Shading Gradients", "comments": null, "journal-ref": "Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., XLII-2/W13,\n  581-588, 2019", "doi": "10.5194/isprs-archives-XLII-2-W13-581-2019", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The comparison of current image data with existing 3D model data of a scene\nprovides an efficient method to keep models up to date. In order to transfer\ninformation between 2D and 3D data, a preliminary co-registration is necessary.\nIn this paper, we present a concept to automatically co-register aerial imagery\nand untextured 3D model data. To refine a given initial camera pose, our\nalgorithm computes dense correspondence fields using SIFT flow between gradient\nrepresentations of the model and camera image, from which 2D-3D correspondences\nare obtained. These correspondences are then used in an iterative optimization\nscheme to refine the initial camera pose by minimizing the reprojection error.\nSince it is assumed that the model does not contain texture information, our\nalgorithm is built up on an existing method based on Average Shading Gradients\n(ASG) to generate gradient images based on raw geometry information only. We\napply our algorithm for the co-registering of aerial photographs to an\nuntextured, noisy mesh model. We have investigated different magnitudes of\ninput error and show that the proposed approach can reduce the final\nreprojection error to a minimum of 1.27 plus-minus 0.54 pixels, which is less\nthan 10 % of its initial value. Furthermore, our evaluation shows that our\napproach outperforms the accuracy of a standard Iterative Closest Point (ICP)\nimplementation.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 07:29:36 GMT"}, {"version": "v2", "created": "Sat, 21 Sep 2019 20:20:25 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Schmitz", "Sylvia", ""], ["Weinmann", "Martin", ""], ["Ruf", "Boitumelo", ""]]}, {"id": "1906.10886", "submitter": "Peng Gao", "authors": "Zibin Zhou, Fei Wang, Wenjuan Xi, Huaying Chen, Peng Gao, Chengkang He", "title": "Joint Multi-frame Detection and Segmentation for Multi-cell Tracking", "comments": "Accepted by International Conference on Image and Graphics (ICIG\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking living cells in video sequence is difficult, because of cell\nmorphology and high similarities between cells. Tracking-by-detection methods\nare widely used in multi-cell tracking. We perform multi-cell tracking based on\nthe cell centroid detection, and the performance of the detector has high\nimpact on tracking performance. In this paper, UNet is utilized to extract\ninter-frame and intra-frame spatio-temporal information of cells. Detection\nperformance of cells in mitotic phase is improved by multi-frame input. Good\ndetection results facilitate multi-cell tracking. A mitosis detection algorithm\nis proposed to detect cell mitosis and the cell lineage is built up. Another\nUNet is utilized to acquire primary segmentation. Jointly using detection and\nprimary segmentation, cells can be fine segmented in highly dense cell\npopulation. Experiments are conducted to evaluate the effectiveness of our\nmethod, and results show its state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 07:41:11 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Zhou", "Zibin", ""], ["Wang", "Fei", ""], ["Xi", "Wenjuan", ""], ["Chen", "Huaying", ""], ["Gao", "Peng", ""], ["He", "Chengkang", ""]]}, {"id": "1906.10887", "submitter": "Jiayun Wang", "authors": "Jiayun Wang, Rudrasis Chakraborty, Stella X. Yu", "title": "Spatial Transformer for 3D Point Clouds", "comments": "To appear in IEEE Transactions on PAMI, 2021", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3070341", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are widely used for understanding 3D point clouds. At\neach point convolution layer, features are computed from local neighborhoods of\n3D points and combined for subsequent processing in order to extract semantic\ninformation. Existing methods adopt the same individual point neighborhoods\nthroughout the network layers, defined by the same metric on the fixed input\npoint coordinates. This common practice is easy to implement but not\nnecessarily optimal. Ideally, local neighborhoods should be different at\ndifferent layers, as more latent information is extracted at deeper layers. We\npropose a novel end-to-end approach to learn different non-rigid\ntransformations of the input point cloud so that optimal local neighborhoods\ncan be adopted at each layer. We propose both linear (affine) and non-linear\n(projective and deformable) spatial transformers for 3D point clouds. With\nspatial transformers on the ShapeNet part segmentation dataset, the network\nachieves higher accuracy for all categories, with 8\\% gain on earphones and\nrockets in particular. Our method also outperforms the state-of-the-art on\nother point cloud tasks such as classification, detection, and semantic\nsegmentation. Visualizations show that spatial transformers can learn features\nmore efficiently by dynamically altering local neighborhoods according to the\ngeometry and semantics of 3D shapes in spite of their within-category\nvariations. Our code is publicly available at\nhttps://github.com/samaonline/spatial-transformer-for-3d-point-clouds.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 07:41:13 GMT"}, {"version": "v2", "created": "Sat, 29 Jun 2019 07:55:34 GMT"}, {"version": "v3", "created": "Tue, 24 Sep 2019 04:54:07 GMT"}, {"version": "v4", "created": "Tue, 30 Mar 2021 03:22:54 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Wang", "Jiayun", ""], ["Chakraborty", "Rudrasis", ""], ["Yu", "Stella X.", ""]]}, {"id": "1906.10908", "submitter": "Tribhuvanesh Orekondy", "authors": "Tribhuvanesh Orekondy, Bernt Schiele, Mario Fritz", "title": "Prediction Poisoning: Towards Defenses Against DNN Model Stealing\n  Attacks", "comments": "ICLR 2020, Project page:\n  https://resources.mpi-inf.mpg.de/d2/orekondy/predpoison/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  High-performance Deep Neural Networks (DNNs) are increasingly deployed in\nmany real-world applications e.g., cloud prediction APIs. Recent advances in\nmodel functionality stealing attacks via black-box access (i.e., inputs in,\npredictions out) threaten the business model of such applications, which\nrequire a lot of time, money, and effort to develop. Existing defenses take a\npassive role against stealing attacks, such as by truncating predicted\ninformation. We find such passive defenses ineffective against DNN stealing\nattacks. In this paper, we propose the first defense which actively perturbs\npredictions targeted at poisoning the training objective of the attacker. We\nfind our defense effective across a wide range of challenging datasets and DNN\nmodel stealing attacks, and additionally outperforms existing defenses. Our\ndefense is the first that can withstand highly accurate model stealing attacks\nfor tens of thousands of queries, amplifying the attacker's error rate up to a\nfactor of 85$\\times$ with minimal impact on the utility for benign users.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 08:32:37 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 10:51:12 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Orekondy", "Tribhuvanesh", ""], ["Schiele", "Bernt", ""], ["Fritz", "Mario", ""]]}, {"id": "1906.10925", "submitter": "Ruoxiang Li", "authors": "Ruoxiang Li, Dianxi Shi, Yongjun Zhang, Kaiyue Li, Ruihao Li", "title": "FA-Harris: A Fast and Asynchronous Corner Detector for Event Cameras", "comments": "7 pages, 3 figures, Accepted by IROS 2019. Video:\n  https://www.youtube.com/watch?v=v5CcBVkmI6w&feature=youtu.be", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the emerging bio-inspired event cameras have demonstrated\npotentials for a wide range of robotic applications in dynamic environments. In\nthis paper, we propose a novel fast and asynchronous event-based corner\ndetection method which is called FA-Harris. FA-Harris consists of several\ncomponents, including an event filter, a Global Surface of Active Events\n(G-SAE) maintaining unit, a corner candidate selecting unit, and a corner\ncandidate refining unit. The proposed G-SAE maintenance algorithm and corner\ncandidate selection algorithm greatly enhance the real-time performance for\ncorner detection, while the corner candidate refinement algorithm maintains the\naccuracy of performance by using an improved event-based Harris detector.\nAdditionally, FA-Harris does not require artificially synthesized event-frames\nand can operate on asynchronous events directly. We implement the proposed\nmethod in C++ and evaluate it on public Event Camera Datasets. The results show\nthat our method achieves approximately 8x speed-up when compared with\npreviously reported event-based Harris detector, and with no compromise on the\naccuracy of performance.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 09:12:40 GMT"}, {"version": "v2", "created": "Sun, 14 Jul 2019 13:19:54 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 13:59:09 GMT"}, {"version": "v4", "created": "Wed, 28 Aug 2019 02:35:09 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Li", "Ruoxiang", ""], ["Shi", "Dianxi", ""], ["Zhang", "Yongjun", ""], ["Li", "Kaiyue", ""], ["Li", "Ruihao", ""]]}, {"id": "1906.10964", "submitter": "Mohammed Abdou", "authors": "Mohammed Abdou, Mahmoud Elkhateeb, Ibrahim Sobh, Ahmad Elsallab", "title": "End-to-End 3D-PointCloud Semantic Segmentation for Autonomous Driving", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D semantic scene labeling is a fundamental task for Autonomous Driving.\nRecent work shows the capability of Deep Neural Networks in labeling 3D point\nsets provided by sensors like LiDAR, and Radar. Imbalanced distribution of\nclasses in the dataset is one of the challenges that face 3D semantic scene\nlabeling task. This leads to misclassifying for the non-dominant classes which\nsuffer from two main problems: a) rare appearance in the dataset, and b) few\nsensor points reflected from one object of these classes. This paper proposes a\nWeighted Self-Incremental Transfer Learning as a generalized methodology that\nsolves the imbalanced training dataset problems. It re-weights the components\nof the loss function computed from individual classes based on their\nfrequencies in the training dataset, and applies Self-Incremental Transfer\nLearning by running the Neural Network model on non-dominant classes first,\nthen dominant classes one-by-one are added. The experimental results introduce\na new 3D point cloud semantic segmentation benchmark for KITTI dataset.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 10:45:50 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Abdou", "Mohammed", ""], ["Elkhateeb", "Mahmoud", ""], ["Sobh", "Ibrahim", ""], ["Elsallab", "Ahmad", ""]]}, {"id": "1906.10973", "submitter": "Yifeng Li", "authors": "Yifeng Li, Lingxi Xie, Ya Zhang, Rui Zhang, Yanfeng Wang, Qi Tian", "title": "Defending Adversarial Attacks by Correcting logits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating and eliminating adversarial examples has been an intriguing topic\nin the field of deep learning. While previous research verified that\nadversarial attacks are often fragile and can be defended via image-level\nprocessing, it remains unclear how high-level features are perturbed by such\nattacks. We investigate this issue from a new perspective, which purely relies\non logits, the class scores before softmax, to detect and defend adversarial\nattacks. Our defender is a two-layer network trained on a mixed set of clean\nand perturbed logits, with the goal being recovering the original prediction.\nUpon a wide range of adversarial attacks, our simple approach shows promising\nresults with relatively high accuracy in defense, and the defender can transfer\nacross attackers with similar properties. More importantly, our defender can\nwork in the scenarios that image data are unavailable, and enjoys high\ninterpretability especially at the semantic level.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 11:07:29 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Li", "Yifeng", ""], ["Xie", "Lingxi", ""], ["Zhang", "Ya", ""], ["Zhang", "Rui", ""], ["Wang", "Yanfeng", ""], ["Tian", "Qi", ""]]}, {"id": "1906.10996", "submitter": "Stefan Balke", "authors": "Stefan Balke, Matthias Dorfer, Luis Carvalho, Andreas Arzt, Gerhard\n  Widmer", "title": "Learning Soft-Attention Models for Tempo-invariant Audio-Sheet Music\n  Retrieval", "comments": "Accepted for publication at ISMIR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Connecting large libraries of digitized audio recordings to their\ncorresponding sheet music images has long been a motivation for researchers to\ndevelop new cross-modal retrieval systems. In recent years, retrieval systems\nbased on embedding space learning with deep neural networks got a step closer\nto fulfilling this vision. However, global and local tempo deviations in the\nmusic recordings still require careful tuning of the amount of temporal context\ngiven to the system. In this paper, we address this problem by introducing an\nadditional soft-attention mechanism on the audio input. Quantitative and\nqualitative results on synthesized piano data indicate that this attention\nincreases the robustness of the retrieval system by focusing on different parts\nof the input representation based on the tempo of the audio. Encouraged by\nthese results, we argue for the potential of attention models as a very general\ntool for many MIR tasks.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 11:52:49 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Balke", "Stefan", ""], ["Dorfer", "Matthias", ""], ["Carvalho", "Luis", ""], ["Arzt", "Andreas", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1906.11009", "submitter": "Luc Brun", "authors": "Nicolas Boria, S'ebastien Bougleux, Benoit Ga\\\"uz\\`ere (LITIS), Luc\n  Brun", "title": "Generalized Median Graph via Iterative Alternate Minimizations", "comments": null, "journal-ref": "IAPR International workshop on Graph-Based Representation in\n  Pattern Recognition, Donatello Conte, Jean-Yves Ramel,, Jun 2019, Tours,\n  France. pp.99-109", "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing a graph prototype may constitute a core element for clustering or\nclassification tasks. However, its computation is an NP-Hard problem, even for\nsimple classes of graphs. In this paper, we propose an efficient approach based\non block coordinate descent to compute a generalized median graph from a set of\ngraphs. This approach relies on a clear definition of the optimization process\nand handles labeling on both edges and nodes. This iterative process optimizes\nthe edit operations to perform on a graph alternatively on nodes and edges.\nSeveral experiments on different datasets show the efficiency of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 12:04:55 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Boria", "Nicolas", "", "LITIS"], ["Bougleux", "S'ebastien", "", "LITIS"], ["Ga\u00fcz\u00e8re", "Benoit", "", "LITIS"], ["Brun", "Luc", ""]]}, {"id": "1906.11010", "submitter": "Shervan Fekri-Ershad", "authors": "Shervan Fekri-Ershad and Farshad Tajeripour", "title": "Color Texture Classification Based on Proposed Impulse-Noise Resistant\n  Color Local Binary Patterns and Significant Points Selection Algorithm", "comments": "10 pages, 7 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The main aim of this paper is to propose a color texture classification\napproach which uses color sensor information and texture features jointly. High\naccuracy, low noise sensitivity and low computational complexity are specified\naims for our proposed approach. One of the efficient texture analysis\noperations is local binary patterns. The proposed approach includes two steps.\nFirst, a noise resistant version of color local binary patterns is proposed to\ndecrease sensitivity to noise of LBP. This step is evaluated based on\ncombination of color sensor information using AND operation. In second step, a\nsignificant points selection algorithm is proposed to select significant LBP.\nThis phase decreases final computational complexity along with increasing\naccuracy rate. The Proposed approach is evaluated using Vistex, Outex, and KTH\nTIPS2a data sets. Our approach has been compared with some state of the art\nmethods. It is experimentally demonstrated that the proposed approach achieves\nhighest accuracy. In two other experiments, result show low noise sensitivity\nand low computational complexity of the proposed approach in comparison with\nprevious versions of LBP. Rotation invariant, multi resolution, general\nusability are other advantages of our proposed approach. In the present paper,\na new version of LBP is proposed originally, which is called Hybrid color local\nbinary patterns. It can be used in many image processing applications to\nextract color and texture features jointly. Also, a significant point selection\nalgorithm is proposed for the first time to select key points of images.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 12:10:01 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Fekri-Ershad", "Shervan", ""], ["Tajeripour", "Farshad", ""]]}, {"id": "1906.11014", "submitter": "Reuben R Shamir", "authors": "Reuben R Shamir and Zeev Bomzon", "title": "Evaluation of head segmentation quality for treatment planning of tumor\n  treating fields in brain tumors", "comments": "published as a long abstract in IPCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tumor treating fields (TTFields) is an FDA approved therapy for the treatment\nof Gliobastoma Multiform (GBM) and currently being investigated for additional\ntumor types. TTFields are delivered to the tumor through the placement of\ntransducer arrays (TAs) placed on the patient scalp. The positions of the TAs\nare associated with treatment outcomes via simulations of the electric fields.\nTherefore, we are currently developing a method for recommending optimal\nplacement of TAs. A key step to achieve this goal is to correctly segment the\nhead into tissues of similar electrical properties. Visual inspection of\nsegmentation quality is invaluable but time-consuming. Automatic quality\nassessment can assist in automatic refinement of the segmentation parameters,\nsuggest flaw points to the user and indicate if the segmented method is of\nsufficient accuracy for TTFields simulation. As a first step in this direction,\nwe identified a set of features that are relevant to atlas-based segmentation\nand show that these are significantly correlated (p < 0.05) with a similarity\nmeasure between validated and automatically computed segmentations.\nFurthermore, we incorporated these features in a decision tree regressor to\npredict the similarity of the validated and computed segmentations of 20\nTTFields patients using a leave-one-out approach. The predicted similarity\nmeasures were highly correlated with the actual ones (average abs. difference\n3% (SD = 3%); r = 0.92, p < 0.001). We conclude that quality estimation of\nsegmentations is feasible by incorporating machine learning and\nsegmentation-relevant features.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 12:17:00 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Shamir", "Reuben R", ""], ["Bomzon", "Zeev", ""]]}, {"id": "1906.11031", "submitter": "Reuben R Shamir", "authors": "Reuben R Shamir, Yuval Duchin, Jinyoung Kim, Guillermo Sapiro, and\n  Noam Harel", "title": "Continuous Dice Coefficient: a Method for Evaluating Probabilistic\n  Segmentations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Overlapping measures are often utilized to quantify the similarity\nbetween two binary regions. However, modern segmentation algorithms output a\nprobability or confidence map with continuous values in the zero-to-one\ninterval. Moreover, these binary overlapping measures are biased to structure\nsize. Addressing these challenges is the objective of this work. Methods: We\nextend the definition of the classical Dice coefficient (DC) overlap to\nfacilitate the direct comparison of a ground truth binary image with a\nprobabilistic map. We call the extended method continuous Dice coefficient\n(cDC) and show that 1) cDC is less or equal to 1 and cDC = 1 if-and-only-if the\nstructures overlap is complete, and, 2) cDC is monotonically decreasing with\nthe amount of overlap. We compare the classical DC and the cDC in a simulation\nof partial volume effects that incorporates segmentations of common targets for\ndeep-brainstimulation. Lastly, we investigate the cDC for an automatic\nsegmentation of the subthalamic-nucleus. Results: Partial volume effect\nsimulation on thalamus (large structure) resulted with DC and cDC averages (SD)\nof 0.98 (0.006) and 0.99 (0.001), respectively. For subthalamic-nucleus (small\nstructure) DC and cDC were 0.86 (0.025) and 0.97 (0.006), respectively. The DC\nand cDC for automatic STN segmentation were 0.66 and 0.80, respectively.\nConclusion: The cDC is well defined for probabilistic segmentation, less biased\nto structure size and more robust to partial volume effects in comparison to\nDC. Significance: The proposed method facilitates a better evaluation of\nsegmentation algorithms. As a better measurement tool, it opens the door for\nthe development of better segmentation methods.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 12:35:00 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Shamir", "Reuben R", ""], ["Duchin", "Yuval", ""], ["Kim", "Jinyoung", ""], ["Sapiro", "Guillermo", ""], ["Harel", "Noam", ""]]}, {"id": "1906.11052", "submitter": "Alex Hern\\'andez Garc\\'ia", "authors": "Alex Hern\\'andez-Garc\\'ia, Peter K\\\"onig", "title": "Further advantages of data augmentation on convolutional neural networks", "comments": "Preprint of the manuscript accepted for presentation at the\n  International Conference on Artificial Neural Networks (ICANN) 2018. Best\n  Paper Award", "journal-ref": "Artificial Neural Networks and Machine Learning - ICANN 2018.\n  ICANN 2018. Lecture Notes in Computer Science, vol 11139. Springer, Cham", "doi": "10.1007/978-3-030-01418-6_10", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is a popular technique largely used to enhance the training\nof convolutional neural networks. Although many of its benefits are well known\nby deep learning researchers and practitioners, its implicit regularization\neffects, as compared to popular explicit regularization techniques, such as\nweight decay and dropout, remain largely unstudied. As a matter of fact,\nconvolutional neural networks for image object classification are typically\ntrained with both data augmentation and explicit regularization, assuming the\nbenefits of all techniques are complementary. In this paper, we systematically\nanalyze these techniques through ablation studies of different network\narchitectures trained with different amounts of training data. Our results\nunveil a largely ignored advantage of data augmentation: networks trained with\njust data augmentation more easily adapt to different architectures and amount\nof training data, as opposed to weight decay and dropout, which require\nspecific fine-tuning of their hyperparameters.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 12:50:13 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Hern\u00e1ndez-Garc\u00eda", "Alex", ""], ["K\u00f6nig", "Peter", ""]]}, {"id": "1906.11096", "submitter": "Marc Eder", "authors": "Marc Eder, True Price, Thanh Vu, Akash Bapat, Jan-Michael Frahm", "title": "Mapped Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a versatile formulation of the convolution operation that we term\na \"mapped convolution.\" The standard convolution operation implicitly samples\nthe pixel grid and computes a weighted sum. Our mapped convolution decouples\nthese two components, freeing the operation from the confines of the image grid\nand allowing the kernel to process any type of structured data. As a test case,\nwe demonstrate its use by applying it to dense inference on spherical data. We\nperform an in-depth study of existing spherical image convolution methods and\npropose an improved sampling method for equirectangular images. Then, we\ndiscuss the impact of data discretization when deriving a sampling function,\nhighlighting drawbacks of the cube map representation for spherical data.\nFinally, we illustrate how mapped convolutions enable us to convolve directly\non a mesh by projecting the spherical image onto a geodesic grid and training\non the textured mesh. This method exceeds the state of the art for spherical\ndepth estimation by nearly 17%. Our findings suggest that mapped convolutions\ncan be instrumental in expanding the application scope of convolutional neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 13:44:40 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Eder", "Marc", ""], ["Price", "True", ""], ["Vu", "Thanh", ""], ["Bapat", "Akash", ""], ["Frahm", "Jan-Michael", ""]]}, {"id": "1906.11109", "submitter": "Davy Neven", "authors": "Davy Neven, Bert De Brabandere, Marc Proesmans and Luc Van Gool", "title": "Instance Segmentation by Jointly Optimizing Spatial Embeddings and\n  Clustering Bandwidth", "comments": "added references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art instance segmentation methods are not suited for\nreal-time applications like autonomous driving, which require fast execution\ntimes at high accuracy. Although the currently dominant proposal-based methods\nhave high accuracy, they are slow and generate masks at a fixed and low\nresolution. Proposal-free methods, by contrast, can generate masks at high\nresolution and are often faster, but fail to reach the same accuracy as the\nproposal-based methods. In this work we propose a new clustering loss function\nfor proposal-free instance segmentation. The loss function pulls the spatial\nembeddings of pixels belonging to the same instance together and jointly learns\nan instance-specific clustering bandwidth, maximizing the\nintersection-over-union of the resulting instance mask. When combined with a\nfast architecture, the network can perform instance segmentation in real-time\nwhile maintaining a high accuracy. We evaluate our method on the challenging\nCityscapes benchmark and achieve top results (5\\% improvement over Mask R-CNN)\nat more than 10 fps on 2MP images. Code will be available at\nhttps://github.com/davyneven/SpatialEmbeddings .\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 13:58:45 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 12:24:44 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Neven", "Davy", ""], ["De Brabandere", "Bert", ""], ["Proesmans", "Marc", ""], ["Van Gool", "Luc", ""]]}, {"id": "1906.11118", "submitter": "Nicolas Brieu", "authors": "Ansh Kapil, Tobias Wiestler, Simon Lanzmich, Abraham Silva, Keith\n  Steele, Marlon Rebelatto, Guenter Schmidt, Nicolas Brieu", "title": "DASGAN -- Joint Domain Adaptation and Segmentation for the Analysis of\n  Epithelial Regions in Histopathology PD-L1 Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of the tumor environment on digital histopathology slides is\nbecoming key for the understanding of the immune response against cancer,\nsupporting the development of novel immuno-therapies. We introduce here a novel\ndeep learning solution to the related problem of tumor epithelium segmentation.\nWhile most existing deep learning segmentation approaches are trained on\ntime-consuming and costly manual annotation on single stain domain (PD-L1), we\nleverage here semi-automatically labeled images from a second stain domain\n(Cytokeratin-CK). We introduce an end-to-end trainable network that jointly\nsegment tumor epithelium on PD-L1 while leveraging unpaired image-to-image\ntranslation between CK and PD-L1, therefore completely bypassing the need for\nserial sections or re-staining of slides. Extending the method to differentiate\nbetween PD-L1 positive and negative tumor epithelium regions enables the\nautomated estimation of the PD-L1 Tumor Cell (TC) score. Quantitative\nexperimental results demonstrate the accuracy of our approach against\nstate-of-the-art segmentation methods.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 14:23:50 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Kapil", "Ansh", ""], ["Wiestler", "Tobias", ""], ["Lanzmich", "Simon", ""], ["Silva", "Abraham", ""], ["Steele", "Keith", ""], ["Rebelatto", "Marlon", ""], ["Schmidt", "Guenter", ""], ["Brieu", "Nicolas", ""]]}, {"id": "1906.11129", "submitter": "Rajeev Yasarla", "authors": "Rajeev Yasarla and Vishal M. Patel", "title": "Uncertainty Guided Multi-Scale Residual Learning-using a Cycle Spinning\n  CNN for Single Image De-Raining", "comments": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image de-raining is an extremely challenging problem since the rainy\nimage may contain rain streaks which may vary in size, direction and density.\nPrevious approaches have attempted to address this problem by leveraging some\nprior information to remove rain streaks from a single image. One of the major\nlimitations of these approaches is that they do not consider the location\ninformation of rain drops in the image. The proposed Uncertainty guided\nMulti-scale Residual Learning (UMRL) network attempts to address this issue by\nlearning the rain content at different scales and using them to estimate the\nfinal de-rained output. In addition, we introduce a technique which guides the\nnetwork to learn the network weights based on the confidence measure about the\nestimate. Furthermore, we introduce a new training and testing procedure based\non the notion of cycle spinning to improve the final de-raining performance.\nExtensive experiments on synthetic and real datasets to demonstrate that the\nproposed method achieves significant improvements over the recent\nstate-of-the-art methods. Code is available at:\nhttps://github.com/rajeevyasarla/UMRL--using-Cycle-Spinning\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 17:13:50 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Yasarla", "Rajeev", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1906.11143", "submitter": "Shujun Wang", "authors": "Shujun Wang, Lequan Yu, Kang Li, Xin Yang, Chi-Wing Fu, and Pheng-Ann\n  Heng", "title": "Boundary and Entropy-driven Adversarial Learning for Fundus Image\n  Segmentation", "comments": "Accepted at MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmentation of the optic disc (OD) and cup (OC)in fundus images\nfrom different datasets is critical for glaucoma disease screening. The\ncross-domain discrepancy (domain shift) hinders the generalization of deep\nneural networks to work on different domain datasets.In this work, we present\nan unsupervised domain adaptation framework,called Boundary and Entropy-driven\nAdversarial Learning (BEAL), to improve the OD and OC segmentation performance,\nespecially on the ambiguous boundary regions. In particular, our proposed BEAL\nframe-work utilizes the adversarial learning to encourage the boundary\nprediction and mask probability entropy map (uncertainty map) of the target\ndomain to be similar to the source ones, generating more accurate boundaries\nand suppressing the high uncertainty predictions of OD and OC segmentation. We\nevaluate the proposed BEAL framework on two public retinal fundus image\ndatasets (Drishti-GS and RIM-ONE-r3), and the experiment results demonstrate\nthat our method outperforms the state-of-the-art unsupervised domain adaptation\nmethods. Codes will be available at https://github.com/EmmaW8/BEAL.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 14:59:40 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 09:14:28 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Wang", "Shujun", ""], ["Yu", "Lequan", ""], ["Li", "Kang", ""], ["Yang", "Xin", ""], ["Fu", "Chi-Wing", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1906.11172", "submitter": "Ekin Dogus Cubuk", "authors": "Barret Zoph, Ekin D. Cubuk, Golnaz Ghiasi, Tsung-Yi Lin, Jonathon\n  Shlens, Quoc V. Le", "title": "Learning Data Augmentation Strategies for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is a critical component of training deep learning models.\nAlthough data augmentation has been shown to significantly improve image\nclassification, its potential has not been thoroughly investigated for object\ndetection. Given the additional cost for annotating images for object\ndetection, data augmentation may be of even greater importance for this\ncomputer vision task. In this work, we study the impact of data augmentation on\nobject detection. We first demonstrate that data augmentation operations\nborrowed from image classification may be helpful for training detection\nmodels, but the improvement is limited. Thus, we investigate how learned,\nspecialized data augmentation policies improve generalization performance for\ndetection models. Importantly, these augmentation policies only affect training\nand leave a trained model unchanged during evaluation. Experiments on the COCO\ndataset indicate that an optimized data augmentation policy improves detection\naccuracy by more than +2.3 mAP, and allow a single inference model to achieve a\nstate-of-the-art accuracy of 50.7 mAP. Importantly, the best policy found on\nCOCO may be transferred unchanged to other detection datasets and models to\nimprove predictive accuracy. For example, the best augmentation policy\nidentified with COCO improves a strong baseline on PASCAL-VOC by +2.7 mAP. Our\nresults also reveal that a learned augmentation policy is superior to\nstate-of-the-art architecture regularization methods for object detection, even\nwhen considering strong baselines. Code for training with the learned policy is\navailable online at\nhttps://github.com/tensorflow/tpu/tree/master/models/official/detection\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 15:39:40 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Zoph", "Barret", ""], ["Cubuk", "Ekin D.", ""], ["Ghiasi", "Golnaz", ""], ["Lin", "Tsung-Yi", ""], ["Shlens", "Jonathon", ""], ["Le", "Quoc V.", ""]]}, {"id": "1906.11176", "submitter": "Stephen James", "authors": "Stephen James, Marc Freese, Andrew J. Davison", "title": "PyRep: Bringing V-REP to Deep Robot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PyRep is a toolkit for robot learning research, built on top of the virtual\nrobotics experimentation platform (V-REP). Through a series of modifications\nand additions, we have created a tailored version of V-REP built with robot\nlearning in mind. The new PyRep toolkit offers three improvements: (1) a simple\nand flexible API for robot control and scene manipulation, (2) a new rendering\nengine, and (3) speed boosts upwards of 10,000x in comparison to the previous\nPython Remote API. With these improvements, we believe PyRep is the ideal\ntoolkit to facilitate rapid prototyping of learning algorithms in the areas of\nreinforcement learning, imitation learning, state estimation, mapping, and\ncomputer vision.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 15:43:41 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["James", "Stephen", ""], ["Freese", "Marc", ""], ["Davison", "Andrew J.", ""]]}, {"id": "1906.11182", "submitter": "Maxim Bazik", "authors": "Maxim Bazik, Brien Flewelling, Manoranjan Majji, Joseph Mundy", "title": "Bayesian Inference of Spacecraft Pose using Particle Filtering", "comments": null, "journal-ref": "Proc. AMOS Technical Conf. (2018) 757-762", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated 3D pose estimation of satellites and other known space objects is a\ncritical component of space situational awareness. Ground-based imagery offers\na convenient data source for satellite characterization; however, analysis\nalgorithms must contend with atmospheric distortion, variable lighting, and\nunknown reflectance properties. Traditional feature-based pose estimation\napproaches are unable to discover an accurate correlation between a known 3D\nmodel and imagery given this challenging image environment. This paper presents\nan innovative method for automated 3D pose estimation of known space objects in\nthe absence of satisfactory texture. The proposed approach fits the silhouette\nof a known satellite model to ground-based imagery via particle filtering. Each\nparticle contains enough information (orientation, position, scale, model\narticulation) to generate an accurate object silhouette. The silhouette of\nindividual particles is compared to an observed image. Comparison is done\nprobabilistically by calculating the joint probability that pixels inside the\nsilhouette belong to the foreground distribution and that pixels outside the\nsilhouette belong to the background distribution. Both foreground and\nbackground distributions are computed by observing empty space. The population\nof particles are resampled at each new image observation, with the probability\nof a particle being resampled proportional to how the particle's silhouette\nmatches the observation image. The resampling process maintains multiple pose\nestimates which is beneficial in preventing and escaping local minimums.\nExperiments were conducted on both commercial imagery and on LEO satellite\nimagery. Imagery from the commercial experiments are shown in this paper.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 16:02:44 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Bazik", "Maxim", ""], ["Flewelling", "Brien", ""], ["Majji", "Manoranjan", ""], ["Mundy", "Joseph", ""]]}, {"id": "1906.11211", "submitter": "Shane Sims", "authors": "Shane D. Sims, Vanessa Putnam, Cristina Conati", "title": "Predicting Confusion from Eye-Tracking Data with Recurrent Neural\n  Networks", "comments": "This work was presented at the 2nd Workshop on Humanizing AI (HAI) at\n  IJCAI'19 in Macau, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encouraged by the success of deep learning in a variety of domains, we\ninvestigate the suitability and effectiveness of Recurrent Neural Networks\n(RNNs) in a domain where deep learning has not yet been used; namely detecting\nconfusion from eye-tracking data. Through experiments with a dataset of user\ninteractions with ValueChart (an interactive visualization tool), we found that\nRNNs learn a feature representation from the raw data that allows for a more\npowerful classifier than previous methods that use engineered features. This is\nevidenced by the stronger performance of the RNN (0.74/0.71\nsensitivity/specificity), as compared to a Random Forest classifier (0.51/0.70\nsensitivity/specificity), when both are trained on an un-augmented dataset.\nHowever, using engineered features allows for simple data augmentation methods\nto be used. These same methods are not as effective at augmentation for the\nfeature representation learned from the raw data, likely due to an inability to\nmatch the temporal dynamics of the data.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 04:47:16 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Sims", "Shane D.", ""], ["Putnam", "Vanessa", ""], ["Conati", "Cristina", ""]]}, {"id": "1906.11235", "submitter": "Fanny Yang", "authors": "Fanny Yang, Zuowen Wang, Christina Heinze-Deml", "title": "Invariance-inducing regularization using worst-case transformations\n  suffices to boost accuracy and spatial robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work provides theoretical and empirical evidence that\ninvariance-inducing regularizers can increase predictive accuracy for\nworst-case spatial transformations (spatial robustness). Evaluated on these\nadversarially transformed examples, we demonstrate that adding regularization\non top of standard or adversarial training reduces the relative error by 20%\nfor CIFAR10 without increasing the computational cost. This outperforms\nhandcrafted networks that were explicitly designed to be spatial-equivariant.\nFurthermore, we observe for SVHN, known to have inherent variance in\norientation, that robust training also improves standard accuracy on the test\nset. We prove that this no-trade-off phenomenon holds for adversarial examples\nfrom transformation groups in the infinite data limit.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 17:57:10 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Yang", "Fanny", ""], ["Wang", "Zuowen", ""], ["Heinze-Deml", "Christina", ""]]}, {"id": "1906.11282", "submitter": "Veronica Sanz", "authors": "Andrew Elkins, Felipe F. Freitas and Veronica Sanz", "title": "Developing an App to interpret Chest X-rays to support the diagnosis of\n  respiratory pathology with Artificial Intelligence", "comments": "28 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present our work to improve access to diagnosis in remote\nareas where good quality medical services may be lacking. We develop new\nMachine Learning methodologies for deployment onto mobile devices to help the\nearly diagnosis of a number of life-threatening conditions using X-ray images.\nBy using the latest developments in fast and portable Artificial Intelligence\nenvironments, we develop a smartphone app using an Artificial Neural Network to\nassist physicians in their diagnostic.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 18:14:18 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Elkins", "Andrew", ""], ["Freitas", "Felipe F.", ""], ["Sanz", "Veronica", ""]]}, {"id": "1906.11307", "submitter": "Vijay Janapa Reddi", "authors": "Matthew Halpern, Behzad Boroujerdian, Todd Mummert, Evelyn\n  Duesterwald, Vijay Janapa Reddi", "title": "One Size Does Not Fit All: Quantifying and Exposing the Accuracy-Latency\n  Trade-off in Machine Learning Cloud Service APIs via Tolerance Tiers", "comments": "2019 IEEE International Symposium on Performance Analysis of Systems\n  and Software (ISPASS)", "journal-ref": null, "doi": "10.1109/ISPASS.2019.00012", "report-no": null, "categories": "cs.LG cs.CV cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's cloud service architectures follow a \"one size fits all\" deployment\nstrategy where the same service version instantiation is provided to the end\nusers. However, consumers are broad and different applications have different\naccuracy and responsiveness requirements, which as we demonstrate renders the\n\"one size fits all\" approach inefficient in practice. We use a production-grade\nspeech recognition engine, which serves several thousands of users, and an open\nsource computer vision based system, to explain our point. To overcome the\nlimitations of the \"one size fits all\" approach, we recommend Tolerance Tiers\nwhere each MLaaS tier exposes an accuracy/responsiveness characteristic, and\nconsumers can programmatically select a tier. We evaluate our proposal on the\nCPU-based automatic speech recognition (ASR) engine and cutting-edge neural\nnetworks for image classification deployed on both CPUs and GPUs. The results\nshow that our proposed approach provides an MLaaS cloud service architecture\nthat can be tuned by the end API user or consumer to outperform the\nconventional \"one size fits all\" approach.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 19:35:59 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Halpern", "Matthew", ""], ["Boroujerdian", "Behzad", ""], ["Mummert", "Todd", ""], ["Duesterwald", "Evelyn", ""], ["Reddi", "Vijay Janapa", ""]]}, {"id": "1906.11335", "submitter": "Mariella Dimiccoli", "authors": "Mariella Dimiccoli and Herwig Wendt", "title": "Enhancing temporal segmentation by nonlocal self-similarity", "comments": "Accepted to ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal segmentation of untrimmed videos and photo-streams is currently an\nactive area of research in computer vision and image processing. This paper\nproposes a new approach to improve the temporal segmentation of photo-streams.\nThe method consists in enhancing image representations by encoding long-range\ntemporal dependencies. Our key contribution is to take advantage of the\ntemporal stationarity assumption of photostreams for modeling each frame by its\nnonlocal self-similarity function. The proposed approach is put to test on the\nEDUB-Seg dataset, a standard benchmark for egocentric photostream temporal\nsegmentation. Starting from seven different (CNN based) image features, the\nmethod yields consistent improvements in event segmentation quality, leading to\nan average increase of F-measure of 3.71% with respect to the state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 12:24:53 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Dimiccoli", "Mariella", ""], ["Wendt", "Herwig", ""]]}, {"id": "1906.11359", "submitter": "Siheng Chen", "authors": "Siheng Chen and Sufeng. Niu, Tian Lan and Baoan Liu", "title": "Large-scale 3D point cloud representations via graph inception networks\n  with applications to autonomous driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel graph-neural-network-based system to effectively represent\nlarge-scale 3D point clouds with the applications to autonomous driving. Many\nprevious works studied the representations of 3D point clouds based on two\napproaches, voxelization, which causes discretization errors and learning,\nwhich is hard to capture huge variations in large-scale scenarios. In this\nwork, we combine voxelization and learning: we discretize the 3D space into\nvoxels and propose novel graph inception networks to represent 3D points in\neach voxel. This combination makes the system avoid discretization errors and\nwork for large-scale scenarios. The entire system for large-scale 3D point\nclouds acts like the blocked discrete cosine transform for 2D images; we thus\ncall it the point cloud neural transform (PCT). We further apply the proposed\nPCT to represent real-time LiDAR sweeps produced by self-driving cars and the\nPCT with graph inception networks significantly outperforms its competitors.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 21:54:53 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Chen", "Siheng", ""], ["Niu", "Sufeng.", ""], ["Lan", "Tian", ""], ["Liu", "Baoan", ""]]}, {"id": "1906.11367", "submitter": "Linguang Zhang", "authors": "Linguang Zhang, Maciej Halber, Szymon Rusinkiewicz", "title": "Accelerating Large-Kernel Convolution Using Summed-Area Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expanding the receptive field to capture large-scale context is key to\nobtaining good performance in dense prediction tasks, such as human pose\nestimation. While many state-of-the-art fully-convolutional architectures\nenlarge the receptive field by reducing resolution using strided convolution or\npooling layers, the most straightforward strategy is adopting large filters.\nThis, however, is costly because of the quadratic increase in the number of\nparameters and multiply-add operations. In this work, we explore using\nlearnable box filters to allow for convolution with arbitrarily large kernel\nsize, while keeping the number of parameters per filter constant. In addition,\nwe use precomputed summed-area tables to make the computational cost of\nconvolution independent of the filter size. We adapt and incorporate the box\nfilter as a differentiable module in a fully-convolutional neural network, and\ndemonstrate its competitive performance on popular benchmarks for the task of\nhuman pose estimation.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 22:24:56 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Zhang", "Linguang", ""], ["Halber", "Maciej", ""], ["Rusinkiewicz", "Szymon", ""]]}, {"id": "1906.11407", "submitter": "Santhosh Kumar Ramakrishnan", "authors": "Santhosh K. Ramakrishnan, Dinesh Jayaraman, Kristen Grauman", "title": "Emergence of Exploratory Look-Around Behaviors through Active\n  Observation Completion", "comments": "Main paper 7 figures, supplementary 6 figures. Published in Science\n  Robotics 2019", "journal-ref": null, "doi": "10.1126/scirobotics.aaw6326", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard computer vision systems assume access to intelligently captured\ninputs (e.g., photos from a human photographer), yet autonomously capturing\ngood observations is a major challenge in itself. We address the problem of\nlearning to look around: how can an agent learn to acquire informative visual\nobservations? We propose a reinforcement learning solution, where the agent is\nrewarded for reducing its uncertainty about the unobserved portions of its\nenvironment. Specifically, the agent is trained to select a short sequence of\nglimpses after which it must infer the appearance of its full environment. To\naddress the challenge of sparse rewards, we further introduce sidekick policy\nlearning, which exploits the asymmetry in observability between training and\ntest time. The proposed methods learn observation policies that not only\nperform the completion task for which they are trained, but also generalize to\nexhibit useful \"look-around\" behavior for a range of active perception tasks.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 01:39:51 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Ramakrishnan", "Santhosh K.", ""], ["Jayaraman", "Dinesh", ""], ["Grauman", "Kristen", ""]]}, {"id": "1906.11415", "submitter": "Kaidi Cao", "authors": "Kaidi Cao, Jingwei Ji, Zhangjie Cao, Chien-Yi Chang, Juan Carlos\n  Niebles", "title": "Few-Shot Video Classification via Temporal Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in learning a model which could recognize novel\nclasses with only a few labeled examples. In this paper, we propose Temporal\nAlignment Module (TAM), a novel few-shot learning framework that can learn to\nclassify a previous unseen video. While most previous works neglect long-term\ntemporal ordering information, our proposed model explicitly leverages the\ntemporal ordering information in video data through temporal alignment. This\nleads to strong data-efficiency for few-shot learning. In concrete, TAM\ncalculates the distance value of query video with respect to novel class\nproxies by averaging the per frame distances along its alignment path. We\nintroduce continuous relaxation to TAM so the model can be learned in an\nend-to-end fashion to directly optimize the few-shot learning objective. We\nevaluate TAM on two challenging real-world datasets, Kinetics and\nSomething-Something-V2, and show that our model leads to significant\nimprovement of few-shot video classification over a wide range of competitive\nbaselines.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 03:04:16 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Cao", "Kaidi", ""], ["Ji", "Jingwei", ""], ["Cao", "Zhangjie", ""], ["Chang", "Chien-Yi", ""], ["Niebles", "Juan Carlos", ""]]}, {"id": "1906.11416", "submitter": "Shizhan Lu", "authors": "Shizhan Lu", "title": "Clustering by the way of atomic fission", "comments": "9 pages, 3 figures", "journal-ref": "IEEE ACCESS 2020", "doi": "10.1109/ACCESS.2020.2987345", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis which focuses on the grouping and categorization of similar\nelements is widely used in various fields of research. Inspired by the\nphenomenon of atomic fission, a novel density-based clustering algorithm is\nproposed in this paper, called fission clustering (FC). It focuses on mining\nthe dense families of a dataset and utilizes the information of the distance\nmatrix to fissure clustering dataset into subsets. When we face the dataset\nwhich has a few points surround the dense families of clusters, K-nearest\nneighbors local density indicator is applied to distinguish and remove the\npoints of sparse areas so as to obtain a dense subset that is constituted by\nthe dense families of clusters. A number of frequently-used datasets were used\nto test the performance of this clustering approach, and to compare the results\nwith those of algorithms. The proposed algorithm is found to outperform other\nalgorithms in speed and accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 03:05:13 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Lu", "Shizhan", ""]]}, {"id": "1906.11428", "submitter": "Linxi Huan", "authors": "Xianwei Zheng, Linxi Huan, Hanjiang Xiong, Jianya Gong", "title": "ELKPPNet: An Edge-aware Neural Network with Large Kernel Pyramid Pooling\n  for Learning Discriminative Features in Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation has been a hot topic across diverse research fields.\nAlong with the success of deep convolutional neural networks, semantic\nsegmentation has made great achievements and improvements, in terms of both\nurban scene parsing and indoor semantic segmentation. However, most of the\nstate-of-the-art models are still faced with a challenge in discriminative\nfeature learning, which limits the ability of a model to detect multi-scale\nobjects and to guarantee semantic consistency inside one object or distinguish\ndifferent adjacent objects with similar appearance. In this paper, a practical\nand efficient edge-aware neural network is presented for semantic segmentation.\nThis end-to-end trainable engine consists of a new encoder-decoder network, a\nlarge kernel spatial pyramid pooling (LKPP) block, and an edge-aware loss\nfunction. The encoder-decoder network was designed as a balanced structure to\nnarrow the semantic and resolution gaps in multi-level feature aggregation,\nwhile the LKPP block was constructed with a densely expanding receptive field\nfor multi-scale feature extraction and fusion. Furthermore, the new powerful\nedge-aware loss function is proposed to refine the boundaries directly from the\nsemantic segmentation prediction for more robust and discriminative features.\nThe effectiveness of the proposed model was demonstrated using Cityscapes,\nCamVid, and NYUDv2 benchmark datasets. The performance of the two structures\nand the edge-aware loss function in ELKPPNet was validated on the Cityscapes\ndataset, while the complete ELKPPNet was evaluated on the CamVid and NYUDv2\ndatasets. A comparative analysis with the state-of-the-art methods under the\nsame conditions confirmed the superiority of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 03:58:45 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Zheng", "Xianwei", ""], ["Huan", "Linxi", ""], ["Xiong", "Hanjiang", ""], ["Gong", "Jianya", ""]]}, {"id": "1906.11435", "submitter": "Yimin Lin", "authors": "Liming Han and Yimin Lin and Guoguang Du and Shiguo Lian", "title": "DeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial\n  Odometry using 3D Geometric Constraints", "comments": "Accepted by IROS 2019, demo video:\n  https://www.youtube.com/watch?v=fMeqCcpBCdM&feature=youtu.be", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an self-supervised deep learning network for monocular\nvisual inertial odometry (named DeepVIO). DeepVIO provides absolute trajectory\nestimation by directly merging 2D optical flow feature (OFF) and Inertial\nMeasurement Unit (IMU) data. Specifically, it firstly estimates the depth and\ndense 3D point cloud of each scene by using stereo sequences, and then obtains\n3D geometric constraints including 3D optical flow and 6-DoF pose as\nsupervisory signals. Note that such 3D optical flow shows robustness and\naccuracy to dynamic objects and textureless environments. In DeepVIO training,\n2D optical flow network is constrained by the projection of its corresponding\n3D optical flow, and LSTM-style IMU preintegration network and the fusion\nnetwork are learned by minimizing the loss functions from ego-motion\nconstraints. Furthermore, we employ an IMU status update scheme to improve IMU\npose estimation through updating the additional gyroscope and accelerometer\nbias. The experimental results on KITTI and EuRoC datasets show that DeepVIO\noutperforms state-of-the-art learning based methods in terms of accuracy and\ndata adaptability. Compared to the traditional methods, DeepVIO reduces the\nimpacts of inaccurate Camera-IMU calibrations, unsynchronized and missing data.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 04:56:12 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 05:54:46 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Han", "Liming", ""], ["Lin", "Yimin", ""], ["Du", "Guoguang", ""], ["Lian", "Shiguo", ""]]}, {"id": "1906.11437", "submitter": "Zhangxuan Gu", "authors": "Zhangxuan Gu, Li Niu, Haohua Zhao, Liqing Zhang", "title": "Hard Pixel Mining for Depth Privileged Semantic Segmentation", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation has achieved remarkable progress but remains\nchallenging due to the complex scene, object occlusion, and so on. Some\nresearch works have attempted to use extra information such as a depth map to\nhelp RGB based semantic segmentation because the depth map could provide\ncomplementary geometric cues. However, due to the inaccessibility of depth\nsensors, depth information is usually unavailable for the test images. In this\npaper, we leverage only the depth of training images as the privileged\ninformation to mine the hard pixels in semantic segmentation, in which depth\ninformation is only available for training images but not available for test\nimages. Specifically, we propose a novel Loss Weight Module, which outputs a\nloss weight map by employing two depth-related measurements of hard pixels:\nDepth Prediction Error and Depthaware Segmentation Error. The loss weight map\nis then applied to segmentation loss, with the goal of learning a more robust\nmodel by paying more attention to the hard pixels. Besides, we also explore a\ncurriculum learning strategy based on the loss weight map. Meanwhile, to fully\nmine the hard pixels on different scales, we apply our loss weight module to\nmulti-scale side outputs. Our hard pixels mining method achieves the\nstate-of-the-art results on two benchmark datasets, and even outperforms the\nmethods which need depth input during testing.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 05:01:04 GMT"}, {"version": "v2", "created": "Sat, 7 Sep 2019 09:26:02 GMT"}, {"version": "v3", "created": "Fri, 15 Nov 2019 01:49:28 GMT"}, {"version": "v4", "created": "Wed, 4 Mar 2020 07:37:01 GMT"}, {"version": "v5", "created": "Wed, 11 Mar 2020 01:28:53 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Gu", "Zhangxuan", ""], ["Niu", "Li", ""], ["Zhao", "Haohua", ""], ["Zhang", "Liqing", ""]]}, {"id": "1906.11443", "submitter": "Zhuotao Tian", "authors": "Zhuotao Tian, Hengshuang Zhao, Michelle Shu, Jiaze Wang, Ruiyu Li,\n  Xiaoyong Shen, Jiaya Jia", "title": "Region Refinement Network for Salient Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Albeit intensively studied, false prediction and unclear boundaries are still\nmajor issues of salient object detection. In this paper, we propose a Region\nRefinement Network (RRN), which recurrently filters redundant information and\nexplicitly models boundary information for saliency detection. Different from\nexisting refinement methods, we propose a Region Refinement Module (RRM) that\noptimizes salient region prediction by incorporating supervised attention masks\nin the intermediate refinement stages. The module only brings a minor increase\nin model size and yet significantly reduces false predictions from the\nbackground. To further refine boundary areas, we propose a Boundary Refinement\nLoss (BRL) that adds extra supervision for better distinguishing foreground\nfrom background. BRL is parameter free and easy to train. We further observe\nthat BRL helps retain the integrity in prediction by refining the boundary.\nExtensive experiments on saliency detection datasets show that our refinement\nmodule and loss bring significant improvement to the baseline and can be easily\napplied to different frameworks. We also demonstrate that our proposed model\ngeneralizes well to portrait segmentation and shadow detection tasks.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 05:45:44 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Tian", "Zhuotao", ""], ["Zhao", "Hengshuang", ""], ["Shu", "Michelle", ""], ["Wang", "Jiaze", ""], ["Li", "Ruiyu", ""], ["Shen", "Xiaoyong", ""], ["Jia", "Jiaya", ""]]}, {"id": "1906.11463", "submitter": "Hemin Ali Qadir", "authors": "Younghak Shin, Hemin Ali Qadir, Lars Aabakken, Jacob Bergsland, and\n  Ilangko Balasingham", "title": "Automatic Colon Polyp Detection using Region based Deep CNN and Post\n  Learning Approaches", "comments": "9 pages", "journal-ref": "IEEE Access 6 (2018): 40950-40962", "doi": "10.1109/ACCESS.2018.2856402", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic detection of colonic polyps is still an unsolved problem due to the\nlarge variation of polyps in terms of shape, texture, size, and color, and the\nexistence of various polyp-like mimics during colonoscopy. In this study, we\napply a recent region based convolutional neural network (CNN) approach for the\nautomatic detection of polyps in images and videos obtained from colonoscopy\nexaminations. We use a deep-CNN model (Inception Resnet) as a transfer learning\nscheme in the detection system. To overcome the polyp detection obstacles and\nthe small number of polyp images, we examine image augmentation strategies for\ntraining deep networks. We further propose two efficient post-learning methods\nsuch as, automatic false positive learning and off-line learning, both of which\ncan be incorporated with the region based detection system for reliable polyp\ndetection. Using the large size of colonoscopy databases, experimental results\ndemonstrate that the suggested detection systems show better performance\ncompared to other systems in the literature. Furthermore, we show improved\ndetection performance using the proposed post-learning schemes for colonoscopy\nvideos.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 07:18:44 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Shin", "Younghak", ""], ["Qadir", "Hemin Ali", ""], ["Aabakken", "Lars", ""], ["Bergsland", "Jacob", ""], ["Balasingham", "Ilangko", ""]]}, {"id": "1906.11465", "submitter": "Lei Wang", "authors": "Lei Wang and Du Q. Huynh and Moussa Reda Mansour", "title": "Loss Switching Fusion with Similarity Search for Video Classification", "comments": "Accepted by ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From video streaming to security and surveillance applications, video data\nplay an important role in our daily living today. However, managing a large\namount of video data and retrieving the most useful information for the user\nremain a challenging task. In this paper, we propose a novel video\nclassification system that would benefit the scene understanding task. We\ndefine our classification problem as classifying background and foreground\nmotions using the same feature representation for outdoor scenes. This means\nthat the feature representation needs to be robust enough and adaptable to\ndifferent classification tasks. We propose a lightweight Loss Switching Fusion\nNetwork (LSFNet) for the fusion of spatiotemporal descriptors and a similarity\nsearch scheme with soft voting to boost the classification performance. The\nproposed system has a variety of potential applications such as content-based\nvideo clustering, video filtering, etc. Evaluation results on two private\nindustry datasets show that our system is robust in both classifying different\nbackground motions and detecting human motions from these background motions.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 07:20:09 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Wang", "Lei", ""], ["Huynh", "Du Q.", ""], ["Mansour", "Moussa Reda", ""]]}, {"id": "1906.11467", "submitter": "Hemin Ali Qadir", "authors": "Younghak Shin, Hemin Ali Qadir, Ilangko Balasingham", "title": "Abnormal Colon Polyp Image Synthesis Using Conditional Adversarial\n  Networks for Improved Detection Performance", "comments": "10 pages", "journal-ref": "IEEE Access 6 (2018): 56007-56017", "doi": "10.1109/ACCESS.2018.2872717", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major obstacles in automatic polyp detection during colonoscopy is\nthe lack of labeled polyp training images. In this paper, we propose a\nframework of conditional adversarial networks to increase the number of\ntraining samples by generating synthetic polyp images. Using a normal binary\nform of polyp mask which represents only the polyp position as an input\nconditioned image, realistic polyp image generation is a difficult task in a\ngenerative adversarial networks approach. We propose an edge filtering-based\ncombined input conditioned image to train our proposed networks. This enables\nrealistic polyp image generations while maintaining the original structures of\nthe colonoscopy image frames. More importantly, our proposed framework\ngenerates synthetic polyp images from normal colonoscopy images which have the\nadvantage of being relatively easy to obtain. The network architecture is based\non the use of multiple dilated convolutions in each encoding part of our\ngenerator network to consider large receptive fields and avoid many\ncontractions of a feature map size. An image resizing with convolution for\nupsampling in the decoding layers is considered to prevent artifacts on\ngenerated images. We show that the generated polyp images are not only\nqualitatively realistic but also help to improve polyp detection performance.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 07:26:07 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Shin", "Younghak", ""], ["Qadir", "Hemin Ali", ""], ["Balasingham", "Ilangko", ""]]}, {"id": "1906.11470", "submitter": "Xiaomei Zhao", "authors": "Xiaomei Zhao, Yihong Wu", "title": "Automatically Extract the Semi-transparent Motion-blurred Hand from a\n  Single Image", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2019.2939754", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When we use video chat, video game, or other video applications,\nmotion-blurred hands often appear. Accurately extracting these hands is very\nuseful for video editing and behavior analysis. However, existing\nmotion-blurred object extraction methods either need user interactions, such as\nuser supplied trimaps and scribbles, or need additional information, such as\nbackground images. In this paper, a novel method which can automatically\nextract the semi-transparent motion-blurred hand just according to the original\nRGB image is proposed. The proposed method separates the extraction task into\ntwo subtasks: alpha matte prediction and foreground prediction. These two\nsubtasks are implemented by Xception based encoder-decoder networks. The\nextracted motion-blurred hand images can be calculated by multiplying the\npredicted alpha mattes and foreground images. Experiments on synthetic and real\ndatasets show that the proposed method has promising performance.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 07:32:53 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Zhao", "Xiaomei", ""], ["Wu", "Yihong", ""]]}, {"id": "1906.11478", "submitter": "Isaak Lim", "authors": "Isaak Lim, Moritz Ibing, Leif Kobbelt", "title": "A Convolutional Decoder for Point Clouds using Adaptive Instance\n  Normalization", "comments": "Symposium on Geometry Processing 2019", "journal-ref": "Computer Graphics Forum 38 (5), 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic synthesis of high quality 3D shapes is an ongoing and challenging\narea of research. While several data-driven methods have been proposed that\nmake use of neural networks to generate 3D shapes, none of them reach the level\nof quality that deep learning synthesis approaches for images provide. In this\nwork we present a method for a convolutional point cloud decoder/generator that\nmakes use of recent advances in the domain of image synthesis. Namely, we use\nAdaptive Instance Normalization and offer an intuition on why it can improve\ntraining. Furthermore, we propose extensions to the minimization of the\ncommonly used Chamfer distance for auto-encoding point clouds. In addition, we\nshow that careful sampling is important both for the input geometry and in our\npoint cloud generation process to improve results. The results are evaluated in\nan auto-encoding setup to offer both qualitative and quantitative analysis. The\nproposed decoder is validated by an extensive ablation study and is able to\noutperform current state of the art results in a number of experiments. We show\nthe applicability of our method in the fields of point cloud upsampling, single\nview reconstruction, and shape synthesis.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 07:53:02 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Lim", "Isaak", ""], ["Ibing", "Moritz", ""], ["Kobbelt", "Leif", ""]]}, {"id": "1906.11479", "submitter": "Hongruixuan Chen", "authors": "Hongruixuan Chen, Chen Wu, Bo Du, Liangpei Zhang", "title": "Change Detection in Multi-temporal VHR Images Based on Deep Siamese\n  Multi-scale Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very-high-resolution (VHR) images can provide abundant ground details and\nspatial geometric information. Change detection in multi-temporal VHR images\nplays a significant role in urban expansion and area internal change analysis.\nNevertheless, traditional change detection methods can neither take full\nadvantage of spatial context information nor cope with the complex internal\nheterogeneity of VHR images. In this paper, a powerful feature extraction model\nentitled multi-scale feature convolution unit (MFCU) is adopted for change\ndetection in multi-temporal VHR images. MFCU can extract multi-scale\nspatial-spectral features in the same layer. Based on the unit two novel deep\nsiamese convolutional neural networks, called as deep siamese multi-scale\nconvolutional network (DSMS-CN) and deep siamese multi-scale fully\nconvolutional network (DSMS-FCN), are designed for unsupervised and supervised\nchange detection, respectively. For unsupervised change detection, an automatic\npre-classification is implemented to obtain reliable training samples, then\nDSMS-CN fits the statistical distribution of changed and unchanged areas from\nselected training samples through MFCU modules and deep siamese architecture.\nFor supervised change detection, the end-to-end deep fully convolutional\nnetwork DSMS-FCN is trained in any size of multi-temporal VHR images, and\ndirectly outputs the binary change map. In addition, for the purpose of solving\nthe inaccurate localization problem, the fully connected conditional random\nfield (FC-CRF) is combined with DSMS-FCN to refine the results. The\nexperimental results with challenging data sets confirm that the two proposed\narchitectures perform better than the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 07:53:41 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 13:43:03 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Chen", "Hongruixuan", ""], ["Wu", "Chen", ""], ["Du", "Bo", ""], ["Zhang", "Liangpei", ""]]}, {"id": "1906.11484", "submitter": "Yuta Hiasa", "authors": "Mitsuki Sakamoto, Yuta Hiasa, Yoshito Otake, Masaki Takao, Yuki\n  Suzuki, Nobuhiko Sugano, Yoshinobu Sato", "title": "Automated Segmentation of Hip and Thigh Muscles in Metal\n  Artifact-Contaminated CT using Convolutional Neural Network-Enhanced\n  Normalized Metal Artifact Reduction", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In total hip arthroplasty, analysis of postoperative medical images is\nimportant to evaluate surgical outcome. Since Computed Tomography (CT) is most\nprevalent modality in orthopedic surgery, we aimed at the analysis of CT image.\nIn this work, we focus on the metal artifact in postoperative CT caused by the\nmetallic implant, which reduces the accuracy of segmentation especially in the\nvicinity of the implant. Our goal was to develop an automated segmentation\nmethod of the bones and muscles in the postoperative CT images. We propose a\nmethod that combines Normalized Metal Artifact Reduction (NMAR), which is one\nof the state-of-the-art metal artifact reduction methods, and a Convolutional\nNeural Network-based segmentation using two U-net architectures. The first\nU-net refines the result of NMAR and the muscle segmentation is performed by\nthe second U-net. We conducted experiments using simulated images of 20\npatients and real images of three patients to evaluate the segmentation\naccuracy of 19 muscles. In simulation study, the proposed method showed\nstatistically significant improvement (p<0.05) in the average symmetric surface\ndistance (ASD) metric for 14 muscles out of 19 muscles and the average ASD of\nall muscles from 1.17 +/- 0.543 mm (mean +/- std over all patients) to 1.10 +/-\n0.509 mm over our previous method. The real image study using the manual trace\nof gluteus maximus and medius muscles showed ASD of 1.32 +/- 0.25 mm. Our\nfuture work includes training of a network in an end-to-end manner for both the\nmetal artifact reduction and muscle segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 08:06:59 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Sakamoto", "Mitsuki", ""], ["Hiasa", "Yuta", ""], ["Otake", "Yoshito", ""], ["Takao", "Masaki", ""], ["Suzuki", "Yuki", ""], ["Sugano", "Nobuhiko", ""], ["Sato", "Yoshinobu", ""]]}, {"id": "1906.11555", "submitter": "Adrien Poulenard", "authors": "Adrien Poulenard, Marie-Julie Rakotosaona, Yann Ponty, Maks Ovsjanikov", "title": "Effective Rotation-invariant Point CNN with Spherical Harmonics kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel rotation invariant architecture operating directly on\npoint cloud data. We demonstrate how rotation invariance can be injected into a\nrecently proposed point-based PCNN architecture, at all layers of the network,\nachieving invariance to both global shape transformations, and to local\nrotations on the level of patches or parts, useful when dealing with non-rigid\nobjects. We achieve this by employing a spherical harmonics based kernel at\ndifferent layers of the network, which is guaranteed to be invariant to rigid\nmotions. We also introduce a more efficient pooling operation for PCNN using\nspace-partitioning data-structures. This results in a flexible, simple and\nefficient architecture that achieves accurate results on challenging shape\nanalysis tasks including classification and segmentation, without requiring\ndata-augmentation, typically employed by non-invariant approaches.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 11:30:49 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 14:53:54 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Poulenard", "Adrien", ""], ["Rakotosaona", "Marie-Julie", ""], ["Ponty", "Yann", ""], ["Ovsjanikov", "Maks", ""]]}, {"id": "1906.11561", "submitter": "Shervan Fekri-Ershad", "authors": "Shervan Fekri-Ershad", "title": "A New Benchmark Dataset for Texture Image Analysis and Surface Defect\n  Detection", "comments": "Texture Image Analysis, Benchmark Texture Dataset, Feature\n  Extraction, Surface Defect Detection, Image Processing, Texture\n  Classification, Visual Inspection Systems", "journal-ref": null, "doi": "10.13140/RG.2.2.33612.46722", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Texture analysis plays an important role in many image processing\napplications to describe the image content or objects. On the other hand,\nvisual surface defect detection is a highly research field in the computer\nvision. Surface defect refers to abnormalities in the texture of the surface.\nSo, in this paper a dual purpose benchmark dataset is proposed for texture\nimage analysis and surface defect detection titled stone texture image (STI\ndataset). The proposed benchmark dataset consist of 4 different class of stone\ntexture images. The proposed benchmark dataset have some unique properties to\nmake it very near to real applications. Local rotation, different zoom rates,\nunbalanced classes, variation of textures in size are some properties of the\nproposed dataset. In the result part, some descriptors are applied on this\ndataset to evaluate the proposed STI dataset in comparison with other\nstate-of-the-art datasets.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 11:36:29 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Fekri-Ershad", "Shervan", ""]]}, {"id": "1906.11577", "submitter": "Alejandro Frery", "authors": "Debanshu Ratha and Eric Pottier and Avik Bhattacharya and Alejandro C.\n  Frery", "title": "A PolSAR Scattering Power Factorization Framework and Novel\n  Roll-Invariant Parameters Based Unsupervised Classification Scheme Using a\n  Geodesic Distance", "comments": "Submitted to IEEE Transactions on Geoscience and Remote Sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generic Scattering Power Factorization Framework (SPFF) for\nPolarimetric Synthetic Aperture Radar (PolSAR) data to directly obtain $N$\nscattering power components along with a residue power component for each\npixel. Each scattering power component is factorized into similarity (or\ndissimilarity) using elementary targets and a generalized random volume model.\nThe similarity measure is derived using a geodesic distance between pairs of\n$4\\times4$ real Kennaugh matrices. In standard model-based decomposition\nschemes, the $3\\times3$ Hermitian positive semi-definite covariance (or\ncoherency) matrix is expressed as a weighted linear combination of scattering\ntargets following a fixed hierarchical process. In contrast, under the proposed\nframework, a convex splitting of unity is performed to obtain the weights while\npreserving the dominance of the scattering components. The product of the total\npower (Span) with these weights provides the non-negative scattering power\ncomponents. Furthermore, the framework along the geodesic distance is\neffectively used to obtain specific roll-invariant parameters which are then\nutilized to design an unsupervised classification scheme. The SPFF, the roll\ninvariant parameters, and the classification results are assessed using C-band\nRADARSAT-2 and L-band ALOS-2 images of San Francisco.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 12:06:00 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Ratha", "Debanshu", ""], ["Pottier", "Eric", ""], ["Bhattacharya", "Avik", ""], ["Frery", "Alejandro C.", ""]]}, {"id": "1906.11578", "submitter": "Anne-Ruth Meijer", "authors": "Anne-Ruth Jos\\'e Meijer and Arnoud Visser", "title": "A shallow residual neural network to predict the visual cortex response", "comments": "3 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how the visual cortex of the human brain really works is still\nan open problem for science today. A better understanding of natural\nintelligence could also benefit object-recognition algorithms based on\nconvolutional neural networks. In this paper we demonstrate the asset of using\na shallow residual neural network for this task. The benefit of this approach\nis that earlier stages of the network can be accurately trained, which allows\nus to add more layers at the earlier stage. With this additional layer the\nprediction of the visual brain activity improves from $10.4\\%$ (block 1) to\n$15.53\\%$ (last fully connected layer). By training the network for more than\n10 epochs this improvement can become even larger.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 12:06:55 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Meijer", "Anne-Ruth Jos\u00e9", ""], ["Visser", "Arnoud", ""]]}, {"id": "1906.11586", "submitter": "Evangello Flouty", "authors": "Maria Grammatikopoulou, Evangello Flouty, Abdolrahim\n  Kadkhodamohammadi, Gwenol'e Quellec, Andre Chow, Jean Nehme, Imanol Luengo\n  and Danail Stoyanov", "title": "CaDIS: Cataract Dataset for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video feedback provides a wealth of information about surgical procedures and\nis the main sensory cue for surgeons. Scene understanding is crucial to\ncomputer assisted interventions (CAI) and to post-operative analysis of the\nsurgical procedure. A fundamental building block of such capabilities is the\nidentification and localization of surgical instruments and anatomical\nstructures through semantic segmentation. Deep learning has advanced semantic\nsegmentation techniques in the recent years but is inherently reliant on the\navailability of labeled datasets for model training. This paper introduces a\ndataset for semantic segmentation of cataract surgery videos. The annotated\nimages are part of the publicly available CATARACTS challenge dataset. In\naddition, we benchmark the performance of several state-of-the-art deep\nlearning models for semantic segmentation on the presented dataset. The dataset\nis publicly available at https://cataracts.grand-challenge.org/CaDIS/ .\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 12:24:03 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 09:11:10 GMT"}, {"version": "v3", "created": "Mon, 1 Jul 2019 08:38:25 GMT"}, {"version": "v4", "created": "Fri, 19 Jul 2019 14:51:57 GMT"}, {"version": "v5", "created": "Thu, 2 Apr 2020 15:55:42 GMT"}, {"version": "v6", "created": "Fri, 3 Apr 2020 08:49:48 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Grammatikopoulou", "Maria", ""], ["Flouty", "Evangello", ""], ["Kadkhodamohammadi", "Abdolrahim", ""], ["Quellec", "Gwenol'e", ""], ["Chow", "Andre", ""], ["Nehme", "Jean", ""], ["Luengo", "Imanol", ""], ["Stoyanov", "Danail", ""]]}, {"id": "1906.11594", "submitter": "Deli Zhao", "authors": "Deli Zhao, Jiapeng Zhu, Zhenfang Guo, Bo Zhang", "title": "Curriculum Learning for Deep Generative Models with Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training generative models like Generative Adversarial Network (GAN) is\nchallenging for noisy data. A novel curriculum learning algorithm pertaining to\nclustering is proposed to address this issue in this paper. The curriculum\nconstruction is based on the centrality of underlying clusters in data points.\nThe data points of high centrality takes priority of being fed into generative\nmodels during training. To make our algorithm scalable to large-scale data, the\nactive set is devised, in the sense that every round of training proceeds only\non an active subset containing a small fraction of already trained data and the\nincremental data of lower centrality. Moreover, the geometric analysis is\npresented to interpret the necessity of cluster curriculum for generative\nmodels. The experiments on cat and human-face data validate that our algorithm\nis able to learn the optimal generative models (e.g. ProGAN) with respect to\nspecified quality metrics for noisy data. An interesting finding is that the\noptimal cluster curriculum is closely related to the critical point of the\ngeometric percolation process formulated in the paper.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 12:44:09 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 03:04:58 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Zhao", "Deli", ""], ["Zhu", "Jiapeng", ""], ["Guo", "Zhenfang", ""], ["Zhang", "Bo", ""]]}, {"id": "1906.11600", "submitter": "Etienne Decenciere", "authors": "Etienne Decenci\\`ere, Santiago Velasco-Forero, Fu Min, Juanjuan Chen,\n  H\\'el\\`ene Burdin, Gervais Gauthier, Bruno La\\\"y, Thomas Bornschloegl,\n  Th\\'er\\`ese Baldeweck", "title": "Dealing with Topological Information within a Fully Convolutional Neural\n  Network", "comments": "International Conference on Advanced Concepts for Intelligent Vision\n  Systems (ACIVS 2018)", "journal-ref": "Advanced Concepts for Intelligent Vision Systems. ACIVS 2018.\n  Lecture Notes in Computer Science, vol 11182. Springer, Cham", "doi": "10.1007/978-3-030-01449-0_39", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fully convolutional neural network has a receptive field of limited size\nand therefore cannot exploit global information, such as topological\ninformation. A solution is proposed in this paper to solve this problem, based\non pre-processing with a geodesic operator. It is applied to the segmentation\nof histological images of pigmented reconstructed epidermis acquired via Whole\nSlide Imaging.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 13:05:48 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Decenci\u00e8re", "Etienne", ""], ["Velasco-Forero", "Santiago", ""], ["Min", "Fu", ""], ["Chen", "Juanjuan", ""], ["Burdin", "H\u00e9l\u00e8ne", ""], ["Gauthier", "Gervais", ""], ["La\u00ff", "Bruno", ""], ["Bornschloegl", "Thomas", ""], ["Baldeweck", "Th\u00e9r\u00e8se", ""]]}, {"id": "1906.11613", "submitter": "Jean-Baptiste Gouray", "authors": "Ya\\\"el Fr\\'egier, Jean-Baptiste Gouray", "title": "Mind2Mind : transfer learning for GANs", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training generative adversarial networks (GANs) on high quality (HQ) images\ninvolves important computing resources. This requirement represents a\nbottleneck for the development of applications of GANs. We propose a transfer\nlearning technique for GANs that significantly reduces training time. Our\napproach consists of freezing the low-level layers of both the critic and\ngenerator of the original GAN. We assume an autoencoder constraint in order to\nensure the compatibility of the internal representations of the critic and the\ngenerator. This assumption explains the gain in training time as it enables us\nto bypass the low-level layers during the forward and backward passes. We\ncompare our method to baselines and observe a significant acceleration of the\ntraining. It can reach two orders of magnitude on HQ datasets when compared\nwith StyleGAN. We prove rigorously, within the framework of optimal transport,\na theorem ensuring the convergence of the learning of the transferred GAN. We\nmoreover provide a precise bound for the convergence of the training in terms\nof the distance between the source and target dataset.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 13:19:29 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 13:15:27 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Fr\u00e9gier", "Ya\u00ebl", ""], ["Gouray", "Jean-Baptiste", ""]]}, {"id": "1906.11661", "submitter": "Camille Couprie", "authors": "Baptiste Rozi\\`ere, Morgane Riviere, Olivier Teytaud, J\\'er\\'emy\n  Rapin, Yann LeCun, Camille Couprie", "title": "Inspirational Adversarial Image Generation", "comments": null, "journal-ref": "TIP 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of image generation started to receive some attention from artists\nand designers to inspire them in new creations. However, exploiting the results\nof deep generative models such as Generative Adversarial Networks can be long\nand tedious given the lack of existing tools. In this work, we propose a simple\nstrategy to inspire creators with new generations learned from a dataset of\ntheir choice, while providing some control on them. We design a simple\noptimization method to find the optimal latent parameters corresponding to the\nclosest generation to any input inspirational image. Specifically, we allow the\ngeneration given an inspirational image of the user choice by performing\nseveral optimization steps to recover optimal parameters from the model's\nlatent space. We tested several exploration methods starting with classic\ngradient descents to gradient-free optimizers. Many gradient-free optimizers\njust need comparisons (better/worse than another image), so that they can even\nbe used without numerical criterion, without inspirational image, but with only\nwith human preference. Thus, by iterating on one's preferences we could make\nrobust Facial Composite or Fashion Generation algorithms. High resolution of\nthe produced design generations are obtained using progressive growing of GANs.\nOur results on four datasets of faces, fashion images, and textures show that\nsatisfactory images are effectively retrieved in most cases.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 06:52:40 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 06:55:30 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Rozi\u00e8re", "Baptiste", ""], ["Riviere", "Morgane", ""], ["Teytaud", "Olivier", ""], ["Rapin", "J\u00e9r\u00e9my", ""], ["LeCun", "Yann", ""], ["Couprie", "Camille", ""]]}, {"id": "1906.11663", "submitter": "Aurobrata Ghosh", "authors": "Aurobrata Ghosh, Zheng Zhong, Terrance E Boult, Maneesh Singh", "title": "SpliceRadar: A Learned Method For Blind Image Forensics", "comments": "CVPR 2019, Workshop on Media Forensics, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection and localization of image manipulations like splices are gaining in\nimportance with the easy accessibility of image editing softwares. While\ndetection generates a verdict for an image it provides no insight into the\nmanipulation. Localization helps explain a positive detection by identifying\nthe pixels of the image which have been tampered. We propose a deep learning\nbased method for splice localization without prior knowledge of a test image's\ncamera-model. It comprises a novel approach for learning rich filters and for\nsuppressing image-edges. Additionally, we train our model on a surrogate task\nof camera model identification, which allows us to leverage large and widely\navailable, unmanipulated, camera-tagged image databases. During inference, we\nassume that the spliced and host regions come from different camera-models and\nwe segment these regions using a Gaussian-mixture model. Experiments on three\ntest databases demonstrate results on par with and above the state-of-the-art\nand a good generalization ability to unknown datasets.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 14:06:58 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Ghosh", "Aurobrata", ""], ["Zhong", "Zheng", ""], ["Boult", "Terrance E", ""], ["Singh", "Maneesh", ""]]}, {"id": "1906.11667", "submitter": "Shashank Kotyan", "authors": "Shashank Kotyan and Danilo Vasconcellos Vargas", "title": "Evolving Robust Neural Architectures to Defend from Adversarial Attacks", "comments": "Pre-print of the published article in Proceedings of the Workshop on\n  Artificial Intelligence Safety 2020, co-located with the 29th International\n  Joint Conference on Artificial Intelligence and the 17th Pacific Rim\n  International Conference on Artificial Intelligence (IJCAI-PRICAI 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CR cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural networks are prone to misclassify slightly modified input images.\nRecently, many defences have been proposed, but none have improved the\nrobustness of neural networks consistently. Here, we propose to use adversarial\nattacks as a function evaluation to search for neural architectures that can\nresist such attacks automatically. Experiments on neural architecture search\nalgorithms from the literature show that although accurate, they are not able\nto find robust architectures. A significant reason for this lies in their\nlimited search space. By creating a novel neural architecture search with\noptions for dense layers to connect with convolution layers and vice-versa as\nwell as the addition of concatenation layers in the search, we were able to\nevolve an architecture that is inherently accurate on adversarial samples.\nInterestingly, this inherent robustness of the evolved architecture rivals\nstate-of-the-art defences such as adversarial training while being trained only\non the non-adversarial samples. Moreover, the evolved architecture makes use of\nsome peculiar traits which might be useful for developing even more robust\nones. Thus, the results here confirm that more robust architectures exist as\nwell as opens up a new realm of feasibilities for the development and\nexploration of neural networks.\n  Code available at http://bit.ly/RobustArchitectureSearch.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 14:12:52 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 20:46:26 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 13:34:50 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Kotyan", "Shashank", ""], ["Vargas", "Danilo Vasconcellos", ""]]}, {"id": "1906.11675", "submitter": "Birgitta Dresp-Langley", "authors": "John Wandeto, Henry Nyongesa, Yves Remond, Birgitta Dresp-Langley", "title": "Detection of small changes in medical and random-dot images comparing\n  self-organizing map performance to human detection", "comments": "arXiv admin note: substantial text overlap with arXiv:1709.02292", "journal-ref": "Informatics in Medecine Unlocked, 2017, 7, 39-45", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radiologists use time series of medical images to monitor the progression of\na patient condition. They compare information gleaned from sequences of images\nto gain insight on progression or remission of the lesions, thus evaluating the\nprogress of a patient condition or response to therapy. Visual methods of\ndetermining differences between one series of images to another can be\nsubjective or fail to detect very small differences. We propose the use of\nquantization errors obtained from Self Organizing Maps for image content\nanalysis. We tested this technique with MRI images to which we progressively\nadded synthetic lesions. We have used a global approach that considers changes\non the entire image as opposed to changes in segmented lesion regions only. We\nclaim that this approach does not suffer from the limitations imposed by\nsegmentation, which may compromise the results. Results show quantization\nerrors increased with the increase in lesions on the images. The results are\nalso consistent with previous studies using alternative approaches. We then\ncompared the detectability ability of our method to that of human novice\nobservers having to detect very small local differences in random-dot images.\nThe quantization errors of the SOM outputs compared with correct positive\nrates, after subtraction of false positive rates (guess rates), increased\nnoticeably and consistently with small increases in local dot size that were\nnot detectable by humans. We conclude that our method detects very small\nchanges in complex images and suggest that it could be implemented to assist\nhuman operators in image based decision making.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 16:42:19 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Wandeto", "John", ""], ["Nyongesa", "Henry", ""], ["Remond", "Yves", ""], ["Dresp-Langley", "Birgitta", ""]]}, {"id": "1906.11713", "submitter": "Alberto Bailoni", "authors": "Alberto Bailoni, Constantin Pape, Steffen Wolf, Thorsten Beier, Anna\n  Kreshuk, Fred A. Hamprecht", "title": "A Generalized Framework for Agglomerative Clustering of Signed Graphs\n  applied to Instance Segmentation", "comments": "19 pages, 8 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel theoretical framework that generalizes algorithms for\nhierarchical agglomerative clustering to weighted graphs with both attractive\nand repulsive interactions between the nodes. This framework defines GASP, a\nGeneralized Algorithm for Signed graph Partitioning, and allows us to explore\nmany combinations of different linkage criteria and cannot-link constraints. We\nprove the equivalence of existing clustering methods to some of those\ncombinations, and introduce new algorithms for combinations which have not been\nstudied. An extensive comparison is performed to evaluate properties of the\nclustering algorithms in the context of instance segmentation in images,\nincluding robustness to noise and efficiency. We show how one of the new\nalgorithms proposed in our framework outperforms all previously known\nagglomerative methods for signed graphs, both on the competitive CREMI 2016 EM\nsegmentation benchmark and on the CityScapes dataset.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 15:00:16 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Bailoni", "Alberto", ""], ["Pape", "Constantin", ""], ["Wolf", "Steffen", ""], ["Beier", "Thorsten", ""], ["Kreshuk", "Anna", ""], ["Hamprecht", "Fred A.", ""]]}, {"id": "1906.11729", "submitter": "Guanxiong Liu", "authors": "Guanxiong Liu, Issa Khalil, Abdallah Khreishah", "title": "Using Intuition from Empirical Properties to Simplify Adversarial\n  Training Defense", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the surprisingly good representation power of complex distributions,\nneural network (NN) classifiers are widely used in many tasks which include\nnatural language processing, computer vision and cyber security. In recent\nworks, people noticed the existence of adversarial examples. These adversarial\nexamples break the NN classifiers' underlying assumption that the environment\nis attack free and can easily mislead fully trained NN classifier without\nnoticeable changes. Among defensive methods, adversarial training is a popular\nchoice. However, original adversarial training with single-step adversarial\nexamples (Single-Adv) can not defend against iterative adversarial examples.\nAlthough adversarial training with iterative adversarial examples (Iter-Adv)\ncan defend against iterative adversarial examples, it consumes too much\ncomputational power and hence is not scalable. In this paper, we analyze\nIter-Adv techniques and identify two of their empirical properties. Based on\nthese properties, we propose modifications which enhance Single-Adv to perform\ncompetitively as Iter-Adv. Through preliminary evaluation, we show that the\nproposed method enhances the test accuracy of state-of-the-art (SOTA)\nSingle-Adv defensive method against iterative adversarial examples by up to\n16.93% while reducing its training cost by 28.75%.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 15:22:56 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Liu", "Guanxiong", ""], ["Khalil", "Issa", ""], ["Khreishah", "Abdallah", ""]]}, {"id": "1906.11796", "submitter": "Aviv Gabbay", "authors": "Aviv Gabbay and Yedid Hoshen", "title": "Demystifying Inter-Class Disentanglement", "comments": "ICLR 2020. Project page: http://www.vision.huji.ac.il/lord", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to disentangle the hidden factors of variations within a set of\nobservations is a key task for artificial intelligence. We present a unified\nformulation for class and content disentanglement and use it to illustrate the\nlimitations of current methods. We therefore introduce LORD, a novel method\nbased on Latent Optimization for Representation Disentanglement. We find that\nlatent optimization, along with an asymmetric noise regularization, is superior\nto amortized inference for achieving disentangled representations. In extensive\nexperiments, our method is shown to achieve better disentanglement performance\nthan both adversarial and non-adversarial methods that use the same level of\nsupervision. We further introduce a clustering-based approach for extending our\nmethod for settings that exhibit in-class variation with promising results on\nthe task of domain translation.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 16:58:26 GMT"}, {"version": "v2", "created": "Sat, 28 Sep 2019 17:28:03 GMT"}, {"version": "v3", "created": "Tue, 18 Feb 2020 18:56:58 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Gabbay", "Aviv", ""], ["Hoshen", "Yedid", ""]]}, {"id": "1906.11818", "submitter": "Henry Kvinge", "authors": "Henry Kvinge, Elin Farnell, Julia R. Dupuis, Michael Kirby, Chris\n  Peterson, Elizabeth C. Schundler", "title": "More chemical detection through less sampling: amplifying chemical\n  signals in hyperspectral data cubes through compressive sensing", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive sensing (CS) is a method of sampling which permits some classes\nof signals to be reconstructed with high accuracy even when they were\nunder-sampled. In this paper we explore a phenomenon in which bandwise CS\nsampling of a hyperspectral data cube followed by reconstruction can actually\nresult in amplification of chemical signals contained in the cube. Perhaps most\nsurprisingly, chemical signal amplification generally seems to increase as the\nlevel of sampling decreases. In some examples, the chemical signal is\nsignificantly stronger in a data cube reconstructed from 10% CS sampling than\nit is in the raw, 100% sampled data cube. We explore this phenomenon in two\nreal-world datasets including the Physical Sciences Inc. Fabry-P\\'{e}rot\ninterferometer sensor multispectral dataset and the Johns Hopkins Applied\nPhysics Lab FTIR-based longwave infrared sensor hyperspectral dataset. Each of\nthese datasets contains the release of a chemical simulant, such as glacial\nacetic acid, triethyl phospate, and sulfur hexafluoride, and in all cases we\nuse the adaptive coherence estimator (ACE) to detect a target signal in the\nhyperspectral data cube. We end the paper by suggesting some theoretical\njustifications for why chemical signals would be amplified in CS sampled and\nreconstructed hyperspectral data cubes and discuss some practical implications.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 17:56:28 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Kvinge", "Henry", ""], ["Farnell", "Elin", ""], ["Dupuis", "Julia R.", ""], ["Kirby", "Michael", ""], ["Peterson", "Chris", ""], ["Schundler", "Elizabeth C.", ""]]}, {"id": "1906.11834", "submitter": "Ringo S.W. Chu Mr.", "authors": "Shuanglong Liu, Ringo S.W. Chu, Xiwei Wang, and Wayne Luk", "title": "Optimizing CNN-based Hyperspectral Image Classification on FPGAs", "comments": "This article is accepted for publication at ARC'2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral image (HSI) classification has been widely adopted in\napplications involving remote sensing imagery analysis which require high\nclassification accuracy and real-time processing speed. Methods based on\nConvolutional neural networks (CNNs) have been proven to achieve\nstate-of-the-art accuracy in classifying HSIs. However, CNN models are often\ntoo computationally intensive to achieve real-time response due to the high\ndimensional nature of HSI, compared to traditional methods such as Support\nVector Machines (SVMs). Besides, previous CNN models used in HSI are not\nspecially designed for efficient implementation on embedded devices such as\nFPGAs. This paper proposes a novel CNN-based algorithm for HSI classification\nwhich takes into account hardware efficiency. A customized architecture which\nenables the proposed algorithm to be mapped effectively onto FPGA resources is\nthen proposed to support real-time on-board classification with low power\nconsumption. Implementation results show that our proposed accelerator on a\nXilinx Zynq 706 FPGA board achieves more than 70x faster than an Intel 8-core\nXeon CPU and 3x faster than an NVIDIA GeForce 1080 GPU. Compared to previous\nSVM-based FPGA accelerators, we achieve comparable processing speed but provide\na much higher classification accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 22:05:22 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Liu", "Shuanglong", ""], ["Chu", "Ringo S. W.", ""], ["Wang", "Xiwei", ""], ["Luk", "Wayne", ""]]}, {"id": "1906.11872", "submitter": "Or Shwartzman", "authors": "Or Shwartzman, Harel Gazit, Ilan Shelef, Tammy Riklin-Raviv", "title": "The Worrisome Impact of an Inter-rater Bias on Neural Network Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of inter-rater variability is often discussed in the context of\nmanual labeling of medical images. The emergence of data-driven approaches such\nas Deep Neural Networks (DNNs) brought this issue of raters' disagreement to\nthe front-stage. In this paper, we highlight the issue of inter-rater bias as\nopposed to random inter-observer variability and demonstrate its influence on\nDNN training, leading to different segmentation results for the same input\nimages. In fact, lower overlap scores are obtained between the outputs of a DNN\ntrained on annotations of one rater and tested on another. Moreover, we\ndemonstrate that inter-rater bias in the training examples is amplified and\nbecomes more consistent, considering the segmentation predictions of the DNNs'\ntest data. We support our findings by showing that a classifier-DNN trained to\ndistinguish between raters based on their manual annotations performs better\nwhen the automatic segmentation predictions rather than the actual raters'\nannotations were tested. For this study, we used two different datasets: the\nISBI 2015 Multiple Sclerosis (MS) challenge dataset, including MRI scans each\nwith annotations provided by two raters with different levels of expertise; and\nIntracerebral Hemorrhage (ICH) CT scans with manual and semi-manual\nsegmentations. The results obtained allow us to underline a worrisome clinical\nimplication of a DNN bias induced by an inter-rater bias during training.\nSpecifically, we present a consistent underestimate of MS-lesion loads when\ncalculated from segmentation predictions of a DNN trained on input provided by\nthe less experienced rater. In the same manner, the differences in ICH volumes\ncalculated based on outputs of identical DNNs, each trained on annotations from\na different source are more consistent and larger than the differences in\nvolumes between the manual and semi-manual annotations used for training.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 12:29:23 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 15:36:02 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Shwartzman", "Or", ""], ["Gazit", "Harel", ""], ["Shelef", "Ilan", ""], ["Riklin-Raviv", "Tammy", ""]]}, {"id": "1906.11873", "submitter": "Hager Radi", "authors": "Hager Radi and Waleed Ali", "title": "VolMap: A Real-time Model for Semantic Segmentation of a LiDAR\n  surrounding view", "comments": "Accepted at Thirty-sixth International Conference on Machine Learning\n  (ICML 2019) Workshop on AI for Autonomous Driving", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces VolMap, a real-time approach for the semantic\nsegmentation of a 3D LiDAR surrounding view system in autonomous vehicles. We\ndesigned an optimized deep convolution neural network that can accurately\nsegment the point cloud produced by a 360\\degree{} LiDAR setup, where the input\nconsists of a volumetric bird-eye view with LiDAR height layers used as input\nchannels. We further investigated the usage of multi-LiDAR setup and its effect\non the performance of the semantic segmentation task. Our evaluations are\ncarried out on a large scale 3D object detection benchmark containing a LiDAR\ncocoon setup, along with KITTI dataset, where the per-point segmentation labels\nare derived from 3D bounding boxes. We show that VolMap achieved an excellent\nbalance between high accuracy and real-time running on CPU.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 16:22:41 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Radi", "Hager", ""], ["Ali", "Waleed", ""]]}, {"id": "1906.11874", "submitter": "Yz Gu", "authors": "Yinzheng Gu and Chuanpeng Li", "title": "Team JL Solution to Google Landmark Recognition 2019", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe our solution to the Google Landmark Recognition\n2019 Challenge held on Kaggle. Due to the large number of classes, noisy data,\nimbalanced class sizes, and the presence of a significant amount of distractors\nin the test set, our method is based mainly on retrieval techniques with both\nglobal and local CNN approaches. Our full pipeline, after ensembling the models\nand applying several steps of re-ranking strategies, scores 0.37606 GAP on the\nprivate leaderboard which won the 1st place in the competition.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 12:48:26 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Gu", "Yinzheng", ""], ["Li", "Chuanpeng", ""]]}, {"id": "1906.11875", "submitter": "Gwenole Quellec", "authors": "Gwenol\\'e Quellec, Mathieu Lamard, Bruno Lay, Alexandre Le Guilcher,\n  Ali Erginay, B\\'eatrice Cochener, Pascale Massin", "title": "Instant automatic diagnosis of diabetic retinopathy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this study is to evaluate the performance of the OphtAI system\nfor the automatic detection of referable diabetic retinopathy (DR) and the\nautomatic assessment of DR severity using color fundus photography. OphtAI\nrelies on ensembles of convolutional neural networks trained to recognize eye\nlaterality, detect referable DR and assess DR severity. The system can either\nprocess single images or full examination records. To document the automatic\ndiagnoses, accurate heatmaps are generated. The system was developed and\nvalidated using a dataset of 763,848 images from 164,660 screening procedures\nfrom the OPHDIAT screening program. For comparison purposes, it was also\nevaluated in the public Messidor-2 dataset. Referable DR can be detected with\nan area under the ROC curve of AUC = 0.989 in the Messidor-2 dataset, using the\nUniversity of Iowa's reference standard (95% CI: 0.984-0.994). This is\nsignificantly better than the only AI system authorized by the FDA, evaluated\nin the exact same conditions (AUC = 0.980). OphtAI can also detect\nvision-threatening DR with an AUC of 0.997 (95% CI: 0.996-0.998) and\nproliferative DR with an AUC of 0.997 (95% CI: 0.995-0.999). The system runs in\n0.3 seconds using a graphics processing unit and less than 2 seconds without.\nOphtAI is safer, faster and more comprehensive than the only AI system\nauthorized by the FDA so far. Instant DR diagnosis is now possible, which is\nexpected to streamline DR screening and to give easy access to DR screening to\nmore diabetic patients.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 08:34:57 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Quellec", "Gwenol\u00e9", ""], ["Lamard", "Mathieu", ""], ["Lay", "Bruno", ""], ["Guilcher", "Alexandre Le", ""], ["Erginay", "Ali", ""], ["Cochener", "B\u00e9atrice", ""], ["Massin", "Pascale", ""]]}, {"id": "1906.11876", "submitter": "William Beluch", "authors": "Jan M.K\\\"ohler, Maximilian Autenrieth, William H. Beluch", "title": "Uncertainty Based Detection and Relabeling of Noisy Image Labels", "comments": "Uncertainty and Robustness in Deep Visual Learning Workshop at CVPR\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are powerful tools in computer vision tasks.\nHowever, in many realistic scenarios label noise is prevalent in the training\nimages, and overfitting to these noisy labels can significantly harm the\ngeneralization performance of DNNs. We propose a novel technique to identify\ndata with noisy labels based on the different distributions of the predictive\nuncertainties from a DNN over the clean and noisy data. Additionally, the\nbehavior of the uncertainty over the course of training helps to identify the\nnetwork weights which best can be used to relabel the noisy labels. Data with\nnoisy labels can therefore be cleaned in an iterative process. Our proposed\nmethod can be easily implemented, and shows promising performance on the task\nof noisy label detection on CIFAR-10 and CIFAR-100.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 13:44:38 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["K\u00f6hler", "Jan M.", ""], ["Autenrieth", "Maximilian", ""], ["Beluch", "William H.", ""]]}, {"id": "1906.11877", "submitter": "Matthew Guzdial", "authors": "Zijin Luo, Matthew Guzdial, and Mark Riedl", "title": "Making CNNs for Video Parsing Accessible", "comments": "11 pages, 6 figures, Foundations of Digital Games 2018", "journal-ref": null, "doi": "10.1145/3337722.3337755", "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to extract sequences of game events for high-resolution e-sport\ngames has traditionally required access to the game's engine. This serves as a\nbarrier to groups who don't possess this access. It is possible to apply deep\nlearning to derive these logs from gameplay video, but it requires\ncomputational power that serves as an additional barrier. These groups would\nbenefit from access to these logs, such as small e-sport tournament organizers\nwho could better visualize gameplay to inform both audience and commentators.\nIn this paper we present a combined solution to reduce the required\ncomputational resources and time to apply a convolutional neural network (CNN)\nto extract events from e-sport gameplay videos. This solution consists of\ntechniques to train a CNN faster and methods to execute predictions more\nquickly. This expands the types of machines capable of training and running\nthese models, which in turn extends access to extracting game logs with this\napproach. We evaluate the approaches in the domain of DOTA2, one of the most\npopular e-sports. Our results demonstrate our approach outperforms standard\nbackpropagation baselines.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 16:00:40 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Luo", "Zijin", ""], ["Guzdial", "Matthew", ""], ["Riedl", "Mark", ""]]}, {"id": "1906.11878", "submitter": "Hossein Ghayoumi Zadeh", "authors": "Mehdi Abbaszadeh, Aliakbar Rahimifard, Mohammadali Eftekhari, Hossein\n  Ghayoumi Zadeh, Ali Fayazi, Ali Dini, Mostafa Danaeian", "title": "Deep Learning-Based Classification Of the Defective Pistachios Via Deep\n  Autoencoder Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pistachio nut is mainly consumed as raw, salted or roasted because of its\nhigh nutritional properties and favorable taste. Pistachio nuts with shell and\nkernel defects, besides not being acceptable for a consumer, are also prone to\ninsects damage, mold decay, and aflatoxin contamination. In this research, a\ndeep learning-based imaging algorithm was developed to improve the sorting of\nnuts with shell and kernel defects that indicate the risk of aflatoxin\ncontamination, such as dark stains, oily stains, adhering hull, fungal decay\nand Aspergillus molds. This paper presents an unsupervised learning method to\nclassify defective and unpleasant pistachios based on deep Auto-encoder neural\nnetworks. The testing of the designed neural network on a validation dataset\nshowed that nuts having dark stain, oily stain or adhering hull with an\naccuracy of 80.3% can be distinguished from normal nuts. Due to the limited\nmemory available in the HPC of university, the results are reasonable and\njustifiable.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 13:02:50 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Abbaszadeh", "Mehdi", ""], ["Rahimifard", "Aliakbar", ""], ["Eftekhari", "Mohammadali", ""], ["Zadeh", "Hossein Ghayoumi", ""], ["Fayazi", "Ali", ""], ["Dini", "Ali", ""], ["Danaeian", "Mostafa", ""]]}, {"id": "1906.11879", "submitter": "Murad Qasaimeh", "authors": "Murad Qasaimeh, Kristof Denolf, Jack Lo, Kees Vissers, Joseph Zambreno\n  and Phillip H. Jones", "title": "Comparing Energy Efficiency of CPU, GPU and FPGA Implementations for\n  Vision Kernels", "comments": "8 pages, Design Automation Conference (DAC), The 15th IEEE\n  International Conference on Embedded Software and Systems, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing high performance embedded vision applications requires balancing\nrun-time performance with energy constraints. Given the mix of hardware\naccelerators that exist for embedded computer vision (e.g. multi-core CPUs,\nGPUs, and FPGAs), and their associated vendor optimized vision libraries, it\nbecomes a challenge for developers to navigate this fragmented solution space.\nTo aid with determining which embedded platform is most suitable for their\napplication, we conduct a comprehensive benchmark of the run-time performance\nand energy efficiency of a wide range of vision kernels. We discuss rationales\nfor why a given underlying hardware architecture innately performs well or\npoorly based on the characteristics of a range of vision kernel categories.\nSpecifically, our study is performed for three commonly used HW accelerators\nfor embedded vision applications: ARM57 CPU, Jetson TX2 GPU and ZCU102 FPGA,\nusing their vendor optimized vision libraries: OpenCV, VisionWorks and\nxfOpenCV. Our results show that the GPU achieves an energy/frame reduction\nratio of 1.1-3.2x compared to the others for simple kernels. While for more\ncomplicated kernels and complete vision pipelines, the FPGA outperforms the\nothers with energy/frame reduction ratios of 1.2-22.3x. It is also observed\nthat the FPGA performs increasingly better as a vision application's pipeline\ncomplexity grows.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 21:25:42 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Qasaimeh", "Murad", ""], ["Denolf", "Kristof", ""], ["Lo", "Jack", ""], ["Vissers", "Kees", ""], ["Zambreno", "Joseph", ""], ["Jones", "Phillip H.", ""]]}, {"id": "1906.11880", "submitter": "Aviv Gabbay", "authors": "Aviv Gabbay and Yedid Hoshen", "title": "Style Generator Inversion for Image Enhancement and Animation", "comments": "Project page: http://www.vision.huji.ac.il/style-image-prior", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main motivations for training high quality image generative models\nis their potential use as tools for image manipulation. Recently, generative\nadversarial networks (GANs) have been able to generate images of remarkable\nquality. Unfortunately, adversarially-trained unconditional generator networks\nhave not been successful as image priors. One of the main requirements for a\nnetwork to act as a generative image prior, is being able to generate every\npossible image from the target distribution. Adversarial learning often\nexperiences mode-collapse, which manifests in generators that cannot generate\nsome modes of the target distribution. Another requirement often not satisfied\nis invertibility i.e. having an efficient way of finding a valid input latent\ncode given a required output image. In this work, we show that differently from\nearlier GANs, the very recently proposed style-generators are quite easy to\ninvert. We use this important observation to propose style generators as\ngeneral purpose image priors. We show that style generators outperform other\nGANs as well as Deep Image Prior as priors for image enhancement tasks. The\nlatent space spanned by style-generators satisfies linear identity-pose\nrelations. The latent space linearity, combined with invertibility, allows us\nto animate still facial images without supervision. Extensive experiments are\nperformed to support the main contributions of this paper.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 17:58:28 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Gabbay", "Aviv", ""], ["Hoshen", "Yedid", ""]]}, {"id": "1906.11881", "submitter": "Nicki Skafte Detlefsen", "authors": "Nicki Skafte Detlefsen and S{\\o}ren Hauberg", "title": "Explicit Disentanglement of Appearance and Perspective in Generative\n  Models", "comments": "9 main pages + 2 pages references + 8 pages of supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disentangled representation learning finds compact, independent and\neasy-to-interpret factors of the data. Learning such has been shown to require\nan inductive bias, which we explicitly encode in a generative model of images.\nSpecifically, we propose a model with two latent spaces: one that represents\nspatial transformations of the input data, and another that represents the\ntransformed data. We find that the latter naturally captures the intrinsic\nappearance of the data. To realize the generative model, we propose a\nVariationally Inferred Transformational Autoencoder (VITAE) that incorporates a\nspatial transformer into a variational autoencoder. We show how to perform\ninference in the model efficiently by carefully designing the encoders and\nrestricting the transformation class to be diffeomorphic. Empirically, our\nmodel separates the visual style from digit type on MNIST, separates shape and\npose in images of human bodies and facial features from facial shape on CelebA.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 10:24:03 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 07:18:04 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Detlefsen", "Nicki Skafte", ""], ["Hauberg", "S\u00f8ren", ""]]}, {"id": "1906.11882", "submitter": "Zhenyu Chen", "authors": "Tianxing He, Shengcheng Yu, Ziyuan Wang, Jieqiong Li, Zhenyu Chen", "title": "From Data Quality to Model Quality: an Exploratory Study on Deep\n  Learning", "comments": "4pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, people strive to improve the accuracy of deep learning models.\nHowever, very little work has focused on the quality of data sets. In fact,\ndata quality determines model quality. Therefore, it is important for us to\nmake research on how data quality affects on model quality. In this paper, we\nmainly consider four aspects of data quality, including Dataset Equilibrium,\nDataset Size, Quality of Label, Dataset Contamination. We deign experiment on\nMNIST and Cifar-10 and try to find out the influence the four aspects make on\nmodel quality. Experimental results show that four aspects all have decisive\nimpact on the quality of models. It means that decrease in data quality in\nthese aspects will reduce the accuracy of model.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 09:19:32 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["He", "Tianxing", ""], ["Yu", "Shengcheng", ""], ["Wang", "Ziyuan", ""], ["Li", "Jieqiong", ""], ["Chen", "Zhenyu", ""]]}, {"id": "1906.11883", "submitter": "Ankush Gupta", "authors": "Tejas Kulkarni, Ankush Gupta, Catalin Ionescu, Sebastian Borgeaud,\n  Malcolm Reynolds, Andrew Zisserman, Volodymyr Mnih", "title": "Unsupervised Learning of Object Keypoints for Perception and Control", "comments": "In NeurIPS 2019. Code\n  https://github.com/deepmind/deepmind-research/tree/master/transporter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of object representations in computer vision has primarily focused\non developing representations that are useful for image classification, object\ndetection, or semantic segmentation as downstream tasks. In this work we aim to\nlearn object representations that are useful for control and reinforcement\nlearning (RL). To this end, we introduce Transporter, a neural network\narchitecture for discovering concise geometric object representations in terms\nof keypoints or image-space coordinates. Our method learns from raw video\nframes in a fully unsupervised manner, by transporting learnt image features\nbetween video frames using a keypoint bottleneck. The discovered keypoints\ntrack objects and object parts across long time-horizons more accurately than\nrecent similar methods. Furthermore, consistent long-term tracking enables two\nnotable results in control domains -- (1) using the keypoint co-ordinates and\ncorresponding image features as inputs enables highly sample-efficient\nreinforcement learning; (2) learning to explore by controlling keypoint\nlocations drastically reduces the search space, enabling deep exploration\n(leading to states unreachable through random action exploration) without any\nextrinsic rewards.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 23:58:54 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 00:27:24 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Kulkarni", "Tejas", ""], ["Gupta", "Ankush", ""], ["Ionescu", "Catalin", ""], ["Borgeaud", "Sebastian", ""], ["Reynolds", "Malcolm", ""], ["Zisserman", "Andrew", ""], ["Mnih", "Volodymyr", ""]]}, {"id": "1906.11884", "submitter": "Tanmay Randhavane", "authors": "Tanmay Randhavane, Uttaran Bhattacharya, Kyra Kapsaskis, Kurt Gray,\n  Aniket Bera, Dinesh Manocha", "title": "Identifying Emotions from Walking using Affective and Deep Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new data-driven model and algorithm to identify the perceived\nemotions of individuals based on their walking styles. Given an RGB video of an\nindividual walking, we extract his/her walking gait in the form of a series of\n3D poses. Our goal is to exploit the gait features to classify the emotional\nstate of the human into one of four emotions: happy, sad, angry, or neutral.\nOur perceived emotion recognition approach uses deep features learned via LSTM\non labeled emotion datasets. Furthermore, we combine these features with\naffective features computed from gaits using posture and movement cues. These\nfeatures are classified using a Random Forest Classifier. We show that our\nmapping between the combined feature space and the perceived emotional state\nprovides 80.07% accuracy in identifying the perceived emotions. In addition to\nclassifying discrete categories of emotions, our algorithm also predicts the\nvalues of perceived valence and arousal from gaits. We also present an EWalk\n(Emotion Walk) dataset that consists of videos of walking individuals with\ngaits and labeled emotions. To the best of our knowledge, this is the first\ngait-based model to identify perceived emotions from videos of walking\nindividuals.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 11:41:37 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 20:07:13 GMT"}, {"version": "v3", "created": "Sun, 21 Jul 2019 22:12:11 GMT"}, {"version": "v4", "created": "Thu, 9 Jan 2020 21:06:50 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Randhavane", "Tanmay", ""], ["Bhattacharya", "Uttaran", ""], ["Kapsaskis", "Kyra", ""], ["Gray", "Kurt", ""], ["Bera", "Aniket", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1906.11885", "submitter": "Chunmei Feng", "authors": "Chun-Mei Feng, Yong Xu, Zuoyong Li, Jian Yang", "title": "Robust Classification with Sparse Representation Fusion on Diverse Data\n  Subsets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse Representation (SR) techniques encode the test samples into a sparse\nlinear combination of all training samples and then classify the test samples\ninto the class with the minimum residual. The classification of SR techniques\ndepends on the representation capability on the test samples. However, most of\nthese models view the representation problem of the test samples as a\ndeterministic problem, ignoring the uncertainty of the representation. The\nuncertainty is caused by two factors, random noise in the samples and the\nintrinsic randomness of the sample set, which means that if we capture a group\nof samples, the obtained set of samples will be different in different\nconditions. In this paper, we propose a novel method based upon Collaborative\nRepresentation that is a special instance of SR and has closed-form solution.\nIt performs Sparse Representation Fusion based on the Diverse Subset of\ntraining samples (SRFDS), which reduces the impact of randomness of the sample\nset and enhances the robustness of classification results. The proposed method\nis suitable for multiple types of data and has no requirement on the pattern\ntype of the tasks. In addition, SRFDS not only preserves a closed-form solution\nbut also greatly improves the classification performance. Promising results on\nvarious datasets serve as the evidence of better performance of SRFDS than\nother SR-based methods. The Matlab code of SRFDS will be accessible at\nhttp://www.yongxu.org/lunwen.html.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 13:22:54 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Feng", "Chun-Mei", ""], ["Xu", "Yong", ""], ["Li", "Zuoyong", ""], ["Yang", "Jian", ""]]}, {"id": "1906.11886", "submitter": "Lucas Possatti", "authors": "Lucas C. Possatti, R\\^anik Guidolini, Vinicius B. Cardoso, Rodrigo F.\n  Berriel, Thiago M. Paix\\~ao, Claudine Badue, Alberto F. De Souza and Thiago\n  Oliveira-Santos", "title": "Traffic Light Recognition Using Deep Learning and Prior Maps for\n  Autonomous Cars", "comments": "Accepted in 2019 International Joint Conference on Neural Networks\n  (IJCNN)", "journal-ref": null, "doi": "10.1109/IJCNN.2019.8851927", "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous terrestrial vehicles must be capable of perceiving traffic lights\nand recognizing their current states to share the streets with human drivers.\nMost of the time, human drivers can easily identify the relevant traffic\nlights. To deal with this issue, a common solution for autonomous cars is to\nintegrate recognition with prior maps. However, additional solution is required\nfor the detection and recognition of the traffic light. Deep learning\ntechniques have showed great performance and power of generalization including\ntraffic related problems. Motivated by the advances in deep learning, some\nrecent works leveraged some state-of-the-art deep detectors to locate (and\nfurther recognize) traffic lights from 2D camera images. However, none of them\ncombine the power of the deep learning-based detectors with prior maps to\nrecognize the state of the relevant traffic lights. Based on that, this work\nproposes to integrate the power of deep learning-based detection with the prior\nmaps used by our car platform IARA (acronym for Intelligent Autonomous Robotic\nAutomobile) to recognize the relevant traffic lights of predefined routes. The\nprocess is divided in two phases: an offline phase for map construction and\ntraffic lights annotation; and an online phase for traffic light recognition\nand identification of the relevant ones. The proposed system was evaluated on\nfive test cases (routes) in the city of Vit\\'oria, each case being composed of\na video sequence and a prior map with the relevant traffic lights for the\nroute. Results showed that the proposed technique is able to correctly identify\nthe relevant traffic light along the trajectory.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 18:05:25 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Possatti", "Lucas C.", ""], ["Guidolini", "R\u00e2nik", ""], ["Cardoso", "Vinicius B.", ""], ["Berriel", "Rodrigo F.", ""], ["Paix\u00e3o", "Thiago M.", ""], ["Badue", "Claudine", ""], ["De Souza", "Alberto F.", ""], ["Oliveira-Santos", "Thiago", ""]]}, {"id": "1906.11887", "submitter": "Zhenyu Chen", "authors": "Benlin Hu, Cheng Lei, Dong Wang, Shu Zhang, Zhenyu Chen", "title": "A Preliminary Study on Data Augmentation of Deep Learning for Image\n  Classification", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models have a large number of freeparameters that need to be\ncalculated by effective trainingof the models on a great deal of training data\nto improvetheir generalization performance. However, data obtaining andlabeling\nis expensive in practice. Data augmentation is one of themethods to alleviate\nthis problem. In this paper, we conduct apreliminary study on how three\nvariables (augmentation method,augmentation rate and size of basic dataset per\nlabel) can affectthe accuracy of deep learning for image classification. The\nstudyprovides some guidelines: (1) it is better to use transformationsthat\nalter the geometry of the images rather than those justlighting and color. (2)\n2-3 times augmentation rate is good enoughfor training. (3) the smaller amount\nof data, the more obviouscontributions could have.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 07:21:09 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Hu", "Benlin", ""], ["Lei", "Cheng", ""], ["Wang", "Dong", ""], ["Zhang", "Shu", ""], ["Chen", "Zhenyu", ""]]}, {"id": "1906.11888", "submitter": "Seungjoo Yoo", "authors": "Seungjoo Yoo, Hyojin Bahng, Sunghyo Chung, Junsoo Lee, Jaehyuk Chang,\n  Jaegul Choo", "title": "Coloring With Limited Data: Few-Shot Colorization via Memory-Augmented\n  Networks", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent advancements in deep learning-based automatic colorization,\nthey are still limited when it comes to few-shot learning. Existing models\nrequire a significant amount of training data. To tackle this issue, we present\na novel memory-augmented colorization model MemoPainter that can produce\nhigh-quality colorization with limited data. In particular, our model is able\nto capture rare instances and successfully colorize them. We also propose a\nnovel threshold triplet loss that enables unsupervised training of memory\nnetworks without the need of class labels. Experiments show that our model has\nsuperior quality in both few-shot and one-shot colorization tasks.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 07:26:54 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Yoo", "Seungjoo", ""], ["Bahng", "Hyojin", ""], ["Chung", "Sunghyo", ""], ["Lee", "Junsoo", ""], ["Chang", "Jaehyuk", ""], ["Choo", "Jaegul", ""]]}, {"id": "1906.11889", "submitter": "Lena A. J\\\"ager", "authors": "Lena A. J\\\"ager, Silvia Makowski, Paul Prasse, Sascha Liehr,\n  Maximilian Seidler and Tobias Scheffer", "title": "Deep Eyedentification: Biometric Identification using Micro-Movements of\n  the Eye", "comments": null, "journal-ref": "In: U. Brefeld et al. (Eds.): Machine Learning and Knowledge\n  Discovery in Databases, ECML PKDD 2019, LNCS 11907, pp. 299-314, Springer\n  Nature, Switzerland, 2020", "doi": "10.1007/978-3-030-46147-8_18", "report-no": null, "categories": "cs.CV cs.CL cs.HC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study involuntary micro-movements of the eye for biometric identification.\nWhile prior studies extract lower-frequency macro-movements from the output of\nvideo-based eye-tracking systems and engineer explicit features of these\nmacro-movements, we develop a deep convolutional architecture that processes\nthe raw eye-tracking signal. Compared to prior work, the network attains a\nlower error rate by one order of magnitude and is faster by two orders of\nmagnitude: it identifies users accurately within seconds.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 10:36:40 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 08:27:02 GMT"}, {"version": "v3", "created": "Thu, 4 Jul 2019 05:14:56 GMT"}, {"version": "v4", "created": "Wed, 15 Apr 2020 10:06:31 GMT"}, {"version": "v5", "created": "Tue, 5 May 2020 08:30:44 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["J\u00e4ger", "Lena A.", ""], ["Makowski", "Silvia", ""], ["Prasse", "Paul", ""], ["Liehr", "Sascha", ""], ["Seidler", "Maximilian", ""], ["Scheffer", "Tobias", ""]]}, {"id": "1906.11890", "submitter": "Matias Tassano", "authors": "Matias Tassano and Julie Delon and Thomas Veit", "title": "DVDnet: A Fast Network for Deep Video Denoising", "comments": null, "journal-ref": null, "doi": "10.1109/ICIP.2019.8803136", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a state-of-the-art video denoising algorithm based\non a convolutional neural network architecture. Previous neural network based\napproaches to video denoising have been unsuccessful as their performance\ncannot compete with the performance of patch-based methods. However, our\napproach outperforms other patch-based competitors with significantly lower\ncomputing times. In contrast to other existing neural network denoisers, our\nalgorithm exhibits several desirable properties such as a small memory\nfootprint, and the ability to handle a wide range of noise levels with a single\nnetwork model. The combination between its denoising performance and lower\ncomputational load makes this algorithm attractive for practical denoising\napplications. We compare our method with different state-of-art algorithms,\nboth visually and with respect to objective quality metrics. The experiments\nshow that our algorithm compares favorably to other state-of-art methods. Video\nexamples, code and models are publicly available at\n\\url{https://github.com/m-tassano/dvdnet}.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 21:06:33 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Tassano", "Matias", ""], ["Delon", "Julie", ""], ["Veit", "Thomas", ""]]}, {"id": "1906.11891", "submitter": "Daniel McDuff", "authors": "Daniel McDuff, Shuang Ma, Yale Song, Ashish Kapoor", "title": "Characterizing Bias in Classifiers using Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models that are learned from real-world data are often biased because the\ndata used to train them is biased. This can propagate systemic human biases\nthat exist and ultimately lead to inequitable treatment of people, especially\nminorities. To characterize bias in learned classifiers, existing approaches\nrely on human oracles labeling real-world examples to identify the \"blind\nspots\" of the classifiers; these are ultimately limited due to the human labor\nrequired and the finite nature of existing image examples. We propose a\nsimulation-based approach for interrogating classifiers using generative\nadversarial models in a systematic manner. We incorporate a progressive\nconditional generative model for synthesizing photo-realistic facial images and\nBayesian Optimization for an efficient interrogation of independent facial\nimage classification systems. We show how this approach can be used to\nefficiently characterize racial and gender biases in commercial systems.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 05:48:40 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["McDuff", "Daniel", ""], ["Ma", "Shuang", ""], ["Song", "Yale", ""], ["Kapoor", "Ashish", ""]]}, {"id": "1906.11892", "submitter": "Boris Oreshkin N", "authors": "Boris N. Oreshkin and Negar Rostamzadeh and Pedro O. Pinheiro and\n  Christopher Pal", "title": "CLAREL: Classification via retrieval loss for zero-shot learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of learning fine-grained cross-modal representations.\nWe propose an instance-based deep metric learning approach in joint visual and\ntextual space. The key novelty of this paper is that it shows that using\nper-image semantic supervision leads to substantial improvement in zero-shot\nperformance over using class-only supervision. On top of that, we provide a\nprobabilistic justification for a metric rescaling approach that solves a very\ncommon problem in the generalized zero-shot learning setting, i.e., classifying\ntest images from unseen classes as one of the classes seen during training. We\nevaluate our approach on two fine-grained zero-shot learning datasets: CUB and\nFLOWERS. We find that on the generalized zero-shot classification task CLAREL\nconsistently outperforms the existing approaches on both datasets.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 22:24:53 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 15:37:48 GMT"}, {"version": "v3", "created": "Sun, 5 Apr 2020 14:59:22 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Oreshkin", "Boris N.", ""], ["Rostamzadeh", "Negar", ""], ["Pinheiro", "Pedro O.", ""], ["Pal", "Christopher", ""]]}, {"id": "1906.11893", "submitter": "Ahmed Elfakharany", "authors": "A. Elfakharany, R. Yusof, N. Ismail, R. Arfa, M. Yunus", "title": "HalalNet: A Deep Neural Network that Classifies the Halalness\n  Slaughtered Chicken from their Images", "comments": "Submitted in the International Conference on Artificial Intelligence\n  and Robotics for Industrial Applications, AIR2018", "journal-ref": "International Journal of Integrated Engineering, Vol. 11, no. 4,\n  Sept. 2019,\n  https://publisher.uthm.edu.my/ojs/index.php/ijie/article/view/4194", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Halal requirement in food is important for millions of Muslims worldwide\nespecially for meat and chicken products, insuring that slaughter houses adhere\nto this requirement is a challenging task to do manually. In this paper a\nmethod is proposed that uses a camera that takes images of slaughtered chicken\non the conveyor in a slaughter house, the images are then analyzed by a deep\nneural network to classify if the image is of a halal slaughtered chicken or\nnot. However, traditional deep learning models require large amounts of data to\ntrain on, which in this case these amounts of data were challenging to collect\nespecially the images of non-halal slaughtered chicken, hence this paper shows\nhow the use of one shot learning [1] and transfer learning [2] can reach high\naccuracy on the few amounts of data that were available. The architecture used\nis based on the Siamese neural networks architecture which ranks the similarity\nbetween two inputs [3] while using the Xception network [4] as the twin\nnetworks. We call it HalalNet. This work was done as part of SYCUT (syriah\ncompliant slaughtering system) which is a monitoring system that monitors the\nhalalness of the slaughtered chicken in a slaughter house. The data used to\ntrain and validate HalalNet was collected from the Azain slaughtering site\n(Semenyih, Selangor, Malaysia) containing images of both halal and non-halal\nslaughtered chicken.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 06:55:14 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Elfakharany", "A.", ""], ["Yusof", "R.", ""], ["Ismail", "N.", ""], ["Arfa", "R.", ""], ["Yunus", "M.", ""]]}, {"id": "1906.11894", "submitter": "Michele Alberti", "authors": "Michele Alberti, Lars V\\\"ogtlin, Vinaychandran Pondenkandath, Mathias\n  Seuret, Rolf Ingold, and Marcus Liwicki", "title": "Labeling, Cutting, Grouping: an Efficient Text Line Segmentation Method\n  for Medieval Manuscripts", "comments": null, "journal-ref": "2019 15th IAPR International Conference on Document Analysis and\n  Recognition (ICDAR), Sydney, Australia", "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new way for text-line extraction by integrating\ndeep-learning based pre-classification and state-of-the-art segmentation\nmethods. Text-line extraction in complex handwritten documents poses a\nsignificant challenge, even to the most modern computer vision algorithms.\nHistorical manuscripts are a particularly hard class of documents as they\npresent several forms of noise, such as degradation, bleed-through, interlinear\nglosses, and elaborated scripts. In this work, we propose a novel method which\nuses semantic segmentation at pixel level as intermediate task, followed by a\ntext-line extraction step. We measured the performance of our method on a\nrecent dataset of challenging medieval manuscripts and surpassed\nstate-of-the-art results by reducing the error by 80.7%. Furthermore, we\ndemonstrate the effectiveness of our approach on various other datasets written\nin different scripts. Hence, our contribution is two-fold. First, we\ndemonstrate that semantic pixel segmentation can be used as strong denoising\npre-processing step before performing text line extraction. Second, we\nintroduce a novel, simple and robust algorithm that leverages the high-quality\nsemantic segmentation to achieve a text-line extraction performance of 99.42%\nline IU on a challenging dataset.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 11:06:43 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 11:28:34 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Alberti", "Michele", ""], ["V\u00f6gtlin", "Lars", ""], ["Pondenkandath", "Vinaychandran", ""], ["Seuret", "Mathias", ""], ["Ingold", "Rolf", ""], ["Liwicki", "Marcus", ""]]}, {"id": "1906.11895", "submitter": "Simon Tamayo Giraldo", "authors": "Salma Benslimane, Simon Tamayo (CAOR), Arnaud de La Fortelle (CAOR)", "title": "Classifying logistic vehicles in cities using Deep learning", "comments": null, "journal-ref": "World Conference on Transport Research, May 2019, Mumbai, India", "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid growth in delivery and freight transportation is increasing in urban\nareas; as a result the use of delivery trucks and light commercial vehicles is\nevolving. Major cities can use traffic counting as a tool to monitor the\npresence of delivery vehicles in order to implement intelligent city planning\nmeasures. Classical methods for counting vehicles use mechanical,\nelectromagnetic or pneumatic sensors, but these devices are costly, difficult\nto implement and only detect the presence of vehicles without giving\ninformation about their category, model or trajectory. This paper proposes a\nDeep Learning tool for classifying vehicles in a given image while considering\ndifferent categories of logistic vehicles, namely: light-duty, medium-duty and\nheavy-duty vehicles. The proposed approach yields two main contributions: first\nwe developed an architecture to create an annotated and balanced database of\nlogistic vehicles, reducing manual annotation efforts. Second, we built a\nclassifier that accurately classifies the logistic vehicles passing through a\ngiven road. The results of this work are: first, a database of 72 000 images\nfor 4 vehicles classes; and second two retrained convolutional neural networks\n(InceptionV3 and MobileNetV2) capable of classifying vehicles with accuracies\nover 90%.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 09:05:20 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Benslimane", "Salma", "", "CAOR"], ["Tamayo", "Simon", "", "CAOR"], ["de La Fortelle", "Arnaud", "", "CAOR"]]}, {"id": "1906.11897", "submitter": "Mark Lee", "authors": "Mark Lee, Zico Kolter", "title": "On Physical Adversarial Patches for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate a physical adversarial patch attack against\nobject detectors, notably the YOLOv3 detector. Unlike previous work on physical\nobject detection attacks, which required the patch to overlap with the objects\nbeing misclassified or avoiding detection, we show that a properly designed\npatch can suppress virtually all the detected objects in the image. That is, we\ncan place the patch anywhere in the image, causing all existing objects in the\nimage to be missed entirely by the detector, even those far away from the patch\nitself. This in turn opens up new lines of physical attacks against object\ndetection systems, which require no modification of the objects in a scene. A\ndemo of the system can be found at https://youtu.be/WXnQjbZ1e7Y.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 11:04:57 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Lee", "Mark", ""], ["Kolter", "Zico", ""]]}, {"id": "1906.11898", "submitter": "L\\'eonard Boussioux", "authors": "L\\'eonard Boussioux, Tom\\'as Giro-Larraz, Charles Guille-Escuret,\n  Mehdi Cherti, Bal\\'azs K\\'egl", "title": "InsectUp: Crowdsourcing Insect Observations to Assess Demographic Shifts\n  and Improve Classification", "comments": "Appearing at the International Conference on Machine Learning, AI for\n  Social Good Workshop, Long Beach, United States, 2019 Appearing at the\n  International Conference on Computer Vision, AI for Wildlife Conservation\n  Workshop, Seoul, South Korea, 2019 5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Insects play such a crucial role in ecosystems that a shift in demography of\njust a few species can have devastating consequences at environmental, social\nand economic levels. Despite this, evaluation of insect demography is strongly\nlimited by the difficulty of collecting census data at sufficient scale. We\npropose a method to gather and leverage observations from bystanders, hikers,\nand entomology enthusiasts in order to provide researchers with data that could\nsignificantly help anticipate and identify environmental threats. Finally, we\nshow that there is indeed interest on both sides for such collaboration.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 00:57:15 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 18:39:03 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Boussioux", "L\u00e9onard", ""], ["Giro-Larraz", "Tom\u00e1s", ""], ["Guille-Escuret", "Charles", ""], ["Cherti", "Mehdi", ""], ["K\u00e9gl", "Bal\u00e1zs", ""]]}, {"id": "1906.11899", "submitter": "Farzad Shafiei Dizaji", "authors": "Farzad Shafiei Dizaji", "title": "Lidar based Detection and Classification of Pedestrians and Vehicles\n  Using Machine Learning Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of this paper is to classify objects mapped by LiDAR sensor into\ndifferent classes such as vehicles, pedestrians and bikers. Utilizing a\nLiDAR-based object detector and Neural Networks-based classifier, a novel\nreal-time object detection is presented essentially with respect to aid\nself-driving vehicles in recognizing and classifying other objects encountered\nin the course of driving and proceed accordingly. We discuss our work using\nmachine learning methods to tackle a common high-level problem found in machine\nlearning applications for self-driving cars: the classification of pointcloud\ndata obtained from a 3D LiDAR sensor.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 01:39:45 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Dizaji", "Farzad Shafiei", ""]]}, {"id": "1906.11900", "submitter": "Shan Jia", "authors": "Shan Jia, Chuanbo Hu, Guodong Guo, and Zhengquan Xu", "title": "A database for face presentation attack using wax figure faces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to 2D face presentation attacks (e.g. printed photos and video\nreplays), 3D type attacks are more challenging to face recognition systems\n(FRS) by presenting 3D characteristics or materials similar to real faces.\nExisting 3D face spoofing databases, however, mostly based on 3D masks, are\nrestricted to small data size or poor authenticity due to the production\ndifficulty and high cost. In this work, we introduce the first wax figure face\ndatabase, WFFD, as one type of super-realistic 3D presentation attacks to spoof\nthe FRS. This database consists of 2200 images with both real and wax figure\nfaces (totally 4400 faces) with a high diversity from online collections.\nExperiments on this database first investigate the vulnerability of three\npopular FRS to this kind of new attack. Further, we evaluate the performance of\nseveral face presentation attack detection methods to show the attack abilities\nof this super-realistic face spoofing database.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 00:50:27 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Jia", "Shan", ""], ["Hu", "Chuanbo", ""], ["Guo", "Guodong", ""], ["Xu", "Zhengquan", ""]]}, {"id": "1906.11901", "submitter": "Jean-Luc Meunier", "authors": "St\\'ephane Clinchant, Herv\\'e D\\'ejean, Jean-Luc Meunier, Eva Lang,\n  Florian Kleber", "title": "Comparing Machine Learning Approaches for Table Recognition in\n  Historical Register Books", "comments": "DAS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper experiments on Table Recognition in hand-written\nregistry books. We first explain how the problem of row and column detection is\nmodeled, and then compare two Machine Learning approaches (Conditional Random\nField and Graph Convolutional Network) for detecting these table elements.\nEvaluation was conducted on death records provided by the Archive of the\nDiocese of Passau. Both methods show similar results, a 89 F1 score, a quality\nwhich allows for Information Extraction. Software and dataset are open\nsource/data.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 06:42:47 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Clinchant", "St\u00e9phane", ""], ["D\u00e9jean", "Herv\u00e9", ""], ["Meunier", "Jean-Luc", ""], ["Lang", "Eva", ""], ["Kleber", "Florian", ""]]}, {"id": "1906.11902", "submitter": "Roshan Prakash Rane", "authors": "Roshan Rane, Edit Sz\\\"ugyi, Vageesh Saxena, Andr\\'e Ofner, Sebastian\n  Stober", "title": "PredNet and Predictive Coding: A Critical Review", "comments": null, "journal-ref": null, "doi": "10.1145/3372278.3390694", "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  PredNet, a deep predictive coding network developed by Lotter et al.,\ncombines a biologically inspired architecture based on the propagation of\nprediction error with self-supervised representation learning in video. While\nthe architecture has drawn a lot of attention and various extensions of the\nmodel exist, there is a lack of a critical analysis. We fill in the gap by\nevaluating PredNet both as an implementation of the predictive coding theory\nand as a self-supervised video prediction model using a challenging video\naction classification dataset. We design an extended model to test if\nconditioning future frame predictions on the action class of the video improves\nthe model performance. We show that PredNet does not yet completely follow the\nprinciples of predictive coding. The proposed top-down conditioning leads to a\nperformance gain on synthetic data, but does not scale up to the more complex\nreal-world action classification dataset. Our analysis is aimed at guiding\nfuture research on similar architectures based on the predictive coding theory.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 21:58:00 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 12:29:05 GMT"}, {"version": "v3", "created": "Mon, 18 May 2020 09:52:22 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Rane", "Roshan", ""], ["Sz\u00fcgyi", "Edit", ""], ["Saxena", "Vageesh", ""], ["Ofner", "Andr\u00e9", ""], ["Stober", "Sebastian", ""]]}, {"id": "1906.11904", "submitter": "Natalya Pya Arnqvist", "authors": "Natalya Pya Arnqvist, Blaise Ngendangenzwa, Eric Lindahl, Leif\n  Nilsson, Jun Yu", "title": "Effective degrees of freedom for surface finish defect detection and\n  classification", "comments": "17 pages, 12 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the primary concerns of product quality control in the automotive\nindustry is an automated detection of defects of small sizes on specular car\nbody surfaces. A new statistical learning approach is presented for surface\nfinish defect detection based on spline smoothing method for feature extraction\nand $k$-nearest neighbour probabilistic classifier. Since the surfaces are\nspecular, structured lightning reflection technique is applied for image\nacquisition. Reduced rank cubic regression splines are used to smooth the pixel\nvalues while the effective degrees of freedom of the obtained smooths serve as\ncomponents of the feature vector. A key advantage of the approach is that it\nallows reaching near zero misclassification error rate when applying standard\nlearning classifiers. We also propose probability based performance evaluation\nmetrics as alternatives to the conventional metrics. The usage of those\nprovides the means for uncertainty estimation of the predictive performance of\na classifier. Experimental classification results on the images obtained from\nthe pilot system located at Volvo GTO Cab plant in Ume{\\aa}, Sweden, show that\nthe proposed approach is much more efficient than the compared methods.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 11:13:52 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Arnqvist", "Natalya Pya", ""], ["Ngendangenzwa", "Blaise", ""], ["Lindahl", "Eric", ""], ["Nilsson", "Leif", ""], ["Yu", "Jun", ""]]}, {"id": "1906.11905", "submitter": "Xinjie Lan", "authors": "Xinjie Lan", "title": "A synthetic dataset for deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel method for generating a synthetic dataset\nobeying Gaussian distribution. Compared to the commonly used benchmark datasets\nwith unknown distribution, the synthetic dataset has an explicit distribution,\ni.e., Gaussian distribution. Meanwhile, it has the same characteristics as the\nbenchmark dataset MNIST. As a result, we can easily apply Deep Neural Networks\n(DNNs) on the synthetic dataset. This synthetic dataset provides a novel\nexperimental tool to verify the proposed theories of deep learning.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 05:16:40 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Lan", "Xinjie", ""]]}, {"id": "1906.11906", "submitter": "Xiaoyi Liu", "authors": "Xiaoyi Liu, Diego Klabjan, Patrick NBless", "title": "Data Extraction from Charts via Single Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic data extraction from charts is challenging for two reasons: there\nexist many relations among objects in a chart, which is not a common\nconsideration in general computer vision problems; and different types of\ncharts may not be processed by the same model. To address these problems, we\npropose a framework of a single deep neural network, which consists of object\ndetection, text recognition and object matching modules. The framework handles\nboth bar and pie charts, and it may also be extended to other types of charts\nby slight revisions and by augmenting the training data. Our model performs\nsuccessfully on 79.4% of test simulated bar charts and 88.0% of test simulated\npie charts, while for charts outside of the training domain it degrades for\n57.5% and 62.3%, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 21:54:50 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Liu", "Xiaoyi", ""], ["Klabjan", "Diego", ""], ["NBless", "Patrick", ""]]}, {"id": "1906.11907", "submitter": "Stephen Law Dr", "authors": "Stephen Law and Mateo Neira", "title": "An unsupervised approach to Geographical Knowledge Discovery using\n  street level and street network images", "comments": "SigSpatial 2019 GeoAI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent researches have shown the increasing use of machine learn-ing methods\nin geography and urban analytics, primarily to extract features and patterns\nfrom spatial and temporal data using a supervised approach. Researches\nintegrating geographical processes in machine learning models and the use of\nunsupervised approacheson geographical data for knowledge discovery had been\nsparse. This research contributes to the ladder, where we show how latent\nvariables learned from unsupervised learning methods on urbanimages can be used\nfor geographic knowledge discovery. In particular, we propose a simple approach\ncalled Convolutional-PCA(ConvPCA) which are applied on both street level and\nstreet network images to find a set of uncorrelated and ordered visual\nlatentcomponents. The approach allows for meaningful explanations using a\ncombination of geographical and generative visualisations to explore the latent\nspace, and to show how the learned representation can be used to predict urban\ncharacteristics such as streetquality and street network attributes. The\nresearch also finds that the visual components from the ConvPCA model achieves\nsimilaraccuracy when compared to less interpretable dimension reduction\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 09:33:09 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 13:27:35 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Law", "Stephen", ""], ["Neira", "Mateo", ""]]}, {"id": "1906.11912", "submitter": "Luna Zhang", "authors": "Luna M. Zhang", "title": "A New Compensatory Genetic Algorithm-Based Method for Effective\n  Compressed Multi-function Convolutional Neural Network Model Selection with\n  Multi-Objective Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there have been many popular Convolutional Neural Networks\n(CNNs), such as Google's Inception-V4, that have performed very well for\nvarious image classification problems. These commonly used CNN models usually\nuse the same activation function, such as RELU, for all neurons in the\nconvolutional layers; they are \"Single-function CNNs.\" However, SCNNs may not\nalways be optimal. Thus, a \"Multi-function CNN\" (MCNN), which uses different\nactivation functions for different neurons, has been shown to outperform a\nSCNN. Also, CNNs typically have very large architectures that use a lot of\nmemory and need a lot of data in order to be trained well. As a result, they\ntend to have very high training and prediction times too. An important research\nproblem is how to automatically and efficiently find the best CNN with both\nhigh classification performance and compact architecture with high training and\nprediction speeds, small power usage, and small memory size for any image\nclassification problem. It is very useful to intelligently find an effective,\nfast, energy-efficient, and memory-efficient \"Compressed Multi-function CNN\"\n(CMCNN) from a large number of candidate MCNNs. A new compensatory algorithm\nusing a new genetic algorithm (GA) is created to find the best CMCNN with an\nideal compensation between performance and architecture size. The optimal CMCNN\nhas the best performance and the smallest architecture size. Simulations using\nthe CIFAR10 dataset showed that the new compensatory algorithm could find\nCMCNNs that could outperform non-compressed MCNNs in terms of classification\nperformance (F1-score), speed, power usage, and memory usage. Other effective,\nfast, power-efficient, and memory-efficient CMCNNs based on popular CNN\narchitectures will be developed for image classification problems in important\nreal-world applications, such as brain informatics and biomedical imaging.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 04:09:19 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Zhang", "Luna M.", ""]]}, {"id": "1906.11925", "submitter": "Xiaojun Zhai", "authors": "Sahar Soheilian Esfahani, Xiaojun Zhai, Minsi Chen, Abbes Amira,\n  Faycal Bensaali, Julien AbiNahed, Sarada Dakua, Georges Younes, Robin A.\n  Richardson, Peter V. Coveney", "title": "HEMELB Acceleration and Visualization for Cerebral Aneurysms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A weakness in the wall of a cerebral artery causing a dilation or ballooning\nof the blood vessel is known as a cerebral aneurysm. Optimal treatment requires\nfast and accurate diagnosis of the aneurysm. HemeLB is a fluid dynamics solver\nfor complex geometries developed to provide neurosurgeons with information\nrelated to the flow of blood in and around aneurysms. On a cost efficient\nplatform, HemeLB could be employed in hospitals to provide surgeons with the\nsimulation results in real-time. In this work, we developed an improved version\nof HemeLB for GPU implementation and result visualization. A visualization\nplatform for smooth interaction with end users is also presented. Finally, a\ncomprehensive evaluation of this implementation is reported. The results\ndemonstrate that the proposed implementation achieves a maximum performance of\n15,168,964 site updates per second, and is capable of speeding up HemeLB for\ndeployment in hospitals and clinical investigations.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 19:23:57 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Esfahani", "Sahar Soheilian", ""], ["Zhai", "Xiaojun", ""], ["Chen", "Minsi", ""], ["Amira", "Abbes", ""], ["Bensaali", "Faycal", ""], ["AbiNahed", "Julien", ""], ["Dakua", "Sarada", ""], ["Younes", "Georges", ""], ["Richardson", "Robin A.", ""], ["Coveney", "Peter V.", ""]]}, {"id": "1906.11927", "submitter": "Daniel Barath", "authors": "Daniel Barath, Zuzana Kukelova", "title": "Homography from two orientation- and scale-covariant features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a geometric interpretation of the angles and scales which\nthe orientation- and scale-covariant feature detectors, e.g. SIFT, provide. Two\nnew general constraints are derived on the scales and rotations which can be\nused in any geometric model estimation tasks. Using these formulas, two new\nconstraints on homography estimation are introduced. Exploiting the derived\nequations, a solver for estimating the homography from the minimal number of\ntwo correspondences is proposed. Also, it is shown how the normalization of the\npoint correspondences affects the rotation and scale parameters, thus achieving\nnumerically stable results. Due to requiring merely two feature pairs, robust\nestimators, e.g. RANSAC, do significantly fewer iterations than by using the\nfour-point algorithm. When using covariant features, e.g. SIFT, the information\nabout the scale and orientation is given at no cost. The proposed homography\nestimation method is tested in a synthetic environment and on publicly\navailable real-world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 19:34:08 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Barath", "Daniel", ""], ["Kukelova", "Zuzana", ""]]}, {"id": "1906.11942", "submitter": "Jianglin Fu", "authors": "Jianglin Fu, Ivan V. Bajic, and Rodney G. Vaughan", "title": "Datasets for Face and Object Detection in Fisheye Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two new fisheye image datasets for training face and object\ndetection models: VOC-360 and Wider-360. The fisheye images are created by\npost-processing regular images collected from two well-known datasets, VOC2012\nand Wider Face, using a model for mapping regular to fisheye images implemented\nin Matlab. VOC-360 contains 39,575 fisheye images for object detection,\nsegmentation, and classification. Wider-360 contains 63,897 fisheye images for\nface detection. These datasets will be useful for developing face and object\ndetectors as well as segmentation modules for fisheye images while the efforts\nto collect and manually annotate true fisheye images are underway.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 20:21:45 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Fu", "Jianglin", ""], ["Bajic", "Ivan V.", ""], ["Vaughan", "Rodney G.", ""]]}, {"id": "1906.11951", "submitter": "Evan Racah Mr.", "authors": "Evan Racah, Christopher Pal", "title": "Supervise Thyself: Examining Self-Supervised Representations in\n  Interactive Environments", "comments": "Accepted to the 2019 ICML Workshop on Self-Supervised Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised methods, wherein an agent learns representations solely by\nobserving the results of its actions, become crucial in environments which do\nnot provide a dense reward signal or have labels. In most cases, such methods\nare used for pretraining or auxiliary tasks for \"downstream\" tasks, such as\ncontrol, exploration, or imitation learning. However, it is not clear which\nmethod's representations best capture meaningful features of the environment,\nand which are best suited for which types of environments. We present a\nsmall-scale study of self-supervised methods on two visual environments: Flappy\nBird and Sonic The Hedgehog. In particular, we quantitatively evaluate the\nrepresentations learned from these tasks in two contexts: a) the extent to\nwhich the representations capture true state information of the agent and b)\nhow generalizable these representations are to novel situations, like new\nlevels and textures. Lastly, we evaluate these self-supervised features by\nvisualizing which parts of the environment they focus on. Our results show that\nthe utility of the representations is highly dependent on the visuals and\ndynamics of the environment.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 20:38:47 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Racah", "Evan", ""], ["Pal", "Christopher", ""]]}, {"id": "1906.11960", "submitter": "Tempestt Neal", "authors": "Khadija Zanna, Sayde King, Tempestt Neal, Shaun Canavan", "title": "Studying the Impact of Mood on Identifying Smartphone Users", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the identification of smartphone users when certain\nsamples collected while the subject felt happy, upset or stressed were absent\nor present. We employ data from 19 subjects using the StudentLife dataset, a\ndataset collected by researchers at Dartmouth College that was originally\ncollected to correlate behaviors characterized by smartphone usage patterns\nwith changes in stress and academic performance. Although many previous works\non behavioral biometrics have implied that mood is a source of intra-person\nvariation which may impact biometric performance, our results contradict this\nassumption. Our findings show that performance worsens when removing samples\nthat were generated when subjects may be happy, upset, or stressed. Thus, there\nis no indication that mood negatively impacts performance. However, we do find\nthat changes existing in smartphone usage patterns may correlate with mood,\nincluding changes in locking, audio, location, calling, homescreen, and e-mail\nhabits. Thus, we show that while mood is a source of intra-person variation, it\nmay be an inaccurate assumption that biometric systems (particularly, mobile\nbiometrics) are likely influenced by mood.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 20:55:16 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Zanna", "Khadija", ""], ["King", "Sayde", ""], ["Neal", "Tempestt", ""], ["Canavan", "Shaun", ""]]}, {"id": "1906.11979", "submitter": "Hanxiang Hao", "authors": "Hanxiang Hao, David G\\\"uera, Amy R. Reibman and Edward J. Delp", "title": "A Utility-Preserving GAN for Face Obscuration", "comments": "6 pages, 5 figures, presented at the ICML 2019 Worksop on Synthetic\n  Realities: Deep Learning for Detecting AudioVisual Fakes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From TV news to Google StreetView, face obscuration has been used for privacy\nprotection. Due to recent advances in the field of deep learning, obscuration\nmethods such as Gaussian blurring and pixelation are not guaranteed to conceal\nidentity. In this paper, we propose a utility-preserving generative model,\nUP-GAN, that is able to provide an effective face obscuration, while preserving\nfacial utility. By utility-preserving we mean preserving facial features that\ndo not reveal identity, such as age, gender, skin tone, pose, and expression.\nWe show that the proposed method achieves the best performance in terms of\nobscuration and utility preservation.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 22:01:27 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Hao", "Hanxiang", ""], ["G\u00fcera", "David", ""], ["Reibman", "Amy R.", ""], ["Delp", "Edward J.", ""]]}, {"id": "1906.11981", "submitter": "Ringo S.W. Chu Mr.", "authors": "Ringo S.W. Chu, Ho-Cheung Ng, Xiwei Wang, Wayne Luk", "title": "Convolution Based Spectral Partitioning Architecture for Hyperspectral\n  Image Classification", "comments": "Accepted for publication in IGARSS'2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral images (HSIs) can distinguish materials with high number of\nspectral bands, which is widely adopted in remote sensing applications and\nbenefits in high accuracy land cover classifications. However, HSIs processing\nare tangled with the problem of high dimensionality and limited amount of\nlabelled data. To address these challenges, this paper proposes a deep learning\narchitecture using three dimensional convolutional neural networks with\nspectral partitioning to perform effective feature extraction. We conduct\nexperiments using Indian Pines and Salinas scenes acquired by NASA Airborne\nVisible/Infra-Red Imaging Spectrometer. In comparison to prior results, our\narchitecture shows competitive performance for classification results over\ncurrent methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 22:06:15 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Chu", "Ringo S. W.", ""], ["Ng", "Ho-Cheung", ""], ["Wang", "Xiwei", ""], ["Luk", "Wayne", ""]]}, {"id": "1906.11992", "submitter": "Huu Le", "authors": "Huu Le, Tuan Hoang, and Michael Milford", "title": "BTEL: A Binary Tree Encoding Approach for Visual Localization", "comments": "Accepted to IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual localization algorithms have achieved significant improvements in\nperformance thanks to recent advances in camera technology and vision-based\ntechniques. However, there remains one critical caveat: all current approaches\nthat are based on image retrieval currently scale at best linearly with the\nsize of the environment with respect to both storage, and consequentially in\nmost approaches, query time. This limitation severely curtails the capability\nof autonomous systems in a wide range of compute, power, storage, size, weight\nor cost constrained applications such as drones. In this work, we present a\nnovel binary tree encoding approach for visual localization which can serve as\nan alternative for existing quantization and indexing techniques. The proposed\ntree structure allows us to derive a compressed training scheme that achieves\nsub-linearity in both required storage and inference time. The encoding memory\ncan be easily configured to satisfy different storage constraints. Moreover,\nour approach is amenable to an optional sequence filtering mechanism to further\nimprove the localization results, while maintaining the same amount of storage.\nOur system is entirely agnostic to the front-end descriptors, allowing it to be\nused on top of recent state-of-the-art image representations. Experimental\nresults show that the proposed method significantly outperforms\nstate-of-the-art approaches under limited storage constraints.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 23:27:06 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Le", "Huu", ""], ["Hoang", "Tuan", ""], ["Milford", "Michael", ""]]}, {"id": "1906.12021", "submitter": "Saeed Anwar", "authors": "Saeed Anwar and Nick Barnes", "title": "Densely Residual Laplacian Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-Resolution convolutional neural networks have recently demonstrated\nhigh-quality restoration for single images. However, existing algorithms often\nrequire very deep architectures and long training times. Furthermore, current\nconvolutional neural networks for super-resolution are unable to exploit\nfeatures at multiple scales and weigh them equally, limiting their learning\ncapability. In this exposition, we present a compact and accurate\nsuper-resolution algorithm namely, Densely Residual Laplacian Network (DRLN).\nThe proposed network employs cascading residual on the residual structure to\nallow the flow of low-frequency information to focus on learning high and\nmid-level features. In addition, deep supervision is achieved via the densely\nconcatenated residual blocks settings, which also helps in learning from\nhigh-level complex features. Moreover, we propose Laplacian attention to model\nthe crucial features to learn the inter and intra-level dependencies between\nthe feature maps. Furthermore, comprehensive quantitative and qualitative\nevaluations on low-resolution, noisy low-resolution, and real historical image\nbenchmark datasets illustrate that our DRLN algorithm performs favorably\nagainst the state-of-the-art methods visually and accurately.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 02:32:44 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 05:31:05 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Anwar", "Saeed", ""], ["Barnes", "Nick", ""]]}, {"id": "1906.12028", "submitter": "Yi Tu", "authors": "Yi Tu, Li Niu, Junjie Chen, Dawei Cheng, and Liqing Zhang", "title": "Learning from Web Data with Self-Organizing Memory Module", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from web data has attracted lots of research interest in recent\nyears. However, crawled web images usually have two types of noises, label\nnoise and background noise, which induce extra difficulties in utilizing them\neffectively. Most existing methods either rely on human supervision or ignore\nthe background noise. In this paper, we propose a novel method, which is\ncapable of handling these two types of noises together, without the supervision\nof clean images in the training stage. Particularly, we formulate our method\nunder the framework of multi-instance learning by grouping ROIs (i.e., images\nand their region proposals) from the same category into bags. ROIs in each bag\nare assigned with different weights based on the representative/discriminative\nscores of their nearest clusters, in which the clusters and their scores are\nobtained via our designed memory module. Our memory module could be naturally\nintegrated with the classification module, leading to an end-to-end trainable\nsystem. Extensive experiments on four benchmark datasets demonstrate the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 03:29:01 GMT"}, {"version": "v2", "created": "Sat, 7 Sep 2019 11:09:45 GMT"}, {"version": "v3", "created": "Sat, 16 Nov 2019 12:53:10 GMT"}, {"version": "v4", "created": "Sun, 8 Mar 2020 02:33:56 GMT"}, {"version": "v5", "created": "Wed, 11 Mar 2020 17:08:27 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Tu", "Yi", ""], ["Niu", "Li", ""], ["Chen", "Junjie", ""], ["Cheng", "Dawei", ""], ["Zhang", "Liqing", ""]]}, {"id": "1906.12064", "submitter": "G\\\"unther Reitberger", "authors": "G\\\"unther Reitberger, Tomas Sauer", "title": "Background Subtraction using Adaptive Singular Value Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important task when processing sensor data is to distinguish relevant from\nirrelevant data. This paper describes a method for an iterative singular value\ndecomposition that maintains a model of the background via singular vectors\nspanning a subspace of the image space, thus providing a way to determine the\namount of new information contained in an incoming frame. We update the\nsingular vectors spanning the background space in a computationally efficient\nmanner and provide the ability to perform block-wise updates, leading to a fast\nand robust adaptive SVD computation. The effects of those two properties and\nthe success of the overall method to perform a state of the art background\nsubtraction are shown in both qualitative and quantitative evaluations.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 07:17:14 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Reitberger", "G\u00fcnther", ""], ["Sauer", "Tomas", ""]]}, {"id": "1906.12075", "submitter": "Nikos Melanitis", "authors": "Nikos Melanitis and Petros Maragos", "title": "A linear method for camera pair self-calibration and multi-view\n  reconstruction with geometrically verified correspondences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine 3D reconstruction of architectural scenes in unordered sets of\nuncalibrated images. We introduce a linear method to self-calibrate and find\nthe metric reconstruction of a camera pair. We assume unknown and different\nfocal lengths but otherwise known internal camera parameters and a known\nprojective reconstruction of the camera pair. We recover two possible camera\nconfigurations in space and use the Cheirality condition, that all 3D scene\npoints are in front of both cameras, to disambiguate the solution. We show in\ntwo Theorems, first that the two solutions are in mirror positions and then the\nrelations between their viewing directions. Our new method performs on par\n(median rotation error $\\Delta R = 3.49^{\\circ}$) with the standard approach of\nKruppa equations ($\\Delta R = 3.77^{\\circ}$) for self-calibration and 5-Point\nalgorithm for calibrated metric reconstruction of a camera pair. We reject\nerroneous image correspondences by introducing a method to examine whether\npoint correspondences appear in the same order along $x, y$ image axes in image\npairs. We evaluate this method by its precision and recall and show that it\nimproves the robustness of point matches in architectural and general scenes.\nFinally, we integrate all the introduced methods to a 3D reconstruction\npipeline. We utilize the numerous camera pair metric recontructions using\nrotation-averaging algorithms and a novel method to average focal length\nestimates.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 07:45:19 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Melanitis", "Nikos", ""], ["Maragos", "Petros", ""]]}, {"id": "1906.12118", "submitter": "Hang Min", "authors": "Hang Min, Devin Wilson, Yinhuang Huang, Siyu Liu, Stuart Crozier,\n  Andrew P Bradley, Shekhar S. Chandra", "title": "Fully automatic computer-aided mass detection and segmentation via\n  pseudo-color mammograms and Mask R-CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mammographic mass detection and segmentation are usually performed as serial\nand separate tasks, with segmentation often only performed on manually\nconfirmed true positive detections in previous studies. We propose a\nfully-integrated computer-aided detection (CAD) system for simultaneous\nmammographic mass detection and segmentation without user intervention. The\nproposed CAD only consists of a pseudo-color image generation and a mass\ndetection-segmentation stage based on Mask R-CNN. Grayscale mammograms are\ntransformed into pseudo-color images based on multi-scale morphological sifting\nwhere mass-like patterns are enhanced to improve the performance of Mask R-CNN.\nTransfer learning with the Mask R-CNN is then adopted to simultaneously detect\nand segment masses on the pseudo-color images. Evaluated on the public dataset\nINbreast, the method outperforms the state-of-the-art methods by achieving an\naverage true positive rate of 0.90 at 0.9 false positive per image and an\naverage Dice similarity index of 0.88 for mass segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 09:58:25 GMT"}, {"version": "v2", "created": "Sat, 19 Oct 2019 06:42:40 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Min", "Hang", ""], ["Wilson", "Devin", ""], ["Huang", "Yinhuang", ""], ["Liu", "Siyu", ""], ["Crozier", "Stuart", ""], ["Bradley", "Andrew P", ""], ["Chandra", "Shekhar S.", ""]]}, {"id": "1906.12151", "submitter": "Maria Leyva Vallina", "authors": "Maria Leyva-Vallina, Nicola Strisciuglio, Nicolai Petkov", "title": "Place recognition in gardens by learning visual representations: data\n  set and benchmark analysis", "comments": "Accepted for the 18th International Conference on Computer Analysis\n  of Images and Patterns", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual place recognition is an important component of systems for camera\nlocalization and loop closure detection. It concerns the recognition of a\npreviously visited place based on visual cues only. Although it is a widely\nstudied problem for indoor and urban environments, the recent use of robots for\nautomation of agricultural and gardening tasks has created new problems, due to\nthe challenging appearance of garden-like environments. Garden scenes\npredominantly contain green colors, as well as repetitive patterns and\ntextures. The lack of available data recorded in gardens and natural\nenvironments makes the improvement of visual localization algorithms difficult.\nIn this paper we propose an extended version of the TB-Places data set, which\nis designed for testing algorithms for visual place recognition. It contains\nimages with ground truth camera pose recorded in real gardens in different\nseasons, with varying light conditions. We constructed and released a ground\ntruth for all possible pairs of images, indicating whether they depict the same\nplace or not. We present the results of a benchmark analysis of methods based\non convolutional neural networks for holistic image description and place\nrecognition. We train existing networks (i.e. ResNet, DenseNet and VGG NetVLAD)\nas backbone of a two-way architecture with a contrastive loss function. The\nresults that we obtained demonstrate that learning garden-tailored\nrepresentations contribute to an improvement of performance, although the\ngeneralization capabilities are limited.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 12:01:55 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Leyva-Vallina", "Maria", ""], ["Strisciuglio", "Nicola", ""], ["Petkov", "Nicolai", ""]]}, {"id": "1906.12158", "submitter": "Zhu Zhang", "authors": "Zhu Zhang, Zhou Zhao, Zhijie Lin, Jingkuan Song and Xiaofei He", "title": "Open-Ended Long-Form Video Question Answering via Hierarchical\n  Convolutional Self-Attention Networks", "comments": "Accepted by IJCAI 2019 as a poster paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-ended video question answering aims to automatically generate the\nnatural-language answer from referenced video contents according to the given\nquestion. Currently, most existing approaches focus on short-form video\nquestion answering with multi-modal recurrent encoder-decoder networks.\nAlthough these works have achieved promising performance, they may still be\nineffectively applied to long-form video question answering due to the lack of\nlong-range dependency modeling and the suffering from the heavy computational\ncost. To tackle these problems, we propose a fast Hierarchical Convolutional\nSelf-Attention encoder-decoder network(HCSA). Concretely, we first develop a\nhierarchical convolutional self-attention encoder to efficiently model\nlong-form video contents, which builds the hierarchical structure for video\nsequences and captures question-aware long-range dependencies from video\ncontext. We then devise a multi-scale attentive decoder to incorporate\nmulti-layer video representations for answer generation, which avoids the\ninformation missing of the top encoder layer. The extensive experiments show\nthe effectiveness and efficiency of our method.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 12:23:37 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Zhang", "Zhu", ""], ["Zhao", "Zhou", ""], ["Lin", "Zhijie", ""], ["Song", "Jingkuan", ""], ["He", "Xiaofei", ""]]}, {"id": "1906.12159", "submitter": "Abhinav Ravi", "authors": "Abhinav Ravi, Arun Patro, Vikram Garg, Anoop Kolar Rajagopal, Aruna\n  Rajan, Rajdeep Hazra Banerjee", "title": "Teaching DNNs to design fast fashion", "comments": "8 pages, 9 figures, KDD conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $ $\"Fast Fashion\" spearheads the biggest disruption in fashion that enabled\nto engineer resilient supply chains to quickly respond to changing fashion\ntrends. The conventional design process in commercial manufacturing is often\nfed through \"trends\" or prevailing modes of dressing around the world that\nindicate sudden interest in a new form of expression, cyclic patterns, and\npopular modes of expression for a given time frame. In this work, we propose a\nfully automated system to explore, detect, and finally synthesize trends in\nfashion into design elements by designing representative prototypes of apparel\ngiven time series signals generated from social media feeds. Our system is\nenvisioned to be the first step in design of Fast Fashion where the production\ncycle for clothes from design inception to manufacturing is meant to be rapid\nand responsive to current \"trends\". It also works to reduce wastage in fashion\nproduction by taking in customer feedback on sellability at the time of design\ngeneration. We also provide an interface wherein the designers can play with\nmultiple trending styles in fashion and visualize designs as interpolations of\nelements of these styles. We aim to aid the creative process through generating\ninteresting and inspiring combinations for a designer to mull by running them\nthrough her key customers.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 06:48:45 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 06:30:09 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Ravi", "Abhinav", ""], ["Patro", "Arun", ""], ["Garg", "Vikram", ""], ["Rajagopal", "Anoop Kolar", ""], ["Rajan", "Aruna", ""], ["Banerjee", "Rajdeep Hazra", ""]]}, {"id": "1906.12165", "submitter": "Zhu Zhang", "authors": "Zhu Zhang, Zhou Zhao, Zhijie Lin, Jingkuan Song and Deng Cai", "title": "Localizing Unseen Activities in Video via Image Query", "comments": "Accepted by IJCAI 2019 as a poster paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action localization in untrimmed videos is an important topic in the field of\nvideo understanding. However, existing action localization methods are\nrestricted to a pre-defined set of actions and cannot localize unseen\nactivities. Thus, we consider a new task to localize unseen activities in\nvideos via image queries, named Image-Based Activity Localization. This task\nfaces three inherent challenges: (1) how to eliminate the influence of\nsemantically inessential contents in image queries; (2) how to deal with the\nfuzzy localization of inaccurate image queries; (3) how to determine the\nprecise boundaries of target segments. We then propose a novel self-attention\ninteraction localizer to retrieve unseen activities in an end-to-end fashion.\nSpecifically, we first devise a region self-attention method with relative\nposition encoding to learn fine-grained image region representations. Then, we\nemploy a local transformer encoder to build multi-step fusion and reasoning of\nimage and video contents. We next adopt an order-sensitive localizer to\ndirectly retrieve the target segment. Furthermore, we construct a new dataset\nActivityIBAL by reorganizing the ActivityNet dataset. The extensive experiments\nshow the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 12:32:41 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Zhang", "Zhu", ""], ["Zhao", "Zhou", ""], ["Lin", "Zhijie", ""], ["Song", "Jingkuan", ""], ["Cai", "Deng", ""]]}, {"id": "1906.12167", "submitter": "Vasile Patrascu", "authors": "Vasile Patrascu", "title": "Gray Level Image Threshold Using Neutrosophic Shannon Entropy", "comments": "The 3rd Conference on Recent Advances in Artificial Intelligence,\n  RAAI 2019, Bucharest, Romania, June 28-30, 2019", "journal-ref": null, "doi": "10.20944/preprints201906.0248.v1", "report-no": "Preprints 2019, 2019060248", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a new method of segmenting grayscale images by\nminimizing Shannon's neutrosophic entropy. For the proposed segmentation\nmethod, the neutrosophic information components, i.e., the degree of truth, the\ndegree of neutrality and the degree of falsity are defined taking into account\nthe belonging to the segmented regions and at the same time to the separation\nthreshold area. The principle of the method is simple and easy to understand\nand can lead to multiple thresholds. The efficacy of the method is illustrated\nusing some test gray level images. The experimental results show that the\nproposed method has good performance for segmentation with optimal gray level\nthresholds.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 05:30:40 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Patrascu", "Vasile", ""]]}, {"id": "1906.12170", "submitter": "Dilip Kumar Margam", "authors": "Dilip Kumar Margam, Rohith Aralikatti, Tanay Sharma, Abhinav Thanda,\n  Pujitha A K, Sharad Roy, Shankar M Venkatesan", "title": "LipReading with 3D-2D-CNN BLSTM-HMM and word-CTC models", "comments": "Submitted to Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning based machine lipreading has gained\nprominence. To this end, several architectures such as LipNet, LCANet and\nothers have been proposed which perform extremely well compared to traditional\nlipreading DNN-HMM hybrid systems trained on DCT features. In this work, we\npropose a simpler architecture of 3D-2D-CNN-BLSTM network with a bottleneck\nlayer. We also present analysis of two different approaches for lipreading on\nthis architecture. In the first approach, 3D-2D-CNN-BLSTM network is trained\nwith CTC loss on characters (ch-CTC). Then BLSTM-HMM model is trained on\nbottleneck lip features (extracted from 3D-2D-CNN-BLSTM ch-CTC network) in a\ntraditional ASR training pipeline. In the second approach, same 3D-2D-CNN-BLSTM\nnetwork is trained with CTC loss on word labels (w-CTC). The first approach\nshows that bottleneck features perform better compared to DCT features. Using\nthe second approach on Grid corpus' seen speaker test set, we report $1.3\\%$\nWER - a $55\\%$ improvement relative to LCANet. On unseen speaker test set we\nreport $8.6\\%$ WER which is $24.5\\%$ improvement relative to LipNet. We also\nverify the method on a second dataset of $81$ speakers which we collected.\nFinally, we also discuss the effect of feature duplication on BLSTM-HMM model\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 14:52:54 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Margam", "Dilip Kumar", ""], ["Aralikatti", "Rohith", ""], ["Sharma", "Tanay", ""], ["Thanda", "Abhinav", ""], ["K", "Pujitha A", ""], ["Roy", "Sharad", ""], ["Venkatesan", "Shankar M", ""]]}, {"id": "1906.12171", "submitter": "Raphael Memmesheimer", "authors": "Pascal Schneider, Raphael Memmesheimer, Ivanna Kramer and Dietrich\n  Paulus", "title": "Gesture Recognition in RGB Videos UsingHuman Body Keypoints and Dynamic\n  Time Warping", "comments": "13 pages, 4 figures, 2 tables, RoboCup 2019 Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Gesture recognition opens up new ways for humans to intuitively interact with\nmachines. Especially for service robots, gestures can be a valuable addition to\nthe means of communication to, for example, draw the robot's attention to\nsomeone or something. Extracting a gesture from video data and classifying it\nis a challenging task and a variety of approaches have been proposed throughout\nthe years. This paper presents a method for gesture recognition in RGB videos\nusing OpenPose to extract the pose of a person and Dynamic Time Warping (DTW)\nin conjunction with One-Nearest-Neighbor (1NN) for time-series classification.\nThe main features of this approach are the independence of any specific\nhardware and high flexibility, because new gestures can be added to the\nclassifier by adding only a few examples of it. We utilize the robustness of\nthe Deep Learning-based OpenPose framework while avoiding the data-intensive\ntask of training a neural network ourselves. We demonstrate the classification\nperformance of our method using a public dataset.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 14:30:38 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Schneider", "Pascal", ""], ["Memmesheimer", "Raphael", ""], ["Kramer", "Ivanna", ""], ["Paulus", "Dietrich", ""]]}, {"id": "1906.12172", "submitter": "Sing-Ho Bae", "authors": "Joonhyun Jeong and Sung-Ho Bae", "title": "New pointwise convolution in Deep Neural Networks through Extremely Fast\n  and Non Parametric Transforms", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some conventional transforms such as Discrete Walsh-Hadamard Transform (DWHT)\nand Discrete Cosine Transform (DCT) have been widely used as feature extractors\nin image processing but rarely applied in neural networks. However, we found\nthat these conventional transforms have the ability to capture the\ncross-channel correlations without any learnable parameters in DNNs. This paper\nfirstly proposes to apply conventional transforms to pointwise convolution,\nshowing that such transforms significantly reduce the computational complexity\nof neural networks without accuracy performance degradation. Especially for\nDWHT, it requires no floating point multiplications but only additions and\nsubtractions, which can considerably reduce computation overheads. In addition,\nits fast algorithm further reduces complexity of floating point addition from\n$\\mathcal{O}(n^2)$ to $\\mathcal{O}(n\\log n)$. These nice properties construct\nextremely efficient networks in the number parameters and operations, enjoying\naccuracy gain. Our proposed DWHT-based model gained 1.49\\% accuracy increase\nwith 79.1\\% reduced parameters and 48.4\\% reduced FLOPs compared with its\nbaseline model (MoblieNet-V1) on the CIFAR 100 dataset.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 10:47:08 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Jeong", "Joonhyun", ""], ["Bae", "Sung-Ho", ""]]}, {"id": "1906.12174", "submitter": "Dongfang Yang", "authors": "Yongfei Li, Dongfang Yang, Shicheng Wang, Hao He", "title": "Road-network-based Rapid Geolocalization", "comments": "19pages, 10 figures, 3 tables. in IEEE Transactions on Geoscience and\n  Remote Sensing", "journal-ref": null, "doi": "10.1109/TGRS.2020.3011034", "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has always been a research hotspot to use geographic information to assist\nthe navigation of unmanned aerial vehicles. In this paper, a road-network-based\nlocalization method is proposed. We match roads in the measurement images to\nthe reference road vector map, and realize successful localization on areas as\nlarge as a whole city. The road network matching problem is treated as a point\ncloud registration problem under two-dimensional projective transformation, and\nsolved under a hypothesise-and-test framework. To deal with the projective\npoint cloud registration problem, a global projective invariant feature is\nproposed, which consists of two road intersections augmented with the\ninformation of their tangents. We call it two road intersections tuple. We\ndeduce the closed-form solution for determining the alignment transformation\nfrom a pair of matching two road intersections tuples. In addition, we propose\nthe necessary conditions for the tuples to match. This can reduce the candidate\nmatching tuples, thus accelerating the search to a great extent. We test all\nthe candidate matching tuples under a hypothesise-and-test framework to search\nfor the best match. The experiments show that our method can localize the\ntarget area over an area of 400 within 1 second on a single cpu.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 01:54:21 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Li", "Yongfei", ""], ["Yang", "Dongfang", ""], ["Wang", "Shicheng", ""], ["He", "Hao", ""]]}, {"id": "1906.12175", "submitter": "Minh Tran", "authors": "Minh Tran, Taylan Sen, Kurtis Haut, Mohammad Rafayet Ali and Mohammed\n  Ehsan Hoque", "title": "Are you really looking at me? A Feature-Extraction Framework for\n  Estimating Interpersonal Eye Gaze from Conventional Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite a revolution in the pervasiveness of video cameras in our daily\nlives, one of the most meaningful forms of nonverbal affective communication,\ninterpersonal eye gaze, i.e. eye gaze relative to a conversation partner, is\nnot available from common video. We introduce the Interpersonal-Calibrating\nEye-gaze Encoder (ICE), which automatically extracts interpersonal gaze from\nvideo recordings without specialized hardware and without prior knowledge of\nparticipant locations. Leveraging the intuition that individuals spend a large\nportion of a conversation looking at each other enables the ICE dynamic\nclustering algorithm to extract interpersonal gaze. We validate ICE in both\nvideo chat using an objective metric with an infrared gaze tracker (F1=0.846,\nN=8), as well as in face-to-face communication with expert-rated evaluations of\neye contact (r= 0.37, N=170). We then use ICE to analyze behavior in two\ndifferent, yet important affective communication domains: interrogation-based\ndeception detection, and communication skill assessment in speed dating. We\nfind that honest witnesses break interpersonal gaze contact and look down more\noften than deceptive witnesses when answering questions (p=0.004, d=0.79). In\npredicting expert communication skill ratings in speed dating videos, we\ndemonstrate that interpersonal gaze alone has more predictive power than facial\nexpressions.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 19:23:31 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 20:32:21 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Tran", "Minh", ""], ["Sen", "Taylan", ""], ["Haut", "Kurtis", ""], ["Ali", "Mohammad Rafayet", ""], ["Hoque", "Mohammed Ehsan", ""]]}, {"id": "1906.12176", "submitter": "Stephen Hausler", "authors": "Stephen Hausler, Adam Jacobson and Michael Milford", "title": "Filter Early, Match Late: Improving Network-Based Visual Place\n  Recognition", "comments": "Pre-print version of article which will be presented at IROS 2019", "journal-ref": null, "doi": "10.1109/iros40897.2019.8967783", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CNNs have excelled at performing place recognition over time, particularly\nwhen the neural network is optimized for localization in the current\nenvironmental conditions. In this paper we investigate the concept of feature\nmap filtering, where, rather than using all the activations within a\nconvolutional tensor, only the most useful activations are used. Since specific\nfeature maps encode different visual features, the objective is to remove\nfeature maps that are detract from the ability to recognize a location across\nappearance changes. Our key innovation is to filter the feature maps in an\nearly convolutional layer, but then continue to run the network and extract a\nfeature vector using a later layer in the same network. By filtering early\nvisual features and extracting a feature vector from a higher, more viewpoint\ninvariant later layer, we demonstrate improved condition and viewpoint\ninvariance. Our approach requires image pairs for training from the deployment\nenvironment, but we show that state-of-the-art performance can regularly be\nachieved with as little as a single training image pair. An exhaustive\nexperimental analysis is performed to determine the full scope of causality\nbetween early layer filtering and late layer extraction. For validity, we use\nthree datasets: Oxford RobotCar, Nordland, and Gardens Point, achieving overall\nsuperior performance to NetVLAD. The work provides a number of new avenues for\nexploring CNN optimizations, without full re-training.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 06:12:27 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Hausler", "Stephen", ""], ["Jacobson", "Adam", ""], ["Milford", "Michael", ""]]}, {"id": "1906.12177", "submitter": "Rihuan Ke", "authors": "Rihuan Ke, Aur\\'elie Bugeau, Nicolas Papadakis, Peter Schuetz,\n  Carola-Bibiane Sch\\\"onlieb", "title": "Learning to segment microscopy images with lazy labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for labour intensive pixel-wise annotation is a major limitation of\nmany fully supervised learning methods for segmenting bioimages that can\ncontain numerous object instances with thin separations. In this paper, we\nintroduce a deep convolutional neural network for microscopy image\nsegmentation. Annotation issues are circumvented by letting the network being\ntrainable on coarse labels combined with only a very small number of images\nwith pixel-wise annotations. We call this new labelling strategy `lazy' labels.\nImage segmentation is stratified into three connected tasks: rough inner region\ndetection, object separation and pixel-wise segmentation. These tasks are\nlearned in an end-to-end multi-task learning framework. The method is\ndemonstrated on two microscopy datasets, where we show that the model gives\naccurate segmentation results even if exact boundary labels are missing for a\nmajority of annotated data. It brings more flexibility and efficiency for\ntraining deep neural networks that are data hungry and is applicable to\nbiomedical images with poor contrast at the object boundaries or with diverse\ntextures and repeated patterns.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 17:38:54 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 10:08:44 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Ke", "Rihuan", ""], ["Bugeau", "Aur\u00e9lie", ""], ["Papadakis", "Nicolas", ""], ["Schuetz", "Peter", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "1906.12181", "submitter": "Xuetong Xue", "authors": "Ziqi Ren, Jie Li, Xuetong Xue, Xin Li, Fan Yang, Zhicheng Jiao, Xinbo\n  Gao", "title": "Reconstructing Perceived Images from Brain Activity by Visually-guided\n  Cognitive Representation and Adversarial Learning", "comments": "11 pages, 7 figures, 1 Table and Reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing visual stimulus (image) only from human brain activity\nmeasured with functional Magnetic Resonance Imaging (fMRI) is a significant and\nmeaningful task in Human-AI collaboration. However, the inconsistent\ndistribution and representation between fMRI signals and visual images cause\nthe heterogeneity gap. Moreover, the fMRI data is often extremely\nhigh-dimensional and contains a lot of visually-irrelevant information.\nExisting methods generally suffer from these issues so that a satisfactory\nreconstruction is still challenging. In this paper, we show that it is possible\nto overcome these challenges by learning visually-guided cognitive latent\nrepresentations from the fMRI signals, and inversely decoding them to the image\nstimuli. The resulting framework is called Dual-Variational Autoencoder/\nGenerative Adversarial Network (D-VAE/GAN), which combines the advantages of\nadversarial representation learning with knowledge distillation. In addition,\nwe introduce a novel three-stage learning approach which enables the\n(cognitive) encoder to gradually distill useful knowledge from the paired\n(visual) encoder during the learning process. Extensive experimental results on\nboth artificial and natural images have demonstrated that our method could\nachieve surprisingly good results and outperform all other alternatives.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 03:08:24 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 15:03:48 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Ren", "Ziqi", ""], ["Li", "Jie", ""], ["Xue", "Xuetong", ""], ["Li", "Xin", ""], ["Yang", "Fan", ""], ["Jiao", "Zhicheng", ""], ["Gao", "Xinbo", ""]]}, {"id": "1906.12187", "submitter": "Daniel Brodeski", "authors": "Daniel Brodeski, Igal Bilik, Raja Giryes", "title": "Deep Radar Detector", "comments": "Accepted to RadarConf 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While camera and LiDAR processing have been revolutionized since the\nintroduction of deep learning, radar processing still relies on classical\ntools. In this paper, we introduce a deep learning approach for radar\nprocessing, working directly with the radar complex data. To overcome the lack\nof radar labeled data, we rely in training only on the radar calibration data\nand introduce new radar augmentation techniques. We evaluate our method on the\nradar 4D detection task and demonstrate superior performance compared to the\nclassical approaches while keeping real-time performance. Applying deep\nlearning on radar data has several advantages such as eliminating the need for\nan expensive radar calibration process each time and enabling classification of\nthe detected objects with almost zero-overhead.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 13:30:45 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Brodeski", "Daniel", ""], ["Bilik", "Igal", ""], ["Giryes", "Raja", ""]]}, {"id": "1906.12188", "submitter": "Ahmad Asadi", "authors": "Ahmad Asadi and Reza Safabakhsh", "title": "A Deep Decoder Structure Based on WordEmbedding Regression for An\n  Encoder-Decoder Based Model for Image Captioning", "comments": "19 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating textual descriptions for images has been an attractive problem for\nthe computer vision and natural language processing researchers in recent\nyears. Dozens of models based on deep learning have been proposed to solve this\nproblem. The existing approaches are based on neural encoder-decoder structures\nequipped with the attention mechanism. These methods strive to train decoders\nto minimize the log likelihood of the next word in a sentence given the\nprevious ones, which results in the sparsity of the output space. In this work,\nwe propose a new approach to train decoders to regress the word embedding of\nthe next word with respect to the previous ones instead of minimizing the log\nlikelihood. The proposed method is able to learn and extract long-term\ninformation and can generate longer fine-grained captions without introducing\nany external memory cell. Furthermore, decoders trained by the proposed\ntechnique can take the importance of the generated words into consideration\nwhile generating captions. In addition, a novel semantic attention mechanism is\nproposed that guides attention points through the image, taking the meaning of\nthe previously generated word into account. We evaluate the proposed approach\nwith the MS-COCO dataset. The proposed model outperformed the state of the art\nmodels especially in generating longer captions. It achieved a CIDEr score\nequal to 125.0 and a BLEU-4 score equal to 50.5, while the best scores of the\nstate of the art models are 117.1 and 48.0, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 13:51:59 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Asadi", "Ahmad", ""], ["Safabakhsh", "Reza", ""]]}, {"id": "1906.12193", "submitter": "Jiajie Mo", "authors": "Zhun Fan, Jiajie Mo, Benzhang Qiu, Wenji Li, Guijie Zhu, Chong Li,\n  Jianye Hu, Yibiao Rong, and Xinjian Chen", "title": "Accurate Retinal Vessel Segmentation via Octave Convolution Neural\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinal vessel segmentation is a crucial step in diagnosing and screening\nvarious diseases, including diabetes, ophthalmologic diseases, and\ncardiovascular diseases. In this paper, we propose an effective and efficient\nmethod for vessel segmentation in color fundus images using encoder-decoder\nbased octave convolution networks. Compared with other convolution networks\nutilizing standard convolution for feature extraction, the proposed method\nutilizes octave convolutions and octave transposed convolutions for learning\nmultiple-spatial-frequency features, thus can better capture retinal\nvasculatures with varying sizes and shapes. To provide the network the\ncapability of learning how to decode multifrequency features, we extend octave\nconvolution and propose a new operation named octave transposed convolution. A\nnovel architecture of convolutional neural network, named as Octave UNet\nintegrating both octave convolutions and octave transposed convolutions is\nproposed based on the encoder-decoder architecture of UNet, which can generate\nhigh resolution vessel segmentation in one single forward feeding without\npost-processing steps. Comprehensive experimental results demonstrate that the\nproposed Octave UNet outperforms the baseline UNet achieving better or\ncomparable performance to the state-of-the-art methods with fast processing\nspeed. Specifically, the proposed method achieves 0.9664 / 0.9713 / 0.9759 /\n0.9698 accuracy, 0.8374 / 0.8664 / 0.8670 / 0.8076 sensitivity, 0.9790 / 0.9798\n/ 0.9840 / 0.9831 specificity, 0.8127 / 0.8191 / 0.8313 / 0.7963 F1 score, and\n0.9835 / 0.9875 / 0.9905 / 0.9845 Area Under Receiver Operating Characteristic\ncurve, on DRIVE, STARE, CHASE_DB1, and HRF datasets, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 13:07:36 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 11:45:32 GMT"}, {"version": "v3", "created": "Wed, 24 Jul 2019 19:15:34 GMT"}, {"version": "v4", "created": "Sat, 27 Jul 2019 19:31:27 GMT"}, {"version": "v5", "created": "Tue, 30 Jul 2019 16:26:45 GMT"}, {"version": "v6", "created": "Mon, 5 Aug 2019 14:38:27 GMT"}, {"version": "v7", "created": "Sun, 11 Aug 2019 06:00:16 GMT"}, {"version": "v8", "created": "Wed, 23 Sep 2020 03:04:25 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Fan", "Zhun", ""], ["Mo", "Jiajie", ""], ["Qiu", "Benzhang", ""], ["Li", "Wenji", ""], ["Zhu", "Guijie", ""], ["Li", "Chong", ""], ["Hu", "Jianye", ""], ["Rong", "Yibiao", ""], ["Chen", "Xinjian", ""]]}, {"id": "1906.12195", "submitter": "Paolo Galeone", "authors": "Emanuele Ghelfi, Paolo Galeone, Michele De Simoni, Federico Di Mattia", "title": "Adversarial Pixel-Level Generation of Semantic Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative Adversarial Networks (GANs) have obtained extraordinary success in\nthe generation of realistic images, a domain where a lower pixel-level accuracy\nis acceptable. We study the problem, not yet tackled in the literature, of\ngenerating semantic images starting from a prior distribution. Intuitively this\nproblem can be approached using standard methods and architectures. However, a\nbetter-suited approach is needed to avoid generating blurry, hallucinated and\nthus unusable images since tasks like semantic segmentation require pixel-level\nexactness. In this work, we present a novel architecture for learning to\ngenerate pixel-level accurate semantic images, namely Semantic Generative\nAdversarial Networks (SemGANs). The experimental evaluation shows that our\narchitecture outperforms standard ones from both a quantitative and a\nqualitative point of view in many semantic image generation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 14:25:11 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Ghelfi", "Emanuele", ""], ["Galeone", "Paolo", ""], ["De Simoni", "Michele", ""], ["Di Mattia", "Federico", ""]]}, {"id": "1906.12213", "submitter": "Norbert B\\'atfai Ph.D.", "authors": "Norbert B\\'atfai and D\\'avid Papp and Gerg\\H{o} Bogacsovics and\n  M\\'at\\'e Szab\\'o and Viktor Szil\\'ard Simk\\'o and M\\'ari\\'o Bersenszki and\n  Gergely Szab\\'o and Lajos Kov\\'acs and Ferencz Kov\\'acs and Erik Szilveszter\n  Varga", "title": "On the notion of number in humans and machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we performed two types of software experiments to study the\nnumerosity classification (subitizing) in humans and machines. Experiments\nfocus on a particular kind of task is referred to as Semantic MNIST or simply\nSMNIST where the numerosity of objects placed in an image must be determined.\nThe experiments called SMNIST for Humans are intended to measure the capacity\nof the Object File System in humans. In this type of experiment the measurement\nresult is in well agreement with the value known from the cognitive psychology\nliterature. The experiments called SMNIST for Machines serve similar purposes\nbut they investigate existing, well known (but originally developed for other\npurpose) and under development deep learning computer programs. These\nmeasurement results can be interpreted similar to the results from SMNIST for\nHumans. The main thesis of this paper can be formulated as follows: in machines\nthe image classification artificial neural networks can learn to distinguish\nnumerosities with better accuracy when these numerosities are smaller than the\ncapacity of OFS in humans. Finally, we outline a conceptual framework to\ninvestigate the notion of number in humans and machines.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 09:26:22 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["B\u00e1tfai", "Norbert", ""], ["Papp", "D\u00e1vid", ""], ["Bogacsovics", "Gerg\u0151", ""], ["Szab\u00f3", "M\u00e1t\u00e9", ""], ["Simk\u00f3", "Viktor Szil\u00e1rd", ""], ["Bersenszki", "M\u00e1ri\u00f3", ""], ["Szab\u00f3", "Gergely", ""], ["Kov\u00e1cs", "Lajos", ""], ["Kov\u00e1cs", "Ferencz", ""], ["Varga", "Erik Szilveszter", ""]]}, {"id": "1906.12223", "submitter": "Mohamed Elmahdy", "authors": "Mohamed S. Elmahdy, Jelmer M. Wolterink, Hessam Sokooti, Ivana\n  I\\v{s}gum and Marius Staring", "title": "Adversarial optimization for joint registration and segmentation in\n  prostate CT radiotherapy", "comments": "Accepted to MICCAI 2019, 13-17 Oct 2019, Shenzhen, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint image registration and segmentation has long been an active area of\nresearch in medical imaging. Here, we reformulate this problem in a deep\nlearning setting using adversarial learning. We consider the case in which\nfixed and moving images as well as their segmentations are available for\ntraining, while segmentations are not available during testing; a common\nscenario in radiotherapy. The proposed framework consists of a 3D end-to-end\ngenerator network that estimates the deformation vector field (DVF) between\nfixed and moving images in an unsupervised fashion and applies this DVF to the\nmoving image and its segmentation. A discriminator network is trained to\nevaluate how well the moving image and segmentation align with the fixed image\nand segmentation. The proposed network was trained and evaluated on follow-up\nprostate CT scans for image-guided radiotherapy, where the planning CT contours\nare propagated to the daily CT images using the estimated DVF. A quantitative\ncomparison with conventional registration using \\texttt{elastix} showed that\nthe proposed method improved performance and substantially reduced computation\ntime, thus enabling real-time contour propagation necessary for online-adaptive\nradiotherapy.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 13:55:03 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Elmahdy", "Mohamed S.", ""], ["Wolterink", "Jelmer M.", ""], ["Sokooti", "Hessam", ""], ["I\u0161gum", "Ivana", ""], ["Staring", "Marius", ""]]}, {"id": "1906.12225", "submitter": "Kin Quan", "authors": "Kin Quan, Ryutaro Tanno, Michael Duong, Arjun Nair, Rebecca Shipley,\n  Mark Jones, Christopher Brereton, John Hurst, David Hawkes and Joseph Jacob", "title": "Modelling Airway Geometry as Stock Market Data using Bayesian\n  Changepoint Detection", "comments": "14 pages, 7 figures, Accepted to The 10th International Workshop on\n  Machine Learning in Medical Imaging (MLMI 2019). In conjunction with MICCAI\n  2019, Shenzhen, China", "journal-ref": "In Lecture Notes in Computer Science, vol 11861. (2019) Springer,\n  Cham", "doi": "10.1007/978-3-030-32692-0_40", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous lung diseases, such as idiopathic pulmonary fibrosis (IPF), exhibit\ndilation of the airways. Accurate measurement of dilatation enables assessment\nof the progression of disease. Unfortunately the combination of image noise and\nairway bifurcations causes high variability in the profiles of cross-sectional\nareas, rendering the identification of affected regions very difficult. Here we\nintroduce a noise-robust method for automatically detecting the location of\nprogressive airway dilatation given two profiles of the same airway acquired at\ndifferent time points. We propose a probabilistic model of abrupt relative\nvariations between profiles and perform inference via Reversible Jump Markov\nChain Monte Carlo sampling. We demonstrate the efficacy of the proposed method\non two datasets; (i) images of healthy airways with simulated dilatation; (ii)\npairs of real images of IPF-affected airways acquired at 1 year intervals. Our\nmodel is able to detect the starting location of airway dilatation with an\naccuracy of 2.5mm on simulated data. The experiments on the IPF dataset display\nreasonable agreement with radiologists. We can compute a relative change in\nairway volume that may be useful for quantifying IPF disease progression. The\ncode is available at\nhttps://github.com/quan14/Modelling_Airway_Geometry_as_Stock_Market_Data\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 13:56:58 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2019 18:09:20 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Quan", "Kin", ""], ["Tanno", "Ryutaro", ""], ["Duong", "Michael", ""], ["Nair", "Arjun", ""], ["Shipley", "Rebecca", ""], ["Jones", "Mark", ""], ["Brereton", "Christopher", ""], ["Hurst", "John", ""], ["Hawkes", "David", ""], ["Jacob", "Joseph", ""]]}, {"id": "1906.12320", "submitter": "Guandao Yang", "authors": "Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge Belongie,\n  Bharath Hariharan", "title": "PointFlow: 3D Point Cloud Generation with Continuous Normalizing Flows", "comments": "Published in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As 3D point clouds become the representation of choice for multiple vision\nand graphics applications, the ability to synthesize or reconstruct\nhigh-resolution, high-fidelity point clouds becomes crucial. Despite the recent\nsuccess of deep learning models in discriminative tasks of point clouds,\ngenerating point clouds remains challenging. This paper proposes a principled\nprobabilistic framework to generate 3D point clouds by modeling them as a\ndistribution of distributions. Specifically, we learn a two-level hierarchy of\ndistributions where the first level is the distribution of shapes and the\nsecond level is the distribution of points given a shape. This formulation\nallows us to both sample shapes and sample an arbitrary number of points from a\nshape. Our generative model, named PointFlow, learns each level of the\ndistribution with a continuous normalizing flow. The invertibility of\nnormalizing flows enables the computation of the likelihood during training and\nallows us to train our model in the variational inference framework.\nEmpirically, we demonstrate that PointFlow achieves state-of-the-art\nperformance in point cloud generation. We additionally show that our model can\nfaithfully reconstruct point clouds and learn useful representations in an\nunsupervised manner. The code will be available at\nhttps://github.com/stevenygd/PointFlow.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 17:25:54 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 16:19:40 GMT"}, {"version": "v3", "created": "Mon, 2 Sep 2019 12:11:36 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Yang", "Guandao", ""], ["Huang", "Xun", ""], ["Hao", "Zekun", ""], ["Liu", "Ming-Yu", ""], ["Belongie", "Serge", ""], ["Hariharan", "Bharath", ""]]}, {"id": "1906.12340", "submitter": "Dan Hendrycks", "authors": "Dan Hendrycks and Mantas Mazeika and Saurav Kadavath and Dawn Song", "title": "Using Self-Supervised Learning Can Improve Model Robustness and\n  Uncertainty", "comments": "NeurIPS 2019; code and data available at\n  https://github.com/hendrycks/ss-ood", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervision provides effective representations for downstream tasks\nwithout requiring labels. However, existing approaches lag behind fully\nsupervised training and are often not thought beneficial beyond obviating or\nreducing the need for annotations. We find that self-supervision can benefit\nrobustness in a variety of ways, including robustness to adversarial examples,\nlabel corruption, and common input corruptions. Additionally, self-supervision\ngreatly benefits out-of-distribution detection on difficult, near-distribution\noutliers, so much so that it exceeds the performance of fully supervised\nmethods. These results demonstrate the promise of self-supervision for\nimproving robustness and uncertainty estimation and establish these tasks as\nnew axes of evaluation for future self-supervised learning research.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 17:44:00 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 17:57:27 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Hendrycks", "Dan", ""], ["Mazeika", "Mantas", ""], ["Kadavath", "Saurav", ""], ["Song", "Dawn", ""]]}]