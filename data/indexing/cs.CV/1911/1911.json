[{"id": "1911.00025", "submitter": "Raymond A. Yeh", "authors": "Iou-Jen Liu, Raymond A. Yeh, Alexander G. Schwing", "title": "PIC: Permutation Invariant Critic for Multi-Agent Deep Reinforcement\n  Learning", "comments": "Accepted to CORL2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sample efficiency and scalability to a large number of agents are two\nimportant goals for multi-agent reinforcement learning systems. Recent works\ngot us closer to those goals, addressing non-stationarity of the environment\nfrom a single agent's perspective by utilizing a deep net critic which depends\non all observations and actions. The critic input concatenates agent\nobservations and actions in a user-specified order. However, since deep nets\naren't permutation invariant, a permuted input changes the critic output\ndespite the environment remaining identical. To avoid this inefficiency, we\npropose a 'permutation invariant critic' (PIC), which yields identical output\nirrespective of the agent permutation. This consistent representation enables\nour model to scale to 30 times more agents and to achieve improvements of test\nepisode reward between 15% to 50% on the challenging multi-agent particle\nenvironment (MPE).\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 18:04:42 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Liu", "Iou-Jen", ""], ["Yeh", "Raymond A.", ""], ["Schwing", "Alexander G.", ""]]}, {"id": "1911.00029", "submitter": "Raymond A. Yeh", "authors": "Raymond A. Yeh, Yuan-Ting Hu, Alexander G. Schwing", "title": "Chirality Nets for Human Pose Regression", "comments": "Accepted to NeurIPS2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Chirality Nets, a family of deep nets that is equivariant to the\n\"chirality transform,\" i.e., the transformation to create a chiral pair.\nThrough parameter sharing, odd and even symmetry, we propose and prove variants\nof standard building blocks of deep nets that satisfy the equivariance\nproperty, including fully connected layers, convolutional layers,\nbatch-normalization, and LSTM/GRU cells. The proposed layers lead to a more\ndata efficient representation and a reduction in computation by exploiting\nsymmetry. We evaluate chirality nets on the task of human pose regression,\nwhich naturally exploits the left/right mirroring of the human body. We study\nthree pose regression tasks: 3D pose estimation from video, 2D pose\nforecasting, and skeleton based activity recognition. Our approach\nachieves/matches state-of-the-art results, with more significant gains on small\ndatasets and limited-data settings.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 18:09:05 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Yeh", "Raymond A.", ""], ["Hu", "Yuan-Ting", ""], ["Schwing", "Alexander G.", ""]]}, {"id": "1911.00036", "submitter": "Mitko Veta", "authors": "Suzanne C Wetstein, Allison M Onken, Christina Luffman, Gabrielle M\n  Baker, Michael E Pyle, Kevin H Kensler, Ying Liu, Bart Bakker, Ruud Vlutters,\n  Marinus B van Leeuwen, Laura C Collins, Stuart J Schnitt, Josien PW Pluim,\n  Rulla M Tamimi, Yujing J Heng and Mitko Veta", "title": "Deep learning assessment of breast terminal duct lobular unit\n  involution: towards automated prediction of breast cancer risk", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0231653", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Terminal ductal lobular unit (TDLU) involution is the regression of\nmilk-producing structures in the breast. Women with less TDLU involution are\nmore likely to develop breast cancer. A major bottleneck in studying TDLU\ninvolution in large cohort studies is the need for labor-intensive manual\nassessment of TDLUs. We developed a computational pathology solution to\nautomatically capture TDLU involution measures. Whole slide images (WSIs) of\nbenign breast biopsies were obtained from the Nurses' Health Study (NHS). A\nfirst set of 92 WSIs was annotated for TDLUs, acini and adipose tissue to train\ndeep convolutional neural network (CNN) models for detection of acini, and\nsegmentation of TDLUs and adipose tissue. These networks were integrated into a\nsingle computational method to capture TDLU involution measures including\nnumber of TDLUs per tissue area, median TDLU span and median number of acini\nper TDLU. We validated our method on 40 additional WSIs by comparing with\nmanually acquired measures. Our CNN models detected acini with an F1 score of\n0.73$\\pm$0.09, and segmented TDLUs and adipose tissue with Dice scores of\n0.86$\\pm$0.11 and 0.86$\\pm$0.04, respectively. The inter-observer ICC scores\nfor manual assessments on 40 WSIs of number of TDLUs per tissue area, median\nTDLU span, and median acini count per TDLU were 0.71, 95% CI [0.51, 0.83],\n0.81, 95% CI [0.67, 0.90], and 0.73, 95% CI [0.54, 0.85], respectively.\nIntra-observer reliability was evaluated on 10/40 WSIs with ICC scores of >0.8.\nInter-observer ICC scores between automated results and the mean of the two\nobservers were: 0.80, 95% CI [0.63, 0.90] for number of TDLUs per tissue area,\n0.57, 95% CI [0.19, 0.77] for median TDLU span, and 0.80, 95% CI [0.62, 0.89]\nfor median acini count per TDLU. TDLU involution measures evaluated by manual\nand automated assessment were inversely associated with age and menopausal\nstatus.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 18:12:44 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Wetstein", "Suzanne C", ""], ["Onken", "Allison M", ""], ["Luffman", "Christina", ""], ["Baker", "Gabrielle M", ""], ["Pyle", "Michael E", ""], ["Kensler", "Kevin H", ""], ["Liu", "Ying", ""], ["Bakker", "Bart", ""], ["Vlutters", "Ruud", ""], ["van Leeuwen", "Marinus B", ""], ["Collins", "Laura C", ""], ["Schnitt", "Stuart J", ""], ["Pluim", "Josien PW", ""], ["Tamimi", "Rulla M", ""], ["Heng", "Yujing J", ""], ["Veta", "Mitko", ""]]}, {"id": "1911.00071", "submitter": "Mohammad Eslami", "authors": "Mohammad Eslami, Mahdi Karami, Sedigheh Eslami, Solale Tabarestani,\n  Farah Torkamani-Azar, Christoph Meinel", "title": "SignCol: Open-Source Software for Collecting Sign Language Gestures", "comments": "The paper is presented at ICSESS conference but the published version\n  by them on the IEEE Xplore is impaired and the quality of figures is\n  inappropriate!! This is the preprint version which had appropriate format and\n  figures", "journal-ref": null, "doi": "10.1109/ICSESS.2018.8663952", "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign(ed) languages use gestures, such as hand or head movements, for\ncommunication. Sign language recognition is an assistive technology for\nindividuals with hearing disability and its goal is to improve such\nindividuals' life quality by facilitating their social involvement. Since sign\nlanguages are vastly varied in alphabets, as known as signs, a sign recognition\nsoftware should be capable of handling eight different types of sign\ncombinations, e.g. numbers, letters, words and sentences. Due to the intrinsic\ncomplexity and diversity of symbolic gestures, recognition algorithms need a\ncomprehensive visual dataset to learn by. In this paper, we describe the design\nand implementation of a Microsoft Kinect-based open source software, called\nSignCol, for capturing and saving the gestures used in sign languages. Our work\nsupports a multi-language database and reports the recorded items statistics.\nSignCol can capture and store colored(RGB) frames, depth frames, infrared\nframes, body index frames, coordinate mapped color-body frames, skeleton\ninformation of each frame and camera parameters simultaneously.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 19:36:18 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Eslami", "Mohammad", ""], ["Karami", "Mahdi", ""], ["Eslami", "Sedigheh", ""], ["Tabarestani", "Solale", ""], ["Torkamani-Azar", "Farah", ""], ["Meinel", "Christoph", ""]]}, {"id": "1911.00077", "submitter": "Alexandros Iosifidis", "authors": "William Lund Sommer and Alexandros Iosifidis", "title": "Text-to-image synthesis method evaluation based on visual patterns", "comments": "11 pages, including supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A commonly used evaluation metric for text-to-image synthesis is the\nInception score (IS) \\cite{inceptionscore}, which has been shown to be a\nquality metric that correlates well with human judgment. However, IS does not\nreveal properties of the generated images indicating the ability of a\ntext-to-image synthesis method to correctly convey semantics of the input text\ndescriptions. In this paper, we introduce an evaluation metric and a visual\nevaluation method allowing for the simultaneous estimation of the realism,\nvariety and semantic accuracy of generated images. The proposed method uses a\npre-trained Inception network \\cite{inceptionnet} to produce high dimensional\nrepresentations for both real and generated images. These image representations\nare then visualized in a $2$-dimensional feature space defined by the\nt-distributed Stochastic Neighbor Embedding (t-SNE) \\cite{tsne}. Visual\nconcepts are determined by clustering the real image representations, and are\nsubsequently used to evaluate the similarity of the generated images to the\nreal ones by classifying them to the closest visual concept. The resulting\nclassification accuracy is shown to be a effective gauge for the semantic\naccuracy of text-to-image synthesis methods.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 19:50:42 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Sommer", "William Lund", ""], ["Iosifidis", "Alexandros", ""]]}, {"id": "1911.00104", "submitter": "Nabeel Seedat", "authors": "Nabeel Seedat and Christopher Kanan", "title": "Towards calibrated and scalable uncertainty representations for neural\n  networks", "comments": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019): 4th workshop on Bayesian Deep Learning, Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many applications it is critical to know the uncertainty of a neural\nnetwork's predictions. While a variety of neural network parameter estimation\nmethods have been proposed for uncertainty estimation, they have not been\nrigorously compared across uncertainty measures. We assess four of these\nparameter estimation methods to calibrate uncertainty estimation using four\ndifferent uncertainty measures: entropy, mutual information, aleatoric\nuncertainty and epistemic uncertainty. We evaluate the calibration of these\nparameter estimation methods using expected calibration error. Additionally, we\npropose a novel method of neural network parameter estimation called RECAST,\nwhich combines cosine annealing with warm restarts with Stochastic Gradient\nLangevin Dynamics, capturing more diverse parameter distributions. When\nbenchmarked against mutilated image data, we show that RECAST is\nwell-calibrated and when combined with predictive entropy and epistemic\nuncertainty it offers the best calibrated measure of uncertainty when compared\nto recent methods.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 02:29:55 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 01:30:14 GMT"}, {"version": "v3", "created": "Wed, 4 Dec 2019 02:19:35 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Seedat", "Nabeel", ""], ["Kanan", "Christopher", ""]]}, {"id": "1911.00127", "submitter": "Yongkai Liu", "authors": "Yongkai Liu, Guang Yang, Sohrab Afshari Mirak, Melina Hosseiny, Afshin\n  Azadikhah, Xinran Zhong, Robert E. Reiter, Yeejin Lee, Steven Raman,\n  Kyunghyun Sung", "title": "Automatic Prostate Zonal Segmentation Using Fully Convolutional Network\n  with Feature Pyramid Attention", "comments": "Has been accepted by IEEE Access", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our main objective is to develop a novel deep learning-based algorithm for\nautomatic segmentation of prostate zone and to evaluate the proposed algorithm\non an additional independent testing data in comparison with inter-reader\nconsistency between two experts. With IRB approval and HIPAA compliance, we\ndesigned a novel convolutional neural network (CNN) for automatic segmentation\nof the prostatic transition zone (TZ) and peripheral zone (PZ) on T2-weighted\n(T2w) MRI. The total study cohort included 359 patients from two sources; 313\nfrom a deidentified publicly available dataset (SPIE-AAPM-NCI PROSTATEX\nchallenge) and 46 from a large U.S. tertiary referral center with 3T MRI\n(external testing dataset (ETD)). The TZ and PZ contours were manually\nannotated by research fellows, supervised by genitourinary (GU) radiologists.\nThe model was developed using 250 patients and tested internally using the\nremaining 63 patients from the PROSTATEX (internal testing dataset (ITD)) and\ntested again (n=46) externally using the ETD. The Dice Similarity Coefficient\n(DSC) was used to evaluate the segmentation performance. DSCs for PZ and TZ\nwere 0.74 and 0.86 in the ITD respectively. In the ETD, DSCs for PZ and TZ were\n0.74 and 0.792, respectively. The inter-reader consistency (Expert 2 vs. Expert\n1) were 0.71 (PZ) and 0.75 (TZ). This novel DL algorithm enabled automatic\nsegmentation of PZ and TZ with high accuracy on both ITD and ETD without a\nperformance difference for PZ and less than 10% TZ difference. In the ETD, the\nproposed method can be comparable to experts in the segmentation of prostate\nzones.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 22:00:30 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Liu", "Yongkai", ""], ["Yang", "Guang", ""], ["Mirak", "Sohrab Afshari", ""], ["Hosseiny", "Melina", ""], ["Azadikhah", "Afshin", ""], ["Zhong", "Xinran", ""], ["Reiter", "Robert E.", ""], ["Lee", "Yeejin", ""], ["Raman", "Steven", ""], ["Sung", "Kyunghyun", ""]]}, {"id": "1911.00140", "submitter": "Hyunseok  Seo", "authors": "Hyunseok Seo, Charles Huang, Maxime Bassenne, Ruoxiu Xiao, and Lei\n  Xing", "title": "Modified U-Net (mU-Net) with Incorporation of Object-Dependent High\n  Level Features for Improved Liver and Liver-Tumor Segmentation in CT Images", "comments": "Accept for publication at IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": "10.1109/TMI.2019.2948320", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of livers and liver tumors is one of the most important steps in\nradiation therapy of hepatocellular carcinoma. The segmentation task is often\ndone manually, making it tedious, labor intensive, and subject to intra-/inter-\noperator variations. While various algorithms for delineating organ-at-risks\n(OARs) and tumor targets have been proposed, automatic segmentation of livers\nand liver tumors remains intractable due to their low tissue contrast with\nrespect to the surrounding organs and their deformable shape in CT images. The\nU-Net has gained increasing popularity recently for image analysis tasks and\nhas shown promising results. Conventional U-Net architectures, however, suffer\nfrom three major drawbacks. To cope with these problems, we added a residual\npath with deconvolution and activation operations to the skip connection of the\nU-Net to avoid duplication of low resolution information of features. In the\ncase of small object inputs, features in the skip connection are not\nincorporated with features in the residual path. Furthermore, the proposed\narchitecture has additional convolution layers in the skip connection in order\nto extract high level global features of small object inputs as well as high\nlevel features of high resolution edge information of large object inputs.\nEfficacy of the modified U-Net (mU-Net) was demonstrated using the public\ndataset of Liver tumor segmentation (LiTS) challenge 2017. The proposed mU-Net\noutperformed existing state-of-art networks.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 22:42:53 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Seo", "Hyunseok", ""], ["Huang", "Charles", ""], ["Bassenne", "Maxime", ""], ["Xiao", "Ruoxiu", ""], ["Xing", "Lei", ""]]}, {"id": "1911.00143", "submitter": "Martin Welk", "authors": "Martin Welk", "title": "Multivariate Medians for Image and Shape Analysis", "comments": "Minor corrections, one additional reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Having been studied since long by statisticians, multivariate median concepts\nfound their way into the image processing literature in the course of the last\ndecades, being used to construct robust and efficient denoising filters for\nmultivariate images such as colour images but also matrix-valued images. Based\non the similarities between image and geometric data as results of the sampling\nof continuous physical quantities, it can be expected that the understanding of\nmultivariate median filters for images provides a starting point for the\ndevelopment of shape processing techniques. This paper presents an overview of\nmultivariate median concepts relevant for image and shape processing. It\nfocusses on their mathematical principles and discusses important properties\nespecially in the context of image processing.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 22:54:31 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 16:37:57 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Welk", "Martin", ""]]}, {"id": "1911.00147", "submitter": "Chris Thomas", "authors": "Christopher Thomas and Adriana Kovashka", "title": "Predicting the Politics of an Image Using Webly Supervised Data", "comments": null, "journal-ref": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019), Vancouver, Canada", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The news media shape public opinion, and often, the visual bias they contain\nis evident for human observers. This bias can be inferred from how different\nmedia sources portray different subjects or topics. In this paper, we model\nvisual political bias in contemporary media sources at scale, using webly\nsupervised data. We collect a dataset of over one million unique images and\nassociated news articles from left- and right-leaning news sources, and develop\na method to predict the image's political leaning. This problem is particularly\nchallenging because of the enormous intra-class visual and semantic diversity\nof our data. We propose a two-stage method to tackle this problem. In the first\nstage, the model is forced to learn relevant visual concepts that, when joined\nwith document embeddings computed from articles paired with the images, enable\nthe model to predict bias. In the second stage, we remove the requirement of\nthe text domain and train a visual classifier from the features of the former\nmodel. We show this two-stage approach facilitates learning and outperforms\nseveral strong baselines. We also present extensive qualitative results\ndemonstrating the nuances of the data.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 23:11:21 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Thomas", "Christopher", ""], ["Kovashka", "Adriana", ""]]}, {"id": "1911.00155", "submitter": "Ali Ayub", "authors": "Ali Ayub, Alan R. Wagner", "title": "Centroid Based Concept Learning for RGB-D Indoor Scene Classification", "comments": "Accepted at BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contributes a novel cognitively-inspired method for RGB-D indoor\nscene classification. High intra-class variance and low inter-class variance\nmake indoor scene classification an extremely challenging task. To cope with\nthis problem, we propose a clustering approach inspired by the concept learning\nmodel of the hippocampus and the neocortex, to generate clusters and centroids\nfor different scene categories. Test images depicting different scenes are\nclassified by using their distance to the closest centroids (concepts).\nModeling of RGB-D scenes as centroids not only leads to state-of-the-art\nclassification performance on benchmark datasets (SUN RGB-D and NYU Depth V2),\nbut also offers a method for inspecting and interpreting the space of\ncentroids. Inspection of the centroids generated by our approach on RGB-D\ndatasets leads us to propose a method for merging conceptually similar\ncategories, resulting in improved accuracy for all approaches.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 00:09:37 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 00:03:56 GMT"}, {"version": "v3", "created": "Thu, 6 Aug 2020 22:25:22 GMT"}, {"version": "v4", "created": "Sat, 15 Aug 2020 01:26:48 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Ayub", "Ali", ""], ["Wagner", "Alan R.", ""]]}, {"id": "1911.00189", "submitter": "Yi-Ling Qiao", "authors": "Yi-Ling Qiao, Lin Gao, Shu-Zhi Liu, Ligang Liu, Yu-Kun Lai, Xilin Chen", "title": "Learning-based Real-time Detection of Intrinsic Reflectional Symmetry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reflectional symmetry is ubiquitous in nature. While extrinsic reflectional\nsymmetry can be easily parametrized and detected, intrinsic symmetry is much\nharder due to the high solution space. Previous works usually solve this\nproblem by voting or sampling, which suffer from high computational cost and\nrandomness. In this paper, we propose \\YL{a} learning-based approach to\nintrinsic reflectional symmetry detection. Instead of directly finding\nsymmetric point pairs, we parametrize this self-isometry using a functional map\nmatrix, which can be easily computed given the signs of Laplacian\neigenfunctions under the symmetric mapping. Therefore, we train a novel deep\nneural network to predict the sign of each eigenfunction under symmetry, which\nin addition takes the first few eigenfunctions as intrinsic features to\ncharacterize the mesh while avoiding coping with the connectivity explicitly.\nOur network aims at learning the global property of functions, and consequently\nconverts the problem defined on the manifold to the functional domain. By\ndisentangling the prediction of the matrix into separated basis, our method\ngeneralizes well to new shapes and is invariant under perturbation of\neigenfunctions. Through extensive experiments, we demonstrate the robustness of\nour method in challenging cases, including different topology and incomplete\nshapes with holes. By avoiding random sampling, our learning-based algorithm is\nover 100 times faster than state-of-the-art methods, and meanwhile, is more\nrobust, achieving higher correspondence accuracy in commonly used metrics.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 02:58:45 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Qiao", "Yi-Ling", ""], ["Gao", "Lin", ""], ["Liu", "Shu-Zhi", ""], ["Liu", "Ligang", ""], ["Lai", "Yu-Kun", ""], ["Chen", "Xilin", ""]]}, {"id": "1911.00195", "submitter": "Chen Zhao", "authors": "Chen Zhao, Jiaqi Yang, Xin Xiong, Angfan Zhu, Zhiguo Cao, Xin Li", "title": "Rotation Invariant Point Cloud Classification: Where Local Geometry\n  Meets Global Topology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud analysis is a fundamental task in 3D computer vision. Most\nprevious works have conducted experiments on synthetic datasets with\nwell-aligned data; while real-world point clouds are often not pre-aligned. How\nto achieve rotation invariance remains an open problem in point cloud analysis.\nTo meet this challenge, we propose a novel approach toward achieving\nrotation-invariant (RI) representations by combining local geometry with global\ntopology. In our local-global-representation (LGR)-Net, we have designed a\ntwo-branch network where one stream encodes local geometric RI features and the\nother encodes global topology-preserving RI features. Motivated by the\nobservation that local geometry and global topology have different yet\ncomplementary RI responses in varying regions, two-branch RI features are fused\nby an innovative multi-layer perceptron (MLP) based attention module. To the\nbest of our knowledge, this work is the first principled approach toward\nadaptively combining global and local information under the context of RI point\ncloud analysis. Extensive experiments have demonstrated that our LGR-Net\nachieves the state-of-the-art performance on various rotation-augmented\nversions of ModelNet40, ShapeNet, ScanObjectNN, and S3DIS.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 04:14:19 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 02:55:26 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 09:42:19 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Zhao", "Chen", ""], ["Yang", "Jiaqi", ""], ["Xiong", "Xin", ""], ["Zhu", "Angfan", ""], ["Cao", "Zhiguo", ""], ["Li", "Xin", ""]]}, {"id": "1911.00212", "submitter": "Siyu Huang", "authors": "Tao Jin, Siyu Huang, Yingming Li, Zhongfei Zhang", "title": "Low-Rank HOCA: Efficient High-Order Cross-Modal Attention for Video\n  Captioning", "comments": "Accepted as a long paper at EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the challenging task of video captioning which aims to\ngenerate descriptions for video data. Recently, the attention-based\nencoder-decoder structures have been widely used in video captioning. In\nexisting literature, the attention weights are often built from the information\nof an individual modality, while, the association relationships between\nmultiple modalities are neglected. Motivated by this observation, we propose a\nvideo captioning model with High-Order Cross-Modal Attention (HOCA) where the\nattention weights are calculated based on the high-order correlation tensor to\ncapture the frame-level cross-modal interaction of different modalities\nsufficiently. Furthermore, we novelly introduce Low-Rank HOCA which adopts\ntensor decomposition to reduce the extremely large space requirement of HOCA,\nleading to a practical and efficient implementation in real-world applications.\nExperimental results on two benchmark datasets, MSVD and MSR-VTT, show that\nLow-rank HOCA establishes a new state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 05:53:50 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Jin", "Tao", ""], ["Huang", "Siyu", ""], ["Li", "Yingming", ""], ["Zhang", "Zhongfei", ""]]}, {"id": "1911.00227", "submitter": "Yuma Kinoshita", "authors": "Ayana Kawamura, Yuma Kinoshita, and Hitoshi Kiya", "title": "Privacy-Preserving Machine Learning Using EtC Images", "comments": "to be presented at IWAIT2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel privacy-preserving machine learning scheme\nwith encrypted images, called EtC (Encryption-then-Compression) images. Using\nmachine learning algorithms in cloud environments has been spreading in many\nfields. However, there are serious issues with it for end users, due to\nsemi-trusted cloud providers. Accordingly, we propose using EtC images, which\nhave been proposed for EtC systems with JPEG compression. In this paper, a\nnovel property of EtC images is considered under the use of z-score\nnormalization. It is demonstrated that the use of EtC images allows us not only\nto protect visual information of images, but also to preserve both the\nEuclidean distance and the inner product between vectors. In addition,\ndimensionality reduction is shown to can be applied to EtC images for fast and\naccurate matching. In an experiment, the proposed scheme is applied to a facial\nrecognition algorithm with classifiers for confirming the effectiveness of the\nscheme under the use of support vector machine (SVM) with the kernel trick.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 06:54:27 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Kawamura", "Ayana", ""], ["Kinoshita", "Yuma", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "1911.00232", "submitter": "Mathew Monfort", "authors": "Mathew Monfort, Kandan Ramakrishnan, Alex Andonian, Barry A McNamara,\n  Alex Lascelles, Bowen Pan, Quanfu Fan, Dan Gutfreund, Rogerio Feris, Aude\n  Oliva", "title": "Multi-Moments in Time: Learning and Interpreting Models for Multi-Action\n  Video Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An event happening in the world is often made of different activities and\nactions that can unfold simultaneously or sequentially within a few seconds.\nHowever, most large-scale datasets built to train models for action recognition\nprovide a single label per video clip. Consequently, models can be incorrectly\npenalized for classifying actions that exist in the videos but are not\nexplicitly labeled and do not learn the full spectrum of information that would\nbe mandatory to more completely comprehend different events and eventually\nlearn causality between them. Towards this goal, we augmented the existing\nvideo dataset, Moments in Time (MiT), to include over two million action labels\nfor over one million three second videos. This multi-label dataset introduces\nnovel challenges on how to train and analyze models for multi-action detection.\nHere, we present baseline results for multi-action recognition using loss\nfunctions adapted for long tail multi-label learning and provide improved\nmethods for visualizing and interpreting models trained for multi-label action\ndetection.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 07:09:36 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 20:24:15 GMT"}, {"version": "v3", "created": "Fri, 10 Jan 2020 00:24:30 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Monfort", "Mathew", ""], ["Ramakrishnan", "Kandan", ""], ["Andonian", "Alex", ""], ["McNamara", "Barry A", ""], ["Lascelles", "Alex", ""], ["Pan", "Bowen", ""], ["Fan", "Quanfu", ""], ["Gutfreund", "Dan", ""], ["Feris", "Rogerio", ""], ["Oliva", "Aude", ""]]}, {"id": "1911.00282", "submitter": "Yao Zhang", "authors": "Yao Zhang, Cheng Zhong, Yang Zhang, Zhongchao Shi, Zhiqiang He", "title": "Semantic Feature Attention Network for Liver Tumor Segmentation in\n  Large-scale CT database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Liver tumor segmentation plays an important role in hepatocellular carcinoma\ndiagnosis and surgical planning. In this paper, we propose a novel Semantic\nFeature Attention Network (SFAN) for liver tumor segmentation from Computed\nTomography (CT) volumes, which exploits the impact of both low-level and\nhigh-level features. In the SFAN, a Semantic Attention Transmission (SAT)\nmodule is designed to select discriminative low-level localization details with\nthe guidance of neighboring high-level semantic information. Furthermore, a\nGlobal Context Attention (GCA) module is proposed to effectively fuse the\nmulti-level features with the guidance of global context. Our experiments are\nbased on 2 challenging databases, the public Liver Tumor Segmentation (LiTS)\nChallenge database and a large-scale in-house clinical database with 912 CT\nvolumes. Experimental results show that our proposed framework can not only\nachieve the state-of-the-art performance with the Dice per case on liver tumor\nsegmentation in LiTS database, but also outperform some widely used\nsegmentation algorithms in the large-scale clinical database.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 10:01:16 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Zhang", "Yao", ""], ["Zhong", "Cheng", ""], ["Zhang", "Yang", ""], ["Shi", "Zhongchao", ""], ["He", "Zhiqiang", ""]]}, {"id": "1911.00314", "submitter": "Josep Ramon Morros", "authors": "Ignasi Mas, Josep Ramon Morros, Veronica Vilaplana", "title": "Picking groups instead of samples: A close look at Static Pool-based\n  Meta-Active Learning", "comments": null, "journal-ref": "ICCV Workshop - MDALC 2019. Seoul, South Korea; 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active Learning techniques are used to tackle learning problems where\nobtaining training labels is costly. In this work we use Meta-Active Learning\nto learn to select a subset of samples from a pool of unsupervised input for\nfurther annotation. This scenario is called Static Pool-based Meta- Active\nLearning. We propose to extend existing approaches by performing the selection\nin a manner that, unlike previous works, can handle the selection of each\nsample based on the whole selected subset.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 12:08:47 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Mas", "Ignasi", ""], ["Morros", "Josep Ramon", ""], ["Vilaplana", "Veronica", ""]]}, {"id": "1911.00353", "submitter": "Shuming Jiao", "authors": "Shuming Jiao, Yang Gao, Jun Feng, Ting Lei, Xiaocong Yuan", "title": "Does deep learning always outperform simple linear regression in optical\n  imaging?", "comments": null, "journal-ref": null, "doi": "10.1364/OE.382319", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been extensively applied in many optical imaging\napplications in recent years. Despite the success, the limitations and\ndrawbacks of deep learning in optical imaging have been seldom investigated. In\nthis work, we show that conventional linear-regression-based methods can\noutperform the previously proposed deep learning approaches for two black-box\noptical imaging problems in some extent. Deep learning demonstrates its\nweakness especially when the number of training samples is small. The\nadvantages and disadvantages of linear-regression-based methods and deep\nlearning are analyzed and compared. Since many optical systems are essentially\nlinear, a deep learning network containing many nonlinearity functions\nsometimes may not be the most suitable option.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 17:26:49 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 10:27:57 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Jiao", "Shuming", ""], ["Gao", "Yang", ""], ["Feng", "Jun", ""], ["Lei", "Ting", ""], ["Yuan", "Xiaocong", ""]]}, {"id": "1911.00354", "submitter": "Josep Ramon Morros", "authors": "Manuel Lopez-Palma, Javier Gago, Montserrat Corbalan and Josep Ramon\n  Morros", "title": "Audience measurement using a top-view camera and oriented trajectories", "comments": null, "journal-ref": "IECON 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A crucial aspect for selecting optimal areas for commercial advertising is\nthe probability with which that publicity will be seen. This paper presents a\nmethod based on top-view camera measurement, where the probability of viewing\nis estimated according to the trajectories and movements of the head of the\npasserby individuals in the area of interest. Using a camera with a depth\nsensor, the head of the people in the range of view can be detected and\nmodeled. That method allows determining the orientation of the head which is\nused to estimate the direction of vision. A tracking by detection algorithm is\nused to compute the trajectory of each user. The attention given at each\nadvertising spot is estimated based on the trajectories and head orientations\nof the individuals in the area of interest\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 13:03:11 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Lopez-Palma", "Manuel", ""], ["Gago", "Javier", ""], ["Corbalan", "Montserrat", ""], ["Morros", "Josep Ramon", ""]]}, {"id": "1911.00357", "submitter": "Erik Wijmans", "authors": "Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa,\n  Devi Parikh, Manolis Savva, Dhruv Batra", "title": "DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion\n  Frames", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Decentralized Distributed Proximal Policy Optimization (DD-PPO), a\nmethod for distributed reinforcement learning in resource-intensive simulated\nenvironments. DD-PPO is distributed (uses multiple machines), decentralized\n(lacks a centralized server), and synchronous (no computation is ever stale),\nmaking it conceptually simple and easy to implement. In our experiments on\ntraining virtual robots to navigate in Habitat-Sim, DD-PPO exhibits near-linear\nscaling -- achieving a speedup of 107x on 128 GPUs over a serial\nimplementation. We leverage this scaling to train an agent for 2.5 Billion\nsteps of experience (the equivalent of 80 years of human experience) -- over 6\nmonths of GPU-time training in under 3 days of wall-clock time with 64 GPUs.\n  This massive-scale training not only sets the state of art on Habitat\nAutonomous Navigation Challenge 2019, but essentially solves the task\n--near-perfect autonomous navigation in an unseen environment without access to\na map, directly from an RGB-D camera and a GPS+Compass sensor. Fortuitously,\nerror vs computation exhibits a power-law-like distribution; thus, 90% of peak\nperformance is obtained relatively early (at 100 million steps) and relatively\ncheaply (under 1 day with 8 GPUs). Finally, we show that the scene\nunderstanding and navigation policies learned can be transferred to other\nnavigation tasks -- the analog of ImageNet pre-training + task-specific\nfine-tuning for embodied AI. Our model outperforms ImageNet pre-trained CNNs on\nthese transfer tasks and can serve as a universal resource (all models and code\nare publicly available).\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 13:07:37 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 04:18:58 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Wijmans", "Erik", ""], ["Kadian", "Abhishek", ""], ["Morcos", "Ari", ""], ["Lee", "Stefan", ""], ["Essa", "Irfan", ""], ["Parikh", "Devi", ""], ["Savva", "Manolis", ""], ["Batra", "Dhruv", ""]]}, {"id": "1911.00361", "submitter": "Xishan Zhang", "authors": "Xishan Zhang, Shaoli Liu, Rui Zhang, Chang Liu, Di Huang, Shiyi Zhou,\n  Jiaming Guo, Yu Kang, Qi Guo, Zidong Du, Yunji Chen", "title": "Adaptive Precision Training: Quantify Back Propagation in Neural\n  Networks with Fixed-point Numbers", "comments": "We would like to withdraw the manuscript because it lacks of\n  comparisons. The main contribution is not well verified by experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive Precision Training: Quantify Back Propagation in Neural Networks\nwith Fixed-point Numbers. Recent emerged quantization technique has been\napplied to inference of deep neural networks for fast and efficient execution.\nHowever, directly applying quantization in training can cause significant\naccuracy loss, thus remaining an open challenge.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 13:12:27 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 00:20:10 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Zhang", "Xishan", ""], ["Liu", "Shaoli", ""], ["Zhang", "Rui", ""], ["Liu", "Chang", ""], ["Huang", "Di", ""], ["Zhou", "Shiyi", ""], ["Guo", "Jiaming", ""], ["Kang", "Yu", ""], ["Guo", "Qi", ""], ["Du", "Zidong", ""], ["Chen", "Yunji", ""]]}, {"id": "1911.00364", "submitter": "Kevin Wu", "authors": "Kevin Wu, Eric Wu, Yaping Wu, Hongna Tan, Greg Sorensen, Meiyun Wang,\n  Bill Lotter", "title": "Validation of a deep learning mammography model in a population with low\n  screening rates", "comments": null, "journal-ref": "NeurIPS 2019. Fair ML for Health Workshop", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key promise of AI applications in healthcare is in increasing access to\nquality medical care in under-served populations and emerging markets. However,\ndeep learning models are often only trained on data from advantaged populations\nthat have the infrastructure and resources required for large-scale data\ncollection. In this paper, we aim to empirically investigate the potential\nimpact of such biases on breast cancer detection in mammograms. We specifically\nexplore how a deep learning algorithm trained on screening mammograms from the\nUS and UK generalizes to mammograms collected at a hospital in China, where\nscreening is not widely implemented. For the evaluation, we use a top-scoring\nmodel developed for the Digital Mammography DREAM Challenge. Despite the change\nin institution and population composition, we find that the model generalizes\nwell, exhibiting similar performance to that achieved in the DREAM Challenge,\neven when controlling for tumor size. We also illustrate a simple but effective\nmethod for filtering predictions based on model variance, which can be\nparticularly useful for deployment in new settings. While there are many\ncomponents in developing a clinically effective system, these results represent\na promising step towards increasing access to life-saving screening mammography\nin populations where screening rates are currently low.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 13:22:22 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Wu", "Kevin", ""], ["Wu", "Eric", ""], ["Wu", "Yaping", ""], ["Tan", "Hongna", ""], ["Sorensen", "Greg", ""], ["Wang", "Meiyun", ""], ["Lotter", "Bill", ""]]}, {"id": "1911.00376", "submitter": "Josep Ramon Morros", "authors": "Marc Maceira, David Varas, Josep-Ramon Morros, JavierRuiz-Hidalgo,\n  Ferran Marques", "title": "3D hierarchical optimization for Multi-view depth map coding", "comments": null, "journal-ref": "Multimedia Tools and Applications, 77(15), 2018", "doi": "10.1007/s11042-017-5409-z", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth data has a widespread use since the popularity of high-resolution 3D\nsensors. In multi-view sequences, depth information is used to supplement the\ncolor data of each view. This article proposes a joint encoding of multiple\ndepth maps with a unique representation. Color and depth images of each view\nare segmented independently and combined in an optimal Rate-Distortion fashion.\nThe resulting partitions are projected to a reference view where a coherent\nhierarchy for the multiple views is built. A Rate-Distortionoptimization is\napplied to obtain the final segmentation choosing nodes of the hierarchy. The\nconsistent segmentation is used to robustly encode depth maps of multiple views\nobtaining competitive results with HEVC coding standards. Available at:\nhttp://link.springer.com/article/10.1007/s11042-017-5409-z\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 13:45:06 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Maceira", "Marc", ""], ["Varas", "David", ""], ["Morros", "Josep-Ramon", ""], ["JavierRuiz-Hidalgo", "", ""], ["Marques", "Ferran", ""]]}, {"id": "1911.00381", "submitter": "S\\\"uleyman Aslan", "authors": "S\\\"uleyman Aslan, U\\u{g}ur G\\\"ud\\\"ukbay", "title": "Multimodal Video-based Apparent Personality Recognition Using Long\n  Short-Term Memory and Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personality computing and affective computing, where the recognition of\npersonality traits is essential, have gained increasing interest and attention\nin many research areas recently. We propose a novel approach to recognize the\nBig Five personality traits of people from videos. Personality and emotion\naffect the speaking style, facial expressions, body movements, and linguistic\nfactors in social contexts, and they are affected by environmental elements. We\ndevelop a multimodal system to recognize apparent personality based on various\nmodalities such as the face, environment, audio, and transcription features. We\nuse modality-specific neural networks that learn to recognize the traits\nindependently and we obtain a final prediction of apparent personality with a\nfeature-level fusion of these networks. We employ pre-trained deep\nconvolutional neural networks such as ResNet and VGGish networks to extract\nhigh-level features and Long Short-Term Memory networks to integrate temporal\ninformation. We train the large model consisting of modality-specific\nsubnetworks using a two-stage training process. We first train the subnetworks\nseparately and then fine-tune the overall model using these trained networks.\nWe evaluate the proposed method using ChaLearn First Impressions V2 challenge\ndataset. Our approach obtains the best overall \"mean accuracy\" score, averaged\nover five personality traits, compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 13:52:49 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Aslan", "S\u00fcleyman", ""], ["G\u00fcd\u00fckbay", "U\u011fur", ""]]}, {"id": "1911.00383", "submitter": "Homayoun Valafar", "authors": "Rishi Mukhopadhyay, Paul Shealy, Homayoun Valafar", "title": "Protein Fold Family Recognition From Unassigned Residual Dipolar\n  Coupling Data", "comments": "BioComp 2008, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite many advances in computational modeling of protein structures, these\nmethods have not been widely utilized by experimental structural biologists.\nTwo major obstacles are preventing the transition from a purely-experimental to\na purely-computational mode of protein structure determination. The first\nproblem is that most computational methods need a large library of computed\nstructures that span a large variety of protein fold families, while structural\ngenomics initiatives have slowed in their ability to provide novel protein\nfolds in recent years. The second problem is an unwillingness to trust\ncomputational models that have no experimental backing. In this paper we test a\npotential solution to these problems that we have called Probability Density\nProfile Analysis (PDPA) that utilizes unassigned residual dipolar coupling data\nthat are relatively cheap to acquire from NMR experiments.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 14:01:25 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Mukhopadhyay", "Rishi", ""], ["Shealy", "Paul", ""], ["Valafar", "Homayoun", ""]]}, {"id": "1911.00387", "submitter": "Dandan Li", "authors": "Dandan Li, Yuan Zhou, Shuwei Huo, Sun-Yuan Kung", "title": "Comb Convolution for Efficient Convolutional Architecture", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are inherently suffering from massively\nredundant computation (FLOPs) due to the dense connection pattern between\nfeature maps and convolution kernels. Recent research has investigated the\nsparse relationship between channels, however, they ignored the spatial\nrelationship within a channel. In this paper, we present a novel convolutional\noperator, namely comb convolution, to exploit the intra-channel sparse\nrelationship among neurons. The proposed convolutional operator eliminates\nnearly 50% of connections by inserting uniform mappings into standard\nconvolutions and removing about half of spatial connections in convolutional\nlayer. Notably, our work is orthogonal and complementary to existing methods\nthat reduce channel-wise redundancy. Thus, it has great potential to further\nincrease efficiency through integrating the comb convolution to existing\narchitectures. Experimental results demonstrate that by simply replacing\nstandard convolutions with comb convolutions on state-of-the-art CNN\narchitectures (e.g., VGGNets, Xception and SE-Net), we can achieve 50% FLOPs\nreduction while still maintaining the accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 14:05:08 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Li", "Dandan", ""], ["Zhou", "Yuan", ""], ["Huo", "Shuwei", ""], ["Kung", "Sun-Yuan", ""]]}, {"id": "1911.00426", "submitter": "Weihao Xia", "authors": "Weihao Xia, Yujiu Yang, Jing-Hao Xue", "title": "Cali-Sketch: Stroke Calibration and Completion for High-Quality Face\n  Image Generation from Poorly-Drawn Sketches", "comments": "10 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image generation task has received increasing attention because of its wide\napplication in security and entertainment. Sketch-based face generation brings\nmore fun and better quality of image generation due to supervised interaction.\nHowever, When a sketch poorly aligned with the true face is given as input,\nexisting supervised image-to-image translation methods often cannot generate\nacceptable photo-realistic face images. To address this problem, in this paper\nwe propose Cali-Sketch, a poorly-drawn-sketch to photo-realistic-image\ngeneration method. Cali-Sketch explicitly models stroke calibration and image\ngeneration using two constituent networks: a Stroke Calibration Network (SCN),\nwhich calibrates strokes of facial features and enriches facial details while\npreserving the original intent features; and an Image Synthesis Network (ISN),\nwhich translates the calibrated and enriched sketches to photo-realistic face\nimages. In this way, we manage to decouple a difficult cross-domain translation\nproblem into two easier steps. Extensive experiments verify that the face\nphotos generated by Cali-Sketch are both photo-realistic and faithful to the\ninput sketches, compared with state-of-the-art methods\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 15:32:42 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Xia", "Weihao", ""], ["Yang", "Yujiu", ""], ["Xue", "Jing-Hao", ""]]}, {"id": "1911.00451", "submitter": "Pierre-Alain Langlois", "authors": "Pierre-Alain Langlois, Alexandre Boulch, Renaud Marlet", "title": "Surface Reconstruction from 3D Line Segments", "comments": "In 3DV 2019 (Oral)", "journal-ref": null, "doi": "10.1109/3DV.2019.00067", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In man-made environments such as indoor scenes, when point-based 3D\nreconstruction fails due to the lack of texture, lines can still be detected\nand used to support surfaces. We present a novel method for watertight\npiecewise-planar surface reconstruction from 3D line segments with visibility\ninformation. First, planes are extracted by a novel RANSAC approach for line\nsegments that allows multiple shape support. Then, each 3D cell of a plane\narrangement is labeled full or empty based on line attachment to planes,\nvisibility and regularization. Experiments show the robustness to sparse input\ndata, noise and outliers.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 16:33:49 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Langlois", "Pierre-Alain", ""], ["Boulch", "Alexandre", ""], ["Marlet", "Renaud", ""]]}, {"id": "1911.00482", "submitter": "Hao Yan", "authors": "Nurettin Sergin, Hao Yan", "title": "High-dimensional Nonlinear Profile Monitoring based on Deep\n  Probabilistic Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wide accessibility of imaging and profile sensors in modern industrial\nsystems created an abundance of high-dimensional sensing variables. This led to\na a growing interest in the research of high-dimensional process monitoring.\nHowever, most of the approaches in the literature assume the in-control\npopulation to lie on a linear manifold with a given basis (i.e., spline,\nwavelet, kernel, etc) or an unknown basis (i.e., principal component analysis\nand its variants), which cannot be used to efficiently model profiles with a\nnonlinear manifold which is common in many real-life cases. We propose deep\nprobabilistic autoencoders as a viable unsupervised learning approach to model\nsuch manifolds. To do so, we formulate nonlinear and probabilistic extensions\nof the monitoring statistics from classical approaches as the expected\nreconstruction error (ERE) and the KL-divergence (KLD) based monitoring\nstatistics. Through extensive simulation study, we provide insights on why\nlatent-space based statistics are unreliable and why residual-space based ones\ntypically perform much better for deep learning based approaches. Finally, we\ndemonstrate the superiority of deep probabilistic models via both simulation\nstudy and a real-life case study involving images of defects from a hot steel\nrolling process.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 17:47:49 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Sergin", "Nurettin", ""], ["Yan", "Hao", ""]]}, {"id": "1911.00483", "submitter": "Sumedha Singla", "authors": "Sumedha Singla, Brian Pollack, Junxiang Chen and Kayhan Batmanghelich", "title": "Explanation by Progressive Exaggeration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As machine learning methods see greater adoption and implementation in high\nstakes applications such as medical image diagnosis, the need for model\ninterpretability and explanation has become more critical. Classical approaches\nthat assess feature importance (e.g. saliency maps) do not explain how and why\na particular region of an image is relevant to the prediction. We propose a\nmethod that explains the outcome of a classification black-box by gradually\nexaggerating the semantic effect of a given class. Given a query input to a\nclassifier, our method produces a progressive set of plausible variations of\nthat query, which gradually changes the posterior probability from its original\nclass to its negation. These counter-factually generated samples preserve\nfeatures unrelated to the classification decision, such that a user can employ\nour method as a \"tuning knob\" to traverse a data manifold while crossing the\ndecision boundary. Our method is model agnostic and only requires the output\nvalue and gradient of the predictor with respect to its input.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 17:48:24 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 21:00:40 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2020 20:32:23 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Singla", "Sumedha", ""], ["Pollack", "Brian", ""], ["Chen", "Junxiang", ""], ["Batmanghelich", "Kayhan", ""]]}, {"id": "1911.00515", "submitter": "Gustav M{\\aa}rtensson", "authors": "Gustav M{\\aa}rtensson, Daniel Ferreira, Tobias Granberg, Lena\n  Cavallin, Ketil Oppedal, Alessandro Padovani, Irena Rektorova, Laura Bonanni,\n  Matteo Pardini, Milica Kramberger, John-Paul Taylor, Jakub Hort, J\\'on\n  Sn{\\ae}dal, Jaime Kulisevsky, Frederic Blanc, Angelo Antonini, Patrizia\n  Mecocci, Bruno Vellas, Magda Tsolaki, Iwona K{\\l}oszewska, Hilkka Soininen,\n  Simon Lovestone, Andrew Simmons, Dag Aarsland, Eric Westman", "title": "The reliability of a deep learning model in clinical out-of-distribution\n  MRI data: a multicohort study", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": "10.1016/j.media.2020.101714", "report-no": null, "categories": "physics.med-ph cs.CV cs.LG eess.IV q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) methods have in recent years yielded impressive results in\nmedical imaging, with the potential to function as clinical aid to\nradiologists. However, DL models in medical imaging are often trained on public\nresearch cohorts with images acquired with a single scanner or with strict\nprotocol harmonization, which is not representative of a clinical setting. The\naim of this study was to investigate how well a DL model performs in unseen\nclinical data sets---collected with different scanners, protocols and disease\npopulations---and whether more heterogeneous training data improves\ngeneralization. In total, 3117 MRI scans of brains from multiple dementia\nresearch cohorts and memory clinics, that had been visually rated by a\nneuroradiologist according to Scheltens' scale of medial temporal atrophy\n(MTA), were included in this study. By training multiple versions of a\nconvolutional neural network on different subsets of this data to predict MTA\nratings, we assessed the impact of including images from a wider distribution\nduring training had on performance in external memory clinic data. Our results\nshowed that our model generalized well to data sets acquired with similar\nprotocols as the training data, but substantially worse in clinical cohorts\nwith visibly different tissue contrasts in the images. This implies that future\nDL studies investigating performance in out-of-distribution (OOD) MRI data need\nto assess multiple external cohorts for reliable results. Further, by including\ndata from a wider range of scanners and protocols the performance improved in\nOOD data, which suggests that more heterogeneous training data makes the model\ngeneralize better. To conclude, this is the most comprehensive study to date\ninvestigating the domain shift in deep learning on MRI data, and we advocate\nrigorous evaluation of DL models on clinical data prior to being certified for\ndeployment.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 15:52:16 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["M\u00e5rtensson", "Gustav", ""], ["Ferreira", "Daniel", ""], ["Granberg", "Tobias", ""], ["Cavallin", "Lena", ""], ["Oppedal", "Ketil", ""], ["Padovani", "Alessandro", ""], ["Rektorova", "Irena", ""], ["Bonanni", "Laura", ""], ["Pardini", "Matteo", ""], ["Kramberger", "Milica", ""], ["Taylor", "John-Paul", ""], ["Hort", "Jakub", ""], ["Sn\u00e6dal", "J\u00f3n", ""], ["Kulisevsky", "Jaime", ""], ["Blanc", "Frederic", ""], ["Antonini", "Angelo", ""], ["Mecocci", "Patrizia", ""], ["Vellas", "Bruno", ""], ["Tsolaki", "Magda", ""], ["K\u0142oszewska", "Iwona", ""], ["Soininen", "Hilkka", ""], ["Lovestone", "Simon", ""], ["Simmons", "Andrew", ""], ["Aarsland", "Dag", ""], ["Westman", "Eric", ""]]}, {"id": "1911.00526", "submitter": "Homayoun Valafar", "authors": "P. Shealy, R. Mukhopadhyay, S. Smith, and H. Valafar", "title": "Automated Assignment of Backbone Resonances Using Residual Dipolar\n  Couplings Acquired from a Protein with Known Structure", "comments": "BioComp 2008, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resonance assignment is a critical first step in the investigation of protein\nstructures using NMR spectroscopy. The development of assignment methods that\nrequire less experimental data is possible with prior knowledge of the\nmacromolecular structure. Automated methods of performing the task of resonance\nassignment can significantly reduce the financial cost and time requirement for\nprotein structure determination. Such methods can also be beneficial in\nvalidating a protein's solution state structure. Here we present a new approach\nto the assignment problem. Our approach uses only RDC data to assign backbone\nresonances. It provides simultaneous order tensor estimation and assignment.\nOur approach compares independent order tensor estimates to determine when the\ncorrect order tensor has been found. We demonstrate the algorithm's viability\nusing simulated data from the protein domain 1A1Z.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 18:01:52 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Shealy", "P.", ""], ["Mukhopadhyay", "R.", ""], ["Smith", "S.", ""], ["Valafar", "H.", ""]]}, {"id": "1911.00571", "submitter": "Ali Abdollahzadeh", "authors": "Ali Abdollahzadeh, Alejandra Sierra, Jussi Tohka", "title": "Cylindrical Shape Decomposition for 3D Segmentation of Tubular Objects", "comments": null, "journal-ref": "in IEEE Access, vol. 9, pp. 23979-23995, 2021", "doi": "10.1109/ACCESS.2021.3056958", "report-no": null, "categories": "cs.CV cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a cylindrical shape decomposition (CSD) algorithm to decompose an\nobject, a union of several tubular structures, into its semantic components. We\ndecompose the object using its curve skeleton and restricted translational\nsweeps. For that, CSD partitions the curve skeleton into maximal-length\nsub-skeletons over an orientation cost, each sub-skeleton corresponds to a\nsemantic component. To find the intersection of the tubular components, CSD\ntranslationally sweeps the object in decomposition intervals to identify\ncritical points at which the shape of the object changes substantially. CSD\ncuts the object at critical points and assigns the same label to parts along\nthe same sub-skeleton, thereby constructing a semantic component. The proposed\nmethod further reconstructs the acquired semantic components at the\nintersection of object parts using generalized cylinders. We apply CSD for\nsegmenting axons in large 3D electron microscopy images and decomposing\nvascular networks and synthetic objects. We show that our proposal is robust to\nsevere surface noise and outperforms state-of-the-art decomposition techniques\nin its applications.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 19:57:18 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 15:00:51 GMT"}, {"version": "v3", "created": "Thu, 11 Feb 2021 15:33:33 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Abdollahzadeh", "Ali", ""], ["Sierra", "Alejandra", ""], ["Tohka", "Jussi", ""]]}, {"id": "1911.00582", "submitter": "Zhipeng Ding", "authors": "Zhipeng Ding, Xu Han, Marc Niethammer", "title": "VoteNet+ : An Improved Deep Learning Label Fusion Method for Multi-atlas\n  Segmentation", "comments": "Accepted by ISBI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we improve the performance of multi-atlas segmentation (MAS) by\nintegrating the recently proposed VoteNet model with the joint label fusion\n(JLF) approach. Specifically, we first illustrate that using a deep\nconvolutional neural network to predict atlas probabilities can better\ndistinguish correct atlas labels from incorrect ones than relying on image\nintensity difference as is typical in JLF. Motivated by this finding, we\npropose VoteNet+, an improved deep network to locally predict the probability\nof an atlas label to differs from the label of the target image. Furthermore,\nwe show that JLF is more suitable for the VoteNet framework as a label fusion\nmethod than plurality voting. Lastly, we use Platt scaling to calibrate the\nprobabilities of our new model. Results on LPBA40 3D MR brain images show that\nour proposed method can achieve better performance than VoteNet.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 20:33:00 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 19:45:13 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Ding", "Zhipeng", ""], ["Han", "Xu", ""], ["Niethammer", "Marc", ""]]}, {"id": "1911.00622", "submitter": "Weihao Xia", "authors": "Weihao Xia, Yujiu Yang, Jing-Hao Xue", "title": "Unsupervised Multi-Domain Multimodal Image-to-Image Translation with\n  Explicit Domain-Constrained Disentanglement", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image translation has drawn great attention during the past few\nyears. It aims to translate an image in one domain to a given reference image\nin another domain. Due to its effectiveness and efficiency, many applications\ncan be formulated as image-to-image translation problems. However, three main\nchallenges remain in image-to-image translation: 1) the lack of large amounts\nof aligned training pairs for different tasks; 2) the ambiguity of multiple\npossible outputs from a single input image; and 3) the lack of simultaneous\ntraining of multiple datasets from different domains within a single network.\nWe also found in experiments that the implicit disentanglement of content and\nstyle could lead to unexpect results. In this paper, we propose a unified\nframework for learning to generate diverse outputs using unpaired training data\nand allow simultaneous training of multiple datasets from different domains via\na single network. Furthermore, we also investigate how to better extract domain\nsupervision information so as to learn better disentangled representations and\nachieve better image translation. Experiments show that the proposed method\noutperforms or is comparable with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 01:09:18 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Xia", "Weihao", ""], ["Yang", "Yujiu", ""], ["Xue", "Jing-Hao", ""]]}, {"id": "1911.00625", "submitter": "Hui Xue PhD", "authors": "Hui Xue, Rhodri Davies, Louis AE Brown, Kristopher D Knott, Tushar\n  Kotecha, Marianna Fontana, Sven Plein, James C Moon, Peter Kellman", "title": "Automated Inline Analysis of Myocardial Perfusion MRI with Deep Learning", "comments": "This work has been submitted to Radiology: Artificial Intelligence\n  for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent development of quantitative myocardial blood flow (MBF) mapping allows\ndirect evaluation of absolute myocardial perfusion, by computing pixel-wise\nflow maps. Clinical studies suggest quantitative evaluation would be more\ndesirable for objectivity and efficiency. Objective assessment can be further\nfacilitated by segmenting the myocardium and automatically generating reports\nfollowing the AHA model. This will free user interaction for analysis and lead\nto a 'one-click' solution to improve workflow. This paper proposes a deep\nneural network based computational workflow for inline myocardial perfusion\nanalysis. Adenosine stress and rest perfusion scans were acquired from three\nhospitals. Training set included N=1,825 perfusion series from 1,034 patients.\nIndependent test set included 200 scans from 105 patients. Data were\nconsecutively acquired at each site. A convolution neural net (CNN) model was\ntrained to provide segmentation for LV cavity, myocardium and right ventricular\nby processing incoming 2D+T perfusion Gd series. Model outputs were compared to\nmanual ground-truth for accuracy of segmentation and flow measures derived on\nglobal and per-sector basis. The trained models were integrated onto MR\nscanners for effective inference. Segmentation accuracy and myocardial flow\nmeasures were compared between CNN models and manual ground-truth. The mean\nDice ratio of CNN derived myocardium was 0.93 +/- 0.04. Both global flow and\nper-sector values showed no significant difference, compared to manual results.\nThe AHA 16 segment model was automatically generated and reported on the MR\nscanner. As a result, the fully automated analysis of perfusion flow mapping\nwas achieved. This solution was integrated on the MR scanner, enabling\n'one-click' analysis and reporting of myocardial blood flow.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 01:33:56 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 14:22:58 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Xue", "Hui", ""], ["Davies", "Rhodri", ""], ["Brown", "Louis AE", ""], ["Knott", "Kristopher D", ""], ["Kotecha", "Tushar", ""], ["Fontana", "Marianna", ""], ["Plein", "Sven", ""], ["Moon", "James C", ""], ["Kellman", "Peter", ""]]}, {"id": "1911.00627", "submitter": "Xiangyu Xu", "authors": "Xiangyu Xu, Li Siyao, Wenxiu Sun, Qian Yin, Ming-Hsuan Yang", "title": "Quadratic video interpolation", "comments": "NeurIPS 2019, project website:\n  https://sites.google.com/view/xiangyuxu/qvi_nips19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video interpolation is an important problem in computer vision, which helps\novercome the temporal limitation of camera sensors. Existing video\ninterpolation methods usually assume uniform motion between consecutive frames\nand use linear models for interpolation, which cannot well approximate the\ncomplex motion in the real world. To address these issues, we propose a\nquadratic video interpolation method which exploits the acceleration\ninformation in videos. This method allows prediction with curvilinear\ntrajectory and variable velocity, and generates more accurate interpolation\nresults. For high-quality frame synthesis, we develop a flow reversal layer to\nestimate flow fields starting from the unknown target frame to the source\nframe. In addition, we present techniques for flow refinement. Extensive\nexperiments demonstrate that our approach performs favorably against the\nexisting linear models on a wide variety of video datasets.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 02:23:33 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Xu", "Xiangyu", ""], ["Siyao", "Li", ""], ["Sun", "Wenxiu", ""], ["Yin", "Qian", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1911.00632", "submitter": "Jia Liu", "authors": "Jia Liu, Quan Zhou, Yong Qiang, Bin Kang, Xiaofu Wu, and Baoyu Zheng", "title": "FDDWNet: A Lightweight Convolutional Neural Network for Real-time\n  Sementic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a lightweight convolutional neural network, called\nFDDWNet, for real-time accurate semantic segmentation. In contrast to recent\nadvances of lightweight networks that prefer to utilize shallow structure,\nFDDWNet makes an effort to design more deeper network architecture, while\nmaintains faster inference speed and higher segmentation accuracy. Our network\nuses factorized dilated depth-wise separable convolutions (FDDWC) to learn\nfeature representations from different scale receptive fields with fewer model\nparameters. Additionally, FDDWNet has multiple branches of skipped connections\nto gather context cues from intermediate convolution layers. The experiments\nshow that FDDWNet only has 0.8M model size, while achieves 60 FPS running speed\non a single RTX 2080Ti GPU with a 1024x512 input image. The comprehensive\nexperiments demonstrate that our model achieves state-of-the-art results in\nterms of available speed and accuracy trade-off on CityScapes and CamVid\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 02:46:54 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 04:59:57 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Liu", "Jia", ""], ["Zhou", "Quan", ""], ["Qiang", "Yong", ""], ["Kang", "Bin", ""], ["Wu", "Xiaofu", ""], ["Zheng", "Baoyu", ""]]}, {"id": "1911.00652", "submitter": "Xinyuan Yu", "authors": "Jiaxiong Qiu, Xinyuan Yu, Guoqiang Yang and Shuaicheng Liu", "title": "DeepBlindness: Fast Blindness Map Estimation and Blindness Type\n  Classification for Outdoor Scene from Single Color Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outdoor vision robotic systems and autonomous cars suffer from many\nimage-quality issues, particularly haze, defocus blur, and motion blur, which\nwe will define generically as \"blindness issues\". These blindness issues may\nseriously affect the performance of robotic systems and could lead to unsafe\ndecisions being made. However, existing solutions either focus on one type of\nblindness only or lack the ability to estimate the degree of blindness\naccurately. Besides, heavy computation is needed so that these solutions cannot\nrun in real-time on practical systems. In this paper, we provide a method which\ncould simultaneously detect the type of blindness and provide a blindness map\nindicating to what degree the vision is limited on a pixel-by-pixel basis. Both\nthe blindness type and the estimate of per-pixel blindness are essential for\ntasks like deblur, dehaze, or the fail-safe functioning of robotic systems. We\ndemonstrate the effectiveness of our approach on the KITTI and CUHK datasets\nwhere experiments show that our method outperforms other state-of-the-art\napproaches, achieving speeds of about 130 frames per second (fps).\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 05:04:46 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Qiu", "Jiaxiong", ""], ["Yu", "Xinyuan", ""], ["Yang", "Guoqiang", ""], ["Liu", "Shuaicheng", ""]]}, {"id": "1911.00655", "submitter": "Rong Huang", "authors": "Rong Huang, Fuming Fang, Huy H. Nguyen, Junichi Yamagishi, Isao\n  Echizen", "title": "A Method for Identifying Origin of Digital Images Using a Convolution\n  Neural Network", "comments": "Submitted to ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of deep learning techniques has created new challenges\nin identifying the origin of digital images because generative adversarial\nnetworks and variational autoencoders can create plausible digital images whose\ncontents are not present in natural scenes. In this paper, we consider the\norigin that can be broken down into three categories: natural photographic\nimage (NPI), computer generated graphic (CGG), and deep network generated image\n(DGI). A method is presented for effectively identifying the origin of digital\nimages that is based on a convolutional neural network (CNN) and uses a\nlocal-to-global framework to reduce training complexity. By feeding labeled\ndata, the CNN is trained to predict the origin of local patches cropped from an\nimage. The origin of the full-size image is then determined by majority voting.\nUnlike previous forensic methods, the CNN takes the raw pixels as input without\nthe aid of \"residual map\". Experimental results revealed that not only the\nhigh-frequency components but also the middle-frequency ones contribute to\norigin identification. The proposed method achieved up to 95.21% identification\naccuracy and behaved robustly against several common post-processing operations\nincluding JPEG compression, scaling, geometric transformation, and contrast\nstretching. The quantitative results demonstrate that the proposed method is\nmore effective than handcrafted feature-based methods.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 05:25:19 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Huang", "Rong", ""], ["Fang", "Fuming", ""], ["Nguyen", "Huy H.", ""], ["Yamagishi", "Junichi", ""], ["Echizen", "Isao", ""]]}, {"id": "1911.00660", "submitter": "Rong Huang", "authors": "Rong Huang, Fuming Fang, Huy H. Nguyen, Junichi Yamagishi, Isao\n  Echizen", "title": "Security of Facial Forensics Models Against Adversarial Attacks", "comments": "Accepted by ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been used in digital forensics to identify\nfake facial images. We investigated several DNN-based forgery forensics models\n(FFMs) to examine whether they are secure against adversarial attacks. We\nexperimentally demonstrated the existence of individual adversarial\nperturbations (IAPs) and universal adversarial perturbations (UAPs) that can\nlead a well-performed FFM to misbehave. Based on iterative procedure, gradient\ninformation is used to generate two kinds of IAPs that can be used to fabricate\nclassification and segmentation outputs. In contrast, UAPs are generated on the\nbasis of over-firing. We designed a new objective function that encourages\nneurons to over-fire, which makes UAP generation feasible even without using\ntraining data. Experiments demonstrated the transferability of UAPs across\nunseen datasets and unseen FFMs. Moreover, we conducted subjective assessment\nfor imperceptibility of the adversarial perturbations, revealing that the\ncrafted UAPs are visually negligible. These findings provide a baseline for\nevaluating the adversarial security of FFMs.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 06:03:51 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 12:56:34 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Huang", "Rong", ""], ["Fang", "Fuming", ""], ["Nguyen", "Huy H.", ""], ["Yamagishi", "Junichi", ""], ["Echizen", "Isao", ""]]}, {"id": "1911.00666", "submitter": "Hui Li", "authors": "Hui Li, Jimin Xiao, Mingjie Sun, Eng Gee Lim, Yao Zhao", "title": "Progressive Sample Mining and Representation Learning for One-Shot\n  Person Re-identification with Adversarial Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to tackle the one-shot person re-identification problem\nwhere only one image is labelled for each person, while other images are\nunlabelled. This task is challenging due to the lack of sufficient labelled\ntraining data. To tackle this problem, we propose to iteratively guess pseudo\nlabels for the unlabeled image samples, which are later used to update the\nre-identification model together with the labelled samples. A new sampling\nmechanism is designed to select unlabeled samples to pseudo labelled samples\nbased on the distance matrix, and to form a training triplet batch including\nboth labelled samples and pseudo labelled samples. We also design an\nHSoften-Triplet-Loss to soften the negative impact of the incorrect pseudo\nlabel, considering the unreliable nature of pseudo labelled samples. Finally,\nwe deploy an adversarial learning method to expand the image samples to\ndifferent camera views. Our experiments show that our framework achieves a new\nstate-of-the-art one-shot Re-ID performance on Market-1501 (mAP 42.7%) and\nDukeMTMC-Reid dataset (mAP 40.3%). Code will be available soon.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 06:44:58 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Li", "Hui", ""], ["Xiao", "Jimin", ""], ["Sun", "Mingjie", ""], ["Lim", "Eng Gee", ""], ["Zhao", "Yao", ""]]}, {"id": "1911.00673", "submitter": "Weihao Xia", "authors": "Weihao Xia, Yujiu Yang, Jing-Hao Xue, Jing Xiao", "title": "Domain Fingerprints for No-reference Image Quality Assessment", "comments": "accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human fingerprints are detailed and nearly unique markers of human identity.\nSuch a unique and stable fingerprint is also left on each acquired image. It\ncan reveal how an image was degraded during the image acquisition procedure and\nthus is closely related to the quality of an image. In this work, we propose a\nnew no-reference image quality assessment (NR-IQA) approach called domain-aware\nIQA (DA-IQA), which for the first time introduces the concept of domain\nfingerprint to the NR-IQA field. The domain fingerprint of an image is learned\nfrom image collections of different degradations and then used as the unique\ncharacteristics to identify the degradation sources and assess the quality of\nthe image. To this end, we design a new domain-aware architecture, which\nenables simultaneous determination of both the distortion sources and the\nquality of an image. With the distortion in an image better characterized, the\nimage quality can be more accurately assessed, as verified by extensive\nexperiments, which show that the proposed DA-IQA performs better than almost\nall the compared state-of-the-art NR-IQA methods.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 07:45:12 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2020 11:56:45 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2020 13:37:24 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Xia", "Weihao", ""], ["Yang", "Yujiu", ""], ["Xue", "Jing-Hao", ""], ["Xiao", "Jing", ""]]}, {"id": "1911.00674", "submitter": "Zhibin Liao", "authors": "Zhibin Liao, Hany Girgis, Amir Abdi, Hooman Vaseli, Jorden\n  Hetherington, Robert Rohling, Ken Gin, Teresa Tsang, Purang Abolmaesumi", "title": "On Modelling Label Uncertainty in Deep Neural Networks: Automatic\n  Estimation of Intra-observer Variability in 2D Echocardiography Quality\n  Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty of labels in clinical data resulting from intra-observer\nvariability can have direct impact on the reliability of assessments made by\ndeep neural networks. In this paper, we propose a method for modelling such\nuncertainty in the context of 2D echocardiography (echo), which is a routine\nprocedure for detecting cardiovascular disease at point-of-care. Echo imaging\nquality and acquisition time is highly dependent on the operator's experience\nlevel. Recent developments have shown the possibility of automating echo image\nquality quantification by mapping an expert's assessment of quality to the echo\nimage via deep learning techniques. Nevertheless, the observer variability in\nthe expert's assessment can impact the quality quantification accuracy. Here,\nwe aim to model the intra-observer variability in echo quality assessment as an\naleatoric uncertainty modelling regression problem with the introduction of a\nnovel method that handles the regression problem with categorical labels. A key\nfeature of our design is that only a single forward pass is sufficient to\nestimate the level of uncertainty for the network output. Compared to the $0.11\n\\pm 0.09$ absolute error (in a scale from 0 to 1) archived by the conventional\nregression method, the proposed method brings the error down to $0.09 \\pm\n0.08$, where the improvement is statistically significant and equivalents to\n$5.7\\%$ test accuracy improvement. The simplicity of the proposed approach\nmeans that it could be generalized to other applications of deep learning in\nmedical imaging, where there is often uncertainty in clinical labels.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 07:51:05 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Liao", "Zhibin", ""], ["Girgis", "Hany", ""], ["Abdi", "Amir", ""], ["Vaseli", "Hooman", ""], ["Hetherington", "Jorden", ""], ["Rohling", "Robert", ""], ["Gin", "Ken", ""], ["Tsang", "Teresa", ""], ["Abolmaesumi", "Purang", ""]]}, {"id": "1911.00678", "submitter": "Beril Sirmacek", "authors": "Gerrit Brugman, Beril Sirmacek", "title": "3D tissue reconstruction with Kinect to evaluate neck lymphedema", "comments": "13 pages, original work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lymphedema is a condition of localized tissue swelling caused by a damaged\nlymphatic system. Therapy to these tissues is applied manually. Some of the\nmethods are lymph drainage, compression therapy or bandaging. However, the\ntherapy methods are still insufficiently evaluated. Especially, because of not\nhaving a reliable method to measure the change of such a soft and flexible\ntissue. In this research, our goal has been providing a 3d computer vision\nbased method to measure the changes of the neck tissues. To do so, we used\nKinect as a depth sensor and built our algorithms for the point cloud data\nacquired from this sensor. The resulting 3D models of the patient necks are\nused for comparing the models in time and measuring the volumetric changes\naccurately. Our discussions with the medical doctors validate that, when used\nin practice this approach would be able to give better indication on which\ntherapy method is helping and how the tissue is changing in time.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 08:37:14 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Brugman", "Gerrit", ""], ["Sirmacek", "Beril", ""]]}, {"id": "1911.00679", "submitter": "Weihao Xia", "authors": "Weihao Xia, Zhanglin Cheng, Yujiu Yang, Jing-Hao Xue", "title": "Cooperative Semantic Segmentation and Image Restoration in Adverse\n  Environmental Conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state-of-the-art semantic segmentation approaches only achieve high\naccuracy in good conditions. In practically-common but less-discussed adverse\nenvironmental conditions, their performance can decrease enormously. Existing\nstudies usually cast the handling of segmentation in adverse conditions as a\nseparate post-processing step after signal restoration, making the segmentation\nperformance largely depend on the quality of restoration. In this paper, we\npropose a novel deep-learning framework to tackle semantic segmentation and\nimage restoration in adverse environmental conditions in a holistic manner. The\nproposed approach contains two components: Semantically-Guided Adaptation,\nwhich exploits semantic information from degraded images to refine the\nsegmentation; and Exemplar-Guided Synthesis, which restores images from\nsemantic label maps given degraded exemplars as the guidance. Our method\ncooperatively leverages the complementarity and interdependence of low-level\nrestoration and high-level segmentation in adverse environmental conditions.\nExtensive experiments on various datasets demonstrate that our approach can not\nonly improve the accuracy of semantic segmentation with degradation cues, but\nalso boost the perceptual quality and structural similarity of image\nrestoration with semantic guidance.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 08:39:52 GMT"}, {"version": "v2", "created": "Sun, 24 Nov 2019 05:09:46 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 00:45:01 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Xia", "Weihao", ""], ["Cheng", "Zhanglin", ""], ["Yang", "Yujiu", ""], ["Xue", "Jing-Hao", ""]]}, {"id": "1911.00686", "submitter": "Janis Keuper", "authors": "Ricard Durall, Margret Keuper, Franz-Josef Pfreundt, Janis Keuper", "title": "Unmasking DeepFakes with simple Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models have recently achieved impressive results for many\nreal-world applications, successfully generating high-resolution and diverse\nsamples from complex datasets. Due to this improvement, fake digital contents\nhave proliferated growing concern and spreading distrust in image content,\nleading to an urgent need for automated ways to detect these AI-generated fake\nimages.\n  Despite the fact that many face editing algorithms seem to produce realistic\nhuman faces, upon closer examination, they do exhibit artifacts in certain\ndomains which are often hidden to the naked eye. In this work, we present a\nsimple way to detect such fake face images - so-called DeepFakes. Our method is\nbased on a classical frequency domain analysis followed by basic classifier.\nCompared to previous systems, which need to be fed with large amounts of\nlabeled data, our approach showed very good results using only a few annotated\ntraining samples and even achieved good accuracies in fully unsupervised\nscenarios. For the evaluation on high resolution face images, we combined\nseveral public datasets of real and fake faces into a new benchmark: Faces-HQ.\nGiven such high-resolution images, our approach reaches a perfect\nclassification accuracy of 100% when it is trained on as little as 20 annotated\nsamples. In a second experiment, in the evaluation of the medium-resolution\nimages of the CelebA dataset, our method achieves 100% accuracy supervised and\n96% in an unsupervised setting. Finally, evaluating a low-resolution video\nsequences of the FaceForensics++ dataset, our method achieves 91% accuracy\ndetecting manipulated videos.\n  Source Code: https://github.com/cc-hpc-itwm/DeepFakeDetection\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 09:42:25 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 08:24:41 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2020 13:51:41 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Durall", "Ricard", ""], ["Keuper", "Margret", ""], ["Pfreundt", "Franz-Josef", ""], ["Keuper", "Janis", ""]]}, {"id": "1911.00689", "submitter": "Cyprien Ruffino", "authors": "Cyprien Ruffino and Romain H\\'erault and Eric Laloy and Gilles Gasso", "title": "Pixel-wise Conditioning of Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have proven successful for\nunsupervised image generation. Several works extended GANs to image inpainting\nby conditioning the generation with parts of the image one wants to\nreconstruct. However, these methods have limitations in settings where only a\nsmall subset of the image pixels is known beforehand. In this paper, we study\nthe effectiveness of conditioning GANs by adding an explicit regularization\nterm to enforce pixel-wise conditions when very few pixel values are provided.\nIn addition, we also investigate the influence of this regularization term on\nthe quality of the generated images and the satisfaction of the conditions.\nConducted experiments on MNIST and FashionMNIST show evidence that this\nregularization term allows for controlling the trade-off between quality of the\ngenerated images and constraint satisfaction.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 10:13:39 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Ruffino", "Cyprien", ""], ["H\u00e9rault", "Romain", ""], ["Laloy", "Eric", ""], ["Gasso", "Gilles", ""]]}, {"id": "1911.00694", "submitter": "Song Yan", "authors": "Song Yan and Johan Wirta and Joni-Kristian K\\\"am\\\"ar\\\"ainen", "title": "Anthropometric clothing measurements from 3D body scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a full processing pipeline to acquire anthropometric measurements\nfrom 3D measurements. The first stage of our pipeline is a commercial point\ncloud scanner. In the second stage, a pre-defined body model is fitted to the\ncaptured point cloud. We have generated one male and one female model from the\nSMPL library. The fitting process is based on non-rigid Iterative Closest Point\n(ICP) algorithm that minimizes overall energy of point distance and local\nstiffness energy terms. In the third stage, we measure multiple circumference\npaths on the fitted model surface and use a non-linear regressor to provide the\nfinal estimates of anthropometric measurements. We scanned 194 male and 181\nfemale subjects and the proposed pipeline provides mean absolute errors from\n2.5 mm to 16.0 mm depending on the anthropometric measurement.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 10:55:45 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Yan", "Song", ""], ["Wirta", "Johan", ""], ["K\u00e4m\u00e4r\u00e4inen", "Joni-Kristian", ""]]}, {"id": "1911.00713", "submitter": "Hao Zhou", "authors": "Hao Zhou, Chongyang Zhang, Chuanping Hu", "title": "Visual Relationship Detection with Relative Location Mining", "comments": "Accepted to ACM MM 2019", "journal-ref": null, "doi": "10.1145/3343031.3351024", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual relationship detection, as a challenging task used to find and\ndistinguish the interactions between object pairs in one image, has received\nmuch attention recently. In this work, we propose a novel visual relationship\ndetection framework by deeply mining and utilizing relative location of\nobject-pair in every stage of the procedure. In both the stages, relative\nlocation information of each object-pair is abstracted and encoded as auxiliary\nfeature to improve the distinguishing capability of object-pairs proposing and\npredicate recognition, respectively; Moreover, one Gated Graph Neural\nNetwork(GGNN) is introduced to mine and measure the relevance of predicates\nusing relative location. With the location-based GGNN, those non-exclusive\npredicates with similar spatial position can be clustered firstly and then be\nsmoothed with close classification scores, thus the accuracy of top $n$ recall\ncan be increased further. Experiments on two widely used datasets VRD and VG\nshow that, with the deeply mining and exploiting of relative location\ninformation, our proposed model significantly outperforms the current\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 13:33:15 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Zhou", "Hao", ""], ["Zhang", "Chongyang", ""], ["Hu", "Chuanping", ""]]}, {"id": "1911.00735", "submitter": "ShahRukh Athar", "authors": "ShahRukh Athar, Zhixin Shu, Dimitris Samaras", "title": "Self-supervised Deformation Modeling for Facial Expression Editing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep generative models have demonstrated impressive\nresults in photo-realistic facial image synthesis and editing. Facial\nexpressions are inherently the result of muscle movement. However, existing\nneural network-based approaches usually only rely on texture generation to edit\nexpressions and largely neglect the motion information. In this work, we\npropose a novel end-to-end network that disentangles the task of facial editing\ninto two steps: a \" \"motion-editing\" step and a \"texture-editing\" step. In the\n\"motion-editing\" step, we explicitly model facial movement through image\ndeformation, warping the image into the desired expression. In the\n\"texture-editing\" step, we generate necessary textures, such as teeth and\nshading effects, for a photo-realistic result. Our physically-based\ntask-disentanglement system design allows each step to learn a focused task,\nremoving the need of generating texture to hallucinate motion. Our system is\ntrained in a self-supervised manner, requiring no ground truth deformation\nannotation. Using Action Units [8] as the representation for facial expression,\nour method improves the state-of-the-art facial expression editing performance\nin both qualitative and quantitative evaluations.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 15:22:17 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 21:47:33 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Athar", "ShahRukh", ""], ["Shu", "Zhixin", ""], ["Samaras", "Dimitris", ""]]}, {"id": "1911.00764", "submitter": "Mark Weber", "authors": "Mark Weber, Jonathon Luiten, Bastian Leibe", "title": "Single-Shot Panoptic Segmentation", "comments": "Accepted to IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel end-to-end single-shot method that segments countable\nobject instances (things) as well as background regions (stuff) into a\nnon-overlapping panoptic segmentation at almost video frame rate. Current\nstate-of-the-art methods are far from reaching video frame rate and mostly rely\non merging instance segmentation with semantic background segmentation, making\nthem impractical to use in many applications such as robotics. Our approach\nrelaxes this requirement by using an object detector but is still able to\nresolve inter- and intra-class overlaps to achieve a non-overlapping\nsegmentation. On top of a shared encoder-decoder backbone, we utilize multiple\nbranches for semantic segmentation, object detection, and instance center\nprediction. Finally, our panoptic head combines all outputs into a panoptic\nsegmentation and can even handle conflicting predictions between branches as\nwell as certain false predictions. Our network achieves 32.6% PQ on MS-COCO at\n23.5 FPS, opening up panoptic segmentation to a broader field of applications.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 18:41:10 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 13:54:04 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Weber", "Mark", ""], ["Luiten", "Jonathon", ""], ["Leibe", "Bastian", ""]]}, {"id": "1911.00767", "submitter": "Shunsuke Saito", "authors": "Shichen Liu, Shunsuke Saito, Weikai Chen, Hao Li", "title": "Learning to Infer Implicit Surfaces without 3D Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in 3D deep learning have shown that it is possible to train\nhighly effective deep models for 3D shape generation, directly from 2D images.\nThis is particularly interesting since the availability of 3D models is still\nlimited compared to the massive amount of accessible 2D images, which is\ninvaluable for training. The representation of 3D surfaces itself is a key\nfactor for the quality and resolution of the 3D output. While explicit\nrepresentations, such as point clouds and voxels, can span a wide range of\nshape variations, their resolutions are often limited. Mesh-based\nrepresentations are more efficient but are limited by their ability to handle\nvarying topologies. Implicit surfaces, however, can robustly handle complex\nshapes, topologies, and also provide flexible resolution control. We address\nthe fundamental problem of learning implicit surfaces for shape inference\nwithout the need of 3D supervision. Despite their advantages, it remains\nnontrivial to (1) formulate a differentiable connection between implicit\nsurfaces and their 2D renderings, which is needed for image-based supervision;\nand (2) ensure precise geometric properties and control, such as local\nsmoothness. In particular, sampling implicit surfaces densely is also known to\nbe a computationally demanding and very slow operation. To this end, we propose\na novel ray-based field probing technique for efficient image-to-field\nsupervision, as well as a general geometric regularizer for implicit surfaces,\nwhich provides natural shape priors in unconstrained regions. We demonstrate\nthe effectiveness of our framework on the task of single-view image-based 3D\nshape digitization and show how we outperform state-of-the-art techniques both\nquantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 19:05:23 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Liu", "Shichen", ""], ["Saito", "Shunsuke", ""], ["Chen", "Weikai", ""], ["Li", "Hao", ""]]}, {"id": "1911.00772", "submitter": "Shadrokh Samavi", "authors": "Maedeh Jamali, Mahnoosh Bagheri, Nader Karimi, Shadrokh Samavi", "title": "Robustness and Imperceptibility Enhancement in Watermarked Images by\n  Color Transformation", "comments": "5 pages 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the effective methods for the preservation of copyright ownership of\ndigital media is watermarking. Different watermarking techniques try to set a\ntradeoff between robustness and transparency of the process. In this research\nwork, we have used color space conversion and frequency transform to achieve\nhigh robustness and transparency. Due to the distribution of image information\nin the RGB domain, we use the YUV color space, which concentrates the visual\ninformation in the Y channel. Embedding of the watermark is performed in the\nDCT coefficients of the specific wavelet subbands. Experimental results show\nhigh transparency and robustness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 19:19:24 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Jamali", "Maedeh", ""], ["Bagheri", "Mahnoosh", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "1911.00792", "submitter": "Franz Heinsen", "authors": "Franz A. Heinsen", "title": "An Algorithm for Routing Capsules in All Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building on recent work on capsule networks, we propose a new,\ngeneral-purpose form of \"routing by agreement\" that activates output capsules\nin a layer as a function of their net benefit to use and net cost to ignore\ninput capsules from earlier layers. To illustrate the usefulness of our routing\nalgorithm, we present two capsule networks that apply it in different domains:\nvision and language. The first network achieves new state-of-the-art accuracy\nof 99.1% on the smallNORB visual recognition task with fewer parameters and an\norder of magnitude less training than previous capsule models, and we find\nevidence that it learns to perform a form of \"reverse graphics.\" The second\nnetwork achieves new state-of-the-art accuracies on the root sentences of the\nStanford Sentiment Treebank: 58.5% on fine-grained and 95.6% on binary labels\nwith a single-task model that routes frozen embeddings from a pretrained\ntransformer as capsules. In both domains, we train with the same regime. Code\nis available at https://github.com/glassroom/heinsen_routing along with\nreplication instructions.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 22:13:18 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 16:42:42 GMT"}, {"version": "v3", "created": "Fri, 8 Nov 2019 02:02:32 GMT"}, {"version": "v4", "created": "Sun, 8 Dec 2019 17:20:13 GMT"}, {"version": "v5", "created": "Sun, 15 Dec 2019 18:49:59 GMT"}, {"version": "v6", "created": "Fri, 28 Feb 2020 16:57:39 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Heinsen", "Franz A.", ""]]}, {"id": "1911.00796", "submitter": "Guoqiang Yu", "authors": "Congchao Wang, Yizhi Wang, and Guoqiang Yu", "title": "Efficient Global Multi-object Tracking Under Minimum-cost Circulation\n  Framework", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed a minimum-cost circulation framework for solving the global data\nassociation problem, which plays a key role in the tracking-by-detection\nparadigm of multi-object tracking. The global data association problem was\nextensively studied under the minimum-cost flow framework, which is\ntheoretically attractive as being flexible and globally solvable. However, the\nhigh computational burden has been a long-standing obstacle to its wide\nadoption in practice. While enjoying the same theoretical advantages and\nmaintaining the same optimal solution as the minimum-cost flow framework, our\nnew framework has a better theoretical complexity bound and leads to orders of\npractical efficiency improvement. This new framework is motivated by the\nobservation that minimum-cost flow only partially models the data association\nproblem and must be accompanied by an additional and time-consuming searching\nscheme to determine the optimal object number. By employing a minimum-cost\ncirculation framework, we eliminate the searching step and naturally integrate\nthe number of objects into the optimization problem. By exploring the special\nproperty of the associated graph, that is, an overwhelming majority of the\nvertices are with unit capacity, we designed an implementation of the framework\nand proved it has the best theoretical complexity so far for the global data\nassociation problem. We evaluated our method with 40 experiments on five MOT\nbenchmark datasets. Our method was always the most efficient and averagely 53\nto 1,192 times faster than the three state-of-the-art methods. When our method\nserved as a sub-module for global data association methods using higher-order\nconstraints, similar efficiency improvement was attained. We further\nillustrated through several case studies how the improved computational\nefficiency enables more sophisticated tracking models and yields better\ntracking accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 23:28:20 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 17:14:49 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Wang", "Congchao", ""], ["Wang", "Yizhi", ""], ["Yu", "Guoqiang", ""]]}, {"id": "1911.00809", "submitter": "Ruosong Wang", "authors": "Zhiyuan Li, Ruosong Wang, Dingli Yu, Simon S. Du, Wei Hu, Ruslan\n  Salakhutdinov, Sanjeev Arora", "title": "Enhanced Convolutional Neural Tangent Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research shows that for training with $\\ell_2$ loss, convolutional\nneural networks (CNNs) whose width (number of channels in convolutional layers)\ngoes to infinity correspond to regression with respect to the CNN Gaussian\nProcess kernel (CNN-GP) if only the last layer is trained, and correspond to\nregression with respect to the Convolutional Neural Tangent Kernel (CNTK) if\nall layers are trained. An exact algorithm to compute CNTK (Arora et al., 2019)\nyielded the finding that classification accuracy of CNTK on CIFAR-10 is within\n6-7% of that of that of the corresponding CNN architecture (best figure being\naround 78%) which is interesting performance for a fixed kernel. Here we show\nhow to significantly enhance the performance of these kernels using two ideas.\n(1) Modifying the kernel using a new operation called Local Average Pooling\n(LAP) which preserves efficient computability of the kernel and inherits the\nspirit of standard data augmentation using pixel shifts. Earlier papers were\nunable to incorporate naive data augmentation because of the quadratic training\ncost of kernel regression. This idea is inspired by Global Average Pooling\n(GAP), which we show for CNN-GP and CNTK is equivalent to full translation data\naugmentation. (2) Representing the input image using a pre-processing technique\nproposed by Coates et al. (2011), which uses a single convolutional layer\ncomposed of random image patches. On CIFAR-10, the resulting kernel, CNN-GP\nwith LAP and horizontal flip data augmentation, achieves 89% accuracy, matching\nthe performance of AlexNet (Krizhevsky et al., 2012). Note that this is the\nbest such result we know of for a classifier that is not a trained neural\nnetwork. Similar improvements are obtained for Fashion-MNIST.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 02:24:39 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Li", "Zhiyuan", ""], ["Wang", "Ruosong", ""], ["Yu", "Dingli", ""], ["Du", "Simon S.", ""], ["Hu", "Wei", ""], ["Salakhutdinov", "Ruslan", ""], ["Arora", "Sanjeev", ""]]}, {"id": "1911.00825", "submitter": "Shadrokh Samavi", "authors": "Zahra Nabizadeh, Ghazale Ghorbanzade, Nader Karimi, Shadrokh Samavi", "title": "Image Inpainting by Adaptive Fusion of Variable Spline Interpolations", "comments": "5 pages 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many methods for image enhancement. Image inpainting is one of them\nwhich could be used in reconstruction and restoration of scratch images or\nediting images by adding or removing objects. According to its application,\ndifferent algorithmic and learning methods are proposed. In this paper, the\nfocus is on applications, which enhance the old and historical scratched\nimages. For this purpose, we proposed an adaptive spline interpolation. In this\nmethod, a different number of neighbors in four directions are considered for\neach pixel in the lost block. In the previous methods, predicting the lost\npixels that are on edges is the problem. To address this problem, we consider\nhorizontal and vertical edge information. If the pixel is located on an edge,\nthen we use the predicted value in that direction. In other situations,\nirrelevant predicted values are omitted, and the average of rest values is used\nas the value of the missing pixel. The method evaluates by PSNR and SSIM\nmetrics on the Kodak dataset. The results show improvement in PSNR and SSIM\ncompared to similar procedures. Also, the run time of the proposed method\noutperforms others.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 04:16:35 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Nabizadeh", "Zahra", ""], ["Ghorbanzade", "Ghazale", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "1911.00830", "submitter": "David Golub", "authors": "David Golub, Ahmed El-Kishky, Roberto Mart\\'in-Mart\\'in", "title": "Leveraging Pretrained Image Classifiers for Language-Based Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current semantic segmentation models cannot easily generalize to new object\nclasses unseen during train time: they require additional annotated images and\nretraining. We propose a novel segmentation model that injects visual priors\ninto semantic segmentation architectures, allowing them to segment out new\ntarget labels without retraining. As visual priors, we use the activations of\npretrained image classifiers, which provide noisy indications of the spatial\nlocation of both the target object and distractor objects in the scene. We\nleverage language semantics to obtain these activations for a target label\nunseen by the classifier. Further experiments show that the visual priors\nobtained via language semantics for both relevant and distracting objects are\nkey to our performance.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 05:03:06 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 17:05:39 GMT"}, {"version": "v3", "created": "Tue, 10 Mar 2020 21:01:01 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Golub", "David", ""], ["El-Kishky", "Ahmed", ""], ["Mart\u00edn-Mart\u00edn", "Roberto", ""]]}, {"id": "1911.00850", "submitter": "Sahana Ramnath", "authors": "Sahana Ramnath, Amrita Saha, Soumen Chakrabarti, Mitesh M. Khapra", "title": "Scene Graph based Image Retrieval -- A case study on the CLEVR Dataset", "comments": "3 pages including references, Accepted at the ICCV 2019 Workshop -\n  'Linguistics Meets Image and Video Retrieval' (received Best Paper Award)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the prolification of multimodal interaction in various domains, recently\nthere has been much interest in text based image retrieval in the computer\nvision community. However most of the state of the art techniques model this\nproblem in a purely neural way, which makes it difficult to incorporate\npragmatic strategies in searching a large scale catalog especially when the\nsearch requirements are insufficient and the model needs to resort to an\ninteractive retrieval process through multiple iterations of\nquestion-answering. Motivated by this, we propose a neural-symbolic approach\nfor a one-shot retrieval of images from a large scale catalog, given the\ncaption description. To facilitate this, we represent the catalog and caption\nas scene-graphs and model the retrieval task as a learnable graph matching\nproblem, trained end-to-end with a REINFORCE algorithm. Further, we briefly\ndescribe an extension of this pipeline to an iterative retrieval framework,\nbased on interactive questioning and answering.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 08:00:38 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Ramnath", "Sahana", ""], ["Saha", "Amrita", ""], ["Chakrabarti", "Soumen", ""], ["Khapra", "Mitesh M.", ""]]}, {"id": "1911.00870", "submitter": "Shai Rozenberg", "authors": "Shai Rozenberg, Gal Elidan, Ran El-Yaniv", "title": "MadNet: Using a MAD Optimization for Defending Against Adversarial\n  Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is concerned with the defense of deep models against adversarial\nattacks. Inspired by the certificate defense approach, we propose a maximal\nadversarial distortion (MAD) optimization method for robustifying deep\nnetworks. MAD captures the idea of increasing separability of class clusters in\nthe embedding space while decreasing the network sensitivity to small\ndistortions. Given a deep neural network (DNN) for a classification problem, an\napplication of MAD optimization results in MadNet, a version of the original\nnetwork, now equipped with an adversarial defense mechanism. MAD optimization\nis intuitive, effective and scalable, and the resulting MadNet can improve the\noriginal accuracy. We present an extensive empirical study demonstrating that\nMadNet improves adversarial robustness performance compared to state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 11:21:35 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 19:05:36 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Rozenberg", "Shai", ""], ["Elidan", "Gal", ""], ["El-Yaniv", "Ran", ""]]}, {"id": "1911.00879", "submitter": "Beril Sirmacek", "authors": "Sheona M.M.D.P. Sequeira, Beril Sirmacek", "title": "A low-cost real-time 3D imaging system for contactless asthma\n  observation", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asthma is becoming a very serious problem with every passing day, especially\nin children. However, it is very difficult to detect this disorder in them,\nsince the breathing motion of children tends to change when they reach an age\nof 6. This, thus makes it very difficult to monitor their respiratory state\neasily. In this paper, we present a cheap non-contact alternative to the\ncurrent methods that are available. This is using a stereo camera, that\ncaptures a video of the patient breathing at a frame rate of 30Hz. For further\nprocessing, the captured video has to be rectified and converted into a point\ncloud. The obtained point clouds need to be aligned in order to have the output\nwith respect to a common plane. They are then converted into a surface mesh.\nThe depth is further estimated by subtracting every point cloud from the\nreference point cloud (the first frame). The output data, however, when plotted\nwith respect to real time produces a very noisy plot. This is filtered by\ndetermining the signal frequency by taking the Fast Fourier Transform of the\nbreathing signal. The system was tested under 4 different breathing conditions:\ndeep, shallow and normal breathing and while coughing. On its success, it was\ntested with mixed breathing (combination of normal and shallow breathing) and\nwas lastly compared with the output of the expensive 3dMD system. The\ncomparison showed that using the stereo camera, we can reach to similar\nsensitivity for respiratory motion observation. The experimental results show\nthat, the proposed method provides a major step towards development of low-cost\nhome-based observation systems for asthma patients and care-givers.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 12:54:02 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Sequeira", "Sheona M. M. D. P.", ""], ["Sirmacek", "Beril", ""]]}, {"id": "1911.00904", "submitter": "Jannes Gladrow", "authors": "Jannes Gladrow", "title": "Digital phase-only holography using deep conditional generative models", "comments": "24 pages, 2 tables, and 18 figures (including supplement)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.comp-ph physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Holographic wave-shaping has found numerous applications across the physical\nsciences, especially since the development of digital spatial-light modulators\n(SLMs). A key challenge in digital holography consists in finding optimal\nhologram patterns which transform the incoming laser beam into desired shapes\nin a conjugate optical plane. The existing repertoire of approaches to solve\nthis inverse problem is built on iterative phase-retrieval algorithms, which do\nnot take optical aberrations and deviations from theoretical models into\naccount. Here, we adopt a physics-free, data-driven, and probabilistic approach\nto the problem. Using deep conditional generative models such as\nGenerative-Adversarial Networks (cGAN) or Variational Autoencoder (cVAE), we\napproximate conditional distributions of holograms for a given target laser\nintensity pattern. In order to reduce the cardinality of the problem, we train\nour models on a proxy mapping relating an 8x8-matrix of complex-valued\nspatial-frequency coefficients to the ensuing 100x100-shaped intensity\ndistribution recorded on a camera. We discuss the degree of 'ill-posedness'\nthat remains in this reduced problem and compare different generative model\narchitectures in terms of their ability to find holograms that reconstruct\ngiven intensity patterns. Finally, we challenge our models to generalise to\nsynthetic target intensities, where the existence of matching holograms cannot\nbe guaranteed. We devise a forward-interpolating training scheme aimed at\nproviding models the ability to interpolate in laser intensity space, rather\nthan hologram space and show that this indeed enhances model performance on\nsynthetic data sets.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 14:44:39 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Gladrow", "Jannes", ""]]}, {"id": "1911.00908", "submitter": "Shadrokh Samavi", "authors": "Zahra Sobhaninia, Ali Emami, Nader Karimi, Shadrokh Samavi", "title": "Localization of Fetal Head in Ultrasound Images by Multiscale View and\n  Deep Neural Networks", "comments": "5 pages 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the routine examinations that are used for prenatal care in many\ncountries is ultrasound imaging. This procedure provides various information\nabout fetus health and development, the progress of the pregnancy and, the\nbaby's due date. Some of the biometric parameters of the fetus, like fetal head\ncircumference (HC), must be measured to check the fetus's health and growth. In\nthis paper, we investigated the effects of using multi-scale inputs in the\nnetwork. We also propose a light convolutional neural network for automatic HC\nmeasurement. Experimental results on an ultrasound dataset of the fetus in\ndifferent trimesters of pregnancy show that the segmentation accuracy and HC\nevaluations performed by a light convolutional neural network are comparable to\ndeep convolutional neural networks. The proposed network has fewer parameters\nand requires less training time.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 15:10:17 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Sobhaninia", "Zahra", ""], ["Emami", "Ali", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "1911.00909", "submitter": "Shadrokh Samavi", "authors": "Safiye Rezaei, Ali Emami, Nader Karimi, Shadrokh Samavi", "title": "Gland Segmentation in Histopathological Images by Deep Neural Network", "comments": "5 pages 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histology method is vital in the diagnosis and prognosis of cancers and many\nother diseases. For the analysis of histopathological images, we need to detect\nand segment all gland structures. These images are very challenging, and the\ntask of segmentation is even challenging for specialists. Segmentation of\nglands determines the grade of cancer such as colon, breast, and prostate.\nGiven that deep neural networks have achieved high performance in medical\nimages, we propose a method based on the LinkNet network for gland\nsegmentation. We found the effects of using different loss functions. By using\nWarwick-Qu dataset, which contains two test sets and one train set, we show\nthat our approach is comparable to state-of-the-art methods. Finally, it is\nshown that enhancing the gland edges and the use of hematoxylin components can\nimprove the performance of the proposed model.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 15:12:30 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Rezaei", "Safiye", ""], ["Emami", "Ali", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "1911.00927", "submitter": "Jiamin Wang", "authors": "Ya-guan Qian, Dan-feng Ma, Bin Wang, Jun Pan, Jia-min Wang, Jian-hai\n  Chen, Wu-jie Zhou, Jing-sheng Lei", "title": "Spot Evasion Attacks: Adversarial Examples for License Plate Recognition\n  Systems with Convolutional Neural Networks", "comments": "26 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown convolution neural networks (CNNs) for image\nrecognition are vulnerable to evasion attacks with carefully manipulated\nadversarial examples. Previous work primarily focused on how to generate\nadversarial examples closed to source images, by introducing pixel-level\nperturbations into the whole or specific part of images. In this paper, we\npropose an evasion attack on CNN classifiers in the context of License Plate\nRecognition (LPR), which adds predetermined perturbations to specific regions\nof license plate images, simulating some sort of naturally formed spots (such\nas sludge, etc.). Therefore, the problem is modeled as an optimization process\nsearching for optimal perturbation positions, which is different from previous\nwork that consider pixel values as decision variables. Notice that this is a\ncomplex nonlinear optimization problem, and we use a genetic-algorithm based\napproach to obtain optimal perturbation positions. In experiments, we use the\nproposed algorithm to generate various adversarial examples in the form of\nrectangle, circle, ellipse and spots cluster. Experimental results show that\nthese adversarial examples are almost ignored by human eyes, but can fool\nHyperLPR with high attack success rate over 93%. Therefore, we believe that\nthis kind of spot evasion attacks would pose a great threat to current LPR\nsystems, and needs to be investigated further by the security community.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 05:34:23 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 02:23:19 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Qian", "Ya-guan", ""], ["Ma", "Dan-feng", ""], ["Wang", "Bin", ""], ["Pan", "Jun", ""], ["Wang", "Jia-min", ""], ["Chen", "Jian-hai", ""], ["Zhou", "Wu-jie", ""], ["Lei", "Jing-sheng", ""]]}, {"id": "1911.00957", "submitter": "Iacopo Masi", "authors": "Iacopo Masi, Joe Mathai, Wael AbdAlmageed", "title": "Towards Learning Structure via Consensus for Face Segmentation and\n  Parsing", "comments": "To appear in the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition CVPR 2020. Project page at\n  https://github.com/isi-vista/structure_via_consensus", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face segmentation is the task of densely labeling pixels on the face\naccording to their semantics. While current methods place an emphasis on\ndeveloping sophisticated architectures, use conditional random fields for\nsmoothness, or rather employ adversarial training, we follow an alternative\npath towards robust face segmentation and parsing. Occlusions, along with other\nparts of the face, have a proper structure that needs to be propagated in the\nmodel during training. Unlike state-of-the-art methods that treat face\nsegmentation as an independent pixel prediction problem, we argue instead that\nit should hold highly correlated outputs within the same object pixels. We\nthereby offer a novel learning mechanism to enforce structure in the prediction\nvia consensus, guided by a robust loss function that forces pixel objects to be\nconsistent with each other. Our face parser is trained by transferring\nknowledge from another model, yet it encourages spatial consistency while\nfitting the labels. Different than current practice, our method enjoys\npixel-wise predictions, yet paves the way for fewer artifacts, less sparse\nmasks, and spatially coherent outputs.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 19:53:05 GMT"}, {"version": "v2", "created": "Sun, 10 Nov 2019 02:24:59 GMT"}, {"version": "v3", "created": "Sat, 28 Mar 2020 16:52:02 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Masi", "Iacopo", ""], ["Mathai", "Joe", ""], ["AbdAlmageed", "Wael", ""]]}, {"id": "1911.00962", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, Yang Zou, Tong Che, Peng Ding, Ping Jia, Jane You, Kumar\n  B.V.K", "title": "Conservative Wasserstein Training for Pose Estimation", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper targets the task with discrete and periodic class labels ($e.g.,$\npose/orientation estimation) in the context of deep learning. The commonly used\ncross-entropy or regression loss is not well matched to this problem as they\nignore the periodic nature of the labels and the class similarity, or assume\nlabels are continuous value. We propose to incorporate inter-class correlations\nin a Wasserstein training framework by pre-defining ($i.e.,$ using arc length\nof a circle) or adaptively learning the ground metric. We extend the ground\nmetric as a linear, convex or concave increasing function $w.r.t.$ arc length\nfrom an optimization perspective. We also propose to construct the conservative\ntarget labels which model the inlier and outlier noises using a wrapped\nunimodal-uniform mixture distribution. Unlike the one-hot setting, the\nconservative label makes the computation of Wasserstein distance more\nchallenging. We systematically conclude the practical closed-form solution of\nWasserstein distance for pose data with either one-hot or conservative target\nlabel. We evaluate our method on head, body, vehicle and 3D object pose\nbenchmarks with exhaustive ablation studies. The Wasserstein loss obtaining\nsuperior performance over the current methods, especially using convex mapping\nfunction for ground metric, conservative label, and closed-form solution.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 20:26:16 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Zou", "Yang", ""], ["Che", "Tong", ""], ["Ding", "Peng", ""], ["Jia", "Ping", ""], ["You", "Jane", ""], ["K", "Kumar B. V.", ""]]}, {"id": "1911.00969", "submitter": "Lin Shao", "authors": "Lin Shao, Toki Migimatsu and Jeannette Bohg", "title": "Learning to Scaffold the Development of Robotic Manipulation Skills", "comments": "Accepted to IEEE International Conference on Robotics and Automation\n  (ICRA) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning contact-rich, robotic manipulation skills is a challenging problem\ndue to the high-dimensionality of the state and action space as well as\nuncertainty from noisy sensors and inaccurate motor control. To combat these\nfactors and achieve more robust manipulation, humans actively exploit contact\nconstraints in the environment. By adopting a similar strategy, robots can also\nachieve more robust manipulation. In this paper, we enable a robot to\nautonomously modify its environment and thereby discover how to ease\nmanipulation skill learning. Specifically, we provide the robot with fixtures\nthat it can freely place within the environment. These fixtures provide hard\nconstraints that limit the outcome of robot actions. Thereby, they funnel\nuncertainty from perception and motor control and scaffold manipulation skill\nlearning. We propose a learning system that consists of two learning loops. In\nthe outer loop, the robot positions the fixture in the workspace. In the inner\nloop, the robot learns a manipulation skill and after a fixed number of\nepisodes, returns the reward to the outer loop. Thereby, the robot is\nincentivised to place the fixture such that the inner loop quickly achieves a\nhigh reward. We demonstrate our framework both in simulation and in the real\nworld on three tasks: peg insertion, wrench manipulation and shallow-depth\ninsertion. We show that manipulation skill learning is dramatically sped up\nthrough this way of scaffolding.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 21:15:46 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 06:03:30 GMT"}, {"version": "v3", "created": "Mon, 5 Oct 2020 05:11:36 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Shao", "Lin", ""], ["Migimatsu", "Toki", ""], ["Bohg", "Jeannette", ""]]}, {"id": "1911.00997", "submitter": "Yichuan Charlie Tang", "authors": "Yichuan Charlie Tang, Ruslan Salakhutdinov", "title": "Multiple Futures Prediction", "comments": "In proceedings of NeurIPS 2019, Vancouver, British Columbia, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.MA cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal prediction is critical for making intelligent and robust decisions\nin complex dynamic environments. Motion prediction needs to model the\ninherently uncertain future which often contains multiple potential outcomes,\ndue to multi-agent interactions and the latent goals of others. Towards these\ngoals, we introduce a probabilistic framework that efficiently learns latent\nvariables to jointly model the multi-step future motions of agents in a scene.\nOur framework is data-driven and learns semantically meaningful latent\nvariables to represent the multimodal future, without requiring explicit\nlabels. Using a dynamic attention-based state encoder, we learn to encode the\npast as well as the future interactions among agents, efficiently scaling to\nany number of agents. Finally, our model can be used for planning via computing\na conditional probability density over the trajectories of other agents given a\nhypothetical rollout of the 'self' agent. We demonstrate our algorithms by\npredicting vehicle trajectories of both simulated and real data, demonstrating\nthe state-of-the-art results on several vehicle trajectory datasets.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 00:42:01 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 23:36:01 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Tang", "Yichuan Charlie", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1911.01015", "submitter": "David Schubert", "authors": "David Schubert, Nikolaus Demmel, Lukas von Stumberg, Vladyslav Usenko\n  and Daniel Cremers", "title": "Rolling-Shutter Modelling for Direct Visual-Inertial Odometry", "comments": null, "journal-ref": null, "doi": "10.1109/IROS40897.2019.8968539", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a direct visual-inertial odometry (VIO) method which estimates the\nmotion of the sensor setup and sparse 3D geometry of the environment based on\nmeasurements from a rolling-shutter camera and an inertial measurement unit\n(IMU).\n  The visual part of the system performs a photometric bundle adjustment on a\nsparse set of points. This direct approach does not extract feature points and\nis able to track not only corners, but any pixels with sufficient gradient\nmagnitude. Neglecting rolling-shutter effects in the visual part severely\ndegrades accuracy and robustness of the system. In this paper, we incorporate a\nrolling-shutter model into the photometric bundle adjustment that estimates a\nset of recent keyframe poses and the inverse depth of a sparse set of points.\n  IMU information is accumulated between several frames using measurement\npreintegration, and is inserted into the optimization as an additional\nconstraint between selected keyframes. For every keyframe we estimate not only\nthe pose but also velocity and biases to correct the IMU measurements. Unlike\nsystems with global-shutter cameras, we use both IMU measurements and\nrolling-shutter effects of the camera to estimate velocity and biases for every\nstate.\n  Last, we evaluate our system on a novel dataset that contains global-shutter\nand rolling-shutter images, IMU data and ground-truth poses for ten different\nsequences, which we make publicly available. Evaluation shows that the proposed\nmethod outperforms a system where rolling shutter is not modelled and achieves\nsimilar accuracy to the global-shutter method on global-shutter data.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 02:54:15 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Schubert", "David", ""], ["Demmel", "Nikolaus", ""], ["von Stumberg", "Lukas", ""], ["Usenko", "Vladyslav", ""], ["Cremers", "Daniel", ""]]}, {"id": "1911.01028", "submitter": "Dibakar Gope", "authors": "Dibakar Gope, Jesse Beu, Urmish Thakker, Matthew Mattina", "title": "Ternary MobileNets via Per-Layer Hybrid Filter Banks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MobileNets family of computer vision neural networks have fueled tremendous\nprogress in the design and organization of resource-efficient architectures in\nrecent years. New applications with stringent real-time requirements on highly\nconstrained devices require further compression of MobileNets-like already\ncompute-efficient networks. Model quantization is a widely used technique to\ncompress and accelerate neural network inference and prior works have quantized\nMobileNets to 4-6 bits albeit with a modest to significant drop in accuracy.\nWhile quantization to sub-byte values (i.e. precision less than or equal to 8\nbits) has been valuable, even further quantization of MobileNets to binary or\nternary values is necessary to realize significant energy savings and possibly\nruntime speedups on specialized hardware, such as ASICs and FPGAs. Under the\nkey observation that convolutional filters at each layer of a deep neural\nnetwork may respond differently to ternary quantization, we propose a novel\nquantization method that generates per-layer hybrid filter banks consisting of\nfull-precision and ternary weight filters for MobileNets. The layer-wise hybrid\nfilter banks essentially combine the strengths of full-precision and ternary\nweight filters to derive a compact, energy-efficient architecture for\nMobileNets. Using this proposed quantization method, we quantized a substantial\nportion of weight filters of MobileNets to ternary values resulting in 27.98%\nsavings in energy, and a 51.07% reduction in the model size, while achieving\ncomparable accuracy and no degradation in throughput on specialized hardware in\ncomparison to the baseline full-precision MobileNets.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 04:32:59 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Gope", "Dibakar", ""], ["Beu", "Jesse", ""], ["Thakker", "Urmish", ""], ["Mattina", "Matthew", ""]]}, {"id": "1911.01045", "submitter": "Jiancheng Cai", "authors": "Jiancheng Cai, Hu Han, Shiguang Shan, Xilin Chen", "title": "FCSR-GAN: Joint Face Completion and Super-resolution via Multi-task\n  Learning", "comments": "To appear in IEEE Trans. BIOM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Combined variations containing low-resolution and occlusion often present in\nface images in the wild, e.g., under the scenario of video surveillance. While\nmost of the existing face image recovery approaches can handle only one type of\nvariation per model, in this work, we propose a deep generative adversarial\nnetwork (FCSR-GAN) for performing joint face completion and face\nsuper-resolution via multi-task learning. The generator of FCSR-GAN aims to\nrecover a high-resolution face image without occlusion given an input\nlow-resolution face image with occlusion. The discriminator of FCSR-GAN uses a\nset of carefully designed losses (an adversarial loss, a perceptual loss, a\npixel loss, a smooth loss, a style loss, and a face prior loss) to assure the\nhigh quality of the recovered high-resolution face images without occlusion.\nThe whole network of FCSR-GAN can be trained end-to-end using our two-stage\ntraining strategy. Experimental results on the public-domain CelebA and Helen\ndatabases show that the proposed approach outperforms the state-of-the-art\nmethods in jointly performing face super-resolution (up to 8 $\\times$) and face\ncompletion, and shows good generalization ability in cross-database testing.\nOur FCSR-GAN is also useful for improving face identification performance when\nthere are low-resolution and occlusion in face images.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 06:35:30 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Cai", "Jiancheng", ""], ["Han", "Hu", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1911.01048", "submitter": "Shishi Qiao", "authors": "Shishi Qiao, Ruiping Wang, Shiguang Shan, and Xilin Chen", "title": "Deep Heterogeneous Hashing for Face Video Retrieval", "comments": "14 pages, 17 figures, 4 tables, accepted by IEEE Transactions on\n  Image Processing (TIP) 2019", "journal-ref": "IEEE Transactions on Image Processing 2019", "doi": "10.1109/TIP.2019.2940683", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrieving videos of a particular person with face image as a query via\nhashing technique has many important applications. While face images are\ntypically represented as vectors in Euclidean space, characterizing face videos\nwith some robust set modeling techniques (e.g. covariance matrices as exploited\nin this study, which reside on Riemannian manifold), has recently shown\nappealing advantages. This hence results in a thorny heterogeneous spaces\nmatching problem. Moreover, hashing with handcrafted features as done in many\nexisting works is clearly inadequate to achieve desirable performance for this\ntask. To address such problems, we present an end-to-end Deep Heterogeneous\nHashing (DHH) method that integrates three stages including image feature\nlearning, video modeling, and heterogeneous hashing in a single framework, to\nlearn unified binary codes for both face images and videos. To tackle the key\nchallenge of hashing on the manifold, a well-studied Riemannian kernel mapping\nis employed to project data (i.e. covariance matrices) into Euclidean space and\nthus enables to embed the two heterogeneous representations into a common\nHamming space, where both intra-space discriminability and inter-space\ncompatibility are considered. To perform network optimization, the gradient of\nthe kernel mapping is innovatively derived via structured matrix\nbackpropagation in a theoretically principled way. Experiments on three\nchallenging datasets show that our method achieves quite competitive\nperformance compared with existing hashing methods.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 07:00:59 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Qiao", "Shishi", ""], ["Wang", "Ruiping", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1911.01049", "submitter": "Thong Huynh", "authors": "Van Thong Huynh, Soo-Hyung Kim, Guee-Sang Lee, Hyung-Jeong Yang", "title": "Eye Semantic Segmentation with a Lightweight Model", "comments": "To appear in ICCVW 2019. Pre-trained models and source code are\n  available https://github.com/th2l/Eye_VR_Segmentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a multi-class eye segmentation method that can run\nthe hardware limitations for real-time inference. Our approach includes three\nmajor stages: get a grayscale image from the input, segment three distinct eye\nregion with a deep network, and remove incorrect areas with heuristic filters.\nOur model based on the encoder decoder structure with the key is the depthwise\nconvolution operation to reduce the computation cost. We experiment on OpenEDS,\na large scale dataset of eye images captured by a head-mounted display with two\nsynchronized eye facing cameras. We achieved the mean intersection over union\n(mIoU) of 94.85% with a model of size 0.4 megabytes. The source code are\navailable https://github.com/th2l/Eye_VR_Segmentation\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 07:04:14 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Huynh", "Van Thong", ""], ["Kim", "Soo-Hyung", ""], ["Lee", "Guee-Sang", ""], ["Yang", "Hyung-Jeong", ""]]}, {"id": "1911.01051", "submitter": "XingJiao Wu", "authors": "Xiangcheng Du, Tianlong Ma, Yingbin Zheng, Hao Ye, Xingjiao Wu, Liang\n  He", "title": "Scene Text Recognition with Temporal Convolutional Encoder", "comments": null, "journal-ref": "IEEE ICASSP 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texts from scene images typically consist of several characters and exhibit a\ncharacteristic sequence structure. Existing methods capture the structure with\nthe sequence-to-sequence models by an encoder to have the visual\nrepresentations and then a decoder to translate the features into the label\nsequence. In this paper, we study text recognition framework by considering the\nlong-term temporal dependencies in the encoder stage. We demonstrate that the\nproposed Temporal Convolutional Encoder with increased sequential extents\nimproves the accuracy of text recognition. We also study the impact of\ndifferent attention modules in convolutional blocks for learning accurate text\nrepresentations. We conduct comparisons on seven datasets and the experiments\ndemonstrate the effectiveness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 07:10:11 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2020 05:32:08 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Du", "Xiangcheng", ""], ["Ma", "Tianlong", ""], ["Zheng", "Yingbin", ""], ["Ye", "Hao", ""], ["Wu", "Xingjiao", ""], ["He", "Liang", ""]]}, {"id": "1911.01054", "submitter": "Arindam Das", "authors": "Arindam Das", "title": "SoildNet: Soiling Degradation Detection in Autonomous Driving", "comments": "Accepted at the NeurIPS 2019 Workshop on Machine Learning for\n  Autonomous Driving", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of autonomous driving, camera sensors are extremely prone to\nsoiling because they are located outside of the car and interact with\nenvironmental sources of soiling such as rain drops, snow, dust, sand, mud and\nso on. This can lead to either partial or complete vision degradation. Hence\ndetecting such decay in vision is very important for safety and overall to\npreserve the functionality of the \"autonomous\" components in autonomous\ndriving. The contribution of this work involves: 1) Designing a Deep\nConvolutional Neural Network (DCNN) based baseline network, 2) Exploiting\nseveral network remodelling techniques such as employing static and dynamic\ngroup convolution, channel reordering to compress the baseline architecture and\nmake it suitable for low power embedded systems with nearly 1 TOPS, 3)\nComparing various result metrics of all interim networks dedicated for soiling\ndegradation detection at tile level of size 64 x 64 on input resolution 1280 x\n768. The compressed network, is called SoildNet (Sand, snOw, raIn/dIrt, oiL,\nDust/muD) that uses only 9.72% trainable parameters of the base network and\nreduces the model size by more than 7 times with no loss in accuracy\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 07:13:26 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 03:56:41 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Das", "Arindam", ""]]}, {"id": "1911.01059", "submitter": "Lei Zhu", "authors": "Lei Zhu, Qi She, Lidan Zhang, Ping Guo", "title": "A Spectral Nonlocal Block for Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nonlocal-based blocks are designed for capturing long-range\nspatial-temporal dependencies in computer vision tasks. Although having shown\nexcellent performances, they lack the mechanism to encode the rich, structured\ninformation among elements in an image. In this paper, to theoretically analyze\nthe property of these nonlocal-based blocks, we provide a unified approach to\ninterpreting them, where we view them as a graph filter generated on a\nfully-connected graph. When the graph filter is approximated by Chebyshev\npolynomials, a generalized formulation can be derived for explaining the\nexisting nonlocal-based blocks ($\\mathit{e.g.,}$ nonlocal block, nonlocal\nstage, double attention block). Furthermore, we propose an efficient and robust\nspectral nonlocal block, which can be flexibly inserted into deep neural\nnetworks to catch the long-range dependencies between spatial pixels or\ntemporal frames. Experimental results demonstrate the clear-cut improvements\nand practical applicabilities of the spectral nonlocal block on image\nclassification (Cifar-10/100, ImageNet), fine-grained image classification\n(CUB-200), action recognition (UCF-101), and person re-identification\n(ILID-SVID, Mars, Prid-2011) tasks.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 07:36:36 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 15:26:11 GMT"}, {"version": "v3", "created": "Wed, 27 Nov 2019 09:32:03 GMT"}, {"version": "v4", "created": "Mon, 10 Feb 2020 09:42:36 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Zhu", "Lei", ""], ["She", "Qi", ""], ["Zhang", "Lidan", ""], ["Guo", "Ping", ""]]}, {"id": "1911.01060", "submitter": "Hongru Li", "authors": "Yuan Zhou, Hongru Li and Sun-Yuan Kung", "title": "Temporal Action Localization using Long Short-Term Dependency", "comments": "12pages, Trans", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal action localization in untrimmed videos is an important but\ndifficult task. Difficulties are encountered in the application of existing\nmethods when modeling temporal structures of videos. In the present study, we\ndeveloped a novel method, referred to as Gemini Network, for effective modeling\nof temporal structures and achieving high-performance temporal action\nlocalization. The significant improvements afforded by the proposed method are\nattributable to three major factors. First, the developed network utilizes two\nsubnets for effective modeling of temporal structures. Second, three parallel\nfeature extraction pipelines are used to prevent interference between the\nextractions of different stage features. Third, the proposed method utilizes\nauxiliary supervision, with the auxiliary classifier losses affording\nadditional constraints for improving the modeling capability of the network. As\na demonstration of its effectiveness, the Gemini Network was used to achieve\nstate-of-the-art temporal action localization performance on two challenging\ndatasets, namely, THUMOS14 and ActivityNet.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 07:38:15 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Zhou", "Yuan", ""], ["Li", "Hongru", ""], ["Kung", "Sun-Yuan", ""]]}, {"id": "1911.01062", "submitter": "Jie Zhao", "authors": "Jie Zhao, Lei Dai, Mo Zhang, Fei Yu, Meng Li, Hongfeng Li, Wenjia\n  Wang, Li Zhang", "title": "PGU-net+: Progressive Growing of U-net+ for Automated Cervical Nuclei\n  Segmentation", "comments": "MICCAI workshop MMMI2019 Best Student Paper Award", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated cervical nucleus segmentation based on deep learning can\neffectively improve the quantitative analysis of cervical cancer. However,\naccurate nuclei segmentation is still challenging. The classic U-net has not\nachieved satisfactory results on this task, because it mixes the information of\ndifferent scales that affect each other, which limits the segmentation accuracy\nof the model. To solve this problem, we propose a progressive growing U-net\n(PGU-net+) model, which uses two paradigms to extract image features at\ndifferent scales in a more independent way. First, we add residual modules\nbetween different scales of U-net, which enforces the model to learn the\napproximate shape of the annotation in the coarser scale, and to learn the\nresidual between the annotation and the approximate shape in the finer scale.\nSecond, we start to train the model with the coarsest part and then\nprogressively add finer part to the training until the full model is included.\nWhen we train a finer part, we will reduce the learning rate of the previous\ncoarser part, which further ensures that the model independently extracts\ninformation from different scales. We conduct several comparative experiments\non the Herlev dataset. The experimental results show that the PGU-net+ has\nsuperior accuracy than the previous state-of-the-art methods on cervical nuclei\nsegmentation.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 07:38:59 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 12:04:02 GMT"}, {"version": "v3", "created": "Tue, 12 Nov 2019 04:09:10 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Zhao", "Jie", ""], ["Dai", "Lei", ""], ["Zhang", "Mo", ""], ["Yu", "Fei", ""], ["Li", "Meng", ""], ["Li", "Hongfeng", ""], ["Wang", "Wenjia", ""], ["Zhang", "Li", ""]]}, {"id": "1911.01071", "submitter": "Dino Ienco", "authors": "Dino Ienco, Roberto Interdonato and Raffaele Gaetano", "title": "Supervised level-wise pretraining for recurrent neural network\n  initialization in multi-class classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) can be seriously impacted by the initial\nparameters assignment, which may result in poor generalization performances on\nnew unseen data. With the objective to tackle this crucial issue, in the\ncontext of RNN based classification, we propose a new supervised layer-wise\npretraining strategy to initialize network parameters. The proposed approach\nleverages a data-aware strategy that sets up a taxonomy of classification\nproblems automatically derived by the model behavior. To the best of our\nknowledge, despite the great interest in RNN-based classification, this is the\nfirst data-aware strategy dealing with the initialization of such models. The\nproposed strategy has been tested on four benchmarks coming from two different\ndomains, i.e., Speech Recognition and Remote Sensing. Results underline the\nsignificance of our approach and point out that data-aware strategies\npositively support the initialization of Recurrent Neural Network based\nclassification models.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 08:30:01 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Ienco", "Dino", ""], ["Interdonato", "Roberto", ""], ["Gaetano", "Raffaele", ""]]}, {"id": "1911.01082", "submitter": "Maxime Ferrera", "authors": "Marcela Carvalho, Maxime Ferrera, Alexandre Boulch, Julien Moras,\n  Bertrand Le Saux, Pauline Trouv\\'e-Peloux", "title": "Technical Report: Co-learning of geometry and semantics for online 3D\n  mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a technical report about our submission for the ECCV 2018 3DRMS\nWorkshop Challenge on Semantic 3D Reconstruction \\cite{Tylecek2018rms}. In this\npaper, we address 3D semantic reconstruction for autonomous navigation using\nco-learning of depth map and semantic segmentation. The core of our pipeline is\na deep multi-task neural network which tightly refines depth and also produces\naccurate semantic segmentation maps. Its inputs are an image and a raw depth\nmap produced from a pair of images by standard stereo vision. The resulting\nsemantic 3D point clouds are then merged in order to create a consistent 3D\nmesh, in turn used to produce dense semantic 3D reconstruction maps. The\nperformances of each step of the proposed method are evaluated on the dataset\nand multiple tasks of the 3DRMS Challenge, and repeatedly surpass\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 09:19:17 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Carvalho", "Marcela", ""], ["Ferrera", "Maxime", ""], ["Boulch", "Alexandre", ""], ["Moras", "Julien", ""], ["Saux", "Bertrand Le", ""], ["Trouv\u00e9-Peloux", "Pauline", ""]]}, {"id": "1911.01103", "submitter": "Stephen James", "authors": "Alessandro Bonardi, Stephen James, Andrew J. Davison", "title": "Learning One-Shot Imitation from Humans without Humans", "comments": "Videos can be found here:\n  https://sites.google.com/view/tecnets-humans", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can naturally learn to execute a new task by seeing it performed by\nother individuals once, and then reproduce it in a variety of configurations.\nEndowing robots with this ability of imitating humans from third person is a\nvery immediate and natural way of teaching new tasks. Only recently, through\nmeta-learning, there have been successful attempts to one-shot imitation\nlearning from humans; however, these approaches require a lot of human\nresources to collect the data in the real world to train the robot. But is\nthere a way to remove the need for real world human demonstrations during\ntraining? We show that with Task-Embedded Control Networks, we can infer\ncontrol polices by embedding human demonstrations that can condition a control\npolicy and achieve one-shot imitation learning. Importantly, we do not use a\nreal human arm to supply demonstrations during training, but instead leverage\ndomain randomisation in an application that has not been seen before:\nsim-to-real transfer on humans. Upon evaluating our approach on pushing and\nplacing tasks in both simulation and in the real world, we show that in\ncomparison to a system that was trained on real-world data we are able to\nachieve similar results by utilising only simulation data.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 10:07:27 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Bonardi", "Alessandro", ""], ["James", "Stephen", ""], ["Davison", "Andrew J.", ""]]}, {"id": "1911.01106", "submitter": "Zhicheng Cao", "authors": "Jiong Chen, Heng Zhao, Zhicheng Cao and Liaojun Pang", "title": "Singular points detection with semantic segmentation networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Singular points detection is one of the most classical and important problem\nin the field of fingerprint recognition. However, current detection rates of\nsingular points are still unsatisfactory, especially for low-quality\nfingerprints. Compared with traditional image processing-based detection\nmethods, methods based on deep learning only need the original fingerprint\nimage but not the fingerprint orientation field. In this paper, different from\nother detection methods based on deep learning, we treat singular points\ndetection as a semantic segmentation problem and just use few data for\ntraining. Furthermore, we propose a new convolutional neural network called\nSinNet to extract the singular regions of interest and then use a blob\ndetection method called SimpleBlobDetector to locate the singular points. The\nexperiments are carried out on the test dataset from SPD2010, and the proposed\nmethod has much better performance than the other advanced methods in most\naspects. Compared with the state-of-art algorithms in SPD2010, our method\nachieves an increase of 11% in the percentage of correctly detected\nfingerprints and an increase of more than 18% in the core detection rate.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 10:10:36 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Chen", "Jiong", ""], ["Zhao", "Heng", ""], ["Cao", "Zhicheng", ""], ["Pang", "Liaojun", ""]]}, {"id": "1911.01126", "submitter": "Florian Dubost", "authors": "Florian Dubost, Benjamin Collery, Antonin Renaudier, Axel Roc, Nicolas\n  Posocco, Gerda Bortsova, Wiro Niessen, Marleen de Bruijne", "title": "Automated Estimation of the Spinal Curvature via Spine Centerline\n  Extraction with Ensembles of Cascaded Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scoliosis is a condition defined by an abnormal spinal curvature. For\ndiagnosis and treatment planning of scoliosis, spinal curvature can be\nestimated using Cobb angles. We propose an automated method for the estimation\nof Cobb angles from X-ray scans. First, the centerline of the spine was\nsegmented using a cascade of two convolutional neural networks. After smoothing\nthe centerline, Cobb angles were automatically estimated using the derivative\nof the centerline. We evaluated the results using the mean absolute error and\nthe average symmetric mean absolute percentage error between the manual\nassessment by experts and the automated predictions. For optimization, we used\n609 X-ray scans from the London Health Sciences Center, and for evaluation, we\nparticipated in the international challenge \"Accurate Automated Spinal\nCurvature Estimation, MICCAI 2019\" (100 scans). On the challenge's test set, we\nobtained an average symmetric mean absolute percentage error of 22.96.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 10:57:36 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 13:44:30 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Dubost", "Florian", ""], ["Collery", "Benjamin", ""], ["Renaudier", "Antonin", ""], ["Roc", "Axel", ""], ["Posocco", "Nicolas", ""], ["Bortsova", "Gerda", ""], ["Niessen", "Wiro", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1911.01138", "submitter": "Karttikeya Mangalam", "authors": "Karttikeya Mangalam, Ehsan Adeli, Kuan-Hui Lee, Adrien Gaidon, Juan\n  Carlos Niebles", "title": "Disentangling Human Dynamics for Pedestrian Locomotion Forecasting with\n  Noisy Supervision", "comments": "Accepted to WACV 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of Human Locomotion Forecasting, a task for jointly\npredicting the spatial positions of several keypoints on the human body in the\nnear future under an egocentric setting. In contrast to the previous work that\naims to solve either the task of pose prediction or trajectory forecasting in\nisolation, we propose a framework to unify the two problems and address the\npractically useful task of pedestrian locomotion prediction in the wild. Among\nthe major challenges in solving this task is the scarcity of annotated\negocentric video datasets with dense annotations for pose, depth, or egomotion.\nTo surmount this difficulty, we use state-of-the-art models to generate (noisy)\nannotations and propose robust forecasting models that can learn from this\nnoisy supervision. We present a method to disentangle the overall pedestrian\nmotion into easier to learn subparts by utilizing a pose completion and a\ndecomposition module. The completion module fills in the missing key-point\nannotations and the decomposition module breaks the cleaned locomotion down to\nglobal (trajectory) and local (pose keypoint movements). Further, with Quasi\nRNN as our backbone, we propose a novel hierarchical trajectory forecasting\nnetwork that utilizes low-level vision domain specific signals like egomotion\nand depth to predict the global trajectory. Our method leads to\nstate-of-the-art results for the prediction of human locomotion in the\negocentric view. Project pade: https://karttikeya.github.io/publication/plf/\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 11:30:12 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 19:33:42 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Mangalam", "Karttikeya", ""], ["Adeli", "Ehsan", ""], ["Lee", "Kuan-Hui", ""], ["Gaidon", "Adrien", ""], ["Niebles", "Juan Carlos", ""]]}, {"id": "1911.01149", "submitter": "Florian Chabot", "authors": "Florian Chabot, Quoc-Cuong Pham, Mohamed Chaouch", "title": "LapNet : Automatic Balanced Loss and Optimal Assignment for Real-Time\n  Dense Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time single-stage object detectors based on deep learning still remain\nless accurate than more complex ones. The trade-off between model performance\nand computational speed is a major challenge. In this paper, we propose a new\nway to efficiently learn a single-shot detector which offers a very good\ncompromise between these two objectives. To this end, we introduce LapNet, an\nanchor based detector, trained end-to-end without any sampling strategy. Our\napproach aims to overcome two important problems encountered in training an\nanchor based detector: (1) ambiguity in the assignment of anchor to ground\ntruth and (2) class and object size imbalance. To address the first limitation,\nwe propose a soft positive/negative anchor assignment procedure based on a new\noverlapping function called \"Per-Object Normalized Overlap\" (PONO). This soft\nassignment can be self-corrected by the network itself to avoid ambiguity\nbetween close objects. To cope with the second limitation, we propose to learn\nadditional weights, that are not used at inference, to efficiently manage\nsample imbalance. These two contributions make the detector learning more\ngeneric whatever the training dataset. Various experiments show the\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 12:13:07 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 15:18:17 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Chabot", "Florian", ""], ["Pham", "Quoc-Cuong", ""], ["Chaouch", "Mohamed", ""]]}, {"id": "1911.01178", "submitter": "Yixing Huang", "authors": "Yixing Huang, Lei Gao, Alexander Preuhs and Andreas Maier", "title": "Field of View Extension in Computed Tomography Using Deep Learning Prior", "comments": "Submitted to Bildverarbeitung fuer die Medizin 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computed tomography (CT), data truncation is a common problem. Images\nreconstructed by the standard filtered back-projection algorithm from truncated\ndata suffer from cupping artifacts inside the field-of-view (FOV), while\nanatomical structures are severely distorted or missing outside the FOV. Deep\nlearning, particularly the U-Net, has been applied to extend the FOV as a\npost-processing method. Since image-to-image prediction neglects the data\nfidelity to measured projection data, incorrect structures, even inside the\nFOV, might be reconstructed by such an approach. Therefore, generating\nreconstructed images directly from a post-processing neural network is\ninadequate. In this work, we propose a data consistent reconstruction method,\nwhich utilizes deep learning reconstruction as prior for extrapolating\ntruncated projections and a conventional iterative reconstruction to constrain\nthe reconstruction consistent to measured raw data. Its efficacy is\ndemonstrated in our study, achieving small average root-mean-square error of 24\nHU inside the FOV and a high structure similarity index of 0.993 for the whole\nbody area on a test patient's CT data.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 13:11:34 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 11:48:59 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Huang", "Yixing", ""], ["Gao", "Lei", ""], ["Preuhs", "Alexander", ""], ["Maier", "Andreas", ""]]}, {"id": "1911.01218", "submitter": "Gerda Bortsova", "authors": "Gerda Bortsova, Florian Dubost, Laurens Hogeweg, Ioannis Katramados,\n  Marleen de Bruijne", "title": "Semi-Supervised Medical Image Segmentation via Learning Consistency\n  under Transformations", "comments": null, "journal-ref": "In proceedings of Medical Image Computing and Computer Assisted\n  Intervention - MICCAI 2019", "doi": "10.1007/978-3-030-32226-7_90", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scarcity of labeled data often limits the application of supervised deep\nlearning techniques for medical image segmentation. This has motivated the\ndevelopment of semi-supervised techniques that learn from a mixture of labeled\nand unlabeled images. In this paper, we propose a novel semi-supervised method\nthat, in addition to supervised learning on labeled training images, learns to\npredict segmentations consistent under a given class of transformations on both\nlabeled and unlabeled images. More specifically, in this work we explore\nlearning equivariance to elastic deformations. We implement this through: 1) a\nSiamese architecture with two identical branches, each of which receives a\ndifferently transformed image, and 2) a composite loss function with a\nsupervised segmentation loss term and an unsupervised term that encourages\nsegmentation consistency between the predictions of the two branches. We\nevaluate the method on a public dataset of chest radiographs with segmentations\nof anatomical structures using 5-fold cross-validation. The proposed method\nreaches significantly higher segmentation accuracy compared to supervised\nlearning. This is due to learning transformation consistency on both labeled\nand unlabeled images, with the latter contributing the most. We achieve the\nperformance comparable to state-of-the-art chest X-ray segmentation methods\nwhile using substantially fewer labeled images.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 13:51:17 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Bortsova", "Gerda", ""], ["Dubost", "Florian", ""], ["Hogeweg", "Laurens", ""], ["Katramados", "Ioannis", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1911.01223", "submitter": "Yixing Huang", "authors": "Lei Gao, Yixing Huang and Andreas Maier", "title": "Superpixel-Based Background Recovery from Multiple Images", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an intuitive method to recover background from\nmultiple images. The implementation consists of three stages: model\ninitialization, model update, and background output. We consider the pixels\nwhose values change little in all input images as background seeds. Images are\nthen segmented into superpixels with simple linear iterative clustering. When\nthe number of pixels labelled as background in a superpixel is bigger than a\npredefined threshold, we label the superpixel as background to initialize the\nbackground candidate masks. Background candidate images are obtained from input\nraw images with the masks. Combining all candidate images, a background image\nis produced. The background candidate masks, candidate images, and the\nbackground image are then updated alternately until convergence. Finally,\nghosting artifacts is removed with the k-nearest neighbour method. An\nexperiment on an outdoor dataset demonstrates that the proposed algorithm can\nachieve promising results.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 13:59:29 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Gao", "Lei", ""], ["Huang", "Yixing", ""], ["Maier", "Andreas", ""]]}, {"id": "1911.01249", "submitter": "Kai Zhang", "authors": "Kai Zhang, Shuhang Gu, Radu Timofte, Zheng Hui, Xiumei Wang, Xinbo\n  Gao, Dongliang Xiong, Shuai Liu, Ruipeng Gang, Nan Nan, Chenghua Li, Xueyi\n  Zou, Ning Kang, Zhan Wang, Hang Xu, Chaofeng Wang, Zheng Li, Linlin Wang, Jun\n  Shi, Wenyu Sun, Zhiqiang Lang, Jiangtao Nie, Wei Wei, Lei Zhang, Yazhe Niu,\n  Peijin Zhuo, Xiangzhen Kong, Long Sun, Wenhao Wang", "title": "AIM 2019 Challenge on Constrained Super-Resolution: Methods and Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the AIM 2019 challenge on constrained example-based single\nimage super-resolution with focus on proposed solutions and results. The\nchallenge had 3 tracks. Taking the three main aspects (i.e., number of\nparameters, inference/running time, fidelity (PSNR)) of MSRResNet as the\nbaseline, Track 1 aims to reduce the amount of parameters while being\nconstrained to maintain or improve the running time and the PSNR result, Tracks\n2 and 3 aim to optimize running time and PSNR result with constrain of the\nother two aspects, respectively. Each track had an average of 64 registered\nparticipants, and 12 teams submitted the final results. They gauge the\nstate-of-the-art in single image super-resolution.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 14:39:51 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Zhang", "Kai", ""], ["Gu", "Shuhang", ""], ["Timofte", "Radu", ""], ["Hui", "Zheng", ""], ["Wang", "Xiumei", ""], ["Gao", "Xinbo", ""], ["Xiong", "Dongliang", ""], ["Liu", "Shuai", ""], ["Gang", "Ruipeng", ""], ["Nan", "Nan", ""], ["Li", "Chenghua", ""], ["Zou", "Xueyi", ""], ["Kang", "Ning", ""], ["Wang", "Zhan", ""], ["Xu", "Hang", ""], ["Wang", "Chaofeng", ""], ["Li", "Zheng", ""], ["Wang", "Linlin", ""], ["Shi", "Jun", ""], ["Sun", "Wenyu", ""], ["Lang", "Zhiqiang", ""], ["Nie", "Jiangtao", ""], ["Wei", "Wei", ""], ["Zhang", "Lei", ""], ["Niu", "Yazhe", ""], ["Zhuo", "Peijin", ""], ["Kong", "Xiangzhen", ""], ["Sun", "Long", ""], ["Wang", "Wenhao", ""]]}, {"id": "1911.01320", "submitter": "Varun Jain", "authors": "Varun Jain, Shivam Aggarwal, Suril Mehta, Ramya Hebbalaguppe", "title": "Synthetic Video Generation for Robust Hand Gesture Recognition in\n  Augmented Reality Applications", "comments": "Presented at the ICCV 2019 Workshop: The 5th International Workshop\n  on Observing And Understanding Hands In Action", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand gestures are a natural means of interaction in Augmented Reality and\nVirtual Reality (AR/VR) applications. Recently, there has been an increased\nfocus on removing the dependence of accurate hand gesture recognition on\ncomplex sensor setup found in expensive proprietary devices such as the\nMicrosoft HoloLens, Daqri and Meta Glasses. Most such solutions either rely on\nmulti-modal sensor data or deep neural networks that can benefit greatly from\nabundance of labelled data. Datasets are an integral part of any deep learning\nbased research. They have been the principal reason for the substantial\nprogress in this field, both, in terms of providing enough data for the\ntraining of these models, and, for benchmarking competing algorithms. However,\nit is becoming increasingly difficult to generate enough labelled data for\ncomplex tasks such as hand gesture recognition. The goal of this work is to\nintroduce a framework capable of generating photo-realistic videos that have\nlabelled hand bounding box and fingertip that can help in designing, training,\nand benchmarking models for hand-gesture recognition in AR/VR applications. We\ndemonstrate the efficacy of our framework in generating videos with diverse\nbackgrounds.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 16:32:07 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 00:40:12 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2019 03:15:32 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Jain", "Varun", ""], ["Aggarwal", "Shivam", ""], ["Mehta", "Suril", ""], ["Hebbalaguppe", "Ramya", ""]]}, {"id": "1911.01333", "submitter": "Erfan Darzi", "authors": "Erfan Darzi, Armin Mohammadie-Zand, Hamid Soltanian-Zadeh", "title": "Using image-extracted features to determine heart rate and blink\n  duration for driver sleepiness detection", "comments": null, "journal-ref": "IEEE International Conference on Biomedical Engineering, 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heart rate and blink duration are two vital physiological signals which give\ninformation about cardiac activity and consciousness. Monitoring these two\nsignals is crucial for various applications such as driver drowsiness\ndetection. As there are several problems posed by the conventional systems to\nbe used for continuous, long-term monitoring, a remote blink and ECG monitoring\nsystem can be used as an alternative. For estimating the blink duration, two\nstrategies are used. In the first approach, pictures of open and closed eyes\nare fed into an Artificial Neural Network (ANN) to decide whether the eyes are\nopen or close. In the second approach, they are classified and labeled using\nLinear Discriminant Analysis (LDA). The labeled images are then be used to\ndetermine the blink duration. For heart rate variability, two strategies are\nused to evaluate the passing blood volume: Independent Component Analysis\n(ICA); and a chrominance based method. Eye recognition yielded 78-92% accuracy\nin classifying open/closed eyes with ANN and 71-91% accuracy with LDA. Heart\nrate evaluations had a mean loss of around 16 Beats Per Minute (BPM) for the\nICA strategy and 13 BPM for the chrominance based technique.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 16:52:28 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 13:12:58 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Darzi", "Erfan", ""], ["Mohammadie-Zand", "Armin", ""], ["Soltanian-Zadeh", "Hamid", ""]]}, {"id": "1911.01346", "submitter": "Andrei Damian I", "authors": "Andrei Damian, Laurentiu Piciu, Alexandru Purdila and Nicolae Tapus", "title": "CloudifierNet -- Deep Vision Models for Artificial Image Processing", "comments": "ITQM 2019", "journal-ref": null, "doi": "10.1016/j.procs.2019.12.043", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Today, more and more, it is necessary that most applications and documents\ndeveloped in previous or current technologies to be accessible online on\ncloud-based infrastructures. That is why the migration of legacy systems\nincluding their hosts of documents to new technologies and online\ninfrastructures, using modern Artificial Intelligence techniques, is absolutely\nnecessary. With the advancement of Artificial Intelligence and Deep Learning\nwith its multitude of applications, a new area of research is emerging - that\nof automated systems development and maintenance. The underlying work objective\nthat led to this paper aims to research and develop truly intelligent systems\nable to analyze user interfaces from various sources and generate real and\nusable inferences ranging from architecture analysis to actual code generation.\nOne key element of such systems is that of artificial scene detection and\nanalysis based on deep learning computer vision systems. Computer vision models\nand particularly deep directed acyclic graphs based on convolutional modules\nare generally constructed and trained based on natural images datasets. Due to\nthis fact, the models will develop during the training process natural image\nfeature detectors apart from the base graph modules that will learn basic\nprimitive features. In the current paper, we will present the base principles\nof a deep neural pipeline for computer vision applied to artificial scenes\n(scenes generated by user interfaces or similar). Finally, we will present the\nconclusions based on experimental development and benchmarking against\nstate-of-the-art transfer-learning implemented deep vision models.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 17:16:35 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 11:13:43 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Damian", "Andrei", ""], ["Piciu", "Laurentiu", ""], ["Purdila", "Alexandru", ""], ["Tapus", "Nicolae", ""]]}, {"id": "1911.01370", "submitter": "Wataru Shimoda", "authors": "Wataru Shimoda, Keiji Yanai", "title": "Self-Supervised Difference Detection for Weakly-Supervised Semantic\n  Segmentation", "comments": "ICCV 2019, source codes: https://github.com/shimoda-uec/ssdd", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To minimize the annotation costs associated with the training of semantic\nsegmentation models, researchers have extensively investigated\nweakly-supervised segmentation approaches. In the current weakly-supervised\nsegmentation methods, the most widely adopted approach is based on\nvisualization. However, the visualization results are not generally equal to\nsemantic segmentation. Therefore, to perform accurate semantic segmentation\nunder the weakly supervised condition, it is necessary to consider the mapping\nfunctions that convert the visualization results into semantic segmentation.\nFor such mapping functions, the conditional random field and iterative\nre-training using the outputs of a segmentation model are usually used.\nHowever, these methods do not always guarantee improvements in accuracy;\ntherefore, if we apply these mapping functions iteratively multiple times,\neventually the accuracy will not improve or will decrease.\n  In this paper, to make the most of such mapping functions, we assume that the\nresults of the mapping function include noise, and we improve the accuracy by\nremoving noise. To achieve our aim, we propose the self-supervised difference\ndetection module, which estimates noise from the results of the mapping\nfunctions by predicting the difference between the segmentation masks before\nand after the mapping. We verified the effectiveness of the proposed method by\nperforming experiments on the PASCAL Visual Object Classes 2012 dataset, and we\nachieved 64.9\\% in the val set and 65.5\\% in the test set. Both of the results\nbecome new state-of-the-art under the same setting of weakly supervised\nsemantic segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 17:57:23 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 14:40:59 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Shimoda", "Wataru", ""], ["Yanai", "Keiji", ""]]}, {"id": "1911.01376", "submitter": "Xiaomeng Li", "authors": "Xiaomeng Li, Xiaowei Hu, Lequan Yu, Lei Zhu, Chi-Wing Fu and Pheng-Ann\n  Heng", "title": "CANet: Cross-disease Attention Network for Joint Diabetic Retinopathy\n  and Diabetic Macular Edema Grading", "comments": "IEEE Transactions on Medical Imaging; code is at\n  https://github.com/xmengli999/CANet", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic retinopathy (DR) and diabetic macular edema (DME) are the leading\ncauses of permanent blindness in the working-age population. Automatic grading\nof DR and DME helps ophthalmologists design tailored treatments to patients,\nthus is of vital importance in the clinical practice. However, prior works\neither grade DR or DME, and ignore the correlation between DR and its\ncomplication, i.e., DME. Moreover, the location information, e.g., macula and\nsoft hard exhaust annotations, are widely used as a prior for grading. Such\nannotations are costly to obtain, hence it is desirable to develop automatic\ngrading methods with only image-level supervision. In this paper, we present a\nnovel cross-disease attention network (CANet) to jointly grade DR and DME by\nexploring the internal relationship between the diseases with only image-level\nsupervision. Our key contributions include the disease-specific attention\nmodule to selectively learn useful features for individual diseases, and the\ndisease-dependent attention module to further capture the internal relationship\nbetween the two diseases. We integrate these two attention modules in a deep\nnetwork to produce disease-specific and disease-dependent features, and to\nmaximize the overall performance jointly for grading DR and DME. We evaluate\nour network on two public benchmark datasets, i.e., ISBI 2018 IDRiD challenge\ndataset and Messidor dataset. Our method achieves the best result on the ISBI\n2018 IDRiD challenge dataset and outperforms other methods on the Messidor\ndataset. Our code is publicly available at https://github.com/xmengli999/CANet.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 18:06:35 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Li", "Xiaomeng", ""], ["Hu", "Xiaowei", ""], ["Yu", "Lequan", ""], ["Zhu", "Lei", ""], ["Fu", "Chi-Wing", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1911.01425", "submitter": "Pablo S\\'anchez Mart\\'in", "authors": "Pablo S\\'anchez-Mart\\'in, Pablo M. Olmos and Fernando Perez-Cruz", "title": "Improved BiGAN training with marginal likelihood equalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel training procedure for improving the performance of\ngenerative adversarial networks (GANs), especially to bidirectional GANs.\nFirst, we enforce that the empirical distribution of the inverse inference\nnetwork matches the prior distribution, which favors the generator network\nreproducibility on the seen samples. Second, we have found that the marginal\nlog-likelihood of the samples shows a severe overrepresentation of a certain\ntype of samples. To address this issue, we propose to train the bidirectional\nGAN using a non-uniform sampling for the mini-batch selection, resulting in\nimproved quality and variety in generated samples measured quantitatively and\nby visual inspection. We illustrate our new procedure with the well-known\nCIFAR10, Fashion MNIST and CelebA datasets.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 17:02:20 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 10:38:03 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["S\u00e1nchez-Mart\u00edn", "Pablo", ""], ["Olmos", "Pablo M.", ""], ["Perez-Cruz", "Fernando", ""]]}, {"id": "1911.01474", "submitter": "Alborz Rezazadeh Sereshkeh", "authors": "Alborz Rezazadeh Sereshkeh, Gary Leung, Krish Perumal, Caleb Phillips,\n  Minfan Zhang, Afsaneh Fazly, Iqbal Mohomed", "title": "VASTA: A Vision and Language-assisted Smartphone Task Automation System", "comments": "Submitted to ACM IUI'20, 10 figures, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present VASTA, a novel vision and language-assisted Programming By\nDemonstration (PBD) system for smartphone task automation. Development of a\nrobust PBD automation system requires overcoming three key challenges: first,\nhow to make a particular demonstration robust to positional and visual changes\nin the user interface (UI) elements; secondly, how to recognize changes in the\nautomation parameters to make the demonstration as generalizable as possible;\nand thirdly, how to recognize from the user utterance what automation the user\nwishes to carry out. To address the first challenge, VASTA leverages\nstate-of-the-art computer vision techniques, including object detection and\noptical character recognition, to accurately label interactions demonstrated by\na user, without relying on the underlying UI structures. To address the second\nand third challenges, VASTA takes advantage of advanced natural language\nunderstanding algorithms for analyzing the user utterance to trigger the VASTA\nautomation scripts, and to determine the automation parameters for\ngeneralization. We run an initial user study that demonstrates the\neffectiveness of VASTA at clustering user utterances, understanding changes in\nthe automation parameters, detecting desired UI elements, and, most\nimportantly, automating various tasks. A demo video of the system is available\nhere: http://y2u.be/kr2xE-FixjI\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 20:21:32 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Sereshkeh", "Alborz Rezazadeh", ""], ["Leung", "Gary", ""], ["Perumal", "Krish", ""], ["Phillips", "Caleb", ""], ["Zhang", "Minfan", ""], ["Fazly", "Afsaneh", ""], ["Mohomed", "Iqbal", ""]]}, {"id": "1911.01477", "submitter": "Farzad Khalvati", "authors": "Khashayar Namdar, Isha Gujrathi, Masoom A. Haider, Farzad Khalvati", "title": "Evolution-based Fine-tuning of CNNs for Prostate Cancer Detection", "comments": "Accepted for the 33rd Conference on Neural Information Processing\n  Systems (NeurIPS 2019), Medical Imaging Meets NEURIPS Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have been used for automated detection\nof prostate cancer where Area Under Receiver Operating Characteristic (ROC)\ncurve (AUC) is usually used as the performance metric. Given that AUC is not\ndifferentiable, common practice is to train the CNN using a loss functions\nbased on other performance metrics such as cross entropy and monitoring AUC to\nselect the best model. In this work, we propose to fine-tune a trained CNN for\nprostate cancer detection using a Genetic Algorithm to achieve a higher AUC.\nOur dataset contained 6-channel Diffusion-Weighted MRI slices of prostate. On a\ncohort of 2,955 training, 1,417 validation, and 1,334 test slices, we reached\ntest AUC of 0.773; a 9.3% improvement compared to the base CNN model.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 20:40:29 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Namdar", "Khashayar", ""], ["Gujrathi", "Isha", ""], ["Haider", "Masoom A.", ""], ["Khalvati", "Farzad", ""]]}, {"id": "1911.01507", "submitter": "Yaroslava Lochman", "authors": "James Pritts, Zuzana Kukelova, Viktor Larsson, Yaroslava Lochman, and\n  Ond\\v{r}ej Chum", "title": "Minimal Solvers for Rectifying from Radially-Distorted Conjugate\n  Translations", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (2020) 1-1", "doi": "10.1109/TPAMI.2020.2992261", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces minimal solvers that jointly solve for radial lens\nundistortion and affine-rectification using local features extracted from the\nimage of coplanar translated and reflected scene texture, which is common in\nman-made environments. The proposed solvers accommodate different types of\nlocal features and sampling strategies, and three of the proposed variants\nrequire just one feature correspondence. State-of-the-art techniques from\nalgebraic geometry are used to simplify the formulation of the solvers. The\ngenerated solvers are stable, small and fast. Synthetic and real-image\nexperiments show that the proposed solvers have superior robustness to noise\ncompared to the state of the art. The solvers are integrated with an automated\nsystem for rectifying imaged scene planes from coplanar repeated texture.\nAccurate rectifications on challenging imagery taken with narrow to wide\nfield-of-view lenses demonstrate the applicability of the proposed solvers.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 22:07:56 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 16:36:54 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2020 21:00:36 GMT"}, {"version": "v4", "created": "Sat, 21 Nov 2020 16:00:10 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Pritts", "James", ""], ["Kukelova", "Zuzana", ""], ["Larsson", "Viktor", ""], ["Lochman", "Yaroslava", ""], ["Chum", "Ond\u0159ej", ""]]}, {"id": "1911.01519", "submitter": "Chuin Hong Yap", "authors": "Chuin Hong Yap, Connah Kendrick and Moi Hoon Yap", "title": "SAMM Long Videos: A Spontaneous Facial Micro- and Macro-Expressions\n  Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growth of popularity of facial micro-expressions in recent years,\nthe demand for long videos with micro- and macro-expressions remains high.\nExtended from SAMM, a micro-expressions dataset released in 2016, this paper\npresents SAMM Long Videos dataset for spontaneous micro- and macro-expressions\nrecognition and spotting. SAMM Long Videos dataset consists of 147 long videos\nwith 343 macro-expressions and 159 micro-expressions. The dataset is FACS-coded\nwith detailed Action Units (AUs). We compare our dataset with Chinese Academy\nof Sciences Macro-Expressions and Micro-Expressions (CAS(ME)2) dataset, which\nis the only available fully annotated dataset with micro- and\nmacro-expressions. Furthermore, we preprocess the long videos using OpenFace,\nwhich includes face alignment and detection of facial AUs. We conduct facial\nexpression spotting using this dataset and compare it with the baseline of MEGC\nIII. Our spotting method outperformed the baseline result with F1-score of\n0.3299.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 22:31:30 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 17:31:34 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Yap", "Chuin Hong", ""], ["Kendrick", "Connah", ""], ["Yap", "Moi Hoon", ""]]}, {"id": "1911.01529", "submitter": "Jan Blumenkamp", "authors": "Jan Blumenkamp, Andreas Baude, Tim Laue", "title": "Closing the Reality Gap with Unsupervised Sim-to-Real Image Translation", "comments": "Accepted to RoboCup Symposium 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning approaches have become the standard solution to many problems\nin computer vision and robotics, but obtaining sufficient training data in high\nenough quality is challenging, as human labor is error prone, time consuming,\nand expensive. Solutions based on simulation have become more popular in recent\nyears, but the gap between simulation and reality is still a major issue. In\nthis paper, we introduce a novel method for augmenting synthetic image data\nthrough unsupervised image-to-image translation by applying the style of real\nworld images to simulated images with open source frameworks. The generated\ndataset is combined with conventional augmentation methods and is then applied\nto a neural network model running in real-time on autonomous soccer robots. Our\nevaluation shows a significant improvement compared to models trained on images\ngenerated entirely in simulation.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 23:17:03 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 10:39:20 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Blumenkamp", "Jan", ""], ["Baude", "Andreas", ""], ["Laue", "Tim", ""]]}, {"id": "1911.01577", "submitter": "Duc Nguyen", "authors": "Duc Nguyen, Nhan Tran, Hung Le", "title": "Improving Long Handwritten Text Line Recognition with Convolutional\n  Multi-way Associative Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Recurrent Neural Networks (CRNNs) excel at scene text\nrecognition. Unfortunately, they are likely to suffer from vanishing/exploding\ngradient problems when processing long text images, which are commonly found in\nscanned documents. This poses a major challenge to goal of completely solving\nOptical Character Recognition (OCR) problem. Inspired by recently proposed\nmemory-augmented neural networks (MANNs) for long-term sequential modeling, we\npresent a new architecture dubbed Convolutional Multi-way Associative Memory\n(CMAM) to tackle the limitation of current CRNNs. By leveraging recent memory\naccessing mechanisms in MANNs, our architecture demonstrates superior\nperformance against other CRNN counterparts in three real-world long text OCR\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 02:42:09 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 06:46:13 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Nguyen", "Duc", ""], ["Tran", "Nhan", ""], ["Le", "Hung", ""]]}, {"id": "1911.01648", "submitter": "Liu Qing", "authors": "Qing Liu, Beiji Zou, Yang Zhao, Yixiong Liang", "title": "A Deep Gradient Boosting Network for Optic Disc and Cup Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of optic disc (OD) and optic cup (OC) is critical in automated\nfundus image analysis system. Existing state-of-the-arts focus on designing\ndeep neural networks with one or multiple dense prediction branches. Such kind\nof designs ignore connections among prediction branches and their learning\ncapacity is limited. To build connections among prediction branches, this paper\nintroduces gradient boosting framework to deep classification model and\nproposes a gradient boosting network called BoostNet. Specifically, deformable\nside-output unit and aggregation unit with deep supervisions are proposed to\nlearn base functions and expansion coefficients in gradient boosting framework.\nBy stacking aggregation units in a deep-to-shallow manner, models' performances\nare gradually boosted along deep to shallow stages. BoostNet achieves superior\nresults to existing deep OD and OC segmentation networks on the public dataset\nORIGA.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 07:23:53 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Liu", "Qing", ""], ["Zou", "Beiji", ""], ["Zhao", "Yang", ""], ["Liang", "Yixiong", ""]]}, {"id": "1911.01655", "submitter": "Ruben Villegas", "authors": "Ruben Villegas, Arkanath Pathak, Harini Kannan, Dumitru Erhan, Quoc V.\n  Le, Honglak Lee", "title": "High Fidelity Video Prediction with Large Stochastic Recurrent Neural\n  Networks", "comments": "In Advances in Neural Information Processing Systems (NeurIPS), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting future video frames is extremely challenging, as there are many\nfactors of variation that make up the dynamics of how frames change through\ntime. Previously proposed solutions require complex inductive biases inside\nnetwork architectures with highly specialized computation, including\nsegmentation masks, optical flow, and foreground and background separation. In\nthis work, we question if such handcrafted architectures are necessary and\ninstead propose a different approach: finding minimal inductive bias for video\nprediction while maximizing network capacity. We investigate this question by\nperforming the first large-scale empirical study and demonstrate\nstate-of-the-art performance by learning large models on three different\ndatasets: one for modeling object interactions, one for modeling human motion,\nand one for modeling car driving.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 07:44:57 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Villegas", "Ruben", ""], ["Pathak", "Arkanath", ""], ["Kannan", "Harini", ""], ["Erhan", "Dumitru", ""], ["Le", "Quoc V.", ""], ["Lee", "Honglak", ""]]}, {"id": "1911.01664", "submitter": "Jun Fu", "authors": "Jun Fu, Jing Liu, Yuhang Wang, Yong Li, Yongjun Bao, Jinhui Tang,\n  Hanqing Lu", "title": "Adaptive Context Network for Scene Parsing", "comments": "Accepted by ICCV 2019", "journal-ref": "International Conference on Computer Vision 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works attempt to improve scene parsing performance by exploring\ndifferent levels of contexts, and typically train a well-designed convolutional\nnetwork to exploit useful contexts across all pixels equally. However, in this\npaper, we find that the context demands are varying from different pixels or\nregions in each image. Based on this observation, we propose an Adaptive\nContext Network (ACNet) to capture the pixel-aware contexts by a competitive\nfusion of global context and local context according to different per-pixel\ndemands. Specifically, when given a pixel, the global context demand is\nmeasured by the similarity between the global feature and its local feature,\nwhose reverse value can be used to measure the local context demand. We model\nthe two demand measurements by the proposed global context module and local\ncontext module, respectively, to generate adaptive contextual features.\nFurthermore, we import multiple such modules to build several adaptive context\nblocks in different levels of network to obtain a coarse-to-fine result.\nFinally, comprehensive experimental evaluations demonstrate the effectiveness\nof the proposed ACNet, and new state-of-the-arts performances are achieved on\nall four public datasets, i.e. Cityscapes, ADE20K, PASCAL Context, and COCO\nStuff.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 08:16:28 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Fu", "Jun", ""], ["Liu", "Jing", ""], ["Wang", "Yuhang", ""], ["Li", "Yong", ""], ["Bao", "Yongjun", ""], ["Tang", "Jinhui", ""], ["Lu", "Hanqing", ""]]}, {"id": "1911.01668", "submitter": "Yuxuan Sun", "authors": "Yuxuan Sun, Chong Sun, Dong Wang, You He, Huchuan Lu", "title": "ROI Pooled Correlation Filters for Visual Tracking", "comments": "CVPR 2019 Accepted.10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ROI (region-of-interest) based pooling method performs pooling operations\non the cropped ROI regions for various samples and has shown great success in\nthe object detection methods. It compresses the model size while preserving the\nlocalization accuracy, thus it is useful in the visual tracking field. Though\nbeing effective, the ROI-based pooling operation is not yet considered in the\ncorrelation filter formula. In this paper, we propose a novel ROI pooled\ncorrelation filter (RPCF) algorithm for robust visual tracking. Through\nmathematical derivations, we show that the ROI-based pooling can be\nequivalently achieved by enforcing additional constraints on the learned filter\nweights, which makes the ROI-based pooling feasible on the virtual circular\nsamples. Besides, we develop an efficient joint training formula for the\nproposed correlation filter algorithm, and derive the Fourier solvers for\nefficient model training. Finally, we evaluate our RPCF tracker on OTB-2013,\nOTB-2015 and VOT-2017 benchmark datasets. Experimental results show that our\ntracker performs favourably against other state-of-the-art trackers.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 08:52:48 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Sun", "Yuxuan", ""], ["Sun", "Chong", ""], ["Wang", "Dong", ""], ["He", "You", ""], ["Lu", "Huchuan", ""]]}, {"id": "1911.01671", "submitter": "Jianchen Zhu", "authors": "Jianchen Zhu, Tong Zhang, Shengjie Zhao, Carlos Hinojosa, Zengli Liu,\n  Gonzalo R. Arce", "title": "Spatial Sparse subspace clustering for Compressive Spectral imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at developing a clustering approach with spectral images\ndirectly from CASSI compressive measurements. The proposed clustering method\nfirst assumes that compressed measurements lie in the union of multiple\nlow-dimensional subspaces. Therefore, sparse subspace clustering (SSC) is an\nunsupervised method that assigns compressed measurements to their respective\nsubspaces. In addition, a 3D spatial regularizer is added into the SSC problem,\nthus taking full advantages of the spatial information contained in spectral\nimages. The performance of the proposed spectral image clustering approach is\nimproved by taking optimal CASSI measurements obtained when optimal coded\napertures are used in CASSI system. Simulation with one real dataset\nillustrates the accuracy of the proposed spectral image clustering approach.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 09:07:27 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Zhu", "Jianchen", ""], ["Zhang", "Tong", ""], ["Zhao", "Shengjie", ""], ["Hinojosa", "Carlos", ""], ["Liu", "Zengli", ""], ["Arce", "Gonzalo R.", ""]]}, {"id": "1911.01672", "submitter": "Gergely Cs\\\"onde", "authors": "Gergely Cs\\\"onde, Yoshihide Sekimoto, Takehiro Kashiyama", "title": "Congestion Analysis of Convolutional Neural Network-Based Pedestrian\n  Counting Methods on Helicopter Footage", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, researchers have presented many different\napplications for convolutional neural networks, including those for the\ndetection and recognition of objects from images. The desire to understand our\nown nature has always been an important motivation for research. Thus, the\nvisual recognition of humans is among the most important issues facing machine\nlearning today. Most solutions for this task have been developed and tested by\nusing several publicly available datasets. These datasets typically contain\nimages taken from street-level closed-circuit television cameras offering a\nlow-angle view. There are major differences between such images and those taken\nfrom the sky. In addition, aerial images are often very congested, containing\nhundreds of targets. These factors may have significant impact on the quality\nof the results. In this paper, we investigate state-of-the-art methods for\ncounting pedestrians and the related performance of aerial footage.\nFurthermore, we analyze this performance with respect to the congestion levels\nof the images.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 09:07:35 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Cs\u00f6nde", "Gergely", ""], ["Sekimoto", "Yoshihide", ""], ["Kashiyama", "Takehiro", ""]]}, {"id": "1911.01685", "submitter": "Jeroen Bertels", "authors": "Jeroen Bertels, Tom Eelbode, Maxim Berman, Dirk Vandermeulen, Frederik\n  Maes, Raf Bisschops, Matthew Blaschko", "title": "Optimizing the Dice Score and Jaccard Index for Medical Image\n  Segmentation: Theory & Practice", "comments": "MICCAI 2019", "journal-ref": "LNCS 11765, Springer Nature Switzerland AG 2019", "doi": "10.1007/978-3-030-32245-8_11", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dice score and Jaccard index are commonly used metrics for the evaluation\nof segmentation tasks in medical imaging. Convolutional neural networks trained\nfor image segmentation tasks are usually optimized for (weighted)\ncross-entropy. This introduces an adverse discrepancy between the learning\noptimization objective (the loss) and the end target metric. Recent works in\ncomputer vision have proposed soft surrogates to alleviate this discrepancy and\ndirectly optimize the desired metric, either through relaxations (soft-Dice,\nsoft-Jaccard) or submodular optimization (Lov\\'asz-softmax). The aim of this\nstudy is two-fold. First, we investigate the theoretical differences in a risk\nminimization framework and question the existence of a weighted cross-entropy\nloss with weights theoretically optimized to surrogate Dice or Jaccard. Second,\nwe empirically investigate the behavior of the aforementioned loss functions\nw.r.t. evaluation with Dice score and Jaccard index on five medical\nsegmentation tasks. Through the application of relative approximation bounds,\nwe show that all surrogates are equivalent up to a multiplicative factor, and\nthat no optimal weighting of cross-entropy exists to approximate Dice or\nJaccard measures. We validate these findings empirically and show that, while\nit is important to opt for one of the target metric surrogates rather than a\ncross-entropy-based loss, the choice of the surrogate does not make a\nstatistical difference on a wide range of medical segmentation tasks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 09:42:25 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Bertels", "Jeroen", ""], ["Eelbode", "Tom", ""], ["Berman", "Maxim", ""], ["Vandermeulen", "Dirk", ""], ["Maes", "Frederik", ""], ["Bisschops", "Raf", ""], ["Blaschko", "Matthew", ""]]}, {"id": "1911.01702", "submitter": "Johannes Rausch", "authors": "Johannes Rausch, Octavio Martinez, Fabian Bissig, Ce Zhang, Stefan\n  Feuerriegel", "title": "DocParser: Hierarchical Structure Parsing of Document Renderings", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translating renderings (e. g. PDFs, scans) into hierarchical document\nstructures is extensively demanded in the daily routines of many real-world\napplications. However, a holistic, principled approach to inferring the\ncomplete hierarchical structure of documents is missing. As a remedy, we\ndeveloped \"DocParser\": an end-to-end system for parsing the complete document\nstructure - including all text elements, nested figures, tables, and table cell\nstructures. Our second contribution is to provide a dataset for evaluating\nhierarchical document structure parsing. Our third contribution is to propose a\nscalable learning framework for settings where domain-specific data are scarce,\nwhich we address by a novel approach to weak supervision that significantly\nimproves the document structure parsing performance. Our experiments confirm\nthe effectiveness of our proposed weak supervision: Compared to the baseline\nwithout weak supervision, it improves the mean average precision for detecting\ndocument entities by 39.1 % and improves the F1 score of classifying\nhierarchical relations by 35.8 %.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 10:42:08 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 11:54:38 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Rausch", "Johannes", ""], ["Martinez", "Octavio", ""], ["Bissig", "Fabian", ""], ["Zhang", "Ce", ""], ["Feuerriegel", "Stefan", ""]]}, {"id": "1911.01711", "submitter": "Johannes M\\\"uller", "authors": "Johannes M\\\"uller, Martin Herrmann, Jan Strohbeck, Vasileios\n  Belagiannis and Michael Buchholz", "title": "LACI: Low-effort Automatic Calibration of Infrastructure Sensors", "comments": "6 pages, published at ITSC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensor calibration usually is a time consuming yet important task. While\nclassical approaches are sensor-specific and often need calibration targets as\nwell as a widely overlapping field of view (FOV), within this work, a\ncooperative intelligent vehicle is used as callibration target. The vehicleis\ndetected in the sensor frame and then matched with the information received\nfrom the cooperative awareness messagessend by the coperative intelligent\nvehicle. The presented algorithm is fully automated as well as\nsensor-independent, relying only on a very common set of assumptions. Due to\nthe direct registration on the world frame, no overlapping FOV is necessary.\nThe algorithm is evaluated through experiment for four laserscanners as well as\none pair of stereo cameras showing a repetition error within the measurement\nuncertainty of the sensors. A plausibility check rules out systematic errors\nthat might not have been covered by evaluating the repetition error.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 11:04:06 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["M\u00fcller", "Johannes", ""], ["Herrmann", "Martin", ""], ["Strohbeck", "Jan", ""], ["Belagiannis", "Vasileios", ""], ["Buchholz", "Michael", ""]]}, {"id": "1911.01727", "submitter": "Yifan Zhou", "authors": "Yifan Zhou and Simon Maskell", "title": "Detecting and Tracking Small Moving Objects in Wide Area Motion Imagery\n  (WAMI) Using Convolutional Neural Networks (CNNs)", "comments": "Accepted for publication in 22nd International Conference on\n  Information Fusion (FUSION 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an approach to detect moving objects in Wide Area Motion\nImagery (WAMI), in which the objects are both small and well separated.\nIdentifying the objects only using foreground appearance is difficult since a\n$100-$pixel vehicle is hard to distinguish from objects comprising the\nbackground. Our approach is based on background subtraction as an efficient and\nunsupervised method that is able to output the shape of objects. In order to\nreliably detect low contrast and small objects, we configure the background\nsubtraction to extract foreground regions that might be objects of interest.\nWhile this dramatically increases the number of false alarms, a Convolutional\nNeural Network (CNN) considering both spatial and temporal information is then\ntrained to reject the false alarms. In areas with heavy traffic, the background\nsubtraction yields merged detections. To reduce the complexity of multi-target\ntracker needed, we train another CNN to predict the positions of multiple\nmoving objects in an area. Our approach shows competitive detection performance\non smaller objects relative to the state-of-the-art. We adopt a GM-PHD filter\nto associate detections over time and analyse the resulting performance.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 11:43:53 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 14:13:54 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Zhou", "Yifan", ""], ["Maskell", "Simon", ""]]}, {"id": "1911.01738", "submitter": "Sergey Pavlov", "authors": "Sergey Pavlov, Alexey Artemov, Maksim Sharaev, Alexander Bernstein,\n  Evgeny Burnaev", "title": "Weakly Supervised Fine Tuning Approach for Brain Tumor Segmentation\n  Problem", "comments": "Accepted to IEEE International Conference on Machine Learning and\n  Applications (ICMLA 2019). Typos corrected, images updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of tumors in brain MRI images is a challenging task, where most\nrecent methods demand large volumes of data with pixel-level annotations, which\nare generally costly to obtain. In contrast, image-level annotations, where\nonly the presence of lesion is marked, are generally cheap, generated in far\nlarger volumes compared to pixel-level labels, and contain less labeling noise.\nIn the context of brain tumor segmentation, both pixel-level and image-level\nannotations are commonly available; thus, a natural question arises whether a\nsegmentation procedure could take advantage of both. In the present work we: 1)\npropose a learning-based framework that allows simultaneous usage of both\npixel- and image-level annotations in MRI images to learn a segmentation model\nfor brain tumor; 2) study the influence of comparative amounts of pixel- and\nimage-level annotations on the quality of brain tumor segmentation; 3) compare\nour approach to the traditional fully-supervised approach and show that the\nperformance of our method in terms of segmentation quality may be competitive.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 12:14:40 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 07:48:01 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Pavlov", "Sergey", ""], ["Artemov", "Alexey", ""], ["Sharaev", "Maksim", ""], ["Bernstein", "Alexander", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1911.01769", "submitter": "Yiming Li", "authors": "Yiming Li, Peidong Liu, Yong Jiang, Shu-Tao Xia", "title": "Visual Privacy Protection via Mapping Distortion", "comments": "Accepted by the ICASSP 2021. The first two authors contributed\n  equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy protection is an important research area, which is especially\ncritical in this big data era. To a large extent, the privacy of visual\nclassification data is mainly in the mapping between the image and its\ncorresponding label, since this relation provides a great amount of information\nand can be used in other scenarios. In this paper, we propose the mapping\ndistortion based protection (MDP) and its augmentation-based extension (AugMDP)\nto protect the data privacy by modifying the original dataset. In the modified\ndataset generated by MDP, the image and its label are not consistent ($e.g.$, a\ncat-like image is labeled as the dog), whereas the DNNs trained on it can still\nachieve good performance on benign testing set. As such, this method can\nprotect privacy when the dataset is leaked. Extensive experiments are\nconducted, which verify the effectiveness and feasibility of our method. The\ncode for reproducing main results is available at\n\\url{https://github.com/PerdonLiu/Visual-Privacy-Protection-via-Mapping-Distortion}.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 13:41:45 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 15:35:15 GMT"}, {"version": "v3", "created": "Sun, 31 Jan 2021 15:08:57 GMT"}, {"version": "v4", "created": "Wed, 3 Feb 2021 09:39:37 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Li", "Yiming", ""], ["Liu", "Peidong", ""], ["Jiang", "Yong", ""], ["Xia", "Shu-Tao", ""]]}, {"id": "1911.01786", "submitter": "Peidong Liu", "authors": "Peidong Liu, Xiyu Yan, Yong Jiang, Shu-Tao Xia", "title": "Deep Flow Collaborative Network for Online Visual Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep learning-based visual tracking algorithms such as MDNet achieve high\nperformance leveraging to the feature extraction ability of a deep neural\nnetwork. However, the tracking efficiency of these trackers is not very high\ndue to the slow feature extraction for each frame in a video. In this paper, we\npropose an effective tracking algorithm to alleviate the time-consuming\nproblem. Specifically, we design a deep flow collaborative network, which\nexecutes the expensive feature network only on sparse keyframes and transfers\nthe feature maps to other frames via optical flow. Moreover, we raise an\neffective adaptive keyframe scheduling mechanism to select the most appropriate\nkeyframe. We evaluate the proposed approach on large-scale datasets: OTB2013\nand OTB2015. The experiment results show that our algorithm achieves\nconsiderable speedup and high precision as well.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 14:13:07 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Liu", "Peidong", ""], ["Yan", "Xiyu", ""], ["Jiang", "Yong", ""], ["Xia", "Shu-Tao", ""]]}, {"id": "1911.01816", "submitter": "Joeri Nicolaes", "authors": "Joeri Nicolaes, Steven Raeymaeckers, David Robben, Guido Wilms, Dirk\n  Vandermeulen, Cesar Libanati and Marc Debois", "title": "Detection of vertebral fractures in CT using 3D Convolutional Neural\n  Networks", "comments": "13 pages, 7 figures, pre-print MICCAI CSI", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Osteoporosis induced fractures occur worldwide about every 3 seconds.\nVertebral compression fractures are early signs of the disease and considered\nrisk predictors for secondary osteoporotic fractures. We present a detection\nmethod to opportunistically screen spine-containing CT images for the presence\nof these vertebral fractures. Inspired by radiology practice, existing methods\nare based on 2D and 2.5D features but we present, to the best of our knowledge,\nthe first method for detecting vertebral fractures in CT using automatically\nlearned 3D feature maps. The presented method explicitly localizes these\nfractures allowing radiologists to interpret its results. We train a\nvoxel-classification 3D Convolutional Neural Network (CNN) with a training\ndatabase of 90 cases that has been semi-automatically generated using\nradiologist readings that are readily available in clinical practice. Our 3D\nmethod produces an Area Under the Curve (AUC) of 95% for patient-level fracture\ndetection and an AUC of 93% for vertebra-level fracture detection in a\nfive-fold cross-validation experiment.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 14:43:00 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Nicolaes", "Joeri", ""], ["Raeymaeckers", "Steven", ""], ["Robben", "David", ""], ["Wilms", "Guido", ""], ["Vandermeulen", "Dirk", ""], ["Libanati", "Cesar", ""], ["Debois", "Marc", ""]]}, {"id": "1911.01857", "submitter": "Huanhou Xiao", "authors": "Huanhou Xiao and Jinglun Shi", "title": "Video Captioning with Text-based Dynamic Attention and Step-by-Step\n  Learning", "comments": "The paper is under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically describing video content with natural language has been\nattracting much attention in CV and NLP communities. Most existing methods\npredict one word at a time, and by feeding the last generated word back as\ninput at the next time, while the other generated words are not fully\nexploited. Furthermore, traditional methods optimize the model using all the\ntraining samples in each epoch without considering their learning situations,\nwhich leads to a lot of unnecessary training and can not target the difficult\nsamples. To address these issues, we propose a text-based dynamic attention\nmodel named TDAM, which imposes a dynamic attention mechanism on all the\ngenerated words with the motivation to improve the context semantic information\nand enhance the overall control of the whole sentence. Moreover, the text-based\ndynamic attention mechanism and the visual attention mechanism are linked\ntogether to focus on the important words. They can benefit from each other\nduring training. Accordingly, the model is trained through two steps: \"starting\nfrom scratch\" and \"checking for gaps\". The former uses all the samples to\noptimize the model, while the latter only trains for samples with poor control.\nExperimental results on the popular datasets MSVD and MSR-VTT demonstrate that\nour non-ensemble model outperforms the state-of-the-art video captioning\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 15:14:12 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Xiao", "Huanhou", ""], ["Shi", "Jinglun", ""]]}, {"id": "1911.01898", "submitter": "Sergey Pavlov", "authors": "Marina Pominova, Ekaterina Kondrateva, Maksim Sharaev, Sergey Pavlov,\n  Alexander Bernstein, Evgeny Burnaev", "title": "3D Deformable Convolutions for MRI classification", "comments": "Accepted to IEEE International Conference on Machine Learning and\n  Applications (ICMLA 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning convolutional neural networks have proved to be a powerful tool\nfor MRI analysis. In current work, we explore the potential of the deformable\nconvolutional deep neural network layers for MRI data classification. We\npropose new 3D deformable convolutions(d-convolutions), implement them in\nVoxResNet architecture and apply for structural MRI data classification. We\nshow that 3D d-convolutions outperform standard ones and are effective for\nunprocessed 3D MR images being robust to particular geometrical properties of\nthe data. Firstly proposed dVoxResNet architecture exhibits high potential for\nthe use in MRI data classification.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 16:02:10 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Pominova", "Marina", ""], ["Kondrateva", "Ekaterina", ""], ["Sharaev", "Maksim", ""], ["Pavlov", "Sergey", ""], ["Bernstein", "Alexander", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1911.01911", "submitter": "Maximilian Denninger", "authors": "Maximilian Denninger, Martin Sundermeyer, Dominik Winkelbauer, Youssef\n  Zidan, Dmitry Olefir, Mohamad Elbadrawy, Ahsan Lodhi, Harinandan Katam", "title": "BlenderProc", "comments": "7 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BlenderProc is a modular procedural pipeline, which helps in generating real\nlooking images for the training of convolutional neural networks. These can be\nused in a variety of use cases including segmentation, depth, normal and pose\nestimation and many others. A key feature of our extension of blender is the\nsimple to use modular pipeline, which was designed to be easily extendable. By\noffering standard modules, which cover a variety of scenarios, we provide a\nstarting point on which new modules can be created.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 16:53:12 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Denninger", "Maximilian", ""], ["Sundermeyer", "Martin", ""], ["Winkelbauer", "Dominik", ""], ["Zidan", "Youssef", ""], ["Olefir", "Dmitry", ""], ["Elbadrawy", "Mohamad", ""], ["Lodhi", "Ahsan", ""], ["Katam", "Harinandan", ""]]}, {"id": "1911.01915", "submitter": "Pablo Morales-\\'Alvarez", "authors": "Pablo Morales-\\'Alvarez and Pablo Ruiz and Scott Coughlin and Rafael\n  Molina and Aggelos K. Katsaggelos", "title": "Scalable Variational Gaussian Processes for Crowdsourcing: Glitch\n  Detection in LIGO", "comments": "16 pages, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV gr-qc stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last years, crowdsourcing is transforming the way classification\ntraining sets are obtained. Instead of relying on a single expert annotator,\ncrowdsourcing shares the labelling effort among a large number of\ncollaborators. For instance, this is being applied to the data acquired by the\nlaureate Laser Interferometer Gravitational Waves Observatory (LIGO), in order\nto detect glitches which might hinder the identification of true\ngravitational-waves. The crowdsourcing scenario poses new challenging\ndifficulties, as it deals with different opinions from a heterogeneous group of\nannotators with unknown degrees of expertise. Probabilistic methods, such as\nGaussian Processes (GP), have proven successful in modeling this setting.\nHowever, GPs do not scale well to large data sets, which hampers their broad\nadoption in real practice (in particular at LIGO). This has led to the recent\nintroduction of deep learning based crowdsourcing methods, which have become\nthe state-of-the-art. However, the accurate uncertainty quantification of GPs\nhas been partially sacrificed. This is an important aspect for astrophysicists\nin LIGO, since a glitch detection system should provide very accurate\nprobability distributions of its predictions. In this work, we leverage the\nmost popular sparse GP approximation to develop a novel GP based crowdsourcing\nmethod that factorizes into mini-batches. This makes it able to cope with\npreviously-prohibitive data sets. The approach, which we refer to as Scalable\nVariational Gaussian Processes for Crowdsourcing (SVGPCR), brings back GP-based\nmethods to the state-of-the-art, and excels at uncertainty quantification.\nSVGPCR is shown to outperform deep learning based methods and previous\nprobabilistic approaches when applied to the LIGO data. Moreover, its behavior\nand main properties are carefully analyzed in a controlled experiment based on\nthe MNIST data set.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 16:20:38 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Morales-\u00c1lvarez", "Pablo", ""], ["Ruiz", "Pablo", ""], ["Coughlin", "Scott", ""], ["Molina", "Rafael", ""], ["Katsaggelos", "Aggelos K.", ""]]}, {"id": "1911.01971", "submitter": "Elena Limonova", "authors": "Elena Limonova, Daniil Matveev, Dmitry Nikolaev, Vladimir V. Arlazarov", "title": "Bipolar Morphological Neural Networks: Convolution Without\n  Multiplication", "comments": "Submitted to International Conference on Machine Vision 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper we introduce a novel bipolar morphological neuron and bipolar\nmorphological layer models. The models use only such operations as addition,\nsubtraction and maximum inside the neuron and exponent and logarithm as\nactivation functions for the layer. The proposed models unlike previously\nintroduced morphological neural networks approximate the classical computations\nand show better recognition results. We also propose layer-by-layer approach to\ntrain the bipolar morphological networks, which can be further developed to an\nincremental approach for separate neurons to get higher accuracy. Both these\napproaches do not require special training algorithms and can use a variety of\ngradient descent methods. To demonstrate efficiency of the proposed model we\nconsider classical convolutional neural networks and convert the pre-trained\nconvolutional layers to the bipolar morphological layers. Seeing that the\nexperiments on recognition of MNIST and MRZ symbols show only moderate decrease\nof accuracy after conversion and training, bipolar neuron model can provide\nfaster inference and be very useful in mobile and embedded systems.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 17:57:35 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Limonova", "Elena", ""], ["Matveev", "Daniil", ""], ["Nikolaev", "Dmitry", ""], ["Arlazarov", "Vladimir V.", ""]]}, {"id": "1911.02001", "submitter": "Hsin-Ying Lee", "authors": "Hsin-Ying Lee, Xiaodong Yang, Ming-Yu Liu, Ting-Chun Wang, Yu-Ding Lu,\n  Ming-Hsuan Yang, Jan Kautz", "title": "Dancing to Music", "comments": "NeurIPS 2019; Project page: https://github.com/NVlabs/Dancing2Music", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dancing to music is an instinctive move by humans. Learning to model the\nmusic-to-dance generation process is, however, a challenging problem. It\nrequires significant efforts to measure the correlation between music and dance\nas one needs to simultaneously consider multiple aspects, such as style and\nbeat of both music and dance. Additionally, dance is inherently multimodal and\nvarious following movements of a pose at any moment are equally likely. In this\npaper, we propose a synthesis-by-analysis learning framework to generate dance\nfrom music. In the analysis phase, we decompose a dance into a series of basic\ndance units, through which the model learns how to move. In the synthesis\nphase, the model learns how to compose a dance by organizing multiple basic\ndancing movements seamlessly according to the input music. Experimental\nqualitative and quantitative results demonstrate that the proposed method can\nsynthesize realistic, diverse,style-consistent, and beat-matching dances from\nmusic.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 18:56:15 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Lee", "Hsin-Ying", ""], ["Yang", "Xiaodong", ""], ["Liu", "Ming-Yu", ""], ["Wang", "Ting-Chun", ""], ["Lu", "Yu-Ding", ""], ["Yang", "Ming-Hsuan", ""], ["Kautz", "Jan", ""]]}, {"id": "1911.02014", "submitter": "Zhanghexuan Ji", "authors": "Zhanghexuan Ji, Yan Shen, Chunwei Ma, Mingchen Gao", "title": "Scribble-based Hierarchical Weakly Supervised Learning for Brain Tumor\n  Segmentation", "comments": "22nd International Conference on Medical Image Computing and Computer\n  Assisted Intervention (MICCAI 2019) Accept", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent state-of-the-art deep learning methods have significantly improved\nbrain tumor segmentation. However, fully supervised training requires a large\namount of manually labeled masks, which is highly time-consuming and needs\ndomain expertise. Weakly supervised learning with scribbles provides a good\ntrade-off between model accuracy and the effort of manual labeling. However,\nfor segmenting the hierarchical brain tumor structures, manually labeling\nscribbles for each substructure could still be demanding. In this paper, we use\nonly two kinds of weak labels, i.e., scribbles on whole tumor and healthy brain\ntissue, and global labels for the presence of each substructure, to train a\ndeep learning model to segment all the sub-regions. Specifically, we train two\nnetworks in two phases: first, we only use whole tumor scribbles to train a\nwhole tumor (WT) segmentation network, which roughly recovers the WT mask of\ntraining data; then we cluster the WT region with the guide of global labels.\nThe rough substructure segmentation from clustering is used as weak labels to\ntrain the second network. The dense CRF loss is used to refine the weakly\nsupervised segmentation. We evaluate our approach on the BraTS2017 dataset and\nachieve competitive WT dice score as well as comparable scores on substructure\nsegmentation compared to an upper bound when trained with fully annotated\nmasks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 16:56:35 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Ji", "Zhanghexuan", ""], ["Shen", "Yan", ""], ["Ma", "Chunwei", ""], ["Gao", "Mingchen", ""]]}, {"id": "1911.02050", "submitter": "Tae Ha Park", "authors": "Mate Kisantal, Sumant Sharma, Tae Ha Park, Dario Izzo, Marcus\n  M\\\"artens, Simone D'Amico", "title": "Satellite Pose Estimation Challenge: Dataset, Competition Design and\n  Results", "comments": "Accepted to IEEE Transactions on Aerospace and Electronic Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable pose estimation of uncooperative satellites is a key technology for\nenabling future on-orbit servicing and debris removal missions. The Kelvins\nSatellite Pose Estimation Challenge aims at evaluating and comparing monocular\nvision-based approaches and pushing the state-of-the-art on this problem. This\nwork is based on the Satellite Pose Estimation Dataset, the first publicly\navailable machine learning set of synthetic and real spacecraft imageries. The\nchoice of dataset reflects one of the unique challenges associated with\nspaceborne computer vision tasks, namely the lack of spaceborne images to train\nand validate the developed algorithms. This work briefly reviews the basic\nproperties and the collection process of the dataset which was made publicly\navailable. The competition design, including the definition of performance\nmetrics and the adopted testbed, is also discussed. The main contribution of\nthis paper is the analysis of the submissions of the 48 competitors, which\ncompares the performance of different approaches and uncovers what factors make\nthe satellite pose estimation problem especially challenging.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 19:29:18 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 18:50:25 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Kisantal", "Mate", ""], ["Sharma", "Sumant", ""], ["Park", "Tae Ha", ""], ["Izzo", "Dario", ""], ["M\u00e4rtens", "Marcus", ""], ["D'Amico", "Simone", ""]]}, {"id": "1911.02052", "submitter": "Zhisheng Xiao", "authors": "Zhisheng Xiao, Qing Yan, Yali Amit", "title": "A Method to Model Conditional Distributions with Normalizing Flows", "comments": "10 pages. Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate the use of normalizing flows to model\nconditional distributions. In particular, we use our proposed method to analyze\ninverse problems with invertible neural networks by maximizing the posterior\nlikelihood. Our method uses only a single loss and is easy to train. This is an\nimprovement on the previous method that solves similar inverse problems with\ninvertible neural networks but which involves a combination of several loss\nterms with ad-hoc weighting. In addition, our method provides a natural\nframework to incorporate conditioning in normalizing flows, and therefore, we\ncan train an invertible network to perform conditional generation. We analyze\nour method and perform a careful comparison with previous approaches. Simple\nexperiments show the effectiveness of our method, and more comprehensive\nexperimental evaluations are undergoing.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 19:37:37 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Xiao", "Zhisheng", ""], ["Yan", "Qing", ""], ["Amit", "Yali", ""]]}, {"id": "1911.02054", "submitter": "Xingchao Peng", "authors": "Xingchao Peng, Zijun Huang, Yizhe Zhu, Kate Saenko", "title": "Federated Adversarial Domain Adaptation", "comments": "Published as a conference paper at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning improves data privacy and efficiency in machine learning\nperformed over networks of distributed devices, such as mobile phones, IoT and\nwearable devices, etc. Yet models trained with federated learning can still\nfail to generalize to new devices due to the problem of domain shift. Domain\nshift occurs when the labeled data collected by source nodes statistically\ndiffers from the target node's unlabeled data. In this work, we present a\nprincipled approach to the problem of federated domain adaptation, which aims\nto align the representations learned among the different nodes with the data\ndistribution of the target node. Our approach extends adversarial adaptation\ntechniques to the constraints of the federated setting. In addition, we devise\na dynamic attention mechanism and leverage feature disentanglement to enhance\nknowledge transfer. Empirically, we perform extensive experiments on several\nimage and text classification tasks and show promising results under\nunsupervised federated domain adaptation setting.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 19:45:49 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2019 22:03:36 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Peng", "Xingchao", ""], ["Huang", "Zijun", ""], ["Zhu", "Yizhe", ""], ["Saenko", "Kate", ""]]}, {"id": "1911.02088", "submitter": "Gregory Meyer", "authors": "Gregory P. Meyer", "title": "An Alternative Probabilistic Interpretation of the Huber Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Huber loss is a robust loss function used for a wide range of regression\ntasks. To utilize the Huber loss, a parameter that controls the transitions\nfrom a quadratic function to an absolute value function needs to be selected.\nWe believe the standard probabilistic interpretation that relates the Huber\nloss to the Huber density fails to provide adequate intuition for identifying\nthe transition point. As a result, a hyper-parameter search is often necessary\nto determine an appropriate value. In this work, we propose an alternative\nprobabilistic interpretation of the Huber loss, which relates minimizing the\nloss to minimizing an upper-bound on the Kullback-Leibler divergence between\nLaplace distributions, where one distribution represents the noise in the\nground-truth and the other represents the noise in the prediction. In addition,\nwe show that the parameters of the Laplace distributions are directly related\nto the transition point of the Huber loss. We demonstrate, through a toy\nproblem, that the optimal transition point of the Huber loss is closely related\nto the distribution of the noise in the ground-truth data. As a result, our\ninterpretation provides an intuitive way to identify well-suited\nhyper-parameters by approximating the amount of noise in the data, which we\ndemonstrate through a case study and experimentation on the Faster R-CNN and\nRetinaNet object detectors.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 21:15:19 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 19:23:10 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2020 19:27:22 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Meyer", "Gregory P.", ""]]}, {"id": "1911.02103", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Alba Herrera-Palacio, Carles Ventura, Carina Silberer, Ionut-Teodor\n  Sorodoc, Gemma Boleda and Xavier Giro-i-Nieto", "title": "Recurrent Instance Segmentation using Sequences of Referring Expressions", "comments": "3rd NeurIPS Workshop on Visually Grounded Interaction and Language\n  (ViGIL, 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this work is to segment the objects in an image that are referred\nto by a sequence of linguistic descriptions (referring expressions). We propose\na deep neural network with recurrent layers that output a sequence of binary\nmasks, one for each referring expression provided by the user. The recurrent\nlayers in the architecture allow the model to condition each predicted mask on\nthe previous ones, from a spatial perspective within the same image. Our\nmultimodal approach uses off-the-shelf architectures to encode both the image\nand the referring expressions. The visual branch provides a tensor of pixel\nembeddings that are concatenated with the phrase embeddings produced by a\nlanguage encoder. Our experiments on the RefCOCO dataset for still images\nindicate how the proposed architecture successfully exploits the sequences of\nreferring expressions to solve a pixel-wise task of instance segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 21:49:55 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Herrera-Palacio", "Alba", ""], ["Ventura", "Carles", ""], ["Silberer", "Carina", ""], ["Sorodoc", "Ionut-Teodor", ""], ["Boleda", "Gemma", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1911.02133", "submitter": "Farley Lai", "authors": "Farley Lai, Ning Xie, Derek Doran and Asim Kadav", "title": "Contextual Grounding of Natural Language Entities in Images", "comments": "Accepted to NeurIPS 2019 workshop on Visually Grounded Interaction\n  and Language (ViGIL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a contextual grounding approach that captures the\ncontext in corresponding text entities and image regions to improve the\ngrounding accuracy. Specifically, the proposed architecture accepts pre-trained\ntext token embeddings and image object features from an off-the-shelf object\ndetector as input. Additional encoding to capture the positional and spatial\ninformation can be added to enhance the feature quality. There are separate\ntext and image branches facilitating respective architectural refinements for\ndifferent modalities. The text branch is pre-trained on a large-scale masked\nlanguage modeling task while the image branch is trained from scratch. Next,\nthe model learns the contextual representations of the text tokens and image\nobjects through layers of high-order interaction respectively. The final\ngrounding head ranks the correspondence between the textual and visual\nrepresentations through cross-modal interaction. In the evaluation, we show\nthat our model achieves the state-of-the-art grounding accuracy of 71.36% over\nthe Flickr30K Entities dataset. No additional pre-training is necessary to\ndeliver competitive results compared with related work that often requires\ntask-agnostic and task-specific pre-training on cross-modal dadasets. The\nimplementation is publicly available at https://gitlab.com/necla-ml/grounding.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 23:23:58 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Lai", "Farley", ""], ["Xie", "Ning", ""], ["Doran", "Derek", ""], ["Kadav", "Asim", ""]]}, {"id": "1911.02155", "submitter": "James Murphy", "authors": "James M. Murphy", "title": "Spatially regularized active diffusion learning for high-dimensional\n  images", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An active learning algorithm for the classification of high-dimensional\nimages is proposed in which spatially-regularized nonlinear diffusion geometry\nis used to characterize cluster cores. The proposed method samples from\nestimated cluster cores in order to generate a small but potent set of training\nlabels which propagate to the remainder of the dataset via the underlying\ndiffusion process. By spatially regularizing the rich, high-dimensional\nspectral information of the image to efficiently estimate the most significant\nand influential points in the data, our approach avoids redundancy in the\ntraining dataset. This allows it to produce high-accuracy labelings with a very\nsmall number of training labels. The proposed algorithm admits an efficient\nnumerical implementation that scales essentially linearly in the number of data\npoints under a suitable data model and enjoys state-of-the-art performance on\nreal hyperspectral images.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 00:58:24 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Murphy", "James M.", ""]]}, {"id": "1911.02163", "submitter": "Xiao Sun", "authors": "Xiao Sun, Zhouhui Lian, Jianguo Xiao", "title": "SRINet: Learning Strictly Rotation-Invariant Representations for Point\n  Cloud Classification and Segmentation", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud analysis has drawn broader attentions due to its increasing\ndemands in various fields. Despite the impressive performance has been achieved\non several databases, researchers neglect the fact that the orientation of\nthose point cloud data is aligned. Varying the orientation of point cloud may\nlead to the degradation of performance, restricting the capacity of\ngeneralizing to real applications where the prior of orientation is often\nunknown. In this paper, we propose the point projection feature, which is\ninvariant to the rotation of the input point cloud. A novel architecture is\ndesigned to mine features of different levels. We adopt a PointNet-based\nbackbone to extract global feature for point cloud, and the graph aggregation\noperation to perceive local shape structure. Besides, we introduce an efficient\nkey point descriptor to assign each point with different response and help\nrecognize the overall geometry. Mathematical analyses and experimental results\ndemonstrate that the proposed method can extract strictly rotation-invariant\nrepresentations for point cloud recognition and segmentation without data\naugmentation, and outperforms other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 02:08:03 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Sun", "Xiao", ""], ["Lian", "Zhouhui", ""], ["Xiao", "Jianguo", ""]]}, {"id": "1911.02172", "submitter": "C.-H. Huck Yang", "authors": "Yi-Chieh Liu, Yung-An Hsieh, Min-Hung Chen, Chao-Han Huck Yang, Jesper\n  Tegner, Yi-Chang James Tsai", "title": "Interpretable Self-Attention Temporal Reasoning for Driving Behavior\n  Understanding", "comments": "Submitted to IEEE ICASSP 2020; Pytorch code will be released soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Performing driving behaviors based on causal reasoning is essential to ensure\ndriving safety. In this work, we investigated how state-of-the-art 3D\nConvolutional Neural Networks (CNNs) perform on classifying driving behaviors\nbased on causal reasoning. We proposed a perturbation-based visual explanation\nmethod to inspect the models' performance visually. By examining the video\nattention saliency, we found that existing models could not precisely capture\nthe causes (e.g., traffic light) of the specific action (e.g., stopping).\nTherefore, the Temporal Reasoning Block (TRB) was proposed and introduced to\nthe models. With the TRB models, we achieved the accuracy of $\\mathbf{86.3\\%}$,\nwhich outperform the state-of-the-art 3D CNNs from previous works. The\nattention saliency also demonstrated that TRB helped models focus on the causes\nmore precisely. With both numerical and visual evaluations, we concluded that\nour proposed TRB models were able to provide accurate driving behavior\nprediction by learning the causal reasoning of the behaviors.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 02:49:30 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Liu", "Yi-Chieh", ""], ["Hsieh", "Yung-An", ""], ["Chen", "Min-Hung", ""], ["Yang", "Chao-Han Huck", ""], ["Tegner", "Jesper", ""], ["Tsai", "Yi-Chang James", ""]]}, {"id": "1911.02202", "submitter": "Mikhail Kopeliovich", "authors": "Mikhail Kopeliovich, Yuriy Mironenko, Mikhail Petrushan", "title": "Architectural Tricks for Deep Learning in Remote Photoplethysmography", "comments": "Workshop on Computer Vision for Physiological Measurement, ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Architectural improvements are studied for convolutional network performing\nestimation of heart rate (HR) values on color signal patches. Color signals are\ntime series of color components averaged over facial regions recorded by\nwebcams in two scenarios: Stationary (without motion of a person) and Mixed\nMotion (different motion patterns of a person). HR estimation problem is\naddressed as a classification task, where classes correspond to different heart\nrate values within the admissible range of [40; 125] bpm. Both adding\nconvolutional filtering layers after fully connected layers and involving\ncombined loss function where first component is a cross entropy and second is a\nsquared error between the network output and smoothed one-hot vector, lead to\nbetter performance of HR estimation model in Stationary and Mixed Motion\nscenarios.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 05:07:38 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Kopeliovich", "Mikhail", ""], ["Mironenko", "Yuriy", ""], ["Petrushan", "Mikhail", ""]]}, {"id": "1911.02222", "submitter": "Priyansh Saxena", "authors": "Vaishnav Chandak, Priyansh Saxena, Manisha Pattanaik and Gaurav\n  Kaushal", "title": "Semantic Image Completion and Enhancement using Deep Learning", "comments": "6 pages, 8 figures. Proceedings of \"The 10th International Conference\n  on Computing, Communication and Networking Technologies (ICCCNT)\". Conference\n  Proceedings ISBN Number: ISBN: 978-1-5386-5906-9; Link:\n  https://ieeexplore.ieee.org/document/8944750", "journal-ref": null, "doi": "10.1109/ICCCNT45670.2019.8944750", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-life applications, certain images utilized are corrupted in which the\nimage pixels are damaged or missing, which increases the complexity of computer\nvision tasks. In this paper, a deep learning architecture is proposed to deal\nwith image completion and enhancement. Generative Adversarial Networks (GAN),\nhas been turned out to be helpful in picture completion tasks. Therefore, in\nGANs, Wasserstein GAN architecture is used for image completion which creates\nthe coarse patches to filling the missing region in the distorted picture, and\nthe enhancement network will additionally refine the resultant pictures\nutilizing residual learning procedures and hence give better complete pictures\nfor computer vision applications. Experimental outcomes show that the proposed\napproach improves the Peak Signal to Noise ratio and Structural Similarity\nIndex values by 2.45% and 4% respectively when compared to the recently\nreported data.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 06:47:08 GMT"}, {"version": "v2", "created": "Sun, 5 Jan 2020 07:07:53 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Chandak", "Vaishnav", ""], ["Saxena", "Priyansh", ""], ["Pattanaik", "Manisha", ""], ["Kaushal", "Gaurav", ""]]}, {"id": "1911.02237", "submitter": "Zihao Xie", "authors": "Zihao Xie, Wenbing Tao, Li Zhu, Lin Zhao", "title": "Localization-aware Channel Pruning for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Channel pruning is one of the important methods for deep model compression.\nMost of existing pruning methods mainly focus on classification. Few of them\nconduct systematic research on object detection. However, object detection is\ndifferent from classification, which requires not only semantic information but\nalso localization information. In this paper, based on discrimination-aware\nchannel pruning (DCP) which is state-of-the-art pruning method for\nclassification, we propose a localization-aware auxiliary network to find out\nthe channels with key information for classification and regression so that we\ncan conduct channel pruning directly for object detection, which saves lots of\ntime and computing resources. In order to capture the localization information,\nwe first design the auxiliary network with a contextual ROIAlign layer which\ncan obtain precise localization information of the default boxes by pixel\nalignment and enlarges the receptive fields of the default boxes when pruning\nshallow layers. Then, we construct a loss function for object detection task\nwhich tends to keep the channels that contain the key information for\nclassification and regression. Extensive experiments demonstrate the\neffectiveness of our method. On MS COCO, we prune 70\\% parameters of the SSD\nbased on ResNet-50 with modest accuracy drop, which outperforms\nthe-state-of-art method.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 07:41:21 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 00:52:35 GMT"}, {"version": "v3", "created": "Sat, 29 Feb 2020 08:29:03 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Xie", "Zihao", ""], ["Tao", "Wenbing", ""], ["Zhu", "Li", ""], ["Zhao", "Lin", ""]]}, {"id": "1911.02265", "submitter": "Priyansh Saxena", "authors": "Priyansh Saxena, Akshat Maheshwari, Shivani Tayal and Saumil\n  Maheshwari", "title": "Predictive modeling of brain tumor: A Deep learning approach", "comments": "An author was incorrectly added and the research is still in\n  progress. Will add the paper soon as soon as the work is complete. Shivani\n  Tayal was mistakenly added as an author due to some trivial error by the\n  uploader", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image processing concepts can visualize the different anatomy structure of\nthe human body. Recent advancements in the field of deep learning have made it\npossible to detect the growth of cancerous tissue just by a patient's brain\nMagnetic Resonance Imaging (MRI) scans. These methods require very high\naccuracy and meager false negative rates to be of any practical use. This paper\npresents a Convolutional Neural Network (CNN) based transfer learning approach\nto classify the brain MRI scans into two classes using three pre-trained\nmodels. The performances of these models are compared with each other.\nExperimental results show that the Resnet-50 model achieves the highest\naccuracy and least false negative rates as 95% and zero respectively. It is\nfollowed by VGG-16 and Inception-V3 model with an accuracy of 90% and 55%\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 09:27:48 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 12:04:18 GMT"}, {"version": "v3", "created": "Thu, 19 Dec 2019 05:48:24 GMT"}, {"version": "v4", "created": "Mon, 9 Mar 2020 07:16:18 GMT"}, {"version": "v5", "created": "Wed, 19 May 2021 15:10:59 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Saxena", "Priyansh", ""], ["Maheshwari", "Akshat", ""], ["Tayal", "Shivani", ""], ["Maheshwari", "Saumil", ""]]}, {"id": "1911.02274", "submitter": "Ahmed Ben Saad", "authors": "Ahmed Ben Saad, Youssef Tamaazousti, Josselin Kherroubi and Alexis He", "title": "Where is the Fake? Patch-Wise Supervised GANs for Texture Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of texture inpainting where the input images are\ntextures with missing values along with masks that indicate the zones that\nshould be generated. Many works have been done in image inpainting with the aim\nto achieve global and local consistency. But these works still suffer from\nlimitations when dealing with textures. In fact, the local information in the\nimage to be completed needs to be used in order to achieve local continuities\nand visually realistic texture inpainting. For this, we propose a new segmentor\ndiscriminator that performs a patch-wise real/fake classification and is\nsupervised by input masks. During training, it aims to locate the fake and thus\nbackpropagates consistent signal to the generator. We tested our approach on\nthe publicly available DTD dataset and showed that it achieves state-of-the-art\nperformances and better deals with local consistency than existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 09:43:45 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 10:22:25 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Saad", "Ahmed Ben", ""], ["Tamaazousti", "Youssef", ""], ["Kherroubi", "Josselin", ""], ["He", "Alexis", ""]]}, {"id": "1911.02278", "submitter": "Jeroen Bertels", "authors": "Jeroen Bertels, David Robben, Dirk Vandermeulen, Paul Suetens", "title": "Optimization with soft Dice can lead to a volumetric bias", "comments": "BrainLes Workshop - MICCAI 2019", "journal-ref": "LNCS 11992, Springer Nature Switzerland AG 2019", "doi": "10.1007/978-3-030-46640-4_9", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation is a fundamental task in medical image analysis. The clinical\ninterest is often to measure the volume of a structure. To evaluate and compare\nsegmentation methods, the similarity between a segmentation and a predefined\nground truth is measured using metrics such as the Dice score. Recent\nsegmentation methods based on convolutional neural networks use a\ndifferentiable surrogate of the Dice score, such as soft Dice, explicitly as\nthe loss function during the learning phase. Even though this approach leads to\nimproved Dice scores, we find that, both theoretically and empirically on four\nmedical tasks, it can introduce a volumetric bias for tasks with high inherent\nuncertainty. As such, this may limit the method's clinical applicability.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 09:52:56 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Bertels", "Jeroen", ""], ["Robben", "David", ""], ["Vandermeulen", "Dirk", ""], ["Suetens", "Paul", ""]]}, {"id": "1911.02285", "submitter": "Anand Sahadevan S", "authors": "Anand S Sahadevan, Arundhati Misra and Praveen Gupta", "title": "Spatial Feature Extraction in Airborne Hyperspectral Images Using Local\n  Spectral Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local spectral similarity (LSS) algorithm has been developed for detecting\nhomogeneous areas and edges in hyperspectral images (HSIs). The proposed\nalgorithm transforms the 3-D data cube (within a spatial window) into a\nspectral similarity matrix by calculating the vector-similarity between the\ncenter pixel-spectrum and the neighborhood spectra. The final edge intensity is\nderived upon order statistics of the similarity matrix or spatial convolution\nof the similarity matrix with the spatial kernels. The LSS algorithm\nfacilitates simultaneous use of spectral-spatial information for the edge\ndetection by considering the spatial pattern of similar spectra within a\nspatial window. The proposed edge-detection method is tested on benchmark HSIs\nas well as the image obtained from\nAirborne-Visible-and-Infra-RedImaging-Spectrometer-Next-Generation (AVIRIS-NG).\nRobustness of the LSS method against multivariate Gaussian noise and low\nspatial resolution scenarios were also verified with the benchmark HSIs.\nFigure-of-merit, false-alarm-count and miss-count were applied to evaluate the\nperformance of edge detection methods. Results showed that Fractional distance\nmeasure and Euclidean distance measure were able to detect the edges in HSIs\nmore precisely as compared to other spectral similarity measures. The proposed\nmethod can be applied to radiance and reflectance data (whole spectrum) and it\nhas shown good performance on principal component images as well. In addition,\nthe proposed algorithm outperforms the traditional multichannel edge detectors\nin terms of both fastness, accuracy and the robustness. The experimental\nresults also confirm that LSS can be applied as a pre-processing approach to\nreduce the errors in clustering as well as classification outputs.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 10:11:41 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Sahadevan", "Anand S", ""], ["Misra", "Arundhati", ""], ["Gupta", "Praveen", ""]]}, {"id": "1911.02286", "submitter": "Marlon Marcon", "authors": "Marlon Marcon and Riccardo Spezialetti and Samuele Salti and Luciano\n  Silva and Luigi Di Stefano", "title": "Boosting Object Recognition in Point Clouds by Saliency Detection", "comments": "International Conference on Image Analysis and Processing (ICIAP)\n  2019", "journal-ref": null, "doi": "10.1007/978-3-030-30754-7_32", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Object recognition in 3D point clouds is a challenging task, mainly when time\nis an important factor to deal with, such as in industrial applications. Local\ndescriptors are an amenable choice whenever the 6 DoF pose of recognized\nobjects should also be estimated. However, the pipeline for this kind of\ndescriptors is highly time-consuming. In this work, we propose an update to the\ntraditional pipeline, by adding a preliminary filtering stage referred to as\nsaliency boost. We perform tests on a standard object recognition benchmark by\nconsidering four keypoint detectors and four local descriptors, in order to\ncompare time and recognition performance between the traditional pipeline and\nthe boosted one. Results on time show that the boosted pipeline could turn out\nup to 5 times faster, with the recognition rate improving in most of the cases\nand exhibiting only a slight decrease in the others. These results suggest that\nthe boosted pipeline can speed-up processing time substantially with limited\nimpacts or even benefits in recognition accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 10:15:03 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Marcon", "Marlon", ""], ["Spezialetti", "Riccardo", ""], ["Salti", "Samuele", ""], ["Silva", "Luciano", ""], ["Di Stefano", "Luigi", ""]]}, {"id": "1911.02322", "submitter": "Nils Gessert", "authors": "Nils Gessert and Marcel Bengs and Alexander Schlaefer", "title": "Melanoma detection with electrical impedance spectroscopy and dermoscopy\n  using joint deep learning models", "comments": "Accepted at SPIE Medical Imaging 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The initial assessment of skin lesions is typically based on dermoscopic\nimages. As this is a difficult and time-consuming task, machine learning\nmethods using dermoscopic images have been proposed to assist human experts.\nOther approaches have studied electrical impedance spectroscopy (EIS) as a\nbasis for clinical decision support systems. Both methods represent different\nways of measuring skin lesion properties as dermoscopy relies on visible light\nand EIS uses electric currents. Thus, the two methods might carry complementary\nfeatures for lesion classification. Therefore, we propose joint deep learning\nmodels considering both EIS and dermoscopy for melanoma detection. For this\npurpose, we first study machine learning methods for EIS that incorporate\ndomain knowledge and previously used heuristics into the design process. As a\nresult, we propose a recurrent model with state-max-pooling which automatically\nlearns the relevance of different EIS measurements. Second, we combine this new\nmodel with different convolutional neural networks that process dermoscopic\nimages. We study ensembling approaches and also propose a cross-attention\nmodule guiding information exchange between the EIS and dermoscopy model. In\ngeneral, combinations of EIS and dermoscopy clearly outperform models that only\nuse either EIS or dermoscopy. We show that our attention-based, combined model\noutperforms other models with specificities of 34.4% (CI 31.3-38.4), 34.7% (CI\n31.0-38.8) and 53.7% (CI 50.1-57.6) for dermoscopy, EIS and the combined model,\nrespectively, at a clinically relevant sensitivity of 98%.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 11:39:12 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 14:28:04 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Gessert", "Nils", ""], ["Bengs", "Marcel", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "1911.02357", "submitter": "Paul Bergmann", "authors": "Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger", "title": "Uninformed Students: Student-Teacher Anomaly Detection with\n  Discriminative Latent Embeddings", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": "10.1109/CVPR42600.2020.00424", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a powerful student-teacher framework for the challenging problem\nof unsupervised anomaly detection and pixel-precise anomaly segmentation in\nhigh-resolution images. Student networks are trained to regress the output of a\ndescriptive teacher network that was pretrained on a large dataset of patches\nfrom natural images. This circumvents the need for prior data annotation.\nAnomalies are detected when the outputs of the student networks differ from\nthat of the teacher network. This happens when they fail to generalize outside\nthe manifold of anomaly-free training data. The intrinsic uncertainty in the\nstudent networks is used as an additional scoring function that indicates\nanomalies. We compare our method to a large number of existing deep learning\nbased methods for unsupervised anomaly detection. Our experiments demonstrate\nimprovements over state-of-the-art methods on a number of real-world datasets,\nincluding the recently introduced MVTec Anomaly Detection dataset that was\nspecifically designed to benchmark anomaly segmentation algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 13:11:13 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 17:53:16 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Bergmann", "Paul", ""], ["Fauser", "Michael", ""], ["Sattlegger", "David", ""], ["Steger", "Carsten", ""]]}, {"id": "1911.02360", "submitter": "Zhaoxia Yin", "authors": "Zhaoxia Yin, Hua Wang, Li Chen, Jie Wang and Weiming Zhang", "title": "Reversible Adversarial Attack based on Reversible Image Transformation", "comments": "2021 International Workshop on Safety & Security of Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to prevent illegal or unauthorized access of image data such as\nhuman faces and ensure legitimate users can use authorization-protected data,\nreversible adversarial attack technique is rise. Reversible adversarial\nexamples (RAE) get both attack capability and reversibility at the same time.\nHowever, the existing technique can not meet application requirements because\nof serious distortion and failure of image recovery when adversarial\nperturbations get strong. In this paper, we take advantage of Reversible Image\nTransformation technique to generate RAE and achieve reversible adversarial\nattack. Experimental results show that proposed RAE generation scheme can\nensure imperceptible image distortion and the original image can be\nreconstructed error-free. What's more, both the attack ability and the image\nquality are not limited by the perturbation amplitude.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 13:15:32 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 12:04:48 GMT"}, {"version": "v3", "created": "Thu, 5 Dec 2019 14:21:11 GMT"}, {"version": "v4", "created": "Sun, 10 May 2020 15:49:06 GMT"}, {"version": "v5", "created": "Tue, 9 Mar 2021 07:55:50 GMT"}, {"version": "v6", "created": "Wed, 12 May 2021 05:55:19 GMT"}, {"version": "v7", "created": "Tue, 25 May 2021 15:11:06 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Yin", "Zhaoxia", ""], ["Wang", "Hua", ""], ["Chen", "Li", ""], ["Wang", "Jie", ""], ["Zhang", "Weiming", ""]]}, {"id": "1911.02396", "submitter": "Homayoun Valafar", "authors": "Ryan Yandle, Rishi Mukhopadhyay, Homayoun Valafar", "title": "Using Residual Dipolar Couplings from Two Alignment Media to Detect\n  Structural Homology", "comments": "BioComp 2009, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of Probability Density Profile Analysis has been introduced\npreviously as a tool to find the best match between a set of experimentally\ngenerated Residual Dipolar Couplings and a set of known protein structures.\nWhile it proved effective on small databases in identifying protein fold\nfamilies, and for picking the best result from computational protein folding\ntool ROBETTA, for larger data sets, more data is required. Here, the method of\n2-D Probability Density Profile Analysis is presented which incorporates paired\nRDC data from 2 alignment media for N-H vectors. The method was tested using\nsynthetic RDC data generated with +/-1 Hz error. The results show that the\naddition of information from a second alignment medium makes 2-D PDPA a much\nmore effective tool that is able to identify a structure from a database of 600\nprotein fold family representatives.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 14:00:23 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Yandle", "Ryan", ""], ["Mukhopadhyay", "Rishi", ""], ["Valafar", "Homayoun", ""]]}, {"id": "1911.02404", "submitter": "Junfeng Hu", "authors": "Junfeng Hu, Zhencheng Fan, Jun Liao, Li Liu", "title": "Predicting Long-Term Skeletal Motions by a Spatio-Temporal Hierarchical\n  Recurrent Network", "comments": "Accepted by the 24th European Conference on Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary goal of skeletal motion prediction is to generate future motion\nby observing a sequence of 3D skeletons. A key challenge in motion prediction\nis the fact that a motion can often be performed in several different ways,\nwith each consisting of its own configuration of poses and their\nspatio-temporal dependencies, and as a result, the predicted poses often\nconverge to the motionless poses or non-human like motions in long-term\nprediction. This leads us to define a hierarchical recurrent network model that\nexplicitly characterizes these internal configurations of poses and their local\nand global spatio-temporal dependencies. The model introduces a latent vector\nvariable from the Lie algebra to represent spatial and temporal relations\nsimultaneously. Furthermore, a structured stack LSTM-based decoder is devised\nto decode the predicted poses with a new loss function defined to estimate the\nquantized weight of each body part in a pose. Empirical evaluations on\nbenchmark datasets suggest our approach significantly outperforms the\nstate-of-the-art methods on both short-term and long-term motion prediction.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 14:07:21 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 03:55:09 GMT"}, {"version": "v3", "created": "Mon, 17 Feb 2020 10:44:42 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Hu", "Junfeng", ""], ["Fan", "Zhencheng", ""], ["Liao", "Jun", ""], ["Liu", "Li", ""]]}, {"id": "1911.02407", "submitter": "Andrew Gilbert", "authors": "Andrew Gilbert, Marit Holden, Line Eikvil, Mariia Rakhmail, Aleksandar\n  Babic, Svein Arne Aase, Eigil Samset and Kristin McLeod", "title": "Doppler Spectrum Classification with CNNs via Heatmap Location Encoding\n  and a Multi-head Output Layer", "comments": "copyright 2019 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral Doppler measurements are an important part of the standard\nechocardiographic examination. These measurements give important insight into\nmyocardial motion and blood flow providing clinicians with parameters for\ndiagnostic decision making. Many of these measurements can currently be\nperformed automatically with high accuracy, increasing the efficiency of the\ndiagnostic pipeline. However, full automation is not yet available because the\nuser must manually select which measurement should be performed on each image.\nIn this work we develop a convolutional neural network (CNN) to automatically\nclassify cardiac Doppler spectra into measurement classes. We show how the\nmulti-modal information in each spectral Doppler recording can be combined\nusing a meta parameter post-processing mapping scheme and heatmaps to encode\ncoordinate locations. Additionally, we experiment with several state-of-the-art\nnetwork architectures to examine the tradeoff between accuracy and memory usage\nfor resource-constrained environments. Finally, we propose a confidence metric\nusing the values in the last fully connected layer of the network. We analyze\nexample images that fall outside of our proposed classes to show our confidence\nmetric can prevent many misclassifications. Our algorithm achieves 96% accuracy\non a test set drawn from a separate clinical site, indicating that the proposed\nmethod is suitable for clinical adoption and enabling a fully automatic\npipeline from acquisition to Doppler spectrum measurements.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 14:24:21 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 10:13:13 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Gilbert", "Andrew", ""], ["Holden", "Marit", ""], ["Eikvil", "Line", ""], ["Rakhmail", "Mariia", ""], ["Babic", "Aleksandar", ""], ["Aase", "Svein Arne", ""], ["Samset", "Eigil", ""], ["McLeod", "Kristin", ""]]}, {"id": "1911.02448", "submitter": "Andrew Gilbert", "authors": "Andrew Gilbert, Marit Holden, Line Eikvil, Svein Arne Aase, Eigil\n  Samset, Kristin McLeod", "title": "Automated Left Ventricle Dimension Measurement in 2D Cardiac Ultrasound\n  via an Anatomically Meaningful CNN Approach", "comments": "Best paper award at Smart Ultrasound Imaging Workshop (SUSI) MICCAI\n  2019", "journal-ref": "Smart Ultrasound Imaging and Perinatal, Preterm and Paediatric\n  Image Analysis, LNCS 11978, pp. 29-37, Springer, 2019", "doi": "10.1007/978-3-030-32875-7_4", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-dimensional echocardiography (2DE) measurements of left ventricle (LV)\ndimensions are highly significant markers of several cardiovascular diseases.\nThese measurements are often used in clinical care despite suffering from large\nvariability between observers. This variability is due to the challenging\nnature of accurately finding the correct temporal and spatial location of\nmeasurement endpoints in ultrasound images. These images often contain fuzzy\nboundaries and varying reflection patterns between frames. In this work, we\npresent a convolutional neural network (CNN) based approach to automate 2DE LV\nmeasurements. Treating the problem as a landmark detection problem, we propose\na modified U-Net CNN architecture to generate heatmaps of likely coordinate\nlocations. To improve the network performance we use anatomically meaningful\nheatmaps as labels and train with a multi-component loss function. Our network\nachieves 13.4%, 6%, and 10.8% mean percent error on intraventricular septum\n(IVS), LV internal dimension (LVID), and LV posterior wall (LVPW) measurements\nrespectively. The design outperforms other networks and matches or approaches\nintra-analyser expert error.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 15:50:06 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Gilbert", "Andrew", ""], ["Holden", "Marit", ""], ["Eikvil", "Line", ""], ["Aase", "Svein Arne", ""], ["Samset", "Eigil", ""], ["McLeod", "Kristin", ""]]}, {"id": "1911.02466", "submitter": "Zhengyu Zhao", "authors": "Zhengyu Zhao, Zhuoran Liu, Martha Larson", "title": "Towards Large yet Imperceptible Adversarial Image Perturbations with\n  Perceptual Color Distance", "comments": "Accepted at CVPR 2020; Code is available at\n  https://github.com/ZhengyuZhao/PerC-Adversarial", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The success of image perturbations that are designed to fool image classifier\nis assessed in terms of both adversarial effect and visual imperceptibility.\nThe conventional assumption on imperceptibility is that perturbations should\nstrive for tight $L_p$-norm bounds in RGB space. In this work, we drop this\nassumption by pursuing an approach that exploits human color perception, and\nmore specifically, minimizing perturbation size with respect to perceptual\ncolor distance. Our first approach, Perceptual Color distance C&W (PerC-C&W),\nextends the widely-used C&W approach and produces larger RGB perturbations.\nPerC-C&W is able to maintain adversarial strength, while contributing to\nimperceptibility. Our second approach, Perceptual Color distance Alternating\nLoss (PerC-AL), achieves the same outcome, but does so more efficiently by\nalternating between the classification loss and perceptual color difference\nwhen updating perturbations. Experimental evaluation shows PerC approaches\noutperform conventional $L_p$ approaches in terms of robustness and\ntransferability, and also demonstrates that the PerC distance can provide added\nvalue on top of existing structure-based methods to creating image\nperturbations.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 16:27:32 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 14:16:26 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Zhao", "Zhengyu", ""], ["Liu", "Zhuoran", ""], ["Larson", "Martha", ""]]}, {"id": "1911.02475", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, Xu Han, Yukai Qiao, Yi Ge, Lu Jun", "title": "Unimodal-uniform Constrained Wasserstein Training for Medical Diagnosis", "comments": "ICCV VRMI workshop Oral. arXiv admin note: text overlap with\n  arXiv:1911.00962", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The labels in medical diagnosis task are usually discrete and successively\ndistributed. For example, the Diabetic Retinopathy Diagnosis (DR) involves five\nhealth risk levels: no DR (0), mild DR (1), moderate DR (2), severe DR (3) and\nproliferative DR (4). This labeling system is common for medical disease.\nPrevious methods usually construct a multi-binary-classification task or\npropose some re-parameter schemes in the output unit. In this paper, we target\non this task from the perspective of loss function. More specifically, the\nWasserstein distance is utilized as an alternative, explicitly incorporating\nthe inter-class correlations by pre-defining its ground metric. Then, the\nground metric which serves as a linear, convex or concave increasing function\nw.r.t. the Euclidean distance in a line is explored from an optimization\nperspective. Meanwhile, this paper also proposes of constructing the smoothed\ntarget labels that model the inlier and outlier noises by using a\nunimodal-uniform mixture distribution. Different from the one-hot setting, the\nsmoothed label endues the computation of Wasserstein distance with more\nchallenging features. With either one-hot or smoothed target label, this paper\nsystematically concludes the practical closed-form solution. We evaluate our\nmethod on several medical diagnosis tasks (e.g., Diabetic Retinopathy and\nUltrasound Breast dataset) and achieve state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 20:41:14 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Han", "Xu", ""], ["Qiao", "Yukai", ""], ["Ge", "Yi", ""], ["Jun", "Lu", ""]]}, {"id": "1911.02497", "submitter": "Vinu Joseph", "authors": "Vinu Joseph, Saurav Muralidharan, Animesh Garg, Michael Garland,\n  Ganesh Gopalakrishnan", "title": "A Programmable Approach to Neural Network Compression", "comments": "This is an updated version of a paper published in IEEE Micro, vol.\n  40, no. 5, pp. 17-25, Sept.-Oct. 2020 at\n  https://ieeexplore.ieee.org/document/9151283", "journal-ref": "IEEE Micro, Volume: 40, Issue: 5, Sept.-Oct. 2020, pp. 17-25", "doi": "10.1109/MM.2020.3012391", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) frequently contain far more weights, represented\nat a higher precision, than are required for the specific task which they are\ntrained to perform. Consequently, they can often be compressed using techniques\nsuch as weight pruning and quantization that reduce both the model size and\ninference time without appreciable loss in accuracy. However, finding the best\ncompression strategy and corresponding target sparsity for a given DNN,\nhardware platform, and optimization objective currently requires expensive,\nfrequently manual, trial-and-error experimentation. In this paper, we introduce\na programmable system for model compression called Condensa. Users\nprogrammatically compose simple operators, in Python, to build more complex and\npractically interesting compression strategies. Given a strategy and\nuser-provided objective (such as minimization of running time), Condensa uses a\nnovel Bayesian optimization-based algorithm to automatically infer desirable\nsparsities. Our experiments on four real-world DNNs demonstrate memory\nfootprint and hardware runtime throughput improvements of 188x and 2.59x,\nrespectively, using at most ten samples per search. We have released a\nreference implementation of Condensa at https://github.com/NVlabs/condensa.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 17:14:32 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 22:55:11 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Joseph", "Vinu", ""], ["Muralidharan", "Saurav", ""], ["Garg", "Animesh", ""], ["Garland", "Michael", ""], ["Gopalakrishnan", "Ganesh", ""]]}, {"id": "1911.02498", "submitter": "Shanxin Yuan", "authors": "Shanxin Yuan, Radu Timofte, Gregory Slabaugh, Ales Leonardis", "title": "AIM 2019 Challenge on Image Demoireing: Dataset and Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel dataset, called LCDMoire, which was created for\nthe first-ever image demoireing challenge that was part of the Advances in\nImage Manipulation (AIM) workshop, held in conjunction with ICCV 2019. The\ndataset comprises 10,200 synthetically generated image pairs (consisting of an\nimage degraded by moire and a clean ground truth image). In addition to\ndescribing the dataset and its creation, this paper also reviews the challenge\ntracks, competition, and results, the latter summarizing the current\nstate-of-the-art on this dataset.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 17:15:52 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Yuan", "Shanxin", ""], ["Timofte", "Radu", ""], ["Slabaugh", "Gregory", ""], ["Leonardis", "Ales", ""]]}, {"id": "1911.02521", "submitter": "Hyunseok  Seo", "authors": "Hyunseok Seo, Masoud Badiei Khuzani, Varun Vasudevan, Charles Huang,\n  Hongyi Ren, Ruoxiu Xiao, Xiao Jia, Lei Xing", "title": "Machine Learning Techniques for Biomedical Image Segmentation: An\n  Overview of Technical Aspects and Introduction to State-of-Art Applications", "comments": "Accept for publication at Medical Physics", "journal-ref": null, "doi": "10.1002/mp.13649", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, significant progress has been made in developing more\naccurate and efficient machine learning algorithms for segmentation of medical\nand natural images. In this review article, we highlight the imperative role of\nmachine learning algorithms in enabling efficient and accurate segmentation in\nthe field of medical imaging. We specifically focus on several key studies\npertaining to the application of machine learning methods to biomedical image\nsegmentation. We review classical machine learning algorithms such as Markov\nrandom fields, k-means clustering, random forest, etc. Although such classical\nlearning models are often less accurate compared to the deep learning\ntechniques, they are often more sample efficient and have a less complex\nstructure. We also review different deep learning architectures, such as the\nartificial neural networks (ANNs), the convolutional neural networks (CNNs),\nand the recurrent neural networks (RNNs), and present the segmentation results\nattained by those learning models that were published in the past three years.\nWe highlight the successes and limitations of each machine learning paradigm.\nIn addition, we discuss several challenges related to the training of different\nmachine learning models, and we present some heuristics to address those\nchallenges.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 17:59:39 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Seo", "Hyunseok", ""], ["Khuzani", "Masoud Badiei", ""], ["Vasudevan", "Varun", ""], ["Huang", "Charles", ""], ["Ren", "Hongyi", ""], ["Xiao", "Ruoxiu", ""], ["Jia", "Xiao", ""], ["Xing", "Lei", ""]]}, {"id": "1911.02559", "submitter": "Zhiqiang Shen", "authors": "Zhiqiang Shen and Harsh Maheshwari and Weichen Yao and Marios Savvides", "title": "SCL: Towards Accurate Domain Adaptive Object Detection via Gradient\n  Detach Based Stacked Complementary Losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptive object detection aims to learn a robust detector\nin the domain shift circumstance, where the training (source) domain is\nlabel-rich with bounding box annotations, while the testing (target) domain is\nlabel-agnostic and the feature distributions between training and testing\ndomains are dissimilar or even totally different. In this paper, we propose a\ngradient detach based stacked complementary losses (SCL) method that uses\ndetection losses as the primary objective, and cuts in several auxiliary losses\nin different network stages accompanying with gradient detach training to learn\nmore discriminative representations. We argue that the prior methods mainly\nleverage more loss functions for training but ignore the interaction of\ndifferent losses and also the compatible training strategy (gradient detach\nupdating in our work). Thus, our proposed method is a more syncretic adaptation\nlearning process. We conduct comprehensive experiments on seven datasets, the\nresults demonstrate that our method performs favorably better than the\nstate-of-the-art methods by a significant margin. For instance, from Cityscapes\nto FoggyCityscapes, we achieve 37.9% mAP, outperforming the previous art\nStrong-Weak by 3.6%.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 18:59:01 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 18:21:36 GMT"}, {"version": "v3", "created": "Thu, 21 Nov 2019 05:43:14 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Shen", "Zhiqiang", ""], ["Maheshwari", "Harsh", ""], ["Yao", "Weichen", ""], ["Savvides", "Marios", ""]]}, {"id": "1911.02620", "submitter": "John Lambert", "authors": "Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet Singh,\n  Slawomir Bak, Andrew Hartnett, De Wang, Peter Carr, Simon Lucey, Deva\n  Ramanan, James Hays", "title": "Argoverse: 3D Tracking and Forecasting with Rich Maps", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Argoverse -- two datasets designed to support autonomous vehicle\nmachine learning tasks such as 3D tracking and motion forecasting. Argoverse\nwas collected by a fleet of autonomous vehicles in Pittsburgh and Miami. The\nArgoverse 3D Tracking dataset includes 360 degree images from 7 cameras with\noverlapping fields of view, 3D point clouds from long range LiDAR, 6-DOF pose,\nand 3D track annotations. Notably, it is the only modern AV dataset that\nprovides forward-facing stereo imagery. The Argoverse Motion Forecasting\ndataset includes more than 300,000 5-second tracked scenarios with a particular\nvehicle identified for trajectory forecasting. Argoverse is the first\nautonomous vehicle dataset to include \"HD maps\" with 290 km of mapped lanes\nwith geometric and semantic metadata. All data is released under a Creative\nCommons license at www.argoverse.org. In our baseline experiments, we\nillustrate how detailed map information such as lane direction, driveable area,\nand ground height improves the accuracy of 3D object tracking and motion\nforecasting. Our tracking and forecasting experiments represent only an initial\nexploration of the use of rich maps in robotic perception. We hope that\nArgoverse will enable the research community to explore these problems in\ngreater depth.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 20:27:27 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Chang", "Ming-Fang", ""], ["Lambert", "John", ""], ["Sangkloy", "Patsorn", ""], ["Singh", "Jagjeet", ""], ["Bak", "Slawomir", ""], ["Hartnett", "Andrew", ""], ["Wang", "De", ""], ["Carr", "Peter", ""], ["Lucey", "Simon", ""], ["Ramanan", "Deva", ""], ["Hays", "James", ""]]}, {"id": "1911.02660", "submitter": "Weilin Fu", "authors": "Weilin Fu and Katharina Breininger and Zhaoya Pan and Andreas Maier", "title": "What Do We Really Need? Degenerating U-Net on Retinal Vessel\n  Segmentation", "comments": "7 pages, 2 figures, submitted in BVM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinal vessel segmentation is an essential step for fundus image analysis.\nWith the recent advances of deep learning technologies, many convolutional\nneural networks have been applied in this field, including the successful\nU-Net. In this work, we firstly modify the U-Net with functional blocks aiming\nto pursue higher performance. The absence of the expected performance boost\nthen lead us to dig into the opposite direction of shrinking the U-Net and\nexploring the extreme conditions such that its segmentation performance is\nmaintained. Experiment series to simplify the network structure, reduce the\nnetwork size and restrict the training conditions are designed. Results show\nthat for retinal vessel segmentation on DRIVE database, U-Net does not\ndegenerate until surprisingly acute conditions: one level, one filter in\nconvolutional layers, and one training sample. This experimental discovery is\nboth counter-intuitive and worthwhile. Not only are the extremes of the U-Net\nexplored on a well-studied application, but also one intriguing warning is\nraised for the research methodology which seeks for marginal performance\nenhancement regardless of the resource cost.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 22:49:55 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Fu", "Weilin", ""], ["Breininger", "Katharina", ""], ["Pan", "Zhaoya", ""], ["Maier", "Andreas", ""]]}, {"id": "1911.02683", "submitter": "Jesse Mu", "authors": "Jesse Mu, Percy Liang, Noah Goodman", "title": "Shaping Visual Representations with Language for Few-shot Classification", "comments": "ACL 2020. Version 1 appeared at the NeurIPS 2019 Workshop on Visually\n  Grounded Interaction and Language (ViGIL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By describing the features and abstractions of our world, language is a\ncrucial tool for human learning and a promising source of supervision for\nmachine learning models. We use language to improve few-shot visual\nclassification in the underexplored scenario where natural language task\ndescriptions are available during training, but unavailable for novel tasks at\ntest time. Existing models for this setting sample new descriptions at test\ntime and use those to classify images. Instead, we propose language-shaped\nlearning (LSL), an end-to-end model that regularizes visual representations to\npredict language. LSL is conceptually simpler, more data efficient, and\noutperforms baselines in two challenging few-shot domains.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 23:47:32 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 18:35:31 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Mu", "Jesse", ""], ["Liang", "Percy", ""], ["Goodman", "Noah", ""]]}, {"id": "1911.02718", "submitter": "Jingwen Fu", "authors": "Jingwen Fu, Licheng Zong, Yinbing Li, Ke Li, Bingqian Yang, Xibei Liu", "title": "Model Adaption Object Detection System for Robot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection for robot guidance is a crucial mission for autonomous\nrobots, which has provoked extensive attention for researchers. However, the\nchanging view of robot movement and limited available data hinder the research\nin this area. To address these matters, we proposed a new vision system for\nrobots, the model adaptation object detection system. Instead of using a single\none to solve problems, We made use of different object detection neural\nnetworks to guide the robot in accordance with various situations, with the\nhelp of a meta neural network to allocate the object detection neural networks.\nFurthermore, taking advantage of transfer learning technology and depthwise\nseparable convolutions, our model is easy to train and can address small\ndataset problems.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 02:20:36 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 05:08:20 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Fu", "Jingwen", ""], ["Zong", "Licheng", ""], ["Li", "Yinbing", ""], ["Li", "Ke", ""], ["Yang", "Bingqian", ""], ["Liu", "Xibei", ""]]}, {"id": "1911.02721", "submitter": "Moo K. Chung", "authors": "Shih-Gu Huang, Ilwoo Lyu, Anqi Qiu, and Moo K. Chung", "title": "Fast Polynomial Approximation of Heat Kernel Convolution on Manifolds\n  and Its Application to Brain Sulcal and Gyral Graph Pattern Analysis", "comments": "Accepted for publication", "journal-ref": "IEEE Transactions on Medical Imaging, 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heat diffusion has been widely used in brain imaging for surface fairing,\nmesh regularization and cortical data smoothing. Motivated by diffusion\nwavelets and convolutional neural networks on graphs, we present a new fast and\naccurate numerical scheme to solve heat diffusion on surface meshes. This is\nachieved by approximating the heat kernel convolution using high degree\northogonal polynomials in the spectral domain. We also derive the closed-form\nexpression of the spectral decomposition of the Laplace-Beltrami operator and\nuse it to solve heat diffusion on a manifold for the first time. The proposed\nfast polynomial approximation scheme avoids solving for the eigenfunctions of\nthe Laplace-Beltrami operator, which is computationally costly for large mesh\nsize, and the numerical instability associated with the finite element method\nbased diffusion solvers. The proposed method is applied in localizing the male\nand female differences in cortical sulcal and gyral graph patterns obtained\nfrom MRI in an innovative way. The MATLAB code is available at\nhttp://www.stat.wisc.edu/~mchung/chebyshev.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 02:27:59 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 18:09:51 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Huang", "Shih-Gu", ""], ["Lyu", "Ilwoo", ""], ["Qiu", "Anqi", ""], ["Chung", "Moo K.", ""]]}, {"id": "1911.02736", "submitter": "Zhan Qi", "authors": "Qi Zhan, Wenjin Wang and Gerard de Haan", "title": "Analysis of CNN-based remote-PPG to understand limitations and\n  sensitivities", "comments": "Biomedical Optics Express journal submission. 15 pages,11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based on Convolutional Neural Network (CNN) has shown promising\nresults in various vision-based applications, recently also in camera-based\nvital signs monitoring. The CNN-based Photoplethysmography (PPG) extraction\nhas, so far, been focused on performance rather than understanding. In this\npaper, we try to answer four questions with experiments aiming at improving our\nunderstanding of this methodology as it gains popularity. We conclude that the\nnetwork exploits the blood absorption variation to extract the physiological\nsignals, and that the choice and parameters (phase, spectral content, etc.) of\nthe reference-signal may be more critical than anticipated. The availability of\nmultiple convolutional kernels is necessary for CNN to arrive at a flexible\nchannel combination through the spatial operation, but may not provide the same\nmotion-robustness as a multi-site measurement using knowledge-based PPG\nextraction. Finally, we conclude that the PPG-related prior knowledge is still\nhelpful for the CNN-based PPG extraction. Consequently, we recommend further\ninvestigation of hybrid CNN-based methods to include prior knowledge in their\ndesign.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 03:09:43 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 06:46:06 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Zhan", "Qi", ""], ["Wang", "Wenjin", ""], ["de Haan", "Gerard", ""]]}, {"id": "1911.02739", "submitter": "Zhihan Zhang", "authors": "Zhihan Zhang, Zhiyi Yin, Shuhuai Ren, Xinhang Li, Shicheng Li", "title": "DCA: Diversified Co-Attention towards Informative Live Video Commenting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the task of Automatic Live Video Commenting (ALVC), which aims to\ngenerate real-time video comments with both video frames and other viewers'\ncomments as inputs. A major challenge in this task is how to properly leverage\nthe rich and diverse information carried by video and text. In this paper, we\naim to collect diversified information from video and text for informative\ncomment generation. To achieve this, we propose a Diversified Co-Attention\n(DCA) model for this task. Our model builds bidirectional interactions between\nvideo frames and surrounding comments from multiple perspectives via metric\nlearning, to collect a diversified and informative context for comment\ngeneration. We also propose an effective parameter orthogonalization technique\nto avoid excessive overlap of information learned from different perspectives.\nResults show that our approach outperforms existing methods in the ALVC task,\nachieving new state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 03:28:38 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2020 05:14:34 GMT"}, {"version": "v3", "created": "Sat, 8 Aug 2020 13:37:10 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Zhang", "Zhihan", ""], ["Yin", "Zhiyi", ""], ["Ren", "Shuhuai", ""], ["Li", "Xinhang", ""], ["Li", "Shicheng", ""]]}, {"id": "1911.02740", "submitter": "Ashwin Shankar", "authors": "Niral Shah, Ashwin Shankar, Jae-hong Park", "title": "Detecting Driveable Area for Autonomous Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving is a challenging problem where there is currently an\nintense focus on research and development. Human drivers are forced to make\nthousands of complex decisions in a short amount of time,quickly processing\ntheir surroundings and moving factors. One of these aspects, recognizing\nregions on the road that are driveable is vital to the success of any\nautonomous system. This problem can be addressed with deep learning framed as a\nregion proposal problem. Utilizing a Mask R-CNN trained on the Berkeley Deep\nDrive (BDD100k) dataset, we aim to see if recognizing driveable areas, while\nalso differentiating between the car's direct (current) lane and alternative\nlanes is feasible.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 03:32:46 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Shah", "Niral", ""], ["Shankar", "Ashwin", ""], ["Park", "Jae-hong", ""]]}, {"id": "1911.02744", "submitter": "Haoxuan You", "authors": "Can Qin, Haoxuan You, Lichen Wang, C.-C. Jay Kuo, Yun Fu", "title": "PointDAN: A Multi-Scale 3D Domain Adaption Network for Point Cloud\n  Representation", "comments": "12 pages, 4 figures, 33rd Conference on Neural Information Processing\n  Systems (NeurIPS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain Adaptation (DA) approaches achieved significant improvements in a wide\nrange of machine learning and computer vision tasks (i.e., classification,\ndetection, and segmentation). However, as far as we are aware, there are few\nmethods yet to achieve domain adaptation directly on 3D point cloud data. The\nunique challenge of point cloud data lies in its abundant spatial geometric\ninformation, and the semantics of the whole object is contributed by including\nregional geometric structures. Specifically, most general-purpose DA methods\nthat struggle for global feature alignment and ignore local geometric\ninformation are not suitable for 3D domain alignment. In this paper, we propose\na novel 3D Domain Adaptation Network for point cloud data (PointDAN). PointDAN\njointly aligns the global and local features in multi-level. For local\nalignment, we propose Self-Adaptive (SA) node module with an adjusted receptive\nfield to model the discriminative local structures for aligning domains. To\nrepresent hierarchically scaled features, node-attention module is further\nintroduced to weight the relationship of SA nodes across objects and domains.\nFor global alignment, an adversarial-training strategy is employed to learn and\nalign global features across domains. Since there is no common evaluation\nbenchmark for 3D point cloud DA scenario, we build a general benchmark (i.e.,\nPointDA-10) extracted from three popular 3D object/scene datasets (i.e.,\nModelNet, ShapeNet and ScanNet) for cross-domain 3D objects classification\nfashion. Extensive experiments on PointDA-10 illustrate the superiority of our\nmodel over the state-of-the-art general-purpose DA methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 04:03:07 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Qin", "Can", ""], ["You", "Haoxuan", ""], ["Wang", "Lichen", ""], ["Kuo", "C. -C. Jay", ""], ["Fu", "Yun", ""]]}, {"id": "1911.02749", "submitter": "Tong Zhang", "authors": "Tong Zhang and Fatih Porikli", "title": "Sparse Coding on Cascaded Residuals", "comments": "ACCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper seeks to combine dictionary learning and hierarchical image\nrepresentation in a principled way. To make dictionary atoms capturing\nadditional information from extended receptive fields and attain improved\ndescriptive capacity, we present a two-pass multi-resolution cascade framework\nfor dictionary learning and sparse coding. The cascade allows collaborative\nreconstructions at different resolutions using the same dimensional dictionary\natoms. Our jointly learned dictionary comprises atoms that adapt to the\ninformation available at the coarsest layer where the support of atoms reaches\ntheir maximum range and the residual images where the supplementary details\nprogressively refine the reconstruction objective. The residual at a layer is\ncomputed by the difference between the aggregated reconstructions of the\nprevious layers and the downsampled original image at that layer. Our method\ngenerates more flexible and accurate representations using much less number of\ncoefficients. Its computational efficiency stems from encoding at the coarsest\nresolution, which is minuscule, and encoding the residuals, which are\nrelatively much sparse. Our extensive experiments on multiple datasets\ndemonstrate that this new method is powerful in image coding, denoising,\ninpainting and artifact removal tasks outperforming the state-of-the-art\ntechniques.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 04:17:46 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Zhang", "Tong", ""], ["Porikli", "Fatih", ""]]}, {"id": "1911.02755", "submitter": "Sue Min Cho", "authors": "Sue Min Cho, Young-Gon Kim, Jinhoon Jeong, Ho-jin Lee, Namkug Kim", "title": "Automatic Tip Detection of Surgical Instruments in Biportal Endoscopic\n  Spine Surgery", "comments": "7 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some endoscopic surgeries require a surgeon to hold the endoscope with one\nhand and the surgical instruments with the other hand to perform the actual\nsurgery with correct vision. Recent technical advances in deep learning as well\nas in robotics can introduce robotics to these endoscopic surgeries. This can\nhave numerous advantages by freeing one hand of the surgeon, which will allow\nthe surgeon to use both hands and to use more intricate and sophisticated\ntechniques. Recently, deep learning with convolutional neural network achieves\nstate-of-the-art results in computer vision. Therefore, the aim of this study\nis to automatically detect the tip of the instrument, localize a point, and\nevaluate detection accuracy in biportal endoscopic spine surgery. The localized\npoint could be used for the controller's inputs of robotic endoscopy in these\ntypes of endoscopic surgeries.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 04:53:06 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 22:32:31 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Cho", "Sue Min", ""], ["Kim", "Young-Gon", ""], ["Jeong", "Jinhoon", ""], ["Lee", "Ho-jin", ""], ["Kim", "Namkug", ""]]}, {"id": "1911.02761", "submitter": "Han Bao", "authors": "Han Bao", "title": "Investigations of the Influences of a CNN's Receptive Field on\n  Segmentation of Subnuclei of Bilateral Amygdalae", "comments": "16 pages, 10 figures, ADEIJ journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of objects with various sizes is relatively less explored in\nmedical imaging, and has been very challenging in computer vision tasks in\ngeneral. We hypothesize that the receptive field of a deep model corresponds\nclosely to the size of object to be segmented, which could critically influence\nthe segmentation accuracy of objects with varied sizes. In this study, we\nemployed \"AmygNet\", a dual-branch fully convolutional neural network (FCNN)\nwith two different sizes of receptive fields, to investigate the effects of\nreceptive field on segmenting four major subnuclei of bilateral amygdalae. The\nexperiment was conducted on 14 subjects, which are all 3-dimensional MRI human\nbrain images. Since the scale of different subnuclear groups are different, by\ninvestigating the accuracy of each subnuclear group while using receptive\nfields of various sizes, we may find which kind of receptive field is suitable\nfor object of which scale respectively. In the given condition, AmygNet with\nmultiple receptive fields presents great potential in segmenting objects of\ndifferent sizes.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 05:39:56 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Bao", "Han", ""]]}, {"id": "1911.02807", "submitter": "Yu Pang", "authors": "Yu Pang, Xinyi Li, Lin Yuan, Haibin Ling", "title": "Improving Human Annotation in Single Object Tracking", "comments": "7 pages, 7 figures, 1 table, submitted to ICRA2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human annotation is always considered as ground truth in video object\ntracking tasks. It is used in both training and evaluation purposes. Thus,\nensuring its high quality is an important task for the success of trackers and\nevaluations between them. In this paper, we give a qualitative and quantitative\nanalysis of the existing human annotations. We show that human annotation tends\nto be non-smooth and is prone to partial visibility and deformation. We propose\na smoothing trajectory strategy with the ability to handle moving scenes. We\nuse a two-step adaptive image alignment algorithm to find the canonical view of\nthe video sequence. We then use different techniques to smooth the trajectories\nat certain degree. Once we convert back to the original image coordination, we\ncan compare with the human annotation. With the experimental results, we can\nget more consistent trajectories. At a certain degree, it can also slightly\nimprove the trained model. If go beyond a certain threshold, the smoothing\nerror will start eating up the benefit. Overall, our method could help\nextrapolate the missing annotation frames or identify and correct human\nannotation outliers as well as help improve the training data quality.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 08:55:46 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Pang", "Yu", ""], ["Li", "Xinyi", ""], ["Yuan", "Lin", ""], ["Ling", "Haibin", ""]]}, {"id": "1911.02837", "submitter": "Mateusz Trokielewicz", "authors": "Mateusz Trokielewicz and Adam Czajka and Piotr Maciejewicz", "title": "Post-mortem Iris Decomposition and its Dynamics in Morgue Conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing interest in employing iris biometrics as a forensic tool for\nidentification by investigation authorities, there is a need for a thorough\nexamination and understanding of post-mortem decomposition processes that take\nplace within the human eyeball, especially the iris. This can prove useful for\nfast and accurate matching of ante-mortem with post-mortem data acquired at\ncrime scenes or mass casualties, as well as for ensuring correct dispatching of\nbodies from the incident scene to a mortuary or funeral homes. Following these\nneeds of forensic community, this paper offers an analysis of the coarse\neffects of eyeball decay done from a perspective of automatic iris recognition\npoint of view. Therefore, we analyze post-mortem iris images acquired in both\nvisible light as well as in near-infrared light (860 nm), as the latter\nwavelength is used in commercial iris recognition systems. Conclusions and\nsuggestions are provided that may aid forensic examiners in successfully\nutilizing iris patterns in post-mortem identification of deceased subjects.\nInitial guidelines regarding the imaging process, types of illumination,\nresolution are also given, together with expectations with respect to the iris\nfeatures decomposition rates.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 10:40:33 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Trokielewicz", "Mateusz", ""], ["Czajka", "Adam", ""], ["Maciejewicz", "Piotr", ""]]}, {"id": "1911.02888", "submitter": "Himalaya Jain", "authors": "Victor Besnier, Himalaya Jain, Andrei Bursuc, Matthieu Cord, Patrick\n  P\\'erez", "title": "This dataset does not exist: training models from generated images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current generative networks are increasingly proficient in generating\nhigh-resolution realistic images. These generative networks, especially the\nconditional ones, can potentially become a great tool for providing new image\ndatasets. This naturally brings the question: Can we train a classifier only on\nthe generated data? This potential availability of nearly unlimited amounts of\ntraining data challenges standard practices for training machine learning\nmodels, which have been crafted across the years for limited and fixed size\ndatasets. In this work we investigate this question and its related challenges.\nWe identify ways to improve significantly the performance over naive training\non randomly generated images with regular heuristics. We propose three\nstandalone techniques that can be applied at different stages of the pipeline,\ni.e., data generation, training on generated data, and deploying on real data.\nWe evaluate our proposed approaches on a subset of the ImageNet dataset and\nshow encouraging results compared to classifiers trained on real images.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 13:23:39 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Besnier", "Victor", ""], ["Jain", "Himalaya", ""], ["Bursuc", "Andrei", ""], ["Cord", "Matthieu", ""], ["P\u00e9rez", "Patrick", ""]]}, {"id": "1911.02897", "submitter": "Matt Angus", "authors": "Matt Angus, Krzysztof Czarnecki, Rick Salay", "title": "Efficacy of Pixel-Level OOD Detection for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of out of distribution samples for image classification has\nbeen widely researched. Safety critical applications, such as autonomous\ndriving, would benefit from the ability to localise the unusual objects causing\nthe image to be out of distribution. This paper adapts state-of-the-art methods\nfor detecting out of distribution images for image classification to the new\ntask of detecting out of distribution pixels, which can localise the unusual\nobjects. It further experimentally compares the adapted methods on two new\ndatasets derived from existing semantic segmentation datasets using PSPNet and\nDeeplabV3+ architectures, as well as proposing a new metric for the task. The\nevaluation shows that the performance ranking of the compared methods does not\ntransfer to the new task and every method performs significantly worse than\ntheir image-level counterparts.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 13:37:38 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Angus", "Matt", ""], ["Czarnecki", "Krzysztof", ""], ["Salay", "Rick", ""]]}, {"id": "1911.02921", "submitter": "Enrique Fita Sanmartin", "authors": "Enrique Fita Sanmartin, Sebastian Damrich, Fred A. Hamprecht (HCI/IWR\n  at Heidelberg University)", "title": "Probabilistic Watershed: Sampling all spanning forests for seeded\n  segmentation and semi-supervised learning", "comments": "To be published in NeurIPS2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The seeded Watershed algorithm / minimax semi-supervised learning on a graph\ncomputes a minimum spanning forest which connects every pixel / unlabeled node\nto a seed / labeled node. We propose instead to consider all possible spanning\nforests and calculate, for every node, the probability of sampling a forest\nconnecting a certain seed with that node. We dub this approach \"Probabilistic\nWatershed\". Leo Grady (2006) already noted its equivalence to the Random Walker\n/ Harmonic energy minimization. We here give a simpler proof of this\nequivalence and establish the computational feasibility of the Probabilistic\nWatershed with Kirchhoff's matrix tree theorem. Furthermore, we show a new\nconnection between the Random Walker probabilities and the triangle inequality\nof the effective resistance. Finally, we derive a new and intuitive\ninterpretation of the Power Watershed.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 13:26:32 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Sanmartin", "Enrique Fita", "", "HCI/IWR\n  at Heidelberg University"], ["Damrich", "Sebastian", "", "HCI/IWR\n  at Heidelberg University"], ["Hamprecht", "Fred A.", "", "HCI/IWR\n  at Heidelberg University"]]}, {"id": "1911.02945", "submitter": "Hemant Kumar Aggarwal", "authors": "Hemant Kumar Aggarwal, Mathews Jacob", "title": "J-MoDL: Joint Model-Based Deep Learning for Optimized Sampling and\n  Reconstruction", "comments": null, "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, 14(6), 2020", "doi": "10.1109/JSTSP.2020.3004094", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern MRI schemes, which rely on compressed sensing or deep learning\nalgorithms to recover MRI data from undersampled multichannel Fourier\nmeasurements, are widely used to reduce scan time. The image quality of these\napproaches is heavily dependent on the sampling pattern. We introduce a\ncontinuous strategy to jointly optimize the sampling pattern and network\nparameters. We use a multichannel forward model, consisting of a non-uniform\nFourier transform with continuously defined sampling locations, to realize the\ndata consistency block within a model-based deep learning image reconstruction\nscheme. This approach facilitates the joint and continuous optimization of the\nsampling pattern and the CNN parameters to improve image quality. We observe\nthat the joint optimization of the sampling patterns and the reconstruction\nmodule significantly improves the performance of most deep learning\nreconstruction algorithms. The source code of the proposed joint learning\nframework is available at https://github.com/hkaggarwal/J-MoDL.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 16:10:45 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 18:18:24 GMT"}, {"version": "v3", "created": "Sun, 3 May 2020 05:21:22 GMT"}, {"version": "v4", "created": "Thu, 2 Jul 2020 19:48:27 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Aggarwal", "Hemant Kumar", ""], ["Jacob", "Mathews", ""]]}, {"id": "1911.02961", "submitter": "Carlos Eduardo Rosar Kos Lassance", "authors": "Carlos Lassance, Yasir Latif, Ravi Garg, Vincent Gripon, Ian Reid", "title": "Improved Visual Localization via Graph Smoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision based localization is the problem of inferring the pose of the camera\ngiven a single image. One solution to this problem is to learn a deep neural\nnetwork to infer the pose of a query image after learning on a dataset of\nimages with known poses. Another more commonly used approach rely on image\nretrieval where the query image is compared against the database of images and\nits pose is inferred with the help of the retrieved images. The latter approach\nassumes that images taken from the same places consists of the same landmarks\nand, thus would have similar feature representations. These representation can\nbe learned using full supervision to be robust to different variations in\ncapture conditions like time of the day and weather. In this work, we introduce\na framework to enhance the performance of these retrieval based localization\nmethods by taking into account the additional information including GPS\ncoordinates and temporal neighbourhood of the images provided by the\nacquisition process in addition to the descriptor similarity of pairs of images\nin the reference or query database which is used traditionally for\nlocalization. Our method constructs a graph based on this additional\ninformation and use it for robust retrieval by smoothing the feature\nrepresentation of reference and/or query images. We show that the proposed\nmethod is able to significantly improve the localization accuracy on two large\nscale datasets over the baselines.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 15:15:24 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Lassance", "Carlos", ""], ["Latif", "Yasir", ""], ["Garg", "Ravi", ""], ["Gripon", "Vincent", ""], ["Reid", "Ian", ""]]}, {"id": "1911.02971", "submitter": "Zhuosheng Zhang", "authors": "Zhuosheng Zhang, Rui Wang, Kehai Chen, Masao Utiyama, Eiichiro Sumita,\n  Hai Zhao", "title": "Probing Contextualized Sentence Representations with Visual Awareness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a universal framework to model contextualized sentence\nrepresentations with visual awareness that is motivated to overcome the\nshortcomings of the multimodal parallel data with manual annotations. For each\nsentence, we first retrieve a diversity of images from a shared cross-modal\nembedding space, which is pre-trained on a large-scale of text-image pairs.\nThen, the texts and images are respectively encoded by transformer encoder and\nconvolutional neural network. The two sequences of representations are further\nfused by a simple and effective attention layer. The architecture can be easily\napplied to text-only natural language processing tasks without manually\nannotating multimodal parallel corpora. We apply the proposed method on three\ntasks, including neural machine translation, natural language inference and\nsequence labeling and experimental results verify the effectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 16:34:31 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Zhang", "Zhuosheng", ""], ["Wang", "Rui", ""], ["Chen", "Kehai", ""], ["Utiyama", "Masao", ""], ["Sumita", "Eiichiro", ""], ["Zhao", "Hai", ""]]}, {"id": "1911.02996", "submitter": "Elijah Bolluyt", "authors": "Elijah D. Bolluyt, Cristina Comaniciu", "title": "Collapse Resistant Deep Convolutional GAN for Multi-Object Image\n  Generation", "comments": "Accepted to IEEE International Conference on Machine Learning and\n  Applications 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a novel system for the generation of images that contain\nmultiple classes of objects. Recent work in Generative Adversarial Networks\nhave produced high quality images, but many focus on generating images of a\nsingle object or set of objects. Our system addresses the task of image\ngeneration conditioned on a list of desired classes to be included in a single\nimage. This enables our system to generate images with any given combination of\nobjects, all composed into a visually realistic natural image. The system\nlearns the interrelationships of all classes represented in a dataset, and can\ngenerate diverse samples including a set of these classes. It displays the\nability to arrange these objects together, accounting for occlusions and\ninter-object spatial relations that characterize complex natural images. To\naccomplish this, we introduce a novel architecture based on Conditional Deep\nConvolutional GANs that is stabilized against collapse relative to both mode\nand condition. The system learns to rectify mode collapse during training,\nself-correcting to avoid suboptimal generation modes.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 02:27:23 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Bolluyt", "Elijah D.", ""], ["Comaniciu", "Cristina", ""]]}, {"id": "1911.03022", "submitter": "Qiyuan Hu", "authors": "Qiyuan Hu, Heather M. Whitney, Maryellen L. Giger", "title": "Transfer Learning in 4D for Breast Cancer Diagnosis using Dynamic\n  Contrast-Enhanced Magnetic Resonance Imaging", "comments": "Machine Learning for Health (ML4H) at NeurIPS 2019 - Extended\n  Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep transfer learning using dynamic contrast-enhanced magnetic resonance\nimaging (DCE-MRI) has shown strong predictive power in characterization of\nbreast lesions. However, pretrained convolutional neural networks (CNNs)\nrequire 2D inputs, limiting the ability to exploit the rich 4D (volumetric and\ntemporal) image information inherent in DCE-MRI that is clinically valuable for\nlesion assessment. Training 3D CNNs from scratch, a common method to utilize\nhigh-dimensional information in medical images, is computationally expensive\nand is not best suited for moderately sized healthcare datasets. Therefore, we\npropose a novel approach using transfer learning that incorporates the 4D\ninformation from DCE-MRI, where volumetric information is collapsed at feature\nlevel by max pooling along the projection perpendicular to the transverse\nslices and the temporal information is contained either in second-post contrast\nsubtraction images. Our methodology yielded an area under the receiver\noperating characteristic curve of 0.89+/-0.01 on a dataset of 1161 breast\nlesions, significantly outperforming a previous approach that incorporates the\n4D information in DCE-MRI by the use of maximum intensity projection (MIP)\nimages.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 03:45:24 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Hu", "Qiyuan", ""], ["Whitney", "Heather M.", ""], ["Giger", "Maryellen L.", ""]]}, {"id": "1911.03029", "submitter": "Wei-Hong Lin", "authors": "Wei-Hong Lin and Jia-Xing Zhong and Shan Liu and Thomas Li and Ge Li", "title": "RoIMix: Proposal-Fusion among Multiple Images for Underwater Object\n  Detection", "comments": "ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generic object detection algorithms have proven their excellent performance\nin recent years. However, object detection on underwater datasets is still less\nexplored. In contrast to generic datasets, underwater images usually have color\nshift and low contrast; sediment would cause blurring in underwater images. In\naddition, underwater creatures often appear closely to each other on images due\nto their living habits. To address these issues, our work investigates\naugmentation policies to simulate overlapping, occluded and blurred objects,\nand we construct a model capable of achieving better generalization. We propose\nan augmentation method called RoIMix, which characterizes interactions among\nimages. Proposals extracted from different images are mixed together. Previous\ndata augmentation methods operate on a single image while we apply RoIMix to\nmultiple images to create enhanced samples as training data. Experiments show\nthat our proposed method improves the performance of region-based object\ndetectors on both Pascal VOC and URPC datasets.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 03:56:22 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 14:02:31 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Lin", "Wei-Hong", ""], ["Zhong", "Jia-Xing", ""], ["Liu", "Shan", ""], ["Li", "Thomas", ""], ["Li", "Ge", ""]]}, {"id": "1911.03052", "submitter": "Mahesh Joshi", "authors": "Mahesh Joshi, Bodhisatwa Mazumdar, and Somnath Dey", "title": "A Novel Approach for Partial Fingerprint Identification to Mitigate\n  MasterPrint Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial fingerprint recognition is a method to recognize an individual when\nthe sensor size has a small form factor in accepting a full fingerprint. It is\nalso used in forensic research to identify the partial fingerprints collected\nfrom the crime scenes. But the distinguishing features in the partial\nfingerprint are relatively low due to small fingerprint captured by the sensor.\nHence, the uniqueness of a partial fingerprint cannot be guaranteed, leading to\na possibility that a single partial fingerprint may identify multiple subjects.\nA MasterPrint is a partial fingerprint that identifies at least 4% different\nindividuals from the enrolled template database. A fingerprint identification\nsystem with such a flaw can play a significant role in convicting an innocent\nin a criminal case. We propose a partial fingerprint identification approach\nthat aims to mitigate MasterPrint generation. The proposed method, when applied\nto partial fingerprint dataset cropped from standard FVC 2002 DB1(A) dataset\nshowed significant improvement in reducing the count of MasterPrints. The\nexperimental result demonstrates improved results on other parameters, such as\nTrue match Rate (TMR) and Equal Error Rate (EER), generally used to evaluate\nthe performance of a fingerprint biometric system.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 05:07:25 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Joshi", "Mahesh", ""], ["Mazumdar", "Bodhisatwa", ""], ["Dey", "Somnath", ""]]}, {"id": "1911.03083", "submitter": "Bhavan Jasani", "authors": "Bhavan Jasani, Rohit Girdhar, Deva Ramanan", "title": "Are we asking the right questions in MovieQA?", "comments": "Spotlight presentation at CLVL workshop, ICCV 2019. Project page:\n  https://bhavanj.github.io/MovieQAWithoutMovies/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint vision and language tasks like visual question answering are\nfascinating because they explore high-level understanding, but at the same\ntime, can be more prone to language biases. In this paper, we explore the\nbiases in the MovieQA dataset and propose a strikingly simple model which can\nexploit them. We find that using the right word embedding is of utmost\nimportance. By using an appropriately trained word embedding, about half the\nQuestion-Answers (QAs) can be answered by looking at the questions and answers\nalone, completely ignoring narrative context from video clips, subtitles, and\nmovie scripts. Compared to the best published papers on the leaderboard, our\nsimple question + answer only model improves accuracy by 5% for video +\nsubtitle category, 5% for subtitle, 15% for DVS and 6% higher for scripts.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 06:49:45 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Jasani", "Bhavan", ""], ["Girdhar", "Rohit", ""], ["Ramanan", "Deva", ""]]}, {"id": "1911.03086", "submitter": "Vajira Thambawita", "authors": "Vajira Thambawita, P{\\aa}l Halvorsen, Hugo Hammer, Michael Riegler,\n  Trine B. Haugen", "title": "Stacked dense optical flows and dropout layers to predict sperm motility\n  and morphology", "comments": "3 pages, 2 figures, MediaEval 19, 27-29 October 2019, Sophia\n  Antipolis, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyse two deep learning methods to predict sperm motility\nand sperm morphology from sperm videos. We use two different inputs: stacked\npure frames of videos and dense optical flows of video frames. To solve this\nregression task of predicting motility and morphology, stacked dense optical\nflows and extracted original frames from sperm videos were used with the\nmodified state of the art convolution neural networks. For modifications of the\nselected models, we have introduced an additional multi-layer perceptron to\novercome the problem of over-fitting. The method which had an additional\nmulti-layer perceptron with dropout layers, shows the best results when the\ninputs consist of both dense optical flows and an original frame of videos.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 06:59:35 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Thambawita", "Vajira", ""], ["Halvorsen", "P\u00e5l", ""], ["Hammer", "Hugo", ""], ["Riegler", "Michael", ""], ["Haugen", "Trine B.", ""]]}, {"id": "1911.03100", "submitter": "Vajira Thambawita", "authors": "Vajira Thambawita, P{\\aa}l Halvorsen, Hugo Hammer, Michael Riegler,\n  Trine B. Haugen", "title": "Extracting temporal features into a spatial domain using autoencoders\n  for sperm video analysis", "comments": "3 pages, 1 figure, MediaEval 19, 27-29 October 2019, Sophia\n  Antipolis, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a two-step deep learning method that is used to\npredict sperm motility and morphology-based on video recordings of human\nspermatozoa. First, we use an autoencoder to extract temporal features from a\ngiven semen video and plot these into image-space, which we call\nfeature-images. Second, these feature-images are used to perform transfer\nlearning to predict the motility and morphology values of human sperm. The\npresented method shows it's capability to extract temporal information into\nspatial domain feature-images which can be used with traditional convolutional\nneural networks. Furthermore, the accuracy of the predicted motility of a given\nsemen sample shows that a deep learning-based model can capture the temporal\ninformation of microscopic recordings of human semen.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 07:29:03 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Thambawita", "Vajira", ""], ["Halvorsen", "P\u00e5l", ""], ["Hammer", "Hugo", ""], ["Riegler", "Michael", ""], ["Haugen", "Trine B.", ""]]}, {"id": "1911.03112", "submitter": "Alina Kloss", "authors": "Alina Kloss, Maria Bauza, Jiajun Wu, Joshua B. Tenenbaum, Alberto\n  Rodriguez and Jeannette Bohg", "title": "Accurate Vision-based Manipulation through Contact Reasoning", "comments": "accepted at ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planning contact interactions is one of the core challenges of many robotic\ntasks. Optimizing contact locations while taking dynamics into account is\ncomputationally costly and, in environments that are only partially observable,\nexecuting contact-based tasks often suffers from low accuracy. We present an\napproach that addresses these two challenges for the problem of vision-based\nmanipulation. First, we propose to disentangle contact from motion\noptimization. Thereby, we improve planning efficiency by focusing computation\non promising contact locations. Second, we use a hybrid approach for perception\nand state estimation that combines neural networks with a physically meaningful\nstate representation. In simulation and real-world experiments on the task of\nplanar pushing, we show that our method is more efficient and achieves a higher\nmanipulation accuracy than previous vision-based approaches.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 08:05:07 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 15:08:59 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Kloss", "Alina", ""], ["Bauza", "Maria", ""], ["Wu", "Jiajun", ""], ["Tenenbaum", "Joshua B.", ""], ["Rodriguez", "Alberto", ""], ["Bohg", "Jeannette", ""]]}, {"id": "1911.03149", "submitter": "Parimala Kancharla", "authors": "Parimala Kancharla, Sumohana S. Channappayya", "title": "Quality Aware Generative Adversarial Networks", "comments": "10 pages, NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have become a very popular tool for\nimplicitly learning high-dimensional probability distributions. Several\nimprovements have been made to the original GAN formulation to address some of\nits shortcomings like mode collapse, convergence issues, entanglement, poor\nvisual quality etc. While a significant effort has been directed towards\nimproving the visual quality of images generated by GANs, it is rather\nsurprising that objective image quality metrics have neither been employed as\ncost functions nor as regularizers in GAN objective functions. In this work, we\nshow how a distance metric that is a variant of the Structural SIMilarity\n(SSIM) index (a popular full-reference image quality assessment algorithm), and\na novel quality aware discriminator gradient penalty function that is inspired\nby the Natural Image Quality Evaluator (NIQE, a popular no-reference image\nquality assessment algorithm) can each be used as excellent regularizers for\nGAN objective functions. Specifically, we demonstrate state-of-the-art\nperformance using the Wasserstein GAN gradient penalty (WGAN-GP) framework over\nCIFAR-10, STL10 and CelebA datasets.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 09:42:35 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Kancharla", "Parimala", ""], ["Channappayya", "Sumohana S.", ""]]}, {"id": "1911.03165", "submitter": "Yilei Shi", "authors": "Yilei Shi, Qingyu Li, Xiao Xiang Zhu", "title": "Building Segmentation through a Gated Graph Convolutional Neural Network\n  with Deep Structured Feature Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic building extraction from optical imagery remains a challenge due\nto, for example, the complexity of building shapes. Semantic segmentation is an\nefficient approach for this task. The latest development in deep convolutional\nneural networks (DCNNs) has made accurate pixel-level classification tasks\npossible. Yet one central issue remains: the precise delineation of boundaries.\nDeep architectures generally fail to produce fine-grained segmentation with\naccurate boundaries due to their progressive down-sampling. Hence, we introduce\na generic framework to overcome the issue, integrating the graph convolutional\nnetwork (GCN) and deep structured feature embedding (DSFE) into an end-to-end\nworkflow. Furthermore, instead of using a classic graph convolutional neural\nnetwork, we propose a gated graph convolutional network, which enables the\nrefinement of weak and coarse semantic predictions to generate sharp borders\nand fine-grained pixel-level classification. Taking the semantic segmentation\nof building footprints as a practical example, we compared different feature\nembedding architectures and graph neural networks. Our proposed framework with\nthe new GCN architecture outperforms state-of-the-art approaches. Although our\nmain task in this work is building footprint extraction, the proposed method\ncan be generally applied to other binary or multi-label segmentation tasks.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 10:19:13 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Shi", "Yilei", ""], ["Li", "Qingyu", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "1911.03267", "submitter": "Chris Ding", "authors": "Chi Ding, Zheng Cao, Matthew S. Emigh, Jose C. Principe, Bing Ouyang,\n  Anni Vuorenkoski, Fraser Dalgleish, Brian Ramos, Yanjun Li", "title": "Algorithmic Design and Implementation of Unobtrusive Multistatic Serial\n  LiDAR Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To fully understand interactions between marine hydrokinetic (MHK) equipment\nand marine animals, a fast and effective monitoring system is required to\ncapture relevant information whenever underwater animals appear. A new\nautomated underwater imaging system composed of LiDAR (Light Detection and\nRanging) imaging hardware and a scene understanding software module named\nUnobtrusive Multistatic Serial LiDAR Imager (UMSLI) to supervise the presence\nof animals near turbines. UMSLI integrates the front end LiDAR hardware and a\nseries of software modules to achieve image preprocessing, detection, tracking,\nsegmentation and classification in a hierarchical manner.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 13:58:00 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Ding", "Chi", ""], ["Cao", "Zheng", ""], ["Emigh", "Matthew S.", ""], ["Principe", "Jose C.", ""], ["Ouyang", "Bing", ""], ["Vuorenkoski", "Anni", ""], ["Dalgleish", "Fraser", ""], ["Ramos", "Brian", ""], ["Li", "Yanjun", ""]]}, {"id": "1911.03281", "submitter": "Zuheng Ming", "authors": "Zuheng Ming, Junshi Xia, Muhammad Muzzamil Luqman, Jean-Christophe\n  Burie, Kaixing Zhao", "title": "Dynamic Multi-Task Learning for Face Recognition with Facial Expression", "comments": "accept by the ICCV2019 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benefiting from the joint learning of the multiple tasks in the deep\nmulti-task networks, many applications have shown the promising performance\ncomparing to single-task learning. However, the performance of multi-task\nlearning framework is highly dependant on the relative weights of the tasks.\nHow to assign the weight of each task is a critical issue in the multi-task\nlearning. Instead of tuning the weights manually which is exhausted and\ntime-consuming, in this paper we propose an approach which can dynamically\nadapt the weights of the tasks according to the difficulty for training the\ntask. Specifically, the proposed method does not introduce the hyperparameters\nand the simple structure allows the other multi-task deep learning networks can\neasily realize or reproduce this method. We demonstrate our approach for face\nrecognition with facial expression and facial expression recognition from a\nsingle input image based on a deep multi-task learning Conventional Neural\nNetworks (CNNs). Both the theoretical analysis and the experimental results\ndemonstrate the effectiveness of the proposed dynamic multi-task learning\nmethod. This multi-task learning with dynamic weights also boosts of the\nperformance on the different tasks comparing to the state-of-art methods with\nsingle-task learning.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 14:30:48 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Ming", "Zuheng", ""], ["Xia", "Junshi", ""], ["Luqman", "Muhammad Muzzamil", ""], ["Burie", "Jean-Christophe", ""], ["Zhao", "Kaixing", ""]]}, {"id": "1911.03341", "submitter": "Zuheng Ming", "authors": "Zuheng Ming, Jean-Christophe Burie, Muhammad Muzzamil Luqman", "title": "Dynamic Deep Multi-task Learning for Caricature-Visual Face Recognition", "comments": "accepted by the GREC2019@ICDAR2019. arXiv admin note: text overlap\n  with arXiv: 1911.03281", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rather than the visual images, the face recognition of the caricatures is far\nfrom the performance of the visual images. The challenge is the extreme\nnon-rigid distortions of the caricatures introduced by exaggerating the facial\nfeatures to strengthen the characters. In this paper, we propose dynamic\nmulti-task learning based on deep CNNs for cross-modal caricature-visual face\nrecognition. Instead of the conventional multi-task learning with fixed weights\nof the tasks, the proposed dynamic multi-task learning dynamically updates the\nweights of tasks according to the importance of the tasks, which enables the\ntraining of the networks focus on the hard task instead of being stuck in the\novertraining of the easy task. The experimental results demonstrate the\neffectiveness of the dynamic multi-task learning for caricature-visual face\nrecognition. The performance evaluated on the datasets CaVI and WebCaricature\nshow the superiority over the state-of-art methods. The implementation code is\navailable here.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 16:04:08 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Ming", "Zuheng", ""], ["Burie", "Jean-Christophe", ""], ["Luqman", "Muhammad Muzzamil", ""]]}, {"id": "1911.03346", "submitter": "Marcel B\\\"uhler", "authors": "Marcel B\\\"uhler, Seonwook Park, Shalini De Mello, Xucong Zhang, Otmar\n  Hilliges", "title": "Content-Consistent Generation of Realistic Eyes with Style", "comments": "4 pages, 4 figures, ICCV Workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately labeled real-world training data can be scarce, and hence recent\nworks adapt, modify or generate images to boost target datasets. However,\nretaining relevant details from input data in the generated images is\nchallenging and failure could be critical to the performance on the final task.\nIn this work, we synthesize person-specific eye images that satisfy a given\nsemantic segmentation mask (content), while following the style of a specified\nperson from only a few reference images. We introduce two approaches, (a) one\nused to win the OpenEDS Synthetic Eye Generation Challenge at ICCV 2019, and\n(b) a principled approach to solving the problem involving simultaneous\ninjection of style and content information at multiple scales. Our\nimplementation is available at https://github.com/mcbuehler/Seg2Eye.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 16:12:59 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["B\u00fchler", "Marcel", ""], ["Park", "Seonwook", ""], ["De Mello", "Shalini", ""], ["Zhang", "Xucong", ""], ["Hilliges", "Otmar", ""]]}, {"id": "1911.03372", "submitter": "Jorge E. Camargo PhD", "authors": "Diego A. Cruz, Sergio S. Lopez, Jorge E. Camargo", "title": "Automatic Identification of Traditional Colombian Music Genres based on\n  Audio Content Analysis and Machine Learning Technique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colombia has a diversity of genres in traditional music, which allows to\nexpress the richness of the Colombian culture according to the region. This\nmusical diversity is the result of a mixture of African, native Indigenous, and\nEuropean influences. Organizing large collections of songs is a time consuming\ntask that requires that a human listens to fragments of audio to identify\ngenre, singer, year, instruments and other relevant characteristics that allow\nto index the song dataset. This paper presents a method to automatically\nidentify the genre of a Colombian song by means of its audio content. The\nmethod extracts audio features that are used to train a machine learning model\nthat learns to classify the genre. The method was evaluated in a dataset of 180\nmusical pieces belonging to six folkloric Colombian music genres: Bambuco,\nCarranga, Cumbia, Joropo, Pasillo, and Vallenato. Results show that it is\npossible to automatically identify the music genre in spite of the complexity\nof Colombian rhythms reaching an average accuracy of 69\\%.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 16:53:39 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Cruz", "Diego A.", ""], ["Lopez", "Sergio S.", ""], ["Camargo", "Jorge E.", ""]]}, {"id": "1911.03391", "submitter": "Abdallah Benzine", "authors": "Abdallah Benzine, Bertrand Luvison, Quoc Cuong Pham, Catherine Achard", "title": "Single-shot 3D multi-person pose estimation in complex images", "comments": null, "journal-ref": null, "doi": "10.1016/j.patcog.2020.107534", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new single shot method for multi-person 3D human\npose estimation in complex images. The model jointly learns to locate the human\njoints in the image, to estimate their 3D coordinates and to group these\npredictions into full human skeletons. The proposed method deals with a\nvariable number of people and does not need bounding boxes to estimate the 3D\nposes. It leverages and extends the Stacked Hourglass Network and its\nmulti-scale feature learning to manage multi-person situations. Thus, we\nexploit a robust 3D human pose formulation to fully describe several 3D human\nposes even in case of strong occlusions or crops. Then, joint grouping and\nhuman pose estimation for an arbitrary number of people are performed using the\nassociative embedding method. Our approach significantly outperforms the state\nof the art on the challenging CMU Panoptic and a previous single shot method on\nthe MuPoTS-3D dataset. Furthermore, it leads to good results on the complex and\nsynthetic images from the newly proposed JTA Dataset.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 17:13:33 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 09:59:22 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Benzine", "Abdallah", ""], ["Luvison", "Bertrand", ""], ["Pham", "Quoc Cuong", ""], ["Achard", "Catherine", ""]]}, {"id": "1911.03443", "submitter": "Rudrasis Chakraborty Dr.", "authors": "Liu Yang and Rudrasis Chakraborty", "title": "An \"augmentation-free\" rotation invariant classification scheme on\n  point-cloud and its application to neuroimaging", "comments": "arXiv admin note: text overlap with arXiv:1910.13050 and\n  arXiv:1911.01705", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the emergence and increasing popularity of 3D\nmedical imaging techniques with the development of 3D sensors and technology.\nHowever, achieving geometric invariance in the processing of 3D medical images\nis computationally expensive but nonetheless essential due to the presence of\npossible errors caused by rigid registration techniques. An alternative way to\nanalyze medical imaging is by understanding the 3D shapes represented in terms\nof point-cloud. Though in the medical imaging community, 3D point-cloud\nprocessing is not a \"go-to\" choice, it is a canonical way to preserve rotation\ninvariance. Unfortunately, due to the presence of discrete topology, one can\nnot use the standard convolution operator on point-cloud. To the best of our\nknowledge, the existing ways to do \"convolution\" can not preserve the rotation\ninvariance without explicit data augmentation. Therefore, we propose a rotation\ninvariant convolution operator by inducing topology from hypersphere.\nExperimental validation has been performed on publicly available OASIS dataset\nin terms of classification accuracy between subjects with (without) dementia,\ndemonstrating the usefulness of our proposed method in terms of model\ncomplexity, classification accuracy, and last but most important invariance to\nrotations.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 10:45:56 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Yang", "Liu", ""], ["Chakraborty", "Rudrasis", ""]]}, {"id": "1911.03461", "submitter": "Shanxin Yuan", "authors": "Shanxin Yuan, Radu Timofte, Gregory Slabaugh, Ales Leonardis, Bolun\n  Zheng, Xin Ye, Xiang Tian, Yaowu Chen, Xi Cheng, Zhenyong Fu, Jian Yang, Ming\n  Hong, Wenying Lin, Wenjin Yang, Yanyun Qu, Hong-Kyu Shin, Joon-Yeon Kim,\n  Sung-Jea Ko, Hang Dong, Yu Guo, Jie Wang, Xuan Ding, Zongyan Han, Sourya\n  Dipta Das, Kuldeep Purohit, Praveen Kandula, Maitreya Suin, A. N. Rajagopalan", "title": "AIM 2019 Challenge on Image Demoireing: Methods and Results", "comments": "arXiv admin note: text overlap with arXiv:1911.02498", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the first-ever image demoireing challenge that was part of\nthe Advances in Image Manipulation (AIM) workshop, held in conjunction with\nICCV 2019. This paper describes the challenge, and focuses on the proposed\nsolutions and their results. Demoireing is a difficult task of removing moire\npatterns from an image to reveal an underlying clean image. A new dataset,\ncalled LCDMoire was created for this challenge, and consists of 10,200\nsynthetically generated image pairs (moire and clean ground truth). The\nchallenge was divided into 2 tracks. Track 1 targeted fidelity, measuring the\nability of demoire methods to obtain a moire-free image compared with the\nground truth, while Track 2 examined the perceptual quality of demoire methods.\nThe tracks had 60 and 39 registered participants, respectively. A total of\neight teams competed in the final testing phase. The entries span the current\nthe state-of-the-art in the image demoireing problem.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 08:10:12 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Yuan", "Shanxin", ""], ["Timofte", "Radu", ""], ["Slabaugh", "Gregory", ""], ["Leonardis", "Ales", ""], ["Zheng", "Bolun", ""], ["Ye", "Xin", ""], ["Tian", "Xiang", ""], ["Chen", "Yaowu", ""], ["Cheng", "Xi", ""], ["Fu", "Zhenyong", ""], ["Yang", "Jian", ""], ["Hong", "Ming", ""], ["Lin", "Wenying", ""], ["Yang", "Wenjin", ""], ["Qu", "Yanyun", ""], ["Shin", "Hong-Kyu", ""], ["Kim", "Joon-Yeon", ""], ["Ko", "Sung-Jea", ""], ["Dong", "Hang", ""], ["Guo", "Yu", ""], ["Wang", "Jie", ""], ["Ding", "Xuan", ""], ["Han", "Zongyan", ""], ["Das", "Sourya Dipta", ""], ["Purohit", "Kuldeep", ""], ["Kandula", "Praveen", ""], ["Suin", "Maitreya", ""], ["Rajagopalan", "A. N.", ""]]}, {"id": "1911.03462", "submitter": "Umberto Michieli", "authors": "Umberto Michieli and Pietro Zanuttigh", "title": "Knowledge Distillation for Incremental Learning in Semantic Segmentation", "comments": "Computer Vision and Image Understanding (CVIU), 2021. arXiv admin\n  note: text overlap with arXiv:1907.13372", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning architectures have shown remarkable results in scene\nunderstanding problems, however they exhibit a critical drop of performances\nwhen they are required to learn incrementally new tasks without forgetting old\nones. This catastrophic forgetting phenomenon impacts on the deployment of\nartificial intelligence in real world scenarios where systems need to learn new\nand different representations over time. Current approaches for incremental\nlearning deal only with image classification and object detection tasks, while\nin this work we formally introduce incremental learning for semantic\nsegmentation. We tackle the problem applying various knowledge distillation\ntechniques on the previous model. In this way, we retain the information about\nlearned classes, whilst updating the current model to learn the new ones. We\ndeveloped four main methodologies of knowledge distillation working on both\noutput layers and internal feature representations. We do not store any image\nbelonging to previous training stages and only the last model is used to\npreserve high accuracy on previously learned classes. Extensive experimental\nresults on the Pascal VOC2012 and MSRC-v2 datasets show the effectiveness of\nthe proposed approaches in several incremental learning scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 08:17:03 GMT"}, {"version": "v2", "created": "Sun, 26 Jan 2020 11:29:10 GMT"}, {"version": "v3", "created": "Tue, 4 Feb 2020 17:53:16 GMT"}, {"version": "v4", "created": "Wed, 20 Jan 2021 20:03:16 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Michieli", "Umberto", ""], ["Zanuttigh", "Pietro", ""]]}, {"id": "1911.03464", "submitter": "Yuan Ma", "authors": "Yuan Ma, Kewen Liu, Hongxia Xiong, Panpan Fang, Xiaojun Li, Yalei\n  Chen, Chaoyang Liu", "title": "Perception-oriented Single Image Super-Resolution via Dual Relativistic\n  Average Generative Adversarial Networks", "comments": "Re-submit after codes reviewing", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The presence of residual and dense neural networks which greatly promotes the\ndevelopment of image Super-Resolution(SR) have witnessed a lot of impressive\nresults. Depending on our observation, although more layers and connections\ncould always improve performance, the increase of model parameters is not\nconducive to launch application of SR algorithms. Furthermore, algorithms\nsupervised by L1/L2 loss can achieve considerable performance on traditional\nmetrics such as PSNR and SSIM, yet resulting in blurry and over-smoothed\noutputs without sufficient high-frequency details, namely low perceptual\nindex(PI). Regarding the issues, this paper develops a perception-oriented\nsingle image SR algorithm via dual relativistic average generative adversarial\nnetworks. In the generator part, a novel residual channel attention block is\nproposed to recalibrate significance of specific channels, further increasing\nfeature expression capabilities. Parameters of convolutional layers within each\nblock are shared to expand receptive fields while maintain the amount of\ntunable parameters unchanged. The feature maps are subsampled using sub-pixel\nconvolution to obtain reconstructed high-resolution images. The discriminator\npart consists of two relativistic average discriminators that work in pixel\ndomain and feature domain, respectively, fully exploiting the prior that half\nof data in a mini-batch are fake. Different weighted combinations of perceptual\nloss and adversarial loss are utilized to supervise the generator to\nequilibrate perceptual quality and objective results. Experimental results and\nablation studies show that our proposed algorithm can rival state-of-the-art SR\nalgorithms, both perceptually(PI-minimization) and\nobjectively(PSNR-maximization) with fewer parameters.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 11:09:43 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 08:00:53 GMT"}, {"version": "v3", "created": "Thu, 20 Feb 2020 06:34:58 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Ma", "Yuan", ""], ["Liu", "Kewen", ""], ["Xiong", "Hongxia", ""], ["Fang", "Panpan", ""], ["Li", "Xiaojun", ""], ["Chen", "Yalei", ""], ["Liu", "Chaoyang", ""]]}, {"id": "1911.03472", "submitter": "Matthias Zisler", "authors": "Matthias Zisler, Artjom Zern, Stefania Petra, Christoph Schn\\\"orr", "title": "Self-Assignment Flows for Unsupervised Data Labeling on Graphs", "comments": "42 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends the recently introduced assignment flow approach for\nsupervised image labeling to unsupervised scenarios where no labels are given.\nThe resulting self-assignment flow takes a pairwise data affinity matrix as\ninput data and maximizes the correlation with a low-rank matrix that is\nparametrized by the variables of the assignment flow, which entails an\nassignment of the data to themselves through the formation of latent labels\n(feature prototypes). A single user parameter, the neighborhood size for the\ngeometric regularization of assignments, drives the entire process. By smooth\ngeodesic interpolation between different normalizations of self-assignment\nmatrices on the positive definite matrix manifold, a one-parameter family of\nself-assignment flows is defined. Accordingly, our approach can be\ncharacterized from different viewpoints, e.g. as performing spatially\nregularized, rank-constrained discrete optimal transport, or as computing\nspatially regularized normalized spectral cuts. Regarding combinatorial\noptimization, our approach successfully determines completely positive\nfactorizations of self-assignments in large-scale scenarios, subject to spatial\nregularization. Various experiments including the unsupervised learning of\npatch dictionaries using a locally invariant distance function, illustrate the\nproperties of the approach.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 16:35:13 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 12:35:23 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Zisler", "Matthias", ""], ["Zern", "Artjom", ""], ["Petra", "Stefania", ""], ["Schn\u00f6rr", "Christoph", ""]]}, {"id": "1911.03558", "submitter": "Xuan Xu", "authors": "Xuan Xu, Yanfang Ye and Xin Li", "title": "Joint Demosaicing and Super-Resolution (JDSR): Network Design and\n  Perceptual Optimization", "comments": "IEEE Transactions on Computational Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image demosaicing and super-resolution are two important tasks in color\nimaging pipeline. So far they have been mostly independently studied in the\nopen literature of deep learning; little is known about the potential benefit\nof formulating a joint demosaicing and super-resolution (JDSR) problem. In this\npaper, we propose an end-to-end optimization solution to the JDSR problem and\ndemonstrate its practical significance in computational imaging. Our technical\ncontributions are mainly two-fold. On network design, we have developed a\nResidual-Dense Squeeze-and-Excitation Networks (RDSEN) supported by a\npre-demosaicing network (PDNet) as the pre-processing step. We address the\nissue of spatio-spectral attention for color-filter-array (CFA) data and\ndiscuss how to achieve better information flow by concatenating Residue-Dense\nSqueeze-and-Excitation Blocks (RDSEBs) for JDSR. Experimental results have\nshown that significant PSNR/SSIM gain can be achieved by RDSEN over previous\nnetwork architectures including state-of-the-art RCAN. On perceptual\noptimization, we propose to leverage the latest ideas including relativistic\ndiscriminator and pre-excitation perceptual loss function to further improve\nthe visual quality of textured regions in reconstructed images. Our extensive\nexperiment results have shown that Texture-enhanced Relativistic average\nGenerative Adversarial Network (TRaGAN) can produce both subjectively more\npleasant images and objectively lower perceptual distortion scores than\nstandard GAN for JDSR. Finally, we have verified the benefit of JDSR to\nhigh-quality image reconstruction from real-world Bayer pattern data collected\nby NASA Mars Curiosity.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 22:01:09 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 20:32:53 GMT"}, {"version": "v3", "created": "Tue, 4 Aug 2020 00:21:28 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Xu", "Xuan", ""], ["Ye", "Yanfang", ""], ["Li", "Xin", ""]]}, {"id": "1911.03565", "submitter": "Chao Wang", "authors": "Zhensong Wei, Chao Wang, Peng Hao, and Matthew Barth", "title": "Vision-Based Lane-Changing Behavior Detection Using Deep Residual Neural\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate lane localization and lane change detection are crucial in advanced\ndriver assistance systems and autonomous driving systems for safer and more\nefficient trajectory planning. Conventional localization devices such as Global\nPositioning System only provide road-level resolution for car navigation, which\nis incompetent to assist in lane-level decision making. The state of art\ntechnique for lane localization is to use Light Detection and Ranging sensors\nto correct the global localization error and achieve centimeter-level accuracy,\nbut the real-time implementation and popularization for LiDAR is still limited\nby its computational burden and current cost. As a cost-effective alternative,\nvision-based lane change detection has been highly regarded for affordable\nautonomous vehicles to support lane-level localization. A deep learning-based\ncomputer vision system is developed to detect the lane change behavior using\nthe images captured by a front-view camera mounted on the vehicle and data from\nthe inertial measurement unit for highway driving. Testing results on\nreal-world driving data have shown that the proposed method is robust with\nreal-time working ability and could achieve around 87% lane change detection\naccuracy. Compared to the average human reaction to visual stimuli, the\nproposed computer vision system works 9 times faster, which makes it capable of\nhelping make life-saving decisions in time.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 22:28:53 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Wei", "Zhensong", ""], ["Wang", "Chao", ""], ["Hao", "Peng", ""], ["Barth", "Matthew", ""]]}, {"id": "1911.03567", "submitter": "Zuheng Ming", "authors": "Souhail Bakkali, Zuheng Ming, Muhammad Muzzamil Luqman,\n  Jean-Christophe Burie", "title": "Face Detection in Camera Captured Images of Identity Documents under\n  Challenging Conditions", "comments": "accepted by the ICDAR2019 workshop CBDAR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benefiting from the advance of deep convolutional neural network approaches\n(CNNs), many face detection algorithms have achieved state-of-the-art\nperformance in terms of accuracy and very high speed in unconstrained\napplications. However, due to the lack of public datasets and due to the\nvariation of the orientation of face images, the complex background and\nlighting, defocus and the varying illumination of camera captured images, face\ndetection on identity documents under unconstrained environments has not been\nsufficiently studied. To address this problem more efficiently, we survey three\nstate-of-the-art face detection methods based on general images, i.e.\nCascade-CNN, MTCNN and PCN, for face detection in camera captured images of\nidentity documents, given different image quality assessments. For that, The\nMIDV-500 dataset, which is the largest and most challenging dataset for\nidentity documents, is used to evaluate the three methods. The evaluation\nresults show the performance and the limitations of the current methods for\nface detection on identity documents under the wild complex environments. These\nresults show that the face detection task in camera captured images of identity\ndocuments is challenging, providing a space to improve in the future works.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 22:39:06 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Bakkali", "Souhail", ""], ["Ming", "Zuheng", ""], ["Luqman", "Muhammad Muzzamil", ""], ["Burie", "Jean-Christophe", ""]]}, {"id": "1911.03584", "submitter": "Jean-Baptiste Cordonnier", "authors": "Jean-Baptiste Cordonnier, Andreas Loukas, Martin Jaggi", "title": "On the Relationship between Self-Attention and Convolutional Layers", "comments": "To appear at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent trends of incorporating attention mechanisms in vision have led\nresearchers to reconsider the supremacy of convolutional layers as a primary\nbuilding block. Beyond helping CNNs to handle long-range dependencies,\nRamachandran et al. (2019) showed that attention can completely replace\nconvolution and achieve state-of-the-art performance on vision tasks. This\nraises the question: do learned attention layers operate similarly to\nconvolutional layers? This work provides evidence that attention layers can\nperform convolution and, indeed, they often learn to do so in practice.\nSpecifically, we prove that a multi-head self-attention layer with sufficient\nnumber of heads is at least as expressive as any convolutional layer. Our\nnumerical experiments then show that self-attention layers attend to pixel-grid\npatterns similarly to CNN layers, corroborating our analysis. Our code is\npublicly available.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 23:48:38 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 09:06:09 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Cordonnier", "Jean-Baptiste", ""], ["Loukas", "Andreas", ""], ["Jaggi", "Martin", ""]]}, {"id": "1911.03599", "submitter": "Wan Yan", "authors": "Yuanyuan Xu, Wan Yan, Haixin Sun, Genke Yang, Jiliang Luo", "title": "CenterFace: Joint Face Detection and Alignment Using Face as Point", "comments": "11 pages, 3 figures. A demo of CenterFace can be available at\n  https://github.com/Star-Clouds/CenterFace", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection and alignment in unconstrained environment is always deployed\non edge devices which have limited memory storage and low computing power. This\npaper proposes a one-stage method named CenterFace to simultaneously predict\nfacial box and landmark location with real-time speed and high accuracy. The\nproposed method also belongs to the anchor free category. This is achieved by:\n(a) learning face existing possibility by the semantic maps, (b) learning\nbounding box, offsets and five landmarks for each position that potentially\ncontains a face. Specifically, the method can run in real-time on a single CPU\ncore and 200 FPS using NVIDIA 2080TI for VGA-resolution images, and can\nsimultaneously achieve superior accuracy (WIDER FACE Val/Test-Easy:\n0.935/0.932, Medium: 0.924/0.921, Hard: 0.875/0.873 and FDDB discontinuous:\n0.980, continuous: 0.732). A demo of CenterFace can be available at\nhttps://github.com/Star-Clouds/CenterFace.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 03:06:11 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Xu", "Yuanyuan", ""], ["Yan", "Wan", ""], ["Sun", "Haixin", ""], ["Yang", "Genke", ""], ["Luo", "Jiliang", ""]]}, {"id": "1911.03603", "submitter": "Ramanpreet Pahwa Singh", "authors": "Ramanpreet Singh Pahwa, Kennard Yanting Chan, Jiamin Bai, Vincensius\n  Billy Saputra, Minh N.Do, and Shaohui Foong", "title": "Dense 3D Reconstruction for Visual Tunnel Inspection using Unmanned\n  Aerial Vehicle", "comments": "8 pages, 12 figures", "journal-ref": "IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS), 2019", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in Unmanned Aerial Vehicle (UAV) opens venues for application such\nas tunnel inspection. Owing to its versatility to fly inside the tunnels, it\ncan quickly identify defects and potential problems related to safety. However,\nlong tunnels, especially with repetitive or uniform structures pose a\nsignificant problem for UAV navigation. Furthermore, post-processing visual\ndata from the camera mounted on the UAV is required to generate useful\ninformation for the inspection task. In this work, we design a UAV with a\nsingle rotating camera to accomplish the task. Compared to other platforms, our\nsolution can fit the stringent requirement for tunnel inspection, in terms of\nbattery life, size and weight. While the current state-of-the-art can estimate\ncamera pose and 3D geometry from a sequence of images, they assume large\noverlap, small rotational motion, and many distinct matching points between\nimages. These assumptions severely limit their effectiveness in tunnel-like\nscenarios where the camera has erratic or large rotational motion, such as the\none mounted on the UAV. This paper presents a novel solution which exploits\nStructure-from-Motion, Bundle Adjustment, and available geometry priors to\nrobustly estimate camera pose and automatically reconstruct a fully-dense 3D\nscene using the least possible number of images in various challenging\ntunnel-like environments. We validate our system with both Virtual Reality\napplication and experimentation with a real dataset. The results demonstrate\nthat the proposed reconstruction along with texture mapping allows for remote\nnavigation and inspection of tunnel-like environments, even those which are\ninaccessible for humans.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 03:22:10 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Pahwa", "Ramanpreet Singh", ""], ["Chan", "Kennard Yanting", ""], ["Bai", "Jiamin", ""], ["Saputra", "Vincensius Billy", ""], ["Do", "Minh N.", ""], ["Foong", "Shaohui", ""]]}, {"id": "1911.03607", "submitter": "Ke Xu", "authors": "Ke Xu, Kaiyu Guan, Jian Peng, Yunan Luo, Sibo Wang", "title": "DeepMask: an algorithm for cloud and cloud shadow detection in optical\n  satellite remote sensing images using deep residual network", "comments": "17 pages, 4 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and masking cloud and cloud shadow from satellite remote sensing\nimages is a pervasive problem in the remote sensing community. Accurate and\nefficient detection of cloud and cloud shadow is an essential step to harness\nthe value of remotely sensed data for almost all downstream analysis. DeepMask,\na new algorithm for cloud and cloud shadow detection in optical satellite\nremote sensing imagery, is proposed in this study. DeepMask utilizes ResNet, a\ndeep convolutional neural network, for pixel-level cloud mask generation. The\nalgorithm is trained and evaluated on the Landsat 8 Cloud Cover Assessment\nValidation Dataset distributed across 8 different land types. Compared with\nCFMask, the most widely used cloud detection algorithm, land-type-specific\nDeepMask models achieve higher accuracy across all land types. The average\naccuracy is 93.56%, compared with 85.36% from CFMask. DeepMask also achieves\n91.02% accuracy on all-land-type dataset. Compared with other CNN-based cloud\nmask algorithms, DeepMask benefits from the parsimonious architecture and the\nresidual connection of ResNet. It is compatible with input of any size and\nshape. DeepMask still maintains high performance when using only red, green,\nblue, and NIR bands, indicating its potential to be applied to other satellite\nplatforms that only have limited optical bands.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 03:44:07 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Xu", "Ke", ""], ["Guan", "Kaiyu", ""], ["Peng", "Jian", ""], ["Luo", "Yunan", ""], ["Wang", "Sibo", ""]]}, {"id": "1911.03621", "submitter": "Heliang Zheng", "authors": "Heliang Zheng, Jianlong Fu, Zheng-Jun Zha, and Jiebo Luo", "title": "Learning Deep Bilinear Transformation for Fine-grained Image\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bilinear feature transformation has shown the state-of-the-art performance in\nlearning fine-grained image representations. However, the computational cost to\nlearn pairwise interactions between deep feature channels is prohibitively\nexpensive, which restricts this powerful transformation to be used in deep\nneural networks. In this paper, we propose a deep bilinear transformation (DBT)\nblock, which can be deeply stacked in convolutional neural networks to learn\nfine-grained image representations. The DBT block can uniformly divide input\nchannels into several semantic groups. As bilinear transformation can be\nrepresented by calculating pairwise interactions within each group, the\ncomputational cost can be heavily relieved. The output of each block is further\nobtained by aggregating intra-group bilinear features, with residuals from the\nentire input features. We found that the proposed network achieves new\nstate-of-the-art in several fine-grained image recognition benchmarks,\nincluding CUB-Bird, Stanford-Car, and FGVC-Aircraft.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 06:33:54 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Zheng", "Heliang", ""], ["Fu", "Jianlong", ""], ["Zha", "Zheng-Jun", ""], ["Luo", "Jiebo", ""]]}, {"id": "1911.03624", "submitter": "Jae Woong Soh", "authors": "Jae Woong Soh, Gu Yong Park, Junho Jo, Nam Ik Cho", "title": "Natural and Realistic Single Image Super-Resolution with Explicit\n  Natural Manifold Discrimination", "comments": "Presented in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many convolutional neural networks for single image\nsuper-resolution (SISR) have been proposed, which focus on reconstructing the\nhigh-resolution images in terms of objective distortion measures. However, the\nnetworks trained with objective loss functions generally fail to reconstruct\nthe realistic fine textures and details that are essential for better\nperceptual quality. Recovering the realistic details remains a challenging\nproblem, and only a few works have been proposed which aim at increasing the\nperceptual quality by generating enhanced textures. However, the generated fake\ndetails often make undesirable artifacts and the overall image looks somewhat\nunnatural. Therefore, in this paper, we present a new approach to\nreconstructing realistic super-resolved images with high perceptual quality,\nwhile maintaining the naturalness of the result. In particular, we focus on the\ndomain prior properties of SISR problem. Specifically, we define the\nnaturalness prior in the low-level domain and constrain the output image in the\nnatural manifold, which eventually generates more natural and realistic images.\nOur results show better naturalness compared to the recent super-resolution\nalgorithms including perception-oriented ones.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 06:48:53 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Soh", "Jae Woong", ""], ["Park", "Gu Yong", ""], ["Jo", "Junho", ""], ["Cho", "Nam Ik", ""]]}, {"id": "1911.03630", "submitter": "Saeed Reza Kheradpisheh", "authors": "Aref Moqadam Mehr, Saeed Reza Kheradpisheh, Hadi Farahani", "title": "Action Recognition Using Supervised Spiking Neural Networks", "comments": "We found a bug in our implementations and we should admit that our\n  reported results were wrong", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological neurons use spikes to process and learn temporally dynamic inputs\nin an energy and computationally efficient way. However, applying the\nstate-of-the-art gradient-based supervised algorithms to spiking neural\nnetworks (SNN) is a challenge due to the non-differentiability of the\nactivation function of spiking neurons. Employing surrogate gradients is one of\nthe main solutions to overcome this challenge. Although SNNs naturally work in\nthe temporal domain, recent studies have focused on developing SNNs to solve\nstatic image categorization tasks. In this paper, we employ a surrogate\ngradient descent learning algorithm to recognize twelve human hand gestures\nrecorded by dynamic vision sensor (DVS) cameras. The proposed SNN could reach\n97.2% recognition accuracy on test data.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 07:16:10 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2020 11:07:11 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Mehr", "Aref Moqadam", ""], ["Kheradpisheh", "Saeed Reza", ""], ["Farahani", "Hadi", ""]]}, {"id": "1911.03678", "submitter": "Desmond Elliott", "authors": "\\'Akos K\\'ad\\'ar, Grzegorz Chrupa{\\l}a, Afra Alishahi, Desmond Elliott", "title": "Bootstrapping Disjoint Datasets for Multilingual Multimodal\n  Representation Learning", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has highlighted the advantage of jointly learning grounded\nsentence representations from multiple languages. However, the data used in\nthese studies has been limited to an aligned scenario: the same images\nannotated with sentences in multiple languages. We focus on the more realistic\ndisjoint scenario in which there is no overlap between the images in\nmultilingual image--caption datasets. We confirm that training with aligned\ndata results in better grounded sentence representations than training with\ndisjoint data, as measured by image--sentence retrieval performance. In order\nto close this gap in performance, we propose a pseudopairing method to generate\nsynthetically aligned English--German--image triplets from the disjoint sets.\nThe method works by first training a model on the disjoint data, and then\ncreating new triples across datasets using sentence similarity under the\nlearned model. Experiments show that pseudopairs improve image--sentence\nretrieval performance compared to disjoint training, despite requiring no\nexternal data or models. However, we do find that using an external machine\ntranslation model to generate the synthetic data sets results in better\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 12:34:01 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["K\u00e1d\u00e1r", "\u00c1kos", ""], ["Chrupa\u0142a", "Grzegorz", ""], ["Alishahi", "Afra", ""], ["Elliott", "Desmond", ""]]}, {"id": "1911.03705", "submitter": "Bill Yuchen Lin", "authors": "Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra\n  Bhagavatula, Yejin Choi, Xiang Ren", "title": "CommonGen: A Constrained Text Generation Challenge for Generative\n  Commonsense Reasoning", "comments": "Accepted to EMNLP 2020 Findings. Add one more human reference for\n  each test example: Table 1,3 & Figure 4 & Section 3.3, 3.4 are updated.\n  Project page: https://inklab.usc.edu/CommonGen/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, large-scale pre-trained language models have demonstrated\nimpressive performance on several commonsense-reasoning benchmark datasets.\nHowever, building machines with commonsense to compose realistically plausible\nsentences remains challenging. In this paper, we present a constrained text\ngeneration task, CommonGen associated with a benchmark dataset, to explicitly\ntest machines for the ability of generative commonsense reasoning. Given a set\nof common concepts (e.g., {dog, frisbee, catch, throw}); the task is to\ngenerate a coherent sentence describing an everyday scenario using these\nconcepts (e.g., \"a man throws a frisbee and his dog catches it\").\n  The CommonGen task is challenging because it inherently requires 1)\nrelational reasoning with background commonsense knowledge, and 2)\ncompositional generalization ability to work on unseen concept combinations.\nOur dataset, constructed through a combination of crowdsourced and existing\ncaption corpora, consists of 79k commonsense descriptions over 35k unique\nconcept-sets. Experiments show that there is a large gap between\nstate-of-the-art text generation models (e.g., T5) and human performance.\nFurthermore, we demonstrate that the learned generative commonsense reasoning\ncapability can be transferred to improve downstream tasks such as CommonsenseQA\nby generating additional context.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 14:53:59 GMT"}, {"version": "v2", "created": "Sat, 2 May 2020 01:26:38 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2020 00:57:08 GMT"}, {"version": "v4", "created": "Mon, 30 Nov 2020 07:53:50 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Lin", "Bill Yuchen", ""], ["Zhou", "Wangchunshu", ""], ["Shen", "Ming", ""], ["Zhou", "Pei", ""], ["Bhagavatula", "Chandra", ""], ["Choi", "Yejin", ""], ["Ren", "Xiang", ""]]}, {"id": "1911.03711", "submitter": "Muhammad Ahmad", "authors": "Muhammad Hussain Khan, Zainab Saleem, Muhammad Ahmad, Ahmed Sohaib,\n  Hamail Ayaz", "title": "Unsupervised adulterated red-chili pepper content transformation for\n  hyperspectral classification", "comments": "10 pages,", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preserving red-chili quality is of utmost importance in which the authorities\ndemand the quality techniques to detect, classify and prevent it from the\nimpurities. For example, salt, wheat flour, wheat bran, and rice bran\ncontamination in grounded red chili, which typically a food, are a serious\nthreat to people who are allergic to such items. This work presents the\nfeasibility of utilizing visible and near-infrared (VNIR) hyperspectral imaging\n(HSI) to detect and classify the aforementioned adulterants in red chili.\nHowever, adulterated red chili data annotation is a big challenge for\nclassification because the acquisition of labeled data for real-time supervised\nlearning is expensive in terms of cost and time. Therefore, this study, for the\nvery first time proposes a novel approach to annotate the red chili samples\nusing a clustering mechanism at 500~nm wavelength spectral response due to its\ndark appearance at a specified wavelength. Later the spectral samples are\nclassified into pure or adulterated using one-class SVM. The classification\nperformance achieves 99% in case of pure adulterants or red chili whereas 85%\nfor adulterated samples. We further investigate that the single classification\nmodel is enough to detect any foreign substance in red chili pepper rather than\ncascading multiple PLS regression models.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 15:12:27 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Khan", "Muhammad Hussain", ""], ["Saleem", "Zainab", ""], ["Ahmad", "Muhammad", ""], ["Sohaib", "Ahmed", ""], ["Ayaz", "Hamail", ""]]}, {"id": "1911.03723", "submitter": "Chen Chen", "authors": "Chen Chen, Chen Qin, Huaqi Qiu, Giacomo Tarroni, Jinming Duan, Wenjia\n  Bai, and Daniel Rueckert", "title": "Deep learning for cardiac image segmentation: A review", "comments": "Under review", "journal-ref": null, "doi": "10.3389/fcvm.2020.00025", "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has become the most widely used approach for cardiac image\nsegmentation in recent years. In this paper, we provide a review of over 100\ncardiac image segmentation papers using deep learning, which covers common\nimaging modalities including magnetic resonance imaging (MRI), computed\ntomography (CT), and ultrasound (US) and major anatomical structures of\ninterest (ventricles, atria and vessels). In addition, a summary of publicly\navailable cardiac image datasets and code repositories are included to provide\na base for encouraging reproducible research. Finally, we discuss the\nchallenges and limitations with current deep learning-based approaches\n(scarcity of labels, model generalizability across different domains,\ninterpretability) and suggest potential directions for future research.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 15:58:48 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Chen", "Chen", ""], ["Qin", "Chen", ""], ["Qiu", "Huaqi", ""], ["Tarroni", "Giacomo", ""], ["Duan", "Jinming", ""], ["Bai", "Wenjia", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1911.03740", "submitter": "Sheng Liu", "authors": "Sheng Liu, Chhavi Yadav, Carlos Fernandez-Granda, Narges Razavian", "title": "On the design of convolutional neural networks for automatic detection\n  of Alzheimer's disease", "comments": "Machine Learning for Health Workshop, NeurIPS2019. Authors\n  Fernandez-Granda and Razavian are joint last authors", "journal-ref": "Proceedings of Machine Learning Research, 2019", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection is a crucial goal in the study of Alzheimer's Disease (AD).\nIn this work, we describe several techniques to boost the performance of 3D\ndeep convolutional neural networks (CNNs) trained to detect AD using structural\nbrain MRI scans. Specifically, we provide evidence that (1) instance\nnormalization outperforms batch normalization, (2) early spatial downsampling\nnegatively affects performance, (3) widening the model brings consistent gains\nwhile increasing the depth does not, and (4) incorporating age information\nyields moderate improvement. Together, these insights yield an increment of\napproximately 14% in test accuracy over existing models when distinguishing\nbetween patients with AD, mild cognitive impairment, and controls in the ADNI\ndataset. Similar performance is achieved on an independent dataset.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 17:08:34 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 18:18:06 GMT"}, {"version": "v3", "created": "Sun, 5 Apr 2020 01:47:59 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Liu", "Sheng", ""], ["Yadav", "Chhavi", ""], ["Fernandez-Granda", "Carlos", ""], ["Razavian", "Narges", ""]]}, {"id": "1911.03786", "submitter": "Fabian Balsiger", "authors": "Fabian Balsiger, Alain Jungo, Olivier Scheidegger, Pierre G. Carlier,\n  Mauricio Reyes, Benjamin Marty", "title": "Spatially Regularized Parametric Map Reconstruction for Fast Magnetic\n  Resonance Fingerprinting", "comments": "Accepted to Medical Image Analysis", "journal-ref": "Medical Image Analysis (2020), 64, 101741", "doi": "10.1016/j.media.2020.101741", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance fingerprinting (MRF) provides a unique concept for\nsimultaneous and fast acquisition of multiple quantitative MR parameters.\nDespite acquisition efficiency, adoption of MRF into the clinics is hindered by\nits dictionary matching-based reconstruction, which is computationally\ndemanding and lacks scalability. Here, we propose a convolutional neural\nnetwork-based reconstruction, which enables both accurate and fast\nreconstruction of parametric maps, and is adaptable based on the needs of\nspatial regularization and the capacity for the reconstruction. We evaluated\nthe method using MRF T1-FF, an MRF sequence for T1 relaxation time of water\n(T1H2O) and fat fraction (FF) mapping. We demonstrate the method's performance\non a highly heterogeneous dataset consisting of 164 patients with various\nneuromuscular diseases imaged at thighs and legs. We empirically show the\nbenefit of incorporating spatial regularization during the reconstruction and\ndemonstrate that the method learns meaningful features from MR physics\nperspective. Further, we investigate the ability of the method to handle highly\nheterogeneous morphometric variations and its generalization to anatomical\nregions unseen during training. The obtained results outperform the\nstate-of-the-art in deep learning-based MRF reconstruction. The method achieved\nnormalized root mean squared errors of 0.048 $\\pm$ 0.011 for T1H2O maps and\n0.027 $\\pm$ 0.004 for FF maps when compared to the dictionary matching in a\ntest set of 50 patients. Coupled with fast MRF sequences, the proposed method\nhas the potential of enabling multiparametric MR imaging in clinically feasible\ntime.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 22:10:24 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 15:00:29 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Balsiger", "Fabian", ""], ["Jungo", "Alain", ""], ["Scheidegger", "Olivier", ""], ["Carlier", "Pierre G.", ""], ["Reyes", "Mauricio", ""], ["Marty", "Benjamin", ""]]}, {"id": "1911.03821", "submitter": "Gaurav Sahu", "authors": "Gaurav Sahu, Olga Vechtomova", "title": "Adaptive Fusion Techniques for Multimodal Data", "comments": "Camera-ready version for EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective fusion of data from multiple modalities, such as video, speech, and\ntext, is challenging due to the heterogeneous nature of multimodal data. In\nthis paper, we propose adaptive fusion techniques that aim to model context\nfrom different modalities effectively. Instead of defining a deterministic\nfusion operation, such as concatenation, for the network, we let the network\ndecide \"how\" to combine a given set of multimodal features more effectively. We\npropose two networks: 1) Auto-Fusion, which learns to compress information from\ndifferent modalities while preserving the context, and 2) GAN-Fusion, which\nregularizes the learned latent space given context from complementing\nmodalities. A quantitative evaluation on the tasks of multimodal machine\ntranslation and emotion recognition suggests that our lightweight, adaptive\nnetworks can better model context from other modalities than existing methods,\nmany of which employ massive transformer-based networks.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 01:39:46 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 08:08:02 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Sahu", "Gaurav", ""], ["Vechtomova", "Olga", ""]]}, {"id": "1911.03826", "submitter": "Fuwen Tan", "authors": "Fuwen Tan, Paola Cascante-Bonilla, Xiaoxiao Guo, Hui Wu, Song Feng,\n  Vicente Ordonez", "title": "Drill-down: Interactive Retrieval of Complex Scenes using Natural\n  Language Queries", "comments": "14 pages, 9 figures, NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper explores the task of interactive image retrieval using natural\nlanguage queries, where a user progressively provides input queries to refine a\nset of retrieval results. Moreover, our work explores this problem in the\ncontext of complex image scenes containing multiple objects. We propose\nDrill-down, an effective framework for encoding multiple queries with an\nefficient compact state representation that significantly extends current\nmethods for single-round image retrieval. We show that using multiple rounds of\nnatural language queries as input can be surprisingly effective to find\narbitrarily specific images of complex scenes. Furthermore, we find that\nexisting image datasets with textual captions can provide a surprisingly\neffective form of weak supervision for this task. We compare our method with\nexisting sequential encoding and embedding networks, demonstrating superior\nperformance on two proposed benchmarks: automatic image retrieval on a\nsimulated scenario that uses region captions as queries, and interactive image\nretrieval using real queries from human evaluators.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 01:50:16 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Tan", "Fuwen", ""], ["Cascante-Bonilla", "Paola", ""], ["Guo", "Xiaoxiao", ""], ["Wu", "Hui", ""], ["Feng", "Song", ""], ["Ordonez", "Vicente", ""]]}, {"id": "1911.03849", "submitter": "Xinghua Qu", "authors": "Xinghua Qu, Zhu Sun, Yew-Soon Ong, Abhishek Gupta, Pengfei Wei", "title": "Minimalistic Attacks: How Little it Takes to Fool a Deep Reinforcement\n  Learning Policy", "comments": "Accepted by IEEE Transactions on Cognitive and Developmental System", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have revealed that neural network-based policies can be easily\nfooled by adversarial examples. However, while most prior works analyze the\neffects of perturbing every pixel of every frame assuming white-box policy\naccess, in this paper we take a more restrictive view towards adversary\ngeneration - with the goal of unveiling the limits of a model's vulnerability.\nIn particular, we explore minimalistic attacks by defining three key settings:\n(1) black-box policy access: where the attacker only has access to the input\n(state) and output (action probability) of an RL policy; (2) fractional-state\nadversary: where only several pixels are perturbed, with the extreme case being\na single-pixel adversary; and (3) tactically-chanced attack: where only\nsignificant frames are tactically chosen to be attacked. We formulate the\nadversarial attack by accommodating the three key settings and explore their\npotency on six Atari games by examining four fully trained state-of-the-art\npolicies. In Breakout, for example, we surprisingly find that: (i) all policies\nshowcase significant performance degradation by merely modifying 0.01% of the\ninput state, and (ii) the policy trained by DQN is totally deceived by\nperturbation to only 1% frames.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 04:39:56 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 08:28:44 GMT"}, {"version": "v3", "created": "Fri, 21 Feb 2020 00:51:06 GMT"}, {"version": "v4", "created": "Fri, 6 Mar 2020 01:46:01 GMT"}, {"version": "v5", "created": "Thu, 29 Oct 2020 13:40:22 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Qu", "Xinghua", ""], ["Sun", "Zhu", ""], ["Ong", "Yew-Soon", ""], ["Gupta", "Abhishek", ""], ["Wei", "Pengfei", ""]]}, {"id": "1911.03852", "submitter": "Amir Gholami", "authors": "Zhen Dong, Zhewei Yao, Yaohui Cai, Daiyaan Arfeen, Amir Gholami,\n  Michael W. Mahoney, Kurt Keutzer", "title": "HAWQ-V2: Hessian Aware trace-Weighted Quantization of Neural Networks", "comments": null, "journal-ref": "NeurIPS 2020 paper, link:\n  https://proceedings.neurips.cc/paper/2020/file/d77c703536718b95308130ff2e5cf9ee-Supplemental.pdf", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantization is an effective method for reducing memory footprint and\ninference time of Neural Networks, e.g., for efficient inference in the cloud,\nespecially at the edge. However, ultra low precision quantization could lead to\nsignificant degradation in model generalization. A promising method to address\nthis is to perform mixed-precision quantization, where more sensitive layers\nare kept at higher precision. However, the search space for a mixed-precision\nquantization is exponential in the number of layers. Recent work has proposed\nHAWQ, a novel Hessian based framework, with the aim of reducing this\nexponential search space by using second-order information. While promising,\nthis prior work has three major limitations: (i) HAWQV1 only uses the top\nHessian eigenvalue as a measure of sensitivity and do not consider the rest of\nthe Hessian spectrum; (ii) HAWQV1 approach only provides relative sensitivity\nof different layers and therefore requires a manual selection of the\nmixed-precision setting; and (iii) HAWQV1 does not consider mixed-precision\nactivation quantization. Here, we present HAWQV2 which addresses these\nshortcomings. For (i), we perform a theoretical analysis showing that a better\nsensitivity metric is to compute the average of all of the Hessian eigenvalues.\nFor (ii), we develop a Pareto frontier based method for selecting the exact bit\nprecision of different layers without any manual selection. For (iii), we\nextend the Hessian analysis to mixed-precision activation quantization. We have\nfound this to be very beneficial for object detection. We show that HAWQV2\nachieves new state-of-the-art results for a wide range of tasks.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 04:46:17 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Dong", "Zhen", ""], ["Yao", "Zhewei", ""], ["Cai", "Yaohui", ""], ["Arfeen", "Daiyaan", ""], ["Gholami", "Amir", ""], ["Mahoney", "Michael W.", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1911.03923", "submitter": "Sara Masoud", "authors": "Sara Masoud, Bijoy Chowdhury, Young-Jun Son, Chieri Kubota, Russell\n  Tronstad", "title": "A Dynamic Modelling Framework for Human Hand Gesture Task Recognition", "comments": "6 pages, 5 figures, 2 tables, conference proceedings", "journal-ref": "(2018). A dynamic modelling framework for human hand gesture task\n  recognition. 563-568. Paper presented at 2018 Institute of Industrial and\n  Systems Engineers Annual Conference and Expo, IISE 2018, Orlando, United\n  States", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gesture recognition and hand motion tracking are important tasks in advanced\ngesture based interaction systems. In this paper, we propose to apply a sliding\nwindows filtering approach to sample the incoming streams of data from data\ngloves and a decision tree model to recognize the gestures in real time for a\nmanual grafting operation of a vegetable seedling propagation facility. The\nsequence of these recognized gestures defines the tasks that are taking place,\nwhich helps to evaluate individuals' performances and to identify any\nbottlenecks in real time. In this work, two pairs of data gloves are utilized,\nwhich reports the location of the fingers, hands, and wrists wirelessly (i.e.,\nvia Bluetooth). To evaluate the performance of the proposed framework, a\npreliminary experiment was conducted in multiple lab settings of tomato\ngrafting operations, where multiple subjects wear the data gloves while\nperforming different tasks. Our results show an accuracy of 91% on average, in\nterms of gesture recognition in real time by employing our proposed framework.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 13:06:48 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 05:01:22 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Masoud", "Sara", ""], ["Chowdhury", "Bijoy", ""], ["Son", "Young-Jun", ""], ["Kubota", "Chieri", ""], ["Tronstad", "Russell", ""]]}, {"id": "1911.03936", "submitter": "Philipp Sadler", "authors": "Philipp Sadler, Tatjana Scheffler and David Schlangen", "title": "Can Neural Image Captioning be Controlled via Forced Attention?", "comments": "Accepted shortpaper for the 12th International Conference on Natural\n  Language Generation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learned dynamic weighting of the conditioning signal (attention) has been\nshown to improve neural language generation in a variety of settings. The\nweights applied when generating a particular output sequence have also been\nviewed as providing a potentially explanatory insight into the internal\nworkings of the generator. In this paper, we reverse the direction of this\nconnection and ask whether through the control of the attention of the model we\ncan control its output. Specifically, we take a standard neural image\ncaptioning model that uses attention, and fix the attention to pre-determined\nareas in the image. We evaluate whether the resulting output is more likely to\nmention the class of the object in that area than the normally generated\ncaption. We introduce three effective methods to control the attention and find\nthat these are producing expected results in up to 28.56% of the cases.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 14:00:27 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Sadler", "Philipp", ""], ["Scheffler", "Tatjana", ""], ["Schlangen", "David", ""]]}, {"id": "1911.03940", "submitter": "Muhammad Hassan Fares", "authors": "Muhammad.H Fares, Hadi Moradi, Mahmoud Shahabadi", "title": "SLTR: Simultaneous Localization of Target and Reflector in NLOS\n  Condition Using Beacons", "comments": "21 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  When the direct view between the target and the observer is not available,\ndue to obstacles with non-zero sizes, the observation is received after\nreflection from a reflector, this is the indirect view or Non-Line-Of Sight\ncondition. Localization of a target in NLOS condition still one of the open\nproblems yet. In this paper, we address this problem by localizing the\nreflector and the target simultaneously using a single stationary receiver, and\na determined number of beacons, in which their placements are also analyzed in\nan unknown map. The work is done in mirror space, when the receiver is a\ncamera, and the reflector is a planar mirror. Furthermore, the distance from\nthe observer to the target is estimated by size constancy concept, and the\nangle of coming signal is the same as the orientation of the camera, with\nrespect to a global frame. The results show the validation of the proposed work\nand the simulation results are matched with the theoretical results.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 14:49:35 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Fares", "Muhammad. H", ""], ["Moradi", "Hadi", ""], ["Shahabadi", "Mahmoud", ""]]}, {"id": "1911.03972", "submitter": "Mohammad Hamed Mozaffari", "authors": "M. Hamed Mozaffari, Md. Aminur Rab Ratul, Won-Sook Lee", "title": "IrisNet: Deep Learning for Automatic and Real-time Tongue Contour\n  Tracking in Ultrasound Video Data using Peripheral Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The progress of deep convolutional neural networks has been successfully\nexploited in various real-time computer vision tasks such as image\nclassification and segmentation. Owing to the development of computational\nunits, availability of digital datasets, and improved performance of deep\nlearning models, fully automatic and accurate tracking of tongue contours in\nreal-time ultrasound data became practical only in recent years. Recent studies\nhave shown that the performance of deep learning techniques is significant in\nthe tracking of ultrasound tongue contours in real-time applications such as\npronunciation training using multimodal ultrasound-enhanced approaches. Due to\nthe high correlation between ultrasound tongue datasets, it is feasible to have\na general model that accomplishes automatic tongue tracking for almost all\ndatasets. In this paper, we proposed a deep learning model comprises of a\nconvolutional module mimicking the peripheral vision ability of the human eye\nto handle real-time, accurate, and fully automatic tongue contour tracking\ntasks, applicable for almost all primary ultrasound tongue datasets.\nQualitative and quantitative assessment of IrisNet on different ultrasound\ntongue datasets and PASCAL VOC2012 revealed its outstanding generalization\nachievement in compare with similar techniques.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 17:59:28 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 20:01:29 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Mozaffari", "M. Hamed", ""], ["Ratul", "Md. Aminur Rab", ""], ["Lee", "Won-Sook", ""]]}, {"id": "1911.03977", "submitter": "Chao Zhang", "authors": "Chao Zhang, Zichao Yang, Xiaodong He, Li Deng", "title": "Multimodal Intelligence: Representation Learning, Information Fusion,\n  and Applications", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2020.2987728", "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have revolutionized speech recognition, image\nrecognition, and natural language processing since 2010. Each of these tasks\ninvolves a single modality in their input signals. However, many applications\nin the artificial intelligence field involve multiple modalities. Therefore, it\nis of broad interest to study the more difficult and complex problem of\nmodeling and learning across multiple modalities. In this paper, we provide a\ntechnical review of available models and learning methods for multimodal\nintelligence. The main focus of this review is the combination of vision and\nnatural language modalities, which has become an important topic in both the\ncomputer vision and natural language processing research communities. This\nreview provides a comprehensive analysis of recent works on multimodal deep\nlearning from three perspectives: learning multimodal representations, fusing\nmultimodal signals at various levels, and multimodal applications. Regarding\nmultimodal representation learning, we review the key concepts of embedding,\nwhich unify multimodal signals into a single vector space and thereby enable\ncross-modality signal processing. We also review the properties of many types\nof embeddings that are constructed and learned for general downstream tasks.\nRegarding multimodal fusion, this review focuses on special architectures for\nthe integration of representations of unimodal signals for a particular task.\nRegarding applications, selected areas of a broad interest in the current\nliterature are covered, including image-to-text caption generation,\ntext-to-image generation, and visual question answering. We believe that this\nreview will facilitate future studies in the emerging field of multimodal\nintelligence for related communities.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 18:58:20 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 11:00:48 GMT"}, {"version": "v3", "created": "Fri, 10 Apr 2020 09:16:13 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Zhang", "Chao", ""], ["Yang", "Zichao", ""], ["He", "Xiaodong", ""], ["Deng", "Li", ""]]}, {"id": "1911.04047", "submitter": "Qi Qian", "authors": "Qi Qian, Juhua Hu, Hao Li", "title": "Hierarchically Robust Representation Learning", "comments": "accepted by CVPR'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the tremendous success of deep learning in visual tasks, the\nrepresentations extracted from intermediate layers of learned models, that is,\ndeep features, attract much attention of researchers. Previous empirical\nanalysis shows that those features can contain appropriate semantic\ninformation. Therefore, with a model trained on a large-scale benchmark data\nset (e.g., ImageNet), the extracted features can work well on other tasks. In\nthis work, we investigate this phenomenon and demonstrate that deep features\ncan be suboptimal due to the fact that they are learned by minimizing the\nempirical risk. When the data distribution of the target task is different from\nthat of the benchmark data set, the performance of deep features can degrade.\nHence, we propose a hierarchically robust optimization method to learn more\ngeneric features. Considering the example-level and concept-level robustness\nsimultaneously, we formulate the problem as a distributionally robust\noptimization problem with Wasserstein ambiguity set constraints, and an\nefficient algorithm with the conventional training pipeline is proposed.\nExperiments on benchmark data sets demonstrate the effectiveness of the robust\ndeep representations.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 02:51:46 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 18:21:02 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Qian", "Qi", ""], ["Hu", "Juhua", ""], ["Li", "Hao", ""]]}, {"id": "1911.04058", "submitter": "Yiming Xu", "authors": "Yiming Xu, Lin Chen, Zhongwei Cheng, Lixin Duan, Jiebo Luo", "title": "Open-Ended Visual Question Answering by Multi-Modal Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of visual question answering (VQA) in images by\nexploiting supervised domain adaptation, where there is a large amount of\nlabeled data in the source domain but only limited labeled data in the target\ndomain with the goal to train a good target model. A straightforward solution\nis to fine-tune a pre-trained source model by using those limited labeled\ntarget data, but it usually cannot work well due to the considerable difference\nbetween the data distributions of the source and target domains. Moreover, the\navailability of multiple modalities (i.e., images, questions and answers) in\nVQA poses further challenges to model the transferability between those\ndifferent modalities. In this paper, we tackle the above issues by proposing a\nnovel supervised multi-modal domain adaptation method for VQA to learn joint\nfeature embeddings across different domains and modalities. Specifically, we\nalign the data distributions of the source and target domains by considering\nall modalities together as well as separately for each individual modality.\nBased on the extensive experiments on the benchmark VQA 2.0 and VizWiz datasets\nfor the realistic open-ended VQA task, we demonstrate that our proposed method\noutperforms the existing state-of-the-art approaches in this challenging domain\nadaptation setting for VQA.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 03:26:58 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Xu", "Yiming", ""], ["Chen", "Lin", ""], ["Cheng", "Zhongwei", ""], ["Duan", "Lixin", ""], ["Luo", "Jiebo", ""]]}, {"id": "1911.04102", "submitter": "Anis Koubaa", "authors": "Anis Koubaa, Adel Ammar, Bilel Benjdira, Abdullatif Al-Hadid, Belal\n  Kawaf, Saleh Ali Al-Yahri, Abdelrahman Babiker, Koutaiba Assaf, Mohannad Ba\n  Ras", "title": "Activity Monitoring of Islamic Prayer (Salat) Postures using Deep\n  Learning", "comments": "Submitted to the 6th International Conference on Data Science and\n  Machine Learning Applications (CDMA 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the Muslim community, the prayer (i.e. Salat) is the second pillar of\nIslam, and it is the most essential and fundamental worshiping activity that\nbelievers have to perform five times a day. From a gestures' perspective, there\nare predefined human postures that must be performed in a precise manner.\nHowever, for several people, these postures are not correctly performed, due to\nbeing new to Salat or even having learned prayers in an incorrect manner.\nFurthermore, the time spent in each posture has to be balanced. To address\nthese issues, we propose to develop an artificial intelligence assistive\nframework that guides worshippers to evaluate the correctness of the postures\nof their prayers. This paper represents the first step to achieve this\nobjective and addresses the problem of the recognition of the basic gestures of\nIslamic prayer using Convolutional Neural Networks (CNN). The contribution of\nthis paper lies in building a dataset for the basic Salat positions, and train\na YOLOv3 neural network for the recognition of the gestures. Experimental\nresults demonstrate that the mean average precision attains 85% for a training\ndataset of 764 images of the different postures. To the best of our knowledge,\nthis is the first work that addresses human activity recognition of Salat using\ndeep learning.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 06:31:40 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Koubaa", "Anis", ""], ["Ammar", "Adel", ""], ["Benjdira", "Bilel", ""], ["Al-Hadid", "Abdullatif", ""], ["Kawaf", "Belal", ""], ["Al-Yahri", "Saleh Ali", ""], ["Babiker", "Abdelrahman", ""], ["Assaf", "Koutaiba", ""], ["Ras", "Mohannad Ba", ""]]}, {"id": "1911.04127", "submitter": "Ying Tai", "authors": "Chuming Lin, Jian Li, Yabiao Wang, Ying Tai, Donghao Luo, Zhipeng Cui,\n  Chengjie Wang, Jilin Li, Feiyue Huang, Rongrong Ji", "title": "Fast Learning of Temporal Action Proposal via Dense Boundary Generator", "comments": "Accepted by AAAI 2020. Ranked No. 1 on ActivityNet Challenge 2019 on\n  Temporal Action Proposals\n  (http://activity-net.org/challenges/2019/evaluation.html)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating temporal action proposals remains a very challenging problem,\nwhere the main issue lies in predicting precise temporal proposal boundaries\nand reliable action confidence in long and untrimmed real-world videos. In this\npaper, we propose an efficient and unified framework to generate temporal\naction proposals named Dense Boundary Generator (DBG), which draws inspiration\nfrom boundary-sensitive methods and implements boundary classification and\naction completeness regression for densely distributed proposals. In\nparticular, the DBG consists of two modules: Temporal boundary classification\n(TBC) and Action-aware completeness regression (ACR). The TBC aims to provide\ntwo temporal boundary confidence maps by low-level two-stream features, while\nthe ACR is designed to generate an action completeness score map by high-level\naction-aware features. Moreover, we introduce a dual stream BaseNet (DSB) to\nencode RGB and optical flow information, which helps to capture discriminative\nboundary and actionness features. Extensive experiments on popular benchmarks\nActivityNet-1.3 and THUMOS14 demonstrate the superiority of DBG over the\nstate-of-the-art proposal generator (e.g., MGG and BMN). Our code will be made\navailable upon publication.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 08:15:13 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Lin", "Chuming", ""], ["Li", "Jian", ""], ["Wang", "Yabiao", ""], ["Tai", "Ying", ""], ["Luo", "Donghao", ""], ["Cui", "Zhipeng", ""], ["Wang", "Chengjie", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""], ["Ji", "Rongrong", ""]]}, {"id": "1911.04131", "submitter": "Wei Peng", "authors": "Wei Peng, Xiaopeng Hong, Haoyu Chen, Guoying Zhao", "title": "Learning Graph Convolutional Network for Skeleton-based Human Action\n  Recognition by Neural Searching", "comments": "Accepted by AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Human action recognition from skeleton data, fueled by the Graph\nConvolutional Network (GCN), has attracted lots of attention, due to its\npowerful capability of modeling non-Euclidean structure data. However, many\nexisting GCN methods provide a pre-defined graph and fix it through the entire\nnetwork, which can loss implicit joint correlations. Besides, the mainstream\nspectral GCN is approximated by one-order hop, thus higher-order connections\nare not well involved. Therefore, huge efforts are required to explore a better\nGCN architecture. To address these problems, we turn to Neural Architecture\nSearch (NAS) and propose the first automatically designed GCN for\nskeleton-based action recognition. Specifically, we enrich the search space by\nproviding multiple dynamic graph modules after fully exploring the\nspatial-temporal correlations between nodes. Besides, we introduce multiple-hop\nmodules and expect to break the limitation of representational capacity caused\nby one-order approximation. Moreover, a sampling- and memory-efficient\nevolution strategy is proposed to search an optimal architecture for this task.\nThe resulted architecture proves the effectiveness of the higher-order\napproximation and the dynamic graph modeling mechanism with temporal\ninteractions, which is barely discussed before. To evaluate the performance of\nthe searched model, we conduct extensive experiments on two very large scaled\ndatasets and the results show that our model gets the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 08:24:10 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Peng", "Wei", ""], ["Hong", "Xiaopeng", ""], ["Chen", "Haoyu", ""], ["Zhao", "Guoying", ""]]}, {"id": "1911.04140", "submitter": "Prashant Pandey", "authors": "Prashant Pandey, Prathosh AP, Manu Kohli, Josh Pritchard", "title": "Guided Weak Supervision for Action Recognition with Scarce Data to\n  Assess Skills of Children with Autism", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnostic and intervention methodologies for skill assessment of autism\ntypically requires a clinician repetitively initiating several stimuli and\nrecording the child's response. In this paper, we propose to automate the\nresponse measurement through video recording of the scene following the use of\nDeep Neural models for human action recognition from videos. However,\nsupervised learning of neural networks demand large amounts of annotated data\nthat are hard to come by. This issue is addressed by leveraging the\n`similarities' between the action categories in publicly available large-scale\nvideo action (source) datasets and the dataset of interest. A technique called\nguided weak supervision is proposed, where every class in the target data is\nmatched to a class in the source data using the principle of posterior\nlikelihood maximization. Subsequently, classifier on the target data is\nre-trained by augmenting samples from the matched source classes, along with a\nnew loss encouraging inter-class separability. The proposed method is evaluated\non two skill assessment autism datasets, SSBD and a real world Autism dataset\ncomprising 37 children of different ages and ethnicity who are diagnosed with\nautism. Our proposed method is found to improve the performance of the\nstate-of-the-art multi-class human action recognition models in-spite of\nsupervision with scarce data.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 08:51:40 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 05:34:18 GMT"}, {"version": "v3", "created": "Tue, 28 Jan 2020 21:17:28 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Pandey", "Prashant", ""], ["AP", "Prathosh", ""], ["Kohli", "Manu", ""], ["Pritchard", "Josh", ""]]}, {"id": "1911.04144", "submitter": "Ya Sun", "authors": "Ya Sun, Minxian Li, Jianfeng Lu", "title": "Part-based Multi-stream Model for Vehicle Searching", "comments": "Published in International Conference on Pattern Recognition 2018", "journal-ref": null, "doi": "10.1109/ICPR.2018.8546191", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the enormous requirement in public security and intelligent\ntransportation system, searching an identical vehicle has become more and more\nimportant. Current studies usually treat vehicle as an integral object and then\ntrain a distance metric to measure the similarity among vehicles. However,\nthese raw images may be exactly similar to ones with different identification\nand include some pixels in background that may disturb the distance metric\nlearning. In this paper, we propose a novel and useful method to segment an\noriginal vehicle image into several discriminative foreground parts, and these\nparts consist of some fine grained regions that are named discriminative\npatches. After that, these parts combined with the raw image are fed into the\nproposed deep learning network. We can easily measure the similarity of two\nvehicle images by computing the Euclidean distance of the features from FC\nlayer. Two main contributions of this paper are as follows. Firstly, a method\nis proposed to estimate if a patch in a raw vehicle image is discriminative or\nnot. Secondly, a new Part-based Multi-Stream Model (PMSM) is designed and\noptimized for vehicle retrieval and re-identification tasks. We evaluate the\nproposed method on the VehicleID dataset, and the experimental results show\nthat our method can outperform the baseline.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 08:58:07 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Sun", "Ya", ""], ["Li", "Minxian", ""], ["Lu", "Jianfeng", ""]]}, {"id": "1911.04169", "submitter": "Michael Spratling", "authors": "M. W. Spratling", "title": "Explaining Away Results in Accurate and Tolerant Template Matching", "comments": null, "journal-ref": null, "doi": "10.1016/j.patcog.2020.107337", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognising and locating image patches or sets of image features is an\nimportant task underlying much work in computer vision. Traditionally this has\nbeen accomplished using template matching. However, template matching is\nnotoriously brittle in the face of changes in appearance caused by, for\nexample, variations in viewpoint, partial occlusion, and non-rigid\ndeformations. This article tests a method of template matching that is more\ntolerant to such changes in appearance and that can, therefore, more accurately\nidentify image patches. In traditional template matching the comparison between\na template and the image is independent of the other templates. In contrast,\nthe method advocated here takes into account the evidence provided by the image\nfor the template at each location and the full range of alternative\nexplanations represented by the same template at other locations and by other\ntemplates. Specifically, the proposed method of template matching is performed\nusing a form of probabilistic inference known as \"explaining away\". The\nalgorithm used to implement explaining away has previously been used to\nsimulate several neurobiological mechanisms, and been applied to image contour\ndetection and pattern recognition tasks. Here it is applied for the first time\nto image patch matching, and is shown to produce superior results in comparison\nto the current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 10:44:42 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Spratling", "M. W.", ""]]}, {"id": "1911.04180", "submitter": "M. Alex O. Vasilescu", "authors": "M. Alex O. Vasilescu and Eric Kim", "title": "Compositional Hierarchical Tensor Factorization: Representing\n  Hierarchical Intrinsic and Extrinsic Causal Factors", "comments": "VERS 2: Fixed out of sync ref. Added\n  [7,14,15,28,37,50,52,53,61,77,78] M.A.O. Vasilescu and E.Kim. Compositional\n  Hierarchical Tensor Factorization: Representing Hierarchical Intrinsic and\n  Extrinsic Causal Factors. In 25th ACM SIGKDD Conference on Knowledge\n  Discovery and Data Mining (KDD'19): Tensor Methods for Emerging Data Science\n  Challenges, August 04-08, 2019, Anchorage, AK.ACM, New York, NY", "journal-ref": "25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n  (KDD'19): Tensor Methods for Emerging Data Science Challenges Workshop,\n  August 04-08, 2019, Anchorage, AK.ACM, New York, NY", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG math.DG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual objects are composed of a recursive hierarchy of perceptual wholes and\nparts, whose properties, such as shape, reflectance, and color, constitute a\nhierarchy of intrinsic causal factors of object appearance. However, object\nappearance is the compositional consequence of both an object's intrinsic and\nextrinsic causal factors, where the extrinsic causal factors are related to\nillumination, and imaging conditions. Therefore, this paper proposes a unified\ntensor model of wholes and parts, and introduces a compositional hierarchical\ntensor factorization that disentangles the hierarchical causal structure of\nobject image formation, and subsumes multilinear block tensor decomposition as\na special case. The resulting object representation is an interpretable\ncombinatorial choice of wholes' and parts' representations that renders object\nrecognition robust to occlusion and reduces training data requirements. We\ndemonstrate ourapproach in the context of face recognition by training on an\nextremely reduced dataset of synthetic images, and report encouragingface\nverification results on two datasets - the Freiburg dataset, andthe Labeled\nFace in the Wild (LFW) dataset consisting of real world images, thus,\nsubstantiating the suitability of our approach for data starved domains.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 11:03:53 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2020 06:23:19 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Vasilescu", "M. Alex O.", ""], ["Kim", "Eric", ""]]}, {"id": "1911.04192", "submitter": "Ruize Wang", "authors": "Ruize Wang, Zhongyu Wei, Ying Cheng, Piji Li, Haijun Shan, Ji Zhang,\n  Qi Zhang, Xuanjing Huang", "title": "Keep it Consistent: Topic-Aware Storytelling from an Image Stream via\n  Iterative Multi-agent Communication", "comments": "Accepted to COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual storytelling aims to generate a narrative paragraph from a sequence of\nimages automatically. Existing approaches construct text description\nindependently for each image and roughly concatenate them as a story, which\nleads to the problem of generating semantically incoherent content. In this\npaper, we propose a new way for visual storytelling by introducing a topic\ndescription task to detect the global semantic context of an image stream. A\nstory is then constructed with the guidance of the topic description. In order\nto combine the two generation tasks, we propose a multi-agent communication\nframework that regards the topic description generator and the story generator\nas two agents and learn them simultaneously via iterative updating mechanism.\nWe validate our approach on VIST dataset, where quantitative results,\nablations, and human evaluation demonstrate our method's good ability in\ngenerating stories with higher quality compared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 11:35:21 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 07:08:10 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Wang", "Ruize", ""], ["Wei", "Zhongyu", ""], ["Cheng", "Ying", ""], ["Li", "Piji", ""], ["Shan", "Haijun", ""], ["Zhang", "Ji", ""], ["Zhang", "Qi", ""], ["Huang", "Xuanjing", ""]]}, {"id": "1911.04227", "submitter": "Valentina Zantedeschi Dr", "authors": "Valentina Zantedeschi, Fabrizio Falasca, Alyson Douglas, Richard\n  Strange, Matt J. Kusner, Duncan Watson-Parris", "title": "Cumulo: A Dataset for Learning Cloud Classes", "comments": null, "journal-ref": "Tackling Climate Change with Machine Learning Workshop, 33rd\n  Conference on Neural Information Processing Systems (NeurIPS 2019),\n  Vancouver, Canada", "doi": null, "report-no": null, "categories": "physics.ao-ph cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the greatest sources of uncertainty in future climate projections\ncomes from limitations in modelling clouds and in understanding how different\ncloud types interact with the climate system. A key first step in reducing this\nuncertainty is to accurately classify cloud types at high spatial and temporal\nresolution. In this paper, we introduce Cumulo, a benchmark dataset for\ntraining and evaluating global cloud classification models. It consists of one\nyear of 1km resolution MODIS hyperspectral imagery merged with pixel-width\n'tracks' of CloudSat cloud labels. Bringing these complementary datasets\ntogether is a crucial first step, enabling the Machine-Learning community to\ndevelop innovative new techniques which could greatly benefit the Climate\ncommunity. To showcase Cumulo, we provide baseline performance analysis using\nan invertible flow generative model (IResNet), which further allows us to\ndiscover new sub-classes for a given cloud class by exploring the latent space.\nTo compare methods, we introduce a set of evaluation criteria, to identify\nmodels that are not only accurate, but also physically-realistic. CUMULO can be\ndownload from\nhttps://www.dropbox.com/sh/6gca7f0mb3b0ikz/AADq2lk4u7k961Qa31FwIDEpa?dl=0 .\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 09:36:16 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 10:01:33 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Zantedeschi", "Valentina", ""], ["Falasca", "Fabrizio", ""], ["Douglas", "Alyson", ""], ["Strange", "Richard", ""], ["Kusner", "Matt J.", ""], ["Watson-Parris", "Duncan", ""]]}, {"id": "1911.04231", "submitter": "Yisheng He", "authors": "Yisheng He, Wei Sun, Haibin Huang, Jianran Liu, Haoqiang Fan, Jian Sun", "title": "PVN3D: A Deep Point-wise 3D Keypoints Voting Network for 6DoF Pose\n  Estimation", "comments": "Accepted to Proceedings of the IEEE Conference on Computer Vision and\n  Pattern Recognition, 2020. (CVPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel data-driven method for robust 6DoF object\npose estimation from a single RGBD image. Unlike previous methods that directly\nregressing pose parameters, we tackle this challenging task with a\nkeypoint-based approach. Specifically, we propose a deep Hough voting network\nto detect 3D keypoints of objects and then estimate the 6D pose parameters\nwithin a least-squares fitting manner. Our method is a natural extension of\n2D-keypoint approaches that successfully work on RGB based 6DoF estimation. It\nallows us to fully utilize the geometric constraint of rigid objects with the\nextra depth information and is easy for a network to learn and optimize.\nExtensive experiments were conducted to demonstrate the effectiveness of\n3D-keypoint detection in the 6D pose estimation task. Experimental results also\nshow our method outperforms the state-of-the-art methods by large margins on\nseveral benchmarks. Code and video are available at\nhttps://github.com/ethnhe/PVN3D.git.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 13:13:21 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 10:16:31 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["He", "Yisheng", ""], ["Sun", "Wei", ""], ["Huang", "Haibin", ""], ["Liu", "Jianran", ""], ["Fan", "Haoqiang", ""], ["Sun", "Jian", ""]]}, {"id": "1911.04237", "submitter": "Daksh Thapar", "authors": "Abhigyan Khaund, Daksh Thapar, Aditya Nigam", "title": "PoshakNet: Framework for matching dresses from real-life photos using\n  GAN and Siamese Network", "comments": "Accepted in NCVPRIPG 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online garment shopping has gained many customers in recent years. Describing\na dress using keywords does not always yield the proper results, which in turn\nleads to dissatisfaction of customers. A visual search based system will be\nenormously beneficent to the industry. Hence, we propose a framework that can\nretrieve similar clothes that can be found in an image. The first task is to\nextract the garment from the input image (street photo). There are various\nchallenges for that, including pose, illumination, and background clutter. We\nuse a Generative Adversarial Network for the task of retrieving the garment\nthat the person in the image was wearing. It has been shown that GAN can\nretrieve the garment very efficiently despite the challenges of street photos.\nFinally, a siamese based matching system takes the retrieved cloth image and\nmatches it with the clothes in the dataset, giving us the top k matches. We\ntake a pre-trained inception-ResNet v1 module as a siamese network (trained\nusing triplet loss for face detection) and fine-tune it on the shopping dataset\nusing center loss. The dataset has been collected inhouse. For training the\nGAN, we use the LookBook dataset, which is publically available.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 13:17:01 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Khaund", "Abhigyan", ""], ["Thapar", "Daksh", ""], ["Nigam", "Aditya", ""]]}, {"id": "1911.04241", "submitter": "Sergey Ilyuhin", "authors": "Sergey A. Ilyuhin, Alexander V. Sheshkus, Vladimir L. Arlazarov", "title": "Recognition of Images of Korean Characters Using Embedded Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the significant success in the field of text recognition, complex and\nunsolved problems still exist in this field. In recent years, the recognition\naccuracy of the English language has greatly increased, while the problem of\nrecognition of hieroglyphs has received much less attention. Hieroglyph\nrecognition or image recognition with Korean, Japanese or Chinese characters\nhave differences from the traditional text recognition task. This article\ndiscusses the main differences between hieroglyph languages and the Latin\nalphabet in the context of image recognition. A light-weight method for\nrecognizing images of the hieroglyphs is proposed and tested on a public\ndataset of Korean hieroglyph images. Despite the existing solutions, the\nproposed method is suitable for mobile devices. Its recognition accuracy is\nbetter than the accuracy of the open-source OCR framework. The presented method\nof training embedded net bases on the similarities in the recognition data.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 13:21:02 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 11:31:28 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Ilyuhin", "Sergey A.", ""], ["Sheshkus", "Alexander V.", ""], ["Arlazarov", "Vladimir L.", ""]]}, {"id": "1911.04252", "submitter": "Qizhe Xie", "authors": "Qizhe Xie, Minh-Thang Luong, Eduard Hovy, Quoc V. Le", "title": "Self-training with Noisy Student improves ImageNet classification", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Noisy Student Training, a semi-supervised learning approach that\nworks well even when labeled data is abundant. Noisy Student Training achieves\n88.4% top-1 accuracy on ImageNet, which is 2.0% better than the\nstate-of-the-art model that requires 3.5B weakly labeled Instagram images. On\nrobustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to\n83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces\nImageNet-P mean flip rate from 27.8 to 12.2.\n  Noisy Student Training extends the idea of self-training and distillation\nwith the use of equal-or-larger student models and noise added to the student\nduring learning. On ImageNet, we first train an EfficientNet model on labeled\nimages and use it as a teacher to generate pseudo labels for 300M unlabeled\nimages. We then train a larger EfficientNet as a student model on the\ncombination of labeled and pseudo labeled images. We iterate this process by\nputting back the student as the teacher. During the learning of the student, we\ninject noise such as dropout, stochastic depth, and data augmentation via\nRandAugment to the student so that the student generalizes better than the\nteacher. Models are available at\nhttps://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.\nCode is available at https://github.com/google-research/noisystudent.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 18:59:27 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 07:07:57 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2020 22:27:37 GMT"}, {"version": "v4", "created": "Fri, 19 Jun 2020 17:36:57 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Xie", "Qizhe", ""], ["Luong", "Minh-Thang", ""], ["Hovy", "Eduard", ""], ["Le", "Quoc V.", ""]]}, {"id": "1911.04254", "submitter": "Shiming Chen", "authors": "Shiming Chen, Peng Zhang, Qinmu Peng, Zehong Cao, Xinge You", "title": "Kernelized Similarity Learning and Embedding for Dynamic Texture\n  Synthesis", "comments": "13 pages, 12 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic texture (DT) exhibits statistical stationarity in the spatial domain\nand stochastic repetitiveness in the temporal dimension, indicating that\ndifferent frames of DT possess a high similarity correlation that is critical\nprior knowledge. However, existing methods cannot effectively learn a promising\nsynthesis model for high-dimensional DT from a small number of training data.\nIn this paper, we propose a novel DT synthesis method, which makes full use of\nsimilarity prior knowledge to address this issue. Our method bases on the\nproposed kernel similarity embedding, which not only can mitigate the\nhigh-dimensionality and small sample issues, but also has the advantage of\nmodeling nonlinear feature relationship. Specifically, we first raise two\nhypotheses that are essential for DT model to generate new frames using\nsimilarity correlation. Then, we integrate kernel learning and extreme learning\nmachine into a unified synthesis model to learn kernel similarity embedding for\nrepresenting DT. Extensive experiments on DT videos collected from the internet\nand two benchmark datasets, i.e., Gatech Graphcut Textures and Dyntex,\ndemonstrate that the learned kernel similarity embedding can effectively\nexhibit the discriminative representation for DT. Accordingly, our method is\ncapable of preserving the long-term temporal continuity of the synthesized DT\nsequences with excellent sustainability and generalization. Meanwhile, it\neffectively generates realistic DT videos with fast speed and low computation,\ncompared with the state-of-the-art methods. The code and more synthesis videos\nare available at our project page\nhttps://shiming-chen.github.io/Similarity-page/Similarit.html.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 13:37:27 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 07:05:40 GMT"}, {"version": "v3", "created": "Sat, 9 Jan 2021 11:18:51 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Chen", "Shiming", ""], ["Zhang", "Peng", ""], ["Peng", "Qinmu", ""], ["Cao", "Zehong", ""], ["You", "Xinge", ""]]}, {"id": "1911.04357", "submitter": "Steven Guan", "authors": "Steven Guan, Amir A. Khan, Siddhartha Sikdar, Parag V. Chitnis", "title": "Limited View and Sparse Photoacoustic Tomography for Neuroimaging with\n  Deep Learning", "comments": null, "journal-ref": "Sci Rep 10, 8510 (2020)", "doi": "10.1038/s41598-020-65235-2", "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photoacoustic tomography (PAT) is a nonionizing imaging modality capable of\nacquiring high contrast and resolution images of optical absorption at depths\ngreater than traditional optical imaging techniques. Practical considerations\nwith instrumentation and geometry limit the number of available acoustic\nsensors and their view of the imaging target, which result in significant image\nreconstruction artifacts degrading image quality. Iterative reconstruction\nmethods can be used to reduce artifacts but are computationally expensive. In\nthis work, we propose a novel deep learning approach termed pixelwise deep\nlearning (PixelDL) that first employs pixelwise interpolation governed by the\nphysics of photoacoustic wave propagation and then uses a convolution neural\nnetwork to directly reconstruct an image. Simulated photoacoustic data from\nsynthetic vasculature phantom and mouse-brain vasculature were used for\ntraining and testing, respectively. Results demonstrated that PixelDL achieved\ncomparable performance to iterative methods and outperformed other CNN-based\napproaches for correcting artifacts. PixelDL is a computationally efficient\napproach that enables for realtime PAT rendering and for improved image\nquality, quantification, and interpretation.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 15:59:11 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 18:01:53 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Guan", "Steven", ""], ["Khan", "Amir A.", ""], ["Sikdar", "Siddhartha", ""], ["Chitnis", "Parag V.", ""]]}, {"id": "1911.04365", "submitter": "Jun He", "authors": "Jun He, Quan-Jie Cao, Lei Zhang", "title": "Conditionally Learn to Pay Attention for Sequential Visual Task", "comments": null, "journal-ref": "IEEE Access 8(2020) 56695-56710", "doi": "10.1109/ACCESS.2020.2982571", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential visual task usually requires to pay attention to its current\ninterested object conditional on its previous observations. Different from\npopular soft attention mechanism, we propose a new attention framework by\nintroducing a novel conditional global feature which represents the weak\nfeature descriptor of the current focused object. Specifically, for a standard\nCNN (Convolutional Neural Network) pipeline, the convolutional layers with\ndifferent receptive fields are used to produce the attention maps by measuring\nhow the convolutional features align to the conditional global feature. The\nconditional global feature can be generated by different recurrent structure\naccording to different visual tasks, such as a simple recurrent neural network\nfor multiple objects recognition, or a moderate complex language model for\nimage caption. Experiments show that our proposed conditional attention model\nachieves the best performance on the SVHN (Street View House Numbers) dataset\nwith / without extra bounding box; and for image caption, our attention model\ngenerates better scores than the popular soft attention model.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 16:11:46 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["He", "Jun", ""], ["Cao", "Quan-Jie", ""], ["Zhang", "Lei", ""]]}, {"id": "1911.04410", "submitter": "Kianoush Falahkheirkhah", "authors": "Kianoush Falahkheirkhah, Kevin Yeh, Shachi Mittal, Luke Pfister, Rohit\n  Bhargava", "title": "A deep learning framework for morphologic detail beyond the diffraction\n  limit in infrared spectroscopic imaging", "comments": "corrected typos (the word \"lack\" was missing in the abstract)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infrared (IR) microscopes measure spectral information that quantifies\nmolecular content to assign the identity of biomedical cells but lack the\nspatial quality of optical microscopy to appreciate morphologic features. Here,\nwe propose a method to utilize the semantic information of cellular identity\nfrom IR imaging with the morphologic detail of pathology images in a deep\nlearning-based approach to image super-resolution. Using Generative Adversarial\nNetworks (GANs), we enhance the spatial detail in IR imaging beyond the\ndiffraction limit while retaining their spectral contrast. This technique can\nbe rapidly integrated with modern IR microscopes to provide a framework useful\nfor routine pathology.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 16:50:27 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 16:33:42 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Falahkheirkhah", "Kianoush", ""], ["Yeh", "Kevin", ""], ["Mittal", "Shachi", ""], ["Pfister", "Luke", ""], ["Bhargava", "Rohit", ""]]}, {"id": "1911.04417", "submitter": "Agisilaos Chartsias", "authors": "Agisilaos Chartsias, Giorgos Papanastasiou, Chengjia Wang, Scott\n  Semple, David E. Newby, Rohan Dharmakumar, Sotirios A. Tsaftaris", "title": "Disentangle, align and fuse for multimodal and semi-supervised image\n  segmentation", "comments": null, "journal-ref": "IEEE Transactions on Medical Imaging (2020)", "doi": "10.1109/TMI.2020.3036584", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance (MR) protocols rely on several sequences to assess\npathology and organ status properly. Despite advances in image analysis, we\ntend to treat each sequence, here termed modality, in isolation. Taking\nadvantage of the common information shared between modalities (an organ's\nanatomy) is beneficial for multi-modality processing and learning. However, we\nmust overcome inherent anatomical misregistrations and disparities in signal\nintensity across the modalities to obtain this benefit. We present a method\nthat offers improved segmentation accuracy of the modality of interest (over a\nsingle input model), by learning to leverage information present in other\nmodalities, even if few (semi-supervised) or no (unsupervised) annotations are\navailable for this specific modality. Core to our method is learning a\ndisentangled decomposition into anatomical and imaging factors. Shared\nanatomical factors from the different inputs are jointly processed and fused to\nextract more accurate segmentation masks. Image misregistrations are corrected\nwith a Spatial Transformer Network, which non-linearly aligns the anatomical\nfactors. The imaging factor captures signal intensity characteristics across\ndifferent modality data and is used for image reconstruction, enabling\nsemi-supervised learning. Temporal and slice pairing between inputs are learned\ndynamically. We demonstrate applications in Late Gadolinium Enhanced (LGE) and\nBlood Oxygenation Level Dependent (BOLD) cardiac segmentation, as well as in T2\nabdominal segmentation. Code is available at\nhttps://github.com/vios-s/multimodal_segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 17:44:00 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 14:58:05 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2020 11:31:55 GMT"}, {"version": "v4", "created": "Fri, 25 Sep 2020 11:26:41 GMT"}, {"version": "v5", "created": "Mon, 9 Nov 2020 19:18:39 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Chartsias", "Agisilaos", ""], ["Papanastasiou", "Giorgos", ""], ["Wang", "Chengjia", ""], ["Semple", "Scott", ""], ["Newby", "David E.", ""], ["Dharmakumar", "Rohan", ""], ["Tsaftaris", "Sotirios A.", ""]]}, {"id": "1911.04432", "submitter": "Hans Pinckaers", "authors": "Hans Pinckaers, Bram van Ginneken, Geert Litjens", "title": "Streaming convolutional neural networks for end-to-end learning with\n  multi-megapixel images", "comments": "In review", "journal-ref": null, "doi": "10.1109/TPAMI.2020.3019563", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to memory constraints on current hardware, most convolution neural\nnetworks (CNN) are trained on sub-megapixel images. For example, most popular\ndatasets in computer vision contain images much less than a megapixel in size\n(0.09MP for ImageNet and 0.001MP for CIFAR-10). In some domains such as medical\nimaging, multi-megapixel images are needed to identify the presence of disease\naccurately. We propose a novel method to directly train convolutional neural\nnetworks using any input image size end-to-end. This method exploits the\nlocality of most operations in modern convolutional neural networks by\nperforming the forward and backward pass on smaller tiles of the image. In this\nwork, we show a proof of concept using images of up to 66-megapixels\n(8192x8192), saving approximately 50GB of memory per image. Using two public\nchallenge datasets, we demonstrate that CNNs can learn to extract relevant\ninformation from these large images and benefit from increasing resolution. We\nimproved the area under the receiver-operating characteristic curve from 0.580\n(4MP) to 0.706 (66MP) for metastasis detection in breast cancer (CAMELYON17).\nWe also obtained a Spearman correlation metric approaching state-of-the-art\nperformance on the TUPAC16 dataset, from 0.485 (1MP) to 0.570 (16MP). Code to\nreproduce a subset of the experiments is available at\nhttps://github.com/DIAGNijmegen/StreamingCNN.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 18:18:22 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Pinckaers", "Hans", ""], ["van Ginneken", "Bram", ""], ["Litjens", "Geert", ""]]}, {"id": "1911.04453", "submitter": "Xiaocong Du", "authors": "Gokul Krishnan, Xiaocong Du, Yu Cao", "title": "Structural Pruning in Deep Neural Networks: A Small-World Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are usually over-parameterized, causing excessive\nmemory and interconnection cost on the hardware platform. Existing pruning\napproaches remove secondary parameters at the end of training to reduce the\nmodel size; but without exploiting the intrinsic network property, they still\nrequire the full interconnection to prepare the network. Inspired by the\nobservation that brain networks follow the Small-World model, we propose a\nnovel structural pruning scheme, which includes (1) hierarchically trimming the\nnetwork into a Small-World model before training, (2) training the network for\na given dataset, and (3) optimizing the network for accuracy. The new scheme\neffectively reduces both the model size and the interconnection needed before\ntraining, achieving a locally clustered and globally sparse model. We\ndemonstrate our approach on LeNet-5 for MNIST and VGG-16 for CIFAR-10,\ndecreasing the number of parameters to 2.3% and 9.02% of the baseline model,\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 18:53:50 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Krishnan", "Gokul", ""], ["Du", "Xiaocong", ""], ["Cao", "Yu", ""]]}, {"id": "1911.04460", "submitter": "Ning-Hsu Wang", "authors": "Ning-Hsu Wang, Bolivar Solarte, Yi-Hsuan Tsai, Wei-Chen Chiu, Min Sun", "title": "360SD-Net: 360{\\deg} Stereo Depth Estimation with Learnable Cost Volume", "comments": "Accepted to 2020 IEEE International Conference on Robotics and\n  Automation (ICRA 2020). Project page and code are at\n  https://albert100121.github.io/360SD-Net-Project-Page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, end-to-end trainable deep neural networks have significantly\nimproved stereo depth estimation for perspective images. However, 360{\\deg}\nimages captured under equirectangular projection cannot benefit from directly\nadopting existing methods due to distortion introduced (i.e., lines in 3D are\nnot projected onto lines in 2D). To tackle this issue, we present a novel\narchitecture specifically designed for spherical disparity using the setting of\ntop-bottom 360{\\deg} camera pairs. Moreover, we propose to mitigate the\ndistortion issue by (1) an additional input branch capturing the position and\nrelation of each pixel in the spherical coordinate, and (2) a cost volume built\nupon a learnable shifting filter. Due to the lack of 360{\\deg} stereo data, we\ncollect two 360{\\deg} stereo datasets from Matterport3D and Stanford3D for\ntraining and evaluation. Extensive experiments and ablation study are provided\nto validate our method against existing algorithms. Finally, we show promising\nresults on real-world environments capturing images with two consumer-level\ncameras.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 18:56:49 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 15:51:54 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Wang", "Ning-Hsu", ""], ["Solarte", "Bolivar", ""], ["Tsai", "Yi-Hsuan", ""], ["Chiu", "Wei-Chen", ""], ["Sun", "Min", ""]]}, {"id": "1911.04469", "submitter": "Ahmed Hammam", "authors": "Ahmed Ali Hammam, Mona Soliman, Aboul Ella Hassanien", "title": "A Proposed Artificial intelligence Model for Real-Time Human Action\n  Localization and Tracking", "comments": "SUBMITTED TO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING\n  SYSTEMS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, artificial intelligence (AI) based on deep learning (DL) has\nsparked tremendous global interest. DL is widely used today and has expanded\ninto various interesting areas. It is becoming more popular in cross-subject\nresearch, such as studies of smart city systems, which combine computer science\nwith engineering applications. Human action detection is one of these areas.\nHuman action detection is an interesting challenge due to its stringent\nrequirements in terms of computing speed and accuracy. High-accuracy real-time\nobject tracking is also considered a significant challenge. This paper\nintegrates the YOLO detection network, which is considered a state-of-the-art\ntool for real-time object detection, with motion vectors and the Coyote\nOptimization Algorithm (COA) to construct a real-time human action localization\nand tracking system. The proposed system starts with the extraction of motion\ninformation from a compressed video stream and the extraction of appearance\ninformation from RGB frames using an object detector. Then, a fusion step\nbetween the two streams is performed, and the results are fed into the proposed\naction tracking model. The COA is used in object tracking due to its accuracy\nand fast convergence. The basic foundation of the proposed model is the\nutilization of motion vectors, which already exist in a compressed video bit\nstream and provide sufficient information to improve the localization of the\ntarget action without requiring high consumption of computational resources\ncompared with other popular methods of extracting motion information, such as\noptical flows. This advantage allows the proposed approach to be implemented in\nchallenging environments where the computational resources are limited, such as\nInternet of Things (IoT) systems.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 19:59:17 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Hammam", "Ahmed Ali", ""], ["Soliman", "Mona", ""], ["Hassanien", "Aboul Ella", ""]]}, {"id": "1911.04470", "submitter": "Yuxin Song", "authors": "Jianjun Lei, Yuxin Song, Bo Peng, Zhanyu Ma, Ling Shao, Yi-Zhe Song", "title": "Semi-Heterogeneous Three-Way Joint Embedding Network for Sketch-Based\n  Image Retrieval", "comments": "Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology", "journal-ref": null, "doi": "10.1109/TCSVT.2019.2936710", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketch-based image retrieval (SBIR) is a challenging task due to the large\ncross-domain gap between sketches and natural images. How to align abstract\nsketches and natural images into a common high-level semantic space remains a\nkey problem in SBIR. In this paper, we propose a novel semi-heterogeneous\nthree-way joint embedding network (Semi3-Net), which integrates three branches\n(a sketch branch, a natural image branch, and an edgemap branch) to learn more\ndiscriminative cross-domain feature representations for the SBIR task. The key\ninsight lies with how we cultivate the mutual and subtle relationships amongst\nthe sketches, natural images, and edgemaps. A semi-heterogeneous feature\nmapping is designed to extract bottom features from each domain, where the\nsketch and edgemap branches are shared while the natural image branch is\nheterogeneous to the other branches. In addition, a joint semantic embedding is\nintroduced to embed the features from different domains into a common\nhigh-level semantic space, where all of the three branches are shared. To\nfurther capture informative features common to both natural images and the\ncorresponding edgemaps, a co-attention model is introduced to conduct common\nchannel-wise feature recalibration between different domains. A hybrid-loss\nmechanism is designed to align the three branches, where an alignment loss and\na sketch-edgemap contrastive loss are presented to encourage the network to\nlearn invariant cross-domain representations. Experimental results on two\nwidely used category-level datasets (Sketchy and TU-Berlin Extension)\ndemonstrate that the proposed method outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 02:53:06 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Lei", "Jianjun", ""], ["Song", "Yuxin", ""], ["Peng", "Bo", ""], ["Ma", "Zhanyu", ""], ["Shao", "Ling", ""], ["Song", "Yi-Zhe", ""]]}, {"id": "1911.04554", "submitter": "Joshua Tobin", "authors": "Josh Tobin, OpenAI Robotics, Pieter Abbeel", "title": "Geometry-Aware Neural Rendering", "comments": "16 pages, 13 figures", "journal-ref": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019), Vancouver, Canada", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the 3-dimensional structure of the world is a core challenge in\ncomputer vision and robotics. Neural rendering approaches learn an implicit 3D\nmodel by predicting what a camera would see from an arbitrary viewpoint. We\nextend existing neural rendering to more complex, higher dimensional scenes\nthan previously possible. We propose Epipolar Cross Attention (ECA), an\nattention mechanism that leverages the geometry of the scene to perform\nefficient non-local operations, requiring only $O(n)$ comparisons per spatial\ndimension instead of $O(n^2)$. We introduce three new simulated datasets\ninspired by real-world robotics and demonstrate that ECA significantly improves\nthe quantitative and qualitative performance of Generative Query Networks\n(GQN).\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 03:10:39 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Tobin", "Josh", ""], ["Robotics", "OpenAI", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1911.04583", "submitter": "Yannet Interian", "authors": "Khoury Ibrahim and Danielle A. Savage and Addie Schnirel and Paul\n  Intrevado and Yannet Interian", "title": "ContamiNet: Detecting Contamination in Municipal Solid Waste", "comments": "8 pages, 3 figures, ICMLA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging over 30,000 images each with up to 89 labels collected by\nRecology---an integrated resource recovery company with both residential and\ncommercial trash, recycling and composting services---the authors develop\nContamiNet, a convolutional neural network, to identify contaminating material\nin residential recycling and compost bins. When training the model on a subset\nof labels that meet a minimum frequency threshold, ContamiNet preforms almost\nas well human experts in detecting contamination (0.86 versus 0.88 AUC).\nRecology is actively piloting ContamiNet in their daily municipal solid waste\n(MSW) collection to identify contaminants in recycling and compost bins to\nsubsequently inform and educate customers about best sorting practices.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 22:10:40 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Ibrahim", "Khoury", ""], ["Savage", "Danielle A.", ""], ["Schnirel", "Addie", ""], ["Intrevado", "Paul", ""], ["Interian", "Yannet", ""]]}, {"id": "1911.04623", "submitter": "Yan Wang", "authors": "Yan Wang, Wei-Lun Chao, Kilian Q. Weinberger, Laurens van der Maaten", "title": "SimpleShot: Revisiting Nearest-Neighbor Classification for Few-Shot\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learners aim to recognize new object classes based on a small number\nof labeled training examples. To prevent overfitting, state-of-the-art few-shot\nlearners use meta-learning on convolutional-network features and perform\nclassification using a nearest-neighbor classifier. This paper studies the\naccuracy of nearest-neighbor baselines without meta-learning. Surprisingly, we\nfind simple feature transformations suffice to obtain competitive few-shot\nlearning accuracies. For example, we find that a nearest-neighbor classifier\nused in combination with mean-subtraction and L2-normalization outperforms\nprior results in three out of five settings on the miniImageNet dataset.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 00:44:10 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 00:35:54 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Wang", "Yan", ""], ["Chao", "Wei-Lun", ""], ["Weinberger", "Kilian Q.", ""], ["van der Maaten", "Laurens", ""]]}, {"id": "1911.04651", "submitter": "Ainaz Hajimoradlou", "authors": "Ainaz Hajimoradlou, Gioachino Roberti, David Poole", "title": "Predicting Landslides Using Locally Aligned Convolutional Neural\n  Networks", "comments": "Published in IJCAI 2020", "journal-ref": null, "doi": "10.24963/ijcai.2020/462", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Landslides, movement of soil and rock under the influence of gravity, are\ncommon phenomena that cause significant human and economic losses every year.\nExperts use heterogeneous features such as slope, elevation, land cover,\nlithology, rock age, and rock family to predict landslides. To work with such\nfeatures, we adapted convolutional neural networks to consider relative spatial\ninformation for the prediction task. Traditional filters in these networks\neither have a fixed orientation or are rotationally invariant. Intuitively, the\nfilters should orient uphill, but there is not enough data to learn the concept\nof uphill; instead, it can be provided as prior knowledge. We propose a model\ncalled Locally Aligned Convolutional Neural Network, LACNN, that follows the\nground surface at multiple scales to predict possible landslide occurrence for\na single point. To validate our method, we created a standardized dataset of\ngeoreferenced images consisting of the heterogeneous features as inputs, and\ncompared our method to several baselines, including linear regression, a neural\nnetwork, and a convolutional network, using log-likelihood error and Receiver\nOperating Characteristic curves on the test set. Our model achieves 2-7%\nimprovement in terms of accuracy and 2-15% boost in terms of log likelihood\ncompared to the other proposed baselines.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 03:21:53 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 01:57:23 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2020 04:19:41 GMT"}, {"version": "v4", "created": "Wed, 29 Apr 2020 03:24:03 GMT"}, {"version": "v5", "created": "Fri, 17 Jul 2020 18:13:17 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Hajimoradlou", "Ainaz", ""], ["Roberti", "Gioachino", ""], ["Poole", "David", ""]]}, {"id": "1911.04692", "submitter": "Quanquan Li", "authors": "Jingru Tan, Changbao Wang, Quanquan Li, Junjie Yan", "title": "Equalization Loss for Large Vocabulary Instance Segmentation", "comments": "Technical Report. Winner of LVIS Challenge 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent object detection and instance segmentation tasks mainly focus on\ndatasets with a relatively small set of categories, e.g. Pascal VOC with 20\nclasses and COCO with 80 classes. The new large vocabulary dataset LVIS brings\nnew challenges to conventional methods. In this work, we propose an\nequalization loss to solve the long tail of rare categories problem. Combined\nwith exploiting the data from detection datasets to alleviate the effect of\nmissing-annotation problems during the training, our method achieves 5.1\\%\noverall AP gain and 11.4\\% AP gain of rare categories on LVIS benchmark without\nany bells and whistles compared to Mask R-CNN baseline. Finally we achieve 28.9\nmask AP on the test-set of the LVIS and rank 1st place in LVIS Challenge 2019.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 06:04:35 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Tan", "Jingru", ""], ["Wang", "Changbao", ""], ["Li", "Quanquan", ""], ["Yan", "Junjie", ""]]}, {"id": "1911.04731", "submitter": "Ziyu Zhang", "authors": "Ziyu Zhang and Feipeng Da and Yi Yu", "title": "Data-Free Point Cloud Network for 3D Face Recognition", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds-based Networks have achieved great attention in 3D object\nclassification, segmentation and indoor scene semantic parsing. In terms of\nface recognition, 3D face recognition method which directly consume point\nclouds as input is still under study. Two main factors account for this: One is\nhow to get discriminative face representations from 3D point clouds using deep\nnetwork; the other is the lack of large 3D training dataset. To address these\nproblems, a data-free 3D face recognition method is proposed only using\nsynthesized unreal data from statistical 3D Morphable Model to train a deep\npoint cloud network. To ease the inconsistent distribution between model data\nand real faces, different point sampling methods are used in train and test\nphase. In this paper, we propose a curvature-aware point sampling(CPS) strategy\nreplacing the original furthest point sampling(FPS) to hierarchically\ndown-sample feature-sensitive points which are crucial to pass and aggregate\nfeatures deeply. A PointNet++ like Network is used to extract face features\ndirectly from point clouds. The experimental results show that the network\ntrained on generated data generalizes well for real 3D faces. Fine tuning on a\nsmall part of FRGCv2.0 and Bosphorus, which include real faces in different\nposes and expressions, further improves recognition accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 08:18:30 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Zhang", "Ziyu", ""], ["Da", "Feipeng", ""], ["Yu", "Yi", ""]]}, {"id": "1911.04762", "submitter": "Prashant Chaudhari", "authors": "Prashant Chaudhari, Franziska Schirrmacher, Andreas Maier, Christian\n  Riess, Thomas K\\\"ohler", "title": "Merging-ISP: Multi-Exposure High Dynamic Range Image Signal Processing", "comments": "8 pages, 6 figures, 2 tables, WACV-2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The image signal processing pipeline (ISP) is a core element of digital\ncameras to capture high-quality displayable images from raw data. In high\ndynamic range (HDR) imaging, ISPs include steps like demosaicing of raw color\nfilter array (CFA) data at different exposure times, alignment of the\nexposures, conversion to HDR domain, and exposure merging into an HDR image.\nTraditionally, such pipelines are built by cascading algorithms addressing the\nindividual subtasks. However, cascaded designs suffer from error propagations\nsince simply combining multiple processing steps is not necessarily optimal for\nthe entire imaging task. This paper proposes a multi-exposure high dynamic\nrange image signal processing pipeline (Merging-ISP) to jointly solve all\nsubtasks for HDR imaging. Our pipeline is modeled by a deep neural network\narchitecture. As such, it is end-to-end trainable, circumvents the use of\ncomplex, hand-crafted algorithms in its core, and mitigates error propagation.\nMerging-ISP enables direct reconstructions of HDR images from multiple\ndifferently exposed raw CFA images captured from dynamic scenes. We compared\nMerging-ISP against different alternative cascaded pipelines. End-to-end\nlearning leads to HDR reconstructions of high perceptual quality and\nquantitatively outperforms competing ISPs by more than 1 dB in terms of PSNR.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 09:53:50 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Chaudhari", "Prashant", ""], ["Schirrmacher", "Franziska", ""], ["Maier", "Andreas", ""], ["Riess", "Christian", ""], ["K\u00f6hler", "Thomas", ""]]}, {"id": "1911.04820", "submitter": "Qiang Ren", "authors": "Qiang Ren", "title": "Grouping Capsules Based Different Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capsule network was introduced as a new architecture of neural networks, it\nencoding features as capsules to overcome the lacking of equivariant in the\nconvolutional neural networks. It uses dynamic routing algorithm to train\nparameters in different capsule layers, but the dynamic routing algorithm need\nto be improved. In this paper, we propose a novel capsule network architecture\nand discussed the effect of initialization method of the coupling coefficient\n$c_{ij}$ on the model. First, we analyze the rate of change of the initial\nvalue of $c_{ij}$ when the dynamic routing algorithm iterates. The larger the\ninitial value of $c_{ij}$, the better effect of the model. Then, we proposed\nimprovement that training different types of capsules by grouping capsules\nbased different types. And this improvement can adjust the initial value of\n$c_{ij}$ to make it more suitable. We experimented with our improvements on\nsome computer vision datasets and achieved better results than the original\ncapsule network\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 12:39:20 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Ren", "Qiang", ""]]}, {"id": "1911.04852", "submitter": "Radu Tudor Ionescu", "authors": "Mariana-Iuliana Georgescu, Radu Tudor Ionescu", "title": "Recognizing Facial Expressions of Occluded Faces using Convolutional\n  Neural Networks", "comments": "Accepted at ICONIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an approach based on convolutional neural networks\n(CNNs) for facial expression recognition in a difficult setting with severe\nocclusions. More specifically, our task is to recognize the facial expression\nof a person wearing a virtual reality (VR) headset which essentially occludes\nthe upper part of the face. In order to accurately train neural networks for\nthis setting, in which faces are severely occluded, we modify the training\nexamples by intentionally occluding the upper half of the face. This forces the\nneural networks to focus on the lower part of the face and to obtain better\naccuracy rates than models trained on the entire faces. Our empirical results\non two benchmark data sets, FER+ and AffectNet, show that our CNN models'\npredictions on lower-half faces are up to 13% higher than the baseline CNN\nmodels trained on entire faces, proving their suitability for the VR setting.\nFurthermore, our models' predictions on lower-half faces are no more than 10%\nunder the baseline models' predictions on full faces, proving that there are\nenough clues in the lower part of the face to accurately predict facial\nexpressions.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 13:53:56 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Georgescu", "Mariana-Iuliana", ""], ["Ionescu", "Radu Tudor", ""]]}, {"id": "1911.04863", "submitter": "Daniela Briola", "authors": "Daniela Briola, Viviana Mascardi, Massimiliano Gioseffi", "title": "OntoScene, A Logic-based Scene Interpreter: Implementation and\n  Application in the Rock Art Domain", "comments": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present OntoScene, a framework aimed at understanding the semantics of\nvisual scenes starting from the semantics of their elements and the spatial\nrelations holding between them. OntoScene exploits ontologies for representing\nknowledge and Prolog for specifying the interpretation rules that domain\nexperts may adopt, and for implementing the SceneInterpreter engine. Ontologies\nallow the designer to formalize the domain in a reusable way, and make the\nsystem modular and interoperable with existing multiagent systems, while Prolog\nprovides a solid basis to define complex rules of interpretation in a way that\ncan be affordable even for people with no background in Computational Logics.\nThe domain selected for experimenting OntoScene is that of prehistoric rock\nart, which provides us with a fascinating and challenging testbed. Under\nconsideration in Theory and Practice of Logic Programming (TPLP)\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 13:22:05 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Briola", "Daniela", ""], ["Mascardi", "Viviana", ""], ["Gioseffi", "Massimiliano", ""]]}, {"id": "1911.04890", "submitter": "Takaki Makino", "authors": "Takaki Makino (1), Hank Liao (1), Yannis Assael (2), Brendan\n  Shillingford (2), Basilio Garcia (1), Otavio Braga (1), Olivier Siohan (1)\n  ((1) Google Inc. (2) DeepMind)", "title": "Recurrent Neural Network Transducer for Audio-Visual Speech Recognition", "comments": "Will be presented in 2019 IEEE Automatic Speech Recognition and\n  Understanding Workshop (ASRU 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.CV cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a large-scale audio-visual speech recognition system based\non a recurrent neural network transducer (RNN-T) architecture. To support the\ndevelopment of such a system, we built a large audio-visual (A/V) dataset of\nsegmented utterances extracted from YouTube public videos, leading to 31k hours\nof audio-visual training content. The performance of an audio-only,\nvisual-only, and audio-visual system are compared on two large-vocabulary test\nsets: a set of utterance segments from public YouTube videos called YTDEV18 and\nthe publicly available LRS3-TED set. To highlight the contribution of the\nvisual modality, we also evaluated the performance of our system on the YTDEV18\nset artificially corrupted with background noise and overlapping speech. To the\nbest of our knowledge, our system significantly improves the state-of-the-art\non the LRS3-TED set.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 22:01:42 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Makino", "Takaki", "", "Google Inc"], ["Liao", "Hank", "", "Google Inc"], ["Assael", "Yannis", "", "DeepMind"], ["Shillingford", "Brendan", "", "DeepMind"], ["Garcia", "Basilio", "", "Google Inc"], ["Braga", "Otavio", "", "Google Inc"], ["Siohan", "Olivier", "", "Google Inc"]]}, {"id": "1911.04940", "submitter": "Majd Zreik", "authors": "Majd Zreik, Tim Leiner, Nadieh Khalili, Robbert W. van Hamersvelt,\n  Jelmer M. Wolterink, Michiel Voskuil, Max A. Viergever, Ivana I\\v{s}gum", "title": "Combined analysis of coronary arteries and the left ventricular\n  myocardium in cardiac CT angiography for detection of patients with\n  functionally significant stenosis", "comments": "Submitted to IEEE Access. arXiv admin note: text overlap with\n  arXiv:1906.04419", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Treatment of patients with obstructive coronary artery disease is guided by\nthe functional significance of a coronary artery stenosis. Fractional flow\nreserve (FFR), measured during invasive coronary angiography (ICA), is\nconsidered the gold standard to define the functional significance of a\ncoronary stenosis. Here, we present a method for non-invasive detection of\npatients with functionally significant coronary artery stenosis, combining\nanalysis of the coronary artery tree and the left ventricular (LV) myocardium\nin cardiac CT angiography (CCTA) images. We retrospectively collected CCTA\nscans of 126 patients who underwent invasive FFR measurements, to determine the\nfunctional significance of coronary stenoses. We combine our previous works for\nthe analysis of the complete coronary artery tree and the LV myocardium:\nCoronary arteries are encoded by two disjoint convolutional autoencoders (CAEs)\nand the LV myocardium is characterized by a convolutional neural network (CNN)\nand a CAE. Thereafter, using the extracted encodings of all coronary arteries\nand the LV myocardium, patients are classified according to the presence of\nfunctionally significant stenosis, as defined by the invasively measured FFR.\nTo handle the varying number of coronary arteries in a patient, the\nclassification is formulated as a multiple instance learning problem and is\nperformed using an attention-based neural network. Cross-validation experiments\nresulted in an average area under the receiver operating characteristic curve\nof $0.74 \\pm 0.01$, and showed that the proposed combined analysis outperformed\nthe analysis of the coronary arteries or the LV myocardium only. The results\ndemonstrate the feasibility of combining the analyses of the complete coronary\nartery tree and the LV myocardium in CCTA images for the detection of patients\nwith functionally significant stenosis in coronary arteries.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 13:43:52 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Zreik", "Majd", ""], ["Leiner", "Tim", ""], ["Khalili", "Nadieh", ""], ["van Hamersvelt", "Robbert W.", ""], ["Wolterink", "Jelmer M.", ""], ["Voskuil", "Michiel", ""], ["Viergever", "Max A.", ""], ["I\u0161gum", "Ivana", ""]]}, {"id": "1911.04967", "submitter": "Louis van Harten", "authors": "Louis D. van Harten, Jelmer M. Wolterink, Joost J.C. Verhoeff, Ivana\n  I\\v{s}gum", "title": "Exploiting Clinically Available Delineations for CNN-based Segmentation\n  in Radiotherapy Treatment Planning", "comments": "SPIE Medical Imaging 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have been widely and successfully used\nfor medical image segmentation. However, CNNs are typically considered to\nrequire large numbers of dedicated expert-segmented training volumes, which may\nbe limiting in practice. This work investigates whether clinically obtained\nsegmentations which are readily available in picture archiving and\ncommunication systems (PACS) could provide a possible source of data to train a\nCNN for segmentation of organs-at-risk (OARs) in radiotherapy treatment\nplanning. In such data, delineations of structures deemed irrelevant to the\ntarget clinical use may be lacking. To overcome this issue, we use multi-label\ninstead of multi-class segmentation. We empirically assess how many clinical\ndelineations would be sufficient to train a CNN for the segmentation of OARs\nand find that increasing the training set size beyond a limited number of\nimages leads to sharply diminishing returns. Moreover, we find that by using\nmulti-label segmentation, missing structures in the reference standard do not\nhave a negative effect on overall segmentation accuracy. These results indicate\nthat segmentations obtained in a clinical workflow can be used to train an\naccurate OAR segmentation model.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 15:58:23 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["van Harten", "Louis D.", ""], ["Wolterink", "Jelmer M.", ""], ["Verhoeff", "Joost J. C.", ""], ["I\u0161gum", "Ivana", ""]]}, {"id": "1911.04969", "submitter": "Babak Hosseini", "authors": "Babak Hosseini, Romain Montagne, Barbara Hammer", "title": "Deep-Aligned Convolutional Neural Network for Skeleton-based Action\n  Recognition and Segmentation", "comments": "19th IEEE International Conference on Data Mining (ICDM 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are deep learning frameworks which are\nwell-known for their notable performance in classification tasks. Hence, many\nskeleton-based action recognition and segmentation (SBARS) algorithms benefit\nfrom them in their designs. However, a shortcoming of such applications is the\ngeneral lack of spatial relationships between the input features in such data\ntypes. Besides, non-uniform temporal scalings is a common issue in\nskeleton-based data streams which leads to having different input sizes even\nwithin one specific action category. In this work, we propose a novel\ndeep-aligned convolutional neural network (DACNN) to tackle the above\nchallenges for the particular problem of SBARS. Our network is designed by\nintroducing a new type of filters in the context of CNNs which are trained\nbased on their alignments to the local subsequences in the inputs. These\nfilters result in efficient predictions as well as learning interpretable\npatterns in the data. We empirically evaluate our framework on real-world\nbenchmarks showing that the proposed DACNN algorithm obtains a competitive\nperformance compared to the state-of-the-art while benefiting from a less\ncomplicated yet more interpretable model.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 16:00:56 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Hosseini", "Babak", ""], ["Montagne", "Romain", ""], ["Hammer", "Barbara", ""]]}, {"id": "1911.04971", "submitter": "Tal Daniel", "authors": "Tal Daniel, Thanard Kurutach and Aviv Tamar", "title": "Deep Variational Semi-Supervised Novelty Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In anomaly detection (AD), one seeks to identify whether a test sample is\nabnormal, given a data set of normal samples. A recent and promising approach\nto AD relies on deep generative models, such as variational autoencoders\n(VAEs), for unsupervised learning of the normal data distribution. In\nsemi-supervised AD (SSAD), the data also includes a small sample of labeled\nanomalies. In this work, we propose two variational methods for training VAEs\nfor SSAD. The intuitive idea in both methods is to train the encoder to\n`separate' between latent vectors for normal and outlier data. We show that\nthis idea can be derived from principled probabilistic formulations of the\nproblem, and propose simple and effective algorithms. Our methods can be\napplied to various data types, as we demonstrate on SSAD datasets ranging from\nnatural images to astronomy and medicine, can be combined with any VAE model\narchitecture, and are naturally compatible with ensembling. When comparing to\nstate-of-the-art SSAD methods that are not specific to particular data types,\nwe obtain marked improvement in outlier detection.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 16:03:50 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 08:52:08 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Daniel", "Tal", ""], ["Kurutach", "Thanard", ""], ["Tamar", "Aviv", ""]]}, {"id": "1911.04978", "submitter": "QiKui Zhu", "authors": "Qikui Zhu, Bo Du, Pingkun Yan", "title": "Multi-hop Convolutions on Weighted Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolutional Networks (GCNs) have made significant advances in\nsemi-supervised learning, especially for classification tasks. However,\nexisting GCN based methods have two main drawbacks. First, to increase the\nreceptive field and improve the representation capability of GCNs, larger\nkernels or deeper network architectures are used, which greatly increases the\ncomputational complexity and the number of parameters. Second, methods working\non higher order graphs computed directly from adjacency matrices may alter the\nrelationship between graph nodes, particularly for weighted graphs. In\naddition, the direct construction of higher-order graphs introduces redundant\ninformation, which may result in lower network performance. To address the\nabove weaknesses, in this paper, we propose a new method of multi-hop\nconvolutional network on weighted graphs. The proposed method consists of\nmultiple convolutional branches, where each branch extracts node representation\nfrom a $k$-hop graph with small kernels. Such design systematically aggregates\nmulti-scale contextual information without adding redundant information.\nFurthermore, to efficiently combine the extracted information from the\nmulti-hop branches, an adaptive weight computation (AWC) layer is proposed. We\ndemonstrate the superiority of our MultiHop in six publicly available datasets,\nincluding three citation network datasets and three medical image datasets. The\nexperimental results show that our proposed MultiHop method achieves the\nhighest classification accuracy and outperforms the state-of-the-art methods.\nThe source code of this work have been released on GitHub\n(https://github.com/ahukui/Multi-hop-Convolutions-on-Weighted-Graphs).\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 16:08:22 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Zhu", "Qikui", ""], ["Du", "Bo", ""], ["Yan", "Pingkun", ""]]}, {"id": "1911.04986", "submitter": "Louis van Harten", "authors": "Louis D. van Harten, Jelmer M. Wolterink, Joost J.C. Verhoeff, Ivana\n  I\\v{s}gum", "title": "Automatic Online Quality Control of Synthetic CTs", "comments": "SPIE Medical Imaging 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate MR-to-CT synthesis is a requirement for MR-only workflows in\nradiotherapy (RT) treatment planning. In recent years, deep learning-based\napproaches have shown impressive results in this field. However, to prevent\ndownstream errors in RT treatment planning, it is important that deep learning\nmodels are only applied to data for which they are trained and that generated\nsynthetic CT (sCT) images do not contain severe errors. For this, a mechanism\nfor online quality control should be in place. In this work, we use an ensemble\nof sCT generators and assess their disagreement as a measure of uncertainty of\nthe results. We show that this uncertainty measure can be used for two kinds of\nonline quality control. First, to detect input images that are outside the\nexpected distribution of MR images. Second, to identify sCT images that were\ngenerated from suitable MR images but potentially contain errors. Such\nautomatic online quality control for sCT generation is likely to become an\nintegral part of MR-only RT workflows.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 16:19:20 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["van Harten", "Louis D.", ""], ["Wolterink", "Jelmer M.", ""], ["Verhoeff", "Joost J. C.", ""], ["I\u0161gum", "Ivana", ""]]}, {"id": "1911.04992", "submitter": "Alexander Zamyatin", "authors": "Alexander Zamyatin", "title": "Recursive Filter for Space-Variant Variance Reduction", "comments": "12 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA eess.IV eess.SP math.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a method to reduce non-uniform sample variance to a predetermined\ntarget level. The proposed space-variant filter can equalize variance of the\nnon-stationary signal, or vary filtering strength based on image features, such\nas edges, etc., as shown by applications in this work. This approach computes\nvariance reduction ratio at each point of the image, based on the given target\nvariance. Then, a space-variant filter with matching variance reduction power\nis applied. A mathematical framework of atomic kernels is developed to\nfacilitate stable and fast computation of the filter bank kernels. Recursive\nformulation allows using small kernel size, which makes the space-variant\nfilter more suitable for fast parallel implementation. Despite the small kernel\nsize, the recursive filter possesses strong variance reduction power. Filter\naccuracy is measured by the variance reduction against the target variance;\ntesting demonstrated high accuracy of variance reduction of the recursive\nfilter compared to the fixed-size filter. The proposed filter was applied to\nadaptive filtering in image reconstruction and edge-preserving denoising.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 16:28:45 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 19:09:51 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Zamyatin", "Alexander", ""]]}, {"id": "1911.05024", "submitter": "Beatriz Quintino Ferreira", "authors": "Beatriz Quintino Ferreira, Jo\\~ao P. Costeira, Ricardo G. Sousa,\n  Liang-Yan Gui, Jo\\~ao P. Gomes", "title": "Pose Guided Attention for Multi-label Fashion Image Classification", "comments": "Published at ICCV 2019 Workshop on Computer Vision for Fashion, Art\n  and Design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a compact framework with guided attention for multi-label\nclassification in the fashion domain. Our visual semantic attention model\n(VSAM) is supervised by automatic pose extraction creating a discriminative\nfeature space. VSAM outperforms the state of the art for an in-house dataset\nand performs on par with previous works on the DeepFashion dataset, even\nwithout using any landmark annotations. Additionally, we show that our semantic\nattention module brings robustness to large quantities of wrong annotations and\nprovides more interpretable results.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 17:32:53 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Ferreira", "Beatriz Quintino", ""], ["Costeira", "Jo\u00e3o P.", ""], ["Sousa", "Ricardo G.", ""], ["Gui", "Liang-Yan", ""], ["Gomes", "Jo\u00e3o P.", ""]]}, {"id": "1911.05033", "submitter": "Shuming Jiao", "authors": "Shuming Jiao, Jun Feng, Yang Gao, Ting Lei, Xiaocong Yuan", "title": "Visual cryptography in single-pixel imaging", "comments": null, "journal-ref": null, "doi": "10.1364/OE.383240", "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two novel visual cryptography (VC) schemes are proposed by combining VC with\nsingle-pixel imaging (SPI) for the first time. It is pointed out that the\noverlapping of visual key images in VC is similar to the superposition of pixel\nintensities by a single-pixel detector in SPI. In the first scheme, QR-code VC\nis designed by using opaque sheets instead of transparent sheets. The secret\nimage can be recovered when identical illumination patterns are projected onto\nmultiple visual key images and a single detector is used to record the total\nlight intensities. In the second scheme, the secret image is shared by multiple\nillumination pattern sequences and it can be recovered when the visual key\npatterns are projected onto identical items. The application of VC can be\nextended to more diversified scenarios by our proposed schemes.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 17:49:50 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Jiao", "Shuming", ""], ["Feng", "Jun", ""], ["Gao", "Yang", ""], ["Lei", "Ting", ""], ["Yuan", "Xiaocong", ""]]}, {"id": "1911.05045", "submitter": "Michele Alberti", "authors": "Michele Alberti, Angela Botros, Narayan Schuez, Rolf Ingold, Marcus\n  Liwicki and Mathias Seuret", "title": "Trainable Spectrally Initializable Matrix Transformations in\n  Convolutional Neural Networks", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate the application of trainable and spectrally\ninitializable matrix transformations on the feature maps produced by\nconvolution operations. While previous literature has already demonstrated the\npossibility of adding static spectral transformations as feature processors,\nour focus is on more general trainable transforms. We study the transforms in\nvarious architectural configurations on four datasets of different nature: from\nmedical (ColorectalHist, HAM10000) and natural (Flowers, ImageNet) images to\nhistorical documents (CB55) and handwriting recognition (GPDS). With rigorous\nexperiments that control for the number of parameters and randomness, we show\nthat networks utilizing the introduced matrix transformations outperform\nvanilla neural networks. The observed accuracy increases by an average of 2.2\nacross all datasets. In addition, we show that the benefit of spectral\ninitialization leads to significantly faster convergence, as opposed to\nrandomly initialized matrix transformations. The transformations are\nimplemented as auto-differentiable PyTorch modules that can be incorporated\ninto any neural network architecture. The entire code base is open-source.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 18:06:52 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 17:36:08 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Alberti", "Michele", ""], ["Botros", "Angela", ""], ["Schuez", "Narayan", ""], ["Ingold", "Rolf", ""], ["Liwicki", "Marcus", ""], ["Seuret", "Mathias", ""]]}, {"id": "1911.05055", "submitter": "Brian Wandell", "authors": "Fabian H. Reith and Brian A. Wandell", "title": "A convolutional neural network reaches optimal sensitivity for detecting\n  some, but not all, patterns", "comments": "22 pages, 8 figures, pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the performance of modern convolutional neural networks (CNN)\nand a linear support vector machine (SVM) with respect to spatial contrast\nsensitivity. Specifically, we compare CNN sensitivity to that of a Bayesian\nideal observer (IO) with the signal-known-exactly and noise known\nstatistically. A ResNet-18 reaches optimal performance for harmonic patterns,\nas well as several classes of real world signals including faces. For these\nstimuli the CNN substantially outperforms the SVM. We further analyzed the case\nin which the signal might appear in one of multiple locations and found that\nCNN spatial sensitivity continues to match the IO. However, the CNN sensitivity\nwas far below optimal at detecting certain complex texture patterns. These\nmeasurements show that CNNs can have very large performance differences when\ndetecting the presence of spatial patterns. These differences may have a\nsignificant impact on the performance of an imaging system designed to detect\nlow contrast spatial patterns.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 18:26:02 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 20:27:01 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2020 17:49:59 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Reith", "Fabian H.", ""], ["Wandell", "Brian A.", ""]]}, {"id": "1911.05063", "submitter": "Krishna Murthy Jatavallabhula", "authors": "Krishna Murthy Jatavallabhula, Edward Smith, Jean-Francois Lafleche,\n  Clement Fuji Tsang, Artem Rozantsev, Wenzheng Chen, Tommy Xiang, Rev\n  Lebaredian, Sanja Fidler", "title": "Kaolin: A PyTorch Library for Accelerating 3D Deep Learning Research", "comments": "Kaolin is available as open-source software at\n  https://github.com/NVIDIAGameWorks/kaolin/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Kaolin, a PyTorch library aiming to accelerate 3D deep learning\nresearch. Kaolin provides efficient implementations of differentiable 3D\nmodules for use in deep learning systems. With functionality to load and\npreprocess several popular 3D datasets, and native functions to manipulate\nmeshes, pointclouds, signed distance functions, and voxel grids, Kaolin\nmitigates the need to write wasteful boilerplate code. Kaolin packages together\nseveral differentiable graphics modules including rendering, lighting, shading,\nand view warping. Kaolin also supports an array of loss functions and\nevaluation metrics for seamless evaluation and provides visualization\nfunctionality to render the 3D results. Importantly, we curate a comprehensive\nmodel zoo comprising many state-of-the-art 3D deep learning architectures, to\nserve as a starting point for future research endeavours. Kaolin is available\nas open-source software at https://github.com/NVIDIAGameWorks/kaolin/.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 18:47:37 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 18:34:03 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Jatavallabhula", "Krishna Murthy", ""], ["Smith", "Edward", ""], ["Lafleche", "Jean-Francois", ""], ["Tsang", "Clement Fuji", ""], ["Rozantsev", "Artem", ""], ["Chen", "Wenzheng", ""], ["Xiang", "Tommy", ""], ["Lebaredian", "Rev", ""], ["Fidler", "Sanja", ""]]}, {"id": "1911.05071", "submitter": "Yen-Chen Lin", "authors": "Lin Yen-Chen, Maria Bauza, Phillip Isola", "title": "Experience-Embedded Visual Foresight", "comments": "CoRL 2019. Project website: http://yenchenlin.me/evf/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual foresight gives an agent a window into the future, which it can use to\nanticipate events before they happen and plan strategic behavior. Although\nimpressive results have been achieved on video prediction in constrained\nsettings, these models fail to generalize when confronted with unfamiliar\nreal-world objects. In this paper, we tackle the generalization problem via\nfast adaptation, where we train a prediction model to quickly adapt to the\nobserved visual dynamics of a novel object. Our method, Experience-embedded\nVisual Foresight (EVF), jointly learns a fast adaptation module, which encodes\nobserved trajectories of the new object into a vector embedding, and a visual\nprediction model, which conditions on this embedding to generate physically\nplausible predictions. For evaluation, we compare our method against baselines\non video prediction and benchmark its utility on two real-world control tasks.\nWe show that our method is able to quickly adapt to new visual dynamics and\nachieves lower error than the baselines when manipulating novel objects.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 18:58:30 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2019 14:45:06 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Yen-Chen", "Lin", ""], ["Bauza", "Maria", ""], ["Isola", "Phillip", ""]]}, {"id": "1911.05072", "submitter": "Zhe Li", "authors": "Zhe Li, Wieland Brendel, Edgar Y. Walker, Erick Cobos, Taliah\n  Muhammad, Jacob Reimer, Matthias Bethge, Fabian H. Sinz, Xaq Pitkow, Andreas\n  S. Tolias", "title": "Learning From Brains How to Regularize Machines", "comments": "14 pages, 7 figures, NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite impressive performance on numerous visual tasks, Convolutional Neural\nNetworks (CNNs) --- unlike brains --- are often highly sensitive to small\nperturbations of their input, e.g. adversarial noise leading to erroneous\ndecisions. We propose to regularize CNNs using large-scale neuroscience data to\nlearn more robust neural features in terms of representational similarity. We\npresented natural images to mice and measured the responses of thousands of\nneurons from cortical visual areas. Next, we denoised the notoriously variable\nneural activity using strong predictive models trained on this large corpus of\nresponses from the mouse visual system, and calculated the representational\nsimilarity for millions of pairs of images from the model's predictions. We\nthen used the neural representation similarity to regularize CNNs trained on\nimage classification by penalizing intermediate representations that deviated\nfrom neural ones. This preserved performance of baseline models when\nclassifying images under standard benchmarks, while maintaining substantially\nhigher performance compared to baseline or control models when classifying\nnoisy images. Moreover, the models regularized with cortical representations\nalso improved model robustness in terms of adversarial attacks. This\ndemonstrates that regularizing with neural data can be an effective tool to\ncreate an inductive bias towards more robust inference.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 21:53:26 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Li", "Zhe", ""], ["Brendel", "Wieland", ""], ["Walker", "Edgar Y.", ""], ["Cobos", "Erick", ""], ["Muhammad", "Taliah", ""], ["Reimer", "Jacob", ""], ["Bethge", "Matthias", ""], ["Sinz", "Fabian H.", ""], ["Pitkow", "Xaq", ""], ["Tolias", "Andreas S.", ""]]}, {"id": "1911.05075", "submitter": "Kira Maag", "authors": "Kira Maag, Matthias Rottmann and Hanno Gottschalk", "title": "Time-Dynamic Estimates of the Reliability of Deep Semantic Segmentation\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the semantic segmentation of street scenes with neural networks, the\nreliability of predictions is of highest interest. The assessment of neural\nnetworks by means of uncertainties is a common ansatz to prevent safety issues.\nAs in applications like automated driving, video streams of images are\navailable, we present a time-dynamic approach to investigating uncertainties\nand assessing the prediction quality of neural networks. We track segments over\ntime and gather aggregated metrics per segment, thus obtaining time series of\nmetrics from which we assess prediction quality. This is done by either\nclassifying between intersection over union equal to 0 and greater than 0 or\npredicting the intersection over union directly. We study different models for\nthese two tasks and analyze the influence of the time series length on the\npredictive power of our metrics.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 13:55:50 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 09:37:54 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Maag", "Kira", ""], ["Rottmann", "Matthias", ""], ["Gottschalk", "Hanno", ""]]}, {"id": "1911.05113", "submitter": "Ho Hin Lee", "authors": "Ho Hin Lee, Yucheng Tang, Olivia Tang, Yuchen Xu, Yunqiang Chen,\n  Dashan Gao, Shizhong Han, Riqiang Gao, Michael R. Savona, Richard G.\n  Abramson, Yuankai Huo, Bennett A. Landman", "title": "Semi-Supervised Multi-Organ Segmentation through Quality Assurance\n  Supervision", "comments": "7 pages, 5 figures, Accepted by SPIE 2020: Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human in-the-loop quality assurance (QA) is typically performed after medical\nimage segmentation to ensure that the systems are performing as intended, as\nwell as identifying and excluding outliers. By performing QA on large-scale,\npreviously unlabeled testing data, categorical QA scores can be generatedIn\nthis paper, we propose a semi-supervised multi-organ segmentation deep neural\nnetwork consisting of a traditional segmentation model generator and a QA\ninvolved discriminator. A large-scale dataset of 2027 volumes are used to train\nthe generator, whose 2-D montage images and segmentation mask with QA scores\nare used to train the discriminator. To generate the QA scores, the 2-D montage\nimages were reviewed manually and coded 0 (success), 1 (errors consistent with\npublished performance), and 2 (gross failure). Then, the ResNet-18 network was\ntrained with 1623 montage images in equal distribution of all three code labels\nand achieved an accuracy 94% for classification predictions with 404 montage\nimages withheld for the test cohort. To assess the performance of using the QA\nsupervision, the discriminator was used as a loss function in a multi-organ\nsegmentation pipeline. The inclusion of QA-loss function boosted performance on\nthe unlabeled test dataset from 714 patients to 951 patients over the baseline\nmodel. Additionally, the number of failures decreased from 606 (29.90%) to 402\n(19.83%). The contributions of the proposed method are threefold: We show that\n(1) the QA scores can be used as a loss function to perform semi-supervised\nlearning for unlabeled data, (2) the well trained discriminator is learnt by QA\nscore rather than traditional true/false, and (3) the performance of\nmulti-organ segmentation on unlabeled datasets can be fine-tuned with more\nrobust and higher accuracy than the original baseline method.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 19:35:58 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Lee", "Ho Hin", ""], ["Tang", "Yucheng", ""], ["Tang", "Olivia", ""], ["Xu", "Yuchen", ""], ["Chen", "Yunqiang", ""], ["Gao", "Dashan", ""], ["Han", "Shizhong", ""], ["Gao", "Riqiang", ""], ["Savona", "Michael R.", ""], ["Abramson", "Richard G.", ""], ["Huo", "Yuankai", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1911.05115", "submitter": "Riqiang Gao", "authors": "Riqiang Gao, Lingfeng Li, Yucheng Tang, Sanja L. Antic, Alexis B.\n  Paulson, Yuankai Huo, Kim L. Sandler, Pierre P. Massion, Bennett A. Landman", "title": "Deep Multi-task Prediction of Lung Cancer and Cancer-free Progression\n  from Censored Heterogenous Clinical Imaging", "comments": "Finalist for the 2020 Robert F. Wagner All Conference Best Student\n  Paper Award, SPIE 2020 Medical Imaging. (Highest award in Image Processing\n  section)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annual low dose computed tomography (CT) lung screening is currently advised\nfor individuals at high risk of lung cancer (e.g., heavy smokers between 55 and\n80 years old). The recommended screening practice significantly reduces\nall-cause mortality, but the vast majority of screening results are negative\nfor cancer. If patients at very low risk could be identified based on\nindividualized, image-based biomarkers, the health care resources could be more\nefficiently allocated to higher risk patients and reduce overall exposure to\nionizing radiation. In this work, we propose a multi-task (diagnosis and\nprognosis) deep convolutional neural network to improve the diagnostic accuracy\nover a baseline model while simultaneously estimating a personalized\ncancer-free progression time (CFPT). A novel Censored Regression Loss (CRL) is\nproposed to perform weakly supervised regression so that even single negative\nscreening scans can provide small incremental value. Herein, we study 2287\nscans from 1433 de-identified patients from the Vanderbilt Lung Screening\nProgram (VLSP) and Molecular Characterization Laboratories (MCL) cohorts. Using\nfive-fold cross-validation, we train a 3D attention-based network under two\nscenarios: (1) single-task learning with only classification, and (2)\nmulti-task learning with both classification and regression. The single-task\nlearning leads to a higher AUC compared with the Kaggle challenge winner\npre-trained model (0.878 v. 0.856), and multi-task learning significantly\nimproves the single-task one (AUC 0.895, p<0.01, McNemar test). In summary, the\nimage-based predicted CFPT can be used in follow-up year lung cancer prediction\nand data assessment.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 19:39:22 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 17:38:44 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Gao", "Riqiang", ""], ["Li", "Lingfeng", ""], ["Tang", "Yucheng", ""], ["Antic", "Sanja L.", ""], ["Paulson", "Alexis B.", ""], ["Huo", "Yuankai", ""], ["Sandler", "Kim L.", ""], ["Massion", "Pierre P.", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1911.05140", "submitter": "Kiret Dhindsa", "authors": "Umaseh Sivanesan and Luis H. Braga and Ranil R. Sonnadara and Kiret\n  Dhindsa", "title": "Unsupervised Medical Image Segmentation with Adversarial Networks: From\n  Edge Diagrams to Segmentation Maps", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and approach to unsupervised semantic medical image segmentation\nthat extends previous work with generative adversarial networks. We use\nexisting edge detection methods to construct simple edge diagrams, train a\ngenerative model to convert them into synthetic medical images, and construct a\ndataset of synthetic images with known segmentations using variations on\nextracted edge diagrams. This synthetic dataset is then used to train a\nsupervised image segmentation model. We test our approach on a clinical dataset\nof kidney ultrasound images and the benchmark ISIC 2018 skin lesion dataset. We\nshow that our unsupervised approach is more accurate than previous unsupervised\nmethods, and performs reasonably compared to supervised image segmentation\nmodels. All code and trained models are available at\nhttps://github.com/kiretd/Unsupervised-MIseg.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 20:56:33 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Sivanesan", "Umaseh", ""], ["Braga", "Luis H.", ""], ["Sonnadara", "Ranil R.", ""], ["Dhindsa", "Kiret", ""]]}, {"id": "1911.05185", "submitter": "Benjamin Joffe", "authors": "Benjamin Joffe, Tevon Walker. Remi Gourdon, Konrad Ahlin", "title": "Pose estimation and bin picking for deformable products", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic systems in manufacturing applications commonly assume known object\ngeometry and appearance. This simplifies the task for the 3D perception\nalgorithms and allows the manipulation to be more deterministic. However, those\napproaches are not easily transferable to the agricultural and food domains due\nto the variability and deformability of natural food. We demonstrate an\napproach applied to poultry products that allows picking up a whole chicken\nfrom an unordered bin using a suction cup gripper, estimating its pose using a\nDeep Learning approach, and placing it in a canonical orientation where it can\nbe further processed. Our robotic system was experimentally evaluated and is\nable to generalize to object variations and achieves high accuracy on bin\npicking and pose estimation tasks in a real-world environment.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 23:09:55 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Joffe", "Benjamin", ""], ["Gourdon", "Tevon Walker. Remi", ""], ["Ahlin", "Konrad", ""]]}, {"id": "1911.05186", "submitter": "Wei Zou", "authors": "Wubo Li, Wei Zou, Xiangang Li", "title": "TCT: A Cross-supervised Learning Method for Multimodal Sequence\n  Representation", "comments": "submitted to ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodalities provide promising performance than unimodality in most tasks.\nHowever, learning the semantic of the representations from multimodalities\nefficiently is extremely challenging. To tackle this, we propose the\nTransformer based Cross-modal Translator (TCT) to learn unimodal sequence\nrepresentations by translating from other related multimodal sequences on a\nsupervised learning method. Combined TCT with Multimodal Transformer Network\n(MTN), we evaluate MTN-TCT on the video-grounded dialogue which uses\nmultimodality. The proposed method reports new state-of-the-art performance on\nvideo-grounded dialogue which indicates representations learned by TCT are more\nsemantics compared to directly use unimodality.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 05:02:15 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Li", "Wubo", ""], ["Zou", "Wei", ""], ["Li", "Xiangang", ""]]}, {"id": "1911.05187", "submitter": "Carl Norman", "authors": "Carl Norman", "title": "AI in Pursuit of Happiness, Finding Only Sadness: Multi-Modal Facial\n  Emotion Recognition Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of automated Facial Emotion Recognition (FER) grows the more\ncommon human-machine interactions become, which will only continue to increase\ndramatically with time. A common method to describe human sentiment or feeling\nis the categorical model the `7 basic emotions', consisting of `Angry',\n`Disgust', `Fear', `Happiness', `Sadness', `Surprise' and `Neutral'. The\n`Emotion Recognition in the Wild' (EmotiW) competition is now in its 7th year\nand has become the standard benchmark for measuring FER performance. The focus\nof this paper is the EmotiW sub-challenge of classifying videos in the `Acted\nFacial Expression in the Wild' (AFEW) dataset, consisting of both visual and\naudio modalities, into one of the above classes. Machine learning has exploded\nas a research topic in recent years, with advancements in `Deep Learning' a key\npart of this. Although Deep Learning techniques have been widely applied to the\nFER task by entrants in previous years, this paper has two main contributions:\n(i) to apply the latest `state-of-the-art' visual and temporal networks and\n(ii) exploring various methods of fusing features extracted from the visual and\naudio elements to enrich the information available to the final model making\nthe prediction. There are a number of complex issues that arise when trying to\nclassify emotions for `in-the-wild' video sequences, which the above two\napproaches attempt to directly address. There are some positive findings when\ncomparing the results of this paper to past submissions, indicating that\nfurther research into the proposed methods and fine-tuning of the models\ndeployed, could result in another step forwards in the field of automated FER.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 14:49:39 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Norman", "Carl", ""]]}, {"id": "1911.05188", "submitter": "Zheng Lian", "authors": "Zheng Lian, Ya Li, Jian-Hua Tao, Jian Huang, Ming-Yue Niu", "title": "Expression Analysis Based on Face Regions in Read-world Conditions", "comments": "International Journal of Automation and Computing 2018", "journal-ref": null, "doi": "10.1007/s11633-019-1176-9", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial emotion recognition is an essential and important aspect of the field\nof human-machine interaction. Past research on facial emotion recognition\nfocuses on the laboratory environment. However, it faces many challenges in\nreal-world conditions, i.e., illumination changes, large pose variations and\npartial or full occlusions. Those challenges lead to different face areas with\ndifferent degrees of sharpness and completeness. Inspired by this fact, we\nfocus on the authenticity of predictions generated by different <emotion,\nregion> pairs. For example, if only the mouth areas are available and the\nemotion classifier predicts happiness, then there is a question of how to judge\nthe authenticity of predictions. This problem can be converted into the\ncontribution of different face areas to different emotions. In this paper, we\ndivide the whole face into six areas: nose areas, mouth areas, eyes areas, nose\nto mouth areas, nose to eyes areas and mouth to eyes areas. To obtain more\nconvincing results, our experiments are conducted on three different databases:\nfacial expression recognition + ( FER+), real-world affective faces database\n(RAF-DB) and expression in-the-wild (ExpW) dataset. Through analysis of the\nclassification accuracy, the confusion matrix and the class activation map\n(CAM), we can establish convincing results. To sum up, the contributions of\nthis paper lie in two areas: 1) We visualize concerned areas of human faces in\nemotion recognition; 2) We analyze the contribution of different face areas to\ndifferent emotions in real-world conditions through experimental analysis. Our\nfindings can be combined with findings in psychology to promote the\nunderstanding of emotional expressions.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 16:04:21 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Lian", "Zheng", ""], ["Li", "Ya", ""], ["Tao", "Jian-Hua", ""], ["Huang", "Jian", ""], ["Niu", "Ming-Yue", ""]]}, {"id": "1911.05189", "submitter": "Dmitry Rodin", "authors": "Dmitry Rodin and Nikita Orlov", "title": "Fast Glare Detection in Document Images", "comments": "4 pages, Workshop on Industrial Applications of Document Analysis and\n  Recognition 2019", "journal-ref": null, "doi": "10.1109/ICDARW.2019.60123", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Glare is a phenomenon that occurs when the scene has a reflection of a light\nsource or has one in it. This luminescence can hide useful information from the\nimage, making text recognition virtually impossible. In this paper, we propose\nan approach to detect glare in images taken by users via mobile devices. Our\nmethod divides the document into blocks and collects luminance features from\nthe original image and black-white strokes histograms of the binarized image.\nFinally, glare is detected using a convolutional neural network on the\naforementioned histograms and luminance features. The network consists of\nseveral feature extraction blocks, one for each type of input, and the\ndetection block, which calculates the resulting glare heatmap based on the\noutput of the extraction part. The proposed solution detects glare with high\nrecall and f-score.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 09:12:01 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Rodin", "Dmitry", ""], ["Orlov", "Nikita", ""]]}, {"id": "1911.05210", "submitter": "Fei Ding", "authors": "Fei Ding and Feng Luo and Yin Yang", "title": "Double cycle-consistent generative adversarial network for unsupervised\n  conditional generation", "comments": "12 pages, 4 figures, and 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional generative models have achieved considerable success in the past\nfew years, but usually require a lot of labeled data. Recently, ClusterGAN\ncombines GAN with an encoder to achieve remarkable clustering performance via\nunsupervised conditional generation. However, it ignores the real conditional\ndistribution of data, which leads to generating less diverse samples for each\nclass and makes the encoder only achieve sub-optimal clustering performance.\nHere, we propose a new unsupervised conditional generation framework, Double\nCycle-Consistent Conditional GAN (DC3-GAN), which can generate diverse\nclass-conditioned samples. We enforce the encoder and the generator of GAN to\nform an encoder-generator pair in addition to the generator-encoder pair, which\nenables us to avoid the low-diversity generation and the triviality of latent\nfeatures. We train the encoder-generator pair using real data, which can\nindirectly estimate the real conditional distribution. Meanwhile, this\nframework enforces the outputs of the encoder to match the inputs of GAN and\nthe prior noise distribution, which disentangles latent space into two parts:\none-hot discrete and continuous latent variables. The former can be directly\nexpressed as clusters and the latter represents remaining unspecified factors.\nThis work demonstrates that enhancing the diversity of unsupervised conditional\ngenerated samples can improve the clustering performance. Experiments on\ndifferent benchmark datasets show that the proposed method outperforms existing\ngenerative model-based clustering methods, and also achieves the optimal\ndisentanglement performance.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 00:11:50 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 00:52:00 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 15:25:55 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Ding", "Fei", ""], ["Luo", "Feng", ""], ["Yang", "Yin", ""]]}, {"id": "1911.05248", "submitter": "Sara Hooker", "authors": "Sara Hooker, Aaron Courville, Gregory Clark, Yann Dauphin, Andrea\n  Frome", "title": "What Do Compressed Deep Neural Networks Forget?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network pruning and quantization techniques have demonstrated it\nis possible to achieve high levels of compression with surprisingly little\ndegradation to test set accuracy. However, this measure of performance conceals\nsignificant differences in how different classes and images are impacted by\nmodel compression techniques. We find that models with radically different\nnumbers of weights have comparable top-line performance metrics but diverge\nconsiderably in behavior on a narrow subset of the dataset. This small subset\nof data points, which we term Pruning Identified Exemplars (PIEs) are\nsystematically more impacted by the introduction of sparsity. Compression\ndisproportionately impacts model performance on the underrepresented long-tail\nof the data distribution. PIEs over-index on atypical or noisy images that are\nfar more challenging for both humans and algorithms to classify. Our work\nprovides intuition into the role of capacity in deep neural networks and the\ntrade-offs incurred by compression. An understanding of this disparate impact\nis critical given the widespread deployment of compressed models in the wild.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 02:02:19 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 18:24:24 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Hooker", "Sara", ""], ["Courville", "Aaron", ""], ["Clark", "Gregory", ""], ["Dauphin", "Yann", ""], ["Frome", "Andrea", ""]]}, {"id": "1911.05250", "submitter": "Xiangyu He", "authors": "Xiangyu He, Zitao Mo, Qiang Chen, Anda Cheng, Peisong Wang, Jian Cheng", "title": "Location-aware Upsampling for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many successful learning targets such as minimizing dice loss and\ncross-entropy loss have enabled unprecedented breakthroughs in segmentation\ntasks. Beyond these semantic metrics, this paper aims to introduce location\nsupervision into semantic segmentation. Based on this idea, we present a\nLocation-aware Upsampling (LaU) that adaptively refines the interpolating\ncoordinates with trainable offsets. Then, location-aware losses are established\nby encouraging pixels to move towards well-classified locations. An LaU is\noffset prediction coupled with interpolation, which is trained end-to-end to\ngenerate confidence score at each position from coarse to fine. Guided by\nlocation-aware losses, the new module can replace its plain counterpart\n(\\textit{e.g.}, bilinear upsampling) in a plug-and-play manner to further boost\nthe leading encoder-decoder approaches. Extensive experiments validate the\nconsistent improvement over the state-of-the-art methods on benchmark datasets.\nOur code is available at\nhttps://github.com/HolmesShuan/Location-aware-Upsampling-for-Semantic-Segmentation\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 02:15:06 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 02:32:57 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["He", "Xiangyu", ""], ["Mo", "Zitao", ""], ["Chen", "Qiang", ""], ["Cheng", "Anda", ""], ["Wang", "Peisong", ""], ["Cheng", "Jian", ""]]}, {"id": "1911.05253", "submitter": "Zhengkai Jiang", "authors": "Zhengkai Jiang, Yu Liu, Ceyuan Yang, Jihao Liu, Peng Gao, Qian Zhang,\n  Shiming Xiang, Chunhong Pan", "title": "Learning Where to Focus for Efficient Video Object Detection", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferring existing image-based detectors to the video is non-trivial since\nthe quality of frames is always deteriorated by part occlusion, rare pose, and\nmotion blur. Previous approaches exploit to propagate and aggregate features\nacross video frames by using optical flow-warping. However, directly applying\nimage-level optical flow onto the high-level features might not establish\naccurate spatial correspondences. Therefore, a novel module called Learnable\nSpatio-Temporal Sampling (LSTS) has been proposed to learn semantic-level\ncorrespondences among adjacent frame features accurately. The sampled locations\nare first randomly initialized, then updated iteratively to find better spatial\ncorrespondences guided by detection supervision progressively. Besides,\nSparsely Recursive Feature Updating (SRFU) module and Dense Feature Aggregation\n(DFA) module are also introduced to model temporal relations and enhance\nper-frame features, respectively. Without bells and whistles, the proposed\nmethod achieves state-of-the-art performance on the ImageNet VID dataset with\nless computational complexity and real-time speed. Code will be made available\nat https://github.com/jiangzhengkai/LSTS.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 02:17:20 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 11:46:16 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Jiang", "Zhengkai", ""], ["Liu", "Yu", ""], ["Yang", "Ceyuan", ""], ["Liu", "Jihao", ""], ["Gao", "Peng", ""], ["Zhang", "Qian", ""], ["Xiang", "Shiming", ""], ["Pan", "Chunhong", ""]]}, {"id": "1911.05256", "submitter": "Michael Lingzhi Li", "authors": "Michael Lingzhi Li, Meng Dong, Jiawei Zhou, Alexander M. Rush", "title": "A Hierarchy of Graph Neural Networks Based on Learnable Local Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs) are a powerful tool to learn representations on\ngraphs by iteratively aggregating features from node neighbourhoods. Many\nvariant models have been proposed, but there is limited understanding on both\nhow to compare different architectures and how to construct GNNs\nsystematically. Here, we propose a hierarchy of GNNs based on their aggregation\nregions. We derive theoretical results about the discriminative power and\nfeature representation capabilities of each class. Then, we show how this\nframework can be utilized to systematically construct arbitrarily powerful\nGNNs. As an example, we construct a simple architecture that exceeds the\nexpressiveness of the Weisfeiler-Lehman graph isomorphism test. We empirically\nvalidate our theory on both synthetic and real-world benchmarks, and\ndemonstrate our example's theoretical power translates to strong results on\nnode classification, graph classification, and graph regression tasks.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 02:22:54 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Li", "Michael Lingzhi", ""], ["Dong", "Meng", ""], ["Zhou", "Jiawei", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1911.05266", "submitter": "Dipan Pal", "authors": "Dipan K. Pal, Akshay Chawla, Marios Savvides", "title": "Learning Non-Parametric Invariances from Data with Permanent Random\n  Connectomes", "comments": "Preprint (accepted at NeurIPS SVRHM 2019 Workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the fundamental problems in supervised classification and in machine\nlearning in general, is the modelling of non-parametric invariances that exist\nin data. Most prior art has focused on enforcing priors in the form of\ninvariances to parametric nuisance transformations that are expected to be\npresent in data. Learning non-parametric invariances directly from data remains\nan important open problem. In this paper, we introduce a new architectural\nlayer for convolutional networks which is capable of learning general\ninvariances from data itself. This layer can learn invariance to non-parametric\ntransformations and interestingly, motivates and incorporates permanent random\nconnectomes, thereby being called Permanent Random Connectome Non-Parametric\nTransformation Networks (PRC-NPTN). PRC-NPTN networks are initialized with\nrandom connections (not just weights) which are a small subset of the\nconnections in a fully connected convolution layer. Importantly, these\nconnections in PRC-NPTNs once initialized remain permanent throughout training\nand testing. Permanent random connectomes make these architectures loosely more\nbiologically plausible than many other mainstream network architectures which\nrequire highly ordered structures. We motivate randomly initialized connections\nas a simple method to learn invariance from data itself while invoking\ninvariance towards multiple nuisance transformations simultaneously. We find\nthat these randomly initialized permanent connections have positive effects on\ngeneralization, outperform much larger ConvNet baselines and the recently\nproposed Non-Parametric Transformation Network (NPTN) on benchmarks that\nenforce learning invariances from the data itself.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 03:03:48 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 15:32:41 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2020 00:54:18 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Pal", "Dipan K.", ""], ["Chawla", "Akshay", ""], ["Savvides", "Marios", ""]]}, {"id": "1911.05275", "submitter": "Gaurav Menghani", "authors": "Gaurav Menghani, Sujith Ravi", "title": "Learning from a Teacher using Unlabeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation is a widely used technique for model compression. We\nposit that the teacher model used in a distillation setup, captures\nrelationships between classes, that extend beyond the original dataset. We\nempirically show that a teacher model can transfer this knowledge to a student\nmodel even on an {\\it out-of-distribution} dataset. Using this approach, we\nshow promising results on MNIST, CIFAR-10, and Caltech-256 datasets using\nunlabeled image data from different sources. Our results are encouraging and\nhelp shed further light from the perspective of understanding knowledge\ndistillation and utilizing unlabeled data to improve model quality.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 03:43:29 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Menghani", "Gaurav", ""], ["Ravi", "Sujith", ""]]}, {"id": "1911.05277", "submitter": "Xu Wang Dr.", "authors": "Xu Wang, Jingming He, Lin Ma", "title": "Exploiting Local and Global Structure for Point Cloud Semantic\n  Segmentation with Contextual Point Representations", "comments": "Accepted by NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose one novel model for point cloud semantic\nsegmentation, which exploits both the local and global structures within the\npoint cloud based on the contextual point representations. Specifically, we\nenrich each point representation by performing one novel gated fusion on the\npoint itself and its contextual points. Afterwards, based on the enriched\nrepresentation, we propose one novel graph pointnet module, relying on the\ngraph attention block to dynamically compose and update each point\nrepresentation within the local point cloud structure. Finally, we resort to\nthe spatial-wise and channel-wise attention strategies to exploit the point\ncloud global structure and thereby yield the resulting semantic label for each\npoint. Extensive results on the public point cloud databases, namely the S3DIS\nand ScanNet datasets, demonstrate the effectiveness of our proposed model,\noutperforming the state-of-the-art approaches. Our code for this paper is\navailable at https://github.com/fly519/ELGS.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 03:51:32 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Wang", "Xu", ""], ["He", "Jingming", ""], ["Ma", "Lin", ""]]}, {"id": "1911.05327", "submitter": "Hanlin Mo", "authors": "Hanlin Mo and Hua Li", "title": "Rotation Differential Invariants of Images Generated by Two Fundamental\n  Differential Operators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we design two fundamental differential operators for the\nderivation of rotation differential invariants of images. Each differential\ninvariant obtained by using the new method can be expressed as a homogeneous\npolynomial of image partial derivatives, which preserve their values when the\nimage is rotated by arbitrary angles. We produce all possible instances of\nhomogeneous invariants up to the given order and degree, and discuss the\nindependence of them in detail. As far as we know, no previous papers have\npublished so many explicit forms of high-order rotation differential invariants\nof images. In the experimental part, texture classification and image patch\nverification are carried out on popular real databases. These rotation\ndifferential invariants are used as image feature vector. We mainly evaluate\nthe effects of various factors on the performance of them. The experimental\nresults also validate that they have better performance than some commonly used\nimage features in some cases.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 07:10:46 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 11:12:18 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Mo", "Hanlin", ""], ["Li", "Hua", ""]]}, {"id": "1911.05329", "submitter": "Junjie Liu", "authors": "Junjie Liu, Dongchao Wen, Hongxing Gao, Wei Tao, Tse-Wei Chen, Kinya\n  Osa, Masami Kato", "title": "Knowledge Representing: Efficient, Sparse Representation of Prior\n  Knowledge for Knowledge Distillation", "comments": null, "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR 2019)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite the recent works on knowledge distillation (KD) have achieved a\nfurther improvement through elaborately modeling the decision boundary as the\nposterior knowledge, their performance is still dependent on the hypothesis\nthat the target network has a powerful capacity (representation ability). In\nthis paper, we propose a knowledge representing (KR) framework mainly focusing\non modeling the parameters distribution as prior knowledge. Firstly, we suggest\na knowledge aggregation scheme in order to answer how to represent the prior\nknowledge from teacher network. Through aggregating the parameters distribution\nfrom teacher network into more abstract level, the scheme is able to alleviate\nthe phenomenon of residual accumulation in the deeper layers. Secondly, as the\ncritical issue of what the most important prior knowledge is for better\ndistilling, we design a sparse recoding penalty for constraining the student\nnetwork to learn with the penalized gradients. With the proposed penalty, the\nstudent network can effectively avoid the over-regularization during knowledge\ndistilling and converge faster. The quantitative experiments exhibit that the\nproposed framework achieves the state-ofthe-arts performance, even though the\ntarget network does not have the expected capacity. Moreover, the framework is\nflexible enough for combining with other KD methods based on the posterior\nknowledge.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 07:14:25 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Liu", "Junjie", ""], ["Wen", "Dongchao", ""], ["Gao", "Hongxing", ""], ["Tao", "Wei", ""], ["Chen", "Tse-Wei", ""], ["Osa", "Kinya", ""], ["Kato", "Masami", ""]]}, {"id": "1911.05341", "submitter": "Junjie Liu", "authors": "Hongxing Gao, Wei Tao, Dongchao Wen, Junjie Liu, Tse-Wei Chen, Kinya\n  Osa, Masami Kato", "title": "DupNet: Towards Very Tiny Quantized CNN with Improved Accuracy for Face\n  Detection", "comments": null, "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR 2019) Workshops", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deploying deep learning based face detectors on edge devices is a challenging\ntask due to the limited computation resources. Even though binarizing the\nweights of a very tiny network gives impressive compactness on model size (e.g.\n240.9 KB for IFQ-Tinier-YOLO), it is not tiny enough to fit in the embedded\ndevices with strict memory constraints. In this paper, we propose DupNet which\nconsists of two parts. Firstly, we employ weights with duplicated channels for\nthe weight-intensive layers to reduce the model size. Secondly, for the\nquantization-sensitive layers whose quantization causes notable accuracy drop,\nwe duplicate its input feature maps. It allows us to use more weights channels\nfor convolving more representative outputs. Based on that, we propose a very\ntiny face detector, DupNet-Tinier-YOLO, which is 6.5X times smaller on model\nsize and 42.0% less complex on computation and meanwhile achieves 2.4% higher\ndetection than IFQ-Tinier-YOLO. Comparing with the full precision Tiny-YOLO,\nour DupNet-Tinier-YOLO gives 1,694.2X and 389.9X times savings on model size\nand computation complexity respectively with only 4.0% drop on detection rate\n(0.880 vs. 0.920). Moreover, our DupNet-Tinier-YOLO is only 36.9 KB, which is\nthe tiniest deep face detector to our best knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 08:00:26 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Gao", "Hongxing", ""], ["Tao", "Wei", ""], ["Wen", "Dongchao", ""], ["Liu", "Junjie", ""], ["Chen", "Tse-Wei", ""], ["Osa", "Kinya", ""], ["Kato", "Masami", ""]]}, {"id": "1911.05351", "submitter": "Ruben Tolosana", "authors": "Jo\\~ao C. Neves, Ruben Tolosana, Ruben Vera-Rodriguez, Vasco Lopes,\n  Hugo Proen\\c{c}a and Julian Fierrez", "title": "GANprintR: Improved Fakes and Evaluation of the State of the Art in Face\n  Manipulation Detection", "comments": null, "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of large-scale facial databases, together with the\nremarkable progresses of deep learning technologies, in particular Generative\nAdversarial Networks (GANs), have led to the generation of extremely realistic\nfake facial content, raising obvious concerns about the potential for misuse.\nSuch concerns have fostered the research on manipulation detection methods\nthat, contrary to humans, have already achieved astonishing results in various\nscenarios. In this study, we focus on the synthesis of entire facial images,\nwhich is a specific type of facial manipulation. The main contributions of this\nstudy are four-fold: i) a novel strategy to remove GAN \"fingerprints\" from\nsynthetic fake images based on autoencoders is described, in order to spoof\nfacial manipulation detection systems while keeping the visual quality of the\nresulting images; ii) an in-depth analysis of the recent literature in facial\nmanipulation detection; iii) a complete experimental assessment of this type of\nfacial manipulation, considering the state-of-the-art fake detection systems\n(based on holistic deep networks, steganalysis, and local artifacts), remarking\nhow challenging is this task in unconstrained scenarios; and finally iv) we\nannounce a novel public database, named iFakeFaceDB, yielding from the\napplication of our proposed GAN-fingerprint Removal approach (GANprintR) to\nalready very realistic synthetic fake images.\n  The results obtained in our empirical evaluation show that additional efforts\nare required to develop robust facial manipulation detection systems against\nunseen conditions and spoof techniques, such as the one proposed in this study.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 08:48:55 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 08:55:21 GMT"}, {"version": "v3", "created": "Mon, 6 Apr 2020 08:54:30 GMT"}, {"version": "v4", "created": "Wed, 1 Jul 2020 13:51:33 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Neves", "Jo\u00e3o C.", ""], ["Tolosana", "Ruben", ""], ["Vera-Rodriguez", "Ruben", ""], ["Lopes", "Vasco", ""], ["Proen\u00e7a", "Hugo", ""], ["Fierrez", "Julian", ""]]}, {"id": "1911.05358", "submitter": "Songxuan Lai", "authors": "Songxuan Lai, Lianwen Jin, Luojun Lin, Yecheng Zhu, Huiyun Mao", "title": "SynSig2Vec: Learning Representations from Synthetic Dynamic Signatures\n  for Real-world Verification", "comments": "To appear in AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An open research problem in automatic signature verification is the skilled\nforgery attacks. However, the skilled forgeries are very difficult to acquire\nfor representation learning. To tackle this issue, this paper proposes to learn\ndynamic signature representations through ranking synthesized signatures.\nFirst, a neuromotor inspired signature synthesis method is proposed to\nsynthesize signatures with different distortion levels for any template\nsignature. Then, given the templates, we construct a lightweight\none-dimensional convolutional network to learn to rank the synthesized samples,\nand directly optimize the average precision of the ranking to exploit relative\nand fine-grained signature similarities. Finally, after training, fixed-length\nrepresentations can be extracted from dynamic signatures of variable lengths\nfor verification. One highlight of our method is that it requires neither\nskilled nor random forgeries for training, yet it surpasses the\nstate-of-the-art by a large margin on two public benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 08:58:19 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 13:04:51 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Lai", "Songxuan", ""], ["Jin", "Lianwen", ""], ["Lin", "Luojun", ""], ["Zhu", "Yecheng", ""], ["Mao", "Huiyun", ""]]}, {"id": "1911.05371", "submitter": "Yuki Asano", "authors": "Yuki Markus Asano, Christian Rupprecht, Andrea Vedaldi", "title": "Self-labelling via simultaneous clustering and representation learning", "comments": "Accepted paper at the International Conference on Learning\n  Representations (ICLR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining clustering and representation learning is one of the most promising\napproaches for unsupervised learning of deep neural networks. However, doing so\nnaively leads to ill posed learning problems with degenerate solutions. In this\npaper, we propose a novel and principled learning formulation that addresses\nthese issues. The method is obtained by maximizing the information between\nlabels and input data indices. We show that this criterion extends standard\ncrossentropy minimization to an optimal transport problem, which we solve\nefficiently for millions of input images and thousands of labels using a fast\nvariant of the Sinkhorn-Knopp algorithm. The resulting method is able to\nself-label visual data so as to train highly competitive image representations\nwithout manual labels. Our method achieves state of the art representation\nlearning performance for AlexNet and ResNet-50 on SVHN, CIFAR-10, CIFAR-100 and\nImageNet and yields the first self-supervised AlexNet that outperforms the\nsupervised Pascal VOC detection baseline. Code and models are available.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 09:47:49 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 13:22:38 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 18:03:39 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Asano", "Yuki Markus", ""], ["Rupprecht", "Christian", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1911.05377", "submitter": "Xinjing Cheng", "authors": "Xinjing Cheng, Peng Wang, Chenye Guan and Ruigang Yang", "title": "CSPN++: Learning Context and Resource Aware Convolutional Spatial\n  Propagation Networks for Depth Completion", "comments": "Camera Ready Version. Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth Completion deals with the problem of converting a sparse depth map to a\ndense one, given the corresponding color image. Convolutional spatial\npropagation network (CSPN) is one of the state-of-the-art (SoTA) methods of\ndepth completion, which recovers structural details of the scene. In this\npaper, we propose CSPN++, which further improves its effectiveness and\nefficiency by learning adaptive convolutional kernel sizes and the number of\niterations for the propagation, thus the context and computational resources\nneeded at each pixel could be dynamically assigned upon requests. Specifically,\nwe formulate the learning of the two hyper-parameters as an architecture\nselection problem where various configurations of kernel sizes and numbers of\niterations are first defined, and then a set of soft weighting parameters are\ntrained to either properly assemble or select from the pre-defined\nconfigurations at each pixel. In our experiments, we find weighted assembling\ncan lead to significant accuracy improvements, which we referred to as\n\"context-aware CSPN\", while weighted selection, \"resource-aware CSPN\" can\nreduce the computational resource significantly with similar or better\naccuracy. Besides, the resource needed for CSPN++ can be adjusted w.r.t. the\ncomputational budget automatically. Finally, to avoid the side effects of noise\nor inaccurate sparse depths, we embed a gated network inside CSPN++, which\nfurther improves the performance. We demonstrate the effectiveness of CSPN++on\nthe KITTI depth completion benchmark, where it significantly improves over CSPN\nand other SoTA methods.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 10:04:05 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 03:44:45 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Cheng", "Xinjing", ""], ["Wang", "Peng", ""], ["Guan", "Chenye", ""], ["Yang", "Ruigang", ""]]}, {"id": "1911.05439", "submitter": "Megumi Nakao", "authors": "Megumi Nakao, Mitsuhiro Nakamura, Takashi Mizowaki, Tetsuya Matsuda", "title": "Statistical Deformation Reconstruction Using Multi-organ Shape Features\n  for Pancreatic Cancer Localization", "comments": null, "journal-ref": "Medical Image Analysis, Vol. 67, 101829, 2021", "doi": "10.1016/j.media.2020.101829", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Respiratory motion and the associated deformations of abdominal organs and\ntumors are essential information in clinical applications. However, inter- and\nintra-patient multi-organ deformations are complex and have not been\nstatistically formulated, whereas single organ deformations have been widely\nstudied. In this paper, we introduce a multi-organ deformation library and its\napplication to deformation reconstruction based on the shape features of\nmultiple abdominal organs. Statistical multi-organ motion/deformation models of\nthe stomach, liver, left and right kidneys, and duodenum were generated by\nshape matching their region labels defined on four-dimensional computed\ntomography images. A total of 250 volumes were measured from 25 pancreatic\ncancer patients. This paper also proposes a per-region-based deformation\nlearning using the reproducing kernel to predict the displacement of pancreatic\ncancer for adaptive radiotherapy. The experimental results show that the\nproposed concept estimates deformations better than general per-patient-based\nlearning models and achieves a clinically acceptable estimation error with a\nmean distance of 1.2 $\\pm$ 0.7 mm and a Hausdorff distance of 4.2 $\\pm$ 2.3 mm\nthroughout the respiratory motion.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 13:10:10 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Nakao", "Megumi", ""], ["Nakamura", "Mitsuhiro", ""], ["Mizowaki", "Takashi", ""], ["Matsuda", "Tetsuya", ""]]}, {"id": "1911.05449", "submitter": "Liqi Yan", "authors": "Liqi Yan, Mingjian Zhu, Changbin Yu", "title": "Crowd Video Captioning", "comments": null, "journal-ref": "IECON 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Describing a video automatically with natural language is a challenging task\nin the area of computer vision. In most cases, the on-site situation of great\nevents is reported in news, but the situation of the off-site spectators in the\nentrance and exit is neglected which also arouses people's interest. Since the\ndeployment of reporters in the entrance and exit costs lots of manpower, how to\nautomatically describe the behavior of a crowd of off-site spectators is\nsignificant and remains a problem. To tackle this problem, we propose a new\ntask called crowd video captioning (CVC) which aims to describe the crowd of\nspectators. We also provide baseline methods for this task and evaluate them on\nthe dataset WorldExpo'10. Our experimental results show that captioning models\nhave a fairly deep understanding of the crowd in video and perform\nsatisfactorily in the CVC task.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 13:38:17 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Yan", "Liqi", ""], ["Zhu", "Mingjian", ""], ["Yu", "Changbin", ""]]}, {"id": "1911.05530", "submitter": "Mikhail Belyaev", "authors": "Artem Pimkin and Alexander Samoylenko and Natalia Antipina and Anna\n  Ovechkina and Andrey Golanov and Alexandra Dalechina and Mikhail Belyaev", "title": "Multi-domain CT Metal Artifacts Reduction Using Partial Convolution\n  Based Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent CT Metal Artifacts Reduction (MAR) methods are often based on\nimage-to-image convolutional neural networks for adjustment of corrupted\nsinograms or images themselves. In this paper, we are exploring the\ncapabilities of a multi-domain method which consists of both sinogram\ncorrection (projection domain step) and restored image correction (image-domain\nstep). Moreover, we propose a formulation of the first step problem as sinogram\ninpainting which allows us to use methods of this specific field such as\npartial convolutions. The proposed method allows to achieve state-of-the-art\n(-75% MSE) improvement in comparison with a classic benchmark - Li-MAR.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 15:08:33 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 09:43:01 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Pimkin", "Artem", ""], ["Samoylenko", "Alexander", ""], ["Antipina", "Natalia", ""], ["Ovechkina", "Anna", ""], ["Golanov", "Andrey", ""], ["Dalechina", "Alexandra", ""], ["Belyaev", "Mikhail", ""]]}, {"id": "1911.05541", "submitter": "Icaro Oliveira", "authors": "Icaro O. de Oliveira, Rayson Laroca, David Menotti, Keiko V. O.\n  Fonseca and Rodrigo Minetto", "title": "Vehicle-Rear: A New Dataset to Explore Feature Fusion for Vehicle\n  Identification Using Convolutional Neural Networks", "comments": null, "journal-ref": "IEEE Access, vol. 9, pp. 101065-101077, 2021", "doi": "10.1109/ACCESS.2021.3097964", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This work addresses the problem of vehicle identification through\nnon-overlapping cameras. As our main contribution, we introduce a novel dataset\nfor vehicle identification, called Vehicle-Rear, that contains more than three\nhours of high-resolution videos, with accurate information about the make,\nmodel, color and year of nearly 3,000 vehicles, in addition to the position and\nidentification of their license plates. To explore our dataset we design a\ntwo-stream CNN that simultaneously uses two of the most distinctive and\npersistent features available: the vehicle's appearance and its license plate.\nThis is an attempt to tackle a major problem: false alarms caused by vehicles\nwith similar designs or by very close license plate identifiers. In the first\nnetwork stream, shape similarities are identified by a Siamese CNN that uses a\npair of low-resolution vehicle patches recorded by two different cameras. In\nthe second stream, we use a CNN for OCR to extract textual information,\nconfidence scores, and string similarities from a pair of high-resolution\nlicense plate patches. Then, features from both streams are merged by a\nsequence of fully connected layers for decision. In our experiments, we\ncompared the two-stream network against several well-known CNN architectures\nusing single or multiple vehicle features. The architectures, trained models,\nand dataset are publicly available at https://github.com/icarofua/vehicle-rear.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 15:23:04 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 11:43:28 GMT"}, {"version": "v3", "created": "Sun, 25 Jul 2021 11:39:59 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["de Oliveira", "Icaro O.", ""], ["Laroca", "Rayson", ""], ["Menotti", "David", ""], ["Fonseca", "Keiko V. O.", ""], ["Minetto", "Rodrigo", ""]]}, {"id": "1911.05542", "submitter": "Zhongliang Yang", "authors": "Zhongliang Yang, Ke Wang, Sai Ma, Yongfeng Huang, Xiangui Kang,\n  Xianfeng Zhao", "title": "IStego100K: Large-scale Image Steganalysis Dataset", "comments": "Accepted by IWDW2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to promote the rapid development of image steganalysis technology,\nin this paper, we construct and release a multivariable large-scale image\nsteganalysis dataset called IStego100K. It contains 208,104 images with the\nsame size of 1024*1024. Among them, 200,000 images (100,000 cover-stego image\npairs) are divided as the training set and the remaining 8,104 as testing set.\nIn addition, we hope that IStego100K can help researchers further explore the\ndevelopment of universal image steganalysis algorithms, so we try to reduce\nlimits on the images in IStego100K. For each image in IStego100K, the quality\nfactors is randomly set in the range of 75-95, the steganographic algorithm is\nrandomly selected from three well-known steganographic algorithms, which are\nJ-uniward, nsF5 and UERD, and the embedding rate is also randomly set to be a\nvalue of 0.1-0.4. In addition, considering the possible mismatch between\ntraining samples and test samples in real environment, we add a test set\n(DS-Test) whose source of samples are different from the training set. We hope\nthat this test set can help to evaluate the robustness of steganalysis\nalgorithms. We tested the performance of some latest steganalysis algorithms on\nIStego100K, with specific results and analysis details in the experimental\npart. We hope that the IStego100K dataset will further promote the development\nof universal image steganalysis technology. The description of IStego100K and\ninstructions for use can be found at https://github.com/YangzlTHU/IStego100K\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 15:25:45 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Yang", "Zhongliang", ""], ["Wang", "Ke", ""], ["Ma", "Sai", ""], ["Huang", "Yongfeng", ""], ["Kang", "Xiangui", ""], ["Zhao", "Xianfeng", ""]]}, {"id": "1911.05546", "submitter": "Daniela Mihai", "authors": "Daniela Mihai and Jonathon Hare", "title": "Avoiding hashing and encouraging visual semantics in referential\n  emergent language games", "comments": "4 pages, presented at Emergent Communication: Towards Natural\n  Language workshop (NeurIPS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been an increasing interest in the area of emergent communication\nbetween agents which learn to play referential signalling games with realistic\nimages. In this work, we consider the signalling game setting of Havrylov and\nTitov and investigate the effect of the feature extractor's weights and of the\ntask being solved on the visual semantics learned or captured by the models. We\nimpose various augmentation to the input images and additional tasks in the\ngame with the aim to induce visual representations which capture conceptual\nproperties of images. Through our set of experiments, we demonstrate that\ncommunication systems which capture visual semantics can be learned in a\ncompletely self-supervised manner by playing the right types of game.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 15:31:48 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Mihai", "Daniela", ""], ["Hare", "Jonathon", ""]]}, {"id": "1911.05548", "submitter": "Joris Roels", "authors": "Joris Roels, Yvan Saeys", "title": "Cost-efficient segmentation of electron microscopy images using active\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade, electron microscopy has improved up to a point that\ngenerating high quality gigavoxel sized datasets only requires a few hours.\nAutomated image analysis, particularly image segmentation, however, has not\nevolved at the same pace. Even though state-of-the-art methods such as U-Net\nand DeepLab have improved segmentation performance substantially, the required\namount of labels remains too expensive. Active learning is the subfield in\nmachine learning that aims to mitigate this burden by selecting the samples\nthat require labeling in a smart way. Many techniques have been proposed,\nparticularly for image classification, to increase the steepness of learning\ncurves. In this work, we extend these techniques to deep CNN based image\nsegmentation. Our experiments on three different electron microscopy datasets\nshow that active learning can improve segmentation quality by 10 to 15% in\nterms of Jaccard score compared to standard randomized sampling.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 15:35:38 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Roels", "Joris", ""], ["Saeys", "Yvan", ""]]}, {"id": "1911.05567", "submitter": "Aakash Kaku", "authors": "Aakash Kaku, Chaitra V. Hegde, Jeffrey Huang, Sohae Chung, Xiuyuan\n  Wang, Matthew Young, Alireza Radmanesh, Yvonne W. Lui, and Narges Razavian", "title": "DARTS: DenseUnet-based Automatic Rapid Tool for brain Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative, volumetric analysis of Magnetic Resonance Imaging (MRI) is a\nfundamental way researchers study the brain in a host of neurological\nconditions including normal maturation and aging. Despite the availability of\nopen-source brain segmentation software, widespread clinical adoption of\nvolumetric analysis has been hindered due to processing times and reliance on\nmanual corrections. Here, we extend the use of deep learning models from\nproof-of-concept, as previously reported, to present a comprehensive\nsegmentation of cortical and deep gray matter brain structures matching the\nstandard regions of aseg+aparc included in the commonly used open-source tool,\nFreesurfer. The work presented here provides a real-life, rapid deep\nlearning-based brain segmentation tool to enable clinical translation as well\nas research application of quantitative brain segmentation. The advantages of\nthe presented tool include short (~1 minute) processing time and improved\nsegmentation quality. This is the first study to perform quick and accurate\nsegmentation of 102 brain regions based on the surface-based protocol (DMK\nprotocol), widely used by experts in the field. This is also the first work to\ninclude an expert reader study to assess the quality of the segmentation\nobtained using a deep-learning-based model. We show the superior performance of\nour deep-learning-based models over the traditional segmentation tool,\nFreesurfer. We refer to the proposed deep learning-based tool as DARTS\n(DenseUnet-based Automatic Rapid Tool for brain Segmentation). Our tool and\ntrained models are available at https://github.com/NYUMedML/DARTS\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 15:57:26 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 13:37:03 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Kaku", "Aakash", ""], ["Hegde", "Chaitra V.", ""], ["Huang", "Jeffrey", ""], ["Chung", "Sohae", ""], ["Wang", "Xiuyuan", ""], ["Young", "Matthew", ""], ["Radmanesh", "Alireza", ""], ["Lui", "Yvonne W.", ""], ["Razavian", "Narges", ""]]}, {"id": "1911.05586", "submitter": "Quanshi Zhang", "authors": "Li Chen, Hailun Ding, Qi Li, Zhuo Li, Jian Peng, Haifeng Li", "title": "Understanding the Importance of Single Directions via Representative\n  Substitution", "comments": "In AAAI-19 Workshop on Network Interpretability for Deep Learning.\n  Published version of arXiv:1811.11053", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the internal representations of deep neural networks (DNNs) is\ncrucal to explain their behavior. The interpretation of individual units, which\nare neurons in MLPs or convolution kernels in convolutional networks, has been\npaid much attention given their fundamental role. However, recent research\n(Morcos et al. 2018) presented a counterintuitive phenomenon, which suggests\nthat an individual unit with high class selectivity, called interpretable\nunits, has poor contributions to generalization of DNNs. In this work, we\nprovide a new perspective to understand this counterintuitive phenomenon, which\nmakes sense when we introduce Representative Substitution (RS). Instead of\nindividually selective units with classes, the RS refers to the independence of\na unit's representations in the same layer without any annotation. Our\nexperiments demonstrate that interpretable units have high RS which are not\ncritical to network's generalization. The RS provides new insights into the\ninterpretation of DNNs and suggests that we need to focus on the independence\nand relationship of the representations.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jan 2019 18:49:17 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Chen", "Li", ""], ["Ding", "Hailun", ""], ["Li", "Qi", ""], ["Li", "Zhuo", ""], ["Peng", "Jian", ""], ["Li", "Haifeng", ""]]}, {"id": "1911.05588", "submitter": "Quanshi Zhang", "authors": "A. Deliege, A. Cioppa and M. Van Droogenbroeck", "title": "An Effective Hit-or-Miss Layer Favoring Feature Interpretation as\n  Learned Prototypes Deformations", "comments": "In AAAI-19 Workshop on Network Interpretability for Deep Learning.\n  Published version of arXiv:1806.06519", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks designed for the task of classification have become a\ncommodity in recent years. Many works target the development of more effective\nnetworks, which results in a complexification of their architectures with more\nlayers, multiple sub-networks, or even the combination of multiple classifiers,\nbut this often comes at the expense of producing uninterpretable black boxes.\nIn this paper, we redesign a simple capsule network to enable it to synthesize\nclass-representative samples, called prototypes, by replacing the last layer\nwith a novel Hit-or-Miss layer. This layer contains activated vectors, called\ncapsules, that we train to hit or miss a fixed target capsule by tailoring a\nspecific centripetal loss function. This possibility allows to develop a data\naugmentation step combining information from the data space and the feature\nspace, resulting in a hybrid data augmentation process. We show that our\nnetwork, named HitNet, is able to reach better performances than those\nreproduced with the initial CapsNet on several datasets, while allowing to\nvisualize the nature of the features extracted as deformations of the\nprototypes, which provides a direct insight into the feature representation\nlearned by the network .\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2019 01:28:27 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Deliege", "A.", ""], ["Cioppa", "A.", ""], ["Van Droogenbroeck", "M.", ""]]}, {"id": "1911.05603", "submitter": "Xuesong Shi", "authors": "Xuesong Shi, Dongjiang Li, Pengpeng Zhao, Qinbin Tian, Yuxin Tian,\n  Qiwei Long, Chunhao Zhu, Jingwei Song, Fei Qiao, Le Song, Yangquan Guo,\n  Zhigang Wang, Yimin Zhang, Baoxing Qin, Wei Yang, Fangshi Wang, Rosa H. M.\n  Chan, Qi She", "title": "Are We Ready for Service Robots? The OpenLORIS-Scene Datasets for\n  Lifelong SLAM", "comments": "To be published on ICRA 2020; 7 pages, 3 figures; v2 fixed a number\n  in Table III", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Service robots should be able to operate autonomously in dynamic and daily\nchanging environments over an extended period of time. While Simultaneous\nLocalization And Mapping (SLAM) is one of the most fundamental problems for\nrobotic autonomy, most existing SLAM works are evaluated with data sequences\nthat are recorded in a short period of time. In real-world deployment, there\ncan be out-of-sight scene changes caused by both natural factors and human\nactivities. For example, in home scenarios, most objects may be movable,\nreplaceable or deformable, and the visual features of the same place may be\nsignificantly different in some successive days. Such out-of-sight dynamics\npose great challenges to the robustness of pose estimation, and hence a robot's\nlong-term deployment and operation. To differentiate the forementioned problem\nfrom the conventional works which are usually evaluated in a static setting in\na single run, the term \\textit{lifelong SLAM} is used here to address SLAM\nproblems in an ever-changing environment over a long period of time. To\naccelerate lifelong SLAM research, we release the OpenLORIS-Scene datasets. The\ndata are collected in real-world indoor scenes, for multiple times in each\nplace to include scene changes in real life. We also design benchmarking\nmetrics for lifelong SLAM, with which the robustness and accuracy of pose\nestimation are evaluated separately. The datasets and benchmark are available\nonline at https://lifelong-robotic-vision.github.io/dataset/scene.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 16:43:16 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 03:39:16 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Shi", "Xuesong", ""], ["Li", "Dongjiang", ""], ["Zhao", "Pengpeng", ""], ["Tian", "Qinbin", ""], ["Tian", "Yuxin", ""], ["Long", "Qiwei", ""], ["Zhu", "Chunhao", ""], ["Song", "Jingwei", ""], ["Qiao", "Fei", ""], ["Song", "Le", ""], ["Guo", "Yangquan", ""], ["Wang", "Zhigang", ""], ["Zhang", "Yimin", ""], ["Qin", "Baoxing", ""], ["Yang", "Wei", ""], ["Wang", "Fangshi", ""], ["Chan", "Rosa H. M.", ""], ["She", "Qi", ""]]}, {"id": "1911.05609", "submitter": "Sicheng Zhao", "authors": "Sicheng Zhao, Shangfei Wang, Mohammad Soleymani, Dhiraj Joshi, Qiang\n  Ji", "title": "Affective Computing for Large-Scale Heterogeneous Multimedia Data: A\n  Survey", "comments": "Accepted by ACM TOMM", "journal-ref": null, "doi": "10.1145/3363560", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide popularity of digital photography and social networks has generated\na rapidly growing volume of multimedia data (i.e., image, music, and video),\nresulting in a great demand for managing, retrieving, and understanding these\ndata. Affective computing (AC) of these data can help to understand human\nbehaviors and enable wide applications. In this article, we survey the\nstate-of-the-art AC technologies comprehensively for large-scale heterogeneous\nmultimedia data. We begin this survey by introducing the typical emotion\nrepresentation models from psychology that are widely employed in AC. We\nbriefly describe the available datasets for evaluating AC algorithms. We then\nsummarize and compare the representative methods on AC of different multimedia\ntypes, i.e., images, music, videos, and multimodal data, with the focus on both\nhandcrafted features-based methods and deep learning methods. Finally, we\ndiscuss some challenges and future directions for multimedia affective\ncomputing.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 21:22:47 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Zhao", "Sicheng", ""], ["Wang", "Shangfei", ""], ["Soleymani", "Mohammad", ""], ["Joshi", "Dhiraj", ""], ["Ji", "Qiang", ""]]}, {"id": "1911.05611", "submitter": "Junjiao Tian", "authors": "Junjiao Tian, Wesley Cheung, Nathan Glaser, Yen-Cheng Liu, Zsolt Kira", "title": "UNO: Uncertainty-aware Noisy-Or Multimodal Fusion for Unanticipated\n  Input Degradation", "comments": "IEEE International Conference on Robotics and Automation (ICRA),\n  2020. IROS Workshop on the Importance of Uncertainty in Deep Learning for\n  Robotics, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fusion of multiple sensor modalities, especially through deep learning\narchitectures, has been an active area of study. However, an under-explored\naspect of such work is whether the methods can be robust to degradations across\ntheir input modalities, especially when they must generalize to degradations\nnot seen during training. In this work, we propose an uncertainty-aware fusion\nscheme to effectively fuse inputs that might suffer from a range of known and\nunknown degradations. Specifically, we analyze a number of uncertainty\nmeasures, each of which captures a different aspect of uncertainty, and we\npropose a novel way to fuse degraded inputs by scaling modality-specific output\nsoftmax probabilities. We additionally propose a novel data-dependent spatial\ntemperature scaling method to complement these existing uncertainty measures.\nFinally, we integrate the uncertainty-scaled output from each modality using a\nprobabilistic noisy-or fusion method. In a photo-realistic simulation\nenvironment (AirSim), we show that our method achieves significantly better\nresults on a semantic segmentation task, compared to state-of-art fusion\narchitectures, on a range of degradations (e.g. fog, snow, frost, and various\nother types of noise), some of which are unknown during training. We\nspecifically improve upon the state-of-art[1] by 28% in mean IoU on various\ndegradations. [1] Abhinav Valada, Rohit Mohan, and Wolfram Burgard.\nSelf-Supervised Model Adaptation for Multimodal Semantic Segmentation. In:\narXiv e-prints, arXiv:1808.03833 (Aug. 2018), arXiv:1808.03833. arXiv:\n1808.03833 [cs.CV].\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 09:42:04 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 03:39:54 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Tian", "Junjiao", ""], ["Cheung", "Wesley", ""], ["Glaser", "Nathan", ""], ["Liu", "Yen-Cheng", ""], ["Kira", "Zsolt", ""]]}, {"id": "1911.05625", "submitter": "Umit Kacar", "authors": "Cihan Akin, Umit Kacar, and Murvet Kirci", "title": "Twins Recognition Using Hierarchical Score Level Fusion", "comments": "4 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the development of technology, the usage areas and importance of\nbiometric systems have started to increase. Since the characteristics of each\nperson are different from each other, a single model biometric system can yield\nsuccessful results. However, because the characteristics of twin people are\nvery close to each other, multiple biometric systems including multiple\ncharacteristics of individuals will be more appropriate and will increase the\nrecognition rate. In this study, a multiple biometric recognition system\nconsisting of a combination of multiple algorithms and multiple models was\ndeveloped to distinguish people from other people and their twins. Ear and\nvoice biometric data were used for the multimodal model and 38 pair of twin ear\nimages and sound recordings were used in the data set. Sound and ear\nrecognition rates were obtained using classical (hand-crafted) and deep\nlearning algorithms. The results obtained were combined with the hierarchical\nscore level fusion method to achieve a success rate of 94.74% in rank-1 and\n100% in rank -2.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 11:33:16 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Akin", "Cihan", ""], ["Kacar", "Umit", ""], ["Kirci", "Murvet", ""]]}, {"id": "1911.05626", "submitter": "Meixin Zhu", "authors": "Meixin Zhu, Jingyun Hu, Ziyuan Pu, Zhiyong Cui, Liangwu Yan, Yinhai\n  Wang", "title": "Traffic Sign Detection and Recognition for Autonomous Driving in Virtual\n  Simulation Environment", "comments": "8 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study developed a traffic sign detection and recognition algorithm based\non the RetinaNet. Two main aspects were revised to improve the detection of\ntraffic signs: image cropping to address the issue of large image and small\ntraffic signs; and using more anchors with various scales to detect traffic\nsigns with different sizes and shapes. The proposed algorithm was trained and\ntested in a series of autonomous driving front-view images in a virtual\nsimulation environment. Results show that the algorithm performed extremely\nwell under good illumination and weather conditions. Its drawbacks are that it\nsometimes failed to detect object under bad weather conditions like snow and\nfailed to distinguish speed limits signs with different limit values.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 03:38:10 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Zhu", "Meixin", ""], ["Hu", "Jingyun", ""], ["Pu", "Ziyuan", ""], ["Cui", "Zhiyong", ""], ["Yan", "Liangwu", ""], ["Wang", "Yinhai", ""]]}, {"id": "1911.05627", "submitter": "Prashnna Gyawali", "authors": "Prashnna K Gyawali, Rudra Saha, Linwei Wang, VSR Veeravasarapu and\n  Maneesh Singh", "title": "Wavelets to the Rescue: Improving Sample Quality of Latent Variable Deep\n  Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Autoencoders (VAE) are probabilistic deep generative models\nunderpinned by elegant theory, stable training processes, and meaningful\nmanifold representations. However, they produce blurry images due to a lack of\nexplicit emphasis over high-frequency textural details of the images, and the\ndifficulty to directly model the complex joint probability distribution over\nthe high-dimensional image space. In this work, we approach these two\nchallenges with a novel wavelet space VAE that uses the decoder to model the\nimages in the wavelet coefficient space. This enables the VAE to emphasize over\nhigh-frequency components within an image obtained via wavelet decomposition.\nAdditionally, by decomposing the complex function of generating\nhigh-dimensional images into inverse wavelet transformation and generation of\nwavelet coefficients, the latter becomes simpler to model by the VAE. We\nempirically validate that deep generative models operating in the wavelet space\ncan generate images of higher quality than the image (RGB) space counterparts.\nQuantitatively, on benchmark natural image datasets, we achieve consistently\nbetter FID scores than VAE based architectures and competitive FID scores with\na variety of GAN models for the same architectural and experimental setup.\nFurthermore, the proposed wavelet-based generative model retains desirable\nattributes like disentangled and informative latent representation without\nlosing the quality in the generated samples.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 15:16:05 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Gyawali", "Prashnna K", ""], ["Saha", "Rudra", ""], ["Wang", "Linwei", ""], ["Veeravasarapu", "VSR", ""], ["Singh", "Maneesh", ""]]}, {"id": "1911.05628", "submitter": "Adam Kashlak", "authors": "Milad Kiaee, Adam B Kashlak, Jisu Kim, Giseon Heo", "title": "Diagnosis of Pediatric Obstructive Sleep Apnea via Face Classification\n  with Persistent Homology and Convolutional Neural Networks", "comments": "23 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obstructive sleep apnea is a serious condition causing a litany of health\nproblems especially in the pediatric population. However, this chronic\ncondition can be treated if diagnosis is possible. The gold standard for\ndiagnosis is an overnight sleep study, which is often unobtainable by many\npotentially suffering from this condition. Hence, we attempt to develop a fast\nnon-invasive diagnostic tool by training a classifier on 2D and 3D facial\nimages of a patient to recognize facial features associated with obstructive\nsleep apnea. In this comparative study, we consider both persistent homology\nand geometric shape analysis from the field of computational topology as well\nas convolutional neural networks, a powerful method from deep learning whose\nsuccess in image and specifically facial recognition has already been\ndemonstrated by computer scientists.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 03:43:44 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Kiaee", "Milad", ""], ["Kashlak", "Adam B", ""], ["Kim", "Jisu", ""], ["Heo", "Giseon", ""]]}, {"id": "1911.05629", "submitter": "Dennis N\\'u\\~nez Fern\\'andez", "authors": "Dennis N\\'u\\~nez-Fern\\'andez, Franklin Porras-Barrientos, Macarena\n  Vittet-Mondo\\~nedo, Robert H. Gilman, Mirko Zimic", "title": "Prediction of gaze direction using Convolutional Neural Networks for\n  Autism diagnosis", "comments": "LatinX in AI Research at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autism is a developmental disorder that affects social interaction and\ncommunication of children. The gold standard diagnostic tools are very\ndifficult to use and time consuming. However, diagnostic could be deduced from\nchild gaze preferences by looking a video with social and abstract scenes. In\nthis work, we propose an algorithm based on convolutional neural networks to\npredict gaze direction for a fast and effective autism diagnosis. Early results\nshow that our algorithm achieves real-time response and robust high accuracy\nfor prediction of gaze direction.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 15:06:56 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["N\u00fa\u00f1ez-Fern\u00e1ndez", "Dennis", ""], ["Porras-Barrientos", "Franklin", ""], ["Vittet-Mondo\u00f1edo", "Macarena", ""], ["Gilman", "Robert H.", ""], ["Zimic", "Mirko", ""]]}, {"id": "1911.05630", "submitter": "Lucas C. Uzal", "authors": "Marcos Pividori and Guillermo L. Grinblat and Lucas C. Uzal", "title": "Exploiting GAN Internal Capacity for High-Quality Reconstruction of\n  Natural Images", "comments": "This preprint is the result of the work done for the undergraduate\n  dissertation of M. Pividori supervised by L.C. Uzal and G.L. Grinblat, and\n  presented in July 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GAN) have demonstrated impressive results in\nmodeling the distribution of natural images, learning latent representations\nthat capture semantic variations in an unsupervised basis. Beyond the\ngeneration of novel samples, it is of special interest to exploit the ability\nof the GAN generator to model the natural image manifold and hence generate\ncredible changes when manipulating images. However, this line of work is\nconditioned by the quality of the reconstruction. Until now, only inversion to\nthe latent space has been considered, we propose to exploit the representation\nin intermediate layers of the generator, and we show that this leads to\nincreased capacity. In particular, we observe that the representation after the\nfirst dense layer, present in all state-of-the-art GAN models, is expressive\nenough to represent natural images with high visual fidelity. It is possible to\ninterpolate around these images obtaining a sequence of new plausible synthetic\nimages that cannot be generated from the latent space. Finally, as an example\nof potential applications that arise from this inversion mechanism, we show\npreliminary results in exploiting the learned representation in the attention\nmap of the generator to obtain an unsupervised segmentation of natural images.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 22:07:24 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Pividori", "Marcos", ""], ["Grinblat", "Guillermo L.", ""], ["Uzal", "Lucas C.", ""]]}, {"id": "1911.05649", "submitter": "Xin Zhang", "authors": "Songbin Xu, Yang Xue, Xin Zhang, Lianwen Jin", "title": "Air-Writing Translater: A Novel Unsupervised Domain Adaptation Method\n  for Inertia-Trajectory Translation of In-air Handwriting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a new way of human-computer interaction, inertial sensor based in-air\nhandwriting can provide a natural and unconstrained interaction to express more\ncomplex and richer information in 3D space. However, most of the existing\nin-air handwriting work is mainly focused on handwritten character recognition,\nwhich makes these work suffer from poor readability of inertial signal and lack\nof labeled samples. To address these two problems, we use unsupervised domain\nadaptation method to reconstruct the trajectory of inertial signal and generate\ninertial samples using online handwritten trajectories. In this paper, we\npropose an AirWriting Translater model to learn the bi-directional translation\nbetween trajectory domain and inertial domain in the absence of paired inertial\nand trajectory samples. Through semantic-level adversarial training and latent\nclassification loss, the proposed model learns to extract domain-invariant\ncontent between inertial signal and trajectory, while preserving semantic\nconsistency during the translation across the two domains. We carefully design\nthe architecture, so that the proposed framework can accept inputs of arbitrary\nlength and translate between different sampling rates. We also conduct\nexperiments on two public datasets: 6DMG (in-air handwriting dataset) and CT\n(handwritten trajectory dataset), the results on the two datasets demonstrate\nthat the proposed network successes in both Inertia-to Trajectory and\nTrajectory-to-Inertia translation tasks.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 14:09:44 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Xu", "Songbin", ""], ["Xue", "Yang", ""], ["Zhang", "Xin", ""], ["Jin", "Lianwen", ""]]}, {"id": "1911.05650", "submitter": "Samuel Remedios", "authors": "Samuel W. Remedios, Zihao Wu, Camilo Bermudez, Cailey I. Kerley,\n  Snehashis Roy, Mayur B. Patel, John A. Butman, Bennett A. Landman, Dzung L.\n  Pham", "title": "Extracting 2D weak labels from volume labels using multiple instance\n  learning in CT hemorrhage detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple instance learning (MIL) is a supervised learning methodology that\naims to allow models to learn instance class labels from bag class labels,\nwhere a bag is defined to contain multiple instances. MIL is gaining traction\nfor learning from weak labels but has not been widely applied to 3D medical\nimaging. MIL is well-suited to clinical CT acquisitions since (1) the highly\nanisotropic voxels hinder application of traditional 3D networks and (2)\npatch-based networks have limited ability to learn whole volume labels. In this\nwork, we apply MIL with a deep convolutional neural network to identify whether\nclinical CT head image volumes possess one or more large hemorrhages (>\n20cm$^3$), resulting in a learned 2D model without the need for 2D slice\nannotations. Individual image volumes are considered separate bags, and the\nslices in each volume are instances. Such a framework sets the stage for\nincorporating information obtained in clinical reports to help train a 2D\nsegmentation approach. Within this context, we evaluate the data requirements\nto enable generalization of MIL by varying the amount of training data. Our\nresults show that a training size of at least 400 patient image volumes was\nneeded to achieve accurate per-slice hemorrhage detection. Over a five-fold\ncross-validation, the leading model, which made use of the maximum number of\ntraining volumes, had an average true positive rate of 98.10%, an average true\nnegative rate of 99.36%, and an average precision of 0.9698. The models have\nbeen made available along with source code to enabled continued exploration and\nadaption of MIL in CT neuroimaging.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 17:24:21 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Remedios", "Samuel W.", ""], ["Wu", "Zihao", ""], ["Bermudez", "Camilo", ""], ["Kerley", "Cailey I.", ""], ["Roy", "Snehashis", ""], ["Patel", "Mayur B.", ""], ["Butman", "John A.", ""], ["Landman", "Bennett A.", ""], ["Pham", "Dzung L.", ""]]}, {"id": "1911.05722", "submitter": "Kaiming He", "authors": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick", "title": "Momentum Contrast for Unsupervised Visual Representation Learning", "comments": "CVPR 2020 camera-ready. Code:\n  https://github.com/facebookresearch/moco", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Momentum Contrast (MoCo) for unsupervised visual representation\nlearning. From a perspective on contrastive learning as dictionary look-up, we\nbuild a dynamic dictionary with a queue and a moving-averaged encoder. This\nenables building a large and consistent dictionary on-the-fly that facilitates\ncontrastive unsupervised learning. MoCo provides competitive results under the\ncommon linear protocol on ImageNet classification. More importantly, the\nrepresentations learned by MoCo transfer well to downstream tasks. MoCo can\noutperform its supervised pre-training counterpart in 7 detection/segmentation\ntasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large\nmargins. This suggests that the gap between unsupervised and supervised\nrepresentation learning has been largely closed in many vision tasks.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 18:53:26 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 17:01:12 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2020 18:36:55 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["He", "Kaiming", ""], ["Fan", "Haoqi", ""], ["Wu", "Yuxin", ""], ["Xie", "Saining", ""], ["Girshick", "Ross", ""]]}, {"id": "1911.05787", "submitter": "Xingxing Zuo", "authors": "Xingxing Zuo, Mingming Zhang, Yiming Chen, Yong Liu, Guoquan Huang,\n  Mingyang Li", "title": "Visual-Inertial Localization for Skid-Steering Robots with Kinematic\n  Constraints", "comments": "16 pages, 5 figures, Published", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While visual localization or SLAM has witnessed great progress in past\ndecades, when deploying it on a mobile robot in practice, few works have\nexplicitly considered the kinematic (or dynamic) constraints of the real\nrobotic system when designing state estimators. To promote the practical\ndeployment of current state-of-the-art visual-inertial localization algorithms,\nin this work we propose a low-cost kinematics-constrained localization system\nparticularly for a skid-steering mobile robot. In particular, we derive in a\nprinciple way the robot's kinematic constraints based on the instantaneous\ncenters of rotation (ICR) model and integrate them in a tightly-coupled manner\ninto the sliding-window bundle adjustment (BA)-based visual-inertial estimator.\nBecause the ICR model parameters are time-varying due to, for example,\ntrack-to-terrain interaction and terrain roughness, we estimate these kinematic\nparameters online along with the navigation state. To this end, we perform\nin-depth the observability analysis and identify motion conditions under which\nthe state/parameter estimation is viable. The proposed kinematics-constrained\nvisual-inertial localization system has been validated extensively in different\nterrain scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 20:00:51 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Zuo", "Xingxing", ""], ["Zhang", "Mingming", ""], ["Chen", "Yiming", ""], ["Liu", "Yong", ""], ["Huang", "Guoquan", ""], ["Li", "Mingyang", ""]]}, {"id": "1911.05821", "submitter": "Kyle Johnston", "authors": "K. B. Johnston, S.M. Caballero-Nieves, V. Petit, A.M. Peter and R.\n  Haber", "title": "Variable Star Classification Using Multi-View Metric Learning", "comments": "16 pages, 11 figures", "journal-ref": null, "doi": "10.1093/mnras/stz3165", "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our multi-view metric learning framework enables robust characterization of\nstar categories by directly learning to discriminate in a multi-faceted feature\nspace, thus, eliminating the need to combine feature representations prior to\nfitting the machine learning model. We also demonstrate how to extend standard\nmulti-view learning, which employs multiple vectorized views, to the\nmatrix-variate case which allows very novel variable star signature\nrepresentations. The performance of our proposed methods is evaluated on the\nUCR Starlight and LINEAR datasets. Both the vector and matrix-variate versions\nof our multi-view learning framework perform favorably --- demonstrating the\nability to discriminate variable star categories.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 21:37:53 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Johnston", "K. B.", ""], ["Caballero-Nieves", "S. M.", ""], ["Petit", "V.", ""], ["Peter", "A. M.", ""], ["Haber", "R.", ""]]}, {"id": "1911.05845", "submitter": "Christopher Sandino", "authors": "Christopher M. Sandino, Peng Lai, Shreyas S. Vasanawala, Joseph Y.\n  Cheng", "title": "Accelerating cardiac cine MRI using a deep learning-based ESPIRiT\n  reconstruction", "comments": "29 pages, 9 figures, 1 table, 7 supplementary videos, Submitted to\n  Magnetic Resonance in Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel neural network architecture, known as DL-ESPIRiT, is proposed to\nreconstruct rapidly acquired cardiac MRI data without field-of-view limitations\nwhich are present in previously proposed deep learning-based reconstruction\nframeworks. Additionally, a novel convolutional neural network based on\nseparable 3D convolutions is integrated into DL-ESPIRiT to more efficiently\nlearn spatiotemporal priors for dynamic image reconstruction. The network is\ntrained on fully-sampled 2D cardiac cine datasets collected from eleven healthy\nvolunteers with IRB approval. DL-ESPIRiT is compared against a state-of-the-art\nparallel imaging and compressed sensing method known as $l_1$-ESPIRiT. The\nreconstruction accuracy of both methods is evaluated on retrospectively\nundersampled datasets (R=12) with respect to standard image quality metrics as\nwell as automatic deep learning-based segmentations of left ventricular\nvolumes. Feasibility of this approach is demonstrated in reconstructions of\nprospectively undersampled data which were acquired in a single heartbeat per\nslice.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 22:41:25 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 16:50:00 GMT"}, {"version": "v3", "created": "Mon, 18 May 2020 22:44:50 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Sandino", "Christopher M.", ""], ["Lai", "Peng", ""], ["Vasanawala", "Shreyas S.", ""], ["Cheng", "Joseph Y.", ""]]}, {"id": "1911.05856", "submitter": "Shunwang Gong", "authors": "Shunwang Gong, Lei Chen, Michael Bronstein, Stefanos Zafeiriou", "title": "SpiralNet++: A Fast and Highly Efficient Mesh Convolution Operator", "comments": "The IEEE International Conference on Computer Vision (ICCV)\n  Workshops, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intrinsic graph convolution operators with differentiable kernel functions\nplay a crucial role in analyzing 3D shape meshes. In this paper, we present a\nfast and efficient intrinsic mesh convolution operator that does not rely on\nthe intricate design of kernel function. We explicitly formulate the order of\naggregating neighboring vertices, instead of learning weights between nodes,\nand then a fully connected layer follows to fuse local geometric structure\ninformation with vertex features. We provide extensive evidence showing that\nmodels based on this convolution operator are easier to train, and can\nefficiently learn invariant shape features. Specifically, we evaluate our\nmethod on three different types of tasks of dense shape correspondence, 3D\nfacial expression classification, and 3D shape reconstruction, and show that it\nsignificantly outperforms state-of-the-art approaches while being significantly\nfaster, without relying on shape descriptors. Our source code is available on\nGitHub.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 22:59:19 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Gong", "Shunwang", ""], ["Chen", "Lei", ""], ["Bronstein", "Michael", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1911.05864", "submitter": "De-An Huang", "authors": "De-An Huang, Yu-Wei Chao, Chris Paxton, Xinke Deng, Li Fei-Fei, Juan\n  Carlos Niebles, Animesh Garg, Dieter Fox", "title": "Motion Reasoning for Goal-Based Imitation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address goal-based imitation learning, where the aim is to output the\nsymbolic goal from a third-person video demonstration. This enables the robot\nto plan for execution and reproduce the same goal in a completely different\nenvironment. The key challenge is that the goal of a video demonstration is\noften ambiguous at the level of semantic actions. The human demonstrators might\nunintentionally achieve certain subgoals in the demonstrations with their\nactions. Our main contribution is to propose a motion reasoning framework that\ncombines task and motion planning to disambiguate the true intention of the\ndemonstrator in the video demonstration. This allows us to robustly recognize\nthe goals that cannot be disambiguated by previous action-based approaches. We\nevaluate our approach by collecting a dataset of 96 video demonstrations in a\nmockup kitchen environment. We show that our motion reasoning plays an\nimportant role in recognizing the actual goal of the demonstrator and improves\nthe success rate by over 20%. We further show that by using the automatically\ninferred goal from the video demonstration, our robot is able to reproduce the\nsame task in a real kitchen environment.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 23:59:44 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Huang", "De-An", ""], ["Chao", "Yu-Wei", ""], ["Paxton", "Chris", ""], ["Deng", "Xinke", ""], ["Fei-Fei", "Li", ""], ["Niebles", "Juan Carlos", ""], ["Garg", "Animesh", ""], ["Fox", "Dieter", ""]]}, {"id": "1911.05870", "submitter": "Kushagra Mahajan", "authors": "Kushagra Mahajan, Monika Sharma, Lovekesh Vig", "title": "Character Keypoint-based Homography Estimation in Scanned Documents for\n  Efficient Information Extraction", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise homography estimation between multiple images is a pre-requisite for\nmany computer vision applications. One application that is particularly\nrelevant in today's digital era is the alignment of scanned or camera-captured\ndocument images such as insurance claim forms for information extraction.\nTraditional learning based approaches perform poorly due to the absence of an\nappropriate gradient. Feature based keypoint extraction techniques for\nhomography estimation in real scene images either detect an extremely large\nnumber of inconsistent keypoints due to sharp textual edges, or produce\ninaccurate keypoint correspondences due to variations in illumination and\nviewpoint differences between document images. In this paper, we propose a\nnovel algorithm for aligning scanned or camera-captured document images using\ncharacter based keypoints and a reference template. The algorithm is both fast\nand accurate and utilizes a standard Optical character recognition (OCR) engine\nsuch as Tesseract to find character based unambiguous keypoints, which are\nutilized to identify precise keypoint correspondences between two images.\nFinally, the keypoints are used to compute the homography mapping between a\ntest document and a template. We evaluated the proposed approach for\ninformation extraction on two real world anonymized datasets comprised of\nhealth insurance claim forms and the results support the viability of the\nproposed technique.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 00:44:55 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Mahajan", "Kushagra", ""], ["Sharma", "Monika", ""], ["Vig", "Lovekesh", ""]]}, {"id": "1911.05871", "submitter": "Ali Ghofrani", "authors": "Ali Ghofrani, Rahil Mahdian Toroghi, Seyed Mojtaba Tabatabaie, Seyed\n  Maziar Tabasi", "title": "LiDAR ICPS-net: Indoor Camera Positioning based-on Generative\n  Adversarial Network for RGB to Point-Cloud Translation", "comments": "5 pages, 10 figures, ICASSP2020 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indoor positioning aims at navigation inside areas with no GPS-data\navailability and could be employed in many applications such as augmented\nreality, autonomous driving specially inside closed areas and tunnels. In this\npaper, a deep neural network-based architecture has been proposed to address\nthis problem. In this regard, a tandem set of convolutional neural networks, as\nwell as a Pix2Pix GAN network have been leveraged to perform as the scene\nclassifier, scene RGB image to point cloud converter, and position regressor,\nrespectively. The proposed architecture outperforms the previous works,\nincluding our recent work, in the sense that it makes data generation task\neasier and more robust against scene small variations, whilst the accuracy of\nthe positioning is remarkably well, for both Cartesian position and quaternion\ninformation of the camera.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 00:46:17 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Ghofrani", "Ali", ""], ["Toroghi", "Rahil Mahdian", ""], ["Tabatabaie", "Seyed Mojtaba", ""], ["Tabasi", "Seyed Maziar", ""]]}, {"id": "1911.05878", "submitter": "Zhengchun Liu", "authors": "Vibhatha Abeykoon, Zhengchun Liu, Rajkumar Kettimuthu, Geoffrey Fox,\n  Ian Foster", "title": "Scientific Image Restoration Anywhere", "comments": "6 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of deep learning models within scientific experimental facilities\nfrequently requires low-latency inference, so that, for example, quality\ncontrol operations can be performed while data are being collected. Edge\ncomputing devices can be useful in this context, as their low cost and compact\nform factor permit them to be co-located with the experimental apparatus. Can\nsuch devices, with their limited resources, can perform neural network\nfeed-forward computations efficiently and effectively? We explore this question\nby evaluating the performance and accuracy of a scientific image restoration\nmodel, for which both model input and output are images, on edge computing\ndevices. Specifically, we evaluate deployments of TomoGAN, an image-denoising\nmodel based on generative adversarial networks developed for low-dose x-ray\nimaging, on the Google Edge TPU and NVIDIA Jetson. We adapt TomoGAN for edge\nexecution, evaluate model inference performance, and propose methods to address\nthe accuracy drop caused by model quantization. We show that these edge\ncomputing devices can deliver accuracy comparable to that of a full-fledged CPU\nor GPU model, at speeds that are more than adequate for use in the intended\ndeployments, denoising a 1024 x 1024 image in less than a second. Our\nexperiments also show that the Edge TPU models can provide 3x faster inference\nresponse than a CPU-based model and 1.5x faster than an edge GPU-based model.\nThis combination of high speed and low cost permits image restoration anywhere.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 21:33:14 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Abeykoon", "Vibhatha", ""], ["Liu", "Zhengchun", ""], ["Kettimuthu", "Rajkumar", ""], ["Fox", "Geoffrey", ""], ["Foster", "Ian", ""]]}, {"id": "1911.05879", "submitter": "Gayathri Radhabai Gopinathan Nair", "authors": "Gayathri R G, Atul Sajjanhar, Yong Xiang", "title": "Image-Based Feature Representation for Insider Threat Classification", "comments": "8 pages, 5 figures", "journal-ref": "Applied Sciences, vol. 10, no. 14, p. 4945, 2020", "doi": "10.3390/app10144945", "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Insiders are the trusted entities in the organization, but poses threat to\nthe with access to sensitive information network and resources. The insider\nthreat detection is a well studied problem in security analytics. Identifying\nthe features from data sources and using them with the right data analytics\nalgorithms makes various kinds of threat analysis possible. The insider threat\nanalysis is mainly done using the frequency based attributes extracted from the\nraw data available from data sources. In this paper, we propose an image-based\nfeature representation of the daily resource usage pattern of users in the\norganization. The features extracted from the audit files of the organization\nare represented as gray scale images. Hence, these images are used to represent\nthe resource access patterns and thereby the behavior of users. Classification\nmodels are applied to the representative images to detect anomalous behavior of\ninsiders. The images are classified to malicious and non-malicious. The\neffectiveness of the proposed representation is evaluated using the CMU CERT\ndata V4.2, and state-of-art image classification models like Mobilenet, VGG and\nResNet. The experimental results showed improved accuracy. The comparison with\nexisting works show a performance improvement in terms of high recall and\nprecision values.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 03:00:55 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["G", "Gayathri R", ""], ["Sajjanhar", "Atul", ""], ["Xiang", "Yong", ""]]}, {"id": "1911.05880", "submitter": "Huidong Xie", "authors": "Huidong Xie, Hongming Shan, Ge Wang", "title": "Deep Encoder-decoder Adversarial Reconstruction (DEAR) Network for 3D CT\n  from Few-view Data", "comments": null, "journal-ref": "Bioengineering 2019, 6(4), 111", "doi": "10.3390/bioengineering6040111", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  X-ray computed tomography (CT) is widely used in clinical practice. The\ninvolved ionizing X-ray radiation, however, could increase cancer risk. Hence,\nthe reduction of the radiation dose has been an important topic in recent\nyears. Few-view CT image reconstruction is one of the main ways to minimize\nradiation dose and potentially allow a stationary CT architecture. In this\npaper, we propose a deep encoder-decoder adversarial reconstruction (DEAR)\nnetwork for 3D CT image reconstruction from few-view data. Since the artifacts\ncaused by few-view reconstruction appear in 3D instead of 2D geometry, a 3D\ndeep network has a great potential for improving the image quality in a\ndata-driven fashion. More specifically, our proposed DEAR-3D network aims at\nreconstructing 3D volume directly from clinical 3D spiral cone-beam image data.\nDEAR is validated on a publicly available abdominal CT dataset prepared and\nauthorized by Mayo Clinic. Compared with other 2D deep-learning methods, the\nproposed DEAR-3D network can utilize 3D information to produce promising\nreconstruction results.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 00:39:50 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 02:50:08 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Xie", "Huidong", ""], ["Shan", "Hongming", ""], ["Wang", "Ge", ""]]}, {"id": "1911.05913", "submitter": "Ming Cheng", "authors": "Ming Cheng, Kunjing Cai, Ming Li", "title": "RWF-2000: An Open Large Scale Video Database for Violence Detection", "comments": "Accepted by ICPR'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, surveillance cameras are widely deployed in public places,\nand the general crime rate has been reduced significantly due to these\nubiquitous devices. Usually, these cameras provide cues and evidence after\ncrimes are conducted, while they are rarely used to prevent or stop criminal\nactivities in time. It is both time and labor consuming to manually monitor a\nlarge amount of video data from surveillance cameras. Therefore, automatically\nrecognizing violent behaviors from video signals becomes essential. This paper\nsummarizes several existing video datasets for violence detection and proposes\nthe RWF-2000 database with 2,000 videos captured by surveillance cameras in\nreal-world scenes. Also, we present a new method that utilizes both the merits\nof 3D-CNNs and optical flow, namely Flow Gated Network. The proposed approach\nobtains an accuracy of 87.25% on the test set of our proposed database. The\ndatabase and source codes are currently open to access.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 02:59:09 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 09:15:41 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 05:37:15 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Cheng", "Ming", ""], ["Cai", "Kunjing", ""], ["Li", "Ming", ""]]}, {"id": "1911.05916", "submitter": "Ziang Yan", "authors": "Ziang Yan, Yiwen Guo, Changshui Zhang", "title": "Adversarial Margin Maximization Networks", "comments": "11 pages + 1 page appendix, accepted by T-PAMI", "journal-ref": null, "doi": "10.1109/TPAMI.2019.2948348", "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tremendous recent success of deep neural networks (DNNs) has sparked a\nsurge of interest in understanding their predictive ability. Unlike the human\nvisual system which is able to generalize robustly and learn with little\nsupervision, DNNs normally require a massive amount of data to learn new\nconcepts. In addition, research works also show that DNNs are vulnerable to\nadversarial examples-maliciously generated images which seem perceptually\nsimilar to the natural ones but are actually formed to fool learning models,\nwhich means the models have problem generalizing to unseen data with certain\ntype of distortions. In this paper, we analyze the generalization ability of\nDNNs comprehensively and attempt to improve it from a geometric point of view.\nWe propose adversarial margin maximization (AMM), a learning-based\nregularization which exploits an adversarial perturbation as a proxy. It\nencourages a large margin in the input space, just like the support vector\nmachines. With a differentiable formulation of the perturbation, we train the\nregularized DNNs simply through back-propagation in an end-to-end manner.\nExperimental results on various datasets (including MNIST, CIFAR-10/100, SVHN\nand ImageNet) and different DNN architectures demonstrate the superiority of\nour method over previous state-of-the-arts. Code and models for reproducing our\nresults will be made publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 03:13:17 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Yan", "Ziang", ""], ["Guo", "Yiwen", ""], ["Zhang", "Changshui", ""]]}, {"id": "1911.05920", "submitter": "Xiang Li", "authors": "Li Xiang, Chen Shuo, Xia Yan and Yang Jian", "title": "Understanding the Disharmony between Weight Normalization Family and\n  Weight Decay: $\\epsilon-$shifted $L_2$ Regularizer", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The merits of fast convergence and potentially better performance of the\nweight normalization family have drawn increasing attention in recent years.\nThese methods use standardization or normalization that changes the weight\n$\\boldsymbol{W}$ to $\\boldsymbol{W}'$, which makes $\\boldsymbol{W}'$\nindependent to the magnitude of $\\boldsymbol{W}$. Surprisingly,\n$\\boldsymbol{W}$ must be decayed during gradient descent, otherwise we will\nobserve a severe under-fitting problem, which is very counter-intuitive since\nweight decay is widely known to prevent deep networks from over-fitting. In\nthis paper, we \\emph{theoretically} prove that the weight decay term\n$\\frac{1}{2}\\lambda||{\\boldsymbol{W}}||^2$ merely modulates the effective\nlearning rate for improving objective optimization, and has no influence on\ngeneralization when the weight normalization family is compositely employed.\nFurthermore, we also expose several critical problems when introducing weight\ndecay term to weight normalization family, including the missing of global\nminimum and training instability. To address these problems, we propose an\n$\\epsilon-$shifted $L_2$ regularizer, which shifts the $L_2$ objective by a\npositive constant $\\epsilon$. Such a simple operation can theoretically\nguarantee the existence of global minimum, while preventing the network weights\nfrom being too small and thus avoiding gradient float overflow. It\nsignificantly improves the training stability and can achieve slightly better\nperformance in our practice. The effectiveness of $\\epsilon-$shifted $L_2$\nregularizer is comprehensively validated on the ImageNet, CIFAR-100, and COCO\ndatasets. Our codes and pretrained models will be released in\nhttps://github.com/implus/PytorchInsight.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 03:31:13 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Xiang", "Li", ""], ["Shuo", "Chen", ""], ["Yan", "Xia", ""], ["Jian", "Yang", ""]]}, {"id": "1911.05931", "submitter": "Leo Furkan Isikdogan", "authors": "Chyuan-Tyng Wu, Leo F. Isikdogan, Sushma Rao, Bhavin Nayak, Timo\n  Gerasimow, Aleksandar Sutic, Liron Ain-kedem, Gilad Michael", "title": "VisionISP: Repurposing the Image Signal Processor for Computer Vision\n  Applications", "comments": null, "journal-ref": "IEEE International Conference on Image Processing (ICIP), 2019,\n  pp. 4624-4628", "doi": "10.1109/ICIP.2019.8803607", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional image signal processors (ISPs) are primarily designed and\noptimized to improve the image quality perceived by humans. However, optimal\nperceptual image quality does not always translate into optimal performance for\ncomputer vision applications. We propose a set of methods, which we\ncollectively call VisionISP, to repurpose the ISP for machine consumption.\nVisionISP significantly reduces data transmission needs by reducing the\nbit-depth and resolution while preserving the relevant information. The blocks\nin VisionISP are simple, content-aware, and trainable. Experimental results\nshow that VisionISP boosts the performance of a subsequent computer vision\nsystem trained to detect objects in an autonomous driving setting. The results\ndemonstrate the potential and the practicality of VisionISP for computer vision\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 04:19:28 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Wu", "Chyuan-Tyng", ""], ["Isikdogan", "Leo F.", ""], ["Rao", "Sushma", ""], ["Nayak", "Bhavin", ""], ["Gerasimow", "Timo", ""], ["Sutic", "Aleksandar", ""], ["Ain-kedem", "Liron", ""], ["Michael", "Gilad", ""]]}, {"id": "1911.05932", "submitter": "Yuan Liu", "authors": "Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, Xiaowei Zhou", "title": "GIFT: Learning Transformation-Invariant Dense Visual Descriptors via\n  Group CNNs", "comments": "Accepted by NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding local correspondences between images with different viewpoints\nrequires local descriptors that are robust against geometric transformations.\nAn approach for transformation invariance is to integrate out the\ntransformations by pooling the features extracted from transformed versions of\nan image. However, the feature pooling may sacrifice the distinctiveness of the\nresulting descriptors. In this paper, we introduce a novel visual descriptor\nnamed Group Invariant Feature Transform (GIFT), which is both discriminative\nand robust to geometric transformations. The key idea is that the features\nextracted from the transformed versions of an image can be viewed as a function\ndefined on the group of the transformations. Instead of feature pooling, we use\ngroup convolutions to exploit underlying structures of the extracted features\non the group, resulting in descriptors that are both discriminative and\nprovably invariant to the group of transformations. Extensive experiments show\nthat GIFT outperforms state-of-the-art methods on several benchmark datasets\nand practically improves the performance of relative pose estimation.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 04:20:57 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Liu", "Yuan", ""], ["Shen", "Zehong", ""], ["Lin", "Zhixuan", ""], ["Peng", "Sida", ""], ["Bao", "Hujun", ""], ["Zhou", "Xiaowei", ""]]}, {"id": "1911.05939", "submitter": "Uehwan Kim", "authors": "Ue-Hwan Kim and Se-Ho Kim and Jong-Hwan Kim", "title": "SimVODIS: Simultaneous Visual Odometry, Object Detection, and Instance\n  Segmentation", "comments": "Submitted to TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent agents need to understand the surrounding environment to provide\nmeaningful services to or interact intelligently with humans. The agents should\nperceive geometric features as well as semantic entities inherent in the\nenvironment. Contemporary methods in general provide one type of information\nregarding the environment at a time, making it difficult to conduct high-level\ntasks. Moreover, running two types of methods and associating two resultant\ninformation requires a lot of computation and complicates the software\narchitecture. To overcome these limitations, we propose a neural architecture\nthat simultaneously performs both geometric and semantic tasks in a single\nthread: simultaneous visual odometry, object detection, and instance\nsegmentation (SimVODIS). Training SimVODIS requires unlabeled video sequences\nand the photometric consistency between input image frames generates\nself-supervision signals. The performance of SimVODIS outperforms or matches\nthe state-of-the-art performance in pose estimation, depth map prediction,\nobject detection, and instance segmentation tasks while completing all the\ntasks in a single thread. We expect SimVODIS would enhance the autonomy of\nintelligent agents and let the agents provide effective services to humans.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 05:03:47 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 10:39:06 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Kim", "Ue-Hwan", ""], ["Kim", "Se-Ho", ""], ["Kim", "Jong-Hwan", ""]]}, {"id": "1911.05942", "submitter": "Quan Chen", "authors": "Bo Wang, Quan Chen, Min Zhou, Zhiqiang Zhang, Xiaogang Jin, Kun Gai", "title": "Progressive Feature Polishing Network for Salient Object Detection", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature matters for salient object detection. Existing methods mainly focus\non designing a sophisticated structure to incorporate multi-level features and\nfilter out cluttered features. We present Progressive Feature Polishing Network\n(PFPN), a simple yet effective framework to progressively polish the\nmulti-level features to be more accurate and representative. By employing\nmultiple Feature Polishing Modules (FPMs) in a recurrent manner, our approach\nis able to detect salient objects with fine details without any\npost-processing. A FPM parallelly updates the features of each level by\ndirectly incorporating all higher level context information. Moreover, it can\nkeep the dimensions and hierarchical structures of the feature maps, which\nmakes it flexible to be integrated with any CNN-based models. Empirical\nexperiments show that our results are monotonically getting better with\nincreasing number of FPMs. Without bells and whistles, PFPN outperforms the\nstate-of-the-art methods significantly on five benchmark datasets under various\nevaluation metrics.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 05:22:12 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Wang", "Bo", ""], ["Chen", "Quan", ""], ["Zhou", "Min", ""], ["Zhang", "Zhiqiang", ""], ["Jin", "Xiaogang", ""], ["Gai", "Kun", ""]]}, {"id": "1911.05946", "submitter": "Alberto Fung", "authors": "Alberto Fung and Daniel McDuff", "title": "A Scalable Approach for Facial Action Unit Classifier Training\n  UsingNoisy Data for Pre-Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning systems are being used to automate many types of laborious\nlabeling tasks. Facial actioncoding is an example of such a labeling task that\nrequires copious amounts of time and a beyond average level of human domain\nexpertise. In recent years, the use of end-to-end deep neural networks has led\nto significant improvements in action unit recognition performance and many\nnetwork architectures have been proposed. Do the more complex deep neural\nnetwork(DNN) architectures perform sufficiently well to justify the additional\ncomplexity? We show that pre-training on a large diverse set of noisy data can\nresult in even a simple CNN model improving over the current state-of-the-art\nDNN architectures.The average F1-score achieved with our proposed method on the\nDISFA dataset is 0.60, compared to a previous state-of-the-art of 0.57.\nAdditionally, we show how the number of subjects and number of images used for\npre-training impacts the model performance. The approach that we have outlined\nis open-source, highly scalable, and not dependent on the model architecture.\nWe release the code and data: https://github.com/facialactionpretrain/facs.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 05:39:59 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Fung", "Alberto", ""], ["McDuff", "Daniel", ""]]}, {"id": "1911.05978", "submitter": "Pradyumna Narayana", "authors": "Pradyumna Narayana, Aniket Pednekar, Abishek Krishnamoorthy, Kazoo\n  Sone, Sugato Basu", "title": "HUSE: Hierarchical Universal Semantic Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a recent surge of interest in cross-modal representation learning\ncorresponding to images and text. The main challenge lies in mapping images and\ntext to a shared latent space where the embeddings corresponding to a similar\nsemantic concept lie closer to each other than the embeddings corresponding to\ndifferent semantic concepts, irrespective of the modality. Ranking losses are\ncommonly used to create such shared latent space -- however, they do not impose\nany constraints on inter-class relationships resulting in neighboring clusters\nto be completely unrelated. The works in the domain of visual semantic\nembeddings address this problem by first constructing a semantic embedding\nspace based on some external knowledge and projecting image embeddings onto\nthis fixed semantic embedding space. These works are confined only to image\ndomain and constraining the embeddings to a fixed space adds additional burden\non learning. This paper proposes a novel method, HUSE, to learn cross-modal\nrepresentation with semantic information. HUSE learns a shared latent space\nwhere the distance between any two universal embeddings is similar to the\ndistance between their corresponding class embeddings in the semantic embedding\nspace. HUSE also uses a classification objective with a shared classification\nlayer to make sure that the image and text embeddings are in the same shared\nlatent space. Experiments on UPMC Food-101 show our method outperforms previous\nstate-of-the-art on retrieval, hierarchical precision and classification\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 07:45:32 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Narayana", "Pradyumna", ""], ["Pednekar", "Aniket", ""], ["Krishnamoorthy", "Abishek", ""], ["Sone", "Kazoo", ""], ["Basu", "Sugato", ""]]}, {"id": "1911.06045", "submitter": "YueFeng Chen", "authors": "Da Chen, Yuefeng Chen, Yuhong Li, Feng Mao, Yuan He, Hui Xue", "title": "Self-Supervised Learning For Few-Shot Image Classification", "comments": "To appear at ICASSP 2021. https://github.com/phecy/ssl-few-shot", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot image classification aims to classify unseen classes with limited\nlabelled samples. Recent works benefit from the meta-learning process with\nepisodic tasks and can fast adapt to class from training to testing. Due to the\nlimited number of samples for each task, the initial embedding network for\nmeta-learning becomes an essential component and can largely affect the\nperformance in practice. To this end, most of the existing methods highly rely\non the efficient embedding network. Due to the limited labelled data, the scale\nof embedding network is constrained under a supervised learning(SL) manner\nwhich becomes a bottleneck of the few-shot learning methods. In this paper, we\nproposed to train a more generalized embedding network with self-supervised\nlearning (SSL) which can provide robust representation for downstream tasks by\nlearning from the data itself. We evaluate our work by extensive comparisons\nwith previous baseline methods on two few-shot classification datasets ({\\em\ni.e.,} MiniImageNet and CUB) and achieve better performance over baselines.\nTests on four datasets in cross-domain few-shot learning classification show\nthat the proposed method achieves state-of-the-art results and further prove\nthe robustness of the proposed model. Our code is available at\n\\hyperref[https://github.com/phecy/SSL-FEW-SHOT.]{https://github.com/phecy/SSL-FEW-SHOT.}\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 11:24:47 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 12:35:20 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2021 08:02:35 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Chen", "Da", ""], ["Chen", "Yuefeng", ""], ["Li", "Yuhong", ""], ["Mao", "Feng", ""], ["He", "Yuan", ""], ["Xue", "Hui", ""]]}, {"id": "1911.06047", "submitter": "Dipu Manandhar", "authors": "Dipu Manandhar, Muhammet Bastan and Kim-Hui Yap", "title": "Semantic Granularity Metric Learning for Visual Search", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep metric learning applied to various applications has shown promising\nresults in identification, retrieval and recognition. Existing methods often do\nnot consider different granularity in visual similarity. However, in many\ndomain applications, images exhibit similarity at multiple granularities with\nvisual semantic concepts, e.g. fashion demonstrates similarity ranging from\nclothing of the exact same instance to similar looks/design or a common\ncategory. Therefore, training image triplets/pairs used for metric learning\ninherently possess different degree of information. However, the existing\nmethods often treats them with equal importance during training. This hinders\ncapturing the underlying granularities in feature similarity required for\neffective visual search.\n  In view of this, we propose a new deep semantic granularity metric learning\n(SGML) that develops a novel idea of leveraging attribute semantic space to\ncapture different granularity of similarity, and then integrate this\ninformation into deep metric learning. The proposed method simultaneously\nlearns image attributes and embeddings using multitask CNNs. The two tasks are\nnot only jointly optimized but are further linked by the semantic granularity\nsimilarity mappings to leverage the correlations between the tasks. To this\nend, we propose a new soft-binomial deviance loss that effectively integrates\nthe degree of information in training samples, which helps to capture visual\nsimilarity at multiple granularities. Compared to recent ensemble-based\nmethods, our framework is conceptually elegant, computationally simple and\nprovides better performance. We perform extensive experiments on benchmark\nmetric learning datasets and demonstrate that our method outperforms recent\nstate-of-the-art methods, e.g., 1-4.5\\% improvement in Recall@1 over the\nprevious state-of-the-arts [1],[2] on DeepFashion In-Shop dataset.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 11:36:16 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Manandhar", "Dipu", ""], ["Bastan", "Muhammet", ""], ["Yap", "Kim-Hui", ""]]}, {"id": "1911.06073", "submitter": "George Plastiras", "authors": "George Plastiras, Christos Kyrkou, Theocharis Theocharides", "title": "Efficient ConvNet-based Object Detection for Unmanned Aerial Vehicles by\n  Selective Tile Processing", "comments": "George Plastiras, Christos Kyrkou, and Theocharis Theocharides. 2018.\n  Efficient ConvNet-based Object Detection for Unmanned Aerial Vehicles by\n  Selective Tile Processing. In Proceedings of the 12th International\n  Conference on Distributed Smart Cameras (ICDSC '18). ACM, New York, NY, USA,\n  Article 3, 6 pages", "journal-ref": "In Proceedings of the 12th International Conference on Distributed\n  Smart Cameras (ICDSC 2018)", "doi": "10.1145/3243394.3243692", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications utilizing Unmanned Aerial Vehicles (UAVs) require the use\nof computer vision algorithms to analyze the information captured from their\non-board camera. Recent advances in deep learning have made it possible to use\nsingle-shot Convolutional Neural Network (CNN) detection algorithms that\nprocess the input image to detect various objects of interest. To keep the\ncomputational demands low these neural networks typically operate on small\nimage sizes which, however, makes it difficult to detect small objects. This is\nfurther emphasized when considering UAVs equipped with cameras where due to the\nviewing range, objects tend to appear relatively small. This paper therefore,\nexplores the trade-offs involved when maintaining the resolution of the objects\nof interest by extracting smaller patches (tiles) from the larger input image\nand processing them using a neural network. Specifically, we introduce an\nattention mechanism to focus on detecting objects only in some of the tiles and\na memory mechanism to keep track of information for tiles that are not\nprocessed. Through the analysis of different methods and experiments we show\nthat by carefully selecting which tiles to process we can considerably improve\nthe detection accuracy while maintaining comparable performance to CNNs that\nresize and process a single image which makes the proposed approach suitable\nfor UAV applications.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 12:50:27 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Plastiras", "George", ""], ["Kyrkou", "Christos", ""], ["Theocharides", "Theocharis", ""]]}, {"id": "1911.06080", "submitter": "Yushuai Hu", "authors": "Yushuai Hu, Yaochu Jin, Runhua Li, Xiangxiang Zhang", "title": "CMSN: Continuous Multi-stage Network and Variable Margin Cosine Loss for\n  Temporal Action Proposal Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately locating the start and end time of an action in untrimmed videos\nis a challenging task. One of the important reasons is the boundary of action\nis not highly distinguishable, and the features around the boundary are\ndifficult to discriminate. To address this problem, we propose a novel\nframework for temporal action proposal generation, namely Continuous\nMulti-stage Network (CMSN), which divides a video that contains a complete\naction instance into six stages, namely Backgroud, Ready, Start, Confirm, End,\nFollow. To distinguish between Ready and Start, End and Follow more accurately,\nwe propose a novel loss function, Variable Margin Cosine Loss (VMCL), which\nallows for different margins between different categories. Our experiments on\nTHUMOS14 show that the proposed method for temporal proposal generation\nperforms better than the state-of-the-art methods using the same network\narchitecture and training dataset.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 13:08:30 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 12:02:02 GMT"}, {"version": "v3", "created": "Wed, 20 Nov 2019 02:22:45 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Hu", "Yushuai", ""], ["Jin", "Yaochu", ""], ["Li", "Runhua", ""], ["Zhang", "Xiangxiang", ""]]}, {"id": "1911.06084", "submitter": "Liang Xie", "authors": "Liang Xie, Chao Xiang, Zhengxu Yu, Guodong Xu, Zheng Yang, Deng Cai,\n  Xiaofei He", "title": "PI-RCNN: An Efficient Multi-sensor 3D Object Detector with Point-based\n  Attentive Cont-conv Fusion Module", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LIDAR point clouds and RGB-images are both extremely essential for 3D object\ndetection. So many state-of-the-art 3D detection algorithms dedicate in fusing\nthese two types of data effectively. However, their fusion methods based on\nBirds Eye View (BEV) or voxel format are not accurate. In this paper, we\npropose a novel fusion approach named Point-based Attentive Cont-conv\nFusion(PACF) module, which fuses multi-sensor features directly on 3D points.\nExcept for continuous convolution, we additionally add a Point-Pooling and an\nAttentive Aggregation to make the fused features more expressive. Moreover,\nbased on the PACF module, we propose a 3D multi-sensor multi-task network\ncalled Pointcloud-Image RCNN(PI-RCNN as brief), which handles the image\nsegmentation and 3D object detection tasks. PI-RCNN employs a segmentation\nsub-network to extract full-resolution semantic feature maps from images and\nthen fuses the multi-sensor features via powerful PACF module. Beneficial from\nthe effectiveness of the PACF module and the expressive semantic features from\nthe segmentation module, PI-RCNN can improve much in 3D object detection. We\ndemonstrate the effectiveness of the PACF module and PI-RCNN on the KITTI 3D\nDetection benchmark, and our method can achieve state-of-the-art on the metric\nof 3D AP.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 13:19:12 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 06:38:09 GMT"}, {"version": "v3", "created": "Mon, 2 Dec 2019 02:34:42 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Xie", "Liang", ""], ["Xiang", "Chao", ""], ["Yu", "Zhengxu", ""], ["Xu", "Guodong", ""], ["Yang", "Zheng", ""], ["Cai", "Deng", ""], ["He", "Xiaofei", ""]]}, {"id": "1911.06091", "submitter": "George Plastiras", "authors": "George Plastiras, Christos Kyrkou, Theocharis Theocharides", "title": "EdgeNet: Balancing Accuracy and Performance for Edge-based Convolutional\n  Neural Network Object Detectors", "comments": "George Plastiras, Christos Kyrkou, and Theocharis Theocharides. 2019.\n  EdgeNet: Balancing Accuracy and Performance for Edge-based Convolutional\n  Neural Network Object Detectors. In Proceedings of the 13th International\n  Conference on Distributed Smart Cameras (ICDSC 2019). ACM, New York, NY, USA,\n  Article 8, 6 pages", "journal-ref": "In Proceedings of the 13th International Conference on Distributed\n  Smart Cameras (ICDSC 2019)", "doi": "10.1145/3349801.3349809", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual intelligence at the edge is becoming a growing necessity for low\nlatency applications and situations where real-time decision is vital. Object\ndetection, the first step in visual data analytics, has enjoyed significant\nimprovements in terms of state-of-the-art accuracy due to the emergence of\nConvolutional Neural Networks (CNNs) and Deep Learning. However, such complex\nparadigms intrude increasing computational demands and hence prevent their\ndeployment on resource-constrained devices. In this work, we propose a\nhierarchical framework that enables to detect objects in high-resolution video\nframes, and maintain the accuracy of state-of-the-art CNN-based object\ndetectors while outperforming existing works in terms of processing speed when\ntargeting a low-power embedded processor using an intelligent data reduction\nmechanism. Moreover, a use-case for pedestrian detection from\nUnmanned-Areal-Vehicle (UAV) is presented showing the impact that the proposed\napproach has on sensitivity, average processing time and power consumption when\nis implemented on different platforms. Using the proposed selection process our\nframework manages to reduce the processed data by 100x leading to under 4W\npower consumption on different edge devices.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 13:49:23 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Plastiras", "George", ""], ["Kyrkou", "Christos", ""], ["Theocharides", "Theocharis", ""]]}, {"id": "1911.06095", "submitter": "Shiyang Cheng", "authors": "Shiyang Cheng, Pingchuan Ma, Georgios Tzimiropoulos, Stavros Petridis,\n  Adrian Bulat, Jie Shen, Maja Pantic", "title": "Towards Pose-invariant Lip-Reading", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lip-reading models have been significantly improved recently thanks to\npowerful deep learning architectures. However, most works focused on frontal or\nnear frontal views of the mouth. As a consequence, lip-reading performance\nseriously deteriorates in non-frontal mouth views. In this work, we present a\nframework for training pose-invariant lip-reading models on synthetic data\ninstead of collecting and annotating non-frontal data which is costly and\ntedious. The proposed model significantly outperforms previous approaches on\nnon-frontal views while retaining the superior performance on frontal and near\nfrontal mouth views. Specifically, we propose to use a 3D Morphable Model\n(3DMM) to augment LRW, an existing large-scale but mostly frontal dataset, by\ngenerating synthetic facial data in arbitrary poses. The newly derived dataset,\nis used to train a state-of-the-art neural network for lip-reading. We\nconducted a cross-database experiment for isolated word recognition on the LRS2\ndataset, and reported an absolute improvement of 2.55%. The benefit of the\nproposed approach becomes clearer in extreme poses where an absolute\nimprovement of up to 20.64% over the baseline is achieved.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 13:57:33 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Cheng", "Shiyang", ""], ["Ma", "Pingchuan", ""], ["Tzimiropoulos", "Georgios", ""], ["Petridis", "Stavros", ""], ["Bulat", "Adrian", ""], ["Shen", "Jie", ""], ["Pantic", "Maja", ""]]}, {"id": "1911.06102", "submitter": "Chaoyue Song", "authors": "Yugang Chen, Muchun Chen, Chaoyue Song, Bingbing Ni", "title": "CartoonRenderer: An Instance-based Multi-Style Cartoon Image Translator", "comments": "26th International Conference on Multimedia Modeling(MMM2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance based photo cartoonization is one of the challenging image\nstylization tasks which aim at transforming realistic photos into cartoon style\nimages while preserving the semantic contents of the photos. State-of-the-art\nDeep Neural Networks (DNNs) methods still fail to produce satisfactory results\nwith input photos in the wild, especially for photos which have high contrast\nand full of rich textures. This is due to that: cartoon style images tend to\nhave smooth color regions and emphasized edges which are contradict to\nrealistic photos which require clear semantic contents, i.e., textures, shapes\netc. Previous methods have difficulty in satisfying cartoon style textures and\npreserving semantic contents at the same time. In this work, we propose a novel\n\"CartoonRenderer\" framework which utilizing a single trained model to generate\nmultiple cartoon styles. In a nutshell, our method maps photo into a feature\nmodel and renders the feature model back into image space. In particular,\ncartoonization is achieved by conducting some transformation manipulation in\nthe feature space with our proposed Soft-AdaIN. Extensive experimental results\nshow our method produces higher quality cartoon style images than prior arts,\nwith accurate semantic content preservation. In addition, due to the decoupling\nof whole generating process into \"Modeling-Coordinating-Rendering\" parts, our\nmethod could easily process higher resolution photos, which is intractable for\nexisting methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 14:15:14 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Chen", "Yugang", ""], ["Chen", "Muchun", ""], ["Song", "Chaoyue", ""], ["Ni", "Bingbing", ""]]}, {"id": "1911.06181", "submitter": "Teppei Suzuki", "authors": "Teppei Suzuki and Ikuro Sato", "title": "Adversarial Transformations for Semi-Supervised Learning", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Regularization framework based on Adversarial Transformations\n(RAT) for semi-supervised learning. RAT is designed to enhance robustness of\nthe output distribution of class prediction for a given data against input\nperturbation. RAT is an extension of Virtual Adversarial Training (VAT) in such\na way that RAT adversarialy transforms data along the underlying data\ndistribution by a rich set of data transformation functions that leave class\nlabel invariant, whereas VAT simply produces adversarial additive noises. In\naddition, we verified that a technique of gradually increasing of perturbation\nregion further improve the robustness. In experiments, we show that RAT\nsignificantly improves classification performance on CIFAR-10 and SVHN compared\nto existing regularization methods under standard semi-supervised image\nclassification settings.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 08:01:47 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 06:53:12 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Suzuki", "Teppei", ""], ["Sato", "Ikuro", ""]]}, {"id": "1911.06185", "submitter": "Wei Zhang", "authors": "Lei Han, Juanzhen Sun, Wei Zhang", "title": "Convolutional Neural Network for Convective Storm Nowcasting Using 3D\n  Doppler Weather Radar Data", "comments": "This version of the paper has some fatal errors that need to be\n  carefully corrected", "journal-ref": null, "doi": "10.1109/TGRS.2019.2948070", "report-no": null, "categories": "physics.geo-ph cs.CV physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convective storms are one of the severe weather hazards found during the warm\nseason. Doppler weather radar is the only operational instrument that can\nfrequently sample the detailed structure of convective storm which has a small\nspatial scale and short lifetime. For the challenging task of short-term\nconvective storm forecasting, 3-D radar images contain information about the\nprocesses in convective storm. However, effectively extracting such information\nfrom multisource raw data has been problematic due to a lack of methodology and\ncomputation limitations. Recent advancements in deep learning techniques and\ngraphics processing units now make it possible. This article investigates the\nfeasibility and performance of an end-to-end deep learning nowcasting method.\nThe nowcasting problem was transformed into a classification problem first, and\nthen, a deep learning method that uses a convolutional neural network was\npresented to make predictions. On the first layer of CNN, a cross-channel 3D\nconvolution was proposed to fuse 3D raw data. The CNN method eliminates the\nhandcrafted feature engineering, i.e., the process of using domain knowledge of\nthe data to manually design features. Operationally produced historical data of\nthe Beijing-Tianjin-Hebei region in China was used to train the nowcasting\nsystem and evaluate its performance; 3737332 samples were collected in the\ntraining data set. The experimental results show that the deep learning method\nimproves nowcasting skills compared with traditional machine learning methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 15:42:12 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 08:05:16 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Han", "Lei", ""], ["Sun", "Juanzhen", ""], ["Zhang", "Wei", ""]]}, {"id": "1911.06188", "submitter": "Yinda Xu", "authors": "Yinda Xu, Zeyu Wang, Zuoxin Li, Ye Yuan, Gang Yu", "title": "SiamFC++: Towards Robust and Accurate Visual Tracking with Target\n  Estimation Guidelines", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual tracking problem demands to efficiently perform robust classification\nand accurate target state estimation over a given target at the same time.\nFormer methods have proposed various ways of target state estimation, yet few\nof them took the particularity of the visual tracking problem itself into\nconsideration. After a careful analysis, we propose a set of practical\nguidelines of target state estimation for high-performance generic object\ntracker design. Following these guidelines, we design our Fully Convolutional\nSiamese tracker++ (SiamFC++) by introducing both classification and target\nstate estimation branch(G1), classification score without ambiguity(G2),\ntracking without prior knowledge(G3), and estimation quality score(G4).\nExtensive analysis and ablation studies demonstrate the effectiveness of our\nproposed guidelines. Without bells and whistles, our SiamFC++ tracker achieves\nstate-of-the-art performance on five challenging benchmarks(OTB2015, VOT2018,\nLaSOT, GOT-10k, TrackingNet), which proves both the tracking and generalization\nability of the tracker. Particularly, on the large-scale TrackingNet dataset,\nSiamFC++ achieves a previously unseen AUC score of 75.4 while running at over\n90 FPS, which is far above the real-time requirement. Code and models are\navailable at: https://github.com/MegviiDetection/video_analyst .\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 15:43:37 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2019 14:21:36 GMT"}, {"version": "v3", "created": "Thu, 2 Jan 2020 12:11:20 GMT"}, {"version": "v4", "created": "Thu, 2 Apr 2020 14:01:38 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Xu", "Yinda", ""], ["Wang", "Zeyu", ""], ["Li", "Zuoxin", ""], ["Yuan", "Ye", ""], ["Yu", "Gang", ""]]}, {"id": "1911.06190", "submitter": "Kostas Loumponias", "authors": "Kostas Loumponias, Nicholas Vretos, George Tsaklidis and Petros Daras", "title": "An Improved Tobit Kalman Filter with Adaptive Censoring Limits", "comments": "21 pages, 32 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the Tobit Kalman filtering (TKF) process when the\nmeasurements are correlated and censored. The case of interval censoring, i.e.,\nthe case of measurements which belong to some interval with given censoring\nlimits, is considered. Two improvements of the standard TKF process are\nproposed, in order to estimate the hidden state vectors. Firstly, the exact\ncovariance matrix of the censored measurements is calculated by taking into\naccount the censoring limits. Secondly, the probability of a latent (normally\ndistributed) measurement to belong in or out of the uncensored region is\ncalculated by taking into account the Kalman residual. The designed algorithm\nis tested using both synthetic and real data sets. The real data set includes\nhuman skeleton joints' coordinates captured by the Microsoft Kinect II sensor.\nIn order to cope with certain real-life situations that cause problems in human\nskeleton tracking, such as (self)-occlusions, closely interacting persons etc.,\nadaptive censoring limits are used in the proposed TKF process. Experiments\nshow that the proposed method outperforms other filtering processes in\nminimizing the overall Root Mean Square Error (RMSE) for synthetic and real\ndata sets.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 15:45:06 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Loumponias", "Kostas", ""], ["Vretos", "Nicholas", ""], ["Tsaklidis", "George", ""], ["Daras", "Petros", ""]]}, {"id": "1911.06216", "submitter": "Jeremiah Johnson", "authors": "Jeremiah W. Johnson", "title": "Detecting Invasive Ductal Carcinoma with Semi-Supervised Conditional\n  GANs", "comments": "5 pages, 3 figures", "journal-ref": "Proceedings of the Future Technologies Conference (FTC) 2020, vol.\n  3, pp.113-120", "doi": "10.1007/978-3-030-63092-8", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Invasive ductal carcinoma (IDC) comprises nearly 80% of all breast cancers.\nThe detection of IDC is a necessary preprocessing step in determining the\naggressiveness of the cancer, determining treatment protocols, and predicting\npatient outcomes, and is usually performed manually by an expert pathologist.\nHere, we describe a novel algorithm for automatically detecting IDC using\nsemi-supervised conditional generative adversarial networks (cGANs). The\nframework is simple and effective at improving scores on a range of metrics\nover a baseline CNN.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 16:16:20 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 20:30:36 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Johnson", "Jeremiah W.", ""]]}, {"id": "1911.06258", "submitter": "Ronghang Hu", "authors": "Ronghang Hu, Amanpreet Singh, Trevor Darrell, Marcus Rohrbach", "title": "Iterative Answer Prediction with Pointer-Augmented Multimodal\n  Transformers for TextVQA", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many visual scenes contain text that carries crucial information, and it is\nthus essential to understand text in images for downstream reasoning tasks. For\nexample, a deep water label on a warning sign warns people about the danger in\nthe scene. Recent work has explored the TextVQA task that requires reading and\nunderstanding text in images to answer a question. However, existing approaches\nfor TextVQA are mostly based on custom pairwise fusion mechanisms between a\npair of two modalities and are restricted to a single prediction step by\ncasting TextVQA as a classification task. In this work, we propose a novel\nmodel for the TextVQA task based on a multimodal transformer architecture\naccompanied by a rich representation for text in images. Our model naturally\nfuses different modalities homogeneously by embedding them into a common\nsemantic space where self-attention is applied to model inter- and intra-\nmodality context. Furthermore, it enables iterative answer decoding with a\ndynamic pointer network, allowing the model to form an answer through\nmulti-step prediction instead of one-step classification. Our model outperforms\nexisting approaches on three benchmark datasets for the TextVQA task by a large\nmargin.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 17:32:10 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 07:41:07 GMT"}, {"version": "v3", "created": "Tue, 24 Mar 2020 23:59:59 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Hu", "Ronghang", ""], ["Singh", "Amanpreet", ""], ["Darrell", "Trevor", ""], ["Rohrbach", "Marcus", ""]]}, {"id": "1911.06278", "submitter": "Fabian Eitel", "authors": "Fabian Eitel, Jan Philipp Albrecht, Friedemann Paul, Kerstin Ritter", "title": "Harnessing spatial MRI normalization: patch individual filter layers for\n  CNNs", "comments": null, "journal-ref": "Medical Imaging meets NeurIPS (MED-NeurIPS) 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neuroimaging studies based on magnetic resonance imaging (MRI) typically\nemploy rigorous forms of preprocessing. Images are spatially normalized to a\nstandard template using linear and non-linear transformations. Thus, one can\nassume that a patch at location (x, y, height, width) contains the same brain\nregion across the entire data set. Most analyses applied on brain MRI using\nconvolutional neural networks (CNNs) ignore this distinction from natural\nimages. Here, we suggest a new layer type called patch individual filter (PIF)\nlayer, which trains higher-level filters locally as we assume that more\nabstract features are locally specific after spatial normalization. We evaluate\nPIF layers on three different tasks, namely sex classification as well as\neither Alzheimer's disease (AD) or multiple sclerosis (MS) detection. We\ndemonstrate that CNNs using PIF layers outperform their counterparts in\nseveral, especially low sample size settings.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 18:01:43 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Eitel", "Fabian", ""], ["Albrecht", "Jan Philipp", ""], ["Paul", "Friedemann", ""], ["Ritter", "Kerstin", ""]]}, {"id": "1911.06283", "submitter": "Mengyuan Yan", "authors": "Mengyuan Yan, Yilin Zhu, Ning Jin, Jeannette Bohg", "title": "Self-Supervised Learning of State Estimation for Manipulating Deformable\n  Linear Objects", "comments": "v3: update acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate model-based, visual robot manipulation of linear deformable\nobjects. Our approach is based on a state-space representation of the physical\nsystem that the robot aims to control. This choice has multiple advantages,\nincluding the ease of incorporating physics priors in the dynamics model and\nperception model, and the ease of planning manipulation actions. In addition,\nphysical states can naturally represent object instances of different\nappearances. Therefore, dynamics in the state space can be learned in one\nsetting and directly used in other visually different settings. This is in\ncontrast to dynamics learned in pixel space or latent space, where\ngeneralization to visual differences are not guaranteed. Challenges in taking\nthe state-space approach are the estimation of the high-dimensional state of a\ndeformable object from raw images, where annotations are very expensive on real\ndata, and finding a dynamics model that is both accurate, generalizable, and\nefficient to compute. We are the first to demonstrate self-supervised training\nof rope state estimation on real images, without requiring expensive\nannotations. This is achieved by our novel self-supervising learning objective,\nwhich is generalizable across a wide range of visual appearances. With\nestimated rope states, we train a fast and differentiable neural network\ndynamics model that encodes the physics of mass-spring systems. Our method has\na higher accuracy in predicting future states compared to models that do not\ninvolve explicit state estimation and do not use any physics prior, while only\nusing 3\\% of training data. We also show that our approach achieves more\nefficient manipulation, both in simulation and on a real robot, when used\nwithin a model predictive controller.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 18:04:51 GMT"}, {"version": "v2", "created": "Sun, 19 Jan 2020 22:31:33 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2020 04:06:53 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Yan", "Mengyuan", ""], ["Zhu", "Yilin", ""], ["Jin", "Ning", ""], ["Bohg", "Jeannette", ""]]}, {"id": "1911.06298", "submitter": "Kezia Irene", "authors": "Kezia Irene, Aditya Yudha P., Harlan Haidi, Nurul Faza, Winston\n  Chandra", "title": "Fetal Head and Abdomen Measurement Using Convolutional Neural Network,\n  Hough Transform, and Difference of Gaussian Revolved along Elliptical Path\n  (Dogell) Algorithm", "comments": "5 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The number of fetal-neonatal death in Indonesia is still high compared to\ndeveloped countries. This is caused by the absence of maternal monitoring\nduring pregnancy. This paper presents an automated measurement for fetal head\ncircumference (HC) and abdominal circumference (AC) from the ultrasonography\n(USG) image. This automated measurement is beneficial to detect early fetal\nabnormalities during the pregnancy period. We used the convolutional neural\nnetwork (CNN) method, to preprocess the USG data. After that, we approximate\nthe head and abdominal circumference using the Hough transform algorithm and\nthe difference of Gaussian Revolved along Elliptical Path (Dogell) Algorithm.\nWe used the data set from national hospitals in Indonesia and for the accuracy\nmeasurement, we compared our results to the annotated images measured by\nprofessional obstetricians. The result shows that by using CNN, we reduced\nerrors caused by a noisy image. We found that the Dogell algorithm performs\nbetter than the Hough transform algorithm in both time and accuracy. This is\nthe first HC and AC approximation that used the CNN method to preprocess the\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 18:34:38 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Irene", "Kezia", ""], ["P.", "Aditya Yudha", ""], ["Haidi", "Harlan", ""], ["Faza", "Nurul", ""], ["Chandra", "Winston", ""]]}, {"id": "1911.06352", "submitter": "Jingjing Pan", "authors": "Jingjing Pan, Yash Goyal, Stefan Lee", "title": "Question-Conditioned Counterfactual Image Generation for VQA", "comments": "Accepted by the VQA Workshop at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Visual Question Answering (VQA) models continue to push the\nstate-of-the-art forward, they largely remain black-boxes - failing to provide\ninsight into how or why an answer is generated. In this ongoing work, we\npropose addressing this shortcoming by learning to generate counterfactual\nimages for a VQA model - i.e. given a question-image pair, we wish to generate\na new image such that i) the VQA model outputs a different answer, ii) the new\nimage is minimally different from the original, and iii) the new image is\nrealistic. Our hope is that providing such counterfactual examples allows users\nto investigate and understand the VQA model's internal mechanisms.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 19:37:33 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Pan", "Jingjing", ""], ["Goyal", "Yash", ""], ["Lee", "Stefan", ""]]}, {"id": "1911.06357", "submitter": "Katharina Hoebel", "authors": "Katharina Hoebel, Ken Chang, Jay Patel, Praveer Singh, Jayashree\n  Kalpathy-Cramer", "title": "Give me (un)certainty -- An exploration of parameters that affect\n  segmentation uncertainty", "comments": "Machine Learning for Health (ML4H) at NeurIPS 2019 - Extended\n  Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation tasks in medical imaging are inherently ambiguous: the boundary\nof a target structure is oftentimes unclear due to image quality and biological\nfactors. As such, predicted segmentations from deep learning algorithms are\ninherently ambiguous. Additionally, \"ground truth\" segmentations performed by\nhuman annotators are in fact weak labels that further increase the uncertainty\nof outputs of supervised models developed on these manual labels. To date, most\ndeep learning segmentation studies utilize predicted segmentations without\nuncertainty quantification. In contrast, we explore the use of Monte Carlo\ndropout U-Nets for the segmentation with additional quantification of\nsegmentation uncertainty. We assess the utility of three measures of\nuncertainty (Coefficient of Variation, Mean Pairwise Dice, and Mean Voxelwise\nUncertainty) for the segmentation of a less ambiguous target structure (liver)\nand a more ambiguous one (liver tumors). Furthermore, we assess how the utility\nof these measures changes with different patch sizes and cost functions. Our\nresults suggest that models trained using larger patches and the weighted\ncategorical cross-entropy as cost function allow the extraction of more\nmeaningful uncertainty measures compared to smaller patches and soft dice loss.\nAmong the three uncertainty measures Mean Pairwise Dice shows the strongest\ncorrelation with segmentation quality. Our study serves as a proof-of-concept\nof how uncertainty measures can be used to assess the quality of a predicted\nsegmentation, potentially serving to flag low quality segmentations from a\ngiven model for further human review.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 19:52:08 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Hoebel", "Katharina", ""], ["Chang", "Ken", ""], ["Patel", "Jay", ""], ["Singh", "Praveer", ""], ["Kalpathy-Cramer", "Jayashree", ""]]}, {"id": "1911.06379", "submitter": "Andr\\'es Almansa", "authors": "Mario Gonz\\'alez, Andr\\'es Almansa, Mauricio Delbracio, Pablo Mus\\'e,\n  Pauline Tan", "title": "Solving Inverse Problems by Joint Posterior Maximization with a VAE\n  Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG eess.IV math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we address the problem of solving ill-posed inverse problems in\nimaging where the prior is a neural generative model. Specifically we consider\nthe decoupled case where the prior is trained once and can be reused for many\ndifferent log-concave degradation models without retraining. Whereas previous\nMAP-based approaches to this problem lead to highly non-convex optimization\nalgorithms, our approach computes the joint (space-latent) MAP that naturally\nleads to alternate optimization algorithms and to the use of a stochastic\nencoder to accelerate computations. The resulting technique is called JPMAP\nbecause it performs Joint Posterior Maximization using an Autoencoding Prior.\nWe show theoretical and experimental evidence that the proposed objective\nfunction is quite close to bi-convex. Indeed it satisfies a weak bi-convexity\nproperty which is sufficient to guarantee that our optimization scheme\nconverges to a stationary point.\n  Experimental results also show the higher quality of the solutions obtained\nby our JPMAP approach with respect to other non-convex MAP approaches which\nmore often get stuck in spurious local optima.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 20:52:09 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Gonz\u00e1lez", "Mario", ""], ["Almansa", "Andr\u00e9s", ""], ["Delbracio", "Mauricio", ""], ["Mus\u00e9", "Pablo", ""], ["Tan", "Pauline", ""]]}, {"id": "1911.06395", "submitter": "Yucheng Tang", "authors": "Yucheng Tang, Ho Hin Lee, Yuchen Xu, Olivia Tang, Yunqiang Chen,\n  Dashan Gao, Shizhong Han, Riqiang Gao, Camilo Bermudez, Michael R. Savona,\n  Richard G. Abramson, Yuankai Huo, Bennett A. Landman", "title": "Contrast Phase Classification with a Generative Adversarial Network", "comments": "8 pages, 4 figures", "journal-ref": "SPIE2020", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic contrast enhanced computed tomography (CT) is an imaging technique\nthat provides critical information on the relationship of vascular structure\nand dynamics in the context of underlying anatomy. A key challenge for image\nprocessing with contrast enhanced CT is that phase discrepancies are latent in\ndifferent tissues due to contrast protocols, vascular dynamics, and metabolism\nvariance. Previous studies with deep learning frameworks have been proposed for\nclassifying contrast enhancement with networks inspired by computer vision.\nHere, we revisit the challenge in the context of whole abdomen contrast\nenhanced CTs. To capture and compensate for the complex contrast changes, we\npropose a novel discriminator in the form of a multi-domain disentangled\nrepresentation learning network. The goal of this network is to learn an\nintermediate representation that separates contrast enhancement from anatomy\nand enables classification of images with varying contrast time. Briefly, our\nunpaired contrast disentangling GAN(CD-GAN) Discriminator follows the ResNet\narchitecture to classify a CT scan from different enhancement phases. To\nevaluate the approach, we trained the enhancement phase classifier on 21060\nslices from two clinical cohorts of 230 subjects. Testing was performed on 9100\nslices from 30 independent subjects who had been imaged with CT scans from all\ncontrast phases. Performance was quantified in terms of the multi-class\nnormalized confusion matrix. The proposed network significantly improved\ncorrespondence over baseline UNet, ResNet50 and StarGAN performance of accuracy\nscores 0.54. 0.55, 0.62 and 0.91, respectively. The proposed discriminator from\nthe disentangled network presents a promising technique that may allow deeper\nmodeling of dynamic imaging against patient specific anatomies.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 21:51:44 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Tang", "Yucheng", ""], ["Lee", "Ho Hin", ""], ["Xu", "Yuchen", ""], ["Tang", "Olivia", ""], ["Chen", "Yunqiang", ""], ["Gao", "Dashan", ""], ["Han", "Shizhong", ""], ["Gao", "Riqiang", ""], ["Bermudez", "Camilo", ""], ["Savona", "Michael R.", ""], ["Abramson", "Richard G.", ""], ["Huo", "Yuankai", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1911.06396", "submitter": "V\\'itor Albiero", "authors": "V\\'itor Albiero, Kevin W. Bowyer, Kushal Vangara, Michael C. King", "title": "Does Face Recognition Accuracy Get Better With Age? Deep Face Matchers\n  Say No", "comments": "Paper will appear at the WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies generally agree that face recognition accuracy is higher for\nolder persons than for younger persons. But most previous studies were before\nthe wave of deep learning matchers, and most considered accuracy only in terms\nof the verification rate for genuine pairs. This paper investigates accuracy\nfor age groups 16-29, 30-49 and 50-70, using three modern deep CNN matchers,\nand considers differences in the impostor and genuine distributions as well as\nverification rates and ROC curves. We find that accuracy is lower for older\npersons and higher for younger persons. In contrast, a pre deep learning\nmatcher on the same dataset shows the traditional result of higher accuracy for\nolder persons, although its overall accuracy is much lower than that of the\ndeep learning matchers. Comparing the impostor and genuine distributions, we\nconclude that impostor scores have a larger effect than genuine scores in\ncausing lower accuracy for the older age group. We also investigate the effects\nof training data across the age groups. Our results show that fine-tuning the\ndeep CNN models on additional images of older persons actually lowers accuracy\nfor the older age group. Also, we fine-tune and train from scratch two models\nusing age-balanced training datasets, and these results also show lower\naccuracy for older age group. These results argue that the lower accuracy for\nthe older age group is not due to imbalance in the original training data.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 21:52:54 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Albiero", "V\u00edtor", ""], ["Bowyer", "Kevin W.", ""], ["Vangara", "Kushal", ""], ["King", "Michael C.", ""]]}, {"id": "1911.06443", "submitter": "Matthew Vowels", "authors": "Matthew J. Vowels and Necati Cihan Camgoz and Richard Bowden", "title": "Gated Variational AutoEncoders: Incorporating Weak Supervision to\n  Encourage Disentanglement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational AutoEncoders (VAEs) provide a means to generate representational\nlatent embeddings. Previous research has highlighted the benefits of achieving\nrepresentations that are disentangled, particularly for downstream tasks.\nHowever, there is some debate about how to encourage disentanglement with VAEs\nand evidence indicates that existing implementations of VAEs do not achieve\ndisentanglement consistently. The evaluation of how well a VAE's latent space\nhas been disentangled is often evaluated against our subjective expectations of\nwhich attributes should be disentangled for a given problem. Therefore, by\ndefinition, we already have domain knowledge of what should be achieved and yet\nwe use unsupervised approaches to achieve it. We propose a weakly-supervised\napproach that incorporates any available domain knowledge into the training\nprocess to form a Gated-VAE. The process involves partitioning the\nrepresentational embedding and gating backpropagation. All partitions are\nutilised on the forward pass but gradients are backpropagated through different\npartitions according to selected image/target pairings. The approach can be\nused to modify existing VAE models such as beta-VAE, InfoVAE and DIP-VAE-II.\nExperiments demonstrate that using gated backpropagation, latent factors are\nrepresented in their intended partition. The approach is applied to images of\nfaces for the purpose of disentangling head-pose from facial expression.\nQuantitative metrics show that using Gated-VAE improves average\ndisentanglement, completeness and informativeness, as compared with un-gated\nimplementations. Qualitative assessment of latent traversals demonstrate its\ndisentanglement of head-pose from expression, even when only weak/noisy\nsupervision is available.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 01:46:16 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Vowels", "Matthew J.", ""], ["Camgoz", "Necati Cihan", ""], ["Bowden", "Richard", ""]]}, {"id": "1911.06460", "submitter": "Juanyong Duan", "authors": "Juanyong Duan, Sim Heng Ong, Qi Zhao", "title": "Human Annotations Improve GAN Performances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Generative Adversarial Networks (GANs) have shown great success in many\napplications. In this work, we present a novel method that leverages human\nannotations to improve the quality of generated images. Unlike previous\nparadigms that directly ask annotators to distinguish between real and fake\ndata in a straightforward way, we propose and annotate a set of carefully\ndesigned attributes that encode important image information at various levels,\nto understand the differences between fake and real images. Specifically, we\nhave collected an annotated dataset that contains 600 fake images and 400 real\nimages. These images are evaluated by 10 workers from the Amazon Mechanical\nTurk (AMT) based on eight carefully defined attributes. Statistical analyses\nhave revealed different distributions of the proposed attributes between real\nand fake images. These attributes are shown to be useful in discriminating fake\nimages from real ones, and deep neural networks are developed to automatically\npredict the attributes. We further utilize the information by integrating the\nattributes into GANs to generate better images. Experimental results evaluated\nby multiple metrics show performance improvement of the proposed model.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 03:09:38 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Duan", "Juanyong", ""], ["Ong", "Sim Heng", ""], ["Zhao", "Qi", ""]]}, {"id": "1911.06464", "submitter": "Kaustav Chakraborty", "authors": "Michael Maring and Kaustav Chakraborty", "title": "Multiple Style-Transfer in Real-Time", "comments": "Authors agreed that there is not much novelty in the work so\n  presented", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Style transfer aims to combine the content of one image with the artistic\nstyle of another. It was discovered that lower levels of convolutional networks\ncaptured style information, while higher levels captures content information.\nThe original style transfer formulation used a weighted combination of VGG-16\nlayer activations to achieve this goal. Later, this was accomplished in\nreal-time using a feed-forward network to learn the optimal combination of\nstyle and content features from the respective images. The first aim of our\nproject was to introduce a framework for capturing the style from several\nimages at once. We propose a method that extends the original real-time style\ntransfer formulation by combining the features of several style images. This\nmethod successfully captures color information from the separate style images.\nThe other aim of our project was to improve the temporal style continuity from\nframe to frame. Accordingly, we have experimented with the temporal stability\nof the output images and discussed the various available techniques that could\nbe employed as alternatives.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 03:49:41 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 03:56:40 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Maring", "Michael", ""], ["Chakraborty", "Kaustav", ""]]}, {"id": "1911.06470", "submitter": "YueFeng Chen", "authors": "Kejiang Chen, Hang Zhou, Yuefeng Chen, Xiaofeng Mao, Yuhong Li, Yuan\n  He, Hui Xue, Weiming Zhang, Nenghai Yu", "title": "Self-supervised Adversarial Training", "comments": "Accepted to ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has demonstrated that neural networks are vulnerable to\nadversarial examples. To escape from the predicament, many works try to harden\nthe model in various ways, in which adversarial training is an effective way\nwhich learns robust feature representation so as to resist adversarial attacks.\nMeanwhile, the self-supervised learning aims to learn robust and semantic\nembedding from data itself. With these views, we introduce self-supervised\nlearning to against adversarial examples in this paper. Specifically, the\nself-supervised representation coupled with k-Nearest Neighbour is proposed for\nclassification. To further strengthen the defense ability, self-supervised\nadversarial training is proposed, which maximizes the mutual information\nbetween the representations of original examples and the corresponding\nadversarial examples. Experimental results show that the self-supervised\nrepresentation outperforms its supervised version in respect of robustness and\nself-supervised adversarial training can further improve the defense ability\nefficiently.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 04:13:11 GMT"}, {"version": "v2", "created": "Sat, 1 Feb 2020 12:10:27 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Chen", "Kejiang", ""], ["Zhou", "Hang", ""], ["Chen", "Yuefeng", ""], ["Mao", "Xiaofeng", ""], ["Li", "Yuhong", ""], ["He", "Yuan", ""], ["Xue", "Hui", ""], ["Zhang", "Weiming", ""], ["Yu", "Nenghai", ""]]}, {"id": "1911.06475", "submitter": "Huy Hieu Pham", "authors": "Hieu H. Pham, Tung T. Le, Dat Q. Tran, Dat T. Ngo, Ha Q. Nguyen", "title": "Interpreting chest X-rays via CNNs that exploit hierarchical disease\n  dependencies and uncertainty labels", "comments": "This is a pre-print of our paper that was accepted by Neurocomputing\n  - Its shorter version has been accepted by Medical Imaging with Deep Learning\n  conference (MIDL 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chest radiography is one of the most common types of diagnostic radiology\nexams, which is critical for screening and diagnosis of many different thoracic\ndiseases. Specialized algorithms have been developed to detect several specific\npathologies such as lung nodule or lung cancer. However, accurately detecting\nthe presence of multiple diseases from chest X-rays (CXRs) is still a\nchallenging task. This paper presents a supervised multi-label classification\nframework based on deep convolutional neural networks (CNNs) for predicting the\nrisk of 14 common thoracic diseases. We tackle this problem by training\nstate-of-the-art CNNs that exploit dependencies among abnormality labels. We\nalso propose to use the label smoothing technique for a better handling of\nuncertain samples, which occupy a significant portion of almost every CXR\ndataset. Our model is trained on over 200,000 CXRs of the recently released\nCheXpert dataset and achieves a mean area under the curve (AUC) of 0.940 in\npredicting 5 selected pathologies from the validation set. This is the highest\nAUC score yet reported to date. The proposed method is also evaluated on the\nindependent test set of the CheXpert competition, which is composed of 500 CXR\nstudies annotated by a panel of 5 experienced radiologists. The performance is\non average better than 2.6 out of 3 other individual radiologists with a mean\nAUC of 0.930, which ranks first on the CheXpert leaderboard at the time of\nwriting this paper.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 04:29:43 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 09:12:09 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2020 15:05:42 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Pham", "Hieu H.", ""], ["Le", "Tung T.", ""], ["Tran", "Dat Q.", ""], ["Ngo", "Dat T.", ""], ["Nguyen", "Ha Q.", ""]]}, {"id": "1911.06479", "submitter": "Shufei Zhang Mr", "authors": "Shufei Zhang and Kaizhu Huang and Zenglin Xu", "title": "On Model Robustness Against Adversarial Examples", "comments": "some theoretical bounds need to be revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the model robustness against adversarial examples, referred to as\nsmall perturbed input data that may however fool many state-of-the-art deep\nlearning models. Unlike previous research, we establish a novel theory\naddressing the robustness issue from the perspective of stability of the loss\nfunction in the small neighborhood of natural examples. We propose to exploit\nan energy function to describe the stability and prove that reducing such\nenergy guarantees the robustness against adversarial examples. We also show\nthat the traditional training methods including adversarial training with the\n$l_2$ norm constraint (AT) and Virtual Adversarial Training (VAT) tend to\nminimize the lower bound of our proposed energy function. We make an analysis\nshowing that minimization of such lower bound can however lead to insufficient\nrobustness within the neighborhood around the input sample. Furthermore, we\ndesign a more rational method with the energy regularization which proves to\nachieve better robustness than previous methods. Through a series of\nexperiments, we demonstrate the superiority of our model on both supervised\ntasks and semi-supervised tasks. In particular, our proposed adversarial\nframework achieves the best performance compared with previous adversarial\ntraining methods on benchmark datasets MNIST, CIFAR-10, and SVHN. Importantly,\nthey demonstrate much better robustness against adversarial examples than all\nthe other comparison methods.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 05:02:25 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 05:26:51 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Zhang", "Shufei", ""], ["Huang", "Kaizhu", ""], ["Xu", "Zenglin", ""]]}, {"id": "1911.06486", "submitter": "Sohini Roychowdhury", "authors": "Sohini Roy Chowdhury, Lars Tornberg, Robin Halvfordsson, Jonatan\n  Nordh, Adam Suhren Gustafsson, Joel Wall, Mattias Westerberg, Adam Wirehed,\n  Louis Tilloy, Zhanying Hu, Haoyuan Tan, Meng Pan and Jonas Sjoberg", "title": "Automated Augmentation with Reinforcement Learning and GANs for Robust\n  Identification of Traffic Signs using Front Camera Images", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": "IEEE Asilomar SSC 2019", "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic sign identification using camera images from vehicles plays a\ncritical role in autonomous driving and path planning. However, the front\ncamera images can be distorted due to blurriness, lighting variations and\nvandalism which can lead to degradation of detection performances. As a\nsolution, machine learning models must be trained with data from multiple\ndomains, and collecting and labeling more data in each new domain is time\nconsuming and expensive. In this work, we present an end-to-end framework to\naugment traffic sign training data using optimal reinforcement learning\npolicies and a variety of Generative Adversarial Network (GAN) models, that can\nthen be used to train traffic sign detector modules. Our automated augmenter\nenables learning from transformed nightime, poor lighting, and varying degrees\nof occlusions using the LISA Traffic Sign and BDD-Nexar dataset. The proposed\nmethod enables mapping training data from one domain to another, thereby\nimproving traffic sign detection precision/recall from 0.70/0.66 to 0.83/0.71\nfor nighttime images.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 06:23:50 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Chowdhury", "Sohini Roy", ""], ["Tornberg", "Lars", ""], ["Halvfordsson", "Robin", ""], ["Nordh", "Jonatan", ""], ["Gustafsson", "Adam Suhren", ""], ["Wall", "Joel", ""], ["Westerberg", "Mattias", ""], ["Wirehed", "Adam", ""], ["Tilloy", "Louis", ""], ["Hu", "Zhanying", ""], ["Tan", "Haoyuan", ""], ["Pan", "Meng", ""], ["Sjoberg", "Jonas", ""]]}, {"id": "1911.06487", "submitter": "Qi She", "authors": "Qi She, Fan Feng, Xinyue Hao, Qihan Yang, Chuanlin Lan, Vincenzo\n  Lomonaco, Xuesong Shi, Zhengwei Wang, Yao Guo, Yimin Zhang, Fei Qiao, Rosa H.\n  M. Chan", "title": "OpenLORIS-Object: A Robotic Vision Dataset and Benchmark for Lifelong\n  Deep Learning", "comments": "7 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent breakthroughs in computer vision have benefited from the\navailability of large representative datasets (e.g. ImageNet and COCO) for\ntraining. Yet, robotic vision poses unique challenges for applying visual\nalgorithms developed from these standard computer vision datasets due to their\nimplicit assumption over non-varying distributions for a fixed set of tasks.\nFully retraining models each time a new task becomes available is infeasible\ndue to computational, storage and sometimes privacy issues, while na\\\"{i}ve\nincremental strategies have been shown to suffer from catastrophic forgetting.\nIt is crucial for the robots to operate continuously under open-set and\ndetrimental conditions with adaptive visual perceptual systems, where lifelong\nlearning is a fundamental capability. However, very few datasets and benchmarks\nare available to evaluate and compare emerging techniques. To fill this gap, we\nprovide a new lifelong robotic vision dataset (\"OpenLORIS-Object\") collected\nvia RGB-D cameras. The dataset embeds the challenges faced by a robot in the\nreal-life application and provides new benchmarks for validating lifelong\nobject recognition algorithms. Moreover, we have provided a testbed of $9$\nstate-of-the-art lifelong learning algorithms. Each of them involves $48$ tasks\nwith $4$ evaluation metrics over the OpenLORIS-Object dataset. The results\ndemonstrate that the object recognition task in the ever-changing difficulty\nenvironments is far from being solved and the bottlenecks are at the\nforward/backward transfer designs. Our dataset and benchmark are publicly\navailable at at\n\\href{https://lifelong-robotic-vision.github.io/dataset/object}{\\underline{https://lifelong-robotic-vision.github.io/dataset/object}}.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 06:27:27 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 05:31:06 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["She", "Qi", ""], ["Feng", "Fan", ""], ["Hao", "Xinyue", ""], ["Yang", "Qihan", ""], ["Lan", "Chuanlin", ""], ["Lomonaco", "Vincenzo", ""], ["Shi", "Xuesong", ""], ["Wang", "Zhengwei", ""], ["Guo", "Yao", ""], ["Zhang", "Yimin", ""], ["Qiao", "Fei", ""], ["Chan", "Rosa H. M.", ""]]}, {"id": "1911.06502", "submitter": "Kazuhiro Takemoto", "authors": "Hokuto Hirano, Kazuhiro Takemoto", "title": "Simple iterative method for generating targeted universal adversarial\n  perturbations", "comments": "4 pages, 3 figures, 1 table", "journal-ref": "Algorithms 13, 268 (2020)", "doi": "10.3390/a13110268", "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are vulnerable to adversarial attacks. In\nparticular, a single perturbation known as the universal adversarial\nperturbation (UAP) can foil most classification tasks conducted by DNNs. Thus,\ndifferent methods for generating UAPs are required to fully evaluate the\nvulnerability of DNNs. A realistic evaluation would be with cases that consider\ntargeted attacks; wherein the generated UAP causes DNN to classify an input\ninto a specific class. However, the development of UAPs for targeted attacks\nhas largely fallen behind that of UAPs for non-targeted attacks. Therefore, we\npropose a simple iterative method to generate UAPs for targeted attacks. Our\nmethod combines the simple iterative method for generating non-targeted UAPs\nand the fast gradient sign method for generating a targeted adversarial\nperturbation for an input. We applied the proposed method to state-of-the-art\nDNN models for image classification and proved the existence of almost\nimperceptible UAPs for targeted attacks; further, we demonstrated that such\nUAPs are easily generatable.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 08:02:20 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 05:53:03 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Hirano", "Hokuto", ""], ["Takemoto", "Kazuhiro", ""]]}, {"id": "1911.06505", "submitter": "Szabolcs-Botond L\\H{o}rincz", "authors": "Szabolcs-Botond L\\H{o}rincz, Szabolcs P\\'avel and Lehel Csat\\'o", "title": "Single View Distortion Correction using Semantic Guidance", "comments": null, "journal-ref": null, "doi": "10.1109/IJCNN.2019.8852065", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most distortion correction methods focus on simple forms of distortion, such\nas radial or linear distortions. These works undistort images either based on\nmeasurements in the presence of a calibration grid, or use multiple views to\nfind point correspondences and predict distortion parameters. When possible\ndistortions are more complex, e.g. in the case of a camera being placed behind\na refractive surface such as glass, the standard method is to use a calibration\ngrid. Considering a high variety of distortions, it is nonviable to conduct\nthese measurements. In this work, we present a single view distortion\ncorrection method which is capable of undistorting images containing\narbitrarily complex distortions by exploiting recent advancements in\ndifferentiable image sampling and in the usage of semantic information to\naugment various tasks. The results of this work show that our model is able to\nestimate and correct highly complex distortions, and that incorporating\nsemantic information mitigates the process of image undistortion.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 08:05:49 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["L\u0151rincz", "Szabolcs-Botond", ""], ["P\u00e1vel", "Szabolcs", ""], ["Csat\u00f3", "Lehel", ""]]}, {"id": "1911.06531", "submitter": "Yunfan Liu", "authors": "Yunfan Liu, Qi Li, Zhenan Sun, Tieniu Tan", "title": "A3GAN: An Attribute-aware Attentive Generative Adversarial Network for\n  Face Aging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face aging, which aims at aesthetically rendering a given face to predict its\nfuture appearance, has received significant research attention in recent years.\nAlthough great progress has been achieved with the success of Generative\nAdversarial Networks (GANs) in synthesizing realistic images, most existing\nGAN-based face aging methods have two main problems: 1) unnatural changes of\nhigh-level semantic information (e.g. facial attributes) due to the\ninsufficient utilization of prior knowledge of input faces, and 2) distortions\nof low-level image content including ghosting artifacts and modifications in\nage-irrelevant regions. In this paper, we introduce A3GAN, an Attribute-Aware\nAttentive face aging model to address the above issues. Facial attribute\nvectors are regarded as the conditional information and embedded into both the\ngenerator and discriminator, encouraging synthesized faces to be faithful to\nattributes of corresponding inputs. To improve the visual fidelity of\ngeneration results, we leverage the attention mechanism to restrict\nmodifications to age-related areas and preserve image details. Moreover, the\nwavelet packet transform is employed to capture textural features at multiple\nscales in the frequency space. Extensive experimental results demonstrate the\neffectiveness of our model in synthesizing photorealistic aged face images and\nachieving state-of-the-art performance on popular face aging datasets.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 09:21:53 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Liu", "Yunfan", ""], ["Li", "Qi", ""], ["Sun", "Zhenan", ""], ["Tan", "Tieniu", ""]]}, {"id": "1911.06587", "submitter": "YueFeng Chen", "authors": "Xiaofeng Mao, Yuefeng Chen, Yuhong Li, Yuan He, Hui Xue", "title": "Learning To Characterize Adversarial Subspaces", "comments": "Submitted to ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are known to be vulnerable to the maliciously\ngenerated adversarial examples. To detect these adversarial examples, previous\nmethods use artificially designed metrics to characterize the properties of\n\\textit{adversarial subspaces} where adversarial examples lie. However, we find\nthese methods are not working in practical attack detection scenarios. Because\nthe artificially defined features are lack of robustness and show limitation in\ndiscriminative power to detect strong attacks. To solve this problem, we\npropose a novel adversarial detection method which identifies adversaries by\nadaptively learning reasonable metrics to characterize adversarial subspaces.\nAs auxiliary context information, \\textit{k} nearest neighbors are used to\nrepresent the surrounded subspace of the detected sample. We propose an\ninnovative model called Neighbor Context Encoder (NCE) to learn from \\textit{k}\nneighbors context and infer if the detected sample is normal or adversarial. We\nconduct thorough experiment on CIFAR-10, CIFAR-100 and ImageNet dataset. The\nresults demonstrate that our approach surpasses all existing methods under\nthree settings: \\textit{attack-aware black-box detection},\n\\textit{attack-unaware black-box detection} and \\textit{white-box detection}.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 12:28:18 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Mao", "Xiaofeng", ""], ["Chen", "Yuefeng", ""], ["Li", "Yuhong", ""], ["He", "Yuan", ""], ["Xue", "Hui", ""]]}, {"id": "1911.06591", "submitter": "YueFeng Chen", "authors": "Xiaodan Li, Yuefeng Chen, Yuan He, Hui Xue", "title": "AdvKnn: Adversarial Attacks On K-Nearest Neighbor Classifiers With\n  Approximate Gradients", "comments": "Submitted to ICASSP 2020, Implementation\n  https://github.com/fiona-lxd/AdvKnn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been shown to be vulnerable to adversarial\nexamples---maliciously crafted examples that can trigger the target model to\nmisbehave by adding imperceptible perturbations. Existing attack methods for\nk-nearest neighbor~(kNN) based algorithms either require large perturbations or\nare not applicable for large k. To handle this problem, this paper proposes a\nnew method called AdvKNN for evaluating the adversarial robustness of kNN-based\nmodels. Firstly, we propose a deep kNN block to approximate the output of kNN\nmethods, which is differentiable thus can provide gradients for attacks to\ncross the decision boundary with small distortions. Second, a new consistency\nlearning for distribution instead of classification is proposed for the\neffectiveness in distribution based methods. Extensive experimental results\nindicate that the proposed method significantly outperforms state of the art in\nterms of attack success rate and the added perturbations.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 12:42:10 GMT"}, {"version": "v2", "created": "Fri, 29 Nov 2019 11:10:18 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Li", "Xiaodan", ""], ["Chen", "Yuefeng", ""], ["He", "Yuan", ""], ["Xue", "Hui", ""]]}, {"id": "1911.06600", "submitter": "Duc Nguyen", "authors": "Anh-Duc Nguyen, Seonghwa Choi, Woojae Kim, Sanghoon Lee", "title": "GraphX-Convolution for Point Cloud Deformation in 2D-to-3D Conversion", "comments": "In Proceedings of the IEEE International Conference on Computer\n  Vision 2019. Fixed minor details and added some updates. Project page:\n  https://git.io/JeovA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel deep method to reconstruct a point cloud of\nan object from a single still image. Prior arts in the field struggle to\nreconstruct an accurate and scalable 3D model due to either the inefficient and\nexpensive 3D representations, the dependency between the output and number of\nmodel parameters or the lack of a suitable computing operation. We propose to\novercome these by deforming a random point cloud to the object shape through\ntwo steps: feature blending and deformation. In the first step, the global and\npoint-specific shape features extracted from a 2D object image are blended with\nthe encoded feature of a randomly generated point cloud, and then this mixture\nis sent to the deformation step to produce the final representative point set\nof the object. In the deformation process, we introduce a new layer termed as\nGraphX that considers the inter-relationship between points like common graph\nconvolutions but operates on unordered sets. Moreover, with a simple trick, the\nproposed model can generate an arbitrary-sized point cloud, which is the first\ndeep method to do so. Extensive experiments verify that we outperform existing\nmodels and halve the state-of-the-art distance score in single image 3D\nreconstruction.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 13:14:13 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Nguyen", "Anh-Duc", ""], ["Choi", "Seonghwa", ""], ["Kim", "Woojae", ""], ["Lee", "Sanghoon", ""]]}, {"id": "1911.06634", "submitter": "Chao Li", "authors": "Chao Li, Yixiao Yang, Kun He, Stephen Lin and John E. Hopcroft", "title": "Single Image Reflection Removal through Cascaded Refinement", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of removing undesirable reflections from a single\nimage captured through a glass surface, which is an ill-posed, challenging but\npractically important problem for photo enhancement. Inspired by iterative\nstructure reduction for hidden community detection in social networks, we\npropose an Iterative Boost Convolutional LSTM Network (IBCLN) that enables\ncascaded prediction for reflection removal. IBCLN is a cascaded network that\niteratively refines the estimates of transmission and reflection layers in a\nmanner that they can boost the prediction quality to each other, and\ninformation across steps of the cascade is transferred using an LSTM. The\nintuition is that the transmission is the strong, dominant structure while the\nreflection is the weak, hidden structure. They are complementary to each other\nin a single image and thus a better estimate and reduction on one side from the\noriginal image leads to a more accurate estimate on the other side. To\nfacilitate training over multiple cascade steps, we employ LSTM to address the\nvanishing gradient problem, and propose residual reconstruction loss as further\ntraining guidance. Besides, we create a dataset of real-world images with\nreflection and ground-truth transmission layers to mitigate the problem of\ninsufficient data. Comprehensive experiments demonstrate that the proposed\nmethod can effectively remove reflections in real and synthetic images compared\nwith state-of-the-art reflection removal methods.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 13:52:31 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 07:04:01 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Li", "Chao", ""], ["Yang", "Yixiao", ""], ["He", "Kun", ""], ["Lin", "Stephen", ""], ["Hopcroft", "John E.", ""]]}, {"id": "1911.06644", "submitter": "Okan K\\\"op\\\"ukl\\\"u", "authors": "Okan K\\\"op\\\"ukl\\\"u, Xiangyu Wei, Gerhard Rigoll", "title": "You Only Watch Once: A Unified CNN Architecture for Real-Time\n  Spatiotemporal Action Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatiotemporal action localization requires the incorporation of two sources\nof information into the designed architecture: (1) temporal information from\nthe previous frames and (2) spatial information from the key frame. Current\nstate-of-the-art approaches usually extract these information with separate\nnetworks and use an extra mechanism for fusion to get detections. In this work,\nwe present YOWO, a unified CNN architecture for real-time spatiotemporal action\nlocalization in video streams. YOWO is a single-stage architecture with two\nbranches to extract temporal and spatial information concurrently and predict\nbounding boxes and action probabilities directly from video clips in one\nevaluation. Since the whole architecture is unified, it can be optimized\nend-to-end. The YOWO architecture is fast providing 34 frames-per-second on\n16-frames input clips and 62 frames-per-second on 8-frames input clips, which\nis currently the fastest state-of-the-art architecture on spatiotemporal action\nlocalization task. Remarkably, YOWO outperforms the previous state-of-the art\nresults on J-HMDB-21 and UCF101-24 with an impressive improvement of ~3% and\n~12%, respectively. We make our code and pretrained models publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 14:09:47 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 18:08:00 GMT"}, {"version": "v3", "created": "Tue, 24 Mar 2020 15:54:52 GMT"}, {"version": "v4", "created": "Tue, 30 Jun 2020 15:14:34 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["K\u00f6p\u00fckl\u00fc", "Okan", ""], ["Wei", "Xiangyu", ""], ["Rigoll", "Gerhard", ""]]}, {"id": "1911.06663", "submitter": "Matthias Schubert", "authors": "Teodora Pandeva and Matthias Schubert", "title": "MMGAN: Generative Adversarial Networks for Multi-Modal Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past years, Generative Adversarial Networks (GANs) have shown a\nremarkable generation performance especially in image synthesis. Unfortunately,\nthey are also known for having an unstable training process and might loose\nparts of the data distribution for heterogeneous input data. In this paper, we\npropose a novel GAN extension for multi-modal distribution learning (MMGAN). In\nour approach, we model the latent space as a Gaussian mixture model with a\nnumber of clusters referring to the number of disconnected data manifolds in\nthe observation space, and include a clustering network, which relates each\ndata manifold to one Gaussian cluster. Thus, the training gets more stable.\nMoreover, MMGAN allows for clustering real data according to the learned data\nmanifold in the latent space. By a series of benchmark experiments, we\nillustrate that MMGAN outperforms competitive state-of-the-art models in terms\nof clustering performance.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 14:31:02 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Pandeva", "Teodora", ""], ["Schubert", "Matthias", ""]]}, {"id": "1911.06667", "submitter": "Youngwan Lee", "authors": "Youngwan Lee, Jongyoul Park", "title": "CenterMask : Real-Time Anchor-Free Instance Segmentation", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple yet efficient anchor-free instance segmentation, called\nCenterMask, that adds a novel spatial attention-guided mask (SAG-Mask) branch\nto anchor-free one stage object detector (FCOS) in the same vein with Mask\nR-CNN. Plugged into the FCOS object detector, the SAG-Mask branch predicts a\nsegmentation mask on each box with the spatial attention map that helps to\nfocus on informative pixels and suppress noise. We also present an improved\nbackbone networks, VoVNetV2, with two effective strategies: (1) residual\nconnection for alleviating the optimization problem of larger VoVNet\n\\cite{lee2019energy} and (2) effective Squeeze-Excitation (eSE) dealing with\nthe channel information loss problem of original SE. With SAG-Mask and\nVoVNetV2, we deign CenterMask and CenterMask-Lite that are targeted to large\nand small models, respectively. Using the same ResNet-101-FPN backbone,\nCenterMask achieves 38.3%, surpassing all previous state-of-the-art methods\nwhile at a much faster speed. CenterMask-Lite also outperforms the\nstate-of-the-art by large margins at over 35fps on Titan Xp. We hope that\nCenterMask and VoVNetV2 can serve as a solid baseline of real-time instance\nsegmentation and backbone network for various vision tasks, respectively. The\nCode is available at https://github.com/youngwanLEE/CenterMask.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 14:38:12 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 13:46:59 GMT"}, {"version": "v3", "created": "Tue, 17 Mar 2020 09:17:47 GMT"}, {"version": "v4", "created": "Wed, 18 Mar 2020 02:06:18 GMT"}, {"version": "v5", "created": "Wed, 25 Mar 2020 09:13:01 GMT"}, {"version": "v6", "created": "Thu, 2 Apr 2020 12:32:33 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Lee", "Youngwan", ""], ["Park", "Jongyoul", ""]]}, {"id": "1911.06687", "submitter": "Ahmad Chaddad", "authors": "Ahmad Chaddad, Saima Rathore, Mingli Zhang, Christian Desrosiers and\n  Tamim Niazi", "title": "Deep radiomic features from MRI scans predict survival outcome of\n  recurrent glioblastoma", "comments": "Accepted in MICCAI RNO workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes to use deep radiomic features (DRFs) from a convolutional\nneural network (CNN) to model fine-grained texture signatures in the radiomic\nanalysis of recurrent glioblastoma (rGBM). We use DRFs to predict survival of\nrGBM patients with preoperative T1-weighted post-contrast MR images (n=100).\nDRFs are extracted from regions of interest labelled by a radiation oncologist\nand used to compare between short-term and long-term survival patient groups.\nRandom forest (RF) classification is employed to predict survival outcome\n(i.e., short or long survival), as well as to identify highly group-informative\ndescriptors. Classification using DRFs results in an area under the ROC curve\n(AUC) of 89.15% (p<0.01) in predicting rGBM patient survival, compared to\n78.07% (p<0.01) when using standard radiomic features (SRF). These results\nindicate the potential of DRFs as a prognostic marker for patients with rGBM.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 15:18:38 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Chaddad", "Ahmad", ""], ["Rathore", "Saima", ""], ["Zhang", "Mingli", ""], ["Desrosiers", "Christian", ""], ["Niazi", "Tamim", ""]]}, {"id": "1911.06721", "submitter": "Maxim Neumann", "authors": "Maxim Neumann, Andre Susano Pinto, Xiaohua Zhai, Neil Houlsby", "title": "In-domain representation learning for remote sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the importance of remote sensing, surprisingly little attention has\nbeen paid to it by the representation learning community. To address it and to\nestablish baselines and a common evaluation protocol in this domain, we provide\nsimplified access to 5 diverse remote sensing datasets in a standardized form.\nSpecifically, we investigate in-domain representation learning to develop\ngeneric remote sensing representations and explore which characteristics are\nimportant for a dataset to be a good source for remote sensing representation\nlearning. The established baselines achieve state-of-the-art performance on\nthese datasets.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 16:09:38 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Neumann", "Maxim", ""], ["Pinto", "Andre Susano", ""], ["Zhai", "Xiaohua", ""], ["Houlsby", "Neil", ""]]}, {"id": "1911.06777", "submitter": "Ali Jahanshahi", "authors": "Ali Jahanshahi", "title": "TinyCNN: A Tiny Modular CNN Accelerator for Embedded FPGA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In recent years, Convolutional Neural Network (CNN) based methods have\nachieved great success in a large number of applications and have been among\nthe most powerful and widely used techniques in computer vision. However,\nCNN-based methods are computational-intensive and resource-consuming, and thus\nare hard to be integrated into embedded systems such as smart phones, smart\nglasses, and robots. FPGA is one of the most promising platforms for\naccelerating CNN, but the limited on-chip memory size limit the performance of\nFPGA accelerator for CNN. In this paper, we propose a framework for designing\nCNN accelerator on embedded FPGA for image classification. The proposed\nframework provides a tool for FPGA resource-aware design space exploration of\nCNNs and automatically generates the hardware description of the CNN to be\nprogrammed on a target FPGA. The framework consists of three main backends;\nsoftware, hardware generation, and simulation/precision adjustment. The\nsoftware backend serves as an API to the designer to design the CNN and train\nit according to the hardware resources that are available. Using the CNN model,\nhardware backend generates the necessary hardware components and integrates\nthem to generate the hardware description of the CNN. Finaly,\nSimulation/precision adjustment backend adjusts the inter-layer precision units\nto minimize the classification error. We used 16-bit fixed-point data in a CNN\naccelerator (FPGA) and compared it to the exactly similar software version\nrunning on an ARM processor (32-bit floating point data). We encounter about 3%\naccuracy loss in classification of the accelerated (FPGA) version. In return,\nwe got up to 15.75x speedup by classifying with the accelerated version on the\nFPGA.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 17:42:52 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Jahanshahi", "Ali", ""]]}, {"id": "1911.06786", "submitter": "Akshay Kulkarni", "authors": "Akshay Kulkarni, Navid Panchi, Sharath Chandra Raparthy and Shital\n  Chiddarwar", "title": "Data Efficient Stagewise Knowledge Distillation", "comments": "15 pages, 1 figure, 6 tables and 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of Deep Learning (DL), the deployment of modern DL models\nrequiring large computational power poses a significant problem for\nresource-constrained systems. This necessitates building compact networks that\nreduce computations while preserving performance. Traditional Knowledge\nDistillation (KD) methods that transfer knowledge from teacher to student (a)\nuse a single-stage and (b) require the whole data set while distilling the\nknowledge to the student. In this work, we propose a new method called\nStagewise Knowledge Distillation (SKD) which builds on traditional KD methods\nby progressive stagewise training to leverage the knowledge gained from the\nteacher, resulting in data-efficient distillation process. We evaluate our\nmethod on classification and semantic segmentation tasks. We show, across the\ntested tasks, significant performance gains even with a fraction of the data\nused in distillation, without compromising on the metric. We also compare our\nmethod with existing KD techniques and show that SKD outperforms them.\nMoreover, our method can be viewed as a generalized model compression technique\nthat complements other model compression methods such as quantization or\npruning.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 18:06:26 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 10:47:40 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 09:02:52 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Kulkarni", "Akshay", ""], ["Panchi", "Navid", ""], ["Raparthy", "Sharath Chandra", ""], ["Chiddarwar", "Shital", ""]]}, {"id": "1911.06816", "submitter": "Zahra Riahi Samani", "authors": "Zahra Riahi Samani, Jacob Antony Alappatt, Drew Parker, Abdol Aziz\n  Ould Ismail, Ragini Verma", "title": "QC-Automator: Deep Learning-based Automated Quality Control for\n  Diffusion MR Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality assessment of diffusion MRI (dMRI) data is essential prior to any\nanalysis, so that appropriate pre-processing can be used to improve data\nquality and ensure that the presence of MRI artifacts do not affect the results\nof subsequent image analysis. Manual quality assessment of the data is\nsubjective, possibly error-prone, and infeasible, especially considering the\ngrowing number of consortium-like studies, underlining the need for automation\nof the process. In this paper, we have developed a deep-learning-based\nautomated quality control (QC) tool, QC-Automator, for dMRI data, that can\nhandle a variety of artifacts such as motion, multiband interleaving, ghosting,\nsusceptibility, herringbone and chemical shifts. QC-Automator uses\nconvolutional neural networks along with transfer learning to train the\nautomated artifact detection on a labeled dataset of ~332000 slices of dMRI\ndata, from 155 unique subjects and 5 scanners with different dMRI acquisitions,\nachieving a 98% accuracy in detecting artifacts. The method is fast and paves\nthe way for efficient and effective artifact detection in large datasets. It is\nalso demonstrated to be replicable on other datasets with different acquisition\nparameters.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 08:11:17 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Samani", "Zahra Riahi", ""], ["Alappatt", "Jacob Antony", ""], ["Parker", "Drew", ""], ["Ismail", "Abdol Aziz Ould", ""], ["Verma", "Ragini", ""]]}, {"id": "1911.06845", "submitter": "Eyob Gebretinsae Beyene", "authors": "Eyob Gebretinsae Beyene", "title": "Handwritten and Machine printed OCR for Geez Numbers Using Artificial\n  Neural Network", "comments": "Presented at NeurIPS 2019 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Researches have been done on Ethiopic scripts. However studies excluded the\nGeez numbers from the studies because of different reasons. This paper presents\noffline handwritten and machine printed Geez number recognition using feed\nforward back propagation artificial neural network. On this study, different\nGeez image characters were collected from google image search and three persons\nare instructed to write the numbers using pencil. In total we have collected\n560 numbers of characters. We have used 460 of the characters for training and\n100 are used for testing. Accordingly we have achieved overall all\nclassification ~89:88%\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 19:37:54 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Beyene", "Eyob Gebretinsae", ""]]}, {"id": "1911.06849", "submitter": "Radu Tudor Ionescu", "authors": "Petru Soviany, Radu Tudor Ionescu, Paolo Rota, Nicu Sebe", "title": "Curriculum Self-Paced Learning for Cross-Domain Object Detection", "comments": "Accepted for publication in Computer Vision and Image Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training (source) domain bias affects state-of-the-art object detectors, such\nas Faster R-CNN, when applied to new (target) domains. To alleviate this\nproblem, researchers proposed various domain adaptation methods to improve\nobject detection results in the cross-domain setting, e.g. by translating\nimages with ground-truth labels from the source domain to the target domain\nusing Cycle-GAN. On top of combining Cycle-GAN transformations and self-paced\nlearning in a smart and efficient way, in this paper, we propose a novel\nself-paced algorithm that learns from easy to hard. Our method is simple and\neffective, without any overhead during inference. It uses only pseudo-labels\nfor samples taken from the target domain, i.e. the domain adaptation is\nunsupervised. We conduct experiments on four cross-domain benchmarks, showing\nbetter results than the state of the art. We also perform an ablation study\ndemonstrating the utility of each component in our framework. Additionally, we\nstudy the applicability of our framework to other object detectors.\nFurthermore, we compare our difficulty measure with other measures from the\nrelated literature, proving that it yields superior results and that it\ncorrelates well with the performance metric.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 19:43:23 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 22:24:32 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2021 18:33:04 GMT"}, {"version": "v4", "created": "Wed, 20 Jan 2021 19:11:38 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Soviany", "Petru", ""], ["Ionescu", "Radu Tudor", ""], ["Rota", "Paolo", ""], ["Sebe", "Nicu", ""]]}, {"id": "1911.06866", "submitter": "Lijun Zhang", "authors": "Lijun Zhang, Srinath Nizampatnam, Ahana Gangopadhyay, and Marcos V.\n  Conde", "title": "Multi-attention Networks for Temporal Localization of Video-level Labels", "comments": "7 pages, 3 figures; This work was presented at the 3rd Workshop on\n  YouTube-8M Large-Scale Video Understanding, at the International Conference\n  on Computer Vision (ICCV 2019) in Seoul, Korea", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal localization remains an important challenge in video understanding.\nIn this work, we present our solution to the 3rd YouTube-8M Video Understanding\nChallenge organized by Google Research. Participants were required to build a\nsegment-level classifier using a large-scale training data set with noisy\nvideo-level labels and a relatively small-scale validation data set with\naccurate segment-level labels. We formulated the problem as a multiple instance\nmulti-label learning and developed an attention-based mechanism to selectively\nemphasize the important frames by attention weights. The model performance is\nfurther improved by constructing multiple sets of attention networks. We\nfurther fine-tuned the model using the segment-level data set. Our final model\nconsists of an ensemble of attention/multi-attention networks, deep bag of\nframes models, recurrent neural networks and convolutional neural networks. It\nranked 13th on the private leader board and stands out for its efficient usage\nof resources.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 20:42:26 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Zhang", "Lijun", ""], ["Nizampatnam", "Srinath", ""], ["Gangopadhyay", "Ahana", ""], ["Conde", "Marcos V.", ""]]}, {"id": "1911.06902", "submitter": "Aniket Anand Deshmukh", "authors": "Urun Dogan, Aniket Anand Deshmukh, Marcin Machura, Christian Igel", "title": "Label-similarity Curriculum Learning", "comments": "Accepted as a conference paper at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Curriculum learning can improve neural network training by guiding the\noptimization to desirable optima. We propose a novel curriculum learning\napproach for image classification that adapts the loss function by changing the\nlabel representation. The idea is to use a probability distribution over\nclasses as target label, where the class probabilities reflect the similarity\nto the true class. Gradually, this label representation is shifted towards the\nstandard one-hot-encoding. That is, in the beginning minor mistakes are\ncorrected less than large mistakes, resembling a teaching process in which\nbroad concepts are explained first before subtle differences are taught.\n  The class similarity can be based on prior knowledge. For the special case of\nthe labels being natural words, we propose a generic way to automatically\ncompute the similarities. The natural words are embedded into Euclidean space\nusing a standard word embedding. The probability of each class is then a\nfunction of the cosine similarity between the vector representations of the\nclass and the true label. The proposed label-similarity curriculum learning\n(LCL) approach was empirically evaluated using several popular deep learning\narchitectures for image classification tasks applied to five datasets including\nImageNet, CIFAR100, and AWA2. In all scenarios, LCL was able to improve the\nclassification accuracy on the test data compared to standard training.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 23:03:58 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 00:48:48 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Dogan", "Urun", ""], ["Deshmukh", "Aniket Anand", ""], ["Machura", "Marcin", ""], ["Igel", "Christian", ""]]}, {"id": "1911.06932", "submitter": "Praneet Dutta", "authors": "Praneet Dutta, Bruce Power, Adam Halpert, Carlos Ezequiel, Aravind\n  Subramanian, Chanchal Chatterjee, Sindhu Hari, Kenton Prindle, Vishal\n  Vaddina, Andrew Leach, Raj Domala, Laura Bandura, Massimo Mascaro", "title": "3D Conditional Generative Adversarial Networks to enable large-scale\n  seismic image enhancement", "comments": "To be Presented at the NeurIPS 2019, Second Workshop on Machine\n  Learning and the Physicial Sciences, Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose GAN-based image enhancement models for frequency enhancement of 2D\nand 3D seismic images. Seismic imagery is used to understand and characterize\nthe Earth's subsurface for energy exploration. Because these images often\nsuffer from resolution limitations and noise contamination, our proposed method\nperforms large-scale seismic volume frequency enhancement and denoising. The\nenhanced images reduce uncertainty and improve decisions about issues, such as\noptimal well placement, that often rely on low signal-to-noise ratio (SNR)\nseismic volumes. We explored the impact of adding lithology class information\nto the models, resulting in improved performance on PSNR and SSIM metrics over\na baseline model with no conditional information.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 01:39:21 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Dutta", "Praneet", ""], ["Power", "Bruce", ""], ["Halpert", "Adam", ""], ["Ezequiel", "Carlos", ""], ["Subramanian", "Aravind", ""], ["Chatterjee", "Chanchal", ""], ["Hari", "Sindhu", ""], ["Prindle", "Kenton", ""], ["Vaddina", "Vishal", ""], ["Leach", "Andrew", ""], ["Domala", "Raj", ""], ["Bandura", "Laura", ""], ["Mascaro", "Massimo", ""]]}, {"id": "1911.06939", "submitter": "Yu Yu", "authors": "Yu Yu and Jean-Marc Odobez", "title": "Unsupervised Representation Learning for Gaze Estimation", "comments": "Paper accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although automatic gaze estimation is very important to a large variety of\napplication areas, it is difficult to train accurate and robust gaze models, in\ngreat part due to the difficulty in collecting large and diverse data\n(annotating 3D gaze is expensive and existing datasets use different setups).\nTo address this issue, our main contribution in this paper is to propose an\neffective approach to learn a low dimensional gaze representation without gaze\nannotations, which to the best of our best knowledge, is the first work to do\nso. The main idea is to rely on a gaze redirection network and use the gaze\nrepresentation difference of the input and target images (of the redirection\nnetwork) as the redirection variable. A redirection loss in image domain allows\nthe joint training of both the redirection network and the gaze representation\nnetwork. In addition, we propose a warping field regularization which not only\nprovides an explicit physical meaning to the gaze representations but also\navoids redirection distortions. Promising results on few-shot gaze estimation\n(competitive results can be achieved with as few as <= 100 calibration\nsamples), cross-dataset gaze estimation, gaze network pretraining, and another\ntask (head pose estimation) demonstrate the validity of our framework.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 02:46:04 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 16:01:04 GMT"}, {"version": "v3", "created": "Fri, 20 Dec 2019 10:30:31 GMT"}, {"version": "v4", "created": "Fri, 3 Apr 2020 14:30:02 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Yu", "Yu", ""], ["Odobez", "Jean-Marc", ""]]}, {"id": "1911.06953", "submitter": "Yongcheng Jing", "authors": "Yongcheng Jing, Xiao Liu, Yukang Ding, Xinchao Wang, Errui Ding,\n  Mingli Song, Shilei Wen", "title": "Dynamic Instance Normalization for Arbitrary Style Transfer", "comments": "Accepted to AAAI 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior normalization methods rely on affine transformations to produce\narbitrary image style transfers, of which the parameters are computed in a\npre-defined way. Such manually-defined nature eventually results in the\nhigh-cost and shared encoders for both style and content encoding, making style\ntransfer systems cumbersome to be deployed in resource-constrained environments\nlike on the mobile-terminal side. In this paper, we propose a new and\ngeneralized normalization module, termed as Dynamic Instance Normalization\n(DIN), that allows for flexible and more efficient arbitrary style transfers.\nComprising an instance normalization and a dynamic convolution, DIN encodes a\nstyle image into learnable convolution parameters, upon which the content image\nis stylized. Unlike conventional methods that use shared complex encoders to\nencode content and style, the proposed DIN introduces a sophisticated style\nencoder, yet comes with a compact and lightweight content encoder for fast\ninference. Experimental results demonstrate that the proposed approach yields\nvery encouraging results on challenging style patterns and, to our best\nknowledge, for the first time enables an arbitrary style transfer using\nMobileNet-based lightweight architecture, leading to a reduction factor of more\nthan twenty in computational cost as compared to existing approaches.\nFurthermore, the proposed DIN provides flexible support for state-of-the-art\nconvolutional operations, and thus triggers novel functionalities, such as\nuniform-stroke placement for non-natural images and automatic spatial-stroke\ncontrol.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 04:03:52 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Jing", "Yongcheng", ""], ["Liu", "Xiao", ""], ["Ding", "Yukang", ""], ["Wang", "Xinchao", ""], ["Ding", "Errui", ""], ["Song", "Mingli", ""], ["Wen", "Shilei", ""]]}, {"id": "1911.06956", "submitter": "Vishwanath Saragadam Raja Venkata", "authors": "Vishwanath Saragadam, Aswin Sankaranarayanan", "title": "On Space-spectrum Uncertainty Analysis for Coded Aperture Systems", "comments": "14 pages", "journal-ref": null, "doi": "10.1364/OE.381154", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and analyze the concept of space-spectrum uncertainty for\ncertain commonly-used designs for spectrally programmable cameras. Our key\nfinding states that, it is impossible to simultaneously capture high-resolution\nspatial images while programming the spectrum at high resolution. This\nphenomenon arises due to a Fourier relationship between the aperture used for\nobtaining spectrum and its corresponding diffraction blur in the (spatial)\nimage. We show that the product of spatial and spectral standard deviations is\nlower bounded by {\\lambda}/4{\\pi}{\\nu_0} femto square-meters, where {\\nu_0} is\nthe density of groves in the diffraction grating and {\\lambda} is the\nwavelength of light. Experiments with a lab prototype for simultaneously\nmeasuring spectrum and image validate our findings and its implication for\nspectral filtering.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 04:56:33 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Saragadam", "Vishwanath", ""], ["Sankaranarayanan", "Aswin", ""]]}, {"id": "1911.06968", "submitter": "Wenbin Li", "authors": "Wenbin Li, Lei Wang, Xingxing Zhang, Jing Huo, Yang Gao and Jiebo Luo", "title": "Defensive Few-shot Adversarial Learning", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The robustness of deep learning models against adversarial attacks has\nreceived increasing attention in recent years. However, both deep learning and\nadversarial training rely on the availability of a large amount of labeled data\nand usually do not generalize well to new, unseen classes when only a few\ntraining samples are accessible. To address this problem, we explicitly\nintroduce a new challenging problem -- how to learn a robust deep model with\nlimited training samples per class, called defensive few-shot learning in this\npaper. Simply employing the existing adversarial training techniques in the\nliterature cannot solve this problem. This is because few-shot learning needs\nto learn transferable knowledge from disjoint auxiliary data, and thus it is\ninvalid to assume the sample-level distribution consistency between the\ntraining and test sets as commonly assumed in existing adversarial training\ntechniques. In this paper, instead of assuming such a distribution consistency,\nwe propose to make this assumption at a task-level in the episodic training\nparadigm in order to better transfer the defense knowledge. Furthermore, inside\neach task, we design a task-conditioned distribution constraint to narrow the\ndistribution gap between clean and adversarial examples at a sample-level.\nThese give rise to a novel mechanism called multi-level distribution based\nadversarial training (MDAT) for learning transferable adversarial defense. In\naddition, a unified $\\mathcal{F}_{\\beta}$ score is introduced to evaluate\ndifferent defense methods under the same principle. Extensive experiments\ndemonstrate that MDAT achieves higher effectiveness and robustness over\nexisting alternatives in the few-shot case.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 05:57:16 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Li", "Wenbin", ""], ["Wang", "Lei", ""], ["Zhang", "Xingxing", ""], ["Huo", "Jing", ""], ["Gao", "Yang", ""], ["Luo", "Jiebo", ""]]}, {"id": "1911.06971", "submitter": "Zhiqin Chen", "authors": "Zhiqin Chen and Andrea Tagliasacchi and Hao Zhang", "title": "BSP-Net: Generating Compact Meshes via Binary Space Partitioning", "comments": "CVPR 2020 Best Student Paper Award. Project page:\n  https://bsp-net.github.io, Code:\n  https://github.com/czq142857/BSP-NET-original", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polygonal meshes are ubiquitous in the digital 3D domain, yet they have only\nplayed a minor role in the deep learning revolution. Leading methods for\nlearning generative models of shapes rely on implicit functions, and generate\nmeshes only after expensive iso-surfacing routines. To overcome these\nchallenges, we are inspired by a classical spatial data structure from computer\ngraphics, Binary Space Partitioning (BSP), to facilitate 3D learning. The core\ningredient of BSP is an operation for recursive subdivision of space to obtain\nconvex sets. By exploiting this property, we devise BSP-Net, a network that\nlearns to represent a 3D shape via convex decomposition. Importantly, BSP-Net\nis unsupervised since no convex shape decompositions are needed for training.\nThe network is trained to reconstruct a shape using a set of convexes obtained\nfrom a BSP-tree built on a set of planes. The convexes inferred by BSP-Net can\nbe easily extracted to form a polygon mesh, without any need for iso-surfacing.\nThe generated meshes are compact (i.e., low-poly) and well suited to represent\nsharp geometry; they are guaranteed to be watertight and can be easily\nparameterized. We also show that the reconstruction quality by BSP-Net is\ncompetitive with state-of-the-art methods while using much fewer primitives.\nCode is available at https://github.com/czq142857/BSP-NET-original.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 06:25:26 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 07:07:37 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2020 17:58:07 GMT"}, {"version": "v4", "created": "Mon, 31 Aug 2020 23:41:51 GMT"}, {"version": "v5", "created": "Mon, 23 Nov 2020 22:12:27 GMT"}, {"version": "v6", "created": "Mon, 7 Dec 2020 20:02:19 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Chen", "Zhiqin", ""], ["Tagliasacchi", "Andrea", ""], ["Zhang", "Hao", ""]]}, {"id": "1911.06975", "submitter": "Andrey Filippov", "authors": "Andrey Filippov, Oleg Dzhimiev", "title": "Long Range 3D with Quadocular Thermal (LWIR) Camera", "comments": "10 pages, 5 figures; fixed abbreviations navigation, added pdf ToC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Long Wave Infrared (LWIR) cameras provide images regardles of the ambient\nillumination, they tolerate fog and are not blinded by the incoming car\nheadlights. These features make LWIR cameras attractive for autonomous\nnavigation, security and military applications. Thermal images can be used\nsimilarly to the visible range ones, including 3D scene reconstruction with two\nor more such cameras mounted on a rigid frame. There are two additional\nchallenges for this spectral range: lower image resolution and lower contrast\nof the textures.\n  In this work, we demonstrate quadocular LWIR camera setup, calibration, image\ncapturing and processing that result in long range 3D perception with 0.077 pix\ndisparity error over 90% of the depth map. With low resolution (160 x 120) LWIR\nsensors we achieved 10% range accuracy at 28 m with 56 degrees horizontal field\nof view (HFoV) and 150 mm baseline. Scaled to the now-standard 640 x 512\nresolution and 200 mm baseline suitable for head-mounted application the result\nwould be 10% accuracy at 130 m.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 06:51:29 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 01:01:15 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Filippov", "Andrey", ""], ["Dzhimiev", "Oleg", ""]]}, {"id": "1911.06978", "submitter": "Jinkyu Kim", "authors": "Jinkyu Kim, Teruhisa Misu, Yi-Ting Chen, Ashish Tawari, and John Canny", "title": "Grounding Human-to-Vehicle Advice for Self-driving Vehicles", "comments": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent success suggests that deep neural control networks are likely to be a\nkey component of self-driving vehicles. These networks are trained on large\ndatasets to imitate human actions, but they lack semantic understanding of\nimage contents. This makes them brittle and potentially unsafe in situations\nthat do not match training data. Here, we propose to address this issue by\naugmenting training data with natural language advice from a human. Advice\nincludes guidance about what to do and where to attend. We present the first\nstep toward advice giving, where we train an end-to-end vehicle controller that\naccepts advice. The controller adapts the way it attends to the scene (visual\nattention) and the control (steering and speed). Attention mechanisms tie\ncontroller behavior to salient objects in the advice. We evaluate our model on\na novel advisable driving dataset with manually annotated human-to-vehicle\nadvice called Honda Research Institute-Advice Dataset (HAD). We show that\ntaking advice improves the performance of the end-to-end network, while the\nnetwork cues on a variety of visual features that are provided by advice. The\ndataset is available at https://usa.honda-ri.com/HAD.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 07:15:01 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Kim", "Jinkyu", ""], ["Misu", "Teruhisa", ""], ["Chen", "Yi-Ting", ""], ["Tawari", "Ashish", ""], ["Canny", "John", ""]]}, {"id": "1911.06982", "submitter": "Zekun Cai", "authors": "Renhe Jiang, Zekun Cai, Zhaonan Wang, Chuang Yang, Zipei Fan, Xuan\n  Song, Kota Tsubouchi, Ryosuke Shibasaki", "title": "VLUC: An Empirical Benchmark for Video-Like Urban Computing on Citywide\n  Crowd and Traffic Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, massive urban human mobility data are being generated from mobile\nphones, car navigation systems, and traffic sensors. Predicting the density and\nflow of the crowd or traffic at a citywide level becomes possible by using the\nbig data and cutting-edge AI technologies. It has been a very significant\nresearch topic with high social impact, which can be widely applied to\nemergency management, traffic regulation, and urban planning. In particular, by\nmeshing a large urban area to a number of fine-grained mesh-grids, citywide\ncrowd and traffic information in a continuous time period can be represented\nlike a video, where each timestamp can be seen as one video frame. Based on\nthis idea, a series of methods have been proposed to address video-like\nprediction for citywide crowd and traffic. In this study, we publish a new\naggregated human mobility dataset generated from a real-world smartphone\napplication and build a standard benchmark for such kind of video-like urban\ncomputing with this new dataset and the existing open datasets. We first\ncomprehensively review the state-of-the-art works of literature and formulate\nthe density and in-out flow prediction problem, then conduct a thorough\nperformance assessment for those methods. With this benchmark, we hope\nresearchers can easily follow up and quickly launch a new solution on this\ntopic.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 07:38:44 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Jiang", "Renhe", ""], ["Cai", "Zekun", ""], ["Wang", "Zhaonan", ""], ["Yang", "Chuang", ""], ["Fan", "Zipei", ""], ["Song", "Xuan", ""], ["Tsubouchi", "Kota", ""], ["Shibasaki", "Ryosuke", ""]]}, {"id": "1911.06987", "submitter": "Ryuichiro Hataya", "authors": "Ryuichiro Hataya, Jan Zdenek, Kazuki Yoshizoe, Hideki Nakayama", "title": "Faster AutoAugment: Learning Augmentation Strategies using\n  Backpropagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation methods are indispensable heuristics to boost the\nperformance of deep neural networks, especially in image recognition tasks.\nRecently, several studies have shown that augmentation strategies found by\nsearch algorithms outperform hand-made strategies. Such methods employ\nblack-box search algorithms over image transformations with continuous or\ndiscrete parameters and require a long time to obtain better strategies. In\nthis paper, we propose a differentiable policy search pipeline for data\naugmentation, which is much faster than previous methods. We introduce\napproximate gradients for several transformation operations with discrete\nparameters as well as the differentiable mechanism for selecting operations. As\nthe objective of training, we minimize the distance between the distributions\nof augmented data and the original data, which can be differentiated. We show\nthat our method, Faster AutoAugment, achieves significantly faster searching\nthan prior work without a performance drop.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 08:10:59 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Hataya", "Ryuichiro", ""], ["Zdenek", "Jan", ""], ["Yoshizoe", "Kazuki", ""], ["Nakayama", "Hideki", ""]]}, {"id": "1911.06993", "submitter": "Hongwei Dong", "authors": "Hongwei Dong and Siyu Zhang and Bin Zou and Lamei Zhang", "title": "Automatic Design of CNNs via Differentiable Neural Architecture Search\n  for PolSAR Image Classification", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, Volume 58,\n  Issue 9, September 2020, Pages 6362-6375", "doi": "10.1109/TGRS.2020.2976694", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have shown good performance in\npolarimetric synthetic aperture radar (PolSAR) image classification due to the\nautomation of feature engineering. Excellent hand-crafted architectures of CNNs\nincorporated the wisdom of human experts, which is an important reason for\nCNN's success. However, the design of the architectures is a difficult problem,\nwhich needs a lot of professional knowledge as well as computational resources.\nMoreover, the architecture designed by hand might be suboptimal, because it is\nonly one of thousands of unobserved but objective existed paths. Considering\nthat the success of deep learning is largely due to its automation of the\nfeature engineering process, how to design automatic architecture searching\nmethods to replace the hand-crafted ones is an interesting topic. In this\npaper, we explore the application of neural architecture search (NAS) in PolSAR\narea for the first time. Different from the utilization of existing NAS\nmethods, we propose a differentiable architecture search (DAS) method which is\ncustomized for PolSAR classification. The proposed DAS is equipped with a\nPolSAR tailored search space and an improved one-shot search strategy. By DAS,\nthe weights parameters and architecture parameters (corresponds to the\nhyperparameters but not the topologies) can be optimized by stochastic gradient\ndescent method during the training. The optimized architecture parameters\nshould be transformed into corresponding CNN architecture and re-train to\nachieve high-precision PolSAR classification. In addition, complex-valued DAS\nis developed to take into account the characteristics of PolSAR images so as to\nfurther improve the performance. Experiments on three PolSAR benchmark datasets\nshow that the CNNs obtained by searching have better classification performance\nthan the hand-crafted ones.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 08:33:33 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 11:13:32 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Dong", "Hongwei", ""], ["Zhang", "Siyu", ""], ["Zou", "Bin", ""], ["Zhang", "Lamei", ""]]}, {"id": "1911.06994", "submitter": "Geesara Prathap Kulathunga", "authors": "Geesara Prathap, Roman Fedorenko, Alexandr Klimchik", "title": "Regions of Interest Segmentation from LiDAR Point Cloud for Multirotor\n  Aerial Vehicles", "comments": null, "journal-ref": null, "doi": "10.1109/ICUAS48674.2020.9214019", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel filter for segmenting the regions of interest from LiDAR\n3D point cloud for multirotor aerial vehicles. It is specially targeted for\nreal-time applications and works on sparse LiDAR point clouds without\npreliminary mapping. We use this filter as a crucial component of fast obstacle\navoidance system for agriculture drone operating at low altitude. As the first\nstep, each point cloud is transformed into a depth image and then identify\nplaces near to the vehicle (local maxima) by locating areas with high pixel\ndensities. Afterwards, we merge the original depth image with identified\nlocations after maximizing intensities of pixels in which local maxima were\nobtained. Next step is to calculate the range angle image that represents\nangles between two consecutive laser beams based on the improved depth image.\nOnce the corresponding range angle image is constructed, smoothing is applied\nto reduce the noise. Finally, we find out connected components within the\nimproved depth image while incorporating smoothed range angle image. This\nallows separating the regions of interest. The filter has been tested on\nvarious simulated environments as well as an actual drone and provides\nreal-time performance. We make our source code, dataset available at\nhttps://github.com/GPrathap/hagen.git and real world experiment result can be\nfound on the following link: https://www.youtube.com/watch?v=iHd_ZkhKPjc\navailable online.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 08:35:26 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 07:05:44 GMT"}, {"version": "v3", "created": "Mon, 4 May 2020 06:58:55 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Prathap", "Geesara", ""], ["Fedorenko", "Roman", ""], ["Klimchik", "Alexandr", ""]]}, {"id": "1911.06997", "submitter": "Ngoc-Trung Tran", "authors": "Ngoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen, Linxiao Yang,\n  Ngai-Man Cheung", "title": "Self-supervised GAN: Analysis and Improvement with Multi-class Minimax\n  Game", "comments": "Accepted at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised (SS) learning is a powerful approach for representation\nlearning using unlabeled data. Recently, it has been applied to Generative\nAdversarial Networks (GAN) training. Specifically, SS tasks were proposed to\naddress the catastrophic forgetting issue in the GAN discriminator. In this\nwork, we perform an in-depth analysis to understand how SS tasks interact with\nlearning of generator. From the analysis, we identify issues of SS tasks which\nallow a severely mode-collapsed generator to excel the SS tasks. To address the\nissues, we propose new SS tasks based on a multi-class minimax game. The\ncompetition between our proposed SS tasks in the game encourages the generator\nto learn the data distribution and generate diverse samples. We provide both\ntheoretical and empirical analysis to support that our proposed SS tasks have\nbetter convergence property. We conduct experiments to incorporate our proposed\nSS tasks into two different GAN baseline models. Our approach establishes\nstate-of-the-art FID scores on CIFAR-10, CIFAR-100, STL-10, CelebA, Imagenet\n$32\\times32$ and Stacked-MNIST datasets, outperforming existing works by\nconsiderable margins in some cases. Our unconditional GAN model approaches\nperformance of conditional GAN without using labeled data. Our code:\nhttps://github.com/tntrung/msgan\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 08:51:50 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2020 18:00:44 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Tran", "Ngoc-Trung", ""], ["Tran", "Viet-Hung", ""], ["Nguyen", "Ngoc-Bao", ""], ["Yang", "Linxiao", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "1911.06998", "submitter": "Xiaowei Hu", "authors": "Xiaowei Hu, Tianyu Wang, Chi-Wing Fu, Yitong Jiang, Qiong Wang, and\n  Pheng-Ann Heng", "title": "Revisiting Shadow Detection: A New Benchmark Dataset for Complex World", "comments": "Accepted to IEEE Transactions on Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shadow detection in general photos is a nontrivial problem, due to the\ncomplexity of the real world. Though recent shadow detectors have already\nachieved remarkable performance on various benchmark data, their performance is\nstill limited for general real-world situations. In this work, we collected\nshadow images for multiple scenarios and compiled a new dataset of 10,500\nshadow images, each with labeled ground-truth mask, for supporting shadow\ndetection in the complex world. Our dataset covers a rich variety of scene\ncategories, with diverse shadow sizes, locations, contrasts, and types.\nFurther, we comprehensively analyze the complexity of the dataset, present a\nfast shadow detection network with a detail enhancement module to harvest\nshadow details, and demonstrate the effectiveness of our method to detect\nshadows in general situations.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 08:54:09 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2020 10:17:05 GMT"}, {"version": "v3", "created": "Sat, 2 Jan 2021 03:25:11 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Hu", "Xiaowei", ""], ["Wang", "Tianyu", ""], ["Fu", "Chi-Wing", ""], ["Jiang", "Yitong", ""], ["Wang", "Qiong", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1911.07004", "submitter": "Guo-Jun Qi", "authors": "Feng Lin, Haohang Xu, Houqiang Li, Hongkai Xiong, Guo-Jun Qi", "title": "AETv2: AutoEncoding Transformations for Self-Supervised Representation\n  Learning by Minimizing Geodesic Distances in Lie Groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning by predicting transformations has demonstrated\noutstanding performances in both unsupervised and (semi-)supervised tasks.\nAmong the state-of-the-art methods is the AutoEncoding Transformations (AET) by\ndecoding transformations from the learned representations of original and\ntransformed images. Both deterministic and probabilistic AETs rely on the\nEuclidean distance to measure the deviation of estimated transformations from\ntheir groundtruth counterparts. However, this assumption is questionable as a\ngroup of transformations often reside on a curved manifold rather staying in a\nflat Euclidean space. For this reason, we should use the geodesic to\ncharacterize how an image transform along the manifold of a transformation\ngroup, and adopt its length to measure the deviation between transformations.\nParticularly, we present to autoencode a Lie group of homography\ntransformations PG(2) to learn image representations. For this, we make an\nestimate of the intractable Riemannian logarithm by projecting PG(2) to a\nsubgroup of rotation transformations SO(3) that allows the closed-form\nexpression of geodesic distances. Experiments demonstrate the proposed AETv2\nmodel outperforms the previous version as well as the other state-of-the-art\nself-supervised models in multiple tasks.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 09:58:58 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Lin", "Feng", ""], ["Xu", "Haohang", ""], ["Li", "Houqiang", ""], ["Xiong", "Hongkai", ""], ["Qi", "Guo-Jun", ""]]}, {"id": "1911.07014", "submitter": "Chao Xia", "authors": "Pengyu Gao, Siyu Xia, Joseph Robinson, Junkang Zhang, Chao Xia, Ming\n  Shao, Yun Fu", "title": "What Will Your Child Look Like? DNA-Net: Age and Gender Aware Kin Face\n  Synthesizer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual kinship recognition aims to identify blood relatives from facial\nimages. Its practical application-- like in law-enforcement, video\nsurveillance, automatic family album management, and more-- has motivated many\nresearchers to put forth effort on the topic as of recent. In this paper, we\nfocus on a new view of visual kinship technology: kin-based face generation.\nSpecifically, we propose a two-stage kin-face generation model to predict the\nappearance of a child given a pair of parents. The first stage includes a deep\ngenerative adversarial autoencoder conditioned on ages and genders to map\nbetween facial appearance and high-level features. The second stage is our\nproposed DNA-Net, which serves as a transformation between the deep and genetic\nfeatures based on a random selection process to fuse genes of a parent pair to\nform the genes of a child. We demonstrate the effectiveness of the proposed\nmethod quantitatively and qualitatively: quantitatively, pre-trained models and\nhuman subjects perform kinship verification on the generated images of\nchildren; qualitatively, we show photo-realistic face images of children that\nclosely resemble the given pair of parents. In the end, experiments validate\nthat the proposed model synthesizes convincing kin-faces using both subjective\nand objective standards.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 11:09:17 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Gao", "Pengyu", ""], ["Xia", "Siyu", ""], ["Robinson", "Joseph", ""], ["Zhang", "Junkang", ""], ["Xia", "Chao", ""], ["Shao", "Ming", ""], ["Fu", "Yun", ""]]}, {"id": "1911.07023", "submitter": "Min Jin Chong", "authors": "Min Jin Chong, David Forsyth", "title": "Effectively Unbiased FID and Inception Score and where to find them", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows that two commonly used evaluation metrics for generative\nmodels, the Fr\\'echet Inception Distance (FID) and the Inception Score (IS),\nare biased -- the expected value of the score computed for a finite sample set\nis not the true value of the score. Worse, the paper shows that the bias term\ndepends on the particular model being evaluated, so model A may get a better\nscore than model B simply because model A's bias term is smaller. This effect\ncannot be fixed by evaluating at a fixed number of samples. This means all\ncomparisons using FID or IS as currently computed are unreliable.\n  We then show how to extrapolate the score to obtain an effectively bias-free\nestimate of scores computed with an infinite number of samples, which we term\n$\\overline{\\textrm{FID}}_\\infty$ and $\\overline{\\textrm{IS}}_\\infty$. In turn,\nthis effectively bias-free estimate requires good estimates of scores with a\nfinite number of samples. We show that using Quasi-Monte Carlo integration\nnotably improves estimates of FID and IS for finite sample sets. Our\nextrapolated scores are simple, drop-in replacements for the finite sample\nscores. Additionally, we show that using low discrepancy sequence in GAN\ntraining offers small improvements in the resulting generator.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 12:54:05 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2020 22:31:57 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2020 23:01:14 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Chong", "Min Jin", ""], ["Forsyth", "David", ""]]}, {"id": "1911.07033", "submitter": "Zhihang Yuan", "authors": "Zhihang Yuan, Bingzhe Wu, Zheng Liang, Shiwan Zhao, Weichen Bi,\n  Guangyu Sun", "title": "S2DNAS:Transforming Static CNN Model for Dynamic Inference via Neural\n  Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, dynamic inference has emerged as a promising way to reduce the\ncomputational cost of deep convolutional neural network (CNN). In contrast to\nstatic methods (e.g. weight pruning), dynamic inference adaptively adjusts the\ninference process according to each input sample, which can considerably reduce\nthe computational cost on \"easy\" samples while maintaining the overall model\nperformance. In this paper, we introduce a general framework, S2DNAS, which can\ntransform various static CNN models to support dynamic inference via neural\narchitecture search. To this end, based on a given CNN model, we first generate\na CNN architecture space in which each architecture is a multi-stage CNN\ngenerated from the given model using some predefined transformations. Then, we\npropose a reinforcement learning based approach to automatically search for the\noptimal CNN architecture in the generated space. At last, with the searched\nmulti-stage network, we can perform dynamic inference by adaptively choosing a\nstage to evaluate for each sample. Unlike previous works that introduce\nirregular computations or complex controllers in the inference or re-design a\nCNN model from scratch, our method can generalize to most of the popular CNN\narchitectures and the searched dynamic network can be directly deployed using\nexisting deep learning frameworks in various hardware devices.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 13:49:44 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 02:54:18 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Yuan", "Zhihang", ""], ["Wu", "Bingzhe", ""], ["Liang", "Zheng", ""], ["Zhao", "Shiwan", ""], ["Bi", "Weichen", ""], ["Sun", "Guangyu", ""]]}, {"id": "1911.07034", "submitter": "Xiaowei Hu", "authors": "Tianyu Wang, Xiaowei Hu, Qiong Wang, Pheng-Ann Heng, and Chi-Wing Fu", "title": "Instance Shadow Detection", "comments": "Accepted to CVPR 2020", "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  pp. 1880-1889, 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance shadow detection is a brand new problem, aiming to find shadow\ninstances paired with object instances. To approach it, we first prepare a new\ndataset called SOBA, named after Shadow-OBject Association, with 3,623 pairs of\nshadow and object instances in 1,000 photos, each with individual labeled\nmasks. Second, we design LISA, named after Light-guided Instance Shadow-object\nAssociation, an end-to-end framework to automatically predict the shadow and\nobject instances, together with the shadow-object associations and light\ndirection. Then, we pair up the predicted shadow and object instances, and\nmatch them with the predicted shadow-object associations to generate the final\nresults. In our evaluations, we formulate a new metric named the shadow-object\naverage precision to measure the performance of our results. Further, we\nconducted various experiments and demonstrate our method's applicability on\nlight direction estimation and photo editing.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 14:06:05 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 08:38:33 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Wang", "Tianyu", ""], ["Hu", "Xiaowei", ""], ["Wang", "Qiong", ""], ["Heng", "Pheng-Ann", ""], ["Fu", "Chi-Wing", ""]]}, {"id": "1911.07036", "submitter": "Shishun Tian", "authors": "Shishun Tian, Lu Zhang, Wenbin Zou, Xia Li, Ting Su, Luce Morin, and\n  Olivier Deforges", "title": "Quality Assessment of DIBR-synthesized views: An Overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Depth-Image-Based-Rendering (DIBR) is one of the main fundamental\ntechnique to generate new views in 3D video applications, such as Multi-View\nVideos (MVV), Free-Viewpoint Videos (FVV) and Virtual Reality (VR). However,\nthe quality assessment of DIBR-synthesized views is quite different from the\ntraditional 2D images/videos. In recent years, several efforts have been made\ntowards this topic, but there {is a lack of} detailed survey in {the}\nliterature. In this paper, we provide a comprehensive survey on various current\napproaches for DIBR-synthesized views. The current accessible datasets of\nDIBR-synthesized views are firstly reviewed{, followed} by a summary analysis\nof the representative state-of-the-art objective metrics. Then, the\nperformances of different objective metrics are evaluated and discussed on all\navailable datasets. Finally, we discuss the potential challenges and suggest\npossible directions for future research.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 14:13:56 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 08:18:10 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Tian", "Shishun", ""], ["Zhang", "Lu", ""], ["Zou", "Wenbin", ""], ["Li", "Xia", ""], ["Su", "Ting", ""], ["Morin", "Luce", ""], ["Deforges", "Olivier", ""]]}, {"id": "1911.07042", "submitter": "Robert Grupp", "authors": "Robert Grupp, Mathias Unberath, Cong Gao, Rachel Hegeman, Ryan Murphy,\n  Clayton Alexander, Yoshito Otake, Benjamin McArthur, Mehran Armand, Russell\n  Taylor", "title": "Automatic Annotation of Hip Anatomy in Fluoroscopy for Robust and\n  Efficient 2D/3D Registration", "comments": "Revised article to address reviewer comments. Accepted to IPCAI 2020.\n  Supplementary video at https://youtu.be/5AwGlNkcp9o and dataset/code at\n  https://github.com/rg2/DeepFluoroLabeling-IPCAI2020", "journal-ref": "International Journal of Computer Assisted Radiology and Surgery\n  15 (2020) 759-769", "doi": "10.1007/s11548-020-02162-7", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fluoroscopy is the standard imaging modality used to guide hip surgery and is\ntherefore a natural sensor for computer-assisted navigation. In order to\nefficiently solve the complex registration problems presented during\nnavigation, human-assisted annotations of the intraoperative image are\ntypically required. This manual initialization interferes with the surgical\nworkflow and diminishes any advantages gained from navigation. We propose a\nmethod for fully automatic registration using annotations produced by a neural\nnetwork. Neural networks are trained to simultaneously segment anatomy and\nidentify landmarks in fluoroscopy. Training data is obtained using an\nintraoperatively incompatible 2D/3D registration of hip anatomy. Ground truth\n2D labels are established using projected 3D annotations. Intraoperative\nregistration couples an intensity-based strategy with annotations inferred by\nthe network and requires no human assistance. Ground truth labels were obtained\nin 366 fluoroscopic images across 6 cadaveric specimens. In a\nleave-one-subject-out experiment, networks obtained mean dice coefficients for\nleft and right hemipelves, left and right femurs of 0.86, 0.87, 0.90, and 0.84.\nThe mean 2D landmark error was 5.0 mm. The pelvis was registered within 1\ndegree for 86% of the images when using the proposed intraoperative approach\nwith an average runtime of 7 seconds. In comparison, an intensity-only approach\nwithout manual initialization, registered the pelvis to 1 degree in 18% of\nimages. We have created the first accurately annotated, non-synthetic, dataset\nof hip fluoroscopy. By using these annotations as training data for neural\nnetworks, state of the art performance in fluoroscopic segmentation and\nlandmark localization was achieved. Integrating these annotations allows for a\nrobust, fully automatic, and efficient intraoperative registration during\nfluoroscopic navigation of the hip.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 14:58:00 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 15:12:51 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Grupp", "Robert", ""], ["Unberath", "Mathias", ""], ["Gao", "Cong", ""], ["Hegeman", "Rachel", ""], ["Murphy", "Ryan", ""], ["Alexander", "Clayton", ""], ["Otake", "Yoshito", ""], ["McArthur", "Benjamin", ""], ["Armand", "Mehran", ""], ["Taylor", "Russell", ""]]}, {"id": "1911.07046", "submitter": "Qitong Wang", "authors": "Qitong Wang, Yi Zheng, Margrit Betke", "title": "A method for detecting text of arbitrary shapes in natural scenes that\n  improves text spotting", "comments": "Accepted by IEEE CVPR-W 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the meaning of text in images of natural scenes like highway\nsigns or store front emblems is particularly challenging if the text is\nforeshortened in the image or the letters are artistically distorted. We\nintroduce a pipeline-based text spotting framework that can both detect and\nrecognize text in various fonts, shapes, and orientations in natural scene\nimages with complicated backgrounds. The main contribution of our work is the\ntext detection component, which we call UHT, short for UNet, Heatmap, and\nTextfill. UHT uses a UNet to compute heatmaps for candidate text regions and a\ntextfill algorithm to produce tight polygonal boundaries around each word in\nthe candidate text. Our method trains the UNet with groundtruth heatmaps that\nwe obtain from text bounding polygons provided by groundtruth annotations. Our\ntext spotting framework, called UHTA, combines UHT with the state-of-the-art\ntext recognition system ASTER. Experiments on four challenging and public\nscene-text-detection datasets (Total-Text, SCUT-CTW1500, MSRA-TD500, and\nCOCO-Text) show the effectiveness and generalization ability of UHT in\ndetecting not only multilingual (potentially rotated) straight but also curved\ntext in scripts of multiple languages. Our experimental results of UHTA on the\nTotal-Text dataset show that UHTA outperforms four state-of-the-art text\nspotting frameworks by at least 9.1 percent points in the F-measure, which\nsuggests that UHTA may be used as a complete text detection and recognition\nsystem in real applications.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 15:23:32 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 15:56:31 GMT"}, {"version": "v3", "created": "Thu, 28 May 2020 03:04:47 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Wang", "Qitong", ""], ["Zheng", "Yi", ""], ["Betke", "Margrit", ""]]}, {"id": "1911.07050", "submitter": "Kamran Ali", "authors": "Kamran Ali, Charles E. Hughes", "title": "All-In-One: Facial Expression Transfer, Editing and Recognition Using A\n  Single Network", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a unified architecture known as Transfer-Editing\nand Recognition Generative Adversarial Network (TER-GAN) which can be used: 1.\nto transfer facial expressions from one identity to another identity, known as\nFacial Expression Transfer (FET), 2. to transform the expression of a given\nimage to a target expression, while preserving the identity of the image, known\nas Facial Expression Editing (FEE), and 3. to recognize the facial expression\nof a face image, known as Facial Expression Recognition (FER). In TER-GAN, we\ncombine the capabilities of generative models to generate synthetic images,\nwhile learning important information about the input images during the\nreconstruction process. More specifically, two encoders are used in TER-GAN to\nencode identity and expression information from two input images, and a\nsynthetic expression image is generated by the decoder part of TER-GAN. To\nimprove the feature disentanglement and extraction process, we also introduce a\nnovel expression consistency loss and an identity consistency loss which\nexploit extra expression and identity information from generated images.\nExperimental results show that the proposed method can be used for efficient\nfacial expression transfer, facial expression editing and facial expression\nrecognition. In order to evaluate the proposed technique and to compare our\nresults with state-of-the-art methods, we have used the Oulu-CASIA dataset for\nour experiments.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 15:58:16 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Ali", "Kamran", ""], ["Hughes", "Charles E.", ""]]}, {"id": "1911.07053", "submitter": "Bowen Zhao", "authors": "Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, Shutao Xia", "title": "Maintaining Discrimination and Fairness in Class Incremental Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been applied in class incremental learning,\nwhich aims to solve common real-world problems of learning new classes\ncontinually. One drawback of standard DNNs is that they are prone to\ncatastrophic forgetting. Knowledge distillation (KD) is a commonly used\ntechnique to alleviate this problem. In this paper, we demonstrate it can\nindeed help the model to output more discriminative results within old classes.\nHowever, it cannot alleviate the problem that the model tends to classify\nobjects into new classes, causing the positive effect of KD to be hidden and\nlimited. We observed that an important factor causing catastrophic forgetting\nis that the weights in the last fully connected (FC) layer are highly biased in\nclass incremental learning. In this paper, we propose a simple and effective\nsolution motivated by the aforementioned observations to address catastrophic\nforgetting. Firstly, we utilize KD to maintain the discrimination within old\nclasses. Then, to further maintain the fairness between old classes and new\nclasses, we propose Weight Aligning (WA) that corrects the biased weights in\nthe FC layer after normal training process. Unlike previous work, WA does not\nrequire any extra parameters or a validation set in advance, as it utilizes the\ninformation provided by the biased weights themselves. The proposed method is\nevaluated on ImageNet-1000, ImageNet-100, and CIFAR-100 under various settings.\nExperimental results show that the proposed method can effectively alleviate\ncatastrophic forgetting and significantly outperform state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 16:05:53 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Zhao", "Bowen", ""], ["Xiao", "Xi", ""], ["Gan", "Guojun", ""], ["Zhang", "Bin", ""], ["Xia", "Shutao", ""]]}, {"id": "1911.07067", "submitter": "Debesh Jha", "authors": "Debesh Jha, Pia H. Smedsrud, Michael A. Riegler, Dag Johansen, Thomas\n  de Lange, Pal Halvorsen, Havard D. Johansen", "title": "ResUNet++: An Advanced Architecture for Medical Image Segmentation", "comments": "7 pages, 3 figures, 21st IEEE International Symposium on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate computer-aided polyp detection and segmentation during colonoscopy\nexaminations can help endoscopists resect abnormal tissue and thereby decrease\nchances of polyps growing into cancer. Towards developing a fully automated\nmodel for pixel-wise polyp segmentation, we propose ResUNet++, which is an\nimproved ResUNet architecture for colonoscopic image segmentation. Our\nexperimental evaluations show that the suggested architecture produces good\nsegmentation results on publicly available datasets. Furthermore, ResUNet++\nsignificantly outperforms U-Net and ResUNet, two key state-of-the-art deep\nlearning architectures, by achieving high evaluation scores with a dice\ncoefficient of 81.33%, and a mean Intersection over Union (mIoU) of 79.27% for\nthe Kvasir-SEG dataset and a dice coefficient of 79.55%, and a mIoU of 79.62%\nwith CVC-612 dataset.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 18:04:17 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Jha", "Debesh", ""], ["Smedsrud", "Pia H.", ""], ["Riegler", "Michael A.", ""], ["Johansen", "Dag", ""], ["de Lange", "Thomas", ""], ["Halvorsen", "Pal", ""], ["Johansen", "Havard D.", ""]]}, {"id": "1911.07068", "submitter": "Owain Evans", "authors": "Owain Evans", "title": "Sensory Optimization: Neural Networks as a Model for Understanding and\n  Creating Art", "comments": "27 pages. Web version with high-resolution images:\n  https://owainevans.github.io/visual_aesthetics/sensory-optimization.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is about the cognitive science of visual art. Artists create\nphysical artifacts (such as sculptures or paintings) which depict people,\nobjects, and events. These depictions are usually stylized rather than\nphoto-realistic. How is it that humans are able to understand and create\nstylized representations? Does this ability depend on general cognitive\ncapacities or an evolutionary adaptation for art? What role is played by\nlearning and culture?\n  Machine Learning can shed light on these questions. It's possible to train\nconvolutional neural networks (CNNs) to recognize objects without training them\non any visual art. If such CNNs can generalize to visual art (by creating and\nunderstanding stylized representations), then CNNs provide a model for how\nhumans could understand art without innate adaptations or cultural learning. I\nargue that Deep Dream and Style Transfer show that CNNs can create a basic form\nof visual art, and that humans could create art by similar processes. This\nsuggests that artists make art by optimizing for effects on the human\nobject-recognition system. Physical artifacts are optimized to evoke real-world\nobjects for this system (e.g. to evoke people or landscapes) and to serve as\nsuperstimuli for this system.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 18:10:00 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Evans", "Owain", ""]]}, {"id": "1911.07069", "submitter": "Debesh Jha", "authors": "Debesh Jha, Pia H. Smedsrud, Michael A. Riegler, P{\\aa}l Halvorsen,\n  Thomas de Lange, Dag Johansen, H{\\aa}vard D. Johansen", "title": "Kvasir-SEG: A Segmented Polyp Dataset", "comments": "12 pages, 4 figures, 26TH INTERNATIONAL CONFERENCE ON MULTIMEDIA\n  MODELING", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pixel-wise image segmentation is a highly demanding task in medical-image\nanalysis. In practice, it is difficult to find annotated medical images with\ncorresponding segmentation masks. In this paper, we present Kvasir-SEG: an\nopen-access dataset of gastrointestinal polyp images and corresponding\nsegmentation masks, manually annotated by a medical doctor and then verified by\nan experienced gastroenterologist. Moreover, we also generated the bounding\nboxes of the polyp regions with the help of segmentation masks. We demonstrate\nthe use of our dataset with a traditional segmentation approach and a modern\ndeep-learning based Convolutional Neural Network (CNN) approach. The dataset\nwill be of value for researchers to reproduce results and compare methods. By\nadding segmentation masks to the Kvasir dataset, which only provide frame-wise\nannotations, we enable multimedia and computer vision researchers to contribute\nin the field of polyp segmentation and automatic analysis of colonoscopy\nimages.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 18:13:43 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Jha", "Debesh", ""], ["Smedsrud", "Pia H.", ""], ["Riegler", "Michael A.", ""], ["Halvorsen", "P\u00e5l", ""], ["de Lange", "Thomas", ""], ["Johansen", "Dag", ""], ["Johansen", "H\u00e5vard D.", ""]]}, {"id": "1911.07072", "submitter": "Xuefei Cao", "authors": "Xuefei Cao, Bor-Chun Chen, Ser-Nam Lim", "title": "Unsupervised Deep Metric Learning via Auxiliary Rotation Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep metric learning is an important area due to its applicability to many\ndomains such as image retrieval and person re-identification. The main drawback\nof such models is the necessity for labeled data. In this work, we propose to\ngenerate pseudo-labels for deep metric learning directly from clustering\nassignment and we introduce unsupervised deep metric learning (UDML)\nregularized by a self-supervision (SS) task. In particular, we propose to\nregularize the training process by predicting image rotations. Our method\n(UDML-SS) jointly learns discriminative embeddings, unsupervised clustering\nassignments of the embeddings, as well as a self-supervised pretext task.\nUDML-SS iteratively cluster embeddings using traditional clustering algorithm\n(e.g., k-means), and sampling training pairs based on the cluster assignment\nfor metric learning, while optimizing self-supervised pretext task in a\nmulti-task fashion. The role of self-supervision is to stabilize the training\nprocess and encourages the model to learn meaningful feature representations\nthat are not distorted due to unreliable clustering assignments. The proposed\nmethod performs well on standard benchmarks for metric learning, where it\noutperforms current state-of-the-art approaches by a large margin and it also\nshows competitive performance with various metric learning loss functions.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 18:28:45 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Cao", "Xuefei", ""], ["Chen", "Bor-Chun", ""], ["Lim", "Ser-Nam", ""]]}, {"id": "1911.07086", "submitter": "Saeid Asgari Taghanaki", "authors": "Saeid Asgari Taghanaki, Kumar Abhishek, Ghassan Hamarneh", "title": "Signed Input Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over-parameterized deep models usually over-fit to a given training\ndistribution, which makes them sensitive to small changes and\nout-of-distribution samples at inference time, leading to low generalization\nperformance. To this end, several model-based and randomized data-dependent\nregularization methods are applied, such as data augmentation, which prevents a\nmodel from memorizing the training distribution. Instead of the random\ntransformation of the input images, we propose SIGN, a new regularization\nmethod, which modifies the input variables using a linear transformation by\nestimating each variable's contribution to the final prediction. Our proposed\ntechnique maps the input data to a new manifold where the less important\nvariables are de-emphasized. To test the effectiveness of the proposed idea and\ncompare it with other competing methods, we design several test scenarios, such\nas classification performance, uncertainty, out-of-distribution, and robustness\nanalyses. We compare the methods using three different datasets and four\nmodels. We find that SIGN encourages more compact class representations, which\nresults in the model's robustness to random corruptions and out-of-distribution\nsamples while also simultaneously achieving superior performance on normal data\ncompared to other competing methods. Our experiments also demonstrate the\nsuccessful transferability of the SIGN samples from one model to another.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 19:56:43 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 18:44:33 GMT"}, {"version": "v3", "created": "Wed, 11 Dec 2019 05:09:35 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Taghanaki", "Saeid Asgari", ""], ["Abhishek", "Kumar", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "1911.07088", "submitter": "Xiaoyuan Guo", "authors": "Xiaoyuan Guo, Fusheng Wang, George Teodorou, Alton B. Farris, and Jun\n  Kong", "title": "Liver Steatosis Segmentation with Deep Learning Methods", "comments": "4 pages", "journal-ref": "2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI\n  2019) Venice, Italy, April 8-11, 2019", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Liver steatosis is known as the abnormal accumulation of lipids within cells.\nAn accurate quantification of steatosis area within the liver histopathological\nmicroscopy images plays an important role in liver disease diagnosis and\ntrans-plantation assessment. Such a quantification analysis often requires a\nprecise steatosis segmentation that is challenging due to abundant presence of\nhighly overlapped steatosis droplets. In this paper, a deep learning model\nMask-RCNN is used to segment the steatosis droplets in clumps. Extended from\nFaster R-CNN, Mask-RCNN can predict object masks in addition to bounding box\ndetection. With transfer learning, the resulting model is able to segment\noverlapped steatosis regions at 75.87% by Average Precision, 60.66% by\nRecall,65.88% by F1-score, and 76.97% by Jaccard index, promising to support\nliver disease diagnosis and allograft rejection prediction in future clinical\npractice.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 20:04:09 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Guo", "Xiaoyuan", ""], ["Wang", "Fusheng", ""], ["Teodorou", "George", ""], ["Farris", "Alton B.", ""], ["Kong", "Jun", ""]]}, {"id": "1911.07107", "submitter": "He Wang", "authors": "He Wang, Feixiang He, Zhexi Peng, Yongliang Yang, Tianjia Shao, Kun\n  Zhou, David Hogg", "title": "SMART: Skeletal Motion Action Recognition aTtack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attack has inspired great interest in computer vision, by showing\nthat classification-based solutions are prone to imperceptible attack in many\ntasks. In this paper, we propose a method, SMART, to attack action recognizers\nwhich rely on 3D skeletal motions. Our method involves an innovative perceptual\nloss which ensures the imperceptibility of the attack. Empirical studies\ndemonstrate that SMART is effective in both white-box and black-box scenarios.\nIts generalizability is evidenced on a variety of action recognizers and\ndatasets. Its versatility is shown in different attacking strategies. Its\ndeceitfulness is proven in extensive perceptual studies. Finally, SMART shows\nthat adversarial attack on 3D skeletal motion, one type of time-series data, is\nsignificantly different from traditional adversarial attack problems.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 22:25:29 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 13:15:49 GMT"}, {"version": "v3", "created": "Tue, 10 Mar 2020 13:12:04 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Wang", "He", ""], ["He", "Feixiang", ""], ["Peng", "Zhexi", ""], ["Yang", "Yongliang", ""], ["Shao", "Tianjia", ""], ["Zhou", "Kun", ""], ["Hogg", "David", ""]]}, {"id": "1911.07144", "submitter": "Qingchao Zhang", "authors": "Qingchao Zhang and Yunmei Chen", "title": "Extra Proximal-Gradient Inspired Non-local Network", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational method and deep learning method are two mainstream powerful\napproaches to solve inverse problems in computer vision. To take advantages of\nadvanced optimization algorithms and powerful representation ability of deep\nneural networks, we propose a novel deep network for image reconstruction. The\narchitecture of this network is inspired by our proposed accelerated extra\nproximal gradient algorithm. It is able to incorporate non-local operation to\nexploit the non-local self-similarity of the images and to learn the nonlinear\ntransform, under which the solution is sparse. All the parameters in our\nnetwork are learned from minimizing a loss function. Our experimental results\nshow that our network outperforms several state-of-the-art deep networks with\nalmost the same number of learnable parameter.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 03:29:17 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Zhang", "Qingchao", ""], ["Chen", "Yunmei", ""]]}, {"id": "1911.07158", "submitter": "Fuxun Yu", "authors": "Fuxun Yu, Di Wang, Yinpeng Chen, Nikolaos Karianakis, Tong Shen, Pei\n  Yu, Dimitrios Lymberopoulos, Sidi Lu, Weisong Shi, Xiang Chen", "title": "Unsupervised Domain Adaptation for Object Detection via Cross-Domain\n  Semi-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current state-of-the-art object detectors can have significant performance\ndrop when deployed in the wild due to domain gaps with training data.\nUnsupervised Domain Adaptation (UDA) is a promising approach to adapt models\nfor new domains/environments without any expensive label cost. However, without\nground truth labels, most prior works on UDA for object detection tasks can\nonly perform coarse image-level and/or feature-level adaptation by using\nadversarial learning methods. In this work, we show that such adversarial-based\nmethods can only reduce the domain style gap, but cannot address the domain\ncontent distribution gap that is shown to be important for object detectors. To\novercome this limitation, we propose the Cross-Domain Semi-Supervised Learning\n(CDSSL) framework by leveraging high-quality pseudo labels to learn better\nrepresentations from the target domain directly. To enable SSL for cross-domain\nobject detection, we propose fine-grained domain transfer,\nprogressive-confidence-based label sharpening and imbalanced sampling strategy\nto address two challenges: (i) non-identical distribution between source and\ntarget domain data, (ii) error amplification/accumulation due to noisy pseudo\nlabeling on the target domain. Experiment results show that our proposed\napproach consistently achieves new state-of-the-art performance (2.2% - 9.5%\nbetter than prior best work on mAP) under various domain gap scenarios. The\ncode will be released.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 05:59:33 GMT"}, {"version": "v2", "created": "Sun, 24 Nov 2019 23:39:46 GMT"}, {"version": "v3", "created": "Sun, 22 Nov 2020 05:07:13 GMT"}, {"version": "v4", "created": "Thu, 25 Feb 2021 23:34:17 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Yu", "Fuxun", ""], ["Wang", "Di", ""], ["Chen", "Yinpeng", ""], ["Karianakis", "Nikolaos", ""], ["Shen", "Tong", ""], ["Yu", "Pei", ""], ["Lymberopoulos", "Dimitrios", ""], ["Lu", "Sidi", ""], ["Shi", "Weisong", ""], ["Chen", "Xiang", ""]]}, {"id": "1911.07160", "submitter": "Ziyi Kou", "authors": "Ziyi Kou, Guofeng Cui, Shaojie Wang, Wentian Zhao, Chenliang Xu", "title": "Improve CAM with Auto-adapted Segmentation and Co-supervised\n  Augmentation", "comments": "Accepted by WACV2021. Equal contribution for the first two authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly Supervised Object Localization (WSOL) methods generate both\nclassification and localization results by learning from only image category\nlabels. Previous methods usually utilize class activation map (CAM) to obtain\ntarget object regions. However, most of them only focus on improving foreground\nobject parts in CAM, but ignore the important effect of its background\ncontents. In this paper, we propose a confidence segmentation (ConfSeg) module\nthat builds confidence score for each pixel in CAM without introducing\nadditional hyper-parameters. The generated sample-specific confidence mask is\nable to indicate the extent of determination for each pixel in CAM, and further\nsupervises additional CAM extended from internal feature maps. Besides, we\nintroduce Co-supervised Augmentation (CoAug) module to capture feature-level\nrepresentation for foreground and background parts in CAM separately. Then a\nmetric loss is applied at batch sample level to augment distinguish ability of\nour model, which helps a lot to localize more related object parts. Our final\nmodel, CSoA, combines the two modules and achieves superior performance, e.g.\n$37.69\\%$ and $48.81\\%$ Top-1 localization error on CUB-200 and ILSVRC\ndatasets, respectively, which outperforms all previous methods and becomes the\nnew state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 06:12:36 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 02:03:14 GMT"}, {"version": "v3", "created": "Mon, 11 May 2020 19:36:18 GMT"}, {"version": "v4", "created": "Thu, 14 May 2020 18:46:35 GMT"}, {"version": "v5", "created": "Wed, 13 Jan 2021 15:15:37 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Kou", "Ziyi", ""], ["Cui", "Guofeng", ""], ["Wang", "Shaojie", ""], ["Zhao", "Wentian", ""], ["Xu", "Chenliang", ""]]}, {"id": "1911.07163", "submitter": "Jian Wang", "authors": "Yilang Zhang, Neal N. Xiong, Zheng Wei, Xin Yuan and Jian Wang", "title": "ADCC: An Effective and Intelligent Attention Dense Color Constancy\n  System for Studying Images in Smart Cities", "comments": "8 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a novel method eliminating chromatic aberration on objects, computational\ncolor constancy has becoming a fundamental prerequisite for many computer\nvision applications. Among algorithms performing this task, the learning-based\nones have achieved great success in recent years. However, they fail to fully\nconsider the spatial information of images, leaving plenty of room for\nimprovement of the accuracy of illuminant estimation. In this paper, by\nexploiting the spatial information of images, we propose a color constancy\nalgorithm called Attention Dense Color Constancy (ADCC) using convolutional\nneural network (CNN). Specifically, based on the 2D log-chrominance histograms\nof the input images as well as their specially augmented ones, ADCC estimates\nthe illuminant with a self-attention DenseNet. The augmented images help to\ntell apart the edge gradients, edge pixels and non-edge ones in log-histogram,\nwhich contribute significantly to the feature extraction and color-ambiguity\nelimination, thereby advancing the accuracy of illuminant estimation.\nSimulations and experiments on benchmark datasets demonstrate that the proposed\nalgorithm is effective for illuminant estimation compared to the\nstate-of-the-art methods. Thus, ADCC offers great potential in promoting\napplications of smart cities, such as smart camera, where color is an important\nfactor for distinguishing objects.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 06:36:39 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 08:21:50 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Zhang", "Yilang", ""], ["Xiong", "Neal N.", ""], ["Wei", "Zheng", ""], ["Yuan", "Xin", ""], ["Wang", "Jian", ""]]}, {"id": "1911.07164", "submitter": "Satoshi Tsutsui", "authors": "Satoshi Tsutsui, Yanwei Fu, David Crandall", "title": "Meta-Reinforced Synthetic Data for One-Shot Fine-Grained Visual\n  Recognition", "comments": "Accepted by Conference on Neural Information Processing System 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-shot fine-grained visual recognition often suffers from the problem of\ntraining data scarcity for new fine-grained classes. To alleviate this problem,\nan off-the-shelf image generator can be applied to synthesize additional\ntraining images, but these synthesized images are often not helpful for\nactually improving the accuracy of one-shot fine-grained recognition. This\npaper proposes a meta-learning framework to combine generated images with\noriginal images, so that the resulting ``hybrid'' training images can improve\none-shot learning. Specifically, the generic image generator is updated by a\nfew training instances of novel classes, and a Meta Image Reinforcing Network\n(MetaIRNet) is proposed to conduct one-shot fine-grained recognition as well as\nimage reinforcement. The model is trained in an end-to-end manner, and our\nexperiments demonstrate consistent improvement over baselines on one-shot\nfine-grained image classification benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 06:37:58 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Tsutsui", "Satoshi", ""], ["Fu", "Yanwei", ""], ["Crandall", "David", ""]]}, {"id": "1911.07167", "submitter": "Gregory Vaksman", "authors": "Gregory Vaksman, Michael Elad, Peyman Milanfar", "title": "LIDIA: Lightweight Learned Image Denoising with Instance Adaptation", "comments": null, "journal-ref": null, "doi": "10.1109/CVPRW50498.2020.00270", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image denoising is a well studied problem with an extensive activity that has\nspread over several decades. Despite the many available denoising algorithms,\nthe quest for simple, powerful and fast denoisers is still an active and\nvibrant topic of research. Leading classical denoising methods are typically\ndesigned to exploit the inner structure in images by modeling local overlapping\npatches, while operating in an unsupervised fashion. In contrast, recent\nnewcomers to this arena are supervised and universal neural-network-based\nmethods that bypass this modeling altogether, targeting the inference goal\ndirectly and globally, while tending to be very deep and parameter heavy.\n  This work proposes a novel lightweight learnable architecture for image\ndenoising, and presents a combination of supervised and unsupervised training\nof it, the first aiming for a universal denoiser and the second for adapting it\nto the incoming image. Our architecture embeds in it several of the main\nconcepts taken from classical methods, relying on patch processing, leveraging\nnon-local self-similarity, exploiting representation sparsity and providing a\nmultiscale treatment. Our proposed universal denoiser achieves near\nstate-of-the-art results, while using a small fraction of the typical number of\nparameters. In addition, we introduce and demonstrate two highly effective ways\nfor further boosting the denoising performance, by adapting this universal\nnetwork to the input image.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 06:56:47 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2020 20:12:42 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Vaksman", "Gregory", ""], ["Elad", "Michael", ""], ["Milanfar", "Peyman", ""]]}, {"id": "1911.07171", "submitter": "Ruoyu Guo", "authors": "Ruoyu Guo, Cheng Cui, Yuning Du, Xianglong Meng, Xiaodi Wang, Jingwei\n  Liu, Jianfeng Zhu, Yuan Feng, Shumin Han", "title": "2nd Place Solution in Google AI Open Images Object Detection Track 2019", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an object detection framework based on PaddlePaddle. We put all\nthe strategies together (multi-scale training, FPN, Cascade, Dcnv2, Non-local,\nlibra loss) based on ResNet200-vd backbone. Our model score on public\nleaderboard comes to 0.6269 with single scale test. We proposed a new voting\nmethod called top-k voting-nms, based on the SoftNMS detection results. The\nvoting method helps us merge all the models' results more easily and achieve\n2nd place in the Google AI Open Images Object Detection Track 2019.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 07:12:47 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Guo", "Ruoyu", ""], ["Cui", "Cheng", ""], ["Du", "Yuning", ""], ["Meng", "Xianglong", ""], ["Wang", "Xiaodi", ""], ["Liu", "Jingwei", ""], ["Zhu", "Jianfeng", ""], ["Feng", "Yuan", ""], ["Han", "Shumin", ""]]}, {"id": "1911.07177", "submitter": "Jian Wang", "authors": "Yiyao Shi, Jian Wang, Xiangyang Xue", "title": "Fast Color Constancy with Patch-wise Bright Pixels", "comments": "7 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a learning-free color constancy algorithm called the\nPatch-wise Bright Pixels (PBP) is proposed. In this algorithm, an input image\nis first downsampled and then cut equally into a few patches. After that,\naccording to the modified brightness of each patch, a proper fraction of\nbrightest pixels in the patch is selected. Finally, Gray World (GW)-based\nmethods are applied to the selected bright pixels to estimate the illuminant of\nthe scene. Experiments on NUS $8$-Camera Dataset show that the PBP algorithm\noutperforms the state-of-the-art learning-free methods as well as a broad range\nof learning-based ones. In particular, PBP processes a $1080$p image within two\nmilliseconds, which is hundreds of times faster than the existing learning-free\nones. Our algorithm offers a potential solution to the full-screen smart phones\nwhose screen-to-body ratio is $100$\\%.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 07:55:42 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Shi", "Yiyao", ""], ["Wang", "Jian", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1911.07185", "submitter": "Qianwei Zhou", "authors": "Qianwei Zhou, Chen Zhou, Haigen Hu, Yuhang Chen, Shengyong Chen, and\n  Xiaoxin Li", "title": "Towards the Automation of Deep Image Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image inverse problem is a notoriously challenging ill-posed problem\nthat aims to restore the original image from one of its corrupted versions.\nRecently, this field has been immensely influenced by the emergence of\ndeep-learning techniques. Deep Image Prior (DIP) offers a new approach that\nforces the recovered image to be synthesized from a given deep architecture.\nWhile DIP is quite an effective unsupervised approach, it is deprecated in\nreal-world applications because of the requirement of human assistance. In this\nwork, we aim to find the best-recovered image without the assistance of humans\nby adding a stopping criterion, which will reach maximum when the iteration no\nlonger improves the image quality. More specifically, we propose to add a\npseudo noise to the corrupted image and measure the pseudo-noise component in\nthe recovered image by the orthogonality between signal and noise. The accuracy\nof the orthogonal stopping criterion has been demonstrated for several tested\nproblems such as denoising, super-resolution, and inpainting, in which 38 out\nof 40 experiments are higher than 95%.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 08:28:42 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Zhou", "Qianwei", ""], ["Zhou", "Chen", ""], ["Hu", "Haigen", ""], ["Chen", "Yuhang", ""], ["Chen", "Shengyong", ""], ["Li", "Xiaoxin", ""]]}, {"id": "1911.07190", "submitter": "Evgenii Zheltonozhskii", "authors": "Yury Nahshan, Brian Chmiel, Chaim Baskin, Evgenii Zheltonozhskii, Ron\n  Banner, Alex M. Bronstein, Avi Mendelson", "title": "Loss Aware Post-training Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural network quantization enables the deployment of large models on\nresource-constrained devices. Current post-training quantization methods fall\nshort in terms of accuracy for INT4 (or lower) but provide reasonable accuracy\nfor INT8 (or above). In this work, we study the effect of quantization on the\nstructure of the loss landscape. Additionally, we show that the structure is\nflat and separable for mild quantization, enabling straightforward\npost-training quantization methods to achieve good results. We show that with\nmore aggressive quantization, the loss landscape becomes highly non-separable\nwith steep curvature, making the selection of quantization parameters more\nchallenging. Armed with this understanding, we design a method that quantizes\nthe layer parameters jointly, enabling significant accuracy improvement over\ncurrent post-training quantization methods. Reference implementation is\navailable at\nhttps://github.com/ynahshan/nn-quantization-pytorch/tree/master/lapq\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 09:10:23 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 09:23:27 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Nahshan", "Yury", ""], ["Chmiel", "Brian", ""], ["Baskin", "Chaim", ""], ["Zheltonozhskii", "Evgenii", ""], ["Banner", "Ron", ""], ["Bronstein", "Alex M.", ""], ["Mendelson", "Avi", ""]]}, {"id": "1911.07192", "submitter": "Qin Zou", "authors": "Qin Zou, Zheng Zhang, Ling Cao, Long Chen, Song Wang", "title": "Transductive Zero-Shot Hashing for Multilabel Image Retrieval", "comments": "15 pages", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, 2020", "doi": "10.1109/TNNLS.2020.3043298", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Hash coding has been widely used in approximate nearest neighbor search for\nlarge-scale image retrieval. Given semantic annotations such as class labels\nand pairwise similarities of the training data, hashing methods can learn and\ngenerate effective and compact binary codes. While some newly introduced images\nmay contain undefined semantic labels, which we call unseen images, zeor-shot\nhashing techniques have been studied. However, existing zeor-shot hashing\nmethods focus on the retrieval of single-label images, and cannot handle\nmulti-label images. In this paper, for the first time, a novel transductive\nzero-shot hashing method is proposed for multi-label unseen image retrieval. In\norder to predict the labels of the unseen/target data, a visual-semantic bridge\nis built via instance-concept coherence ranking on the seen/source data. Then,\npairwise similarity loss and focal quantization loss are constructed for\ntraining a hashing model using both the seen/source and unseen/target data.\nExtensive evaluations on three popular multi-label datasets demonstrate that,\nthe proposed hashing method achieves significantly better results than the\ncompeting methods.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 09:21:14 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 05:09:10 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Zou", "Qin", ""], ["Zhang", "Zheng", ""], ["Cao", "Ling", ""], ["Chen", "Long", ""], ["Wang", "Song", ""]]}, {"id": "1911.07198", "submitter": "Evgenii Zheltonozhskii", "authors": "Yaniv Nemcovsky, Evgenii Zheltonozhskii, Chaim Baskin, Brian Chmiel,\n  Maxim Fishman, Alex M. Bronstein, Avi Mendelson", "title": "Smoothed Inference for Adversarially-Trained Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep neural networks are known to be vulnerable to adversarial attacks.\nCurrent methods of defense from such attacks are based on either implicit or\nexplicit regularization, e.g., adversarial training. Randomized smoothing, the\naveraging of the classifier outputs over a random distribution centered in the\nsample, has been shown to guarantee the performance of a classifier subject to\nbounded perturbations of the input. In this work, we study the application of\nrandomized smoothing as a way to improve performance on unperturbed data as\nwell as to increase robustness to adversarial attacks. The proposed technique\ncan be applied on top of any existing adversarial defense, but works\nparticularly well with the randomized approaches. We examine its performance on\ncommon white-box (PGD) and black-box (transfer and NAttack) attacks on CIFAR-10\nand CIFAR-100, substantially outperforming previous art for most scenarios and\ncomparable on others. For example, we achieve 60.4% accuracy under a PGD attack\non CIFAR-10 using ResNet-20, outperforming previous art by 11.7%. Since our\nmethod is based on sampling, it lends itself well for trading-off between the\nmodel inference complexity and its performance. A reference implementation of\nthe proposed techniques is provided at https://github.com/yanemcovsky/SIAM\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 09:38:45 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 14:13:03 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Nemcovsky", "Yaniv", ""], ["Zheltonozhskii", "Evgenii", ""], ["Baskin", "Chaim", ""], ["Chmiel", "Brian", ""], ["Fishman", "Maxim", ""], ["Bronstein", "Alex M.", ""], ["Mendelson", "Avi", ""]]}, {"id": "1911.07201", "submitter": "Manish Agnihotri", "authors": "Aman Apte, Aritra Bandyopadhyay, K Akhilesh Shenoy, Jason Peter\n  Andrews, Aditya Rathod, Manish Agnihotri, Aditya Jajodia", "title": "Countering Inconsistent Labelling by Google's Vision API for Rotated\n  Images", "comments": "11 pages, 9 figures, Accepted at ICICV 2020 Jaipur India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Google's Vision API analyses images and provides a variety of output\npredictions, one such type is context-based labelling. In this paper, it is\nshown that adversarial examples that cause incorrect label prediction and\nspoofing can be generated by rotating the images. Due to the black-boxed nature\nof the API, a modular context-based pre-processing pipeline is proposed\nconsisting of a Res-Net50 model, that predicts the angle by which the image\nmust be rotated to correct its orientation. The pipeline successfully performs\nthe correction whilst maintaining the image's resolution and feeds it to the\nAPI which generates labels similar to the original correctly oriented image and\nusing a Percentage Error metric, the performance of the corrected images as\ncompared to its rotated counter-parts is found to be significantly higher.\nThese observations imply that the API can benefit from such a pre-processing\npipeline to increase robustness to rotational perturbances.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 09:49:58 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Apte", "Aman", ""], ["Bandyopadhyay", "Aritra", ""], ["Shenoy", "K Akhilesh", ""], ["Andrews", "Jason Peter", ""], ["Rathod", "Aditya", ""], ["Agnihotri", "Manish", ""], ["Jajodia", "Aditya", ""]]}, {"id": "1911.07217", "submitter": "Feifan Lv", "authors": "Haiyang Si, Zhiqiang Zhang, Feifan Lv, Gang Yu and Feng Lu", "title": "Real-Time Semantic Segmentation via Multiply Spatial Fusion Network", "comments": "This is an under review version with 9 pages and 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time semantic segmentation plays a significant role in industry\napplications, such as autonomous driving, robotics and so on. It is a\nchallenging task as both efficiency and performance need to be considered\nsimultaneously. To address such a complex task, this paper proposes an\nefficient CNN called Multiply Spatial Fusion Network (MSFNet) to achieve fast\nand accurate perception. The proposed MSFNet uses Class Boundary Supervision to\nprocess the relevant boundary information based on our proposed Multi-features\nFusion Module which can obtain spatial information and enlarge receptive field.\nTherefore, the final upsampling of the feature maps of 1/8 original image size\ncan achieve impressive results while maintaining a high speed. Experiments on\nCityscapes and Camvid datasets show an obvious advantage of the proposed\napproach compared with the existing approaches. Specifically, it achieves 77.1%\nMean IOU on the Cityscapes test dataset with the speed of 41 FPS for a\n1024*2048 input, and 75.4% Mean IOU with the speed of 91 FPS on the Camvid test\ndataset.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 12:10:40 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Si", "Haiyang", ""], ["Zhang", "Zhiqiang", ""], ["Lv", "Feifan", ""], ["Yu", "Gang", ""], ["Lu", "Feng", ""]]}, {"id": "1911.07241", "submitter": "Dongyan Guo", "authors": "Dongyan Guo, Jun Wang, Ying Cui, Zhenhua Wang, Shengyong Chen", "title": "SiamCAR: Siamese Fully Convolutional Classification and Regression for\n  Visual Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By decomposing the visual tracking task into two subproblems as\nclassification for pixel category and regression for object bounding box at\nthis pixel, we propose a novel fully convolutional Siamese network to solve\nvisual tracking end-to-end in a per-pixel manner. The proposed framework\nSiamCAR consists of two simple subnetworks: one Siamese subnetwork for feature\nextraction and one classification-regression subnetwork for bounding box\nprediction. Our framework takes ResNet-50 as backbone. Different from\nstate-of-the-art trackers like Siamese-RPN, SiamRPN++ and SPM, which are based\non region proposal, the proposed framework is both proposal and anchor free.\nConsequently, we are able to avoid the tricky hyper-parameter tuning of anchors\nand reduce human intervention. The proposed framework is simple, neat and\neffective. Extensive experiments and comparisons with state-of-the-art trackers\nare conducted on many challenging benchmarks like GOT-10K, LaSOT, UAV123 and\nOTB-50. Without bells and whistles, our SiamCAR achieves the leading\nperformance with a considerable real-time speed.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 14:03:12 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 03:37:39 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Guo", "Dongyan", ""], ["Wang", "Jun", ""], ["Cui", "Ying", ""], ["Wang", "Zhenhua", ""], ["Chen", "Shengyong", ""]]}, {"id": "1911.07246", "submitter": "Youngwoon Lee", "authors": "Youngwoon Lee, Edward S. Hu, Zhengyu Yang, Alex Yin, and Joseph J. Lim", "title": "IKEA Furniture Assembly Environment for Long-Horizon Complex\n  Manipulation Tasks", "comments": "Simulator", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The IKEA Furniture Assembly Environment is one of the first benchmarks for\ntesting and accelerating the automation of complex manipulation tasks. The\nenvironment is designed to advance reinforcement learning from simple toy tasks\nto complex tasks requiring both long-term planning and sophisticated low-level\ncontrol. Our environment supports over 80 different furniture models, Sawyer\nand Baxter robot simulation, and domain randomization. The IKEA Furniture\nAssembly Environment is a testbed for methods aiming to solve complex\nmanipulation tasks. The environment is publicly available at\nhttps://clvrai.com/furniture\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 14:32:20 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Lee", "Youngwoon", ""], ["Hu", "Edward S.", ""], ["Yang", "Zhengyu", ""], ["Yin", "Alex", ""], ["Lim", "Joseph J.", ""]]}, {"id": "1911.07251", "submitter": "Xiaoze Jiang", "authors": "Xiaoze Jiang, Jing Yu, Zengchang Qin, Yingying Zhuang, Xingxing Zhang,\n  Yue Hu, Qi Wu", "title": "DualVD: An Adaptive Dual Encoding Model for Deep Visual Understanding in\n  Visual Dialogue", "comments": "Accepted by the Thirty-Fourth AAAI Conference on Artificial\n  Intelligence (AAAI-2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different from Visual Question Answering task that requires to answer only\none question about an image, Visual Dialogue involves multiple questions which\ncover a broad range of visual content that could be related to any objects,\nrelationships or semantics. The key challenge in Visual Dialogue task is thus\nto learn a more comprehensive and semantic-rich image representation which may\nhave adaptive attentions on the image for variant questions. In this research,\nwe propose a novel model to depict an image from both visual and semantic\nperspectives. Specifically, the visual view helps capture the appearance-level\ninformation, including objects and their relationships, while the semantic view\nenables the agent to understand high-level visual semantics from the whole\nimage to the local regions. Futhermore, on top of such multi-view image\nfeatures, we propose a feature selection framework which is able to adaptively\ncapture question-relevant information hierarchically in fine-grained level. The\nproposed method achieved state-of-the-art results on benchmark Visual Dialogue\ndatasets. More importantly, we can tell which modality (visual or semantic) has\nmore contribution in answering the current question by visualizing the gate\nvalues. It gives us insights in understanding of human cognition in Visual\nDialogue.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 14:58:17 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Jiang", "Xiaoze", ""], ["Yu", "Jing", ""], ["Qin", "Zengchang", ""], ["Zhuang", "Yingying", ""], ["Zhang", "Xingxing", ""], ["Hu", "Yue", ""], ["Wu", "Qi", ""]]}, {"id": "1911.07255", "submitter": "Amit Boyarski", "authors": "Amit Boyarski, Sanketh Vedula, Alex Bronstein", "title": "Spectral Geometric Matrix Completion", "comments": "Accepted to Mathematical and Scientific Machine Learning (MSML) 2021\n  https://msml21.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep Matrix Factorization (DMF) is an emerging approach to the problem of\nmatrix completion. Recent works have established that gradient descent applied\nto a DMF model induces an implicit regularization on the rank of the recovered\nmatrix. In this work we interpret the DMF model through the lens of spectral\ngeometry. This allows us to incorporate explicit regularization without\nbreaking the DMF structure, thus enjoying the best of both worlds. In\nparticular, we focus on matrix completion problems with underlying geometric or\ntopological relations between the rows and/or columns. Such relations are\nprevalent in matrix completion problems that arise in many applications, such\nas recommender systems and drug-target interaction. Our contributions enable\nDMF models to exploit these relations, and make them competitive on real\nbenchmarks, while exhibiting one of the first successful applications of deep\nlinear networks.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 15:06:34 GMT"}, {"version": "v2", "created": "Sun, 23 Feb 2020 12:10:30 GMT"}, {"version": "v3", "created": "Mon, 7 Jun 2021 14:27:53 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Boyarski", "Amit", ""], ["Vedula", "Sanketh", ""], ["Bronstein", "Alex", ""]]}, {"id": "1911.07257", "submitter": "Hao-Yun Chen", "authors": "Hao-Yun Chen, Li-Huang Tsai, Shih-Chieh Chang, Jia-Yu Pan, Yu-Ting\n  Chen, Wei Wei, Da-Cheng Juan", "title": "Learning with Hierarchical Complement Objective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label hierarchies widely exist in many vision-related problems, ranging from\nexplicit label hierarchies existed in image classification to latent label\nhierarchies existed in semantic segmentation. Nevertheless, state-of-the-art\nmethods often deploy cross-entropy loss that implicitly assumes class labels to\nbe exclusive and thus independence from each other. Motivated by the fact that\nclasses from the same parental category usually share certain similarity, we\ndesign a new training diagram called Hierarchical Complement Objective Training\n(HCOT) that leverages the information from label hierarchy. HCOT maximizes the\nprobability of the ground truth class, and at the same time, neutralizes the\nprobabilities of rest of the classes in a hierarchical fashion, making the\nmodel take advantage of the label hierarchy explicitly. The proposed HCOT is\nevaluated on both image classification and semantic segmentation tasks.\nExperimental results confirm that HCOT outperforms state-of-the-art models in\nCIFAR-100, ImageNet-2012, and PASCAL-Context. The study further demonstrates\nthat HCOT can be applied on tasks with latent label hierarchies, which is a\ncommon characteristic in many machine learning tasks.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 15:46:38 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Chen", "Hao-Yun", ""], ["Tsai", "Li-Huang", ""], ["Chang", "Shih-Chieh", ""], ["Pan", "Jia-Yu", ""], ["Chen", "Yu-Ting", ""], ["Wei", "Wei", ""], ["Juan", "Da-Cheng", ""]]}, {"id": "1911.07262", "submitter": "Renjiao Yi", "authors": "Renjiao Yi, Ping Tan, Stephen Lin", "title": "Leveraging Multi-view Image Sets for Unsupervised Intrinsic Image\n  Decomposition and Highlight Separation", "comments": "27 pages, with supplementary material, to appear in AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an unsupervised approach for factorizing object appearance into\nhighlight, shading, and albedo layers, trained by multi-view real images. To do\nso, we construct a multi-view dataset by collecting numerous customer product\nphotos online, which exhibit large illumination variations that make them\nsuitable for training of reflectance separation and can facilitate object-level\ndecomposition. The main contribution of our approach is a proposed image\nrepresentation based on local color distributions that allows training to be\ninsensitive to the local misalignments of multi-view images. In addition, we\npresent a new guidance cue for unsupervised training that exploits synergy\nbetween highlight separation and intrinsic image decomposition. Over a broad\nrange of objects, our technique is shown to yield state-of-the-art results for\nboth of these tasks.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 15:57:41 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Yi", "Renjiao", ""], ["Tan", "Ping", ""], ["Lin", "Stephen", ""]]}, {"id": "1911.07268", "submitter": "Mohammed Brahimi", "authors": "Mohammed Brahimi, Yvain Qu\\'eau, Bjoern Haefner and Daniel Cremers", "title": "On the well-posedness of uncalibrated photometric stereo under general\n  lighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncalibrated photometric stereo aims at estimating the 3D-shape of a surface,\ngiven a set of images captured from the same viewing angle, but under unknown,\nvarying illumination. While the theoretical foundations of this inverse problem\nunder directional lighting are well-established, there is a lack of\nmathematical evidence for the uniqueness of a solution under general lighting.\nOn the other hand, stable and accurate heuristical solutions of uncalibrated\nphotometric stereo under such general lighting have recently been proposed. The\nquality of the results demonstrated therein tends to indicate that the problem\nmay actually be well-posed, but this still has to be established. The present\npaper addresses this theoretical issue, considering first-order spherical\nharmonics approximation of general lighting. Two important theoretical results\nare established. First, the orthographic integrability constraint ensures\nuniqueness of a solution up to a global concave-convex ambiguity, which had\nalready been conjectured, yet not proven. Second, the perspective integrability\nconstraint makes the problem well-posed, which generalizes a previous result\nlimited to directional lighting. Eventually, a closed-form expression for the\nunique least-squares solution of the problem under perspective projection is\nprovided, allowing numerical simulations on synthetic data to empirically\nvalidate our findings.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 16:10:42 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 00:11:37 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Brahimi", "Mohammed", ""], ["Qu\u00e9au", "Yvain", ""], ["Haefner", "Bjoern", ""], ["Cremers", "Daniel", ""]]}, {"id": "1911.07272", "submitter": "Zhibo Wang", "authors": "Zhibo Wang, Shen Yan, Xiaoyu Zhang, Niels Lobo", "title": "Unsupervised Visual Representation Learning with Increasing Object Shape\n  Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  (Very early draft)Traditional supervised learning keeps pushing convolution\nneural network(CNN) achieving state-of-art performance. However, lack of\nlarge-scale annotation data is always a big problem due to the high cost of it,\neven ImageNet dataset is over-fitted by complex models now. The success of\nunsupervised learning method represented by the Bert model in natural language\nprocessing(NLP) field shows its great potential. And it makes that unlimited\ntraining samples becomes possible and the great universal generalization\nability changes NLP research direction directly. In this article, we purpose a\nnovel unsupervised learning method based on contrastive predictive coding.\nUnder that, we are able to train model with any non-annotation images and\nimprove model's performance to reach state-of-art performance at the same level\nof model complexity. Beside that, since the number of training images could be\nunlimited amplification, an universal large-scale pre-trained computer vision\nmodel is possible in the future.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 16:26:46 GMT"}, {"version": "v2", "created": "Sun, 24 Nov 2019 02:48:05 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Wang", "Zhibo", ""], ["Yan", "Shen", ""], ["Zhang", "Xiaoyu", ""], ["Lobo", "Niels", ""]]}, {"id": "1911.07273", "submitter": "Zhigang Chang", "authors": "Zhigang Chang, Qin Zhou, Mingyang Yu, Shibao Zheng, Hua Yang, Tai-Pang\n  Wu", "title": "Distribution Context Aware Loss for Person Re-identification", "comments": "IEEE VCIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To learn the optimal similarity function between probe and gallery images in\nPerson re-identification, effective deep metric learning methods have been\nextensively explored to obtain discriminative feature embedding. However,\nexisting metric loss like triplet loss and its variants always emphasize\npair-wise relations but ignore the distribution context in feature space,\nleading to inconsistency and sub-optimal. In fact, the similarity of one pair\nnot only decides the match of this pair, but also has potential impacts on\nother sample pairs. In this paper, we propose a novel Distribution Context\nAware (DCA) loss based on triplet loss to combine both numerical similarity and\nrelation similarity in feature space for better clustering. Extensive\nexperiments on three benchmarks including Market-1501, DukeMTMC-reID and\nMSMT17, evidence the favorable performance of our method against the\ncorresponding baseline and other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 16:28:35 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Chang", "Zhigang", ""], ["Zhou", "Qin", ""], ["Yu", "Mingyang", ""], ["Zheng", "Shibao", ""], ["Yang", "Hua", ""], ["Wu", "Tai-Pang", ""]]}, {"id": "1911.07279", "submitter": "Ekin Gedik", "authors": "Alessio Rosatelli, Ekin Gedik, Hayley Hung", "title": "Detecting F-formations & Roles in Crowded Social Scenes with Wearables:\n  Combining Proxemics & Dynamics using LSTMs", "comments": "2019 8th International Conference on Affective Computing and\n  Intelligent Interaction Workshops and Demos (ACIIW)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the use of proxemics and dynamics for\nautomatically identifying conversing groups, or so-called F-formations. More\nformally we aim to automatically identify whether wearable sensor data coming\nfrom 2 people is indicative of F-formation membership. We also explore the\nproblem of jointly detecting membership and more descriptive information about\nthe pair relating to the role they take in the conversation (i.e. speaker or\nlistener). We jointly model the concepts of proxemics and dynamics using binary\nproximity and acceleration obtained through a single wearable sensor per\nperson. We test our approaches on the publicly available MatchNMingle dataset\nwhich was collected during real-life mingling events. We find out that fusion\nof these two modalities performs significantly better than them independently,\nproviding an AUC of 0.975 when data from 30-second windows are used.\nFurthermore, our investigation into roles detection shows that each role pair\nrequires a different time resolution for accurate detection.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 16:43:44 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Rosatelli", "Alessio", ""], ["Gedik", "Ekin", ""], ["Hung", "Hayley", ""]]}, {"id": "1911.07308", "submitter": "Tsu-Jui Fu", "authors": "Tsu-Jui Fu, Xin Eric Wang, Matthew Peterson, Scott Grafton, Miguel\n  Eckstein, William Yang Wang", "title": "Counterfactual Vision-and-Language Navigation via Adversarial Path\n  Sampling", "comments": "ECCV 2020 (spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-and-Language Navigation (VLN) is a task where agents must decide how\nto move through a 3D environment to reach a goal by grounding natural language\ninstructions to the visual surroundings. One of the problems of the VLN task is\ndata scarcity since it is difficult to collect enough navigation paths with\nhuman-annotated instructions for interactive environments. In this paper, we\nexplore the use of counterfactual thinking as a human-inspired data\naugmentation method that results in robust models. Counterfactual thinking is a\nconcept that describes the human propensity to create possible alternatives to\nlife events that have already occurred. We propose an adversarial-driven\ncounterfactual reasoning model that can consider effective conditions instead\nof low-quality augmented data. In particular, we present a model-agnostic\nadversarial path sampler (APS) that learns to sample challenging paths that\nforce the navigator to improve based on the navigation performance. APS also\nserves to do pre-exploration of unseen environments to strengthen the model's\nability to generalize. We evaluate the influence of APS on the performance of\ndifferent VLN baseline models using the room-to-room dataset (R2R). The results\nshow that the adversarial training process with our proposed APS benefits VLN\nmodels under both seen and unseen environments. And the pre-exploration process\ncan further gain additional improvements under unseen environments.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 18:02:51 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 15:46:58 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 00:18:45 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Fu", "Tsu-Jui", ""], ["Wang", "Xin Eric", ""], ["Peterson", "Matthew", ""], ["Grafton", "Scott", ""], ["Eckstein", "Miguel", ""], ["Wang", "William Yang", ""]]}, {"id": "1911.07344", "submitter": "Harald Hanselmann", "authors": "Harald Hanselmann and Hermann Ney", "title": "ELoPE: Fine-Grained Visual Classification with Efficient Localization,\n  Pooling and Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of fine-grained visual classification (FGVC) deals with\nclassification problems that display a small inter-class variance such as\ndistinguishing between different bird species or car models. State-of-the-art\napproaches typically tackle this problem by integrating an elaborate attention\nmechanism or (part-) localization method into a standard convolutional neural\nnetwork (CNN). Also in this work the aim is to enhance the performance of a\nbackbone CNN such as ResNet by including three efficient and lightweight\ncomponents specifically designed for FGVC. This is achieved by using global\nk-max pooling, a discriminative embedding layer trained by optimizing class\nmeans and an efficient bounding box estimator that only needs class labels for\ntraining. The resulting model achieves new best state-of-the-art recognition\naccuracies on the Stanford cars and FGVC-Aircraft datasets.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 21:30:30 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Hanselmann", "Harald", ""], ["Ney", "Hermann", ""]]}, {"id": "1911.07346", "submitter": "Haichao Yu", "authors": "Haichao Yu, Haoxiang Li, Honghui Shi, Thomas S. Huang, Gang Hua", "title": "Any-Precision Deep Neural Networks", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present any-precision deep neural networks (DNNs), which are trained with\na new method that allows the learned DNNs to be flexible in numerical precision\nduring inference. The same model in runtime can be flexibly and directly set to\ndifferent bit-widths, by truncating the least significant bits, to support\ndynamic speed and accuracy trade-off. When all layers are set to low-bits, we\nshow that the model achieved accuracy comparable to dedicated models trained at\nthe same precision. This nice property facilitates flexible deployment of deep\nlearning models in real-world applications, where in practice trade-offs\nbetween model accuracy and runtime efficiency are often sought. Previous\nliterature presents solutions to train models at each individual fixed\nefficiency/accuracy trade-off point. But how to produce a model flexible in\nruntime precision is largely unexplored. When the demand of efficiency/accuracy\ntrade-off varies from time to time or even dynamically changes in runtime, it\nis infeasible to re-train models accordingly, and the storage budget may forbid\nkeeping multiple models. Our proposed framework achieves this flexibility\nwithout performance degradation. More importantly, we demonstrate that this\nachievement is agnostic to model architectures and applicable to multiple\nvision tasks. Our code is released at\nhttps://github.com/SHI-Labs/Any-Precision-DNNs.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 21:35:32 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 08:13:10 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Yu", "Haichao", ""], ["Li", "Haoxiang", ""], ["Shi", "Honghui", ""], ["Huang", "Thomas S.", ""], ["Hua", "Gang", ""]]}, {"id": "1911.07347", "submitter": "Abhinav Jain", "authors": "Abhinav Jain and Frank Dellaert", "title": "Fast 3D Pose Refinement with RGB Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pose estimation is a vital step in many robotics and perception tasks such as\nrobotic manipulation, autonomous vehicle navigation, etc. Current\nstate-of-the-art pose estimation methods rely on deep neural networks with\ncomplicated structures and long inference times. While highly robust, they\nrequire computing power often unavailable on mobile robots. We propose a\nCNN-based pose refinement system which takes a coarsely estimated 3D pose from\na computationally cheaper algorithm along with a bounding box image of the\nobject, and returns a highly refined pose. Our experiments on the YCB-Video\ndataset show that our system can refine 3D poses to an extremely high precision\nwith minimal training data.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 21:40:05 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Jain", "Abhinav", ""], ["Dellaert", "Frank", ""]]}, {"id": "1911.07349", "submitter": "Mengmi Zhang", "authors": "Mengmi Zhang, Claire Tseng, Gabriel Kreiman", "title": "Putting visual object recognition in context", "comments": "8 pages, CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context plays an important role in visual recognition. Recent studies have\nshown that visual recognition networks can be fooled by placing objects in\ninconsistent contexts (e.g., a cow in the ocean). To model the role of\ncontextual information in visual recognition, we systematically investigated\nten critical properties of where, when, and how context modulates recognition,\nincluding the amount of context, context and object resolution, geometrical\nstructure of context, context congruence, and temporal dynamics of contextual\nmodulation. The tasks involved recognizing a target object surrounded with\ncontext in a natural image. As an essential benchmark, we conducted a series of\npsychophysics experiments where we altered one aspect of context at a time, and\nquantified recognition accuracy. We propose a biologically-inspired\ncontext-aware object recognition model consisting of a two-stream architecture.\nThe model processes visual information at the fovea and periphery in parallel,\ndynamically incorporates object and contextual information, and sequentially\nreasons about the class label for the target object. Across a wide range of\nbehavioral tasks, the model approximates human level performance without\nretraining for each task, captures the dependence of context enhancement on\nimage properties, and provides initial steps towards integrating scene and\nobject information for visual recognition. All source code and data are\npublicly available: https://github.com/kreimanlab/Put-In-Context.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 21:43:00 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 02:08:31 GMT"}, {"version": "v3", "created": "Wed, 25 Mar 2020 22:46:13 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Zhang", "Mengmi", ""], ["Tseng", "Claire", ""], ["Kreiman", "Gabriel", ""]]}, {"id": "1911.07381", "submitter": "Srikrishna Karanam", "authors": "Meng Zheng, Srikrishna Karanam, Terrence Chen, Richard J. Radke, and\n  Ziyan Wu", "title": "Learning Similarity Attention", "comments": "10 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning similarity functions. While there has\nbeen substantial progress in learning suitable distance metrics, these\ntechniques in general lack decision reasoning, i.e., explaining why the input\nset of images is similar or dissimilar. In this work, we solve this key problem\nby proposing the first method to generate generic visual similarity\nexplanations with gradient-based attention. We demonstrate that our technique\nis agnostic to the specific similarity model type, e.g., we show applicability\nto Siamese, triplet, and quadruplet models. Furthermore, we make our proposed\nsimilarity attention a principled part of the learning process, resulting in a\nnew paradigm for learning similarity functions. We demonstrate that our\nlearning mechanism results in more generalizable, as well as explainable,\nsimilarity models. Finally, we demonstrate the generality of our framework by\nmeans of experiments on a variety of tasks, including image retrieval, person\nre-identification, and low-shot semantic segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 00:46:40 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Zheng", "Meng", ""], ["Karanam", "Srikrishna", ""], ["Chen", "Terrence", ""], ["Radke", "Richard J.", ""], ["Wu", "Ziyan", ""]]}, {"id": "1911.07383", "submitter": "Srikrishna Karanam", "authors": "Ren Li, Changjiang Cai, Georgios Georgakis, Srikrishna Karanam,\n  Terrence Chen, and Ziyan Wu", "title": "Towards Robust RGB-D Human Mesh Recovery", "comments": "10 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of human pose estimation. While much recent work has\nfocused on the RGB domain, these techniques are inherently under-constrained\nsince there can be many 3D configurations that explain the same 2D projection.\nTo this end, we propose a new method that uses RGB-D data to estimate a\nparametric human mesh model. Our key innovations include (a) the design of a\nnew dynamic data fusion module that facilitates learning with a combination of\nRGB-only and RGB-D datasets, (b) a new constraint generator module that\nprovides SMPL supervisory signals when explicit SMPL annotations are not\navailable, and (c) the design of a new depth ranking learning objective, all of\nwhich enable principled model training with RGB-D data. We conduct extensive\nexperiments on a variety of RGB-D datasets to demonstrate efficacy.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 00:55:37 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Li", "Ren", ""], ["Cai", "Changjiang", ""], ["Georgakis", "Georgios", ""], ["Karanam", "Srikrishna", ""], ["Chen", "Terrence", ""], ["Wu", "Ziyan", ""]]}, {"id": "1911.07384", "submitter": "Zehua Cheng", "authors": "Zehua Cheng, Weiyang Wang, Yan Pan, Thomas Lukasiewicz", "title": "Distributed Low Precision Training Without Mixed Precision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low precision training is one of the most popular strategies for deploying\nthe deep model on limited hardware resources. Fixed point implementation of\nDCNs has the potential to alleviate complexities and facilitate potential\ndeployment on embedded hardware. However, most low precision training solution\nis based on a mixed precision strategy. In this paper, we have presented an\nablation study on different low precision training strategy and propose a\nsolution for IEEE FP-16 format throughout the training process. We tested the\nResNet50 on 128 GPU cluster on ImageNet-full dataset. We have viewed that it is\nnot essential to use FP32 format to train the deep models. We have viewed that\ncommunication cost reduction, model compression, and large-scale distributed\ntraining are three coupled problems.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 00:56:52 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2019 05:37:54 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Cheng", "Zehua", ""], ["Wang", "Weiyang", ""], ["Pan", "Yan", ""], ["Lukasiewicz", "Thomas", ""]]}, {"id": "1911.07389", "submitter": "Srikrishna Karanam", "authors": "Wenqian Liu, Runze Li, Meng Zheng, Srikrishna Karanam, Ziyan Wu, Bir\n  Bhanu, Richard J. Radke, Octavia Camps", "title": "Towards Visually Explaining Variational Autoencoders", "comments": "10 pages, 9 figures, 2 tables, CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Convolutional Neural Network (CNN) model interpretability\nhave led to impressive progress in visualizing and understanding model\npredictions. In particular, gradient-based visual attention methods have driven\nmuch recent effort in using visual attention maps as a means for visual\nexplanations. A key problem, however, is these methods are designed for\nclassification and categorization tasks, and their extension to explaining\ngenerative models, e.g. variational autoencoders (VAE) is not trivial. In this\nwork, we take a step towards bridging this crucial gap, proposing the first\ntechnique to visually explain VAEs by means of gradient-based attention. We\npresent methods to generate visual attention from the learned latent space, and\nalso demonstrate such attention explanations serve more than just explaining\nVAE predictions. We show how these attention maps can be used to localize\nanomalies in images, demonstrating state-of-the-art performance on the MVTec-AD\ndataset. We also show how they can be infused into model training, helping\nbootstrap the VAE into learning improved latent space disentanglement,\ndemonstrated on the Dsprites dataset.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 01:05:41 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 16:11:16 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2020 15:52:04 GMT"}, {"version": "v4", "created": "Wed, 11 Mar 2020 14:20:02 GMT"}, {"version": "v5", "created": "Mon, 23 Mar 2020 15:09:27 GMT"}, {"version": "v6", "created": "Tue, 24 Mar 2020 15:51:40 GMT"}, {"version": "v7", "created": "Tue, 14 Apr 2020 16:52:49 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Liu", "Wenqian", ""], ["Li", "Runze", ""], ["Zheng", "Meng", ""], ["Karanam", "Srikrishna", ""], ["Wu", "Ziyan", ""], ["Bhanu", "Bir", ""], ["Radke", "Richard J.", ""], ["Camps", "Octavia", ""]]}, {"id": "1911.07401", "submitter": "Zhenxing Mi", "authors": "Zhenxing Mi, Yiming Luo, Wenbing Tao", "title": "SSRNet: Scalable 3D Surface Reconstruction Network", "comments": "Accepted by CVPR2020, typos corrected, references added, images\n  revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing learning-based surface reconstruction methods from point clouds are\nstill facing challenges in terms of scalability and preservation of details on\nlarge-scale point clouds. In this paper, we propose the SSRNet, a novel\nscalable learning-based method for surface reconstruction. The proposed SSRNet\nconstructs local geometry-aware features for octree vertices and designs a\nscalable reconstruction pipeline, which not only greatly enhances the\npredication accuracy of the relative position between the vertices and the\nimplicit surface facilitating the surface reconstruction quality, but also\nallows dividing the point cloud and octree vertices and processing different\nparts in parallel for superior scalability on large-scale point clouds with\nmillions of points. Moreover, SSRNet demonstrates outstanding generalization\ncapability and only needs several surface data for training, much less than\nother learning-based reconstruction methods, which can effectively avoid\noverfitting. The trained model of SSRNet on one dataset can be directly used on\nother datasets with superior performance. Finally, the time consumption with\nSSRNet on a large-scale point cloud is acceptable and competitive. To our\nknowledge, the proposed SSRNet is the first to really bring a convincing\nsolution to the scalability issue of the learning-based surface reconstruction\nmethods, and is an important step to make learning-based methods competitive\nwith respect to geometry processing methods on real-world and challenging data.\nExperiments show that our method achieves a breakthrough in scalability and\nquality compared with state-of-the-art learning-based methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 02:41:39 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 03:24:28 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Mi", "Zhenxing", ""], ["Luo", "Yiming", ""], ["Tao", "Wenbing", ""]]}, {"id": "1911.07410", "submitter": "Se Young Chun", "authors": "Dongwon Park and Dong Un Kang and Jisoo Kim and Se Young Chun", "title": "Multi-Temporal Recurrent Neural Networks For Progressive Non-Uniform\n  Single Image Deblurring With Incremental Temporal Training", "comments": "10 pages, 8 figures, 6 tables, work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-scale (MS) approaches have been widely investigated for blind single\nimage / video deblurring that sequentially recovers deblurred images in low\nspatial scale first and then in high spatial scale later with the output of\nlower scales. MS approaches have been effective especially for severe blurs\ninduced by large motions in high spatial scale since those can be seen as small\nblurs in low spatial scale. In this work, we investigate alternative approach\nto MS, called multi-temporal (MT) approach, for non-uniform single image\ndeblurring. We propose incremental temporal training with constructed MT level\ndataset from time-resolved dataset, develop novel MT-RNNs with recurrent\nfeature maps, and investigate progressive single image deblurring over\niterations. Our proposed MT methods outperform state-of-the-art MS methods on\nthe GoPro dataset in PSNR with the smallest number of parameters.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 03:36:59 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Park", "Dongwon", ""], ["Kang", "Dong Un", ""], ["Kim", "Jisoo", ""], ["Chun", "Se Young", ""]]}, {"id": "1911.07414", "submitter": "Shan Su", "authors": "Shan Su, Cheng Peng, Jianbo Shi, Chiho Choi", "title": "Potential Field: Interpretable and Unified Representation for Trajectory\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting an agent's future trajectory is a challenging task given the\ncomplicated stimuli (environmental/inertial/social) of motion. Prior works\nlearn individual stimulus from different modules and fuse the representations\nin an end-to-end manner, which makes it hard to understand what are actually\ncaptured and how they are fused. In this work, we borrow the notion of\npotential field from physics as an interpretable and unified representation to\nmodel all stimuli. This allows us to not only supervise the intermediate\nlearning process, but also have a coherent method to fuse the information of\ndifferent sources. From the generated potential fields, we further estimate\nfuture motion direction and speed, which are modeled as Gaussian distributions\nto account for the multi-modal nature of the problem. The final prediction\nresults are generated by recurrently moving past location based on the\nestimated motion direction and speed. We show state-of-the-art results on the\nETH, UCY, and Stanford Drone datasets.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 04:00:34 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 21:46:07 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Su", "Shan", ""], ["Peng", "Cheng", ""], ["Shi", "Jianbo", ""], ["Choi", "Chiho", ""]]}, {"id": "1911.07421", "submitter": "Xiaofeng Liu", "authors": "Tong Che, Xiaofeng Liu, Site Li, Yubin Ge, Ruixiang Zhang, Caiming\n  Xiong, Yoshua Bengio", "title": "Deep Verifier Networks: Verification of Deep Discriminative Models with\n  Deep Generative Models", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI Safety is a major concern in many deep learning applications such as\nautonomous driving. Given a trained deep learning model, an important natural\nproblem is how to reliably verify the model's prediction. In this paper, we\npropose a novel framework -- deep verifier networks (DVN) to verify the inputs\nand outputs of deep discriminative models with deep generative models. Our\nproposed model is based on conditional variational auto-encoders with\ndisentanglement constraints. We give both intuitive and theoretical\njustifications of the model. Our verifier network is trained independently with\nthe prediction model, which eliminates the need of retraining the verifier\nnetwork for a new model. We test the verifier network on out-of-distribution\ndetection and adversarial example detection problems, as well as anomaly\ndetection problems in structured prediction tasks such as image caption\ngeneration. We achieve state-of-the-art results in all of these problems.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 04:23:12 GMT"}, {"version": "v2", "created": "Sat, 8 Feb 2020 03:10:15 GMT"}, {"version": "v3", "created": "Fri, 1 Jan 2021 21:08:11 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Che", "Tong", ""], ["Liu", "Xiaofeng", ""], ["Li", "Site", ""], ["Ge", "Yubin", ""], ["Zhang", "Ruixiang", ""], ["Xiong", "Caiming", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1911.07423", "submitter": "Xiaoqian Li", "authors": "XiaoQian Li, Jie Liu, ShuWu Zhang, GuiXuan Zhang", "title": "Learning to Predict More Accurate Text Instances for Scene Text\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At present, multi-oriented text detection methods based on deep neural\nnetwork have achieved promising performances on various benchmarks.\nNevertheless, there are still some difficulties for arbitrary shape text\ndetection, especially for a simple and proper representation of arbitrary shape\ntext instances. In this paper, a pixel-based text detector is proposed to\nfacilitate the representation and prediction of text instances with arbitrary\nshapes in a simple manner. Firstly, to alleviate the effect of the target\nvertex sorting and achieve the direct regression of arbitrary shape text\ninstances, the starting-point-independent coordinates regression loss is\nproposed. Furthermore, to predict more accurate text instances, the text\ninstance accuracy loss is proposed as an assistant task to refine the predicted\ncoordinates under the guidance of IoU. To evaluate the effectiveness of our\ndetector, extensive experiments have been carried on public benchmarks which\ncontain arbitrary shape text instances and multi-oriented text instances. We\nobtain 84.8% of F-measure on Total-Text benchmark. The results show that our\nmethod can reach state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 04:35:47 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 01:27:35 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Li", "XiaoQian", ""], ["Liu", "Jie", ""], ["Zhang", "ShuWu", ""], ["Zhang", "GuiXuan", ""]]}, {"id": "1911.07424", "submitter": "Cheol-Hwan Yoo", "authors": "Cheol-hwan Yoo, Seo-won Ji, Yong-goo Shin, Seung-wook Kim, and\n  Sung-jea Ko", "title": "Fast and Accurate 3D Hand Pose Estimation via Recurrent Neural Network\n  for Capturing Hand Articulations", "comments": null, "journal-ref": "IEEE Access. 8 (2020) 114010-114019", "doi": "10.1109/ACCESS.2020.3001637", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D hand pose estimation from a single depth image plays an important role in\ncomputer vision and human-computer interaction. Although recent hand pose\nestimation methods using convolution neural network (CNN) have shown notable\nimprovements in accuracy, most of them have a limitation that they rely on a\ncomplex network structure without fully exploiting the articulated structure of\nthe hand. A hand, which is an articulated object, is composed of six local\nparts: the palm and five independent fingers. Each finger consists of\nsequential-joints that provide constrained motion, referred to as a kinematic\nchain. In this paper, we propose a hierarchically-structured convolutional\nrecurrent neural network (HCRNN) with six branches that estimate the 3D\nposition of the palm and five fingers independently. The palm position is\npredicted via fully-connected layers. Each sequential-joint, i.e. finger\nposition, is obtained using a recurrent neural network (RNN) to capture the\nspatial dependencies between adjacent joints. Then the output features of the\npalm and finger branches are concatenated to estimate the global hand position.\nHCRNN directly takes the depth map as an input without a time-consuming data\nconversion, such as 3D voxels and point clouds. Experimental results on public\ndatasets demonstrate that the proposed HCRNN not only outperforms most 2D\nCNN-based methods using the depth image as their inputs but also achieves\ncompetitive results with state-of-the-art 3D CNN-based methods with a highly\nefficient running speed of 285 fps on a single GPU.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 04:38:25 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 04:36:31 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Yoo", "Cheol-hwan", ""], ["Ji", "Seo-won", ""], ["Shin", "Yong-goo", ""], ["Kim", "Seung-wook", ""], ["Ko", "Sung-jea", ""]]}, {"id": "1911.07440", "submitter": "Muhammet Bastan", "authors": "Muhammet Bastan, Hao-Yu Wu, Tian Cao, Bhargava Kota, Mehmet Tek", "title": "Large Scale Open-Set Deep Logo Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an open-set logo detection (OSLD) system, which can detect\n(localize and recognize) any number of unseen logo classes without re-training;\nit only requires a small set of canonical logo images for each logo class. We\nachieve this using a two-stage approach: (1) Generic logo detection to detect\ncandidate logo regions in an image. (2) Logo matching for matching the detected\nlogo regions to a set of canonical logo images to recognize them. We also\nintroduce a 'simple deep metric learning' (SDML) framework that outperformed\nmore complicated ensemble and attention models and boosted the logo matching\naccuracy. Furthermore, we constructed a new open-set logo detection dataset\nwith thousands of logo classes, and will release it for research purposes. We\ndemonstrate the effectiveness of OSLD on our dataset and on the standard\nFlickr-32 logo dataset, outperforming the state-of-the-art open-set and\nclosed-set logo detection methods by a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 05:44:17 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Bastan", "Muhammet", ""], ["Wu", "Hao-Yu", ""], ["Cao", "Tian", ""], ["Kota", "Bhargava", ""], ["Tek", "Mehmet", ""]]}, {"id": "1911.07446", "submitter": "Cong Hao", "authors": "Cong Hao, Yao Chen, Xinheng Liu, Atif Sarwari, Daryl Sew, Ashutosh\n  Dhar, Bryan Wu, Dongdong Fu, Jinjun Xiong, Wen-mei Hwu, Junli Gu, Deming Chen", "title": "NAIS: Neural Architecture and Implementation Search and its Applications\n  in Autonomous Driving", "comments": "8 pages, ICCAD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapidly growing demands for powerful AI algorithms in many application\ndomains have motivated massive investment in both high-quality deep neural\nnetwork (DNN) models and high-efficiency implementations. In this position\npaper, we argue that a simultaneous DNN/implementation co-design methodology,\nnamed Neural Architecture and Implementation Search (NAIS), deserves more\nresearch attention to boost the development productivity and efficiency of both\nDNN models and implementation optimization. We propose a stylized design\nmethodology that can drastically cut down the search cost while preserving the\nquality of the end solution.As an illustration, we discuss this\nDNN/implementation methodology in the context of both FPGAs and GPUs. We take\nautonomous driving as a key use case as it is one of the most demanding areas\nfor high quality AI algorithms and accelerators. We discuss how such a\nco-design methodology can impact the autonomous driving industry significantly.\nWe identify several research opportunities in this exciting domain.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 06:17:14 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Hao", "Cong", ""], ["Chen", "Yao", ""], ["Liu", "Xinheng", ""], ["Sarwari", "Atif", ""], ["Sew", "Daryl", ""], ["Dhar", "Ashutosh", ""], ["Wu", "Bryan", ""], ["Fu", "Dongdong", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-mei", ""], ["Gu", "Junli", ""], ["Chen", "Deming", ""]]}, {"id": "1911.07450", "submitter": "Juncheng Li", "authors": "Juncheng Li, Xin Wang, Siliang Tang, Haizhou Shi, Fei Wu, Yueting\n  Zhuang, William Yang Wang", "title": "Unsupervised Reinforcement Learning of Transferable Meta-Skills for\n  Embodied Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual navigation is a task of training an embodied agent by intelligently\nnavigating to a target object (e.g., television) using only visual\nobservations. A key challenge for current deep reinforcement learning models\nlies in the requirements for a large amount of training data. It is exceedingly\nexpensive to construct sufficient 3D synthetic environments annotated with the\ntarget object information. In this paper, we focus on visual navigation in the\nlow-resource setting, where we have only a few training environments annotated\nwith object information. We propose a novel unsupervised reinforcement learning\napproach to learn transferable meta-skills (e.g., bypass obstacles, go\nstraight) from unannotated environments without any supervisory signals. The\nagent can then fast adapt to visual navigation through learning a high-level\nmaster policy to combine these meta-skills, when the\nvisual-navigation-specified reward is provided. Evaluation in the AI2-THOR\nenvironments shows that our method significantly outperforms the baseline by\n53.34% relatively on SPL, and further qualitative analysis demonstrates that\nour method learns transferable motor primitives for visual navigation.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 06:53:51 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 03:53:02 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Li", "Juncheng", ""], ["Wang", "Xin", ""], ["Tang", "Siliang", ""], ["Shi", "Haizhou", ""], ["Wu", "Fei", ""], ["Zhuang", "Yueting", ""], ["Wang", "William Yang", ""]]}, {"id": "1911.07451", "submitter": "Chunhua Shen", "authors": "Zhi Tian, Hao Chen, Chunhua Shen", "title": "DirectPose: Direct End-to-End Multi-Person Pose Estimation", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose the first direct end-to-end multi-person pose estimation\nframework, termed DirectPose. Inspired by recent anchor-free object detectors,\nwhich directly regress the two corners of target bounding-boxes, the proposed\nframework directly predicts instance-aware keypoints for all the instances from\na raw input image, eliminating the need for heuristic grouping in bottom-up\nmethods or bounding-box detection and RoI operations in top-down ones. We also\npropose a novel Keypoint Alignment (KPAlign) mechanism, which overcomes the\nmain difficulty: lack of the alignment between the convolutional features and\npredictions in this end-to-end framework. KPAlign improves the framework's\nperformance by a large margin while still keeping the framework end-to-end\ntrainable. With the only postprocessing non-maximum suppression (NMS), our\nproposed framework can detect multi-person keypoints with or without\nbounding-boxes in a single shot. Experiments demonstrate that the end-to-end\nparadigm can achieve competitive or better performance than previous strong\nbaselines, in both bottom-up and top-down methods. We hope that our end-to-end\napproach can provide a new perspective for the human pose estimation task.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 06:55:08 GMT"}, {"version": "v2", "created": "Sun, 24 Nov 2019 05:03:13 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Tian", "Zhi", ""], ["Chen", "Hao", ""], ["Shen", "Chunhua", ""]]}, {"id": "1911.07460", "submitter": "Ilja Manakov", "authors": "Ilja Manakov, Markus Rohm, Volker Tresp", "title": "Walking the Tightrope: An Investigation of the Convolutional Autoencoder\n  Bottleneck", "comments": "code available at https://github.com/IljaManakov/WalkingTheTightrope", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we present an in-depth investigation of the convolutional\nautoencoder (CAE) bottleneck. Autoencoders (AE), and especially their\nconvolutional variants, play a vital role in the current deep learning toolbox.\nResearchers and practitioners employ CAEs for a variety of tasks, ranging from\noutlier detection and compression to transfer and representation learning.\nDespite their widespread adoption, we have limited insight into how the\nbottleneck shape impacts the emergent properties of the CAE. We demonstrate\nthat increased height and width of the bottleneck drastically improves\ngeneralization, which in turn leads to better performance of the latent codes\nin downstream transfer learning tasks. The number of channels in the\nbottleneck, on the other hand, is secondary in importance. Furthermore, we show\nempirically that, contrary to popular belief, CAEs do not learn to copy their\ninput, even when the bottleneck has the same number of neurons as there are\npixels in the input. Copying does not occur, despite training the CAE for 1,000\nepochs on a tiny ($\\approx$ 600 images) dataset. We believe that the findings\nin this paper are directly applicable and will lead to improvements in models\nthat rely on CAEs.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 07:19:14 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 19:27:36 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Manakov", "Ilja", ""], ["Rohm", "Markus", ""], ["Tresp", "Volker", ""]]}, {"id": "1911.07471", "submitter": "Tiancheng Wen", "authors": "Tiancheng Wen, Shenqi Lai, Xueming Qian", "title": "Preparing Lessons: Improve Knowledge Distillation with Better\n  Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation (KD) is widely used for training a compact model with\nthe supervision of another large model, which could effectively improve the\nperformance. Previous methods mainly focus on two aspects: 1) training the\nstudent to mimic representation space of the teacher; 2) training the model\nprogressively or adding extra module like discriminator. Knowledge from teacher\nis useful, but it is still not exactly right compared with ground truth.\nBesides, overly uncertain supervision also influences the result. We introduce\ntwo novel approaches, Knowledge Adjustment (KA) and Dynamic Temperature\nDistillation (DTD), to penalize bad supervision and improve student model.\nExperiments on CIFAR-100, CINIC-10 and Tiny ImageNet show that our methods get\nencouraging performance compared with state-of-the-art methods. When combined\nwith other KD-based methods, the performance will be further improved.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 07:47:29 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 15:55:48 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2020 14:16:45 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Wen", "Tiancheng", ""], ["Lai", "Shenqi", ""], ["Qian", "Xueming", ""]]}, {"id": "1911.07472", "submitter": "Wu Shi", "authors": "Wu Shi, Tak-Wai Hui, Ziwei Liu, Dahua Lin, Chen Change Loy", "title": "Learning to Synthesize Fashion Textures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing unconditional generative models mainly focus on modeling general\nobjects, such as faces and indoor scenes. Fashion textures, another important\ntype of visual elements around us, have not been extensively studied. In this\nwork, we propose an effective generative model for fashion textures and also\ncomprehensively investigate the key components involved: internal\nrepresentation, latent space sampling and the generator architecture. We use\nGram matrix as a suitable internal representation for modeling realistic\nfashion textures, and further design two dedicated modules for modulating Gram\nmatrix into a low-dimension vector. Since fashion textures are scale-dependent,\nwe propose a recursive auto-encoder to capture the dependency between multiple\ngranularity levels of texture feature. Another important observation is that\nfashion textures are multi-modal. We fit and sample from a Gaussian mixture\nmodel in the latent space to improve the diversity of the generated textures.\nExtensive experiments demonstrate that our approach is capable of synthesizing\nmore realistic and diverse fashion textures over other state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 07:48:12 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Shi", "Wu", ""], ["Hui", "Tak-Wai", ""], ["Liu", "Ziwei", ""], ["Lin", "Dahua", ""], ["Loy", "Chen Change", ""]]}, {"id": "1911.07478", "submitter": "Heewon Kim", "authors": "Heewon Kim, Seokil Hong, Bohyung Han, Heesoo Myeong, Kyoung Mu Lee", "title": "Fine-Grained Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an elegant framework of fine-grained neural architecture search\n(FGNAS), which allows to employ multiple heterogeneous operations within a\nsingle layer and can even generate compositional feature maps using several\ndifferent base operations. FGNAS runs efficiently in spite of significantly\nlarge search space compared to other methods because it trains networks\nend-to-end by a stochastic gradient descent method. Moreover, the proposed\nframework allows to optimize the network under predefined resource constraints\nin terms of number of parameters, FLOPs and latency. FGNAS has been applied to\ntwo crucial applications in resource demanding computer vision\ntasks---large-scale image classification and image super-resolution---and\ndemonstrates the state-of-the-art performance through flexible operation search\nand channel pruning.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 07:56:06 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Kim", "Heewon", ""], ["Hong", "Seokil", ""], ["Han", "Bohyung", ""], ["Myeong", "Heesoo", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "1911.07509", "submitter": "Anis Koubaa", "authors": "Marwa Ben Jabra, Adel Ammar, Anis Koubaa, Omar Cheikhrouhou, Habib\n  Hamam", "title": "AI-based Pilgrim Detection using Convolutional Neural Networks", "comments": "Accepted in ATSIP'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pilgrimage represents the most important Islamic religious gathering in the\nworld where millions of pilgrims visit the holy places of Makkah and Madinah to\nperform their rituals. The safety and security of pilgrims is the highest\npriority for the authorities. In Makkah, 5000 cameras are spread around the\nholy for monitoring pilgrims, but it is almost impossible to track all events\nby humans considering the huge number of images collected every second. To\naddress this issue, we propose to use artificial intelligence technique based\non deep learning and convolution neural networks to detect and identify\nPilgrims and their features. For this purpose, we built a comprehensive dataset\nfor the detection of pilgrims and their genders. Then, we develop two\nconvolutional neural networks based on YOLOv3 and Faster-RCNN for the detection\nof Pilgrims. Experiments results show that Faster RCNN with Inception v2\nfeature extractor provides the best mean average precision over all classes of\n51%.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 09:46:54 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 19:06:10 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Jabra", "Marwa Ben", ""], ["Ammar", "Adel", ""], ["Koubaa", "Anis", ""], ["Cheikhrouhou", "Omar", ""], ["Hamam", "Habib", ""]]}, {"id": "1911.07515", "submitter": "Syed Jawad Shah", "authors": "Ahmed Awad Albishri, Syed Jawad Hussain Shah, Anthony Schmiedler,\n  Seung Suk Kang, Yugyung Lee", "title": "Automated Human Claustrum Segmentation using Deep Learning Technologies", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Deep Learning (DL) has shown promising results in conducting\nAI tasks such as computer vision and image segmentation. Specifically,\nConvolutional Neural Network (CNN) models in DL have been applied to\nprevention,detection, and diagnosis in predictive medicine. Image segmentation\nplays a significant role in disease detection and prevention.However, there are\nenormous challenges in performing DL-based automatic segmentation due to the\nnature of medical images such as heterogeneous modalities and formats,\ninsufficient labeled training data, and the high-class imbalance in the labeled\ndata. Furthermore, automating segmentation of medical images,like magnetic\nresonance images (MRI), becomes a challenging task. The need for automated\nsegmentation or annotation is what motivates our work. In this paper, we\npropose a fully automated approach that aims to segment the human claustrum for\nanalytical purposes. We applied a U-Net CNN model to segment the claustrum (Cl)\nfrom a MRI dataset. With this approach, we have achieved an average Dice per\ncase score of 0.72 for Cl segmentation, with K=5 for cross-validation. The\nexpert in the medical domain also evaluates these results.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 09:59:09 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Albishri", "Ahmed Awad", ""], ["Shah", "Syed Jawad Hussain", ""], ["Schmiedler", "Anthony", ""], ["Kang", "Seung Suk", ""], ["Lee", "Yugyung", ""]]}, {"id": "1911.07518", "submitter": "Tao Gui", "authors": "Tao Gui, Lizhi Qing, Qi Zhang, Jiacheng Ye, Hang Yan, Zichu Fei,\n  Xuanjing Huang", "title": "Constructing Multiple Tasks for Augmentation: Improving Neural Image\n  Classification With K-means Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) has received considerable attention, and numerous\ndeep learning applications benefit from MTL with multiple objectives. However,\nconstructing multiple related tasks is difficult, and sometimes only a single\ntask is available for training in a dataset. To tackle this problem, we\nexplored the idea of using unsupervised clustering to construct a variety of\nauxiliary tasks from unlabeled data or existing labeled data. We found that\nsome of these newly constructed tasks could exhibit semantic meanings\ncorresponding to certain human-specific attributes, but some were non-ideal. In\norder to effectively reduce the impact of non-ideal auxiliary tasks on the main\ntask, we further proposed a novel meta-learning-based multi-task learning\napproach, which trained the shared hidden layers on auxiliary tasks, while the\nmeta-optimization objective was to minimize the loss on the main task, ensuring\nthat the optimizing direction led to an improvement on the main task.\nExperimental results across five image datasets demonstrated that the proposed\nmethod significantly outperformed existing single task learning,\nsemi-supervised learning, and some data augmentation methods, including an\nimprovement of more than 9% on the Omniglot dataset.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 10:04:08 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Gui", "Tao", ""], ["Qing", "Lizhi", ""], ["Zhang", "Qi", ""], ["Ye", "Jiacheng", ""], ["Yan", "Hang", ""], ["Fei", "Zichu", ""], ["Huang", "Xuanjing", ""]]}, {"id": "1911.07524", "submitter": "Junjie Huang", "authors": "Junjie Huang, Zheng Zhu, Feng Guo, Guan Huang, Dalong Du", "title": "The Devil is in the Details: Delving into Unbiased Data Processing for\n  Human Pose Estimation", "comments": "project:https://github.com/HuangJunJie2017/UDP-Pose", "journal-ref": "CVPR2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being a fundamental component in training and inference, data processing has\nnot been systematically considered in human pose estimation community, to the\nbest of our knowledge. In this paper, we focus on this problem and find that\nthe devil of human pose estimation evolution is in the biased data processing.\nSpecifically, by investigating the standard data processing in state-of-the-art\napproaches mainly including coordinate system transformation and keypoint\nformat transformation (i.e., encoding and decoding), we find that the results\nobtained by common flipping strategy are unaligned with the original ones in\ninference. Moreover, there is a statistical error in some keypoint format\ntransformation methods. Two problems couple together, significantly degrade the\npose estimation performance and thus lay a trap for the research community.\nThis trap has given bone to many suboptimal remedies, which are always\nunreported, confusing but influential. By causing failure in reproduction and\nunfair in comparison, the unreported remedies seriously impedes the\ntechnological development. To tackle this dilemma from the source, we propose\nUnbiased Data Processing (UDP) consist of two technique aspect for the two\naforementioned problems respectively (i.e., unbiased coordinate system\ntransformation and unbiased keypoint format transformation). As a\nmodel-agnostic approach and a superior solution, UDP successfully pushes the\nperformance boundary of human pose estimation and offers a higher and more\nreliable baseline for research community. Code is public available in\nhttps://github.com/HuangJunJie2017/UDP-Pose\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 10:17:12 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 04:39:42 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Huang", "Junjie", ""], ["Zhu", "Zheng", ""], ["Guo", "Feng", ""], ["Huang", "Guan", ""], ["Du", "Dalong", ""]]}, {"id": "1911.07527", "submitter": "Yibo Yang", "authors": "Yibo Yang, Hongyang Li, Xia Li, Qijie Zhao, Jianlong Wu, Zhouchen Lin", "title": "SOGNet: Scene Overlap Graph Network for Panoptic Segmentation", "comments": "To appear in AAAI 2020. Our method also won the Innovation Award in\n  COCO 2019 challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The panoptic segmentation task requires a unified result from semantic and\ninstance segmentation outputs that may contain overlaps. However, current\nstudies widely ignore modeling overlaps. In this study, we aim to model overlap\nrelations among instances and resolve them for panoptic segmentation. Inspired\nby scene graph representation, we formulate the overlapping problem as a\nsimplified case, named scene overlap graph. We leverage each object's category,\ngeometry and appearance features to perform relational embedding, and output a\nrelation matrix that encodes overlap relations. In order to overcome the lack\nof supervision, we introduce a differentiable module to resolve the overlap\nbetween any pair of instances. The mask logits after removing overlaps are fed\ninto per-pixel instance \\verb|id| classification, which leverages the panoptic\nsupervision to assist in the modeling of overlap relations. Besides, we\ngenerate an approximate ground truth of overlap relations as the weak\nsupervision, to quantify the accuracy of overlap relations predicted by our\nmethod. Experiments on COCO and Cityscapes demonstrate that our method is able\nto accurately predict overlap relations, and outperform the state-of-the-art\nperformance for panoptic segmentation. Our method also won the Innovation Award\nin COCO 2019 challenge.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 10:26:35 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Yang", "Yibo", ""], ["Li", "Hongyang", ""], ["Li", "Xia", ""], ["Zhao", "Qijie", ""], ["Wu", "Jianlong", ""], ["Lin", "Zhouchen", ""]]}, {"id": "1911.07528", "submitter": "Mo Zhou", "authors": "Mo Zhou, Zhenxing Niu, Le Wang, Zhanning Gao, Qilin Zhang, Gang Hua", "title": "Ladder Loss for Coherent Visual-Semantic Embedding", "comments": "Accepted to AAAI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For visual-semantic embedding, the existing methods normally treat the\nrelevance between queries and candidates in a bipolar way -- relevant or\nirrelevant, and all \"irrelevant\" candidates are uniformly pushed away from the\nquery by an equal margin in the embedding space, regardless of their various\nproximity to the query. This practice disregards relatively discriminative\ninformation and could lead to suboptimal ranking in the retrieval results and\npoorer user experience, especially in the long-tail query scenario where a\nmatching candidate may not necessarily exist. In this paper, we introduce a\ncontinuous variable to model the relevance degree between queries and multiple\ncandidates, and propose to learn a coherent embedding space, where candidates\nwith higher relevance degrees are mapped closer to the query than those with\nlower relevance degrees. In particular, the new ladder loss is proposed by\nextending the triplet loss inequality to a more general inequality chain, which\nimplements variable push-away margins according to respective relevance\ndegrees. In addition, a proper Coherent Score metric is proposed to better\nmeasure the ranking results including those \"irrelevant\" candidates. Extensive\nexperiments on multiple datasets validate the efficacy of our proposed method,\nwhich achieves significant improvement over existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 10:31:17 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Zhou", "Mo", ""], ["Niu", "Zhenxing", ""], ["Wang", "Le", ""], ["Gao", "Zhanning", ""], ["Zhang", "Qilin", ""], ["Hua", "Gang", ""]]}, {"id": "1911.07538", "submitter": "Debayan Deb", "authors": "Debayan Deb, Divyansh Aggarwal, Anil K. Jain", "title": "Finding Missing Children: Aging Deep Face Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a gallery of face images of missing children, state-of-the-art face\nrecognition systems fall short in identifying a child (probe) recovered at a\nlater age. We propose an age-progression module that can age-progress deep face\nfeatures output by any commodity face matcher. For time lapses larger than 10\nyears (the missing child is found after 10 or more years), the proposed\nage-progression module improves the closed-set identification accuracy of\nFaceNet from 40% to 49.56% and CosFace from 56.88% to 61.25% on a child\ncelebrity dataset, namely ITWCC. The proposed method also outperforms\nstate-of-the-art approaches with a rank-1 identification rate from 94.91% to\n95.91% on a public aging dataset, FG-NET, and from 99.50% to 99.58% on CACD-VS.\nThese results suggest that aging face features enhances the ability to identify\nyoung children who are possible victims of child trafficking or abduction.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 10:58:04 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 04:03:42 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Deb", "Debayan", ""], ["Aggarwal", "Divyansh", ""], ["Jain", "Anil K.", ""]]}, {"id": "1911.07543", "submitter": "Marcela Carvalho", "authors": "Marcela Carvalho and Bertrand Le Saux and Pauline Trouv\\'e-Peloux and\n  Fr\\'ed\\'eric Champagnat and Andr\\'es Almansa", "title": "Multi-Task Learning of Height and Semantics from Aerial Images", "comments": "Published IEEE Geoscience and Remote Sensing Letters. Code\n  https://github.com/marcelampc/mtl_aerial_images", "journal-ref": null, "doi": "10.1109/LGRS.2019.2947783", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aerial or satellite imagery is a great source for land surface analysis,\nwhich might yield land use maps or elevation models. In this investigation, we\npresent a neural network framework for learning semantics and local height\ntogether. We show how this joint multi-task learning benefits to each task on\nthe large dataset of the 2018 Data Fusion Contest. Moreover, our framework also\nyields an uncertainty map which allows assessing the prediction of the model.\nCode is available at https://github.com/marcelampc/mtl_aerial_images .\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 11:08:11 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Carvalho", "Marcela", ""], ["Saux", "Bertrand Le", ""], ["Trouv\u00e9-Peloux", "Pauline", ""], ["Champagnat", "Fr\u00e9d\u00e9ric", ""], ["Almansa", "Andr\u00e9s", ""]]}, {"id": "1911.07559", "submitter": "Xu Qin", "authors": "Xu Qin and Zhilin Wang and Yuanchao Bai and Xiaodong Xie and Huizhu\n  Jia", "title": "FFA-Net: Feature Fusion Attention Network for Single Image Dehazing", "comments": "Accepted by AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an end-to-end feature fusion at-tention network\n(FFA-Net) to directly restore the haze-free image. The FFA-Net architecture\nconsists of three key components:\n  1) A novel Feature Attention (FA) module combines Channel Attention with\nPixel Attention mechanism, considering that different channel-wise features\ncontain totally different weighted information and haze distribution is uneven\non the different image pixels. FA treats different features and pixels\nunequally, which provides additional flexibility in dealing with different\ntypes of information, expanding the representational ability of CNNs. 2) A\nbasic block structure consists of Local Residual Learning and Feature\nAttention, Local Residual Learning allowing the less important information such\nas thin haze region or low-frequency to be bypassed through multiple local\nresidual connections, let main network architecture focus on more effective\ninformation. 3) An Attention-based different levels Feature Fusion (FFA)\nstructure, the feature weights are adaptively learned from the Feature\nAttention (FA) module, giving more weight to important features. This structure\ncan also retain the information of shallow layers and pass it into deep layers.\n  The experimental results demonstrate that our proposed FFA-Net surpasses\nprevious state-of-the-art single image dehazing methods by a very large margin\nboth quantitatively and qualitatively, boosting the best published PSNR metric\nfrom 30.23db to 36.39db on the SOTS indoor test dataset.\n  Code has been made available at GitHub.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 11:43:58 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 06:33:42 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Qin", "Xu", ""], ["Wang", "Zhilin", ""], ["Bai", "Yuanchao", ""], ["Xie", "Xiaodong", ""], ["Jia", "Huizhu", ""]]}, {"id": "1911.07566", "submitter": "Felipe Moser", "authors": "Felipe Moser, Ruobing Huang, Aris T. Papageorghiou, Bartlomiej W.\n  Papiez, Ana I. L. Namburete", "title": "Automated fetal brain extraction from clinical Ultrasound volumes using\n  3D Convolutional Neural Networks", "comments": "13 pages, 7 figures, MIUA conference", "journal-ref": null, "doi": "10.1007/978-3-030-39343-4_13", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  To improve the performance of most neuroimiage analysis pipelines, brain\nextraction is used as a fundamental first step in the image processing. But in\nthe case of fetal brain development, there is a need for a reliable US-specific\ntool. In this work we propose a fully automated 3D CNN approach to fetal brain\nextraction from 3D US clinical volumes with minimal preprocessing. Our method\naccurately and reliably extracts the brain regardless of the large data\nvariation inherent in this imaging modality. It also performs consistently\nthroughout a gestational age range between 14 and 31 weeks, regardless of the\npose variation of the subject, the scale, and even partial feature-obstruction\nin the image, outperforming all current alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 11:56:28 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 10:16:23 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Moser", "Felipe", ""], ["Huang", "Ruobing", ""], ["Papageorghiou", "Aris T.", ""], ["Papiez", "Bartlomiej W.", ""], ["Namburete", "Ana I. L.", ""]]}, {"id": "1911.07574", "submitter": "Wen-Yen Chang", "authors": "Wen-Yen Chang and Wen-Huan Chiang and Shao-Hao Lu and Tingfan Wu and\n  Min Sun", "title": "Bias-Aware Heapified Policy for Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data efficiency of learning-based algorithms is more and more important\nsince high-quality and clean data is expensive as well as hard to collect. In\norder to achieve high model performance with the least number of samples,\nactive learning is a technique that queries the most important subset of data\nfrom the original dataset. In active learning domain, one of the mainstream\nresearch is the heuristic uncertainty-based method which is useful for the\nlearning-based system. Recently, a few works propose to apply policy\nreinforcement learning (PRL) for querying important data. It seems more general\nthan heuristic uncertainty-based method owing that PRL method depends on data\nfeature which is reliable than human prior. However, there have two problems -\nsample inefficiency of policy learning and overconfidence, when applying PRL on\nactive learning. To be more precise, sample inefficiency of policy learning\noccurs when sampling within a large action space, in the meanwhile, class\nimbalance can lead to the overconfidence. In this paper, we propose a\nbias-aware policy network called Heapified Active Learning (HAL), which\nprevents overconfidence, and improves sample efficiency of policy learning by\nheapified structure without ignoring global inforamtion(overview of the whole\nunlabeled set). In our experiment, HAL outperforms other baseline methods on\nMNIST dataset and duplicated MNIST. Last but not least, we investigate the\ngeneralization of the HAL policy learned on MNIST dataset by directly applying\nit on MNIST-M. We show that the agent can generalize and outperform\ndirectly-learned policy under constrained labeled sets.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 12:08:09 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Chang", "Wen-Yen", ""], ["Chiang", "Wen-Huan", ""], ["Lu", "Shao-Hao", ""], ["Wu", "Tingfan", ""], ["Sun", "Min", ""]]}, {"id": "1911.07590", "submitter": "Stefano Gasperini", "authors": "Stefano Gasperini, Magdalini Paschali, Carsten Hopke, David Wittmann,\n  Nassir Navab", "title": "Signal Clustering with Class-independent Segmentation", "comments": "Under Review for IEEE ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radar signals have been dramatically increasing in complexity, limiting the\nsource separation ability of traditional approaches. In this paper we propose a\nDeep Learning-based clustering method, which encodes concurrent signals into\nimages, and, for the first time, tackles clustering with image segmentation.\nNovel loss functions are introduced to optimize a Neural Network to separate\nthe input pulses into pure and non-fragmented clusters. Outperforming a variety\nof baselines, the proposed approach is capable of clustering inputs directly\nwith a Neural Network, in an end-to-end fashion.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 12:43:57 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Gasperini", "Stefano", ""], ["Paschali", "Magdalini", ""], ["Hopke", "Carsten", ""], ["Wittmann", "David", ""], ["Navab", "Nassir", ""]]}, {"id": "1911.07602", "submitter": "Julian Bock", "authors": "Julian Bock, Robert Krajewski, Tobias Moers, Steffen Runde, Lennart\n  Vater and Lutz Eckstein", "title": "The inD Dataset: A Drone Dataset of Naturalistic Road User Trajectories\n  at German Intersections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated vehicles rely heavily on data-driven methods, especially for\ncomplex urban environments. Large datasets of real world measurement data in\nthe form of road user trajectories are crucial for several tasks like road user\nprediction models or scenario-based safety validation. So far, though, this\ndemand is unmet as no public dataset of urban road user trajectories is\navailable in an appropriate size, quality and variety. By contrast, the highway\ndrone dataset (highD) has recently shown that drones are an efficient method\nfor acquiring naturalistic road user trajectories. Compared to driving studies\nor ground-level infrastructure sensors, one major advantage of using a drone is\nthe possibility to record naturalistic behavior, as road users do not notice\nmeasurements taking place. Due to the ideal viewing angle, an entire\nintersection scenario can be measured with significantly less occlusion than\nwith sensors at ground level. Both the class and the trajectory of each road\nuser can be extracted from the video recordings with high precision using\nstate-of-the-art deep neural networks. Therefore, we propose the creation of a\ncomprehensive, large-scale urban intersection dataset with naturalistic road\nuser behavior using camera-equipped drones as successor of the highD dataset.\nThe resulting dataset contains more than 11500 road users including vehicles,\nbicyclists and pedestrians at intersections in Germany and is called inD. The\ndataset consists of 10 hours of measurement data from four intersections and is\navailable online for non-commercial research at: http://www.inD-dataset.com\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 13:20:41 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Bock", "Julian", ""], ["Krajewski", "Robert", ""], ["Moers", "Tobias", ""], ["Runde", "Steffen", ""], ["Vater", "Lennart", ""], ["Eckstein", "Lutz", ""]]}, {"id": "1911.07644", "submitter": "Yan Zhang", "authors": "Yan Zhang, Steve Farrell, Michael Crowley, Lee Makowski, Jack Deslippe", "title": "A Molecular-MNIST Dataset for Machine Learning Study on Diffraction\n  Imaging and Microscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An image dataset of 10 different size molecules, where each molecule has\n2,000 structural variants, is generated from the 2D cross-sectional projection\nof Molecular Dynamics trajectories. The purpose of this dataset is to provide a\nbenchmark dataset for the increasing need of machine learning, deep learning\nand image processing on the study of scattering, imaging and microscopy.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 18:48:02 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Zhang", "Yan", ""], ["Farrell", "Steve", ""], ["Crowley", "Michael", ""], ["Makowski", "Lee", ""], ["Deslippe", "Jack", ""]]}, {"id": "1911.07661", "submitter": "Toshihiko Matsuura", "authors": "Toshihiko Matsuura and Tatsuya Harada", "title": "Domain Generalization Using a Mixture of Multiple Latent Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When domains, which represent underlying data distributions, vary during\ntraining and testing processes, deep neural networks suffer a drop in their\nperformance. Domain generalization allows improvements in the generalization\nperformance for unseen target domains by using multiple source domains.\nConventional methods assume that the domain to which each sample belongs is\nknown in training. However, many datasets, such as those collected via web\ncrawling, contain a mixture of multiple latent domains, in which the domain of\neach sample is unknown. This paper introduces domain generalization using a\nmixture of multiple latent domains as a novel and more realistic scenario,\nwhere we try to train a domain-generalized model without using domain labels.\nTo address this scenario, we propose a method that iteratively divides samples\ninto latent domains via clustering, and which trains the domain-invariant\nfeature extractor shared among the divided latent domains via adversarial\nlearning. We assume that the latent domain of images is reflected in their\nstyle, and thus, utilize style features for clustering. By using these\nfeatures, our proposed method successfully discovers latent domains and\nachieves domain generalization even if the domain labels are not given.\nExperiments show that our proposed method can train a domain-generalized model\nwithout using domain labels. Moreover, it outperforms conventional domain\ngeneralization methods, including those that utilize domain labels.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 14:31:36 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Matsuura", "Toshihiko", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1911.07681", "submitter": "Bo Jiang", "authors": "Bo Jiang, Pengfei Sun, Jin Tang, Bin Luo", "title": "GLMNet: Graph Learning-Matching Networks for Feature Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, graph convolutional networks (GCNs) have shown great potential for\nthe task of graph matching. It can integrate graph node feature embedding,\nnode-wise affinity learning and matching optimization together in a unified\nend-to-end model. One important aspect of graph matching is the construction of\ntwo matching graphs. However, the matching graphs we feed to existing graph\nconvolutional matching networks are generally fixed and independent of graph\nmatching, which thus are not guaranteed to be optimal for the graph matching\ntask. Also, existing GCN matching method employs several general\nsmoothing-based graph convolutional layers to generate graph node embeddings,\nin which extensive smoothing convolution operation may dilute the desired\ndiscriminatory information of graph nodes. To overcome these issues, we propose\na novel Graph Learning-Matching Network (GLMNet) for graph matching problem.\nGLMNet has three main aspects. (1) It integrates graph learning into graph\nmatching which thus adaptively learn a pair of optimal graphs that best serve\ngraph matching task. (2) It further employs a Laplacian sharpening\nconvolutional module to generate more discriminative node embeddings for graph\nmatching. (3) A new constraint regularized loss is designed for GLMNet training\nwhich can encode the desired one-to-one matching constraints in matching\noptimization. Experiments on two benchmarks demonstrate the effectiveness of\nGLMNet and advantages of its main modules.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 15:04:59 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Jiang", "Bo", ""], ["Sun", "Pengfei", ""], ["Tang", "Jin", ""], ["Luo", "Bin", ""]]}, {"id": "1911.07685", "submitter": "Xiabi Liu", "authors": "Xiabi Liu, Xin Duan", "title": "Automatic Image Co-Segmentation: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image co-segmentation is important for its advantage of alleviating the\nill-pose nature of image segmentation through exploring the correlation between\nrelated images. Many automatic image co-segmentation algorithms have been\ndeveloped in the last decade, which are investigated comprehensively in this\npaper. We firstly analyze visual/semantic cues for guiding image\nco-segmentation, including object cues and correlation cues. Then we describe\nthe traditional methods in three categories of object elements based, object\nregions/contours based, common object model based. In the next part, deep\nlearning based methods are reviewed. Furthermore, widely used test datasets and\nevaluation criteria are introduced and the reported performances of the\nsurveyed algorithms are compared with each other. Finally, we discuss the\ncurrent challenges and possible future directions and conclude the paper.\nHopefully, this comprehensive investigation will be helpful for the development\nof image co-segmentation technique.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 15:07:34 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Liu", "Xiabi", ""], ["Duan", "Xin", ""]]}, {"id": "1911.07704", "submitter": "Nichita Diaconu", "authors": "Nichita Diaconu, Daniel E Worrall", "title": "Affine Self Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanisms, and most prominently self-attention, are a powerful\nbuilding block for processing not only text but also images. These provide a\nparameter efficient method for aggregating inputs. We focus on self-attention\nin vision models, and we combine it with convolution, which as far as we know,\nare the first to do. What emerges is a convolution with data dependent filters.\nWe call this an Affine Self Convolution. While this is applied differently at\neach spatial location, we show that it is translation equivariant. We also\nmodify the Squeeze and Excitation variant of attention, extending both variants\nof attention to the roto-translation group. We evaluate these new models on\nCIFAR10 and CIFAR100 and show an improvement in the number of parameters, while\nreaching comparable or higher accuracy at test time against self-trained\nbaselines.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 15:33:00 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Diaconu", "Nichita", ""], ["Worrall", "Daniel E", ""]]}, {"id": "1911.07716", "submitter": "Farhad Pourkamali-Anaraki", "authors": "Farhad Pourkamali-Anaraki and Michael B. Wakin", "title": "The Effectiveness of Variational Autoencoders for Active Learning", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high cost of acquiring labels is one of the main challenges in deploying\nsupervised machine learning algorithms. Active learning is a promising approach\nto control the learning process and address the difficulties of data labeling\nby selecting labeled training examples from a large pool of unlabeled\ninstances. In this paper, we propose a new data-driven approach to active\nlearning by choosing a small set of labeled data points that are both\ninformative and representative. To this end, we present an efficient geometric\ntechnique to select a diverse core-set in a low-dimensional latent space\nobtained by training a Variational Autoencoder (VAE). Our experiments\ndemonstrate an improvement in accuracy over two related techniques and, more\nimportantly, signify the representation power of generative modeling for\ndeveloping new active learning methods in high-dimensional data settings.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 15:42:20 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Pourkamali-Anaraki", "Farhad", ""], ["Wakin", "Michael B.", ""]]}, {"id": "1911.07731", "submitter": "Bernhard Stimpel", "authors": "Bernhard Stimpel, Christopher Syben, Franziska Schirrmacher, Philipp\n  Hoelter, Arnd D\\\"orfler, and Andreas Maier", "title": "Multi-modal Deep Guided Filtering for Comprehensible Medical Image\n  Processing", "comments": null, "journal-ref": "IEEE Transactions on Medical Imaging, vol. 39, no. 5, pp.\n  1703-1711, May 2020", "doi": "10.1109/TMI.2019.2955184", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based image processing is capable of creating highly appealing\nresults. However, it is still widely considered as a \"blackbox\" transformation.\nIn medical imaging, this lack of comprehensibility of the results is a\nsensitive issue. The integration of known operators into the deep learning\nenvironment has proven to be advantageous for the comprehensibility and\nreliability of the computations. Consequently, we propose the use of the\nlocally linear guided filter in combination with a learned guidance map for\ngeneral purpose medical image processing. The output images are only processed\nby the guided filter while the guidance map can be trained to be task-optimal\nin an end-to-end fashion. We investigate the performance based on two popular\ntasks: image super resolution and denoising. The evaluation is conducted based\non pairs of multi-modal magnetic resonance imaging and cross-modal computed\ntomography and magnetic resonance imaging datasets. For both tasks, the\nproposed approach is on par with state-of-the-art approaches. Additionally, we\ncan show that the input image's content is almost unchanged after the\nprocessing which is not the case for conventional deep learning approaches. On\ntop, the proposed pipeline offers increased robustness against degraded input\nas well as adversarial attacks.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 16:01:09 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 09:50:48 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Stimpel", "Bernhard", ""], ["Syben", "Christopher", ""], ["Schirrmacher", "Franziska", ""], ["Hoelter", "Philipp", ""], ["D\u00f6rfler", "Arnd", ""], ["Maier", "Andreas", ""]]}, {"id": "1911.07732", "submitter": "Patrick Follmann", "authors": "Patrick Follmann, Rebecca K\\\"onig", "title": "Oriented Boxes for Accurate Instance Segmentation", "comments": "v3 with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art instance-aware semantic segmentation algorithms use\naxis-aligned bounding boxes as an intermediate processing step to infer the\nfinal instance mask output. This often leads to coarse and inaccurate mask\nproposals due to the following reasons: Axis-aligned boxes have a high\nbackground to foreground pixel-ratio, there is a strong variation of mask\ntargets with respect to the underlying box, and neighboring instances\nfrequently reach into the axis-aligned bounding box of the instance mask of\ninterest. In this work, we overcome these problems by proposing to use oriented\nboxes as the basis to infer instance masks. We show that oriented instance\nsegmentation improves the mask predictions, especially when objects are\ndiagonally aligned, touching, or overlapping each other. We evaluate our model\non the D2S and Screws datasets and show that we can significantly improve the\nmask accuracy by 10% and 12% mAP compared to instance segmentation using\naxis-aligned bounding boxes, respectively. On the newly introduced Pill Bags\ndataset we outperform the baseline using only 10% of the mask annotations.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 16:01:22 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 08:39:38 GMT"}, {"version": "v3", "created": "Fri, 13 Mar 2020 10:36:01 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Follmann", "Patrick", ""], ["K\u00f6nig", "Rebecca", ""]]}, {"id": "1911.07736", "submitter": "Tianyu Hua", "authors": "Tianyu Hua, Maithilee Kunda", "title": "Modeling Gestalt Visual Reasoning on the Raven's Progressive Matrices\n  Intelligence Test Using Generative Image Inpainting Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Psychologists recognize Raven's Progressive Matrices as a very effective test\nof general human intelligence. While many computational models have been\ndeveloped by the AI community to investigate different forms of top-down,\ndeliberative reasoning on the test, there has been less research on bottom-up\nperceptual processes, like Gestalt image completion, that are also critical in\nhuman test performance. In this work, we investigate how Gestalt visual\nreasoning on the Raven's test can be modeled using generative image inpainting\ntechniques from computer vision. We demonstrate that a self-supervised\ninpainting model trained only on photorealistic images of objects achieves a\nscore of 27/36 on the Colored Progressive Matrices, which corresponds to\naverage performance for nine-year-old children. We also show that models\ntrained on other datasets (faces, places, and textures) do not perform as well.\nOur results illustrate how learning visual regularities in real-world images\ncan translate into successful reasoning about artificial test stimuli. On the\nflip side, our results also highlight the limitations of such transfer, which\nmay explain why intelligence tests like the Raven's are often sensitive to\npeople's individual sociocultural backgrounds.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 16:16:55 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 08:32:20 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Hua", "Tianyu", ""], ["Kunda", "Maithilee", ""]]}, {"id": "1911.07747", "submitter": "Qun Liu", "authors": "Qun Liu, Saikat Basu, Sangram Ganguly, Supratik Mukhopadhyay, Robert\n  DiBiano, Manohar Karki, Ramakrishna Nemani", "title": "DeepSat V2: Feature Augmented Convolutional Neural Nets for Satellite\n  Image Classification", "comments": "This is an Accepted Manuscript of an article published by Taylor &\n  Francis Group in Remote Sensing Letters. arXiv admin note: text overlap with\n  arXiv:1509.03602", "journal-ref": null, "doi": "10.1080/2150704X.2019.1693071", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Satellite image classification is a challenging problem that lies at the\ncrossroads of remote sensing, computer vision, and machine learning. Due to the\nhigh variability inherent in satellite data, most of the current object\nclassification approaches are not suitable for handling satellite datasets. The\nprogress of satellite image analytics has also been inhibited by the lack of a\nsingle labeled high-resolution dataset with multiple class labels. In a\npreliminary version of this work, we introduced two new high resolution\nsatellite imagery datasets (SAT-4 and SAT-6) and proposed DeepSat framework for\nclassification based on \"handcrafted\" features and a deep belief network (DBN).\nThe present paper is an extended version, we present an end-to-end framework\nleveraging an improved architecture that augments a convolutional neural\nnetwork (CNN) with handcrafted features (instead of using DBN-based\narchitecture) for classification. Our framework, having access to fused spatial\ninformation obtained from handcrafted features as well as CNN feature maps,\nhave achieved accuracies of 99.90% and 99.84% respectively, on SAT-4 and SAT-6,\nsurpassing all the other state-of-the-art results. A statistical analysis based\non Distribution Separability Criterion substantiates the robustness of our\napproach in learning better representations for satellite imagery.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 04:07:09 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Liu", "Qun", ""], ["Basu", "Saikat", ""], ["Ganguly", "Sangram", ""], ["Mukhopadhyay", "Supratik", ""], ["DiBiano", "Robert", ""], ["Karki", "Manohar", ""], ["Nemani", "Ramakrishna", ""]]}, {"id": "1911.07757", "submitter": "Vivien Sainte Fare Garnot", "authors": "Vivien Sainte Fare Garnot and Loic Landrieu and Sebastien Giordano and\n  Nesrine Chehata", "title": "Satellite Image Time Series Classification with Pixel-Set Encoders and\n  Temporal Self-Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satellite image time series, bolstered by their growing availability, are at\nthe forefront of an extensive effort towards automated Earth monitoring by\ninternational institutions. In particular, large-scale control of agricultural\nparcels is an issue of major political and economic importance. In this regard,\nhybrid convolutional-recurrent neural architectures have shown promising\nresults for the automated classification of satellite image time series.We\npropose an alternative approach in which the convolutional layers are\nadvantageously replaced with encoders operating on unordered sets of pixels to\nexploit the typically coarse resolution of publicly available satellite images.\nWe also propose to extract temporal features using a bespoke neural\narchitecture based on self-attention instead of recurrent networks. We\ndemonstrate experimentally that our method not only outperforms previous\nstate-of-the-art approaches in terms of precision, but also significantly\ndecreases processing time and memory requirements. Lastly, we release a large\nopen-access annotated dataset as a benchmark for future work on satellite image\ntime series.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 16:39:06 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Garnot", "Vivien Sainte Fare", ""], ["Landrieu", "Loic", ""], ["Giordano", "Sebastien", ""], ["Chehata", "Nesrine", ""]]}, {"id": "1911.07771", "submitter": "Lu\\'is Alexandre", "authors": "Nuno Pereira and Lu\\'is A. Alexandre", "title": "MaskedFusion: Mask-based 6D Object Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MaskedFusion is a framework to estimate the 6D pose of objects using RGB-D\ndata, with an architecture that leverages multiple sub-tasks in a pipeline to\nachieve accurate 6D poses. 6D pose estimation is an open challenge due to\ncomplex world objects and many possible problems when capturing data from the\nreal world, e.g., occlusions, truncations, and noise in the data. Achieving\naccurate 6D poses will improve results in other open problems like robot\ngrasping or positioning objects in augmented reality. MaskedFusion improves the\nstate-of-the-art by using object masks to eliminate non-relevant data. With the\ninclusion of the masks on the neural network that estimates the 6D pose of an\nobject we also have features that represent the object shape. MaskedFusion is a\nmodular pipeline where each sub-task can have different methods that achieve\nthe objective. MaskedFusion achieved 97.3% on average using the ADD metric on\nthe LineMOD dataset and 93.3% using the ADD-S AUC metric on YCB-Video Dataset,\nwhich is an improvement, compared to the state-of-the-art methods. The code is\navailable on GitHub (https://github.com/kroglice/MaskedFusion).\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 17:09:19 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 14:10:00 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Pereira", "Nuno", ""], ["Alexandre", "Lu\u00eds A.", ""]]}, {"id": "1911.07776", "submitter": "Shubham Kumar Singh", "authors": "Shubham Kumar Singh, Krishna P Miyapuram and Shanmuganathan Raman", "title": "DeepPFCN: Deep Parallel Feature Consensus Network For Person\n  Re-Identification", "comments": "8 pages, 3 Figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Person re-identification aims to associate images of the same person over\nmultiple non-overlapping camera views at different times. Depending on the\nhuman operator, manual re-identification in large camera networks is highly\ntime consuming and erroneous. Automated person re-identification is required\ndue to the extensive quantity of visual data produced by rapid inflation of\nlarge scale distributed multi-camera systems. The state-of-the-art works focus\non learning and factorize person appearance features into latent discriminative\nfactors at multiple semantic levels. We propose Deep Parallel Feature Consensus\nNetwork (DeepPFCN), a novel network architecture that learns multi-scale person\nappearance features using convolutional neural networks. This model factorizes\nthe visual appearance of a person into latent discriminative factors at\nmultiple semantic levels. Finally consensus is built. The feature\nrepresentations learned by DeepPFCN are more robust for the person\nre-identification task, as we learn discriminative scale-specific features and\nmaximize multi-scale feature fusion selections in multi-scale image inputs. We\nfurther exploit average and max pooling in separate scale for person-specific\ntask to discriminate features globally and locally. We demonstrate the\nre-identification advantages of the proposed DeepPFCN model over the\nstate-of-the-art re-identification methods on three benchmark datasets:\nMarket1501, DukeMTMCreID, and CUHK03. We have achieved mAP results of 75.8%,\n64.3%, and 52.6% respectively on these benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 17:21:35 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Singh", "Shubham Kumar", ""], ["Miyapuram", "Krishna P", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "1911.07783", "submitter": "Andreas Lugmayr", "authors": "Andreas Lugmayr, Martin Danelljan, Radu Timofte, Manuel Fritsche,\n  Shuhang Gu, Kuldeep Purohit, Praveen Kandula, Maitreya Suin, A N Rajagopalan,\n  Nam Hyung Joon, Yu Seung Won, Guisik Kim, Dokyeong Kwon, Chih-Chung Hsu,\n  Chia-Hsiang Lin, Yuanfei Huang, Xiaopeng Sun, Wen Lu, Jie Li, Xinbo Gao, Sefi\n  Bell-Kligler", "title": "AIM 2019 Challenge on Real-World Image Super-Resolution: Methods and\n  Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the AIM 2019 challenge on real world super-resolution. It\nfocuses on the participating methods and final results. The challenge addresses\nthe real world setting, where paired true high and low-resolution images are\nunavailable. For training, only one set of source input images is therefore\nprovided in the challenge. In Track 1: Source Domain the aim is to\nsuper-resolve such images while preserving the low level image characteristics\nof the source input domain. In Track 2: Target Domain a set of high-quality\nimages is also provided for training, that defines the output domain and\ndesired quality of the super-resolved images. To allow for quantitative\nevaluation, the source input images in both tracks are constructed using\nartificial, but realistic, image degradations. The challenge is the first of\nits kind, aiming to advance the state-of-the-art and provide a standard\nbenchmark for this newly emerging task. In total 7 teams competed in the final\ntesting phase, demonstrating new and innovative solutions to the problem.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 17:31:02 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 09:38:19 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Lugmayr", "Andreas", ""], ["Danelljan", "Martin", ""], ["Timofte", "Radu", ""], ["Fritsche", "Manuel", ""], ["Gu", "Shuhang", ""], ["Purohit", "Kuldeep", ""], ["Kandula", "Praveen", ""], ["Suin", "Maitreya", ""], ["Rajagopalan", "A N", ""], ["Joon", "Nam Hyung", ""], ["Won", "Yu Seung", ""], ["Kim", "Guisik", ""], ["Kwon", "Dokyeong", ""], ["Hsu", "Chih-Chung", ""], ["Lin", "Chia-Hsiang", ""], ["Huang", "Yuanfei", ""], ["Sun", "Xiaopeng", ""], ["Lu", "Wen", ""], ["Li", "Jie", ""], ["Gao", "Xinbo", ""], ["Bell-Kligler", "Sefi", ""]]}, {"id": "1911.07806", "submitter": "Yuge Shi", "authors": "Yuge Shi, Basura Fernando, Richard Hartley", "title": "Action Anticipation with RBF Kernelized Feature Mapping RNN", "comments": "Accepted for publication in ECCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a novel Recurrent Neural Network-based algorithm for future\nvideo feature generation and action anticipation called feature mapping RNN.\nOur novel RNN architecture builds upon three effective principles of machine\nlearning, namely parameter sharing, Radial Basis Function kernels and\nadversarial training. Using only some of the earliest frames of a video, the\nfeature mapping RNN is able to generate future features with a fraction of the\nparameters needed in traditional RNN. By feeding these future features into a\nsimple multi-layer perceptron facilitated with an RBF kernel layer, we are able\nto accurately predict the action in the video. In our experiments, we obtain\n18% improvement on JHMDB-21 dataset, 6% on UCF101-24 and 13% improvement on\nUT-Interaction datasets over prior state-of-the-art for action anticipation.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 18:13:56 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 09:49:41 GMT"}, {"version": "v3", "created": "Sun, 11 Jul 2021 16:07:38 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Shi", "Yuge", ""], ["Fernando", "Basura", ""], ["Hartley", "Richard", ""]]}, {"id": "1911.07808", "submitter": "Timo Milbich", "authors": "Timo Milbich, Omair Ghori, Ferran Diego, Bj\\\"orn Ommer", "title": "Unsupervised Representation Learning by Discovering Reliable Image\n  Relations", "comments": "Accepted for Publication in 'Pattern Recognition Journal'", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning robust representations that allow to reliably establish relations\nbetween images is of paramount importance for virtually all of computer vision.\nAnnotating the quadratic number of pairwise relations between training images\nis simply not feasible, while unsupervised inference is prone to noise, thus\nleaving the vast majority of these relations to be unreliable. To nevertheless\nfind those relations which can be reliably utilized for learning, we follow a\ndivide-and-conquer strategy: We find reliable similarities by extracting\ncompact groups of images and reliable dissimilarities by partitioning these\ngroups into subsets, converting the complicated overall problem into few\nreliable local subproblems. For each of the subsets we obtain a representation\nby learning a mapping to a target feature space so that their reliable\nrelations are kept. Transitivity relations between the subsets are then\nexploited to consolidate the local solutions into a concerted global\nrepresentation. While iterating between grouping, partitioning, and learning,\nwe can successively use more and more reliable relations which, in turn,\nimproves our image representation. In experiments, our approach shows\nstate-of-the-art performance on unsupervised classification on ImageNet with\n46.0% and competes favorably on different transfer learning tasks on PASCAL\nVOC.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 18:15:28 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Milbich", "Timo", ""], ["Ghori", "Omair", ""], ["Diego", "Ferran", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "1911.07817", "submitter": "Alla Eddine Guissous", "authors": "Alla Eddine Guissous", "title": "Skin Lesion Classification Using Deep Neural Network", "comments": "7 pages, 3 figures, accepted paper in 13th International Conference\n  on Interactive Mobile and Communication Technologies and Learning 2019 \"IMCL\n  Conference\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper reports the methods and techniques we have developed for classify\ndermoscopic images (task 1) of the ISIC 2019 challenge dataset for skin lesion\nclassification, our approach aims to use ensemble deep neural network with some\npowerful techniques to deal with unbalance data sets as its the main problem\nfor this challenge in a move to increase the performance of CNNs model.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 18:29:41 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Guissous", "Alla Eddine", ""]]}, {"id": "1911.07841", "submitter": "Tiancheng Xu", "authors": "Tiancheng Xu, Boyuan Tian, Yuhao Zhu", "title": "Tigris: Architecture and Algorithms for 3D Perception in Point Clouds", "comments": "Published at MICRO-52 (52nd IEEE/ACM International Symposium on\n  Microarchitecture); Tiancheng Xu and Boyuan Tian are co-primary authors", "journal-ref": null, "doi": "10.1145/3352460.3358259", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine perception applications are increasingly moving toward manipulating\nand processing 3D point cloud. This paper focuses on point cloud registration,\na key primitive of 3D data processing widely used in high-level tasks such as\nodometry, simultaneous localization and mapping, and 3D reconstruction. As\nthese applications are routinely deployed in energy-constrained environments,\nreal-time and energy-efficient point cloud registration is critical.\n  We present Tigris, an algorithm-architecture co-designed system specialized\nfor point cloud registration. Through an extensive exploration of the\nregistration pipeline design space, we find that, while different design points\nmake vastly different trade-offs between accuracy and performance, KD-tree\nsearch is a common performance bottleneck, and thus is an ideal candidate for\narchitectural specialization. While KD-tree search is inherently sequential, we\npropose an acceleration-amenable data structure and search algorithm that\nexposes different forms of parallelism of KD-tree search in the context of\npoint cloud registration. The co-designed accelerator systematically exploits\nthe parallelism while incorporating a set of architectural techniques that\nfurther improve the accelerator efficiency. Overall, Tigris achieves\n77.2$\\times$ speedup and 7.4$\\times$ power reduction in KD-tree search over an\nRTX 2080 Ti GPU, which translates to a 41.7% registration performance\nimprovements and 3.0$\\times$ power reduction.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 16:54:53 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 01:41:17 GMT"}, {"version": "v3", "created": "Thu, 21 Nov 2019 01:42:42 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Xu", "Tiancheng", ""], ["Tian", "Boyuan", ""], ["Zhu", "Yuhao", ""]]}, {"id": "1911.07844", "submitter": "Tharindu Fernando", "authors": "Tharindu Fernando, Clinton Fookes, Simon Denman, Sridha Sridharan", "title": "Exploiting Human Social Cognition for the Detection of Fake and\n  Fraudulent Faces via Memory Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in computer vision have brought us to the point where we have the\nability to synthesise realistic fake content. Such approaches are seen as a\nsource of disinformation and mistrust, and pose serious concerns to governments\naround the world. Convolutional Neural Networks (CNNs) demonstrate encouraging\nresults when detecting fake images that arise from the specific type of\nmanipulation they are trained on. However, this success has not transitioned to\nunseen manipulation types, resulting in a significant gap in the\nline-of-defense. We propose a Hierarchical Memory Network (HMN) architecture,\nwhich is able to successfully detect faked faces by utilising knowledge stored\nin neural memories as well as visual cues to reason about the perceived face\nand anticipate its future semantic embeddings. This renders a generalisable\nface tampering detection framework. Experimental results demonstrate the\nproposed approach achieves superior performance for fake and fraudulent face\ndetection compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 23:20:23 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Fernando", "Tharindu", ""], ["Fookes", "Clinton", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""]]}, {"id": "1911.07845", "submitter": "Yun-Hao Cao", "authors": "Yun-Hao Cao and Jianxin Wu and Hanchen Wang and Joan Lasenby", "title": "Neural Random Subspace", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The random subspace method, known as the pillar of random forests, is good at\nmaking precise and robust predictions. However, there is not a straightforward\nway yet to combine it with deep learning. In this paper, we therefore propose\nNeural Random Subspace (NRS), a novel deep learning based random subspace\nmethod. In contrast to previous forest methods, NRS enjoys the benefits of\nend-to-end, data-driven representation learning, as well as pervasive support\nfrom deep learning software and hardware platforms, hence achieving faster\ninference speed and higher accuracy. Furthermore, as a non-linear component to\nbe encoded into Convolutional Neural Networks (CNNs), NRS learns non-linear\nfeature representations in CNNs more efficiently than previous higher-order\npooling methods, producing good results with negligible increase in parameters,\nfloating point operations (FLOPs) and real running time. Compared with random\nsubspaces, random forests and gradient boosting decision trees (GBDTs), NRS\nachieves superior performance on 35 machine learning datasets. Moreover, on\nboth 2D image and 3D point cloud recognition tasks, integration of NRS with CNN\narchitectures achieves consistent improvements with minor extra cost. Code is\navailable at https://github.com/CupidJay/NRS_pytorch.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 02:28:04 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 01:03:43 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 01:07:47 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Cao", "Yun-Hao", ""], ["Wu", "Jianxin", ""], ["Wang", "Hanchen", ""], ["Lasenby", "Joan", ""]]}, {"id": "1911.07846", "submitter": "Shi Yin", "authors": "Shangfei Wang, Shi Yin, Longfei Hao and Guang Liang", "title": "Multiple Face Analyses through Adversarial Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This inherent relations among multiple face analysis tasks, such as landmark\ndetection, head pose estimation, gender recognition and face attribute\nestimation are crucial to boost the performance of each task, but have not been\nthoroughly explored since typically these multiple face analysis tasks are\nhandled as separate tasks. In this paper, we propose a novel deep multi-task\nadversarial learning method to localize facial landmark, estimate head pose and\nrecognize gender jointly or estimate multiple face attributes simultaneously\nthrough exploring their dependencies from both image representation-level and\nlabel-level. Specifically, the proposed method consists of a deep recognition\nnetwork R and a discriminator D. The deep recognition network is used to learn\nthe shared middle-level image representation and conducts multiple face\nanalysis tasks simultaneously. Through multi-task learning mechanism, the\nrecognition network explores the dependencies among multiple face analysis\ntasks, such as facial landmark localization, head pose estimation, gender\nrecognition and face attribute estimation from image representation-level. The\ndiscriminator is introduced to enforce the distribution of the multiple face\nanalysis tasks to converge to that inherent in the ground-truth labels. During\ntraining, the recognizer tries to confuse the discriminator, while the\ndiscriminator competes with the recognizer through distinguishing the predicted\nlabel combination from the ground-truth one. Though adversarial learning, we\nexplore the dependencies among multiple face analysis tasks from label-level.\nExperimental results on four benchmark databases, i.e., the AFLW database, the\nMulti-PIE database, the CelebA database and the LFWA database, demonstrate the\neffectiveness of the proposed method for multiple face analyses.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 04:24:17 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Wang", "Shangfei", ""], ["Yin", "Shi", ""], ["Hao", "Longfei", ""], ["Liang", "Guang", ""]]}, {"id": "1911.07847", "submitter": "Ghouthi Boukli Hacene Gbh", "authors": "Ghouthi Boukli Hacene, Vincent Gripon, Nicolas Farrugia, Matthieu\n  Arzel and Michel Jezequel", "title": "Efficient Hardware Implementation of Incremental Learning and Inference\n  on Chip", "comments": "In 2019 IEEE International NEWCAS Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of incrementally learning a classifier,\none example at a time, directly on chip. To this end, we propose an efficient\nhardware implementation of a recently introduced incremental learning procedure\nthat achieves state-of-the-art performance by combining transfer learning with\nmajority votes and quantization techniques. The proposed design is able to\naccommodate for both new examples and new classes directly on the chip. We\ndetail the hardware implementation of the method (implemented on FPGA target)\nand show it requires limited resources while providing a significant\nacceleration compared to using a CPU.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 04:42:09 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Hacene", "Ghouthi Boukli", ""], ["Gripon", "Vincent", ""], ["Farrugia", "Nicolas", ""], ["Arzel", "Matthieu", ""], ["Jezequel", "Michel", ""]]}, {"id": "1911.07848", "submitter": "Sijie Mai", "authors": "Sijie Mai and Haifeng Hu and Songlong Xing", "title": "Modality to Modality Translation: An Adversarial Representation Learning\n  and Graph Fusion Network for Multimodal Fusion", "comments": "Accepted by AAAI-2020; code is available at:\n  https://github.com/TmacMai/ARGF_multimodal_fusion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning joint embedding space for various modalities is of vital importance\nfor multimodal fusion. Mainstream modality fusion approaches fail to achieve\nthis goal, leaving a modality gap which heavily affects cross-modal fusion. In\nthis paper, we propose a novel adversarial encoder-decoder-classifier framework\nto learn a modality-invariant embedding space. Since the distributions of\nvarious modalities vary in nature, to reduce the modality gap, we translate the\ndistributions of source modalities into that of target modality via their\nrespective encoders using adversarial training. Furthermore, we exert\nadditional constraints on embedding space by introducing reconstruction loss\nand classification loss. Then we fuse the encoded representations using\nhierarchical graph neural network which explicitly explores unimodal, bimodal\nand trimodal interactions in multi-stage. Our method achieves state-of-the-art\nperformance on multiple datasets. Visualization of the learned embeddings\nsuggests that the joint embedding space learned by our method is\ndiscriminative. code is available at:\n\\url{https://github.com/TmacMai/ARGF_multimodal_fusion}\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 08:29:20 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 08:34:28 GMT"}, {"version": "v3", "created": "Thu, 12 Dec 2019 15:43:06 GMT"}, {"version": "v4", "created": "Thu, 10 Dec 2020 01:52:20 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Mai", "Sijie", ""], ["Hu", "Haifeng", ""], ["Xing", "Songlong", ""]]}, {"id": "1911.07849", "submitter": "David W. Romero", "authors": "David W. Romero, Mark Hoogendoorn", "title": "Co-Attentive Equivariant Neural Networks: Focusing Equivariance On\n  Transformations Co-Occurring In Data", "comments": "Proceedings of the 8th International Conference on Learning\n  Representations (ICLR), 2020", "journal-ref": "Proceedings of the International Conference on Learning\n  Representations, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Equivariance is a nice property to have as it produces much more parameter\nefficient neural architectures and preserves the structure of the input through\nthe feature mapping. Even though some combinations of transformations might\nnever appear (e.g. an upright face with a horizontal nose), current equivariant\narchitectures consider the set of all possible transformations in a\ntransformation group when learning feature representations. Contrarily, the\nhuman visual system is able to attend to the set of relevant transformations\noccurring in the environment and utilizes this information to assist and\nimprove object recognition. Based on this observation, we modify conventional\nequivariant feature mappings such that they are able to attend to the set of\nco-occurring transformations in data and generalize this notion to act on\ngroups consisting of multiple symmetries. We show that our proposed\nco-attentive equivariant neural networks consistently outperform conventional\nrotation equivariant and rotation & reflection equivariant neural networks on\nrotated MNIST and CIFAR-10.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 12:41:12 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 13:56:10 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Romero", "David W.", ""], ["Hoogendoorn", "Mark", ""]]}, {"id": "1911.07850", "submitter": "Manuel Fritsche", "authors": "Manuel Fritsche, Shuhang Gu, Radu Timofte", "title": "Frequency Separation for Real-World Super-Resolution", "comments": "winner of AIM 2019 Real World Super-Resolution challenge, paper\n  published in ICCV 2019 Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the recent literature on image super-resolution (SR) assumes the\navailability of training data in the form of paired low resolution (LR) and\nhigh resolution (HR) images or the knowledge of the downgrading operator\n(usually bicubic downscaling). While the proposed methods perform well on\nstandard benchmarks, they often fail to produce convincing results in\nreal-world settings. This is because real-world images can be subject to\ncorruptions such as sensor noise, which are severely altered by bicubic\ndownscaling. Therefore, the models never see a real-world image during\ntraining, which limits their generalization capabilities. Moreover, it is\ncumbersome to collect paired LR and HR images in the same source domain.\n  To address this problem, we propose DSGAN to introduce natural image\ncharacteristics in bicubically downscaled images. It can be trained in an\nunsupervised fashion on HR images, thereby generating LR images with the same\ncharacteristics as the original images. We then use the generated data to train\na SR model, which greatly improves its performance on real-world images.\nFurthermore, we propose to separate the low and high image frequencies and\ntreat them differently during training. Since the low frequencies are preserved\nby downsampling operations, we only require adversarial training to modify the\nhigh frequencies. This idea is applied to our DSGAN model as well as the SR\nmodel. We demonstrate the effectiveness of our method in several experiments\nthrough quantitative and qualitative analysis. Our solution is the winner of\nthe AIM Challenge on Real World SR at ICCV 2019.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 17:08:28 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Fritsche", "Manuel", ""], ["Gu", "Shuhang", ""], ["Timofte", "Radu", ""]]}, {"id": "1911.07883", "submitter": "Fengda Zhu", "authors": "Fengda Zhu, Yi Zhu, Xiaojun Chang, Xiaodan Liang", "title": "Vision-Language Navigation with Self-Supervised Auxiliary Reasoning\n  Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-Language Navigation (VLN) is a task where agents learn to navigate\nfollowing natural language instructions. The key to this task is to perceive\nboth the visual scene and natural language sequentially. Conventional\napproaches exploit the vision and language features in cross-modal grounding.\nHowever, the VLN task remains challenging, since previous works have neglected\nthe rich semantic information contained in the environment (such as implicit\nnavigation graphs or sub-trajectory semantics). In this paper, we introduce\nAuxiliary Reasoning Navigation (AuxRN), a framework with four self-supervised\nauxiliary reasoning tasks to take advantage of the additional training signals\nderived from the semantic information. The auxiliary tasks have four reasoning\nobjectives: explaining the previous actions, estimating the navigation\nprogress, predicting the next orientation, and evaluating the trajectory\nconsistency. As a result, these additional training signals help the agent to\nacquire knowledge of semantic representations in order to reason about its\nactivity and build a thorough perception of the environment. Our experiments\nindicate that auxiliary reasoning tasks improve both the performance of the\nmain task and the model generalizability by a large margin. Empirically, we\ndemonstrate that an agent trained with self-supervised auxiliary reasoning\ntasks substantially outperforms the previous state-of-the-art method, being the\nbest existing approach on the standard benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 19:17:57 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 12:14:35 GMT"}, {"version": "v3", "created": "Thu, 28 Nov 2019 10:45:18 GMT"}, {"version": "v4", "created": "Wed, 1 Apr 2020 04:24:49 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Zhu", "Fengda", ""], ["Zhu", "Yi", ""], ["Chang", "Xiaojun", ""], ["Liang", "Xiaodan", ""]]}, {"id": "1911.07896", "submitter": "Vishnu Sashank Dorbala", "authors": "Vishnu Sashank Dorbala, A.H. Abdul Hafez, C.V. Jawahar", "title": "A Deep Learning Approach for Robust Corridor Following", "comments": "7 pages, 7 figures. Paper published at 2019 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an autonomous corridor following task where the environment is\ncontinuously changing, several forms of environmental noise prevent an\nautomated feature extraction procedure from performing reliably. Moreover, in\ncases where pre-defined features are absent from the captured data, a well\ndefined control signal for performing the servoing task fails to get produced.\nIn order to overcome these drawbacks, we present in this work, using a\nconvolutional neural network (CNN) to directly estimate the required control\nsignal from an image, encompassing feature extraction and control law\ncomputation into one single end-to-end framework. In particular, we study the\ntask of autonomous corridor following using a CNN and present clear advantages\nin cases where a traditional method used for performing the same task fails to\ngive a reliable outcome. We evaluate the performance of our method on this task\non a Wheelchair Platform developed at our institute for this purpose.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 19:46:54 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Dorbala", "Vishnu Sashank", ""], ["Hafez", "A. H. Abdul", ""], ["Jawahar", "C. V.", ""]]}, {"id": "1911.07916", "submitter": "Adonis Emmanuel Tio", "authors": "Adonis Emmanuel Tio", "title": "Face shape classification using Inception v3", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present experimental results obtained from retraining the\nlast layer of the Inception v3 model in classifying images of human faces into\none of five basic face shapes. The accuracy of the retrained Inception v3 model\nwas compared with that of the following classification methods that uses facial\nlandmark distance ratios and angles as features: linear discriminant analysis\n(LDA), support vector machines with linear kernel (SVM-LIN), support vector\nmachines with radial basis function kernel (SVM-RBF), artificial neural\nnetworks or multilayer perceptron (MLP), and k-nearest neighbors (KNN). All\nclassifiers were trained and tested using a total of 500 images of female\ncelebrities with known face shapes collected from the Internet. Results show\nthat training accuracy and overall accuracy ranges from 98.0% to 100% and from\n84.4% to 84.8% for Inception v3 and from 50.6% to 73.0% and from 36.4% to 64.6%\nfor the other classifiers depending on the training set size used. This result\nshows that the retrained Inception v3 model was able to fit the training data\nwell and outperform the other classifiers without the need to handpick specific\nfeatures to include in model training. Future work should consider expanding\nthe labeled dataset, preferably one that can also be freely distributed to the\nresearch community, so that proper model cross-validation can be performed. As\nfar as we know, this is the first in the literature to use convolutional neural\nnetworks in face-shape classification. The scripts are available at\nhttps://github.com/adonistio/inception-face-shape-classifier.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 02:29:59 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Tio", "Adonis Emmanuel", ""]]}, {"id": "1911.07917", "submitter": "Xin Shu", "authors": "Shaoyong Jia, Xin Shu, Yang Yang, Dawei Liang, Qiyue Liu, Junhui Liu", "title": "Cross-modal supervised learning for better acoustic representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining large-scale human-labeled datasets to train acoustic representation\nmodels is a very challenging task. On the contrary, we can easily collect data\nwith machine-generated labels. In this work, we propose to exploit\nmachine-generated labels to learn better acoustic representations, based on the\nsynchronization between vision and audio. Firstly, we collect a large-scale\nvideo dataset with 15 million samples, which totally last 16,320 hours. Each\nvideo is 3 to 5 seconds in length and annotated automatically by publicly\navailable visual and audio classification models. Secondly, we train various\nclassical convolutional neural networks (CNNs) including VGGish, ResNet 50 and\nMobilenet v2. We also make several improvements to VGGish and achieve better\nresults. Finally, we transfer our models on three external standard benchmarks\nfor audio classification task, and achieve significant performance boost over\nthe state-of-the-art results. Models and codes are available at:\nhttps://github.com/Deeperjia/vgg-like-audio-models.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 02:23:23 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2020 06:22:39 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Jia", "Shaoyong", ""], ["Shu", "Xin", ""], ["Yang", "Yang", ""], ["Liang", "Dawei", ""], ["Liu", "Qiyue", ""], ["Liu", "Junhui", ""]]}, {"id": "1911.07919", "submitter": "Yu Feng", "authors": "Yu Feng, Paul Whatmough, Yuhao Zhu", "title": "ASV: Accelerated Stereo Vision System", "comments": "MICRO 2019", "journal-ref": "In Proceedings of the 52nd Annual IEEE/ACM International Symposium\n  on Microarchitecture (MICRO '52). ACM, New York, NY, USA, 643-656 (2019)", "doi": "10.1145/3352460.3358253", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating depth from stereo vision cameras, i.e., \"depth from stereo\", is\ncritical to emerging intelligent applications deployed in energy- and\nperformance-constrained devices, such as augmented reality headsets and mobile\nautonomous robots. While existing stereo vision systems make trade-offs between\naccuracy, performance and energy-efficiency, we describe ASV, an accelerated\nstereo vision system that simultaneously improves both performance and\nenergy-efficiency while achieving high accuracy. The key to ASV is to exploit\nunique characteristics inherent to stereo vision, and apply stereo-specific\noptimizations, both algorithmically and computationally. We make two\ncontributions. Firstly, we propose a new stereo algorithm, invariant-based\nstereo matching (ISM), that achieves significant speedup while retaining high\naccuracy. The algorithm combines classic \"hand-crafted\" stereo algorithms with\nrecent developments in Deep Neural Networks (DNNs), by leveraging the\ncorrespondence invariant unique to stereo vision systems. Secondly, we observe\nthat the bottleneck of the ISM algorithm is the DNN inference, and in\nparticular the deconvolution operations that introduce massive\ncompute-inefficiencies. We propose a set of software optimizations that\nmitigate these inefficiencies. We show that with less than 0.5% hardware area\noverhead, these algorithmic and computational optimizations can be effectively\nintegrated within a conventional DNN accelerator. Overall, ASV achieves 5x\nspeedup and 85% energy saving with 0.02% accuracy loss compared to today\nDNN-based stereo vision systems.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 18:44:25 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Feng", "Yu", ""], ["Whatmough", "Paul", ""], ["Zhu", "Yuhao", ""]]}, {"id": "1911.07922", "submitter": "Marcus Bloice", "authors": "Marcus D. Bloice, Peter M. Roth, Andreas Holzinger", "title": "Patch augmentation: Towards efficient decision boundaries for neural\n  networks", "comments": "Version 2: updated author list, reduced abstract length, plots\n  consolidated as sub-plots", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new augmentation technique, called patch\naugmentation, that, in our experiments, improves model accuracy and makes\nnetworks more robust to adversarial attacks. In brief, this data-independent\napproach creates new image data based on image/label pairs, where a patch from\none of the two images in the pair is superimposed on to the other image,\ncreating a new augmented sample. The new image's label is a linear combination\nof the image pair's corresponding labels. Initial experiments show a several\npercentage point increase in accuracy on CIFAR-10, from a baseline of\napproximately 81% to 89%. CIFAR-100 sees larger improvements still, from a\nbaseline of 52% to 68% accuracy. Networks trained using patch augmentation are\nalso more robust to adversarial attacks, which we demonstrate using the Fast\nGradient Sign Method.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 07:11:31 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 14:05:08 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Bloice", "Marcus D.", ""], ["Roth", "Peter M.", ""], ["Holzinger", "Andreas", ""]]}, {"id": "1911.07923", "submitter": "Lu Wang", "authors": "Lu Wang, Jie Yang", "title": "Cluster-wise Unsupervised Hashing for Cross-Modal Similarity Search", "comments": "13 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale cross-modal hashing similarity retrieval has attracted more and\nmore attention in modern search applications such as search engines and\nautopilot, showing great superiority in computation and storage. However,\ncurrent unsupervised cross-modal hashing methods still have some limitations:\n(1)many methods relax the discrete constraints to solve the optimization\nobjective which may significantly degrade the retrieval performance;(2)most\nexisting hashing model project heterogenous data into a common latent space,\nwhich may always lose sight of diversity in heterogenous data;(3)transforming\nreal-valued data point to binary codes always results in abundant loss of\ninformation, producing the suboptimal continuous latent space. To overcome\nabove problems, in this paper, a novel Cluster-wise Unsupervised Hashing (CUH)\nmethod is proposed. Specifically, CUH jointly performs the multi-view\nclustering that projects the original data points from different modalities\ninto its own low-dimensional latent semantic space and finds the cluster\ncentroid points and the common clustering indicators in its own low-dimensional\nspace, and learns the compact hash codes and the corresponding linear hash\nfunctions. An discrete optimization framework is developed to learn the unified\nbinary codes across modalities under the guidance cluster-wise code-prototypes.\nThe reasonableness and effectiveness of CUH is well demonstrated by\ncomprehensive experiments on diverse benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 05:50:01 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 11:51:18 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Wang", "Lu", ""], ["Yang", "Jie", ""]]}, {"id": "1911.07924", "submitter": "Weiqing Min", "authors": "Jing Wang, Weiqing Min, Sujuan Hou, Shengnan Ma, Yuanjie Zheng,\n  Haishuai Wang, Shuqiang Jiang", "title": "Logo-2K+: A Large-Scale Logo Dataset for Scalable Logo Classification", "comments": "Accepted by AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logo classification has gained increasing attention for its various\napplications, such as copyright infringement detection, product recommendation\nand contextual advertising. Compared with other types of object images, the\nreal-world logo images have larger variety in logo appearance and more\ncomplexity in their background. Therefore, recognizing the logo from images is\nchallenging. To support efforts towards scalable logo classification task, we\nhave curated a dataset, Logo-2K+, a new large-scale publicly available\nreal-world logo dataset with 2,341 categories and 167,140 images. Compared with\nexisting popular logo datasets, such as FlickrLogos-32 and LOGO-Net, Logo-2K+\nhas more comprehensive coverage of logo categories and larger quantity of logo\nimages. Moreover, we propose a Discriminative Region Navigation and\nAugmentation Network (DRNA-Net), which is capable of discovering more\ninformative logo regions and augmenting these image regions for logo\nclassification. DRNA-Net consists of four sub-networks: the navigator\nsub-network first selected informative logo-relevant regions guided by the\nteacher sub-network, which can evaluate its confidence belonging to the\nground-truth logo class. The data augmentation sub-network then augments the\nselected regions via both region cropping and region dropping. Finally, the\nscrutinizer sub-network fuses features from augmented regions and the whole\nimage for logo classification. Comprehensive experiments on Logo-2K+ and other\nthree existing benchmark datasets demonstrate the effectiveness of proposed\nmethod. Logo-2K+ and the proposed strong baseline DRNA-Net are expected to\nfurther the development of scalable logo image recognition, and the Logo-2K+\ndataset can be found at https://github.com/msn199959/Logo-2k-plus-Dataset.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 09:24:08 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Wang", "Jing", ""], ["Min", "Weiqing", ""], ["Hou", "Sujuan", ""], ["Ma", "Shengnan", ""], ["Zheng", "Yuanjie", ""], ["Wang", "Haishuai", ""], ["Jiang", "Shuqiang", ""]]}, {"id": "1911.07925", "submitter": "Tianfu Li", "authors": "Tianfu Li, Zhibin Zhao, Chuang Sun, Li Cheng, Xuefeng Chen, Ruqiang\n  Yan, Robert X. Gao", "title": "WaveletKernelNet: An Interpretable Deep Neural Network for Industrial\n  Intelligent Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN), with ability of feature learning and\nnonlinear mapping, has demonstrated its effectiveness in prognostics and health\nmanagement (PHM). However, explanation on the physical meaning of a CNN\narchitecture has rarely been studied. In this paper, a novel wavelet driven\ndeep neural network termed as WaveletKernelNet (WKN) is presented, where a\ncontinuous wavelet convolutional (CWConv) layer is designed to replace the\nfirst convolutional layer of the standard CNN. This enables the first CWConv\nlayer to discover more meaningful filters. Furthermore, only the scale\nparameter and translation parameter are directly learned from raw data at this\nCWConv layer. This provides a very effective way to obtain a customized filter\nbank, specifically tuned for extracting defect-related impact component\nembedded in the vibration signal. In addition, three experimental verification\nusing data from laboratory environment are carried out to verify effectiveness\nof the proposed method for mechanical fault diagnosis. The results show the\nimportance of the designed CWConv layer and the output of CWConv layer is\ninterpretable. Besides, it is found that WKN has fewer parameters, higher fault\nclassification accuracy and faster convergence speed than standard CNN.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 07:22:56 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 12:47:17 GMT"}, {"version": "v3", "created": "Wed, 6 May 2020 04:37:47 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Li", "Tianfu", ""], ["Zhao", "Zhibin", ""], ["Sun", "Chuang", ""], ["Cheng", "Li", ""], ["Chen", "Xuefeng", ""], ["Yan", "Ruqiang", ""], ["Gao", "Robert X.", ""]]}, {"id": "1911.07926", "submitter": "Shion Honda", "authors": "Shion Honda", "title": "VITON-GAN: Virtual Try-on Image Generator Trained with Adversarial Loss", "comments": "2 pages, 4 figures. Accepted to Eurographics 2019 (Posters)", "journal-ref": "Eurographics, 2019", "doi": "10.2312/egp.20191043", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating a virtual try-on image from in-shop clothing images and a model\nperson's snapshot is a challenging task because the human body and clothes have\nhigh flexibility in their shapes. In this paper, we develop a Virtual Try-on\nGenerative Adversarial Network (VITON-GAN), that generates virtual try-on\nimages using images of in-shop clothing and a model person. This method\nenhances the quality of the generated image when occlusion is present in a\nmodel person's image (e.g., arms crossed in front of the clothes) by adding an\nadversarial mechanism in the training pipeline.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 10:09:11 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Honda", "Shion", ""]]}, {"id": "1911.07927", "submitter": "Vishwesh Nath", "authors": "Vishwesh Nath, Kurt G. Schilling, Colin B. Hansen, Prasanna\n  Parvathaneni, Allison E. Hainline, Camilo Bermudez, Andrew J. Plassard,\n  Vaibhav Janve, Yurui Gao, Justin A. Blaber, Iwona St\\k{e}pniewska, Adam W.\n  Anderson, Bennett A. Landman", "title": "Deep Learning Captures More Accurate Diffusion Fiber Orientations\n  Distributions than Constrained Spherical Deconvolution", "comments": "2 pages, 4 figures. This work was accepted and published as an\n  abstract at ISMRM 2018 held in Paris, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confocal histology provides an opportunity to establish intra-voxel fiber\norientation distributions that can be used to quantitatively assess the\nbiological relevance of diffusion weighted MRI models, e.g., constrained\nspherical deconvolution (CSD). Here, we apply deep learning to investigate the\npotential of single shell diffusion weighted MRI to explain histologically\nobserved fiber orientation distributions (FOD) and compare the derived deep\nlearning model with a leading CSD approach. This study (1) demonstrates that\nthere exists additional information in the diffusion signal that is not\ncurrently exploited by CSD, and (2) provides an illustrative data-driven model\nthat makes use of this information.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 17:00:11 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Nath", "Vishwesh", ""], ["Schilling", "Kurt G.", ""], ["Hansen", "Colin B.", ""], ["Parvathaneni", "Prasanna", ""], ["Hainline", "Allison E.", ""], ["Bermudez", "Camilo", ""], ["Plassard", "Andrew J.", ""], ["Janve", "Vaibhav", ""], ["Gao", "Yurui", ""], ["Blaber", "Justin A.", ""], ["St\u0119pniewska", "Iwona", ""], ["Anderson", "Adam W.", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1911.07928", "submitter": "Wei Pang Xubu", "authors": "Wei Pang and Xiaojie Wang", "title": "Visual Dialogue State Tracking for Question Generation", "comments": "8 pages, 4 figures, Accept-Oral by AAAI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GuessWhat?! is a visual dialogue task between a guesser and an oracle. The\nguesser aims to locate an object supposed by the oracle oneself in an image by\nasking a sequence of Yes/No questions. Asking proper questions with the\nprogress of dialogue is vital for achieving successful final guess. As a\nresult, the progress of dialogue should be properly represented and tracked.\nPrevious models for question generation pay less attention on the\nrepresentation and tracking of dialogue states, and therefore are prone to\nasking low quality questions such as repeated questions. This paper proposes\nvisual dialogue state tracking (VDST) based method for question generation. A\nvisual dialogue state is defined as the distribution on objects in the image as\nwell as representations of objects. Representations of objects are updated with\nthe change of the distribution on objects. An object-difference based attention\nis used to decode new question. The distribution on objects is updated by\ncomparing the question-answer pair and objects. Experimental results on\nGuessWhat?! dataset show that our model significantly outperforms existing\nmethods and achieves new state-of-the-art performance. It is also noticeable\nthat our model reduces the rate of repeated questions from more than 50% to\n21.9% compared with previous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 15:54:55 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 03:32:28 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Pang", "Wei", ""], ["Wang", "Xiaojie", ""]]}, {"id": "1911.07929", "submitter": "Jessica Velasco", "authors": "Jessica Velasco, Cherry Pascion, Jean Wilmar Alberio, Jonathan Apuang,\n  John Stephen Cruz, Mark Angelo Gomez, Benjamin Jr. Molina, Lyndon Tuala,\n  August Thio-ac and Romeo Jr. Jorda", "title": "A Smartphone-Based Skin Disease Classification Using MobileNet CNN", "comments": null, "journal-ref": "International Journal of Advanced Trends in Computer Science and\n  Engineering (2019) 2632-2637", "doi": "10.30534/ijatcse/2019/116852019", "report-no": null, "categories": "cs.CV cs.CY cs.LG eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The MobileNet model was used by applying transfer learning on the 7 skin\ndiseases to create a skin disease classification system on Android application.\nThe proponents gathered a total of 3,406 images and it is considered as\nimbalanced dataset because of the unequal number of images on its classes.\nUsing different sampling method and preprocessing of input data was explored to\nfurther improved the accuracy of the MobileNet. Using under-sampling method and\nthe default preprocessing of input data achieved an 84.28% accuracy. While,\nusing imbalanced dataset and default preprocessing of input data achieved a\n93.6% accuracy. Then, researchers explored oversampling the dataset and the\nmodel attained a 91.8% accuracy. Lastly, by using oversampling technique and\ndata augmentation on preprocessing the input data provide a 94.4% accuracy and\nthis model was deployed on the developed Android application.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 11:04:05 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Velasco", "Jessica", ""], ["Pascion", "Cherry", ""], ["Alberio", "Jean Wilmar", ""], ["Apuang", "Jonathan", ""], ["Cruz", "John Stephen", ""], ["Gomez", "Mark Angelo", ""], ["Molina", "Benjamin Jr.", ""], ["Tuala", "Lyndon", ""], ["Thio-ac", "August", ""], ["Jorda", "Romeo Jr.", ""]]}, {"id": "1911.07930", "submitter": "Maruf Ahmed Dhali", "authors": "Maruf A. Dhali, Jan Willem de Wit, Lambert Schomaker", "title": "BiNet: Degraded-Manuscript Binarization in Diverse Document Textures and\n  Layouts using Deep Encoder-Decoder Networks", "comments": "26 pages, 15 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handwritten document-image binarization is a semantic segmentation process to\ndifferentiate ink pixels from background pixels. It is one of the essential\nsteps towards character recognition, writer identification, and script-style\nevolution analysis. The binarization task itself is challenging due to the vast\ndiversity of writing styles, inks, and paper materials. It is even more\ndifficult for historical manuscripts due to the aging and degradation of the\ndocuments over time. One of such manuscripts is the Dead Sea Scrolls (DSS)\nimage collection, which poses extreme challenges for the existing binarization\ntechniques. This article proposes a new binarization technique for the DSS\nimages using the deep encoder-decoder networks. Although the artificial neural\nnetwork proposed here is primarily designed to binarize the DSS images, it can\nbe trained on different manuscript collections as well. Additionally, the use\nof transfer learning makes the network already utilizable for a wide range of\nhandwritten documents, making it a unique multi-purpose tool for binarization.\nQualitative results and several quantitative comparisons using both historical\nmanuscripts and datasets from handwritten document image binarization\ncompetition (H-DIBCO and DIBCO) exhibit the robustness and the effectiveness of\nthe system. The best performing network architecture proposed here is a variant\nof the U-Net encoder-decoders.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 20:12:35 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Dhali", "Maruf A.", ""], ["de Wit", "Jan Willem", ""], ["Schomaker", "Lambert", ""]]}, {"id": "1911.07931", "submitter": "Pengcheng Zhang", "authors": "Pengcheng Zhang, Qiyin Dai, Patrizio Pelliccione", "title": "CAGFuzz: Coverage-Guided Adversarial Generative Fuzzing Testing of Deep\n  Learning Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning systems (DL) based on Deep Neural Networks (DNNs) are more and\nmore used in various aspects of our life, including unmanned vehicles, speech\nprocessing, and robotics. However, due to the limited dataset and the\ndependence on manual labeling data, DNNs often fail to detect their erroneous\nbehaviors, which may lead to serious problems. Several approaches have been\nproposed to enhance the input examples for testing DL systems. However, they\nhave the following limitations. First, they design and generate adversarial\nexamples from the perspective of model, which may cause low generalization\nability when they are applied to other models. Second, they only use surface\nfeature constraints to judge the difference between the adversarial example\ngenerated and the original example. The deep feature constraints, which contain\nhigh-level semantic information, such as image object category and scene\nsemantics are completely neglected. To address these two problems, in this\npaper, we propose CAGFuzz, a Coverage-guided Adversarial Generative Fuzzing\ntesting approach, which generates adversarial examples for a targeted DNN to\ndiscover its potential defects. First, we train an adversarial case generator\n(AEG) from the perspective of general data set. Second, we extract the depth\nfeatures of the original and adversarial examples, and constrain the\nadversarial examples by cosine similarity to ensure that the semantic\ninformation of adversarial examples remains unchanged. Finally, we retrain\neffective adversarial examples to improve neuron testing coverage rate. Based\non several popular data sets, we design a set of dedicated experiments to\nevaluate CAGFuzz. The experimental results show that CAGFuzz can improve the\nneuron coverage rate, detect hidden errors, and also improve the accuracy of\nthe target DNN.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 10:32:43 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 02:32:29 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Zhang", "Pengcheng", ""], ["Dai", "Qiyin", ""], ["Pelliccione", "Patrizio", ""]]}, {"id": "1911.07932", "submitter": "Akash Kumar", "authors": "Akash Kumar and Arnav Bhavsar", "title": "Copy-Move Forgery Classification via Unsupervised Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the current era, image manipulation is becoming increasingly easier,\nyielding more natural looking images, owing to the modern tools in image\nprocessing and computer vision techniques. The task of the segregation of\nforged images has become very challenging. To tackle such problems, publicly\navailable datasets are insufficient. In this paper, we propose to create a\nsynthetic forged dataset using deep semantic image inpainting algorithm.\nFurthermore, we use an unsupervised domain adaptation network to detect\ncopy-move forgery in images. Our approach can be helpful in those cases, where\nthe classification of data is unavailable.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 11:49:21 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Kumar", "Akash", ""], ["Bhavsar", "Arnav", ""]]}, {"id": "1911.07933", "submitter": "Pengkai Zhu", "authors": "Pengkai Zhu, Hanxiao Wang, Venkatesh Saligrama", "title": "Dont Even Look Once: Synthesizing Features for Zero-Shot Detection", "comments": "Accepted at CVPR 2020. 10 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot detection, namely, localizing both seen and unseen objects,\nincreasingly gains importance for large-scale applications, with large number\nof object classes, since, collecting sufficient annotated data with ground\ntruth bounding boxes is simply not scalable. While vanilla deep neural networks\ndeliver high performance for objects available during training, unseen object\ndetection degrades significantly. At a fundamental level, while vanilla\ndetectors are capable of proposing bounding boxes, which include unseen\nobjects, they are often incapable of assigning high-confidence to unseen\nobjects, due to the inherent precision/recall tradeoffs that requires rejecting\nbackground objects. We propose a novel detection algorithm Dont Even Look Once\n(DELO), that synthesizes visual features for unseen objects and augments\nexisting training algorithms to incorporate unseen object detection. Our\nproposed scheme is evaluated on Pascal VOC and MSCOCO, and we demonstrate\nsignificant improvements in test accuracy over vanilla and other state-of-art\nzero-shot detectors\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 20:38:04 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 01:41:12 GMT"}, {"version": "v3", "created": "Fri, 10 Apr 2020 15:34:04 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Zhu", "Pengkai", ""], ["Wang", "Hanxiao", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1911.07934", "submitter": "Matthew Ciolino", "authors": "Matthew Ciolino, David Noever, Josh Kalin", "title": "Training Set Effect on Super Resolution for Automated Target Recognition", "comments": "10 pages, 19 figures, 26 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Single Image Super Resolution (SISR) is the process of mapping a\nlow-resolution image to a high resolution image. This inherently has\napplications in remote sensing as a way to increase the spatial resolution in\nsatellite imagery. This suggests a possible improvement to automated target\nrecognition in image classification and object detection. We explore the effect\nthat different training sets have on SISR with the network, Super Resolution\nGenerative Adversarial Network (SRGAN). We train 5 SRGANs on different land-use\nclasses (e.g. agriculture, cities, ports) and test them on the same unseen\ndataset. We attempt to find the qualitative and quantitative differences in\nSISR, binary classification, and object detection performance. We find that\ncurated training sets that contain objects in the test ontology perform better\non both computer vision tasks while having a complex distribution of images\nallows object detection models to perform better. However, Super Resolution\n(SR) might not be beneficial to certain problems and will see a diminishing\namount of returns for datasets that are closer to being solved.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 16:44:48 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 18:58:52 GMT"}, {"version": "v3", "created": "Fri, 7 Feb 2020 20:02:58 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Ciolino", "Matthew", ""], ["Noever", "David", ""], ["Kalin", "Josh", ""]]}, {"id": "1911.07935", "submitter": "Zhengzhong Tu", "authors": "Yun Chen, Yiyue Chen, and Zhengzhong Tu", "title": "Fitness Done Right: a Real-time Intelligent Personal Trainer for\n  Exercise Correction", "comments": "7 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keeping fit has been increasingly important for people nowadays. However,\npeople may not get expected exercise results without following professional\nguidance while hiring personal trainers is expensive. In this paper, an\neffective real-time system called Fitness Done Right (FDR) is proposed for\nhelping people exercise correctly on their own. The system includes detecting\nhuman body parts, recognizing exercise pose and detecting errors for test poses\nas well as giving correction advice. Generally, two branch multi-stage CNN is\nused for training data sets in order to learn human body parts and\nassociations. Then, considering two poses, which are plank and squat in our\nmodel, we design a detection algorithm, combining Euclidean and angle\ndistances, to determine the pose in the image. Finally, key values for key\nfeatures of the two poses are computed correspondingly in the pose error\ndetection part, which helps give correction advice. We conduct our system in\nreal-time situation with error rate down to $1.2\\%$, and the screenshots of\nexperimental results are also presented.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 00:05:00 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Chen", "Yun", ""], ["Chen", "Yiyue", ""], ["Tu", "Zhengzhong", ""]]}, {"id": "1911.07936", "submitter": "Efe Bozkir", "authors": "Efe Bozkir, Ali Burak \\\"Unal, Mete Akg\\\"un, Enkelejda Kasneci, Nico\n  Pfeifer", "title": "Privacy Preserving Gaze Estimation using Synthetic Images via a\n  Randomized Encoding Based Framework", "comments": "In Symposium on Eye Tracking Research and Applications (ETRA '20).\n  Authors' copy of the published paper, refer to the doi for the definitive\n  version", "journal-ref": null, "doi": "10.1145/3379156.3391364", "report-no": null, "categories": "cs.CV cs.CR cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye tracking is handled as one of the key technologies for applications that\nassess and evaluate human attention, behavior, and biometrics, especially using\ngaze, pupillary, and blink behaviors. One of the challenges with regard to the\nsocial acceptance of eye tracking technology is however the preserving of\nsensitive and personal information. To tackle this challenge, we employ a\nprivacy-preserving framework based on randomized encoding to train a Support\nVector Regression model using synthetic eye images privately to estimate the\nhuman gaze. During the computation, none of the parties learn about the data or\nthe result that any other party has. Furthermore, the party that trains the\nmodel cannot reconstruct pupil, blinks or visual scanpath. The experimental\nresults show that our privacy-preserving framework is capable of working in\nreal-time, with the same accuracy as compared to non-private version and could\nbe extended to other eye tracking related problems.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 12:52:09 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 13:57:04 GMT"}, {"version": "v3", "created": "Mon, 6 Apr 2020 21:09:16 GMT"}, {"version": "v4", "created": "Tue, 13 Jul 2021 13:04:07 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Bozkir", "Efe", ""], ["\u00dcnal", "Ali Burak", ""], ["Akg\u00fcn", "Mete", ""], ["Kasneci", "Enkelejda", ""], ["Pfeifer", "Nico", ""]]}, {"id": "1911.07937", "submitter": "Talip Ucar", "authors": "Talip Ucar", "title": "Inverse Graphics: Unsupervised Learning of 3D Shapes from Single Images", "comments": "10 pages, 15 figures. In the second version of the paper, a link to a\n  demo site is added under Figure-12", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using generative models for Inverse Graphics is an active area of research.\nHowever, most works focus on developing models for supervised and\nsemi-supervised methods. In this paper, we study the problem of unsupervised\nlearning of 3D geometry from single images. Our approach is to use a generative\nmodel that produces 2-D images as projections of a latent 3D voxel grid, which\nwe train either as a variational auto-encoder or using adversarial methods. Our\ncontributions are as follows: First, we show how to recover 3D shape and pose\nfrom general datasets such as MNIST, and MNIST Fashion in good quality. Second,\nwe compare the shapes learned using adversarial and variational methods.\nAdversarial approach gives denser 3D shapes. Third, we explore the idea of\nmodelling the pose of an object as uniform distribution to recover 3D shape\nfrom a single image. Our experiment with the CelebA dataset\n\\cite{liu2015faceattributes} proves that we can recover complete 3D shape from\na single image when the object is symmetric along one, or more axis whilst\nresults obtained using ModelNet40 \\cite{wu20153d} show the potential\nside-effects, in which the model learns 3D shapes such that it can render the\nsame image from any viewpoint. Forth, we present a general end-to-end approach\nto learning 3D shapes from single images in a completely unsupervised fashion\nby modelling the factors of variation such as azimuth as independent latent\nvariables. Our method makes no assumptions about the dataset, and can work with\nsynthetic as well as real images (i.e. unsupervised in true sense). We present\nour results, by training the model using the $\\mu$-VAE objective\n\\cite{ucar2019bridging} and a dataset combining all images from MNIST, MNIST\nFashion, CelebA and six categories of ModelNet40. The model is able to learn 3D\nshapes and the pose in qood quality and leverages information learned across\nall datasets.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 09:14:28 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 16:19:18 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Ucar", "Talip", ""]]}, {"id": "1911.07938", "submitter": "Dongdong Yu", "authors": "Dongdong Yu, Kai Su, Changhu Wang", "title": "Towards Good Practices for Multi-Person Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Person Pose Estimation is an interesting yet challenging task in\ncomputer vision. In this paper, we conduct a series of refinements with the\nMSPN and PoseFix Networks, and empirically evaluate their impact on the final\nmodel performance through ablation studies. By taking all the refinements, we\nachieve 78.7 on the COCO test-dev dataset and 76.3 on the COCO test-challenge\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 03:00:28 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Yu", "Dongdong", ""], ["Su", "Kai", ""], ["Wang", "Changhu", ""]]}, {"id": "1911.07939", "submitter": "Dongdong Yu", "authors": "Dongdong Yu, Zehuan Yuan, Jinlai Liu, Kun Yuan, Changhu Wang", "title": "Towards Good Practices for Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance Segmentation is an interesting yet challenging task in computer\nvision. In this paper, we conduct a series of refinements with the Hybrid Task\nCascade (HTC) Network, and empirically evaluate their impact on the final model\nperformance through ablation studies. By taking all the refinements, we achieve\n0.47 on the COCO test-dev dataset and 0.47 on the COCO test-challenge dataset.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 03:03:48 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Yu", "Dongdong", ""], ["Yuan", "Zehuan", ""], ["Liu", "Jinlai", ""], ["Yuan", "Kun", ""], ["Wang", "Changhu", ""]]}, {"id": "1911.07940", "submitter": "Phawis Thammasorn", "authors": "Phawis Thammasorn, Daniel Hippe, Wanpracha Chaovalitwongse, Matthew\n  Spraker, Landon Wootton, Matthew Nyflot, Stephanie Combs, Jan Peeken, Eric\n  Ford", "title": "Neighborhood Watch: Representation Learning with Local-Margin Triplet\n  Loss and Sampling Strategy for K-Nearest-Neighbor Image Classification", "comments": "Triplet Network, Representation Learning, Transfer Learning, Nearest\n  Neighbor, Medical Image Classification", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep representation learning using triplet network for classification suffers\nfrom a lack of theoretical foundation and difficulty in tuning both the network\nand classifiers for performance. To address the problem, local-margin triplet\nloss along with local positive and negative mining strategy is proposed with\ntheory on how the strategy integrate nearest-neighbor hyper-parameter with\ntriplet learning to increase subsequent classification performance. Results in\nexperiments with 2 public datasets, MNIST and Cifar-10, and 2 small medical\nimage datasets demonstrate that proposed strategy outperforms end-to-end\nsoftmax and typical triplet loss in settings without data augmentation while\nmaintaining utility of transferable feature for related tasks. The method\nserves as a good performance baseline where end-to-end methods encounter\ndifficulties such as small sample data with limited allowable data\naugmentation.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 06:35:01 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Thammasorn", "Phawis", ""], ["Hippe", "Daniel", ""], ["Chaovalitwongse", "Wanpracha", ""], ["Spraker", "Matthew", ""], ["Wootton", "Landon", ""], ["Nyflot", "Matthew", ""], ["Combs", "Stephanie", ""], ["Peeken", "Jan", ""], ["Ford", "Eric", ""]]}, {"id": "1911.07954", "submitter": "Patrick Hansen", "authors": "Patrick Hansen, Alexey Vilkin, Yury Khrustalev, James Imber, David\n  Hanwell, Matthew Mattina, Paul N. Whatmough", "title": "ISP4ML: Understanding the Role of Image Signal Processing in Efficient\n  Deep Learning Vision Systems", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Convolutional neural networks (CNNs) are now predominant components in a\nvariety of computer vision (CV) systems. These systems typically include an\nimage signal processor (ISP), even though the ISP is traditionally designed to\nproduce images that look appealing to humans. In CV systems, it is not clear\nwhat the role of the ISP is, or if it is even required at all for accurate\nprediction. In this work, we investigate the efficacy of the ISP in CNN\nclassification tasks, and outline the system-level trade-offs between\nprediction accuracy and computational cost. To do so, we build software models\nof a configurable ISP and an imaging sensor in order to train CNNs on ImageNet\nwith a range of different ISP settings and functionality. Results on ImageNet\nshow that an ISP improves accuracy by 4.6%-12.2% on MobileNet architectures of\ndifferent widths. Results using ResNets demonstrate that these trends also\ngeneralize to deeper networks. An ablation study of the various processing\nstages in a typical ISP reveals that the tone mapper is the most significant\nstage when operating on high dynamic range (HDR) images, by providing 5.8%\naverage accuracy improvement alone. Overall, the ISP benefits system efficiency\nbecause the memory and computational costs of the ISP is minimal compared to\nthe cost of using a larger CNN to achieve the same accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 21:05:44 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 16:14:25 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2020 19:49:15 GMT"}, {"version": "v4", "created": "Wed, 17 Mar 2021 15:15:35 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Hansen", "Patrick", ""], ["Vilkin", "Alexey", ""], ["Khrustalev", "Yury", ""], ["Imber", "James", ""], ["Hanwell", "David", ""], ["Mattina", "Matthew", ""], ["Whatmough", "Paul N.", ""]]}, {"id": "1911.07956", "submitter": "Xiaoixa Wu", "authors": "Xiaoxia Wu and Edgar Dobriban and Tongzheng Ren and Shanshan Wu and\n  Zhiyuan Li and Suriya Gunasekar and Rachel Ward and Qiang Liu", "title": "Implicit Regularization and Convergence for Weight Normalization", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalization methods such as batch [Ioffe and Szegedy, 2015], weight\n[Salimansand Kingma, 2016], instance [Ulyanov et al., 2016], and layer\nnormalization [Baet al., 2016] have been widely used in modern machine\nlearning. Here, we study the weight normalization (WN) method [Salimans and\nKingma, 2016] and a variant called reparametrized projected gradient descent\n(rPGD) for overparametrized least-squares regression. WN and rPGD reparametrize\nthe weights with a scale g and a unit vector w and thus the objective function\nbecomes non-convex. We show that this non-convex formulation has beneficial\nregularization effects compared to gradient descent on the original objective.\nThese methods adaptively regularize the weights and converge close to the\nminimum l2 norm solution, even for initializations far from zero. For certain\nstepsizes of g and w , we show that they can converge close to the minimum norm\nsolution. This is different from the behavior of gradient descent, which\nconverges to the minimum norm solution only when started at a point in the\nrange space of the feature matrix, and is thus more sensitive to\ninitialization.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 21:10:21 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 04:36:05 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 06:05:43 GMT"}, {"version": "v4", "created": "Mon, 7 Dec 2020 19:09:58 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Wu", "Xiaoxia", ""], ["Dobriban", "Edgar", ""], ["Ren", "Tongzheng", ""], ["Wu", "Shanshan", ""], ["Li", "Zhiyuan", ""], ["Gunasekar", "Suriya", ""], ["Ward", "Rachel", ""], ["Liu", "Qiang", ""]]}, {"id": "1911.07959", "submitter": "Heng Fan", "authors": "Heng Fan, Fan Yang, Peng Chu, Lin Yuan, Haibin Ling", "title": "TracKlinic: Diagnosis of Challenge Factors in Visual Tracking", "comments": "Tech. Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generic visual tracking is difficult due to many challenge factors (e.g.,\nocclusion, blur, etc.). Each of these factors may cause serious problems for a\ntracking algorithm, and when they work together can make things even more\ncomplicated. Despite a great amount of efforts devoted to understanding the\nbehavior of tracking algorithms, reliable and quantifiable ways for studying\nthe per factor tracking behavior remain barely available. Addressing this\nissue, in this paper we contribute to the community a tracking diagnosis\ntoolkit, TracKlinic, for diagnosis of challenge factors of tracking algorithms.\n  TracKlinic consists of two novel components focusing on the data and analysis\naspects, respectively. For the data component, we carefully prepare a set of\n2,390 annotated videos, each involving one and only one major challenge factor.\nWhen analyzing an algorithm for a specific challenge factor, such\none-factor-per-sequence rule greatly inhibits the disturbance from other\nfactors and consequently leads to more faithful analysis. For the analysis\ncomponent, given the tracking results on all sequences, it investigates the\nbehavior of the tracker under each individual factor and generates the report\nautomatically. With TracKlinic, a thorough study is conducted on ten\nstate-of-the-art trackers on nine challenge factors (including two compound\nones). The results suggest that, heavy shape variation and occlusion are the\ntwo most challenging factors faced by most trackers. Besides, out-of-view,\nthough does not happen frequently, is often fatal. By sharing TracKlinic, we\nexpect to make it much easier for diagnosing tracking algorithms, and to thus\nfacilitate developing better ones.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 21:15:25 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 20:28:34 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Fan", "Heng", ""], ["Yang", "Fan", ""], ["Chu", "Peng", ""], ["Yuan", "Lin", ""], ["Ling", "Haibin", ""]]}, {"id": "1911.07968", "submitter": "Jindong Gu", "authors": "Jindong Gu and Volker Tresp", "title": "Improving the Robustness of Capsule Networks to Image Affine\n  Transformations", "comments": null, "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) achieve translational invariance by\nusing pooling operations. However, the operations do not preserve the spatial\nrelationships in the learned representations. Hence, CNNs cannot extrapolate to\nvarious geometric transformations of inputs. Recently, Capsule Networks\n(CapsNets) have been proposed to tackle this problem. In CapsNets, each entity\nis represented by a vector and routed to high-level entity representations by a\ndynamic routing algorithm. CapsNets have been shown to be more robust than CNNs\nto affine transformations of inputs. However, there is still a huge gap between\ntheir performance on transformed inputs compared to untransformed versions. In\nthis work, we first revisit the routing procedure by (un)rolling its forward\nand backward passes. Our investigation reveals that the routing procedure\ncontributes neither to the generalization ability nor to the affine robustness\nof the CapsNets. Furthermore, we explore the limitations of capsule\ntransformations and propose affine CapsNets (Aff-CapsNets), which are more\nrobust to affine transformations. On our benchmark task, where models are\ntrained on the MNIST dataset and tested on the AffNIST dataset, our\nAff-CapsNets improve the benchmark performance by a large margin (from 79% to\n93.21%), without using any routing mechanism.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 21:43:17 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 18:50:42 GMT"}, {"version": "v3", "created": "Tue, 31 Mar 2020 08:03:12 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Gu", "Jindong", ""], ["Tresp", "Volker", ""]]}, {"id": "1911.07980", "submitter": "Georgios Georgakis", "authors": "Georgios Georgakis, Yimeng Li, Jana Kosecka", "title": "Simultaneous Mapping and Target Driven Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a modular architecture for simultaneous mapping and target\ndriven navigation in indoors environments. The semantic and appearance stored\nin 2.5D map is distilled from RGB images, semantic segmentation and outputs of\nobject detectors by convolutional neural networks. Given this representation,\nthe mapping module learns to localize the agent and register consecutive\nobservations in the map. The navigation task is then formulated as a problem of\nlearning a policy for reaching semantic targets using current observations and\nthe up-to-date map. We demonstrate that the use of semantic information\nimproves localization accuracy and the ability of storing spatial semantic map\naids the target driven navigation policy. The two modules are evaluated\nseparately and jointly on Active Vision Dataset and Matterport3D environments,\ndemonstrating improved performance on both localization and navigation tasks.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 22:11:03 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Georgakis", "Georgios", ""], ["Li", "Yimeng", ""], ["Kosecka", "Jana", ""]]}, {"id": "1911.07982", "submitter": "Qian Wang", "authors": "Qian Wang, Toby P. Breckon", "title": "Unsupervised Domain Adaptation via Structured Prediction Based Selective\n  Pseudo-Labeling", "comments": "Accepted to AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation aims to address the problem of classifying\nunlabeled samples from the target domain whilst labeled samples are only\navailable from the source domain and the data distributions are different in\nthese two domains. As a result, classifiers trained from labeled samples in the\nsource domain suffer from significant performance drop when directly applied to\nthe samples from the target domain. To address this issue, different approaches\nhave been proposed to learn domain-invariant features or domain-specific\nclassifiers. In either case, the lack of labeled samples in the target domain\ncan be an issue which is usually overcome by pseudo-labeling. Inaccurate\npseudo-labeling, however, could result in catastrophic error accumulation\nduring learning. In this paper, we propose a novel selective pseudo-labeling\nstrategy based on structured prediction. The idea of structured prediction is\ninspired by the fact that samples in the target domain are well clustered\nwithin the deep feature space so that unsupervised clustering analysis can be\nused to facilitate accurate pseudo-labeling. Experimental results on four\ndatasets (i.e. Office-Caltech, Office31, ImageCLEF-DA and Office-Home) validate\nour approach outperforms contemporary state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 22:21:47 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Wang", "Qian", ""], ["Breckon", "Toby P.", ""]]}, {"id": "1911.07984", "submitter": "Shivam Kalra", "authors": "Shivam Kalra, Mohammed Adnan, Graham Taylor, Hamid Tizhoosh", "title": "Learning Permutation Invariant Representations using Memory Networks", "comments": "Accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world tasks such as classification of digital histopathology images\nand 3D object detection involve learning from a set of instances. In these\ncases, only a group of instances or a set, collectively, contains meaningful\ninformation and therefore only the sets have labels, and not individual data\ninstances. In this work, we present a permutation invariant neural network\ncalled Memory-based Exchangeable Model (MEM) for learning set functions. The\nMEM model consists of memory units that embed an input sequence to high-level\nfeatures enabling the model to learn inter-dependencies among instances through\na self-attention mechanism. We evaluated the learning ability of MEM on various\ntoy datasets, point cloud classification, and classification of lung whole\nslide images (WSIs) into two subtypes of lung cancer---Lung Adenocarcinoma, and\nLung Squamous Cell Carcinoma. We systematically extracted patches from lung\nWSIs downloaded from The Cancer Genome Atlas~(TCGA) dataset, the largest public\nrepository of WSIs, achieving a competitive accuracy of 84.84\\% for\nclassification of two sub-types of lung cancer. The results on other datasets\nare promising as well, and demonstrate the efficacy of our model.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 22:28:30 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 16:27:23 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Kalra", "Shivam", ""], ["Adnan", "Mohammed", ""], ["Taylor", "Graham", ""], ["Tizhoosh", "Hamid", ""]]}, {"id": "1911.07989", "submitter": "Micah Goldblum", "authors": "Ping-Yeh Chiang, Jonas Geiping, Micah Goldblum, Tom Goldstein, Renkun\n  Ni, Steven Reich, Ali Shafahi", "title": "WITCHcraft: Efficient PGD attacks with random step size", "comments": "Authors contributed equally and are listed in alphabetical order", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  State-of-the-art adversarial attacks on neural networks use expensive\niterative methods and numerous random restarts from different initial points.\nIterative FGSM-based methods without restarts trade off performance for\ncomputational efficiency because they do not adequately explore the image space\nand are highly sensitive to the choice of step size. We propose a variant of\nProjected Gradient Descent (PGD) that uses a random step size to improve\nperformance without resorting to expensive random restarts. Our method, Wide\nIterative Stochastic crafting (WITCHcraft), achieves results superior to the\nclassical PGD attack on the CIFAR-10 and MNIST data sets but without additional\ncomputational cost. This simple modification of PGD makes crafting attacks more\neconomical, which is important in situations like adversarial training where\nattacks need to be crafted in real time.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 22:40:08 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Chiang", "Ping-Yeh", ""], ["Geiping", "Jonas", ""], ["Goldblum", "Micah", ""], ["Goldstein", "Tom", ""], ["Ni", "Renkun", ""], ["Reich", "Steven", ""], ["Shafahi", "Ali", ""]]}, {"id": "1911.07990", "submitter": "Qian Wang", "authors": "Qian Wang, Toby P. Breckon", "title": "Crowd Counting via Segmentation Guided Attention Networks and Curriculum\n  Loss", "comments": "Technical Report, Durham University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic crowd behaviour analysis is an important task for intelligent\ntransportation systems to enable effective flow control and dynamic route\nplanning for varying road participants. Crowd counting is one of the keys to\nautomatic crowd behaviour analysis. Crowd counting using deep convolutional\nneural networks (CNN) has achieved encouraging progress in recent years.\nResearchers have devoted much effort to the design of variant CNN architectures\nand most of them are based on the pre-trained VGG16 model. Due to the\ninsufficient expressive capacity, the backbone network of VGG16 is usually\nfollowed by another cumbersome network specially designed for good counting\nperformance. Although VGG models have been outperformed by Inception models in\nimage classification tasks, the existing crowd counting networks built with\nInception modules still only have a small number of layers with basic types of\nInception modules. To fill in this gap, in this paper, we firstly benchmark the\nbaseline Inception-v3 model on commonly used crowd counting datasets and\nachieve surprisingly good performance comparable with or better than most\nexisting crowd counting models. Subsequently, we push the boundary of this\ndisruptive work further by proposing a Segmentation Guided Attention Network\n(SGANet) with Inception-v3 as the backbone and a novel curriculum loss for\ncrowd counting. We conduct thorough experiments to compare the performance of\nour SGANet with prior arts and the proposed model can achieve state-of-the-art\nperformance with MAE of 57.6, 6.3 and 87.6 on ShanghaiTechA, ShanghaiTechB and\nUCF\\_QNRF, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 22:40:13 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 21:06:44 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Wang", "Qian", ""], ["Breckon", "Toby P.", ""]]}, {"id": "1911.07995", "submitter": "Sascha Xu", "authors": "Sascha Xu and Jan Bauer and Benjamin Axmann", "title": "CD2 : Combined Distances of Contrast Distributions for the Assessment of\n  Perceptual Quality of Image Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of visual input is very important for both human and machine\nperception. Consequently many processing techniques exist that deal with\ndifferent distortions. Usually image processing is applied freely and lacks\nredundancy regarding safety. We propose a novel image comparison method called\nthe Combined Distances of Contrast Distributions (CD2) to protect against\nerrors that arise during processing. Based on the distribution of image\ncontrasts a new reduced-reference image quality assessment (IQA) method is\nintroduced. By combining various distance functions excellent performance on\nIQA benchmarks is achieved with only a small data and computation overhead.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 22:58:09 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 14:06:17 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Xu", "Sascha", ""], ["Bauer", "Jan", ""], ["Axmann", "Benjamin", ""]]}, {"id": "1911.08007", "submitter": "Fahad Alhasoun", "authors": "Fahad Alhasoun, Marta Gonzalez", "title": "Streetify: Using Street View Imagery And Deep Learning For Urban Streets\n  Development", "comments": "Paper to appear at IEEE Big Data 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classification of streets on road networks has been focused on the\nvehicular transportational features of streets such as arterials, major roads,\nminor roads and so forth based on their transportational use. City authorities\non the other hand have been shifting to more urban inclusive planning of\nstreets, encompassing the side use of a street combined with the\ntransportational features of a street. In such classification schemes, streets\nare labeled for example as commercial throughway, residential neighborhood,\npark etc. This modern approach to urban planning has been adopted by major\ncities such as the city of San Francisco, the states of Florida and\nPennsylvania among many others. Currently, the process of labeling streets\naccording to their contexts is manual and hence is tedious and time consuming.\nIn this paper, we propose an approach to collect and label imagery data then\ndeploy advancements in computer vision towards modern urban planning. We\ncollect and label street imagery then train deep convolutional neural networks\n(CNN) to perform the classification of street context. We show that CNN models\ncan perform well achieving accuracies in the 81% to 87%, we then visualize\nsamples from the embedding space of streets using the t-SNE method and apply\nclass activation mapping methods to interpret the features in street imagery\ncontributing to output classification from a model.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 23:45:29 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Alhasoun", "Fahad", ""], ["Gonzalez", "Marta", ""]]}, {"id": "1911.08008", "submitter": "Stylianos Ploumpis", "authors": "Stylianos Ploumpis, Evangelos Ververas, Eimear O' Sullivan, Stylianos\n  Moschoglou, Haoyang Wang, Nick Pears, William A. P. Smith, Baris Gecer,\n  Stefanos Zafeiriou", "title": "Towards a complete 3D morphable model of the human head", "comments": "18 pages, 18 figures, submitted to Transactions on Pattern Analysis\n  and Machine Intelligence (TPAMI) on the 9th of October as an extension paper\n  of the original oral CVPR paper : arXiv:1903.03785", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Three-dimensional Morphable Models (3DMMs) are powerful statistical tools for\nrepresenting the 3D shapes and textures of an object class. Here we present the\nmost complete 3DMM of the human head to date that includes face, cranium, ears,\neyes, teeth and tongue. To achieve this, we propose two methods for combining\nexisting 3DMMs of different overlapping head parts: i. use a regressor to\ncomplete missing parts of one model using the other, ii. use the Gaussian\nProcess framework to blend covariance matrices from multiple models. Thus we\nbuild a new combined face-and-head shape model that blends the variability and\nfacial detail of an existing face model (the LSFM) with the full head modelling\ncapability of an existing head model (the LYHM). Then we construct and fuse a\nhighly-detailed ear model to extend the variation of the ear shape. Eye and eye\nregion models are incorporated into the head model, along with basic models of\nthe teeth, tongue and inner mouth cavity. The new model achieves\nstate-of-the-art performance. We use our model to reconstruct full head\nrepresentations from single, unconstrained images allowing us to parameterize\ncraniofacial shape and texture, along with the ear shape, eye gaze and eye\ncolor.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 23:58:34 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 23:23:21 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Ploumpis", "Stylianos", ""], ["Ververas", "Evangelos", ""], ["Sullivan", "Eimear O'", ""], ["Moschoglou", "Stylianos", ""], ["Wang", "Haoyang", ""], ["Pears", "Nick", ""], ["Smith", "William A. P.", ""], ["Gecer", "Baris", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1911.08010", "submitter": "Daouda Diouf Dr", "authors": "Daouda Diouf, Djibril Seck, Mountaga Diop and Abdoulye Ba", "title": "Convolutional Neural Network and decision support in medical imaging:\n  case study of the recognition of blood cell subtypes", "comments": "7 pages, 6 figures, 1 table", "journal-ref": "CEUR-WS.org/Vol-2647 (2019), pp. 128-140", "doi": null, "report-no": "ISSN 1613-0073", "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying and characterizing the patient's blood samples is indispensable\nin diagnostics of malignance suspicious. A painstaking and sometimes subjective\ntask is used in laboratories to manually classify white blood cells. Neural\nmathematical methods as deep learnings can be very useful in the automated\nrecognition of blood cells. This study uses a particular type of deep learning\ni.e., convolutional neural networks (CNNs or ConvNets) for image recognition of\nthe four (4) blood cell types (neutrophil, eosinophil, lymphocyte and monocyte)\nand to enable it to tag them employing a dataset of blood cells with labels for\nthe corresponding cell types. The elements of the database are the input of our\nCNN and they allowed us to create learning models for the image\nrecognition/classification of the blood cells. We evaluated the recognition\nperformance and outputs learned by the networks in order to implement a neural\nimage recognition model capable of distinguishing polynuclear cells (neutrophil\nand eosinophil) from those of mononuclear cells (lymphocyte and monocyte). The\nvalidation accuracy is 97.77%.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 00:10:41 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 01:11:47 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Diouf", "Daouda", ""], ["Seck", "Djibril", ""], ["Diop", "Mountaga", ""], ["Ba", "Abdoulye", ""]]}, {"id": "1911.08019", "submitter": "Eugene Belilovsky", "authors": "Lucas Caccia, Eugene Belilovsky, Massimo Caccia, Joelle Pineau", "title": "Online Learned Continual Compression with Adaptive Quantization Modules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study the problem of Online Continual Compression, where one\nattempts to simultaneously learn to compress and store a representative dataset\nfrom a non i.i.d data stream, while only observing each sample once. A naive\napplication of auto-encoders in this setting encounters a major challenge:\nrepresentations derived from earlier encoder states must be usable by later\ndecoder states. We show how to use discrete auto-encoders to effectively\naddress this challenge and introduce Adaptive Quantization Modules (AQM) to\ncontrol variation in the compression ability of the module at any given stage\nof learning. This enables selecting an appropriate compression for incoming\nsamples, while taking into account overall memory constraints and current\nprogress of the learned compression. Unlike previous methods, our approach does\nnot require any pretraining, even on challenging datasets. We show that using\nAQM to replace standard episodic memory in continual learning settings leads to\nsignificant gains on continual learning benchmarks. Furthermore we demonstrate\nthis approach with larger images, LiDAR, and reinforcement learning\nenvironments.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 00:43:16 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 00:23:18 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 19:19:56 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Caccia", "Lucas", ""], ["Belilovsky", "Eugene", ""], ["Caccia", "Massimo", ""], ["Pineau", "Joelle", ""]]}, {"id": "1911.08028", "submitter": "Hanjiang Lai", "authors": "Haien Zeng, Hanjiang Lai, Jian Yin", "title": "Simultaneous Region Localization and Hash Coding for Fine-grained Image\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained image hashing is a challenging problem due to the difficulties\nof discriminative region localization and hash code generation. Most existing\ndeep hashing approaches solve the two tasks independently. While these two\ntasks are correlated and can reinforce each other. In this paper, we propose a\ndeep fine-grained hashing to simultaneously localize the discriminative regions\nand generate the efficient binary codes. The proposed approach consists of a\nregion localization module and a hash coding module. The region localization\nmodule aims to provide informative regions to the hash coding module. The hash\ncoding module aims to generate effective binary codes and give feedback for\nlearning better localizer. Moreover, to better capture subtle differences,\nmulti-scale regions at different layers are learned without the need of\nbounding-box/part annotations. Extensive experiments are conducted on two\npublic benchmark fine-grained datasets. The results demonstrate significant\nimprovements in the performance of our method relative to other fine-grained\nhashing algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 01:12:41 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Zeng", "Haien", ""], ["Lai", "Hanjiang", ""], ["Yin", "Jian", ""]]}, {"id": "1911.08039", "submitter": "Bingfeng Zhang", "authors": "Bingfeng Zhang, Jimin Xiao, Yunchao Wei, Mingjie Sun, Kaizhu Huang", "title": "Reliability Does Matter: An End-to-End Weakly Supervised Semantic\n  Segmentation Approach", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised semantic segmentation is a challenging task as it only\ntakes image-level information as supervision for training but produces\npixel-level predictions for testing. To address such a challenging task, most\nrecent state-of-the-art approaches propose to adopt two-step solutions,\n\\emph{i.e. } 1) learn to generate pseudo pixel-level masks, and 2) engage FCNs\nto train the semantic segmentation networks with the pseudo masks. However, the\ntwo-step solutions usually employ many bells and whistles in producing\nhigh-quality pseudo masks, making this kind of methods complicated and\ninelegant. In this work, we harness the image-level labels to produce reliable\npixel-level annotations and design a fully end-to-end network to learn to\npredict segmentation maps. Concretely, we firstly leverage an image\nclassification branch to generate class activation maps for the annotated\ncategories, which are further pruned into confident yet tiny object/background\nregions. Such reliable regions are then directly served as ground-truth labels\nfor the parallel segmentation branch, where a newly designed dense energy loss\nfunction is adopted for optimization. Despite its apparent simplicity, our\none-step solution achieves competitive mIoU scores (\\emph{val}: 62.6,\n\\emph{test}: 62.9) on Pascal VOC compared with those two-step\nstate-of-the-arts. By extending our one-step method to two-step, we get a new\nstate-of-the-art performance on the Pascal VOC (\\emph{val}: 66.3, \\emph{test}:\n66.5).\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 01:58:16 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Zhang", "Bingfeng", ""], ["Xiao", "Jimin", ""], ["Wei", "Yunchao", ""], ["Sun", "Mingjie", ""], ["Huang", "Kaizhu", ""]]}, {"id": "1911.08040", "submitter": "Alvin Chan", "authors": "Alvin Chan and Yew-Soon Ong", "title": "Poison as a Cure: Detecting & Neutralizing Variable-Sized Backdoor\n  Attacks in Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning models have recently shown to be vulnerable to backdoor\npoisoning, an insidious attack where the victim model predicts clean images\ncorrectly but classifies the same images as the target class when a trigger\npoison pattern is added. This poison pattern can be embedded in the training\ndataset by the adversary. Existing defenses are effective under certain\nconditions such as a small size of the poison pattern, knowledge about the\nratio of poisoned training samples or when a validated clean dataset is\navailable. Since a defender may not have such prior knowledge or resources, we\npropose a defense against backdoor poisoning that is effective even when those\nprerequisites are not met. It is made up of several parts: one to extract a\nbackdoor poison signal, detect poison target and base classes, and filter out\npoisoned from clean samples with proven guarantees. The final part of our\ndefense involves retraining the poisoned model on a dataset augmented with the\nextracted poison signal and corrective relabeling of poisoned samples to\nneutralize the backdoor. Our approach has shown to be effective in defending\nagainst backdoor attacks that use both small and large-sized poison patterns on\nnine different target-base class pairs from the CIFAR10 dataset.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 01:59:59 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Chan", "Alvin", ""], ["Ong", "Yew-Soon", ""]]}, {"id": "1911.08053", "submitter": "Xu XiangRui", "authors": "XiangRui Xu, YaQin Li, Cao Yuan", "title": "A novel method for identifying the deep neural network model with the\n  Serial Number", "comments": "9pages,9 figures,conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network (DNN) with the state of art performance has emerged as a\nviable and lucrative business service. However, those impressive performances\nrequire a large number of computational resources, which comes at a high cost\nfor the model creators. The necessity for protecting DNN models from illegal\nreproducing and distribution appears salient now. Recently, trigger-set\nwatermarking, breaking the white-box restriction, relying on adversarial\ntraining pre-defined (incorrect) labels for crafted inputs, and subsequently\nusing them to verify the model authenticity, has been the main topic of DNN\nownership verification. While these methods have successfully demonstrated\nrobustness against removal attacks, few are effective against the tampering\nattacks from competitors forging the fake watermarks and dogging in the\nmanager. In this paper, we put forth a new framework of the trigger-set\nwatermark by embedding a unique Serial Number (relatedness less original\nlabels) to the deep neural network for model ownership identification, which is\nboth robust to model pruning and resist to tampering attacks. Experiment\nresults demonstrate that the DNN Serial Number only incurs slight accuracy\ndegradation of the original performance and is valid for ownership\nverification.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 02:43:11 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Xu", "XiangRui", ""], ["Li", "YaQin", ""], ["Yuan", "Cao", ""]]}, {"id": "1911.08076", "submitter": "Dongchao Wen", "authors": "Hongxing Gao, Wei Tao, Dongchao Wen, Tse-Wei Chen, Kinya Osa, Masami\n  Kato", "title": "IFQ-Net: Integrated Fixed-point Quantization Networks for Embedded\n  Vision", "comments": "9 pages, 6 figures", "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR 2018) Workshops", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deploying deep models on embedded devices has been a challenging problem\nsince the great success of deep learning based networks. Fixed-point networks,\nwhich represent their data with low bits fixed-point and thus give remarkable\nsavings on memory usage, are generally preferred. Even though current\nfixed-point networks employ relative low bits (e.g. 8-bits), the memory saving\nis far from enough for the embedded devices. On the other hand, quantization\ndeep networks, for example XNOR-Net and HWGQNet, quantize the data into 1 or 2\nbits resulting in more significant memory savings but still contain lots of\nfloatingpoint data. In this paper, we propose a fixed-point network for\nembedded vision tasks through converting the floatingpoint data in a\nquantization network into fixed-point. Furthermore, to overcome the data loss\ncaused by the conversion, we propose to compose floating-point data operations\nacross multiple layers (e.g. convolution, batch normalization and quantization\nlayers) and convert them into fixedpoint. We name the fixed-point network\nobtained through such integrated conversion as Integrated Fixed-point\nQuantization Networks (IFQ-Net). We demonstrate that our IFQNet gives 2.16x and\n18x more savings on model size and runtime feature map memory respectively with\nsimilar accuracy on ImageNet. Furthermore, based on YOLOv2, we design\nIFQ-Tinier-YOLO face detector which is a fixed-point network with 256x\nreduction in model size (246k Bytes) than Tiny-YOLO. We illustrate the\npromising performance of our face detector in terms of detection rate on Face\nDetection Data Set and Bencmark (FDDB) and qualitative results of detecting\nsmall faces of Wider Face dataset.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 03:29:03 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Gao", "Hongxing", ""], ["Tao", "Wei", ""], ["Wen", "Dongchao", ""], ["Chen", "Tse-Wei", ""], ["Osa", "Kinya", ""], ["Kato", "Masami", ""]]}, {"id": "1911.08079", "submitter": "MinhDuc Vo", "authors": "Duc Minh Vo, Akihiro Sugimoto", "title": "Two-Stream FCNs to Balance Content and Style for Style Transfer", "comments": "published in Machine Vision and Applications", "journal-ref": null, "doi": "10.1007/s00138-020-01086-1", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Style transfer is to render given image contents in given styles, and it has\nan important role in both computer vision fundamental research and industrial\napplications. Following the success of deep learning based approaches, this\nproblem has been re-launched recently, but still remains a difficult task\nbecause of trade-off between preserving contents and faithful rendering of\nstyles. Indeed, how well-balanced content and style are is crucial in\nevaluating the quality of stylized images. In this paper, we propose an\nend-to-end two-stream Fully Convolutional Networks (FCNs) aiming at balancing\nthe contributions of the content and the style in rendered images. Our proposed\nnetwork consists of the encoder and decoder parts. The encoder part utilizes a\nFCN for content and a FCN for style where the two FCNs have feature injections\nand are independently trained to preserve the semantic content and to learn the\nfaithful style representation in each. The semantic content feature and the\nstyle representation feature are then concatenated adaptively and fed into the\ndecoder to generate style-transferred (stylized) images. In order to train our\nproposed network, we employ a loss network, the pre-trained VGG-16, to compute\ncontent loss and style loss, both of which are efficiently used for the feature\ninjection as well as the feature concatenation. Our intensive experiments show\nthat our proposed model generates more balanced stylized images in content and\nstyle than state-of-the-art methods. Moreover, our proposed network achieves\nefficiency in speed.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 03:41:18 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 14:55:48 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Vo", "Duc Minh", ""], ["Sugimoto", "Akihiro", ""]]}, {"id": "1911.08080", "submitter": "Sixue Gong Miss", "authors": "Sixue Gong, Xiaoming Liu, and Anil K. Jain", "title": "Jointly De-biasing Face Recognition and Demographic Attribute Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of bias in automated face recognition and demographic\nattribute estimation algorithms, where errors are lower on certain cohorts\nbelonging to specific demographic groups. We present a novel de-biasing\nadversarial network (DebFace) that learns to extract disentangled feature\nrepresentations for both unbiased face recognition and demographics estimation.\nThe proposed network consists of one identity classifier and three demographic\nclassifiers (for gender, age, and race) that are trained to distinguish\nidentity and demographic attributes, respectively. Adversarial learning is\nadopted to minimize correlation among feature factors so as to abate bias\ninfluence from other factors. We also design a new scheme to combine\ndemographics with identity features to strengthen robustness of face\nrepresentation in different demographic groups. The experimental results show\nthat our approach is able to reduce bias in face recognition as well as\ndemographics estimation while achieving state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 03:44:34 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 22:32:56 GMT"}, {"version": "v3", "created": "Sun, 19 Apr 2020 21:06:47 GMT"}, {"version": "v4", "created": "Fri, 31 Jul 2020 07:41:15 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Gong", "Sixue", ""], ["Liu", "Xiaoming", ""], ["Jain", "Anil K.", ""]]}, {"id": "1911.08090", "submitter": "Sarfaraz Hussein", "authors": "Javier Echauz, Keith Kenemer, Sarfaraz Hussein, Jay Dhaliwal, Saurabh\n  Shintre, Slawomir Grzonkowski and Andrew Gardner", "title": "Deep Detector Health Management under Adversarial Campaigns", "comments": "International Journal of Prognostics and Health Management, Special\n  Issue: PHM Applications of Deep Learning and Emerging Analytics, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are vulnerable to adversarial inputs that induce\nseemingly unjustifiable errors. As automated classifiers are increasingly used\nin industrial control systems and machinery, these adversarial errors could\ngrow to be a serious problem. Despite numerous studies over the past few years,\nthe field of adversarial ML is still considered alchemy, with no practical\nunbroken defenses demonstrated to date, leaving PHM practitioners with few\nmeaningful ways of addressing the problem. We introduce turbidity detection as\na practical superset of the adversarial input detection problem, coping with\nadversarial campaigns rather than statistically invisible one-offs. This\nperspective is coupled with ROC-theoretic design guidance that prescribes an\ninexpensive domain adaptation layer at the output of a deep learning model\nduring an attack campaign. The result aims to approximate the Bayes optimal\nmitigation that ameliorates the detection model's degraded health. A\nproactively reactive type of prognostics is achieved via Monte Carlo simulation\nof various adversarial campaign scenarios, by sampling from the model's own\nturbidity distribution to quickly deploy the correct mitigation during a\nreal-world campaign.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 04:33:05 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Echauz", "Javier", ""], ["Kenemer", "Keith", ""], ["Hussein", "Sarfaraz", ""], ["Dhaliwal", "Jay", ""], ["Shintre", "Saurabh", ""], ["Grzonkowski", "Slawomir", ""], ["Gardner", "Andrew", ""]]}, {"id": "1911.08097", "submitter": "Julian Faraone", "authors": "Julian Faraone, Martin Kumm, Martin Hardieck, Peter Zipf, Xueyuan Liu,\n  David Boland, Philip H.W. Leong", "title": "AddNet: Deep Neural Networks Using FPGA-Optimized Multipliers", "comments": "14 pages", "journal-ref": null, "doi": "10.1109/TVLSI.2019.2939429", "report-no": null, "categories": "eess.SP cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-precision arithmetic operations to accelerate deep-learning applications\non field-programmable gate arrays (FPGAs) have been studied extensively,\nbecause they offer the potential to save silicon area or increase throughput.\nHowever, these benefits come at the cost of a decrease in accuracy. In this\narticle, we demonstrate that reconfigurable constant coefficient multipliers\n(RCCMs) offer a better alternative for saving the silicon area than utilizing\nlow-precision arithmetic. RCCMs multiply input values by a restricted choice of\ncoefficients using only adders, subtractors, bit shifts, and multiplexers\n(MUXes), meaning that they can be heavily optimized for FPGAs. We propose a\nfamily of RCCMs tailored to FPGA logic elements to ensure their efficient\nutilization. To minimize information loss from quantization, we then develop\nnovel training techniques that map the possible coefficient representations of\nthe RCCMs to neural network weight parameter distributions. This enables the\nusage of the RCCMs in hardware, while maintaining high accuracy. We demonstrate\nthe benefits of these techniques using AlexNet, ResNet-18, and ResNet-50\nnetworks. The resulting implementations achieve up to 50% resource savings over\ntraditional 8-bit quantized networks, translating to significant speedups and\npower savings. Our RCCM with the lowest resource requirements exceeds 6-bit\nfixed point accuracy, while all other implementations with RCCMs achieve at\nleast similar accuracy to an 8-bit uniformly quantized design, while achieving\nsignificant resource savings.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 05:00:41 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Faraone", "Julian", ""], ["Kumm", "Martin", ""], ["Hardieck", "Martin", ""], ["Zipf", "Peter", ""], ["Liu", "Xueyuan", ""], ["Boland", "David", ""], ["Leong", "Philip H. W.", ""]]}, {"id": "1911.08098", "submitter": "Kangfu Mei", "authors": "Kangfu Mei and Juncheng Li and Jiajie Zhang and Haoyu Wu and Jie Li\n  and Rui Huang", "title": "HighEr-Resolution Network for Image Demosaicing and Enhancing", "comments": "Accepted in ICCV 2019 Workshop (AIM2019 Raw to RGB Challenge Winner)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural-networks based image restoration methods tend to use low-resolution\nimage patches for training. Although higher-resolution image patches can\nprovide more global information, state-of-the-art methods cannot utilize them\ndue to their huge GPU memory usage, as well as the instable training process.\nHowever, plenty of studies have shown that global information is crucial for\nimage restoration tasks like image demosaicing and enhancing. In this work, we\npropose a HighEr-Resolution Network (HERN) to fully learning global information\nin high-resolution image patches. To achieve this, the HERN employs two\nparallel paths to learn image features in two different resolutions,\nrespectively. By combining global-aware features and multi-scale features, our\nHERN is able to learn global information with feasible GPU memory usage.\nBesides, we introduce a progressive training method to solve the instability\nissue and accelerate model convergence. On the task of image demosaicing and\nenhancing, our HERN achieves state-of-the-art performance on the AIM2019 RAW to\nRGB mapping challenge. The source code of our implementation is available at\nhttps://github.com/MKFMIKU/RAW2RGBNet.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 05:02:35 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Mei", "Kangfu", ""], ["Li", "Juncheng", ""], ["Zhang", "Jiajie", ""], ["Wu", "Haoyu", ""], ["Li", "Jie", ""], ["Huang", "Rui", ""]]}, {"id": "1911.08105", "submitter": "Megumi Nakao", "authors": "Megumi Nakao, Keiho Imanishi, Nobuhiro Ueda, Yuichiro Imai, Tadaaki\n  Kirita, Tetsuya Matsuda", "title": "Three-dimensional Generative Adversarial Nets for Unsupervised Metal\n  Artifact Reduction", "comments": null, "journal-ref": "IEEE Access, 8, 109453-109465 (2020)", "doi": "10.1109/ACCESS.2020.3002090", "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The reduction of metal artifacts in computed tomography (CT) images,\nspecifically for strong artifacts generated from multiple metal objects, is a\nchallenging issue in medical imaging research. Although there have been some\nstudies on supervised metal artifact reduction through the learning of\nsynthesized artifacts, it is difficult for simulated artifacts to cover the\ncomplexity of the real physical phenomena that may be observed in X-ray\npropagation. In this paper, we introduce metal artifact reduction methods based\non an unsupervised volume-to-volume translation learned from clinical CT\nimages. We construct three-dimensional adversarial nets with a regularized loss\nfunction designed for metal artifacts from multiple dental fillings. The\nresults of experiments using 915 CT volumes from real patients demonstrate that\nthe proposed framework has an outstanding capacity to reduce strong artifacts\nand to recover underlying missing voxels, while preserving the anatomical\nfeatures of soft tissues and tooth structures from the original images.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 05:56:54 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 03:40:43 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2020 04:50:09 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Nakao", "Megumi", ""], ["Imanishi", "Keiho", ""], ["Ueda", "Nobuhiro", ""], ["Imai", "Yuichiro", ""], ["Kirita", "Tadaaki", ""], ["Matsuda", "Tetsuya", ""]]}, {"id": "1911.08114", "submitter": "Jian-Hao Luo", "authors": "Jian-Hao Luo, Jianxin Wu", "title": "Neural Network Pruning with Residual-Connections and Limited-Data", "comments": "CVPR 2020 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filter level pruning is an effective method to accelerate the inference speed\nof deep CNN models. Although numerous pruning algorithms have been proposed,\nthere are still two open issues. The first problem is how to prune residual\nconnections. We propose to prune both channels inside and outside the residual\nconnections via a KL-divergence based criterion. The second issue is pruning\nwith limited data. We observe an interesting phenomenon: directly pruning on a\nsmall dataset is usually worse than fine-tuning a small model which is pruned\nor trained from scratch on the large dataset. Knowledge distillation is an\neffective approach to compensate for the weakness of limited data. However, the\nlogits of a teacher model may be noisy. In order to avoid the influence of\nlabel noise, we propose a label refinement approach to solve this problem.\nExperiments have demonstrated the effectiveness of our method (CURL,\nCompression Using Residual-connections and Limited-data). CURL significantly\noutperforms previous state-of-the-art methods on ImageNet. More importantly,\nwhen pruning on small datasets, CURL achieves comparable or much better\nperformance than fine-tuning a pretrained small model.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 06:43:34 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 11:20:10 GMT"}, {"version": "v3", "created": "Sat, 25 Apr 2020 08:02:47 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Luo", "Jian-Hao", ""], ["Wu", "Jianxin", ""]]}, {"id": "1911.08119", "submitter": "Qiang Ren", "authors": "Qiang Ren, Shaohua Shang, Lianghua He", "title": "Adaptive Routing Between Capsules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capsule network is the most recent exciting advancement in the deep learning\nfield and represents positional information by stacking features into vectors.\nThe dynamic routing algorithm is used in the capsule network, however, there\nare some disadvantages such as the inability to stack multiple layers and a\nlarge amount of computation. In this paper, we propose an adaptive routing\nalgorithm that can solve the problems mentioned above. First, the low-layer\ncapsules adaptively adjust their direction and length in the routing algorithm\nand removing the influence of the coupling coefficient on the gradient\npropagation, so that the network can work when stacked in multiple layers.\nThen, the iterative process of routing is simplified to reduce the amount of\ncomputation and we introduce the gradient coefficient $\\lambda$. Further, we\ntested the performance of our proposed adaptive routing algorithm on CIFAR10,\nFashion-MNIST, SVHN and MNIST, while achieving better results than the dynamic\nrouting algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 06:56:36 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Ren", "Qiang", ""], ["Shang", "Shaohua", ""], ["He", "Lianghua", ""]]}, {"id": "1911.08121", "submitter": "Nina Miolane", "authors": "Nina Miolane, Fr\\'ed\\'eric Poitevin, Yee-Ting Li, Susan Holmes", "title": "Estimation of Orientation and Camera Parameters from Cryo-Electron\n  Microscopy Images with Variational Autoencoders and Generative Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryo-electron microscopy (cryo-EM) is capable of producing reconstructed 3D\nimages of biomolecules at near-atomic resolution. As such, it represents one of\nthe most promising imaging techniques in structural biology. However, raw\ncryo-EM images are only highly corrupted - noisy and band-pass filtered - 2D\nprojections of the target 3D biomolecules. Reconstructing the 3D molecular\nshape starts with the removal of image outliers, the estimation of the\norientation of the biomolecule that has produced the given 2D image, and the\nestimation of camera parameters to correct for intensity defects. Current\ntechniques performing these tasks are often computationally expensive, while\nthe dataset sizes keep growing. There is a need for next-generation algorithms\nthat preserve accuracy while improving speed and scalability. In this paper, we\ncombine variational autoencoders (VAEs) and generative adversarial networks\n(GANs) to learn a low-dimensional latent representation of cryo-EM images. We\nperform an exploratory analysis of the obtained latent space, that is shown to\nhave a structure of \"orbits\", in the sense of Lie group theory, consistent with\nthe acquisition procedure of cryo-EM images. This analysis leads us to design\nan estimation method for orientation and camera parameters of single-particle\ncryo-EM images, together with an outliers detection procedure. As such, it\nopens the door to geometric approaches for unsupervised estimations of\norientations and camera parameters, making possible fast cryo-EM biomolecule\nreconstruction.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 07:04:43 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 19:11:06 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Miolane", "Nina", ""], ["Poitevin", "Fr\u00e9d\u00e9ric", ""], ["Li", "Yee-Ting", ""], ["Holmes", "Susan", ""]]}, {"id": "1911.08139", "submitter": "Sungjoo Ha", "authors": "Sungjoo Ha, Martin Kersner, Beomsu Kim, Seokjun Seo, Dongyoung Kim", "title": "MarioNETte: Few-shot Face Reenactment Preserving Identity of Unseen\n  Targets", "comments": "In AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When there is a mismatch between the target identity and the driver identity,\nface reenactment suffers severe degradation in the quality of the result,\nespecially in a few-shot setting. The identity preservation problem, where the\nmodel loses the detailed information of the target leading to a defective\noutput, is the most common failure mode. The problem has several potential\nsources such as the identity of the driver leaking due to the identity\nmismatch, or dealing with unseen large poses. To overcome such problems, we\nintroduce components that address the mentioned problem: image attention block,\ntarget feature alignment, and landmark transformer. Through attending and\nwarping the relevant features, the proposed architecture, called MarioNETte,\nproduces high-quality reenactments of unseen identities in a few-shot setting.\nIn addition, the landmark transformer dramatically alleviates the identity\npreservation problem by isolating the expression geometry through landmark\ndisentanglement. Comprehensive experiments are performed to verify that the\nproposed framework can generate highly realistic faces, outperforming all other\nbaselines, even under a significant mismatch of facial characteristics between\nthe target and the driver.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 08:02:59 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Ha", "Sungjoo", ""], ["Kersner", "Martin", ""], ["Kim", "Beomsu", ""], ["Seo", "Seokjun", ""], ["Kim", "Dongyoung", ""]]}, {"id": "1911.08141", "submitter": "Daesik Kim", "authors": "Daesik Kim, Gyujeong Lee, Jisoo Jeong, Nojun Kwak", "title": "Tell Me What They're Holding: Weakly-supervised Object Detection with\n  Transferable Knowledge from Human-object Interaction", "comments": "AAAI 2020 Oral Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a novel weakly supervised object detection (WSOD)\nparadigm to detect objects belonging to rare classes that have not many\nexamples using transferable knowledge from human-object interactions (HOI).\nWhile WSOD shows lower performance than full supervision, we mainly focus on\nHOI as the main context which can strongly supervise complex semantics in\nimages. Therefore, we propose a novel module called RRPN (relational region\nproposal network) which outputs an object-localizing attention map only with\nhuman poses and action verbs. In the source domain, we fully train an object\ndetector and the RRPN with full supervision of HOI. With transferred knowledge\nabout localization map from the trained RRPN, a new object detector can learn\nunseen objects with weak verbal supervision of HOI without bounding box\nannotations in the target domain. Because the RRPN is designed as an add-on\ntype, we can apply it not only to the object detection but also to other\ndomains such as semantic segmentation. The experimental results on HICO-DET\ndataset show the possibility that the proposed method can be a cheap\nalternative for the current supervised object detection paradigm. Moreover,\nqualitative results demonstrate that our model can properly localize unseen\nobjects on HICO-DET and V-COCO datasets.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 08:03:11 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Kim", "Daesik", ""], ["Lee", "Gyujeong", ""], ["Jeong", "Jisoo", ""], ["Kwak", "Nojun", ""]]}, {"id": "1911.08142", "submitter": "Xiang Gao", "authors": "Xiang Gao, Wei Hu, Guo-Jun Qi", "title": "GraphTER: Unsupervised Learning of Graph Transformation Equivariant\n  Representations via Auto-Encoding Node-wise Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Graph Convolutional Neural Networks (GCNNs) have shown\ntheir efficiency for non-Euclidean data on graphs, which often require a large\namount of labeled data with high cost. It it thus critical to learn graph\nfeature representations in an unsupervised manner in practice. To this end, we\npropose a novel unsupervised learning of Graph Transformation Equivariant\nRepresentations (GraphTER), aiming to capture intrinsic patterns of graph\nstructure under both global and local transformations. Specifically, we allow\nto sample different groups of nodes from a graph and then transform them\nnode-wise isotropically or anisotropically. Then, we self-train a\nrepresentation encoder to capture the graph structures by reconstructing these\nnode-wise transformations from the feature representations of the original and\ntransformed graphs. In experiments, we apply the learned GraphTER to graphs of\n3D point cloud data, and results on point cloud segmentation/classification\nshow that GraphTER significantly outperforms state-of-the-art unsupervised\napproaches and pushes greatly closer towards the upper bound set by the fully\nsupervised counterparts. The code is available at:\nhttps://github.com/gyshgx868/graph-ter.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 08:03:12 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 02:50:11 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Gao", "Xiang", ""], ["Hu", "Wei", ""], ["Qi", "Guo-Jun", ""]]}, {"id": "1911.08149", "submitter": "Zhiqiang Xiong", "authors": "Zhiqiang Xiong, Zhicheng Wang, Zhaohui Yu, Xi Gu", "title": "Differentiating Features for Scene Segmentation Based on Dedicated\n  Attention Mechanisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is a challenge in scene parsing. It requires both\ncontext information and rich spatial information. In this paper, we\ndifferentiate features for scene segmentation based on dedicated attention\nmechanisms (DF-DAM), and two attention modules are proposed to optimize the\nhigh-level and low-level features in the encoder, respectively. Specifically,\nwe use the high-level and low-level features of ResNet as the source of context\ninformation and spatial information, respectively, and optimize them with\nattention fusion module and 2D position attention module, respectively. For\nattention fusion module, we adopt dual channel weight to selectively adjust the\nchannel map for the highest two stage features of ResNet, and fuse them to get\ncontext information. For 2D position attention module, we use the context\ninformation obtained by attention fusion module to assist the selection of the\nlowest-stage features of ResNet as supplementary spatial information. Finally,\nthe two sets of information obtained by the two modules are simply fused to\nobtain the prediction. We evaluate our approach on Cityscapes and PASCAL VOC\n2012 datasets. In particular, there aren't complicated and redundant processing\nmodules in our architecture, which greatly reduces the complexity, and we\nachieving 82.3% Mean IoU on PASCAL VOC 2012 test dataset without pre-training\non MS-COCO dataset.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 08:17:59 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Xiong", "Zhiqiang", ""], ["Wang", "Zhicheng", ""], ["Yu", "Zhaohui", ""], ["Gu", "Xi", ""]]}, {"id": "1911.08163", "submitter": "Bernhard Stimpel", "authors": "Bernhard Stimpel, Christopher Syben, Tobias W\\\"urfl, Katharina\n  Breininger, Philipp Hoelter, Arnd D\\\"orfler, and Andreas Maier", "title": "Projection-to-Projection Translation for Hybrid X-ray and Magnetic\n  Resonance Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid X-ray and magnetic resonance (MR) imaging promises large potential in\ninterventional medical imaging applications due to the broad variety of\ncontrast of MRI combined with fast imaging of X-ray-based modalities. To fully\nutilize the potential of the vast amount of existing image enhancement\ntechniques, the corresponding information from both modalities must be present\nin the same domain. For image-guided interventional procedures, X-ray\nfluoroscopy has proven to be the modality of choice. Synthesizing one modality\nfrom another in this case is an ill-posed problem due to ambiguous signal and\noverlapping structures in projective geometry. To take on these challenges, we\npresent a learning-based solution to MR to X-ray projection-to-projection\ntranslation. We propose an image generator network that focuses on high\nrepresentation capacity in higher resolution layers to allow for accurate\nsynthesis of fine details in the projection images. Additionally, a weighting\nscheme in the loss computation that favors high-frequency structures is\nproposed to focus on the important details and contours in projection imaging.\nThe proposed extensions prove valuable in generating X-ray projection images\nwith natural appearance. Our approach achieves a deviation from the ground\ntruth of only $6$% and structural similarity measure of $0.913\\,\\pm\\,0.005$. In\nparticular the high frequency weighting assists in generating projection images\nwith sharp appearance and reduces erroneously synthesized fine details.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 09:05:30 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Stimpel", "Bernhard", ""], ["Syben", "Christopher", ""], ["W\u00fcrfl", "Tobias", ""], ["Breininger", "Katharina", ""], ["Hoelter", "Philipp", ""], ["D\u00f6rfler", "Arnd", ""], ["Maier", "Andreas", ""]]}, {"id": "1911.08169", "submitter": "Chao Tian", "authors": "Chao Tian, Cong Li and Jianping Shi", "title": "Dense Fusion Classmate Network for Land Cover Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, FCNs based methods have made great progress in semantic\nsegmentation. Different with ordinary scenes, satellite image owns specific\ncharacteristics, which elements always extend to large scope and no regular or\nclear boundaries. Therefore, effective mid-level structure information\nextremely missing, precise pixel-level classification becomes tough issues. In\nthis paper, a Dense Fusion Classmate Network (DFCNet) is proposed to adopt in\nland cover classification.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 09:25:59 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Tian", "Chao", ""], ["Li", "Cong", ""], ["Shi", "Jianping", ""]]}, {"id": "1911.08177", "submitter": "Oriane Sim\\'eoni", "authors": "Oriane Sim\\'eoni, Mateusz Budnik, Yannis Avrithis, Guillaume Gravier", "title": "Rethinking deep active learning: Using unlabeled data at model training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning typically focuses on training a model on few labeled examples\nalone, while unlabeled ones are only used for acquisition. In this work we\ndepart from this setting by using both labeled and unlabeled data during model\ntraining across active learning cycles. We do so by using unsupervised feature\nlearning at the beginning of the active learning pipeline and semi-supervised\nlearning at every active learning cycle, on all available data. The former has\nnot been investigated before in active learning, while the study of latter in\nthe context of deep learning is scarce and recent findings are not conclusive\nwith respect to its benefit. Our idea is orthogonal to acquisition strategies\nby using more data, much like ensemble methods use more models. By\nsystematically evaluating on a number of popular acquisition strategies and\ndatasets, we find that the use of unlabeled data during model training brings a\nsurprising accuracy improvement in image classification, compared to the\ndifferences between acquisition strategies. We thus explore smaller label\nbudgets, even one label per class.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 09:42:33 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Sim\u00e9oni", "Oriane", ""], ["Budnik", "Mateusz", ""], ["Avrithis", "Yannis", ""], ["Gravier", "Guillaume", ""]]}, {"id": "1911.08199", "submitter": "Zhijie Lin", "authors": "Zhijie Lin, Zhou Zhao, Zhu Zhang, Qi Wang and Huasheng Liu", "title": "Weakly-Supervised Video Moment Retrieval via Semantic Completion Network", "comments": "Accepted by AAAI 2020 as a full paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video moment retrieval is to search the moment that is most relevant to the\ngiven natural language query. Existing methods are mostly trained in a\nfully-supervised setting, which requires the full annotations of temporal\nboundary for each query. However, manually labeling the annotations is actually\ntime-consuming and expensive. In this paper, we propose a novel\nweakly-supervised moment retrieval framework requiring only coarse video-level\nannotations for training. Specifically, we devise a proposal generation module\nthat aggregates the context information to generate and score all candidate\nproposals in one single pass. We then devise an algorithm that considers both\nexploitation and exploration to select top-K proposals. Next, we build a\nsemantic completion module to measure the semantic similarity between the\nselected proposals and query, compute reward and provide feedbacks to the\nproposal generation module for scoring refinement. Experiments on the\nActivityCaptions and Charades-STA demonstrate the effectiveness of our proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 10:31:43 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 12:40:37 GMT"}, {"version": "v3", "created": "Wed, 15 Jan 2020 11:09:43 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Lin", "Zhijie", ""], ["Zhao", "Zhou", ""], ["Zhang", "Zhu", ""], ["Wang", "Qi", ""], ["Liu", "Huasheng", ""]]}, {"id": "1911.08206", "submitter": "Haim Barad", "authors": "Barak Battash, Haim Barad, Hanlin Tang, Amit Bleiweiss", "title": "Mimic The Raw Domain: Accelerating Action Recognition in the Compressed\n  Domain", "comments": "CVPR 2020: Joint Workshop on Efficient Deep Learning in Computer\n  Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video understanding usually requires expensive computation that prohibits its\ndeployment, yet videos contain significant spatiotemporal redundancy that can\nbe exploited. In particular, operating directly on the motion vectors and\nresiduals in the compressed video domain can significantly accelerate the\ncompute, by not using the raw videos which demand colossal storage capacity.\nExisting methods approach this task as a multiple modalities problem. In this\npaper we are approaching the task in a completely different way; we are looking\nat the data from the compressed stream as a one unit clip and propose that the\nresidual frames can replace the original RGB frames from the raw domain.\nFurthermore, we are using teacher-student method to aid the network in the\ncompressed domain to mimic the teacher network in the raw domain. We show\nexperiments on three leading datasets (HMDB51, UCF1, and Kinetics) that\napproach state-of-the-art accuracy on raw video data by using compressed data.\nOur model MFCD-Net outperforms prior methods in the compressed domain and more\nimportantly, our model has 11X fewer parameters and 3X fewer Flops,\ndramatically improving the efficiency of video recognition inference. This\napproach enables applying neural networks exclusively in the compressed domain\nwithout compromising accuracy while accelerating performance.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 11:05:46 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 05:38:09 GMT"}, {"version": "v3", "created": "Wed, 22 Apr 2020 08:21:16 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Battash", "Barak", ""], ["Barad", "Haim", ""], ["Tang", "Hanlin", ""], ["Bleiweiss", "Amit", ""]]}, {"id": "1911.08216", "submitter": "Neelanjan Bhowmik", "authors": "Neelanjan Bhowmik, Yona Falinie A. Gaus, Samet Akcay, Jack W. Barker,\n  Toby P. Breckon", "title": "On the Impact of Object and Sub-component Level Segmentation Strategies\n  for Supervised Anomaly Detection within X-ray Security Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  X-ray security screening is in widespread use to maintain transportation\nsecurity against a wide range of potential threat profiles. Of particular\ninterest is the recent focus on the use of automated screening approaches,\nincluding the potential anomaly detection as a methodology for concealment\ndetection within complex electronic items. Here we address this problem\nconsidering varying segmentation strategies to enable the use of both object\nlevel and sub-component level anomaly detection via the use of secondary\nconvolutional neural network (CNN) architectures. Relative performance is\nevaluated over an extensive dataset of exemplar cluttered X-ray imagery, with a\nfocus on consumer electronics items. We find that sub-component level\nsegmentation produces marginally superior performance in the secondary anomaly\ndetection via classification stage, with true positive of ~98% of anomalies,\nwith a ~3% false positive.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 11:54:18 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Bhowmik", "Neelanjan", ""], ["Gaus", "Yona Falinie A.", ""], ["Akcay", "Samet", ""], ["Barker", "Jack W.", ""], ["Breckon", "Toby P.", ""]]}, {"id": "1911.08217", "submitter": "Huizhou Li", "authors": "Chao Yang, Huizhou Li, Fangting Lin, Bin Jiang, Hao Zhao", "title": "Constrained R-CNN: A general image manipulation detection model", "comments": "Accepted to IEEE International Conference on Multimedia and Expo\n  (ICME2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning-based models have exhibited remarkable performance\nfor image manipulation detection. However, most of them suffer from poor\nuniversality of handcrafted or predetermined features. Meanwhile, they only\nfocus on manipulation localization and overlook manipulation classification. To\naddress these issues, we propose a coarse-to-fine architecture named\nConstrained R-CNN for complete and accurate image forensics. First, the\nlearnable manipulation feature extractor learns a unified feature\nrepresentation directly from data. Second, the attention region proposal\nnetwork effectively discriminates manipulated regions for the next manipulation\nclassification and coarse localization. Then, the skip structure fuses\nlow-level and high-level information to refine the global manipulation\nfeatures. Finally, the coarse localization information guides the model to\nfurther learn the finer local features and segment out the tampered region.\nExperimental results show that our model achieves state-of-the-art performance.\nEspecially, the F1 score is increased by 28.4%, 73.2%, 13.3% on the NIST16,\nCOVERAGE, and Columbia dataset.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 12:12:20 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 12:14:58 GMT"}, {"version": "v3", "created": "Sun, 15 Mar 2020 11:01:38 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Yang", "Chao", ""], ["Li", "Huizhou", ""], ["Lin", "Fangting", ""], ["Jiang", "Bin", ""], ["Zhao", "Hao", ""]]}, {"id": "1911.08233", "submitter": "You Hao", "authors": "You Hao, Hanlin Mo, Qi Li, He Zhang, Hua Li", "title": "Dual affine moment invariants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affine transformation is one of the most common transformations in nature,\nwhich is an important issue in the field of computer vision and shape analysis.\nAnd affine transformations often occur in both shape and color space\nsimultaneously, which can be termed as Dual-Affine Transformation (DAT). In\ngeneral, we should derive invariants of different data formats separately, such\nas 2D color images, 3D color objects, or even higher-dimensional data. To the\nbest of our knowledge, there is no general framework to derive invariants for\nall of these data formats. In this paper, we propose a general framework to\nderive moment invariants under DAT for objects in M-dimensional space with N\nchannels, which can be called dual-affine moment invariants (DAMI). Following\nthis framework, we present the generating formula of DAMI under DAT for 3D\ncolor objects. Then, we instantiated a complete set of DAMI for 3D color\nobjects with orders and degrees no greater than 4. Finally, we analyze the\ncharacteristic of these DAMI and conduct classification experiments to evaluate\nthe stability and discriminability of them. The results prove that DAMI is\nrobust for DAT. Our derivation framework can be applied to data in any\ndimension with any number of channels.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 12:55:17 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Hao", "You", ""], ["Mo", "Hanlin", ""], ["Li", "Qi", ""], ["Zhang", "He", ""], ["Li", "Hua", ""]]}, {"id": "1911.08251", "submitter": "Gabriele Cesa", "authors": "Maurice Weiler and Gabriele Cesa", "title": "General $E(2)$-Equivariant Steerable CNNs", "comments": "Conference on Neural Information Processing Systems (NeurIPS), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The big empirical success of group equivariant networks has led in recent\nyears to the sprouting of a great variety of equivariant network architectures.\nA particular focus has thereby been on rotation and reflection equivariant CNNs\nfor planar images. Here we give a general description of $E(2)$-equivariant\nconvolutions in the framework of Steerable CNNs. The theory of Steerable CNNs\nthereby yields constraints on the convolution kernels which depend on group\nrepresentations describing the transformation laws of feature spaces. We show\nthat these constraints for arbitrary group representations can be reduced to\nconstraints under irreducible representations. A general solution of the kernel\nspace constraint is given for arbitrary representations of the Euclidean group\n$E(2)$ and its subgroups. We implement a wide range of previously proposed and\nentirely new equivariant network architectures and extensively compare their\nperformances. $E(2)$-steerable convolutions are further shown to yield\nremarkable gains on CIFAR-10, CIFAR-100 and STL-10 when used as a drop-in\nreplacement for non-equivariant convolutions.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 13:25:49 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 17:46:56 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Weiler", "Maurice", ""], ["Cesa", "Gabriele", ""]]}, {"id": "1911.08261", "submitter": "Qianhui Liu", "authors": "Qianhui Liu, Gang Pan, Haibo Ruan, Dong Xing, Qi Xu, and Huajin Tang", "title": "Unsupervised AER Object Recognition Based on Multiscale Spatio-Temporal\n  Features and Spiking Neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an unsupervised address event representation (AER) object\nrecognition approach. The proposed approach consists of a novel multiscale\nspatio-temporal feature (MuST) representation of input AER events and a spiking\nneural network (SNN) using spike-timing-dependent plasticity (STDP) for object\nrecognition with MuST. MuST extracts the features contained in both the spatial\nand temporal information of AER event flow, and meanwhile forms an informative\nand compact feature spike representation. We show not only how MuST exploits\nspikes to convey information more effectively, but also how it benefits the\nrecognition using SNN. The recognition process is performed in an unsupervised\nmanner, which does not need to specify the desired status of every single\nneuron of SNN, and thus can be flexibly applied in real-world recognition\ntasks. The experiments are performed on five AER datasets including a new one\nnamed GESTURE-DVS. Extensive experimental results show the effectiveness and\nadvantages of this proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 13:47:47 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Liu", "Qianhui", ""], ["Pan", "Gang", ""], ["Ruan", "Haibo", ""], ["Xing", "Dong", ""], ["Xu", "Qi", ""], ["Tang", "Huajin", ""]]}, {"id": "1911.08287", "submitter": "Dongwei Ren", "authors": "Zhaohui Zheng and Ping Wang and Wei Liu and Jinze Li and Rongguang Ye\n  and Dongwei Ren", "title": "Distance-IoU Loss: Faster and Better Learning for Bounding Box\n  Regression", "comments": "Accepted to AAAI 2020. The source code and trained models are\n  available at https://github.com/Zzh-tju/DIoU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bounding box regression is the crucial step in object detection. In existing\nmethods, while $\\ell_n$-norm loss is widely adopted for bounding box\nregression, it is not tailored to the evaluation metric, i.e., Intersection\nover Union (IoU). Recently, IoU loss and generalized IoU (GIoU) loss have been\nproposed to benefit the IoU metric, but still suffer from the problems of slow\nconvergence and inaccurate regression. In this paper, we propose a Distance-IoU\n(DIoU) loss by incorporating the normalized distance between the predicted box\nand the target box, which converges much faster in training than IoU and GIoU\nlosses. Furthermore, this paper summarizes three geometric factors in bounding\nbox regression, \\ie, overlap area, central point distance and aspect ratio,\nbased on which a Complete IoU (CIoU) loss is proposed, thereby leading to\nfaster convergence and better performance. By incorporating DIoU and CIoU\nlosses into state-of-the-art object detection algorithms, e.g., YOLO v3, SSD\nand Faster RCNN, we achieve notable performance gains in terms of not only IoU\nmetric but also GIoU metric. Moreover, DIoU can be easily adopted into\nnon-maximum suppression (NMS) to act as the criterion, further boosting\nperformance improvement. The source code and trained models are available at\nhttps://github.com/Zzh-tju/DIoU.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 14:20:07 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Zheng", "Zhaohui", ""], ["Wang", "Ping", ""], ["Liu", "Wei", ""], ["Li", "Jinze", ""], ["Ye", "Rongguang", ""], ["Ren", "Dongwei", ""]]}, {"id": "1911.08299", "submitter": "Wen Qian Tom", "authors": "Wen Qian, Xue Yang, Silong Peng, Yue Guo, Junchi Yan", "title": "Learning Modulated Loss for Rotated Object Detection", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular rotated detection methods usually use five parameters (coordinates of\nthe central point, width, height, and rotation angle) to describe the rotated\nbounding box and l1-loss as the loss function. In this paper, we argue that the\naforementioned integration can cause training instability and performance\ndegeneration, due to the loss discontinuity resulted from the inherent\nperiodicity of angles and the associated sudden exchange of width and height.\nThis problem is further pronounced given the regression inconsistency among\nfive parameters with different measurement units. We refer to the above issues\nas rotation sensitivity error (RSE) and propose a modulated rotation loss to\ndismiss the loss discontinuity. Our new loss is combined with the\neight-parameter regression to further solve the problem of inconsistent\nparameter regression. Experiments show the state-of-art performances of our\nmethod on the public aerial image benchmark DOTA and UCAS-AOD. Its\ngeneralization abilities are also verified on ICDAR2015, HRSC2016, and FDDB.\nQualitative improvements can be seen in Fig 1, and the source code will be\nreleased with the publication of the paper.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 14:37:41 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 02:33:55 GMT"}, {"version": "v3", "created": "Fri, 20 Dec 2019 13:21:28 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Qian", "Wen", ""], ["Yang", "Xue", ""], ["Peng", "Silong", ""], ["Guo", "Yue", ""], ["Yan", "Junchi", ""]]}, {"id": "1911.08303", "submitter": "Nikhila Ponugoti", "authors": "Ponugoti Nikhila, Sabari Nathan, Elmer Jeto Gomes Ataide, Alfredo\n  Illanes, Dr. Michael Friebe, Srichandana Abbineni", "title": "Lightweight Residual Network for The Classification of Thyroid Nodules", "comments": "1 Page , 1 Figure , IEEE EMBS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound is a useful technique for diagnosing thyroid nodules. Benign and\nmalignant nodules that automatically discriminate in the ultrasound pictures\ncan provide diagnostic recommendations or, improve diagnostic accuracy in the\nabsence of specialists. The main issue here is how to collect suitable features\nfor this particular task. We suggest here a technique for extracting features\nfrom ultrasound pictures based on the Residual U-net. We attempt to introduce\nsignificant semantic characteristics to the classification. Our model gained\n95% classification accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 06:32:24 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Nikhila", "Ponugoti", ""], ["Nathan", "Sabari", ""], ["Ataide", "Elmer Jeto Gomes", ""], ["Illanes", "Alfredo", ""], ["Friebe", "Dr. Michael", ""], ["Abbineni", "Srichandana", ""]]}, {"id": "1911.08324", "submitter": "Yinlin Hu", "authors": "Yinlin Hu and Pascal Fua and Wei Wang and Mathieu Salzmann", "title": "Single-Stage 6D Object Pose Estimation", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most recent 6D pose estimation frameworks first rely on a deep network to\nestablish correspondences between 3D object keypoints and 2D image locations\nand then use a variant of a RANSAC-based Perspective-n-Point (PnP) algorithm.\nThis two-stage process, however, is suboptimal: First, it is not end-to-end\ntrainable. Second, training the deep network relies on a surrogate loss that\ndoes not directly reflect the final 6D pose estimation task.\n  In this work, we introduce a deep architecture that directly regresses 6D\nposes from correspondences. It takes as input a group of candidate\ncorrespondences for each 3D keypoint and accounts for the fact that the order\nof the correspondences within each group is irrelevant, while the order of the\ngroups, that is, of the 3D keypoints, is fixed. Our architecture is generic and\ncan thus be exploited in conjunction with existing correspondence-extraction\nnetworks so as to yield single-stage 6D pose estimation frameworks. Our\nexperiments demonstrate that these single-stage frameworks consistently\noutperform their two-stage counterparts in terms of both accuracy and speed.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 14:56:54 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 21:54:00 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Hu", "Yinlin", ""], ["Fua", "Pascal", ""], ["Wang", "Wei", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "1911.08333", "submitter": "Tim Barfoot", "authors": "Timothy D. Barfoot and James R. Forbes and David Yoon", "title": "Exactly Sparse Gaussian Variational Inference with Application to\n  Derivative-Free Batch Nonlinear State Estimation", "comments": "Accepted to the International Journal of Robotics Research (IJRR) on\n  8 April 2020, # IJR-19-3748; 31 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Gaussian Variational Inference (GVI) technique that can be\napplied to large-scale nonlinear batch state estimation problems. The main\ncontribution is to show how to fit both the mean and (inverse) covariance of a\nGaussian to the posterior efficiently, by exploiting factorization of the joint\nlikelihood of the state and data, as is common in practical problems. This is\ndifferent than Maximum A Posteriori (MAP) estimation, which seeks the point\nestimate for the state that maximizes the posterior (i.e., the mode). The\nproposed Exactly Sparse Gaussian Variational Inference (ESGVI) technique stores\nthe inverse covariance matrix, which is typically very sparse (e.g.,\nblock-tridiagonal for classic state estimation). We show that the only blocks\nof the (dense) covariance matrix that are required during the calculations\ncorrespond to the non-zero blocks of the inverse covariance matrix, and further\nshow how to calculate these blocks efficiently in the general GVI problem.\nESGVI operates iteratively, and while we can use analytical derivatives at each\niteration, Gaussian cubature can be substituted, thereby producing an efficient\nderivative-free batch formulation. ESGVI simplifies to precisely the\nRauch-Tung-Striebel (RTS) smoother in the batch linear estimation case, but\ngoes beyond the 'extended' RTS smoother in the nonlinear case since it finds\nthe best-fit Gaussian (mean and covariance), not the MAP point estimate. We\ndemonstrate the technique on controlled simulation problems and a batch\nnonlinear Simultaneous Localization and Mapping (SLAM) problem with an\nexperimental dataset.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 22:41:01 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 20:18:27 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Barfoot", "Timothy D.", ""], ["Forbes", "James R.", ""], ["Yoon", "David", ""]]}, {"id": "1911.08344", "submitter": "Ying Huang", "authors": "Ying Huang, Bin Sun, Haipeng Kan, Jiankai Zhuang, Zengchang Qin", "title": "FollowMeUp Sports: New Benchmark for 2D Human Keypoint Recognition", "comments": "12 pages, accepted at PRCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Human pose estimation has made significant advancement in recent years.\nHowever, the existing datasets are limited in their coverage of pose variety.\nIn this paper, we introduce a novel benchmark FollowMeUp Sports that makes an\nimportant advance in terms of specific postures, self-occlusion and class\nbalance, a contribution that we feel is required for future development in\nhuman body models. This comprehensive dataset was collected using an\nestablished taxonomy of over 200 standard workout activities with three\ndifferent shot angles. The collected videos cover a wider variety of specific\nworkout activities than previous datasets including push-up, squat and body\nmoving near the ground with severe self-occlusion or occluded by some sport\nequipment and outfits. Given these rich images, we perform a detailed analysis\nof the leading human pose estimation approaches gaining insights for the\nsuccess and failures of these methods.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 15:23:23 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Huang", "Ying", ""], ["Sun", "Bin", ""], ["Kan", "Haipeng", ""], ["Zhuang", "Jiankai", ""], ["Qin", "Zengchang", ""]]}, {"id": "1911.08348", "submitter": "Oran Gafni", "authors": "Oran Gafni, Lior Wolf, Yaniv Taigman", "title": "Live Face De-Identification in Video", "comments": "ICCV 2019", "journal-ref": "Proceedings of the IEEE International Conference on Computer\n  Vision (2019) 9378--9387", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for face de-identification that enables fully automatic\nvideo modification at high frame rates. The goal is to maximally decorrelate\nthe identity, while having the perception (pose, illumination and expression)\nfixed. We achieve this by a novel feed-forward encoder-decoder network\narchitecture that is conditioned on the high-level representation of a person's\nfacial image. The network is global, in the sense that it does not need to be\nretrained for a given video or for a given identity, and it creates natural\nlooking image sequences with little distortion in time.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 15:28:35 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Gafni", "Oran", ""], ["Wolf", "Lior", ""], ["Taigman", "Yaniv", ""]]}, {"id": "1911.08350", "submitter": "Juan Banda", "authors": "Toqi Tahamid Sarker and Juan M. Banda", "title": "Solar Event Tracking with Deep Regression Networks: A Proof of Concept\n  Evaluation", "comments": "8 pages, 5 figures, this has been submitted and accepted for\n  publication at IEEE Big Data 2019 - SABID Workshop", "journal-ref": "2019 IEEE International Conference on Big Data (Big Data)", "doi": "10.1109/BigData47090.2019.9006273", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of deep learning for computer vision tasks, the need for\naccurately labeled data in large volumes is vital for any application. The\nincreasingly available large amounts of solar image data generated by the Solar\nDynamic Observatory (SDO) mission make this domain particularly interesting for\nthe development and testing of deep learning systems. The currently available\nlabeled solar data is generated by the SDO mission's Feature Finding Team's\n(FFT) specialized detection modules. The major drawback of these modules is\nthat detection and labeling is performed with a cadence of every 4 to 12 hours,\ndepending on the module. Since SDO image data products are created every 10\nseconds, there is a considerable gap between labeled observations and the\ncontinuous data stream. In order to address this shortcoming, we trained a deep\nregression network to track the movement of two solar phenomena: Active Region\nand Coronal Hole events. To the best of our knowledge, this is the first\nattempt of solar event tracking using a deep learning approach. Since it is\nimpossible to fully evaluate the performance of the suggested event tracks with\nthe original data (only partial ground truth is available), we demonstrate with\nseveral metrics the effectiveness of our approach. With the purpose of\ngenerating continuously labeled solar image data, we present this feasibility\nanalysis showing the great promise of deep regression networks for this task.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 15:32:10 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Sarker", "Toqi Tahamid", ""], ["Banda", "Juan M.", ""]]}, {"id": "1911.08388", "submitter": "Mohammadreza Soltaninejad PhD", "authors": "Mehdi Amian and Mohammadreza Soltaninejad", "title": "Multi-Resolution 3D CNN for MRI Brain Tumor Segmentation and Survival\n  Prediction", "comments": "Submitted to Lecture Notes in Computer Science (LNCS) BraTS\n  proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, an automated three dimensional (3D) deep segmentation approach\nfor detecting gliomas in 3D pre-operative MRI scans is proposed. Then, a\nclassi-fication algorithm based on random forests, for survival prediction is\npresented. The objective is to segment the glioma area and produce segmentation\nlabels for its different sub-regions, i.e. necrotic and the non-enhancing tumor\ncore, the peri-tumoral edema, and enhancing tumor. The proposed deep\narchitecture for the segmentation task encompasses two parallel streamlines\nwith two different reso-lutions. One deep convolutional neural network is to\nlearn local features of the input data while the other one is set to have a\nglobal observation on whole image. Deemed to be complementary, the outputs of\neach stream are then merged to pro-vide an ensemble complete learning of the\ninput image. The proposed network takes the whole image as input instead of\npatch-based approaches in order to con-sider the semantic features throughout\nthe whole volume. The algorithm is trained on BraTS 2019 which included 335\ntraining cases, and validated on 127 unseen cases from the validation dataset\nusing a blind testing approach. The proposed method was also evaluated on the\nBraTS 2019 challenge test dataset of 166 cases. The results show that the\nproposed methods provide promising segmentations as well as survival\nprediction. The mean Dice overlap measures of automatic brain tumor\nsegmentation for validation set were 0.84, 0.74 and 0.71 for the whole tu-mor,\ncore and enhancing tumor, respectively. The corresponding results for the\nchallenge test dataset were 0.82, 0.72, and 0.70, respectively. The overall\naccura-cy of the proposed model for the survival prediction task is %52 for the\nvalida-tion and %49 for the test dataset.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 16:36:54 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Amian", "Mehdi", ""], ["Soltaninejad", "Mohammadreza", ""]]}, {"id": "1911.08400", "submitter": "Christian Bartz", "authors": "Christian Bartz, Joseph Bethge, Haojin Yang and Christoph Meinel", "title": "KISS: Keeping It Simple for Scene Text Recognition", "comments": "Code and Models available at https://github.com/Bartzi/kiss", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, several new methods for scene text recognition have\nbeen proposed. Most of these methods propose novel building blocks for neural\nnetworks. These novel building blocks are specially tailored for the task of\nscene text recognition and can thus hardly be used in any other tasks. In this\npaper, we introduce a new model for scene text recognition that only consists\nof off-the-shelf building blocks for neural networks. Our model (KISS) consists\nof two ResNet based feature extractors, a spatial transformer, and a\ntransformer. We train our model only on publicly available, synthetic training\ndata and evaluate it on a range of scene text recognition benchmarks, where we\nreach state-of-the-art or competitive performance, although our model does not\nuse methods like 2D-attention, or image rectification.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 17:13:18 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Bartz", "Christian", ""], ["Bethge", "Joseph", ""], ["Yang", "Haojin", ""], ["Meinel", "Christoph", ""]]}, {"id": "1911.08402", "submitter": "Zhiguo Wang", "authors": "Zhiguo Wang, Zhongliang Yang, Yu-Jin Zhang", "title": "A Promotion Method for Generation Error Based Video Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surveillance video anomaly detection is to detect events that rarely or never\nhappened in a certain scene. The generation error (GE)-based methods exhibit\nexcellent performance on this task. They firstly train a generative neural\nnetwork (GNN) to generate normal samples, then judge the samples with large GEs\nas anomalies. Almost all the GE-based methods utilize frame-level GEs to detect\nanomalies. However, anomalies generally occur in local areas, the frame-level\nGE introduces GEs of normal areas to anomaly discriminations, that brings two\nproblems: i) The GE of normal areas reduces the anomaly saliency of the\nanomalous frame. ii) Different videos have different normal-GE-levels, thus it\nis hard to set a uniform threshold for all videos to detect anomalies. To\naddress these problems, we propose a promotion method: utilize the maximum of\nblock-level GEs on the frame to detect anomaly. Firstly, we calculate the\nblock-level GEs at each position on the frame. Then, we utilize the maximum of\nthe block-level GEs on the frame to detect anomalies. Based on the existed GNN\nmodels, experiments are carried out on multiple datasets. The results\ndemonstrate the effectiveness of the proposed method and achieve\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 17:18:26 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 13:47:15 GMT"}, {"version": "v3", "created": "Tue, 17 Dec 2019 03:08:42 GMT"}, {"version": "v4", "created": "Tue, 17 Mar 2020 02:24:24 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Wang", "Zhiguo", ""], ["Yang", "Zhongliang", ""], ["Zhang", "Yu-Jin", ""]]}, {"id": "1911.08432", "submitter": "Tiange Luo", "authors": "Tiange Luo, Tianle Cai, Mengxiao Zhang, Siyu Chen, Di He, Liwei Wang", "title": "Defective Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robustness of convolutional neural networks (CNNs) has gained in importance\non account of adversarial examples, i.e., inputs added as well-designed\nperturbations that are imperceptible to humans but can cause the model to\npredict incorrectly. Recent research suggests that the noises in adversarial\nexamples break the textural structure, which eventually leads to wrong\npredictions. To mitigate the threat of such adversarial attacks, we propose\ndefective convolutional networks that make predictions relying less on textural\ninformation but more on shape information by properly integrating defective\nconvolutional layers into standard CNNs. The defective convolutional layers\ncontain defective neurons whose activations are set to be a constant function.\nAs defective neurons contain no information and are far different from standard\nneurons in its spatial neighborhood, the textural features cannot be accurately\nextracted, and so the model has to seek other features for classification, such\nas the shape. We show extensive evidence to justify our proposal and\ndemonstrate that defective CNNs can defense against black-box attacks better\nthan standard CNNs. In particular, they achieve state-of-the-art performance\nagainst transfer-based attacks without any adversarial training being applied.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 17:56:22 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 20:47:57 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Luo", "Tiange", ""], ["Cai", "Tianle", ""], ["Zhang", "Mengxiao", ""], ["Chen", "Siyu", ""], ["He", "Di", ""], ["Wang", "Liwei", ""]]}, {"id": "1911.08434", "submitter": "Jo\\~ao Pedrosa", "authors": "Jo\\~ao Pedrosa, Guilherme Aresta, Carlos Ferreira, M\\'arcio Rodrigues,\n  Patr\\'icia Leit\\~ao, Andr\\'e Silva Carvalho, Jo\\~ao Rebelo, Eduardo Negr\\~ao,\n  Isabel Ramos, Ant\\'onio Cunha and Aur\\'elio Campilho", "title": "LNDb: A Lung Nodule Database on Computed Tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Lung cancer is the deadliest type of cancer worldwide and late detection is\nthe major factor for the low survival rate of patients. Low dose computed\ntomography has been suggested as a potential screening tool but manual\nscreening is costly, time-consuming and prone to variability. This has fueled\nthe development of automatic methods for the detection, segmentation and\ncharacterisation of pulmonary nodules but its application to clinical routine\nis challenging. In this study, a new database for the development and testing\nof pulmonary nodule computer-aided strategies is presented which intends to\ncomplement current databases by giving additional focus to radiologist\nvariability and local clinical reality. State-of-the-art nodule detection,\nsegmentation and characterization methods are tested and compared to manual\nannotations as well as collaborative strategies combining multiple radiologists\nand radiologists and computer-aided systems. It is shown that state-of-the-art\nmethodologies can determine a patient's follow-up recommendation as accurately\nas a radiologist, though the nodule detection method used shows decreased\nperformance in this database.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 17:58:59 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 09:20:30 GMT"}, {"version": "v3", "created": "Thu, 19 Dec 2019 09:40:20 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Pedrosa", "Jo\u00e3o", ""], ["Aresta", "Guilherme", ""], ["Ferreira", "Carlos", ""], ["Rodrigues", "M\u00e1rcio", ""], ["Leit\u00e3o", "Patr\u00edcia", ""], ["Carvalho", "Andr\u00e9 Silva", ""], ["Rebelo", "Jo\u00e3o", ""], ["Negr\u00e3o", "Eduardo", ""], ["Ramos", "Isabel", ""], ["Cunha", "Ant\u00f3nio", ""], ["Campilho", "Aur\u00e9lio", ""]]}, {"id": "1911.08478", "submitter": "Ankur Mali", "authors": "Ankur Mali, Alexander G. Ororbia, Clyde Lee Giles", "title": "Sibling Neural Estimators: Improving Iterative Image Decoding with\n  Gradient Communication", "comments": "11 Pages, 2 figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For lossy image compression, we develop a neural-based system which learns a\nnonlinear estimator for decoding from quantized representations. The system\nlinks two recurrent networks that \\help\" each other reconstruct same target\nimage patches using complementary portions of spatial context that communicate\nvia gradient signals. This dual agent system builds upon prior work that\nproposed the iterative refinement algorithm for recurrent neural network\n(RNN)based decoding which improved image reconstruction compared to standard\ndecoding techniques. Our approach, which works with any encoder, neural or\nnon-neural, This system progressively reduces image patch reconstruction error\nover a fixed number of steps. Experiment with variants of RNN memory cells,\nwith and without future information, find that our model consistently creates\nlower distortion images of higher perceptual quality compared to other\napproaches. Specifically, on the Kodak Lossless True Color Image Suite, we\nobserve as much as a 1:64 decibel (dB) gain over JPEG, a 1:46 dB gain over JPEG\n2000, a 1:34 dB gain over the GOOG neural baseline, 0:36 over E2E (a modern\ncompetitive neural compression model), and 0:37 over a single iterative neural\ndecoder.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 00:18:44 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Mali", "Ankur", ""], ["Ororbia", "Alexander G.", ""], ["Giles", "Clyde Lee", ""]]}, {"id": "1911.08479", "submitter": "Hanjiang Lai", "authors": "Haien Zeng, Hanjiang Lai, Hanlu Chu, Yong Tang, Jian Yin", "title": "Modal-aware Features for Multimodal Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many retrieval applications can benefit from multiple modalities, e.g., text\nthat contains images on Wikipedia, for which how to represent multimodal data\nis the critical component. Most deep multimodal learning methods typically\ninvolve two steps to construct the joint representations: 1) learning of\nmultiple intermediate features, with each intermediate feature corresponding to\na modality, using separate and independent deep models; 2) merging the\nintermediate features into a joint representation using a fusion strategy.\nHowever, in the first step, these intermediate features do not have previous\nknowledge of each other and cannot fully exploit the information contained in\nthe other modalities. In this paper, we present a modal-aware operation as a\ngeneric building block to capture the non-linear dependences among the\nheterogeneous intermediate features that can learn the underlying correlation\nstructures in other multimodal data as soon as possible. The modal-aware\noperation consists of a kernel network and an attention network. The kernel\nnetwork is utilized to learn the non-linear relationships with other\nmodalities. Then, to learn better representations for binary hash codes, we\npresent an attention network that finds the informative regions of these\nmodal-aware features that are favorable for retrieval. Experiments conducted on\nthree public benchmark datasets demonstrate significant improvements in the\nperformance of our method relative to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 02:17:21 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Zeng", "Haien", ""], ["Lai", "Hanjiang", ""], ["Chu", "Hanlu", ""], ["Tang", "Yong", ""], ["Yin", "Jian", ""]]}, {"id": "1911.08483", "submitter": "Chengliang Dai", "authors": "Shuo Wang, Chengliang Dai, Yuanhan Mo, Elsa Angelini, Yike Guo, Wenjia\n  Bai", "title": "Automatic Brain Tumour Segmentation and Biophysics-Guided Survival\n  Prediction", "comments": "MICCAI BraTS 2019 Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gliomas are the most common malignant brain tumourswith intrinsic\nheterogeneity. Accurate segmentation of gliomas and theirsub-regions on\nmulti-parametric magnetic resonance images (mpMRI)is of great clinical\nimportance, which defines tumour size, shape andappearance and provides\nabundant information for preoperative diag-nosis, treatment planning and\nsurvival prediction. Recent developmentson deep learning have significantly\nimproved the performance of auto-mated medical image segmentation. In this\npaper, we compare severalstate-of-the-art convolutional neural network models\nfor brain tumourimage segmentation. Based on the ensembled segmentation, we\npresenta biophysics-guided prognostic model for patient overall survival\npredic-tion which outperforms a data-driven radiomics approach. Our methodwon\nthe second place of the MICCAI 2019 BraTS Challenge for theoverall survival\nprediction.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 14:44:55 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Wang", "Shuo", ""], ["Dai", "Chengliang", ""], ["Mo", "Yuanhan", ""], ["Angelini", "Elsa", ""], ["Guo", "Yike", ""], ["Bai", "Wenjia", ""]]}, {"id": "1911.08511", "submitter": "Michael Peven", "authors": "Michael Peven, Gregory D. Hager, Austin Reiter", "title": "Action Recognition Using Volumetric Motion Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Traditional action recognition models are constructed around the paradigm of\n2D perspective imagery. Though sophisticated time-series models have pushed the\nfield forward, much of the information is still not exploited by confining the\ndomain to 2D. In this work, we introduce a novel representation of motion as a\nvoxelized 3D vector field and demonstrate how it can be used to improve\nperformance of action recognition networks. This volumetric representation is a\nnatural fit for 3D CNNs, and allows out-of-plane data augmentation techniques\nduring training of these networks. Both the construction of this representation\nfrom RGB-D video and inference can be run in real time. We demonstrate superior\nresults using this representation with our network design on the open-source\nNTU RGB+D dataset where it outperforms state-of-the-art on both of the defined\nevaluation metrics. Furthermore, we experimentally show how the out-of-plane\naugmentation techniques create viewpoint invariance and allow the model trained\nusing this representation to generalize to unseen camera angles. Code is\navailable here: https://github.com/mpeven/ntu_rgb.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 19:13:57 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Peven", "Michael", ""], ["Hager", "Gregory D.", ""], ["Reiter", "Austin", ""]]}, {"id": "1911.08541", "submitter": "Shuang Zhang", "authors": "Shuang Zhang, Ada Zhen, Robert L. Stevenson", "title": "Deep Motion Blur Removal Using Noisy/Blurry Image Pairs", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Removing spatially variant motion blur from a blurry image is a challenging\nproblem as blur sources are complicated and difficult to model accurately.\nRecent progress in deep neural networks suggests that kernel free single image\ndeblurring can be efficiently performed, but questions about deblurring\nperformance persist. Thus, we propose to restore a sharp image by fusing a pair\nof noisy/blurry images captured in a burst. Two neural network structures,\nDeblurRNN and DeblurMerger, are presented to exploit the pair of images in a\nsequential manner or parallel manner. To boost the training, gradient loss,\nadversarial loss and spectral normalization are leveraged. The training dataset\nthat consists of pairs of noisy/blurry images and the corresponding ground\ntruth sharp image is synthesized based on the benchmark dataset GOPRO. We\nevaluated the trained networks on a variety of synthetic datasets and real\nimage pairs. The results demonstrate that the proposed approach outperforms the\nstate-of-the-art both qualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 20:13:58 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 17:57:39 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Zhang", "Shuang", ""], ["Zhen", "Ada", ""], ["Stevenson", "Robert L.", ""]]}, {"id": "1911.08548", "submitter": "Junwei Ma", "authors": "Junwei Ma, Satya Krishna Gorti, Maksims Volkovs, Ilya Stanevich,\n  Guangwei Yu", "title": "Cross-Class Relevance Learning for Temporal Concept Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel Cross-Class Relevance Learning approach for the task of\ntemporal concept localization. Most localization architectures rely on feature\nextraction layers followed by a classification layer which outputs class\nprobabilities for each segment. However, in many real-world applications\nclasses can exhibit complex relationships that are difficult to model with this\narchitecture. In contrast, we propose to incorporate target class and\nclass-related features as input, and learn a pairwise binary model to predict\ngeneral segment to class relevance. This facilitates learning of shared\ninformation between classes, and allows for arbitrary class-specific feature\nengineering. We apply this approach to the 3rd YouTube-8M Video Understanding\nChallenge together with other leading models, and achieve first place out of\nover 280 teams. In this paper we describe our approach and show some empirical\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 20:31:04 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Ma", "Junwei", ""], ["Gorti", "Satya Krishna", ""], ["Volkovs", "Maksims", ""], ["Stanevich", "Ilya", ""], ["Yu", "Guangwei", ""]]}, {"id": "1911.08564", "submitter": "Oran Shayer", "authors": "Or Isaacs, Oran Shayer, Michael Lindenbaum", "title": "Enhancing Generic Segmentation with Learned Region Representations", "comments": "CVPR 2020. arXiv admin note: substantial text overlap with\n  arXiv:1909.11735", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current successful approaches for generic (non-semantic) segmentation rely\nmostly on edge detection and have leveraged the strengths of deep learning\nmainly by improving the edge detection stage in the algorithmic pipeline. This\nis in contrast to semantic and instance segmentation, where DNNs are applied\ndirectly to generate pixel-wise segment representations. We propose a new\nmethod for learning a pixel-wise representation that reflects segment\nrelatedness. This representation is combined with an edge map to yield a new\nsegmentation algorithm. We show that the representations themselves achieve\nstate-of-the-art segment similarity scores. Moreover, the proposed combined\nsegmentation algorithm provides results that are either state of the art or\nimprove upon it, for most quality measures.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 13:31:10 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 13:46:23 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Isaacs", "Or", ""], ["Shayer", "Oran", ""], ["Lindenbaum", "Michael", ""]]}, {"id": "1911.08566", "submitter": "Yu Yin", "authors": "Yu Yin, Joseph P. Robinson, Yulun Zhang and Yun Fu", "title": "Joint Super-Resolution and Alignment of Tiny Faces", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution (SR) and landmark localization of tiny faces are highly\ncorrelated tasks. On the one hand, landmark localization could obtain higher\naccuracy with faces of high-resolution (HR). On the other hand, face SR would\nbenefit from prior knowledge of facial attributes such as landmarks. Thus, we\npropose a joint alignment and SR network to simultaneously detect facial\nlandmarks and super-resolve tiny faces. More specifically, a shared deep\nencoder is applied to extract features for both tasks by leveraging\ncomplementary information. To exploit the representative power of the\nhierarchical encoder, intermediate layers of a shared feature extraction module\nare fused to form efficient feature representations. The fused features are\nthen fed to task-specific modules to detect landmarks and super-resolve face\nimages in parallel. Extensive experiments demonstrate that the proposed model\nsignificantly outperforms the state-of-the-art in both landmark localization\nand SR of faces. We show a large improvement for landmark localization of tiny\nfaces (i.e., 16*16). Furthermore, the proposed framework yields comparable\nresults for landmark localization on low-resolution (LR) faces (i.e., 64*64) to\nexisting methods on HR (i.e., 256*256). As for SR, the proposed method recovers\nsharper edges and more details from LR face images than other state-of-the-art\nmethods, which we demonstrate qualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 20:39:49 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Yin", "Yu", ""], ["Robinson", "Joseph P.", ""], ["Zhang", "Yulun", ""], ["Fu", "Yun", ""]]}, {"id": "1911.08568", "submitter": "Michael Diodato", "authors": "Michael Diodato, Yu Li, Antonia Lovjer, Minsu Yeom, Albert Song,\n  Yiyang Zeng, Abhay Khosla, Benedikt Schifferer, Manik Goyal, Iddo Drori", "title": "Accurate Trajectory Prediction for Autonomous Vehicles", "comments": "arXiv admin note: text overlap with arXiv:1910.10318,\n  arXiv:1910.10317", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting vehicle trajectories, angle and speed is important for safe and\ncomfortable driving. We demonstrate the best predicted angle, speed, and best\nperformance overall winning the top three places of the ICCV 2019 Learning to\nDrive challenge. Our key contributions are (i) a general neural network system\narchitecture which embeds and fuses together multiple inputs by encoding, and\ndecodes multiple outputs using neural networks, (ii) using pre-trained neural\nnetworks for augmenting the given input data with segmentation maps and\nsemantic information, and (iii) leveraging the form and distribution of the\nexpected output in the model.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 06:38:33 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Diodato", "Michael", ""], ["Li", "Yu", ""], ["Lovjer", "Antonia", ""], ["Yeom", "Minsu", ""], ["Song", "Albert", ""], ["Zeng", "Yiyang", ""], ["Khosla", "Abhay", ""], ["Schifferer", "Benedikt", ""], ["Goyal", "Manik", ""], ["Drori", "Iddo", ""]]}, {"id": "1911.08571", "submitter": "Adam Kortylewski", "authors": "Adam Kortylewski, Qing Liu, Huiyu Wang, Zhishuai Zhang, Alan Yuille", "title": "Localizing Occluders with Compositional Convolutional Networks", "comments": "Presented at \"NeurIPS 2019 workshop on Perception as generative\n  reasoning\" and \"NeurIPS 2019 workshop on Context and Compositionality in\n  Biological and Artificial Neural Systems\". arXiv admin note: text overlap\n  with arXiv:1905.11826", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositional convolutional networks are generative compositional models of\nneural network features, that achieve state of the art results when classifying\npartially occluded objects, even when they have not been exposed to occluded\nobjects during training. In this work, we study the performance of\nCompositionalNets at localizing occluders in images. We show that the original\nmodel is not able to localize occluders well. We propose to overcome this\nlimitation by modeling the feature activations as a mixture of von-Mises-Fisher\ndistributions, which also allows for an end-to-end training of\nCompositionalNets. Our experimental results demonstrate that the proposed\nextensions increase the model's performance at localizing occluders as well as\nat classifying partially occluded objects.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 13:49:04 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Kortylewski", "Adam", ""], ["Liu", "Qing", ""], ["Wang", "Huiyu", ""], ["Zhang", "Zhishuai", ""], ["Yuille", "Alan", ""]]}, {"id": "1911.08582", "submitter": "Jan Blumenkamp", "authors": "Jan Blumenkamp", "title": "End to end collision avoidance based on optical flow and neural networks", "comments": "Technical Report for project work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optical flow is believed to play an important role in the agile flight of\nbirds and insects. Even though it is a very simple concept, it is rarely used\nin computer vision for collision avoidance. This work implements a neural\nnetwork based collision avoidance which was deployed and evaluated on a solely\nfor this purpose refitted car.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 14:50:17 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Blumenkamp", "Jan", ""]]}, {"id": "1911.08588", "submitter": "Qilei Chen", "authors": "Qilei Chen, Xinzi Sun, Ning Zhang, Yu Cao, Benyuan Liu", "title": "Mini Lesions Detection on Diabetic Retinopathy Images via Large Scale\n  CNN Features", "comments": "diabetic retinopathy, mini lesion detection, FPN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic retinopathy (DR) is a diabetes complication that affects eyes. DR is\na primary cause of blindness in working-age people and it is estimated that 3\nto 4 million people with diabetes are blinded by DR every year worldwide. Early\ndiagnosis have been considered an effective way to mitigate such problem. The\nultimate goal of our research is to develop novel machine learning techniques\nto analyze the DR images generated by the fundus camera for automatically DR\ndiagnosis. In this paper, we focus on identifying small lesions on DR fundus\nimages. The results from our analysis, which include the lesion category and\ntheir exact locations in the image, can be used to facilitate the determination\nof DR severity (indicated by DR stages). Different from traditional object\ndetection for natural images, lesion detection for fundus images have unique\nchallenges. Specifically, the size of a lesion instance is usually very small,\ncompared with the original resolution of the fundus images, making them\ndiffcult to be detected. We analyze the lesion-vs-image scale carefully and\npropose a large-size feature pyramid network (LFPN) to preserve more image\ndetails for mini lesion instance detection. Our method includes an effective\nregion proposal strategy to increase the sensitivity. The experimental results\nshow that our proposed method is superior to the original feature pyramid\nnetwork (FPN) method and Faster RCNN.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 21:06:50 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Chen", "Qilei", ""], ["Sun", "Xinzi", ""], ["Zhang", "Ning", ""], ["Cao", "Yu", ""], ["Liu", "Benyuan", ""]]}, {"id": "1911.08591", "submitter": "Deepali Aneja", "authors": "Deepali Aneja, Alex Colburn, Gary Faigin, Linda Shapiro, and Barbara\n  Mones", "title": "Learning Stylized Character Expressions from Humans", "comments": "2017 IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR) Women in Computer Vision (WiCV) Workshop Honolulu, Hawaii, USA, July\n  21st - July 26th, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DeepExpr, a novel expression transfer system from humans to\nmultiple stylized characters via deep learning. We developed : 1) a data-driven\nperceptual model of facial expressions, 2) a novel stylized character data set\nwith cardinal expression annotations : FERG (Facial Expression Research Group)\n- DB (added two new characters), and 3) . We evaluated our method on a set of\nretrieval tasks on our collected stylized character dataset of expressions. We\nhave also shown that the ranking order predicted by the proposed features is\nhighly correlated with the ranking order provided by a facial expression expert\nand Mechanical Turk (MT) experiments.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 21:12:43 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Aneja", "Deepali", ""], ["Colburn", "Alex", ""], ["Faigin", "Gary", ""], ["Shapiro", "Linda", ""], ["Mones", "Barbara", ""]]}, {"id": "1911.08606", "submitter": "Luca Mocerino", "authors": "Luca Mocerino, Andrea Calimera", "title": "CoopNet: Cooperative Convolutional Neural Network for Low-Power MCUs", "comments": null, "journal-ref": "2019 26th IEEE International Conference on Electronics, Circuits\n  and Systems (ICECS)", "doi": "10.1109/ICECS46596.2019.8964993", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fixed-point quantization and binarization are two reduction methods adopted\nto deploy Convolutional Neural Networks (CNN) on end-nodes powered by low-power\nmicro-controller units (MCUs). While most of the existing works use them as\nstand-alone optimizations, this work aims at demonstrating there is margin for\na joint cooperation that leads to inferential engines with lower latency and\nhigher accuracy. Called CoopNet, the proposed heterogeneous model is conceived,\nimplemented and tested on off-the-shelf MCUs with small on-chip memory and few\ncomputational resources. Experimental results conducted on three different CNNs\nusing as test-bench the low-power RISC core of the Cortex-M family by ARM\nvalidate the CoopNet proposal by showing substantial improvements w.r.t.\ndesigns where quantization and binarization are applied separately.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 21:47:23 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 18:35:18 GMT"}, {"version": "v3", "created": "Sat, 25 Jan 2020 12:10:51 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Mocerino", "Luca", ""], ["Calimera", "Andrea", ""]]}, {"id": "1911.08609", "submitter": "Bing Xu", "authors": "Bing Xu, Andrew Tulloch, Yunpeng Chen, Xiaomeng Yang, Lin Qiao", "title": "Hybrid Composition with IdleBlock: More Efficient Networks for Image\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new building block, IdleBlock, which naturally prunes\nconnections within the block. To fully utilize the IdleBlock we break the\ntradition of monotonic design in state-of-the-art networks, and introducing\nhybrid composition with IdleBlock. We study hybrid composition on MobileNet v3\nand EfficientNet-B0, two of the most efficient networks. Without any neural\narchitecture search, the deeper \"MobileNet v3\" with hybrid composition design\nsurpasses possibly all state-of-the-art image recognition network designed by\nhuman experts or neural architecture search algorithms. Similarly, the\nhybridized EfficientNet-B0 networks are more efficient than previous\nstate-of-the-art networks with similar computation budgets. These results\nsuggest a new simpler and more efficient direction for network design and\nneural architecture search.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 22:09:11 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Xu", "Bing", ""], ["Tulloch", "Andrew", ""], ["Chen", "Yunpeng", ""], ["Yang", "Xiaomeng", ""], ["Qiao", "Lin", ""]]}, {"id": "1911.08616", "submitter": "Shashanka Venkataramanan", "authors": "Shashanka Venkataramanan, Kuan-Chuan Peng, Rajat Vikram Singh, Abhijit\n  Mahalanobis", "title": "Attention Guided Anomaly Localization in Images", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly localization is an important problem in computer vision which\ninvolves localizing anomalous regions within images with applications in\nindustrial inspection, surveillance, and medical imaging. This task is\nchallenging due to the small sample size and pixel coverage of the anomaly in\nreal-world scenarios. Most prior works need to use anomalous training images to\ncompute a class-specific threshold to localize anomalies. Without the need of\nanomalous training images, we propose Convolutional Adversarial Variational\nautoencoder with Guided Attention (CAVGA), which localizes the anomaly with a\nconvolutional latent variable to preserve the spatial information. In the\nunsupervised setting, we propose an attention expansion loss where we encourage\nCAVGA to focus on all normal regions in the image. Furthermore, in the\nweakly-supervised setting we propose a complementary guided attention loss,\nwhere we encourage the attention map to focus on all normal regions while\nminimizing the attention map corresponding to anomalous regions in the image.\nCAVGA outperforms the state-of-the-art (SOTA) anomaly localization methods on\nMVTec Anomaly Detection (MVTAD), modified ShanghaiTech Campus (mSTC) and\nLarge-scale Attention based Glaucoma (LAG) datasets in the unsupervised setting\nand when using only 2% anomalous images in the weakly-supervised setting. CAVGA\nalso outperforms SOTA anomaly detection methods on the MNIST, CIFAR-10,\nFashion-MNIST, MVTAD, mSTC and LAG datasets.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 22:28:17 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2020 03:04:50 GMT"}, {"version": "v3", "created": "Sat, 11 Jul 2020 02:13:01 GMT"}, {"version": "v4", "created": "Fri, 17 Jul 2020 02:38:04 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Venkataramanan", "Shashanka", ""], ["Peng", "Kuan-Chuan", ""], ["Singh", "Rajat Vikram", ""], ["Mahalanobis", "Abhijit", ""]]}, {"id": "1911.08618", "submitter": "Badri Narayana Patro", "authors": "Badri N. Patro, Anupriy and Vinay P. Namboodiri", "title": "Explanation vs Attention: A Two-Player Game to Obtain Attention for VQA", "comments": "AAAI-2020(Accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we aim to obtain improved attention for a visual question\nanswering (VQA) task. It is challenging to provide supervision for attention.\nAn observation we make is that visual explanations as obtained through class\nactivation mappings (specifically Grad-CAM) that are meant to explain the\nperformance of various networks could form a means of supervision. However, as\nthe distributions of attention maps and that of Grad-CAMs differ, it would not\nbe suitable to directly use these as a form of supervision. Rather, we propose\nthe use of a discriminator that aims to distinguish samples of visual\nexplanation and attention maps. The use of adversarial training of the\nattention regions as a two-player game between attention and explanation serves\nto bring the distributions of attention maps and visual explanations closer.\nSignificantly, we observe that providing such a means of supervision also\nresults in attention maps that are more closely related to human attention\nresulting in a substantial improvement over baseline stacked attention network\n(SAN) models. It also results in a good improvement in rank correlation metric\non the VQA task. This method can also be combined with recent MCB based methods\nand results in consistent improvement. We also provide comparisons with other\nmeans for learning distributions such as based on Correlation Alignment\n(Coral), Maximum Mean Discrepancy (MMD) and Mean Square Error (MSE) losses and\nobserve that the adversarial loss outperforms the other forms of learning the\nattention maps. Visualization of the results also confirms our hypothesis that\nattention maps improve using this form of supervision.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 22:30:13 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Patro", "Badri N.", ""], ["Anupriy", "", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "1911.08621", "submitter": "William Thong", "authors": "William Thong, Pascal Mettes, Cees G.M. Snoek", "title": "Open Cross-Domain Visual Search", "comments": "Accepted at Computer Vision and Image Understanding (CVIU)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses cross-domain visual search, where visual queries\nretrieve category samples from a different domain. For example, we may want to\nsketch an airplane and retrieve photographs of airplanes. Despite considerable\nprogress, the search occurs in a closed setting between two pre-defined\ndomains. In this paper, we make the step towards an open setting where multiple\nvisual domains are available. This notably translates into a search between any\npair of domains, from a combination of domains or within multiple domains. We\nintroduce a simple -- yet effective -- approach. We formulate the search as a\nmapping from every visual domain to a common semantic space, where categories\nare represented by hyperspherical prototypes. Open cross-domain visual search\nis then performed by searching in the common semantic space, regardless of\nwhich domains are used as source or target. Domains are combined in the common\nspace to search from or within multiple domains simultaneously. A separate\ntraining of every domain-specific mapping function enables an efficient scaling\nto any number of domains without affecting the search performance. We\nempirically illustrate our capability to perform open cross-domain visual\nsearch in three different scenarios. Our approach is competitive with respect\nto existing closed settings, where we obtain state-of-the-art results on\nseveral benchmarks for three sketch-based search tasks.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 22:42:01 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 13:36:23 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Thong", "William", ""], ["Mettes", "Pascal", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "1911.08630", "submitter": "Rahul Duggal", "authors": "Rahul Duggal, Cao Xiao, Richard Vuduc, Jimeng Sun", "title": "CUP: Cluster Pruning for Compressing Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Cluster Pruning (CUP) for compressing and accelerating deep neural\nnetworks. Our approach prunes similar filters by clustering them based on\nfeatures derived from both the incoming and outgoing weight connections. With\nCUP, we overcome two limitations of prior work-(1) non-uniform pruning: CUP can\nefficiently determine the ideal number of filters to prune in each layer of a\nneural network. This is in contrast to prior methods that either prune all\nlayers uniformly or otherwise use resource-intensive methods such as manual\nsensitivity analysis or reinforcement learning to determine the ideal number.\n(2) Single-shot operation: We extend CUP to CUP-SS (for CUP single shot)\nwhereby pruning is integrated into the initial training phase itself. This\nleads to large savings in training time compared to traditional pruning\npipelines. Through extensive evaluation on multiple datasets (MNIST, CIFAR-10,\nand Imagenet) and models(VGG-16, Resnets-18/34/56) we show that CUP outperforms\nrecent state of the art. Specifically, CUP-SS achieves 2.2x flops reduction for\na Resnet-50 model trained on Imagenet while staying within 0.9% top-5 accuracy.\nIt saves over 14 hours in training time with respect to the original Resnet-50.\nThe code to reproduce results is available.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 23:44:59 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Duggal", "Rahul", ""], ["Xiao", "Cao", ""], ["Vuduc", "Richard", ""], ["Sun", "Jimeng", ""]]}, {"id": "1911.08651", "submitter": "Cheng Yan", "authors": "Cheng Yan, Guansong Pang, Xiao Bai, Chunhua Shen", "title": "Unified Multifaceted Feature Learning for Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (ReID) aims at re-identifying persons from different\nviewpoints across multiple cameras, of which it is of great importance to learn\nmultifaceted features expressed in different parts of a person, e.g., clothes,\nbags, and other accessories in the main body, appearance in the head, and shoes\nin the foot. To learn such features, existing methods are focused on the\nstriping-based approach that builds multi-branch neural networks to learn local\nfeatures in each part of the identities, with one-branch network dedicated to\none part. This results in complex models with a large number of parameters. To\naddress this issue, this paper proposes to learn the multifaceted features in a\nsimple unified single-branch neural network. The Unified Multifaceted Feature\nLearning (UMFL) framework is introduced to fulfill this goal, which consists of\ntwo key collaborative modules: compound batch image erasing (including batch\nconstant erasing and random erasing) and hierarchical structured loss. The loss\nstructures the augmented images resulted by the two types of image erasing in a\ntwo-level hierarchy and enforces multifaceted attention to different parts. As\nwe show in the extensive experimental results on four benchmark person ReID\ndatasets, despite the use of significantly simplified network structure, our\nmethod performs substantially better than state-of-the-art competing methods.\nOur method can also effectively generalize to vehicle ReID, achieving similar\nimprovement on two vehicle ReID datasets.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 00:56:16 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 22:18:25 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Yan", "Cheng", ""], ["Pang", "Guansong", ""], ["Bai", "Xiao", ""], ["Shen", "Chunhua", ""]]}, {"id": "1911.08656", "submitter": "Kwang-Hyun Uhm", "authors": "Kwang-Hyun Uhm, Seung-Wook Kim, Seo-Won Ji, Sung-Jin Cho, Jun-Pyo\n  Hong, Sung-Jea Ko", "title": "W-Net: Two-stage U-Net with misaligned data for raw-to-RGB mapping", "comments": "Accepted by ICCVW 2019", "journal-ref": null, "doi": "10.1109/ICCVW.2019.00448", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on learning a mapping between raw Bayer images and RGB images\nhas progressed with the development of deep convolutional neural networks. A\nchallenging data set namely the Zurich Raw-to-RGB data set (ZRR) has been\nreleased in the AIM 2019 raw-to-RGB mapping challenge. In ZRR, input raw and\ntarget RGB images are captured by two different cameras and thus not perfectly\naligned. Moreover, camera metadata such as white balance gains and color\ncorrection matrix are not provided, which makes the challenge more difficult.\nIn this paper, we explore an effective network structure and a loss function to\naddress these issues. We exploit a two-stage U-Net architecture and also\nintroduce a loss function that is less variant to alignment and more sensitive\nto color differences. In addition, we show an ensemble of networks trained with\ndifferent loss functions can bring a significant performance gain. We\ndemonstrate the superiority of our method by achieving the highest score in\nterms of both the peak signal-to-noise ratio and the structural similarity and\nobtaining the second-best mean-opinion-score in the challenge.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 01:17:41 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 02:05:47 GMT"}, {"version": "v3", "created": "Fri, 22 Nov 2019 02:59:47 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Uhm", "Kwang-Hyun", ""], ["Kim", "Seung-Wook", ""], ["Ji", "Seo-Won", ""], ["Cho", "Sung-Jin", ""], ["Hong", "Jun-Pyo", ""], ["Ko", "Sung-Jea", ""]]}, {"id": "1911.08670", "submitter": "Hamid Reza Vaezi Joze", "authors": "Hamid Reza Vaezi Joze, Amirreza Shaban, Michael L. Iuzzolino and\n  Kazuhito Koishida", "title": "MMTM: Multimodal Transfer Module for CNN Fusion", "comments": null, "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In late fusion, each modality is processed in a separate unimodal\nConvolutional Neural Network (CNN) stream and the scores of each modality are\nfused at the end. Due to its simplicity late fusion is still the predominant\napproach in many state-of-the-art multimodal applications. In this paper, we\npresent a simple neural network module for leveraging the knowledge from\nmultiple modalities in convolutional neural networks. The propose unit, named\nMultimodal Transfer Module (MMTM), can be added at different levels of the\nfeature hierarchy, enabling slow modality fusion. Using squeeze and excitation\noperations, MMTM utilizes the knowledge of multiple modalities to recalibrate\nthe channel-wise features in each CNN stream. Despite other intermediate fusion\nmethods, the proposed module could be used for feature modality fusion in\nconvolution layers with different spatial dimensions. Another advantage of the\nproposed method is that it could be added among unimodal branches with minimum\nchanges in the their network architectures, allowing each branch to be\ninitialized with existing pretrained weights. Experimental results show that\nour framework improves the recognition accuracy of well-known multimodal\nnetworks. We demonstrate state-of-the-art or competitive performance on four\ndatasets that span the task domains of dynamic hand gesture recognition, speech\nenhancement, and action recognition with RGB and body joints.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 02:32:16 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 22:40:45 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Joze", "Hamid Reza Vaezi", ""], ["Shaban", "Amirreza", ""], ["Iuzzolino", "Michael L.", ""], ["Koishida", "Kazuhito", ""]]}, {"id": "1911.08680", "submitter": "Zhao Zhang", "authors": "Yulin Sun, Zhao Zhang, Weiming Jiang, Zheng Zhang, Li Zhang, Shuicheng\n  Yan and Meng Wang", "title": "Discriminative Local Sparse Representation by Robust Adaptive Dictionary\n  Pair Learning", "comments": "Accepted by IEEE TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a structured Robust Adaptive Dic-tionary Pair\nLearning (RA-DPL) framework for the discrim-inative sparse representation\nlearning. To achieve powerful representation ability of the available samples,\nthe setting of RA-DPL seamlessly integrates the robust projective dictionary\npair learning, locality-adaptive sparse representations and discriminative\ncoding coefficients learning into a unified learning framework. Specifically,\nRA-DPL improves existing projective dictionary pair learning in four\nperspectives. First, it applies a sparse l2,1-norm based metric to encode the\nrecon-struction error to deliver the robust projective dictionary pairs, and\nthe l2,1-norm has the potential to minimize the error. Sec-ond, it imposes the\nrobust l2,1-norm clearly on the analysis dictionary to ensure the sparse\nproperty of the coding coeffi-cients rather than using the costly l0/l1-norm.\nAs such, the robustness of the data representation and the efficiency of the\nlearning process are jointly considered to guarantee the effi-cacy of our\nRA-DPL. Third, RA-DPL conceives a structured reconstruction weight learning\nparadigm to preserve the local structures of the coding coefficients within\neach class clearly in an adaptive manner, which encourages to produce the\nlocality preserving representations. Fourth, it also considers improving the\ndiscriminating ability of coding coefficients and dictionary by incorporating a\ndiscriminating function, which can ensure high intra-class compactness and\ninter-class separation in the code space. Extensive experiments show that our\nRA-DPL can obtain superior performance over other state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 03:13:49 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Sun", "Yulin", ""], ["Zhang", "Zhao", ""], ["Jiang", "Weiming", ""], ["Zhang", "Zheng", ""], ["Zhang", "Li", ""], ["Yan", "Shuicheng", ""], ["Wang", "Meng", ""]]}, {"id": "1911.08688", "submitter": "Sheng Jin", "authors": "Sheng Jin, Shangchen Zhou, Yao Liu, Chao Chen, Xiaoshuai Sun, Hongxun\n  Yao, Xiansheng Hua", "title": "SSAH: Semi-supervised Adversarial Deep Hashing with Self-paced Hard\n  Sample Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep hashing methods have been proved to be effective and efficient for\nlarge-scale Web media search. The success of these data-driven methods largely\ndepends on collecting sufficient labeled data, which is usually a crucial\nlimitation in practical cases. The current solutions to this issue utilize\nGenerative Adversarial Network (GAN) to augment data in semi-supervised\nlearning. However, existing GAN-based methods treat image generations and\nhashing learning as two isolated processes, leading to generation\nineffectiveness. Besides, most works fail to exploit the semantic information\nin unlabeled data. In this paper, we propose a novel Semi-supervised Self-pace\nAdversarial Hashing method, named SSAH to solve the above problems in a unified\nframework. The SSAH method consists of an adversarial network (A-Net) and a\nhashing network (H-Net). To improve the quality of generative images, first,\nthe A-Net learns hard samples with multi-scale occlusions and multi-angle\nrotated deformations which compete against the learning of accurate hashing\ncodes. Second, we design a novel self-paced hard generation policy to gradually\nincrease the hashing difficulty of generated samples. To make use of the\nsemantic information in unlabeled ones, we propose a semi-supervised consistent\nloss. The experimental results show that our method can significantly improve\nstate-of-the-art models on both the widely-used hashing datasets and\nfine-grained datasets.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 03:45:34 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Jin", "Sheng", ""], ["Zhou", "Shangchen", ""], ["Liu", "Yao", ""], ["Chen", "Chao", ""], ["Sun", "Xiaoshuai", ""], ["Yao", "Hongxun", ""], ["Hua", "Xiansheng", ""]]}, {"id": "1911.08691", "submitter": "Xiaolong Hu", "authors": "Xiaolong Hu, Zhulin An, Chuanguang Yang, Hui Zhu, Kaiqaing Xu, and\n  Yongjun Xu", "title": "DRNet: Dissect and Reconstruct the Convolutional Neural Network via\n  Interpretable Manners", "comments": "ECAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (ConvNets) are widely used in real life. People\nusually use ConvNets which pre-trained on a fixed number of classes. However,\nfor different application scenarios, we usually do not need all of the classes,\nwhich means ConvNets are redundant when dealing with these tasks. This paper\nfocuses on the redundancy of ConvNet channels. We proposed a novel idea: using\nan interpretable manner to find the most important channels for every single\nclass (dissect), and dynamically run channels according to classes in need\n(reconstruct). For VGG16 pre-trained on CIFAR-10, we only run 11\\% parameters\nfor two-classes sub-tasks on average with negligible accuracy loss. For VGG16\npre-trained on ImageNet, our method averagely gains 14.29\\% accuracy promotion\nfor two-classes sub-tasks. In addition, analysis show that our method captures\nsome semantic meanings of channels, and uses the context information more\ntargeted for sub-tasks of ConvNets.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 03:52:28 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 07:41:37 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Hu", "Xiaolong", ""], ["An", "Zhulin", ""], ["Yang", "Chuanguang", ""], ["Zhu", "Hui", ""], ["Xu", "Kaiqaing", ""], ["Xu", "Yongjun", ""]]}, {"id": "1911.08705", "submitter": "Xin He", "authors": "Xin He, Shihao Wang, Shaohuai Shi, Zhenheng Tang, Yuxin Wang, Zhihao\n  Zhao, Jing Dai, Ronghao Ni, Xiaofeng Zhang, Xiaoming Liu, Zhili Wu, Wu Yu,\n  Xiaowen Chu", "title": "Computer-Aided Clinical Skin Disease Diagnosis Using CNN and Object\n  Detection Models", "comments": "KDDBHI Workshop 2019, IEEE BigData Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin disease is one of the most common types of human diseases, which may\nhappen to everyone regardless of age, gender or race. Due to the high visual\ndiversity, human diagnosis highly relies on personal experience; and there is a\nserious shortage of experienced dermatologists in many countries. To alleviate\nthis problem, computer-aided diagnosis with state-of-the-art (SOTA) machine\nlearning techniques would be a promising solution. In this paper, we aim at\nunderstanding the performance of convolutional neural network (CNN) based\napproaches. We first build two versions of skin disease datasets from Internet\nimages: (a) Skin-10, which contains 10 common classes of skin disease with a\ntotal of 10,218 images; (b) Skin-100, which is a larger dataset that consists\nof 19,807 images of 100 skin disease classes. Based on these datasets, we\nbenchmark several SOTA CNN models and show that the accuracy of skin-100 is\nmuch lower than the accuracy of skin-10. We then implement an ensemble method\nbased on several CNN models and achieve the best accuracy of 79.01\\% for\nSkin-10 and 53.54\\% for Skin-100. We also present an object detection based\napproach by introducing bounding boxes into the Skin-10 dataset. Our results\nshow that object detection can help improve the accuracy of some skin disease\nclasses.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 04:53:18 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["He", "Xin", ""], ["Wang", "Shihao", ""], ["Shi", "Shaohuai", ""], ["Tang", "Zhenheng", ""], ["Wang", "Yuxin", ""], ["Zhao", "Zhihao", ""], ["Dai", "Jing", ""], ["Ni", "Ronghao", ""], ["Zhang", "Xiaofeng", ""], ["Liu", "Xiaoming", ""], ["Wu", "Zhili", ""], ["Yu", "Wu", ""], ["Chu", "Xiaowen", ""]]}, {"id": "1911.08708", "submitter": "Uttaran Bhattacharya", "authors": "Uttaran Bhattacharya, Christian Roncal, Trisha Mittal, Rohan Chandra,\n  Kyra Kapsaskis, Kurt Gray, Aniket Bera, Dinesh Manocha", "title": "Take an Emotion Walk: Perceiving Emotions from Gaits Using Hierarchical\n  Attention Pooling and Affective Mapping", "comments": "In proceedings of the 16th European Conference on Computer Vision,\n  2020. Total pages 18. Total figures 5. Total tables 3", "journal-ref": "European Conference on Computer Vision, 2020, Lecture Notes in\n  Computer Science, Vol. 12355, PP 145-163", "doi": "10.1007/978-3-030-58607-2_9", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an autoencoder-based semi-supervised approach to classify\nperceived human emotions from walking styles obtained from videos or\nmotion-captured data and represented as sequences of 3D poses. Given the motion\non each joint in the pose at each time step extracted from 3D pose sequences,\nwe hierarchically pool these joint motions in a bottom-up manner in the\nencoder, following the kinematic chains in the human body. We also constrain\nthe latent embeddings of the encoder to contain the space of\npsychologically-motivated affective features underlying the gaits. We train the\ndecoder to reconstruct the motions per joint per time step in a top-down manner\nfrom the latent embeddings. For the annotated data, we also train a classifier\nto map the latent embeddings to emotion labels. Our semi-supervised approach\nachieves a mean average precision of 0.84 on the Emotion-Gait benchmark\ndataset, which contains both labeled and unlabeled gaits collected from\nmultiple sources. We outperform current state-of-art algorithms for both\nemotion recognition and action recognition from 3D gaits by 7%--23% on the\nabsolute. More importantly, we improve the average precision by 10%--50% on the\nabsolute on classes that each makes up less than 25% of the labeled part of the\nEmotion-Gait benchmark dataset.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 05:04:16 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 00:24:06 GMT"}, {"version": "v3", "created": "Sun, 14 Feb 2021 02:19:49 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Bhattacharya", "Uttaran", ""], ["Roncal", "Christian", ""], ["Mittal", "Trisha", ""], ["Chandra", "Rohan", ""], ["Kapsaskis", "Kyra", ""], ["Gray", "Kurt", ""], ["Bera", "Aniket", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1911.08711", "submitter": "Chih-Chung Hsu", "authors": "Chih-Chung Hsu and Chia-Hsiang Lin", "title": "Dual Reconstruction with Densely Connected Residual Network for Single\n  Image Super-Resolution", "comments": "Accepted to ICCV Workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based single image super-resolution enables very fast and\nhigh-visual-quality reconstruction. Recently, an enhanced super-resolution\nbased on generative adversarial network (ESRGAN) has achieved excellent\nperformance in terms of both qualitative and quantitative quality of the\nreconstructed high-resolution image. In this paper, we propose to add one more\nshortcut between two dense-blocks, as well as add shortcut between two\nconvolution layers inside a dense-block. With this simple strategy of adding\nmore shortcuts in the proposed network, it enables a faster learning process as\nthe gradient information can be back-propagated more easily. Based on the\nimproved ESRGAN, the dual reconstruction is proposed to learn different aspects\nof the super-resolved image for judiciously enhancing the quality of the\nreconstructed image. In practice, the super-resolution model is pre-trained\nsolely based on pixel distance, followed by fine-tuning the parameters in the\nmodel based on adversarial loss and perceptual loss. Finally, we fuse two\ndifferent models by weighted-summing their parameters to obtain the final\nsuper-resolution model. Experimental results demonstrated that the proposed\nmethod achieves excellent performance in the real-world image super-resolution\nchallenge. We have also verified that the proposed dual reconstruction does\nfurther improve the quality of the reconstructed image in terms of both PSNR\nand SSIM.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 05:24:00 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Hsu", "Chih-Chung", ""], ["Lin", "Chia-Hsiang", ""]]}, {"id": "1911.08712", "submitter": "Aming Wu", "authors": "Aming Wu, Yahong Han, Linchao Zhu, Yi Yang", "title": "Instance-Invariant Domain Adaptive Object Detection via Progressive\n  Disentanglement", "comments": "Accepted in T-PAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state-of-the-art methods of object detection suffer from poor\ngeneralization ability when the training and test data are from different\ndomains, e.g., with different styles. To address this problem, previous methods\nmainly use holistic representations to align feature-level and pixel-level\ndistributions of different domains, which may neglect the instance-level\ncharacteristics of objects in images. Besides, when transferring detection\nability across different domains, it is important to obtain the instance-level\nfeatures that are domain-invariant, instead of the styles that are\ndomain-specific. Therefore, in order to extract instance-invariant features, we\nshould disentangle the domain-invariant features from the domain-specific\nfeatures. To this end, a progressive disentangled framework is first proposed\nto solve domain adaptive object detection. Particularly, base on disentangled\nlearning used for feature decomposition, we devise two disentangled layers to\ndecompose domain-invariant and domain-specific features. And the\ninstance-invariant features are extracted based on the domain-invariant\nfeatures. Finally, to enhance the disentanglement, a three-stage training\nmechanism including multiple loss functions is devised to optimize our model.\nIn the experiment, we verify the effectiveness of our method on three\ndomain-shift scenes. Our method is separately 2.3\\%, 3.6\\%, and 4.0\\% higher\nthan the baseline method \\cite{saito2019strong}.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 05:24:15 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 14:13:05 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2020 04:17:06 GMT"}, {"version": "v4", "created": "Sat, 13 Feb 2021 17:01:17 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Wu", "Aming", ""], ["Han", "Yahong", ""], ["Zhu", "Linchao", ""], ["Yang", "Yi", ""]]}, {"id": "1911.08715", "submitter": "Fatmatulzehra Uslu Ms.", "authors": "Fatmatulzehra Uslu", "title": "An Inception Inspired Deep Network to Analyse Fundus Images", "comments": "5 pages, 5 figures, accepted at ELECO 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundus image usually contains the optic disc, pathologies and other\nstructures in addition to vessels to be segmented. This study proposes a deep\nnetwork for vessel segmentation, whose architecture is inspired by inception\nmodules. The network contains three sub-networks, each with a different filter\nsize, which are connected in the last layer of the proposed network. According\nto experiments conducted in the DRIVE and IOSTAR, the performance of our\nnetwork is found to be better than or comparable to that of the previous\nmethods. We also observe that the sub-networks pay attention to different parts\nof an input image when producing an output map in the last layer of the\nproposed network; though, training of the proposed network is not constrained\nfor this purpose.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 05:48:05 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Uslu", "Fatmatulzehra", ""]]}, {"id": "1911.08716", "submitter": "Yuan Liu", "authors": "Amirata Ghorbani, Vivek Natarajan, David Coz, Yuan Liu", "title": "DermGAN: Synthetic Generation of Clinical Skin Images with Pathology", "comments": "In full proceedings of NeurIPS ML4H workshop, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent success in applying supervised deep learning to medical\nimaging tasks, the problem of obtaining large and diverse expert-annotated\ndatasets required for the development of high performant models remains\nparticularly challenging. In this work, we explore the possibility of using\nGenerative Adverserial Networks (GAN) to synthesize clinical images with skin\ncondition. We propose DermGAN, an adaptation of the popular Pix2Pix\narchitecture, to create synthetic images for a pre-specified skin condition\nwhile being able to vary its size, location and the underlying skin color. We\ndemonstrate that the generated images are of high fidelity using objective GAN\nevaluation metrics. In a Human Turing test, we note that the synthetic images\nare not only visually similar to real images, but also embody the respective\nskin condition in dermatologists' eyes. Finally, when using the synthetic\nimages as a data augmentation technique for training a skin condition\nclassifier, we observe that the model performs comparably to the baseline model\noverall while improving on rare but malignant conditions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 05:48:16 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Ghorbani", "Amirata", ""], ["Natarajan", "Vivek", ""], ["Coz", "David", ""], ["Liu", "Yuan", ""]]}, {"id": "1911.08718", "submitter": "Xiaodong Cun", "authors": "Xiaodong Cun, Chi-Man Pun, Cheng Shi", "title": "Towards Ghost-free Shadow Removal via Dual Hierarchical Aggregation\n  Network and Shadow Matting GAN", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shadow removal is an essential task for scene understanding. Many studies\nconsider only matching the image contents, which often causes two types of\nghosts: color in-consistencies in shadow regions or artifacts on shadow\nboundaries. In this paper, we tackle these issues in two ways. First, to\ncarefully learn the border artifacts-free image, we propose a novel network\nstructure named the dual hierarchically aggregation network~(DHAN). It contains\na series of growth dilated convolutions as the backbone without any\ndown-samplings, and we hierarchically aggregate multi-context features for\nattention and prediction, respectively. Second, we argue that training on a\nlimited dataset restricts the textural understanding of the network, which\nleads to the shadow region color in-consistencies. Currently, the largest\ndataset contains 2k+ shadow/shadow-free image pairs. However, it has only 0.1k+\nunique scenes since many samples share exactly the same background with\ndifferent shadow positions. Thus, we design a shadow matting generative\nadversarial network~(SMGAN) to synthesize realistic shadow mattings from a\ngiven shadow mask and shadow-free image. With the help of novel masks or\nscenes, we enhance the current datasets using synthesized shadow images.\nExperiments show that our DHAN can erase the shadows and produce high-quality\nghost-free images. After training on the synthesized and real datasets, our\nnetwork outperforms other state-of-the-art methods by a large margin. The code\nis available: http://github.com/vinthony/ghost-free-shadow-removal/\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 05:52:14 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 04:59:03 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Cun", "Xiaodong", ""], ["Pun", "Chi-Man", ""], ["Shi", "Cheng", ""]]}, {"id": "1911.08724", "submitter": "Shunta Maeda", "authors": "Shunta Maeda", "title": "Fast and Flexible Image Blind Denoising via Competition of Experts", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and flexible processing are two essential requirements for a number of\npractical applications of image denoising. Current state-of-the-art methods,\nhowever, still require either high computational cost or limited scopes of the\ntarget. We introduce an efficient ensemble network trained via a competition of\nexpert networks, as an application for image blind denoising. We realize\nautomatic division of unlabeled noisy datasets into clusters respectively\noptimized to enhance denoising performance. The architecture is scalable, can\nbe extended to deal with diverse noise sources/levels without increasing the\ncomputation time. Taking advantage of this method, we save up to approximately\n90% of computational cost without sacrifice of the denoising performance\ncompared to single network models with identical architectures. We also compare\nthe proposed method with several existing algorithms and observe significant\noutperformance over prior arts in terms of computational efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 06:13:23 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Maeda", "Shunta", ""]]}, {"id": "1911.08730", "submitter": "Saeed Afshar", "authors": "Saeed Afshar, Andrew P Nicholson, Andre van Schaik, Gregory Cohen", "title": "Event-based Object Detection and Tracking for Space Situational\n  Awareness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we present optical space imaging using an unconventional yet\npromising class of imaging devices known as neuromorphic event-based sensors.\nThese devices, which are modeled on the human retina, do not operate with\nframes, but rather generate asynchronous streams of events in response to\nchanges in log-illumination at each pixel. These devices are therefore\nextremely fast, do not have fixed exposure times, allow for imaging whilst the\ndevice is moving and enable low power space imaging during daytime as well as\nnight without modification of the sensors. Recorded at multiple remote sites,\nwe present the first event-based space imaging dataset including recordings\nfrom multiple event-based sensors from multiple providers, greatly lowering the\nbarrier to entry for other researchers given the scarcity of such sensors and\nthe expertise required to operate them. The dataset contains 236 separate\nrecordings and 572 labeled resident space objects. The event-based imaging\nparadigm presents unique opportunities and challenges motivating the\ndevelopment of specialized event-based algorithms that can perform tasks such\nas detection and tracking in an event-based manner. Here we examine a range of\nsuch event-based algorithms for detection and tracking. The presented methods\nare designed specifically for space situational awareness applications and are\nevaluated in terms of accuracy and speed and suitability for implementation in\nneuromorphic hardware on remote or space-based imaging platforms.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 06:38:31 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Afshar", "Saeed", ""], ["Nicholson", "Andrew P", ""], ["van Schaik", "Andre", ""], ["Cohen", "Gregory", ""]]}, {"id": "1911.08736", "submitter": "Hamid Tizhoosh", "authors": "Shivam Kalra, H.R. Tizhoosh, Sultaan Shah, Charles Choi, Savvas\n  Damaskinos, Amir Safarpoor, Sobhan Shafiei, Morteza Babaie, Phedias\n  Diamandis, Clinton JV Campbell, and Liron Pantanowitz", "title": "Pan-Cancer Diagnostic Consensus Through Searching Archival\n  Histopathology Images Using Artificial Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of digital pathology has opened new horizons for histopathology\nand cytology. Artificial-intelligence algorithms are able to operate on\ndigitized slides to assist pathologists with diagnostic tasks. Whereas machine\nlearning involving classification and segmentation methods have obvious\nbenefits for image analysis in pathology, image search represents a fundamental\nshift in computational pathology. Matching the pathology of new patients with\nalready diagnosed and curated cases offers pathologist a novel approach to\nimprove diagnostic accuracy through visual inspection of similar cases and\ncomputational majority vote for consensus building. In this study, we report\nthe results from searching the largest public repository (The Cancer Genome\nAtlas [TCGA] program by National Cancer Institute, USA) of whole slide images\nfrom almost 11,000 patients depicting different types of malignancies. For the\nfirst time, we successfully indexed and searched almost 30,000 high-resolution\ndigitized slides constituting 16 terabytes of data comprised of 20 million\n1000x1000 pixels image patches. The TCGA image database covers 25 anatomic\nsites and contains 32 cancer subtypes. High-performance storage and GPU power\nwere employed for experimentation. The results were assessed with conservative\n\"majority voting\" to build consensus for subtype diagnosis through vertical\nsearch and demonstrated high accuracy values for both frozen sections slides\n(e.g., bladder urothelial carcinoma 93%, kidney renal clear cell carcinoma 97%,\nand ovarian serous cystadenocarcinoma 99%) and permanent histopathology slides\n(e.g., prostate adenocarcinoma 98%, skin cutaneous melanoma 99%, and thymoma\n100%). The key finding of this validation study was that computational\nconsensus appears to be possible for rendering diagnoses if a sufficiently\nlarge number of searchable cases are available for each cancer subtype.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 06:53:07 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Kalra", "Shivam", ""], ["Tizhoosh", "H. R.", ""], ["Shah", "Sultaan", ""], ["Choi", "Charles", ""], ["Damaskinos", "Savvas", ""], ["Safarpoor", "Amir", ""], ["Shafiei", "Sobhan", ""], ["Babaie", "Morteza", ""], ["Diamandis", "Phedias", ""], ["Campbell", "Clinton JV", ""], ["Pantanowitz", "Liron", ""]]}, {"id": "1911.08739", "submitter": "Nikhil Thakurdesai", "authors": "Nikhil Thakurdesai, Anupam Tripathi, Dheeraj Butani, Smita Sankhe", "title": "Vision: A Deep Learning Approach to provide walking assistance to the\n  visually impaired", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind people face a lot of problems in their daily routines. They have to\nstruggle a lot just to do their day-to-day chores. In this paper, we have\nproposed a system with the objective to help the visually impaired by providing\naudio aid guiding them to avoid obstacles, which will assist them to move in\ntheir surroundings. Object Detection using YOLO will help them detect the\nnearby objects and Depth Estimation using monocular vision will tell the\napproximate distance of the detected objects from the user. Despite a higher\naccuracy, stereo vision has many hardware constraints, which makes monocular\nvision the preferred choice for this application.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 07:02:00 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Thakurdesai", "Nikhil", ""], ["Tripathi", "Anupam", ""], ["Butani", "Dheeraj", ""], ["Sankhe", "Smita", ""]]}, {"id": "1911.08748", "submitter": "Hamid Tizhoosh", "authors": "S. Kalra, C. Choi, S. Shah, L. Pantanowitz, H.R. Tizhoosh", "title": "Yottixel -- An Image Search Engine for Large Archives of Histopathology\n  Whole Slide Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of digital pathology, searching for similar images in\nlarge archives has gained considerable attention. Image retrieval can provide\npathologists with unprecedented access to the evidence embodied in already\ndiagnosed and treated cases from the past. This paper proposes a search engine\nspecialized for digital pathology, called Yottixel, a portmanteau for \"one\nyotta pixel,\" alluding to the big-data nature of histopathology images. The\nmost impressive characteristic of Yottixel is its ability to represent whole\nslide images (WSIs) in a compact manner. Yottixel can perform millions of\nsearches in real-time with a high search accuracy and low storage profile.\nYottixel uses an intelligent indexing algorithm capable of representing WSIs\nwith a mosaic of patches by converting them into a small number of methodically\nextracted barcodes, called \"Bunch of Barcodes\" (BoB), the most prominent\nperformance enabler of Yottixel. The performance of the prototype platform is\nqualitatively tested using 300 WSIs from the University of Pittsburgh Medical\nCenter (UPMC) and 2,020 WSIs from The Cancer Genome Atlas Program (TCGA)\nprovided by the National Cancer Institute. Both datasets amount to more than\n4,000,000 patches of 1000x1000 pixels. We report three sets of experiments that\nshow that Yottixel can accurately retrieve organs and malignancies, and its\nsemantic ordering shows good agreement with the subjective evaluation of human\nobservers.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 07:34:49 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Kalra", "S.", ""], ["Choi", "C.", ""], ["Shah", "S.", ""], ["Pantanowitz", "L.", ""], ["Tizhoosh", "H. R.", ""]]}, {"id": "1911.08764", "submitter": "Matteo Testa", "authors": "Matteo Testa, Arslan Ali, Tiziano Bianchi, Enrico Magli", "title": "Learning mappings onto regularized latent spaces for biometric\n  authentication", "comments": "Accepted at IEEE MMSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel architecture for generic biometric authentication based on\ndeep neural networks: RegNet. Differently from other methods, RegNet learns a\nmapping of the input biometric traits onto a target distribution in a\nwell-behaved space in which users can be separated by means of simple and\ntunable boundaries. More specifically, authorized and unauthorized users are\nmapped onto two different and well behaved Gaussian distributions. The novel\napproach of learning the mapping instead of the boundaries further avoids the\nproblem encountered in typical classifiers for which the learnt boundaries may\nbe complex and difficult to analyze. RegNet achieves high performance in terms\nof security metrics such as Equal Error Rate (EER), False Acceptance Rate (FAR)\nand Genuine Acceptance Rate (GAR). The experiments we conducted on publicly\navailable datasets of face and fingerprint confirm the effectiveness of the\nproposed system.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 08:40:44 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Testa", "Matteo", ""], ["Ali", "Arslan", ""], ["Bianchi", "Tiziano", ""], ["Magli", "Enrico", ""]]}, {"id": "1911.08769", "submitter": "Md. Aminur Rab Ratul", "authors": "Syeda Noor Jaha Azim, Md. Aminur Rab Ratul", "title": "Inspect Transfer Learning Architecture with Dilated Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many award-winning pre-trained Convolutional Neural Network (CNN),\nwhich have a common phenomenon of increasing depth in convolutional layers.\nHowever, I inspect on VGG network, which is one of the famous model submitted\nto ILSVRC-2014, to show that slight modification in the basic architecture can\nenhance the accuracy result of the image classification task. In this paper, We\npresent two improve architectures of pre-trained VGG-16 and VGG-19 networks\nthat apply transfer learning when trained on a different dataset. I report a\nseries of experimental result on various modification of the primary VGG\nnetworks and achieved significant out-performance on image classification task\nby: (1) freezing the first two blocks of the convolutional layers to prevent\nover-fitting and (2) applying different combination of dilation rate in the\nlast three blocks of convolutional layer to reduce image resolution for feature\nextraction. Both the proposed architecture achieves a competitive result on\nCIFAR-10 and CIFAR-100 dataset.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 08:45:56 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Azim", "Syeda Noor Jaha", ""], ["Ratul", "Md. Aminur Rab", ""]]}, {"id": "1911.08777", "submitter": "Fei Ding", "authors": "Fei Ding, Gang Yang, Jinlu Liu, Jun Wu, Dayong Ding, Jie Xv, Gangwei\n  Cheng, Xirong Li", "title": "Hierarchical Attention Networks for Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The medical image is characterized by the inter-class indistinction, high\nvariability, and noise, where the recognition of pixels is challenging. Unlike\nprevious self-attention based methods that capture context information from one\nlevel, we reformulate the self-attention mechanism from the view of the\nhigh-order graph and propose a novel method, namely Hierarchical Attention\nNetwork (HANet), to address the problem of medical image segmentation.\nConcretely, an HA module embedded in the HANet captures context information\nfrom neighbors of multiple levels, where these neighbors are extracted from the\nhigh-order graph. In the high-order graph, there will be an edge between two\nnodes only if the correlation between them is high enough, which naturally\nreduces the noisy attention information caused by the inter-class\nindistinction. The proposed HA module is robust to the variance of input and\ncan be flexibly inserted into the existing convolution neural networks. We\nconduct experiments on three medical image segmentation tasks including optic\ndisc/cup segmentation, blood vessel segmentation, and lung segmentation.\nExtensive results show our method is more effective and robust than the\nexisting state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 09:07:16 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 13:35:29 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Ding", "Fei", ""], ["Yang", "Gang", ""], ["Liu", "Jinlu", ""], ["Wu", "Jun", ""], ["Ding", "Dayong", ""], ["Xv", "Jie", ""], ["Cheng", "Gangwei", ""], ["Li", "Xirong", ""]]}, {"id": "1911.08790", "submitter": "Junjie Hu", "authors": "Junjie Hu and Takayuki Okatani", "title": "Analysis of Deep Networks for Monocular Depth Estimation Through\n  Adversarial Attacks with Proposal of a Defense Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider adversarial attacks against a system of monocular\ndepth estimation (MDE) based on convolutional neural networks (CNNs). The\nmotivation is two-fold. One is to study the security of MDE systems, which has\nnot been actively considered in the community. The other is to improve our\nunderstanding of the computational mechanism of CNNs performing MDE. Toward\nthis end, we apply the method recently proposed for visualization of MDE to\ndefending attacks. It trains another CNN to predict a saliency map from an\ninput image, such that the CNN for MDE continues to accurately estimate the\ndepth map from the image with its non-salient part masked out. We report the\nfollowing findings. First, unsurprisingly, attacks by IFGSM (or equivalently\nPGD) succeed in making the CNNs yield inaccurate depth estimates. Second, the\nattacks can be defended by masking out non-salient pixels, indicating that the\nattacks function by perturbing mostly non-salient pixels. However, the\nprediction of saliency maps is itself vulnerable to the attacks, even though it\nis not the direct target of the attacks. We show that the attacks can be\ndefended by using a saliency map predicted by a CNN trained to be robust to the\nattacks. These results provide an effective defense method as well as a clue to\nunderstanding the computational mechanism of CNNs for MDE.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 09:41:53 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Hu", "Junjie", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1911.08797", "submitter": "Andrew Calway", "authors": "Noe Samano and Mengjie Zhou and Andrew Calway", "title": "You Are Here: Geolocation by Embedding Maps and Images", "comments": "18 pages, new version accepted for ECCV 2020 (poster), with new\n  results on publicly available dataset and comparison with implementation of\n  previously published alternative approach", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to geolocalising panoramic images on a 2-D\ncartographic map based on learning a low dimensional embedded space, which\nallows a comparison between an image captured at a location and local\nneighbourhoods of the map. The representation is not sufficiently\ndiscriminatory to allow localisation from a single image, but when concatenated\nalong a route, localisation converges quickly, with over 90% accuracy being\nachieved for routes of around 200m in length when using Google Street View and\nOpen Street Map data. The method generalises a previous fixed semantic feature\nbased approach and achieves significantly higher localisation accuracy and\nfaster convergence.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 10:05:09 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 21:25:03 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Samano", "Noe", ""], ["Zhou", "Mengjie", ""], ["Calway", "Andrew", ""]]}, {"id": "1911.08805", "submitter": "Old\\v{r}ich Kodym", "authors": "Old\\v{r}ich Kodym, Michal \\v{S}pan\\v{e}l, Adam Herout", "title": "Segmentation of Defective Skulls from CT Data for Tissue Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a method of automatic segmentation of defective\nskulls for custom cranial implant design and 3D printing purposes. Since such\ntissue models are usually required in patient cases with complex anatomical\ndefects and variety of external objects present in the acquired data, most deep\nlearning-based approaches fall short because it is not possible to create a\nsufficient training dataset that would encompass the spectrum of all possible\nstructures. Because CNN segmentation experiments in this application domain\nhave been so far limited to simple patch-based CNN architectures, we first show\nhow the usage of the encoder-decoder architecture can substantially improve the\nsegmentation accuracy. Then, we show how the number of segmentation artifacts,\nwhich usually require manual corrections, can be further reduced by adding a\nboundary term to CNN training and by globally optimizing the segmentation with\ngraph-cut. Finally, we show that using the proposed method, 3D segmentation\naccurate enough for clinical application can be achieved with 2D CNN\narchitectures as well as their 3D counterparts.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 10:31:38 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 10:02:53 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Kodym", "Old\u0159ich", ""], ["\u0160pan\u011bl", "Michal", ""], ["Herout", "Adam", ""]]}, {"id": "1911.08850", "submitter": "Hiroharu Kato", "authors": "Hiroharu Kato, Tatsuya Harada", "title": "Self-supervised Learning of 3D Objects from Natural Images", "comments": "Technical report. Project page:\n  http://hiroharu-kato.com/projects_en/cifar10_3d.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to learn single-view reconstruction of the 3D shape,\npose, and texture of objects from categorized natural images in a\nself-supervised manner. Since this is a severely ill-posed problem, carefully\ndesigning a training method and introducing constraints are essential. To avoid\nthe difficulty of training all elements at the same time, we propose training\ncategory-specific base shapes with fixed pose distribution and simple textures\nfirst, and subsequently training poses and textures using the obtained shapes.\nAnother difficulty is that shapes and backgrounds sometimes become excessively\ncomplicated to mistakenly reconstruct textures on object surfaces. To suppress\nit, we propose using strong regularization and constraints on object surfaces\nand background images. With these two techniques, we demonstrate that we can\nuse natural image collections such as CIFAR-10 and PASCAL objects for training,\nwhich indicates the possibility to realize 3D object reconstruction on diverse\nobject categories beyond synthetic datasets.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 12:07:12 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Kato", "Hiroharu", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1911.08854", "submitter": "Krzysztof Domino", "authors": "Agnieszka A. Tomaka, Leszek Luchowski, Dariusz Pojda, Micha{\\l}\n  Tarnawski, Krzysztof Domino", "title": "The dynamics of the stomatognathic system from 4D multimodal data", "comments": "Chapter 3 in A.Gadomski (ed.): Multiscale Locomotion: Its\n  Active-Matter Addressing Physical Principles; UTP University of Science &\n  Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this chapter is to discuss methods of acquisition,\nvisualization and analysis of the dynamics of a complex biomedical system,\nillustrated by the human stomatognathic system. The stomatognathic system\nconsists of the teeth and the skull bones with the maxilla and the mandible.\nIts dynamics can be described by the change of mutual position of the\nlower/mandibular part versus the upper/maxillary one due to the physiological\nmotion of opening, chewing and swallowing. In order to analyse the dynamics of\nthe stomatognathic system its morphology and motion has to be digitized, which\nis done using static and dynamic multimodal imagery like CBCT and 3D scans data\nand temporal measurements of motion. The integration of multimodal data\nincorporates different direct and indirect methods of registration - aligning\nof all the data in the same coordinate system. The integrated sets of data form\n4D multimodal data which can be further visualized, modeled, and subjected to\nmultivariate time series analysis. Example results are shown. Although there is\nno direct method of imaging the TMJ motion, the integration of multimodal data\nforms an adequate tool. As medical imaging becomes ever more diverse and ever\nmore accessible, organizing the imagery and measurements into unified,\ncomprehensive records can deliver to the doctor the most information in the\nmost accessible form, creating a new quality in data simulation, analysis and\ninterpretation.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 12:25:58 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Tomaka", "Agnieszka A.", ""], ["Luchowski", "Leszek", ""], ["Pojda", "Dariusz", ""], ["Tarnawski", "Micha\u0142", ""], ["Domino", "Krzysztof", ""]]}, {"id": "1911.08855", "submitter": "Chen Chen", "authors": "Chen Chen, Mengyuan Liu, Xiandong Meng, Wanpeng Xiao, Qi Ju", "title": "RefineDetLite: A Lightweight One-stage Object Detection Framework for\n  CPU-only Devices", "comments": "16 pages, 8 figures", "journal-ref": "2020 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition Workshops (CVPRW)", "doi": "10.1109/CVPRW50498.2020.00358", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous state-of-the-art real-time object detectors have been reported on\nGPUs which are extremely expensive for processing massive data and in\nresource-restricted scenarios. Therefore, high efficiency object detectors on\nCPU-only devices are urgently-needed in industry. The floating-point operations\n(FLOPs) of networks are not strictly proportional to the running speed on CPU\ndevices, which inspires the design of an exactly \"fast\" and \"accurate\" object\ndetector. After investigating the concern gaps between classification networks\nand detection backbones, and following the design principles of efficient\nnetworks, we propose a lightweight residual-like backbone with large receptive\nfields and wide dimensions for low-level features, which are crucial for\ndetection tasks. Correspondingly, we also design a light-head detection part to\nmatch the backbone capability. Furthermore, by analyzing the drawbacks of\ncurrent one-stage detector training strategies, we also propose three\northogonal training strategies---IOU-guided loss, classes-aware weighting\nmethod and balanced multi-task training approach. Without bells and whistles,\nour proposed RefineDetLite achieves 26.8 mAP on the MSCOCO benchmark at a speed\nof 130 ms/pic on a single-thread CPU. The detection accuracy can be further\nincreased to 29.6 mAP by integrating all the proposed training strategies,\nwithout apparent speed drop.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 12:27:26 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 07:41:44 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Chen", "Chen", ""], ["Liu", "Mengyuan", ""], ["Meng", "Xiandong", ""], ["Xiao", "Wanpeng", ""], ["Ju", "Qi", ""]]}, {"id": "1911.08860", "submitter": "Vladyslav Usenko", "authors": "Christiane Sommer, Vladyslav Usenko, David Schubert, Nikolaus Demmel\n  and Daniel Cremers", "title": "Efficient Derivative Computation for Cumulative B-Splines on Lie Groups", "comments": "First two authors contributed equally", "journal-ref": null, "doi": "10.1109/CVPR42600.2020.01116", "report-no": null, "categories": "cs.CV cs.NA cs.RO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous-time trajectory representation has recently gained popularity for\ntasks where the fusion of high-frame-rate sensors and multiple unsynchronized\ndevices is required. Lie group cumulative B-splines are a popular way of\nrepresenting continuous trajectories without singularities. They have been used\nin near real-time SLAM and odometry systems with IMU, LiDAR, regular, RGB-D and\nevent cameras, as well as for offline calibration. These applications require\nefficient computation of time derivatives (velocity, acceleration), but all\nprior works rely on a computationally suboptimal formulation. In this work we\npresent an alternative derivation of time derivatives based on recurrence\nrelations that needs $\\mathcal{O}(k)$ instead of $\\mathcal{O}(k^2)$ matrix\noperations (for a spline of order $k$) and results in simple and elegant\nexpressions. While producing the same result, the proposed approach\nsignificantly speeds up the trajectory optimization and allows for computing\nsimple analytic derivatives with respect to spline knots. The results presented\nin this paper pave the way for incorporating continuous-time trajectory\nrepresentations into more applications where real-time performance is required.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 12:33:46 GMT"}, {"version": "v2", "created": "Sat, 30 May 2020 16:38:23 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Sommer", "Christiane", ""], ["Usenko", "Vladyslav", ""], ["Schubert", "David", ""], ["Demmel", "Nikolaus", ""], ["Cremers", "Daniel", ""]]}, {"id": "1911.08862", "submitter": "Alan Lukezic", "authors": "Alan Luke\\v{z}i\\v{c}, Ji\\v{r}\\'i Matas, Matej Kristan", "title": "D3S -- A Discriminative Single Shot Segmentation Tracker", "comments": "The paper is accepted to the CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Template-based discriminative trackers are currently the dominant tracking\nparadigm due to their robustness, but are restricted to bounding box tracking\nand a limited range of transformation models, which reduces their localization\naccuracy. We propose a discriminative single-shot segmentation tracker - D3S,\nwhich narrows the gap between visual object tracking and video object\nsegmentation. A single-shot network applies two target models with\ncomplementary geometric properties, one invariant to a broad range of\ntransformations, including non-rigid deformations, the other assuming a rigid\nobject to simultaneously achieve high robustness and online target\nsegmentation. Without per-dataset finetuning and trained only for segmentation\nas the primary output, D3S outperforms all trackers on VOT2016, VOT2018 and\nGOT-10k benchmarks and performs close to the state-of-the-art trackers on the\nTrackingNet. D3S outperforms the leading segmentation tracker SiamMask on video\nobject segmentation benchmark and performs on par with top video object\nsegmentation algorithms, while running an order of magnitude faster, close to\nreal-time.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 12:41:21 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 10:04:34 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Luke\u017ei\u010d", "Alan", ""], ["Matas", "Ji\u0159\u00ed", ""], ["Kristan", "Matej", ""]]}, {"id": "1911.08877", "submitter": "Lei Ding", "authors": "Lei Ding, Hao Tang, Lorenzo Bruzzone", "title": "Improving Semantic Segmentation of Aerial Images Using Patch-based\n  Attention", "comments": "[J]. IEEE Transactions on Geoscience and Remote Sensing, 2020", "journal-ref": "Ding L, Tang H, Bruzzone L. LANet: Local attention embedding to\n  improve the semantic segmentation of remote sensing images[J]. IEEE\n  Transactions on Geoscience and Remote Sensing, 2020", "doi": "10.1109/TGRS.2020.2994150", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The trade-off between feature representation power and spatial localization\naccuracy is crucial for the dense classification/semantic segmentation of\naerial images. High-level features extracted from the late layers of a neural\nnetwork are rich in semantic information, yet have blurred spatial details;\nlow-level features extracted from the early layers of a network contain more\npixel-level information, but are isolated and noisy. It is therefore difficult\nto bridge the gap between high and low-level features due to their difference\nin terms of physical information content and spatial distribution. In this\nwork, we contribute to solve this problem by enhancing the feature\nrepresentation in two ways. On the one hand, a patch attention module (PAM) is\nproposed to enhance the embedding of context information based on a patch-wise\ncalculation of local attention. On the other hand, an attention embedding\nmodule (AEM) is proposed to enrich the semantic information of low-level\nfeatures by embedding local focus from high-level features. Both of the\nproposed modules are light-weight and can be applied to process the extracted\nfeatures of convolutional neural networks (CNNs). Experiments show that, by\nintegrating the proposed modules into the baseline Fully Convolutional Network\n(FCN), the resulting local attention network (LANet) greatly improves the\nperformance over the baseline and outperforms other attention based methods on\ntwo aerial image datasets.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 13:12:04 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Ding", "Lei", ""], ["Tang", "Hao", ""], ["Bruzzone", "Lorenzo", ""]]}, {"id": "1911.08896", "submitter": "Jian Xie", "authors": "Jian Xie", "title": "Shift Convolution Network for Stereo Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present Shift Convolution Network (ShiftConvNet) to provide\nmatching capability between two feature maps for stereo estimation. The\nproposed method can speedily produce a highly accurate disparity map from\nstereo images. A module called shift convolution layer is proposed to replace\nthe traditional correlation layer to perform patch comparisons between two\nfeature maps. By using a novel architecture of convolutional network to learn\nthe matching process, ShiftConvNet can produce better results than\nDispNet-C[1], also running faster with 5 fps. Moreover, with a proposed auto\nshift convolution refine part, further improvement is obtained. The proposed\napproach was evaluated on FlyingThings 3D. It achieves state-of-the-art results\non the benchmark dataset. Codes will be made available at github.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 13:29:21 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Xie", "Jian", ""]]}, {"id": "1911.08916", "submitter": "Saimunur Rahman", "authors": "Saimunur Rahman, Lei Wang, Changming Sun, Luping Zhou", "title": "Deep Learning based HEp-2 Image Classification: A Comprehensive Review", "comments": "Published in Medical Image Analysis", "journal-ref": "Medical Image Analysis (2020): 101764", "doi": "10.1016/j.media.2020.101764", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification of HEp-2 cell patterns plays a significant role in the\nindirect immunofluorescence test for identifying autoimmune diseases in the\nhuman body. Many automatic HEp-2 cell classification methods have been proposed\nin recent years, amongst which deep learning based methods have shown\nimpressive performance. This paper provides a comprehensive review of the\nexisting deep learning based HEp-2 cell image classification methods. These\nmethods perform HEp-2 image classification at two levels, namely, cell-level\nand specimen-level. Both levels are covered in this review. At each level, the\nmethods are organized with a deep network usage based taxonomy. The core idea,\nnotable achievements, and key strengths and weaknesses of each method are\ncritically analyzed. Furthermore, a concise review of the existing HEp-2\ndatasets that are commonly used in the literature is given. The paper ends with\na discussion on novel opportunities and future research directions in this\nfield. It is hoped that this paper would provide readers with a thorough\nreference of this novel, challenging, and thriving field.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 14:03:27 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 09:32:42 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Rahman", "Saimunur", ""], ["Wang", "Lei", ""], ["Sun", "Changming", ""], ["Zhou", "Luping", ""]]}, {"id": "1911.08928", "submitter": "Matteo Saveriano", "authors": "Pietro Falco, Matteo Saveriano, Eka Gibran Hasany, Nicholas H. Kirk\n  and Dongheui Lee", "title": "A Human Action Descriptor Based on Motion Coordination", "comments": null, "journal-ref": null, "doi": "10.1109/LRA.2017.2652494", "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a descriptor for human whole-body actions based on\nmotion coordination. We exploit the principle, well known in neuromechanics,\nthat humans move their joints in a coordinated fashion. Our coordination-based\ndescriptor (CODE) is computed by two main steps. The first step is to identify\nthe most informative joints which characterize the motion. The second step\nenriches the descriptor considering minimum and maximum joint velocities and\nthe correlations between the most informative joints. In order to compute the\ndistances between action descriptors, we propose a novel correlation-based\nsimilarity measure. The performance of CODE is tested on two public datasets,\nnamely HDM05 and Berkeley MHAD, and compared with state-of-the-art approaches,\nshowing recognition results.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 14:22:28 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Falco", "Pietro", ""], ["Saveriano", "Matteo", ""], ["Hasany", "Eka Gibran", ""], ["Kirk", "Nicholas H.", ""], ["Lee", "Dongheui", ""]]}, {"id": "1911.08947", "submitter": "Minghui Liao", "authors": "Minghui Liao, Zhaoyi Wan, Cong Yao, Kai Chen, Xiang Bai", "title": "Real-time Scene Text Detection with Differentiable Binarization", "comments": "Accepted to AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, segmentation-based methods are quite popular in scene text\ndetection, as the segmentation results can more accurately describe scene text\nof various shapes such as curve text. However, the post-processing of\nbinarization is essential for segmentation-based detection, which converts\nprobability maps produced by a segmentation method into bounding boxes/regions\nof text. In this paper, we propose a module named Differentiable Binarization\n(DB), which can perform the binarization process in a segmentation network.\nOptimized along with a DB module, a segmentation network can adaptively set the\nthresholds for binarization, which not only simplifies the post-processing but\nalso enhances the performance of text detection. Based on a simple segmentation\nnetwork, we validate the performance improvements of DB on five benchmark\ndatasets, which consistently achieves state-of-the-art results, in terms of\nboth detection accuracy and speed. In particular, with a light-weight backbone,\nthe performance improvements by DB are significant so that we can look for an\nideal tradeoff between detection accuracy and efficiency. Specifically, with a\nbackbone of ResNet-18, our detector achieves an F-measure of 82.8, running at\n62 FPS, on the MSRA-TD500 dataset. Code is available at:\nhttps://github.com/MhLiao/DB\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 14:56:47 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 13:33:45 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Liao", "Minghui", ""], ["Wan", "Zhaoyi", ""], ["Yao", "Cong", ""], ["Chen", "Kai", ""], ["Bai", "Xiang", ""]]}, {"id": "1911.08953", "submitter": "Ferran Par\\'es", "authors": "Ferran Par\\'es, Dario Garcia-Gasulla, Harald Servat, Jes\\'us Labarta\n  and Eduard Ayguad\\'e", "title": "MetH: A family of high-resolution and variable-shape image challenges", "comments": "An improved and extended version of this paper has been published in\n  arXiv:2007.13693 This version is now obsolete", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-resolution and variable-shape images have not yet been properly\naddressed by the AI community. The approach of down-sampling data often used\nwith convolutional neural networks is sub-optimal for many tasks, and has too\nmany drawbacks to be considered a sustainable alternative. In sight of the\nincreasing importance of problems that can benefit from exploiting\nhigh-resolution (HR) and variable-shape, and with the goal of promoting\nresearch in that direction, we introduce a new family of datasets (MetH). The\nfour proposed problems include two image classification, one image regression\nand one super resolution task. Each of these datasets contains thousands of art\npieces captured by HR and variable-shape images, labeled by experts at the\nMetropolitan Museum of Art. We perform an analysis, which shows how the\nproposed tasks go well beyond current public alternatives in both pixel size\nand aspect ratio variance. At the same time, the performance obtained by\npopular architectures on these tasks shows that there is ample room for\nimprovement. To wrap up the relevance of the contribution we review the fields,\nboth in AI and high-performance computing, that could benefit from the proposed\nchallenges.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 15:01:22 GMT"}, {"version": "v2", "created": "Fri, 29 Nov 2019 17:30:59 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2020 09:27:49 GMT"}, {"version": "v4", "created": "Tue, 29 Sep 2020 11:37:54 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Par\u00e9s", "Ferran", ""], ["Garcia-Gasulla", "Dario", ""], ["Servat", "Harald", ""], ["Labarta", "Jes\u00fas", ""], ["Ayguad\u00e9", "Eduard", ""]]}, {"id": "1911.08966", "submitter": "Neelanjan Bhowmik", "authors": "Yona Falinie A. Gaus, Neelanjan Bhowmik, Samet Akcay, Toby P. Breckon", "title": "Evaluating the Transferability and Adversarial Discrimination of\n  Convolutional Neural Networks for Threat Object Detection and Classification\n  within X-Ray Security Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  X-ray imagery security screening is essential to maintaining transport\nsecurity against a varying profile of threat or prohibited items. Particular\ninterest lies in the automatic detection and classification of weapons such as\nfirearms and knives within complex and cluttered X-ray security imagery. Here,\nwe address this problem by exploring various end-to-end object detection\nConvolutional Neural Network (CNN) architectures. We evaluate several leading\nvariants spanning the Faster R-CNN, Mask R-CNN, and RetinaNet architectures to\nexplore the transferability of such models between varying X-ray scanners with\ndiffering imaging geometries, image resolutions and material colour profiles.\nWhilst the limited availability of X-ray threat imagery can pose a challenge,\nwe employ a transfer learning approach to evaluate whether such inter-scanner\ngeneralisation may exist over a multiple class detection problem. Overall, we\nachieve maximal detection performance using a Faster R-CNN architecture with a\nResNet$_{101}$ classification network, obtaining 0.88 and 0.86 of mean Average\nPrecision (mAP) for a three-class and two class item from varying X-ray imaging\nsources. Our results exhibit a remarkable degree of generalisability in terms\nof cross-scanner performance (mAP: 0.87, firearm detection: 0.94 AP). In\naddition, we examine the inherent adversarial discriminative capability of such\nnetworks using a specifically generated adversarial dataset for firearms\ndetection - with a variable low false positive, as low as 5%, this shows both\nthe challenge and promise of such threat detection within X-ray security\nimagery.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 15:29:12 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Gaus", "Yona Falinie A.", ""], ["Bhowmik", "Neelanjan", ""], ["Akcay", "Samet", ""], ["Breckon", "Toby P.", ""]]}, {"id": "1911.08995", "submitter": "Okan K\\\"op\\\"ukl\\\"u", "authors": "Yinglong Feng, Shuncheng Wu, Okan K\\\"op\\\"ukl\\\"u, Xueyang Kang,\n  Federico Tombari", "title": "Unsupervised Monocular Depth Prediction for Indoor Continuous Video\n  Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies unsupervised monocular depth prediction problem. Most of\nexisting unsupervised depth prediction algorithms are developed for outdoor\nscenarios, while the depth prediction work in the indoor environment is still\nvery scarce to our knowledge. Therefore, this work focuses on narrowing the gap\nby firstly evaluating existing approaches in the indoor environments and then\nimproving the state-of-the-art design of architecture. Unlike typical outdoor\ntraining dataset, such as KITTI with motion constraints, data for indoor\nenvironment contains more arbitrary camera movement and short baseline between\ntwo consecutive images, which deteriorates the network training for the pose\nestimation. To address this issue, we propose two methods: Firstly, we propose\na novel reconstruction loss function to constraint pose estimation, resulting\nin accuracy improvement of the predicted disparity map; secondly, we use an\nensemble learning with a flipping strategy along with a median filter, directly\ntaking operation on the output disparity map. We evaluate our approaches on the\nTUM RGB-D and self-collected datasets. The results have shown that both\napproaches outperform the previous state-of-the-art unsupervised learning\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 16:08:10 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Feng", "Yinglong", ""], ["Wu", "Shuncheng", ""], ["K\u00f6p\u00fckl\u00fc", "Okan", ""], ["Kang", "Xueyang", ""], ["Tombari", "Federico", ""]]}, {"id": "1911.09010", "submitter": "Neelanjan Bhowmik", "authors": "Ganesh Samarth C.A., Neelanjan Bhowmik, Toby P. Breckon", "title": "Experimental Exploration of Compact Convolutional Neural Network\n  Architectures for Non-temporal Real-time Fire Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we explore different Convolutional Neural Network (CNN)\narchitectures and their variants for non-temporal binary fire detection and\nlocalization in video or still imagery. We consider the performance of\nexperimentally defined, reduced complexity deep CNN architectures for this task\nand evaluate the effects of different optimization and normalization techniques\napplied to different CNN architectures (spanning the Inception, ResNet and\nEfficientNet architectural concepts). Contrary to contemporary trends in the\nfield, our work illustrates a maximum overall accuracy of 0.96 for full frame\nbinary fire detection and 0.94 for superpixel localization using an\nexperimentally defined reduced CNN architecture based on the concept of\nInceptionV4. We notably achieve a lower false positive rate of 0.06 compared to\nprior work in the field presenting an efficient, robust and real-time solution\nfor fire region detection.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 16:27:10 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["A.", "Ganesh Samarth C.", ""], ["Bhowmik", "Neelanjan", ""], ["Breckon", "Toby P.", ""]]}, {"id": "1911.09017", "submitter": "Quanshi Zhang", "authors": "Hao Zhang, Jiayi Chen, Haotian Xue, Quanshi Zhang", "title": "Towards a Unified Evaluation of Explanation Methods without Ground Truth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a set of criteria to evaluate the objectiveness of\nexplanation methods of neural networks, which is crucial for the development of\nexplainable AI, but it also presents significant challenges. The core challenge\nis that people usually cannot obtain ground-truth explanations of the neural\nnetwork. To this end, we design four metrics to evaluate explanation results\nwithout ground-truth explanations. Our metrics can be broadly applied to nine\nbenchmark methods of interpreting neural networks, which provides new insights\nof explanation methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 16:44:48 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Zhang", "Hao", ""], ["Chen", "Jiayi", ""], ["Xue", "Haotian", ""], ["Zhang", "Quanshi", ""]]}, {"id": "1911.09026", "submitter": "Paolo Andreini", "authors": "Simone Bonechi, Paolo Andreini, Monica Bianchini, Franco Scarselli", "title": "Weak Supervision for Generating Pixel-Level Annotations in Scene Text\n  Segmentation", "comments": "arXiv admin note: substantial text overlap with arXiv:1904.00818", "journal-ref": null, "doi": "10.1016/j.patrec.2020.06.023", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing pixel-level supervisions for scene text segmentation is inherently\ndifficult and costly, so that only few small datasets are available for this\ntask. To face the scarcity of training data, previous approaches based on\nConvolutional Neural Networks (CNNs) rely on the use of a synthetic dataset for\npre-training. However, synthetic data cannot reproduce the complexity and\nvariability of natural images. In this work, we propose to use a weakly\nsupervised learning approach to reduce the domain-shift between synthetic and\nreal data. Leveraging the bounding-box supervision of the COCO-Text and the MLT\ndatasets, we generate weak pixel-level supervisions of real images. In\nparticular, the COCO-Text-Segmentation (COCO_TS) and the MLT-Segmentation\n(MLT_S) datasets are created and released. These two datasets are used to train\na CNN, the Segmentation Multiscale Attention Network (SMANet), which is\nspecifically designed to face some peculiarities of the scene text segmentation\ntask. The SMANet is trained end-to-end on the proposed datasets, and the\nexperiments show that COCO_TS and MLT_S are a valid alternative to synthetic\nimages, allowing to use only a fraction of the training samples and improving\nsignificantly the performances.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 09:55:27 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Bonechi", "Simone", ""], ["Andreini", "Paolo", ""], ["Bianchini", "Monica", ""], ["Scarselli", "Franco", ""]]}, {"id": "1911.09033", "submitter": "Eric Crawford", "authors": "Eric Crawford, Joelle Pineau", "title": "Exploiting Spatial Invariance for Scalable Unsupervised Object Tracking", "comments": "Accepted at AAAI 2020. Code: https://github.com/e2crawfo/silot.\n  Visualizations: https://sites.google.com/view/silot", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to detect and track objects in the visual world is a crucial\nskill for any intelligent agent, as it is a necessary precursor to any\nobject-level reasoning process. Moreover, it is important that agents learn to\ntrack objects without supervision (i.e. without access to annotated training\nvideos) since this will allow agents to begin operating in new environments\nwith minimal human assistance. The task of learning to discover and track\nobjects in videos, which we call \\textit{unsupervised object tracking}, has\ngrown in prominence in recent years; however, most architectures that address\nit still struggle to deal with large scenes containing many objects. In the\ncurrent work, we propose an architecture that scales well to the large-scene,\nmany-object setting by employing spatially invariant computations (convolutions\nand spatial attention) and representations (a spatially local object\nspecification scheme). In a series of experiments, we demonstrate a number of\nattractive features of our architecture; most notably, that it outperforms\ncompeting methods at tracking objects in cluttered scenes with many objects,\nand that it can generalize well to videos that are larger and/or contain more\nobjects than videos encountered during training.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 17:03:51 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Crawford", "Eric", ""], ["Pineau", "Joelle", ""]]}, {"id": "1911.09040", "submitter": "Quanshi Zhang", "authors": "Wen Shen, Binbin Zhang, Shikun Huang, Zhihua Wei, Quanshi Zhang", "title": "3D-Rotation-Equivariant Quaternion Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a set of rules to revise various neural networks for 3D\npoint cloud processing to rotation-equivariant quaternion neural networks\n(REQNNs). We find that when a neural network uses quaternion features under\ncertain conditions, the network feature naturally has the rotation-equivariance\nproperty. Rotation equivariance means that applying a specific rotation\ntransformation to the input point cloud is equivalent to applying the same\nrotation transformation to all intermediate-layer quaternion features. Besides,\nthe REQNN also ensures that the intermediate-layer features are invariant to\nthe permutation of input points. Compared with the original neural network, the\nREQNN exhibits higher rotation robustness.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 17:10:52 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 08:24:30 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Shen", "Wen", ""], ["Zhang", "Binbin", ""], ["Huang", "Shikun", ""], ["Wei", "Zhihua", ""], ["Zhang", "Quanshi", ""]]}, {"id": "1911.09042", "submitter": "Yongfei Liu", "authors": "Yongfei Liu, Bo Wan, Xiaodan Zhu, Xuming He", "title": "Learning Cross-modal Context Graph for Visual Grounding", "comments": "AAAI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual grounding is a ubiquitous building block in many vision-language tasks\nand yet remains challenging due to large variations in visual and linguistic\nfeatures of grounding entities, strong context effect and the resulting\nsemantic ambiguities. Prior works typically focus on learning representations\nof individual phrases with limited context information. To address their\nlimitations, this paper proposes a language-guided graph representation to\ncapture the global context of grounding entities and their relations, and\ndevelop a cross-modal graph matching strategy for the multiple-phrase visual\ngrounding task. In particular, we introduce a modular graph neural network to\ncompute context-aware representations of phrases and object proposals\nrespectively via message propagation, followed by a graph-based matching module\nto generate globally consistent localization of grounding phrases. We train the\nentire graph neural network jointly in a two-stage strategy and evaluate it on\nthe Flickr30K Entities benchmark. Extensive experiments show that our method\noutperforms the prior state of the arts by a sizable margin, evidencing the\nefficacy of our grounding framework. Code is available at\n\"https://github.com/youngfly11/LCMCG-PyTorch\".\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 17:16:04 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 07:56:23 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Liu", "Yongfei", ""], ["Wan", "Bo", ""], ["Zhu", "Xiaodan", ""], ["He", "Xuming", ""]]}, {"id": "1911.09046", "submitter": "Xiangfeng Wang", "authors": "Junjie Wang, Xiangfeng Wang, Bo Jin, Junchi Yan, Wenjie Zhang,\n  Hongyuan Zha", "title": "Heterogeneous Graph-based Knowledge Transfer for Generalized Zero-shot\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized zero-shot learning (GZSL) tackles the problem of learning to\nclassify instances involving both seen classes and unseen ones. The key issue\nis how to effectively transfer the model learned from seen classes to unseen\nclasses. Existing works in GZSL usually assume that some prior information\nabout unseen classes are available. However, such an assumption is unrealistic\nwhen new unseen classes appear dynamically. To this end, we propose a novel\nheterogeneous graph-based knowledge transfer method (HGKT) for GZSL, agnostic\nto unseen classes and instances, by leveraging graph neural network.\nSpecifically, a structured heterogeneous graph is constructed with high-level\nrepresentative nodes for seen classes, which are chosen through Wasserstein\nbarycenter in order to simultaneously capture inter-class and intra-class\nrelationship. The aggregation and embedding functions can be learned through\ngraph neural network, which can be used to compute the embeddings of unseen\nclasses by transferring the knowledge from their neighbors. Extensive\nexperiments on public benchmark datasets show that our method achieves\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 17:20:05 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Wang", "Junjie", ""], ["Wang", "Xiangfeng", ""], ["Jin", "Bo", ""], ["Yan", "Junchi", ""], ["Zhang", "Wenjie", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1911.09053", "submitter": "Quanshi Zhang", "authors": "Wen Shen, Zhihua Wei, Shikun Huang, Binbin Zhang, Panyue Chen, Ping\n  Zhao, Quanshi Zhang", "title": "Verifiability and Predictability: Interpreting Utilities of Network\n  Architectures for Point Cloud Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we diagnose deep neural networks for 3D point cloud processing\nto explore utilities of different intermediate-layer network architectures. We\npropose a number of hypotheses on the effects of specific intermediate-layer\nnetwork architectures on the representation capacity of DNNs. In order to prove\nthe hypotheses, we design five metrics to diagnose various types of DNNs from\nthe following perspectives, information discarding, information concentration,\nrotation robustness, adversarial robustness, and neighborhood inconsistency. We\nconduct comparative studies based on such metrics to verify the hypotheses. We\nfurther use the verified hypotheses to revise intermediate-layer architectures\nof existing DNNs and improve their utilities. Experiments demonstrate the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 17:33:19 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 09:00:39 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 03:54:03 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Shen", "Wen", ""], ["Wei", "Zhihua", ""], ["Huang", "Shikun", ""], ["Zhang", "Binbin", ""], ["Chen", "Panyue", ""], ["Zhao", "Ping", ""], ["Zhang", "Quanshi", ""]]}, {"id": "1911.09054", "submitter": "Linwei Zheng", "authors": "Umar Ozgunalp, Rui Fan, Shanshan Cheng, Yuxiang Sun, Weixun Zuo,\n  Yilong Zhu, Bohuan Xue, Linwei Zheng, Qing Liang, Ming Liu", "title": "Robust Lane Marking Detection Algorithm Using Drivable Area Segmentation\n  and Extended SLT", "comments": "4 pages, 3 figures, 2019 IEEE International Conference on Robotics\n  and Biomimetics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a robust lane detection algorithm is proposed, where the\nvertical road profile of the road is estimated using dynamic programming from\nthe v-disparity map and, based on the estimated profile, the road area is\nsegmented. Since the lane markings are on the road area and any feature point\nabove the ground will be a noise source for the lane detection, a mask is\ncreated for the road area to remove some of the noise for lane detection. The\nestimated mask is multiplied by the lane feature map in a bird's eye view\n(BEV). The lane feature points are extracted by using an extended version of\nsymmetrical local threshold (SLT), which not only considers dark light dark\ntransition (DLD) of the lane markings, like (SLT), but also considers\nparallelism on the lane marking borders. The segmentation then uses only the\nfeature points that are on the road area. A maximum of two linear lane markings\nare detected using an efficient 1D Hough transform. Then, the detected linear\nlane markings are used to create a region of interest (ROI) for parabolic lane\ndetection. Finally, based on the estimated region of interest, parabolic lane\nmodels are fitted using robust fitting. Due to the robust lane feature\nextraction and road area segmentation, the proposed algorithm robustly detects\nlane markings and achieves lane marking detection with an accuracy of 91% when\ntested on a sequence from the KITTI dataset.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 17:34:31 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Ozgunalp", "Umar", ""], ["Fan", "Rui", ""], ["Cheng", "Shanshan", ""], ["Sun", "Yuxiang", ""], ["Zuo", "Weixun", ""], ["Zhu", "Yilong", ""], ["Xue", "Bohuan", ""], ["Zheng", "Linwei", ""], ["Liang", "Qing", ""], ["Liu", "Ming", ""]]}, {"id": "1911.09058", "submitter": "Omid Poursaeed", "authors": "Omid Poursaeed, Tianxing Jiang, Yordanos Goshu, Harry Yang, Serge\n  Belongie, Ser-Nam Lim", "title": "Fine-grained Synthesis of Unrestricted Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for generating unrestricted adversarial examples\nby manipulating fine-grained aspects of image generation. Unlike existing\nunrestricted attacks that typically hand-craft geometric transformations, we\nlearn stylistic and stochastic modifications leveraging state-of-the-art\ngenerative models. This allows us to manipulate an image in a controlled,\nfine-grained manner without being bounded by a norm threshold. Our approach can\nbe used for targeted and non-targeted unrestricted attacks on classification,\nsemantic segmentation and object detection models. Our attacks can bypass\ncertified defenses, yet our adversarial images look indistinguishable from\nnatural images as verified by human evaluation. Moreover, we demonstrate that\nadversarial training with our examples improves performance of the model on\nclean images without requiring any modifications to the architecture. We\nperform experiments on LSUN, CelebA-HQ and COCO-Stuff as high resolution\ndatasets to validate efficacy of our proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 17:42:12 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 16:53:26 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Poursaeed", "Omid", ""], ["Jiang", "Tianxing", ""], ["Goshu", "Yordanos", ""], ["Yang", "Harry", ""], ["Belongie", "Serge", ""], ["Lim", "Ser-Nam", ""]]}, {"id": "1911.09070", "submitter": "Mingxing Tan", "authors": "Mingxing Tan, Ruoming Pang, Quoc V. Le", "title": "EfficientDet: Scalable and Efficient Object Detection", "comments": "CVPR 2020", "journal-ref": "Proceedings of the IEEE Conference on Computer Vision and Pattern\n  Recognition (2020)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model efficiency has become increasingly important in computer vision. In\nthis paper, we systematically study neural network architecture design choices\nfor object detection and propose several key optimizations to improve\nefficiency. First, we propose a weighted bi-directional feature pyramid network\n(BiFPN), which allows easy and fast multiscale feature fusion; Second, we\npropose a compound scaling method that uniformly scales the resolution, depth,\nand width for all backbone, feature network, and box/class prediction networks\nat the same time. Based on these optimizations and better backbones, we have\ndeveloped a new family of object detectors, called EfficientDet, which\nconsistently achieve much better efficiency than prior art across a wide\nspectrum of resource constraints. In particular, with single model and\nsingle-scale, our EfficientDet-D7 achieves state-of-the-art 55.1 AP on COCO\ntest-dev with 77M parameters and 410B FLOPs, being 4x - 9x smaller and using\n13x - 42x fewer FLOPs than previous detectors. Code is available at\nhttps://github.com/google/automl/tree/master/efficientdet.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 18:16:09 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2020 22:55:42 GMT"}, {"version": "v3", "created": "Tue, 31 Mar 2020 18:47:08 GMT"}, {"version": "v4", "created": "Fri, 3 Apr 2020 20:34:21 GMT"}, {"version": "v5", "created": "Sun, 24 May 2020 07:12:44 GMT"}, {"version": "v6", "created": "Sun, 14 Jun 2020 18:28:53 GMT"}, {"version": "v7", "created": "Mon, 27 Jul 2020 15:55:16 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Tan", "Mingxing", ""], ["Pang", "Ruoming", ""], ["Le", "Quoc V.", ""]]}, {"id": "1911.09071", "submitter": "Katherine Hermann", "authors": "Katherine L. Hermann, Ting Chen, and Simon Kornblith", "title": "The Origins and Prevalence of Texture Bias in Convolutional Neural\n  Networks", "comments": "NeurIPS'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has indicated that, unlike humans, ImageNet-trained CNNs tend to\nclassify images by texture rather than by shape. How pervasive is this bias,\nand where does it come from? We find that, when trained on datasets of images\nwith conflicting shape and texture, CNNs learn to classify by shape at least as\neasily as by texture. What factors, then, produce the texture bias in CNNs\ntrained on ImageNet? Different unsupervised training objectives and different\narchitectures have small but significant and largely independent effects on the\nlevel of texture bias. However, all objectives and architectures still lead to\nmodels that make texture-based classification decisions a majority of the time,\neven if shape information is decodable from their hidden representations. The\neffect of data augmentation is much larger. By taking less aggressive random\ncrops at training time and applying simple, naturalistic augmentation (color\ndistortion, noise, and blur), we train models that classify ambiguous images by\nshape a majority of the time, and outperform baselines on out-of-distribution\ntest sets. Our results indicate that apparent differences in the way humans and\nImageNet-trained CNNs process images may arise not primarily from differences\nin their internal workings, but from differences in the data that they see.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 18:16:38 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 20:48:56 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2020 22:51:23 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Hermann", "Katherine L.", ""], ["Chen", "Ting", ""], ["Kornblith", "Simon", ""]]}, {"id": "1911.09074", "submitter": "Yu Liu", "authors": "Yu Liu, Xuhui Jia, Mingxing Tan, Raviteja Vemulapalli, Yukun Zhu,\n  Bradley Green, Xiaogang Wang", "title": "Search to Distill: Pearls are Everywhere but not the Eyes", "comments": "Accepted as an oral representation to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard Knowledge Distillation (KD) approaches distill the knowledge of a\ncumbersome teacher model into the parameters of a student model with a\npre-defined architecture. However, the knowledge of a neural network, which is\nrepresented by the network's output distribution conditioned on its input,\ndepends not only on its parameters but also on its architecture. Hence, a more\ngeneralized approach for KD is to distill the teacher's knowledge into both the\nparameters and architecture of the student. To achieve this, we present a new\nArchitecture-aware Knowledge Distillation (AKD) approach that finds student\nmodels (pearls for the teacher) that are best for distilling the given teacher\nmodel. In particular, we leverage Neural Architecture Search (NAS), equipped\nwith our KD-guided reward, to search for the best student architectures for a\ngiven teacher. Experimental results show our proposed AKD consistently\noutperforms the conventional NAS plus KD approach, and achieves\nstate-of-the-art results on the ImageNet classification task under various\nlatency settings. Furthermore, the best AKD student architecture for the\nImageNet classification task also transfers well to other tasks such as million\nlevel face recognition and ensemble learning.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 18:19:25 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 03:48:49 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Liu", "Yu", ""], ["Jia", "Xuhui", ""], ["Tan", "Mingxing", ""], ["Vemulapalli", "Raviteja", ""], ["Zhu", "Yukun", ""], ["Green", "Bradley", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1911.09092", "submitter": "Suryansh Kumar", "authors": "Suryansh Kumar, Yuchao Dai, Hongdong Li", "title": "Superpixel Soup: Monocular Dense 3D Reconstruction of a Complex Dynamic\n  Scene", "comments": "12 pages, 18 Figures, 2 Tables. Accepted for publication in IEEE,\n  T-PAMI 2019 Journal. arXiv version is slightly different from the camera\n  ready submission. arXiv admin note: text overlap with arXiv:1708.04398", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the task of dense 3D reconstruction of a complex dynamic\nscene from images. The prevailing idea to solve this task is composed of a\nsequence of steps and is dependent on the success of several pipelines in its\nexecution. To overcome such limitations with the existing algorithm, we propose\na unified approach to solve this problem. We assume that a dynamic scene can be\napproximated by numerous piecewise planar surfaces, where each planar surface\nenjoys its own rigid motion, and the global change in the scene between two\nframes is as-rigid-as-possible (ARAP). Consequently, our model of a dynamic\nscene reduces to a soup of planar structures and rigid motion of these local\nplanar structures. Using planar over-segmentation of the scene, we reduce this\ntask to solving a \"3D jigsaw puzzle\" problem. Hence, the task boils down to\ncorrectly assemble each rigid piece to construct a 3D shape that complies with\nthe geometry of the scene under the ARAP assumption. Further, we show that our\napproach provides an effective solution to the inherent scale-ambiguity in\nstructure-from-motion under perspective projection. We provide extensive\nexperimental results and evaluation on several benchmark datasets. Quantitative\ncomparison with competing approaches shows state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 20:16:06 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Kumar", "Suryansh", ""], ["Dai", "Yuchao", ""], ["Li", "Hongdong", ""]]}, {"id": "1911.09098", "submitter": "Pierrick Coupe", "authors": "Pierrick Coup\\'e, Boris Mansencal, Micha\\\"el Cl\\'ement, R\\'emi Giraud,\n  Baudouin Denis de Senneville, Vinh-Thong Ta, Vincent Lepetit, Jos\\'e V.\n  Manjon", "title": "AssemblyNet: A large ensemble of CNNs for 3D Whole Brain MRI\n  Segmentation", "comments": "arXiv admin note: substantial text overlap with arXiv:1906.01862", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whole brain segmentation using deep learning (DL) is a very challenging task\nsince the number of anatomical labels is very high compared to the number of\navailable training images. To address this problem, previous DL methods\nproposed to use a single convolution neural network (CNN) or few independent\nCNNs. In this paper, we present a novel ensemble method based on a large number\nof CNNs processing different overlapping brain areas. Inspired by parliamentary\ndecision-making systems, we propose a framework called AssemblyNet, made of two\n\"assemblies\" of U-Nets. Such a parliamentary system is capable of dealing with\ncomplex decisions, unseen problem and reaching a consensus quickly. AssemblyNet\nintroduces sharing of knowledge among neighboring U-Nets, an \"amendment\"\nprocedure made by the second assembly at higher-resolution to refine the\ndecision taken by the first one, and a final decision obtained by majority\nvoting. During our validation, AssemblyNet showed competitive performance\ncompared to state-of-the-art methods such as U-Net, Joint label fusion and\nSLANT. Moreover, we investigated the scan-rescan consistency and the robustness\nto disease effects of our method. These experiences demonstrated the\nreliability of AssemblyNet. Finally, we showed the interest of using\nsemi-supervised learning to improve the performance of our method.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 13:37:16 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Coup\u00e9", "Pierrick", ""], ["Mansencal", "Boris", ""], ["Cl\u00e9ment", "Micha\u00ebl", ""], ["Giraud", "R\u00e9mi", ""], ["de Senneville", "Baudouin Denis", ""], ["Ta", "Vinh-Thong", ""], ["Lepetit", "Vincent", ""], ["Manjon", "Jos\u00e9 V.", ""]]}, {"id": "1911.09099", "submitter": "Hyojin Park", "authors": "Hyojin Park, Lars Lowe Sj\\\"osund, YoungJoon Yoo, Nicolas Monet, Jihwan\n  Bang and Nojun Kwak", "title": "SINet: Extreme Lightweight Portrait Segmentation Networks with Spatial\n  Squeeze Modules and Information Blocking Decoder", "comments": "https://github.com/HYOJINPARK/ExtPortraitSeg. arXiv admin note: text\n  overlap with arXiv:1908.03093", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing a lightweight and robust portrait segmentation algorithm is an\nimportant task for a wide range of face applications. However, the problem has\nbeen considered as a subset of the object segmentation problem and less handled\nin the semantic segmentation field. Obviously, portrait segmentation has its\nunique requirements. First, because the portrait segmentation is performed in\nthe middle of a whole process of many real-world applications, it requires\nextremely lightweight models. Second, there has not been any public datasets in\nthis domain that contain a sufficient number of images with unbiased\nstatistics. To solve the first problem, we introduce the new extremely\nlightweight portrait segmentation model SINet, containing an information\nblocking decoder and spatial squeeze modules. The information blocking decoder\nuses confidence estimates to recover local spatial information without spoiling\nglobal consistency. The spatial squeeze module uses multiple receptive fields\nto cope with various sizes of consistency in the image. To tackle the second\nproblem, we propose a simple method to create additional portrait segmentation\ndata which can improve accuracy on the EG1800 dataset. In our qualitative and\nquantitative analysis on the EG1800 dataset, we show that our method\noutperforms various existing lightweight segmentation models. Our method\nreduces the number of parameters from 2.1M to 86.9K (around 95.9% reduction),\nwhile maintaining the accuracy under an 1% margin from the state-of-the-art\nportrait segmentation method. We also show our model is successfully executed\non a real mobile device with 100.6 FPS. In addition, we demonstrate that our\nmethod can be used for general semantic segmentation on the Cityscapes dataset.\nThe code and dataset are available in\nhttps://github.com/HYOJINPARK/ExtPortraitSeg .\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 15:39:24 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 10:44:57 GMT"}, {"version": "v3", "created": "Mon, 9 Dec 2019 16:29:20 GMT"}, {"version": "v4", "created": "Sun, 9 Feb 2020 05:17:09 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Park", "Hyojin", ""], ["Sj\u00f6sund", "Lars Lowe", ""], ["Yoo", "YoungJoon", ""], ["Monet", "Nicolas", ""], ["Bang", "Jihwan", ""], ["Kwak", "Nojun", ""]]}, {"id": "1911.09143", "submitter": "Xinshao Wang Mr", "authors": "Xinshao Wang, Elyor Kodirov, Yang Hua, Neil M. Robertson", "title": "ID-aware Quality for Set-based Person Re-identification", "comments": "A Set-based Person Re-identification Baseline: Simple Average Fusion\n  of Global Spatial Representations, without temporal information, without\n  parts/poses/attributes information", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Set-based person re-identification (SReID) is a matching problem that aims to\nverify whether two sets are of the same identity (ID). Existing SReID models\ntypically generate a feature representation per image and aggregate them to\nrepresent the set as a single embedding. However, they can easily be perturbed\nby noises--perceptually/semantically low quality images--which are inevitable\ndue to imperfect tracking/detection systems, or overfit to trivial images. In\nthis work, we present a novel and simple solution to this problem based on\nID-aware quality that measures the perceptual and semantic quality of images\nguided by their ID information. Specifically, we propose an ID-aware Embedding\nthat consists of two key components: (1) Feature learning attention that aims\nto learn robust image embeddings by focusing on 'medium' hard images. This way\nit can prevent overfitting to trivial images, and alleviate the influence of\noutliers. (2) Feature fusion attention is to fuse image embeddings in the set\nto obtain the set-level embedding. It ignores noisy information and pays more\nattention to discriminative images to aggregate more discriminative\ninformation. Experimental results on four datasets show that our method\noutperforms state-of-the-art approaches despite the simplicity of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 19:49:27 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Wang", "Xinshao", ""], ["Kodirov", "Elyor", ""], ["Hua", "Yang", ""], ["Robertson", "Neil M.", ""]]}, {"id": "1911.09168", "submitter": "Hamed Aghdam", "authors": "Hamed H. Aghdam, Abel Gonzalez-Garcia, Joost van de Weijer, Antonio M.\n  L\\'opez", "title": "Active Learning for Deep Detection Neural Networks", "comments": "Accepted at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cost of drawing object bounding boxes (i.e. labeling) for millions of\nimages is prohibitively high. For instance, labeling pedestrians in a regular\nurban image could take 35 seconds on average. Active learning aims to reduce\nthe cost of labeling by selecting only those images that are informative to\nimprove the detection network accuracy. In this paper, we propose a method to\nperform active learning of object detectors based on convolutional neural\nnetworks. We propose a new image-level scoring process to rank unlabeled images\nfor their automatic selection, which clearly outperforms classical scores. The\nproposed method can be applied to videos and sets of still images. In the\nformer case, temporal selection rules can complement our scoring process. As a\nrelevant use case, we extensively study the performance of our method on the\ntask of pedestrian detection. Overall, the experiments show that the proposed\nmethod performs better than random selection. Our codes are publicly available\nat www.gitlab.com/haghdam/deep_active_learning.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 20:57:44 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Aghdam", "Hamed H.", ""], ["Gonzalez-Garcia", "Abel", ""], ["van de Weijer", "Joost", ""], ["L\u00f3pez", "Antonio M.", ""]]}, {"id": "1911.09177", "submitter": "Ankit Desai", "authors": "Jekishan K. Parmar, Ankit Desai", "title": "Feature Extraction in Augmented Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Augmented Reality (AR) is used for various applications associated with the\nreal world. In this paper, first, describe characteristics and essential\nservices of AR. Brief history on Virtual Reality (VR) and AR is also mentioned\nin the introductory section. Then, AR Technologies along with its workflow is\ndepicted, which includes the complete AR Process consisting of the stages of\nImage Acquisition, Feature Extraction, Feature Matching, Geometric\nVerification, and Associated Information Retrieval. Feature extraction is the\nessence of AR hence its details are furnished in the paper.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 14:24:56 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Parmar", "Jekishan K.", ""], ["Desai", "Ankit", ""]]}, {"id": "1911.09178", "submitter": "Chengjiang Long", "authors": "Ling Zhang, Chengjiang Long, Xiaolong Zhang, Chunxia Xiao", "title": "RIS-GAN: Explore Residual and Illumination with Generative Adversarial\n  Networks for Shadow Removal", "comments": "The paper was accepted to the Thirty-Fourth AAAI Conference on\n  Artificial Intelligence (AAAI'2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual images and illumination estimation have been proved very helpful in\nimage enhancement. In this paper, we propose a general and novel framework\nRIS-GAN which explores residual and illumination with Generative Adversarial\nNetworks for shadow removal. Combined with the coarse shadow-removal image, the\nestimated negative residual images and inverse illumination maps can be used to\ngenerate indirect shadow-removal images to refine the coarse shadow-removal\nresult to the fine shadow-free image in a coarse-to-fine fashion. Three\ndiscriminators are designed to distinguish whether the predicted negative\nresidual images, shadow-removal images, and the inverse illumination maps are\nreal or fake jointly compared with the corresponding ground-truth information.\nTo our best knowledge, we are the first one to explore residual and\nillumination for shadow removal. We evaluate our proposed method on two\nbenchmark datasets, i.e., SRD and ISTD, and the extensive experiments\ndemonstrate that our proposed method achieves the superior performance to\nstate-of-the-arts, although we have no particular shadow-aware components\ndesigned in our generators.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 21:29:48 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 19:05:50 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Zhang", "Ling", ""], ["Long", "Chengjiang", ""], ["Zhang", "Xiaolong", ""], ["Xiao", "Chunxia", ""]]}, {"id": "1911.09188", "submitter": "Christopher George", "authors": "Christopher A. George and Bradley M. West", "title": "Localized Compression: Applying Convolutional Neural Networks to\n  Compressed Images", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the challenge of applying existing convolutional neural network\n(CNN) architectures to compressed images. Existing CNN architectures represent\nimages as a matrix of pixel intensities with a specified dimension; this\ndesired dimension is achieved by downgrading or cropping. Downgrading and\ncropping are attractive in that the result is also an image; however, an\nalgorithm producing an alternative \"compressed\" representation could yield\nbetter classification performance. This compression algorithm need not be\nreversible, but must be compatible with the CNN's operations. This problem is\nthus the counterpart of the well-studied problem of applying compressed CNNs to\nuncompressed images, which has attracted great interest as CNNs are deployed to\nsize-, weight-, and power- (SWaP)-limited devices. We introduce Localized\nCompression, a generalization of downgrading in which the original image is\ndivided into blocks and each block is compressed to a smaller size using either\nsampling- or random-matrix-based techniques. By aligning the size of the\ncompressed blocks with the size of the CNN's convolutional region, localized\ncompression can be made compatible with any CNN architecture. Our experimental\nresults show that Localized Compression results in classification accuracy\napproximately 1-2% higher than is achieved by downgrading to the equivalent\nresolution.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 21:55:36 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["George", "Christopher A.", ""], ["West", "Bradley M.", ""]]}, {"id": "1911.09199", "submitter": "Jingru Yi", "authors": "Jingru Yi, Hui Tang, Pengxiang Wu, Bo Liu, Daniel J. Hoeppner,\n  Dimitris N. Metaxas, Lianyi Han, Wei Fan", "title": "Object-Guided Instance Segmentation for Biological Images", "comments": "accepted to AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance segmentation of biological images is essential for studying object\nbehaviors and properties. The challenges, such as clustering, occlusion, and\nadhesion problems of the objects, make instance segmentation a non-trivial\ntask. Current box-free instance segmentation methods typically rely on local\npixel-level information. Due to a lack of global object view, these methods are\nprone to over- or under-segmentation. On the contrary, the box-based instance\nsegmentation methods incorporate object detection into the segmentation,\nperforming better in identifying the individual instances. In this paper, we\npropose a new box-based instance segmentation method. Mainly, we locate the\nobject bounding boxes from their center points. The object features are\nsubsequently reused in the segmentation branch as a guide to separate the\nclustered instances within an RoI patch. Along with the instance normalization,\nthe model is able to recover the target object distribution and suppress the\ndistribution of neighboring attached objects. Consequently, the proposed model\nperforms excellently in segmenting the clustered objects while retaining the\ntarget object details. The proposed method achieves state-of-the-art\nperformances on three biological datasets: cell nuclei, plant phenotyping\ndataset, and neural cells.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 22:38:35 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Yi", "Jingru", ""], ["Tang", "Hui", ""], ["Wu", "Pengxiang", ""], ["Liu", "Bo", ""], ["Hoeppner", "Daniel J.", ""], ["Metaxas", "Dimitris N.", ""], ["Han", "Lianyi", ""], ["Fan", "Wei", ""]]}, {"id": "1911.09204", "submitter": "Akshay Gadi Patil", "authors": "Jiongchao Jin, Akshay Gadi Patil, Zhang Xiong, Hao Zhang", "title": "DR-KFS: A Differentiable Visual Similarity Metric for 3D Shape\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a differential visual similarity metric to train deep neural\nnetworks for 3D reconstruction, aimed at improving reconstruction quality. The\nmetric compares two 3D shapes by measuring distances between multi-view images\ndifferentiably rendered from the shapes. Importantly, the image-space distance\nis also differentiable and measures visual similarity, rather than pixel-wise\ndistortion. Specifically, the similarity is defined by mean-squared errors over\nHardNet features computed from probabilistic keypoint maps of the compared\nimages. Our differential visual shape similarity metric can be easily plugged\ninto various 3D reconstruction networks, replacing their distortion-based\nlosses, such as Chamfer or Earth Mover distances, so as to optimize the network\nweights to produce reconstructions with better structural fidelity and visual\nquality. We demonstrate this both objectively, using well-known shape metrics\nfor retrieval and classification tasks that are independent from our new\nmetric, and subjectively through a perceptual study.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 22:57:51 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 21:34:27 GMT"}, {"version": "v3", "created": "Sat, 28 Mar 2020 20:29:58 GMT"}, {"version": "v4", "created": "Tue, 31 Mar 2020 18:16:52 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Jin", "Jiongchao", ""], ["Patil", "Akshay Gadi", ""], ["Xiong", "Zhang", ""], ["Zhang", "Hao", ""]]}, {"id": "1911.09217", "submitter": "Mat\\'ias Mendieta", "authors": "Christopher Neff, Mat\\'ias Mendieta, Shrey Mohan, Mohammadreza\n  Baharani, Samuel Rogers, Hamed Tabkhi", "title": "REVAMP$^2$T: Real-time Edge Video Analytics for Multi-camera\n  Privacy-aware Pedestrian Tracking", "comments": "Published as an article paper in IEEE Internet of Things Journal:\n  Special Issue on Privacy and Security in Distributed Edge Computing and\n  Evolving IoT", "journal-ref": null, "doi": "10.1109/JIOT.2019.2954804", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents REVAMP$^2$T, Real-time Edge Video Analytics for\nMulti-camera Privacy-aware Pedestrian Tracking, as an integrated end-to-end IoT\nsystem for privacy-built-in decentralized situational awareness. REVAMP$^2$T\npresents novel algorithmic and system constructs to push deep learning and\nvideo analytics next to IoT devices (i.e. video cameras). On the algorithm\nside, REVAMP$^2$T proposes a unified integrated computer vision pipeline for\ndetection, re-identification, and tracking across multiple cameras without the\nneed for storing the streaming data. At the same time, it avoids facial\nrecognition, and tracks and re-identifies pedestrians based on their key\nfeatures at runtime. On the IoT system side, REVAMP$^2$T provides\ninfrastructure to maximize hardware utilization on the edge, orchestrates\nglobal communications, and provides system-wide re-identification, without the\nuse of personally identifiable information, for a distributed IoT network. For\nthe results and evaluation, this article also proposes a new metric,\nAccuracy$\\cdot$Efficiency (\\AE), for holistic evaluation of IoT systems for\nreal-time video analytics based on accuracy, performance, and power efficiency.\nREVAMP$^2$T outperforms current state-of-the-art by as much as thirteen-fold\n\\AE~improvement.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 23:34:20 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 16:40:40 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Neff", "Christopher", ""], ["Mendieta", "Mat\u00edas", ""], ["Mohan", "Shrey", ""], ["Baharani", "Mohammadreza", ""], ["Rogers", "Samuel", ""], ["Tabkhi", "Hamed", ""]]}, {"id": "1911.09224", "submitter": "Yuqian Zhou", "authors": "Kuangxiao Gu, Yuqian Zhou, Thomas Huang", "title": "FLNet: Landmark Driven Fetching and Learning Network for Faithful\n  Talking Facial Animation Synthesis", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Talking face synthesis has been widely studied in either appearance-based or\nwarping-based methods. Previous works mostly utilize single face image as a\nsource, and generate novel facial animations by merging other person's facial\nfeatures. However, some facial regions like eyes or teeth, which may be hidden\nin the source image, can not be synthesized faithfully and stably. In this\npaper, We present a landmark driven two-stream network to generate faithful\ntalking facial animation, in which more facial details are created, preserved\nand transferred from multiple source images instead of a single one.\nSpecifically, we propose a network consisting of a learning and fetching\nstream. The fetching sub-net directly learns to attentively warp and merge\nfacial regions from five source images of distinctive landmarks, while the\nlearning pipeline renders facial organs from the training face space to\ncompensate. Compared to baseline algorithms, extensive experiments demonstrate\nthat the proposed method achieves a higher performance both quantitatively and\nqualitatively. Codes are at https://github.com/kgu3/FLNet_AAAI2020.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 00:07:43 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Gu", "Kuangxiao", ""], ["Zhou", "Yuqian", ""], ["Huang", "Thomas", ""]]}, {"id": "1911.09228", "submitter": "Weitang Liu", "authors": "Weitang Liu, Lifeng Wei, James Sharpnack, John D. Owens", "title": "Unsupervised Object Segmentation with Explicit Localization Module", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel architecture that iteratively discovers and\nsegments out the objects of a scene based on the image reconstruction quality.\nDifferent from other approaches, our model uses an explicit localization module\nthat localizes objects of the scene based on the pixel-level reconstruction\nqualities at each iteration, where simpler objects tend to be reconstructed\nbetter at earlier iterations and thus are segmented out first. We show that our\nlocalization module improves the quality of the segmentation, especially on a\nchallenging background.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 00:50:48 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Liu", "Weitang", ""], ["Wei", "Lifeng", ""], ["Sharpnack", "James", ""], ["Owens", "John D.", ""]]}, {"id": "1911.09243", "submitter": "Dongliang He", "authors": "Ya Wang, Dongliang He, Fu Li, Xiang Long, Zhichao Zhou, Jinwen Ma,\n  Shilei Wen", "title": "Multi-Label Classification with Label Graph Superimposing", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images or videos always contain multiple objects or actions. Multi-label\nrecognition has been witnessed to achieve pretty performance attribute to the\nrapid development of deep learning technologies. Recently, graph convolution\nnetwork (GCN) is leveraged to boost the performance of multi-label recognition.\nHowever, what is the best way for label correlation modeling and how feature\nlearning can be improved with label system awareness are still unclear. In this\npaper, we propose a label graph superimposing framework to improve the\nconventional GCN+CNN framework developed for multi-label recognition in the\nfollowing two aspects. Firstly, we model the label correlations by\nsuperimposing label graph built from statistical co-occurrence information into\nthe graph constructed from knowledge priors of labels, and then multi-layer\ngraph convolutions are applied on the final superimposed graph for label\nembedding abstraction. Secondly, we propose to leverage embedding of the whole\nlabel system for better representation learning. In detail, lateral connections\nbetween GCN and CNN are added at shallow, middle and deep layers to inject\ninformation of label system into backbone CNN for label-awareness in the\nfeature learning process. Extensive experiments are carried out on MS-COCO and\nCharades datasets, showing that our proposed solution can greatly improve the\nrecognition performance and achieves new state-of-the-art recognition\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 02:08:20 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Wang", "Ya", ""], ["He", "Dongliang", ""], ["Li", "Fu", ""], ["Long", "Xiang", ""], ["Zhou", "Zhichao", ""], ["Ma", "Jinwen", ""], ["Wen", "Shilei", ""]]}, {"id": "1911.09245", "submitter": "Diogo Luvizon", "authors": "Diogo C Luvizon, Hedi Tabia, David Picard", "title": "Consensus-based Optimization for 3D Human Pose Estimation in Camera\n  Coordinates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D human pose estimation is frequently seen as the task of estimating 3D\nposes relative to the root body joint. Alternatively, in this paper, we propose\na 3D human pose estimation method in camera coordinates, which allows effective\ncombination of 2D annotated data and 3D poses, as well as a straightforward\nmulti-view generalization. To that end, we cast the problem into a different\nperspective, where 3D poses are predicted in the image plane, in pixels, and\nthe absolute depth is estimated in millimeters. Based on this, we propose a\nconsensus-based optimization algorithm for multi-view predictions from\nuncalibrated images, which requires a single monocular training procedure. Our\nmethod improves the state-of-the-art on well known 3D human pose datasets,\nreducing the prediction error by 32% in the most common benchmark. In addition,\nwe also reported our results in absolute pose position error, achieving 80mm\nfor monocular estimations and 51mm for multi-view, on average.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 02:19:08 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 03:08:10 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Luvizon", "Diogo C", ""], ["Tabia", "Hedi", ""], ["Picard", "David", ""]]}, {"id": "1911.09249", "submitter": "Hasnine Haque", "authors": "Hasnine Haque, Masahiro Hashimoto, Nozomu Uetake, Masahiro Jinzaki", "title": "Semantic Segmentation of Thigh Muscle using 2.5D Deep Learning Network\n  Trained with Limited Datasets", "comments": "7 pages, 5 figures, This manuscript was a detailed version of our\n  accepted oral paper in RSNA 2018. Ref: Haque,H, Hashimoto,M, Uetake,N,\n  Jinzaki,M, End to End Solution for Complete Thigh Muscle Semantic\n  Segmentation from Musculoskeletal CT using Deep Learning.\n  http://archive.rsna.org/2018/18006583.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: We propose a 2.5D deep learning neural network (DLNN) to\nautomatically classify thigh muscle into 11 classes and evaluate its\nclassification accuracy over 2D and 3D DLNN when trained with limited datasets.\nEnables operator invariant quantitative assessment of the thigh muscle volume\nchange with respect to the disease progression. Materials and methods:\nRetrospective datasets consist of 48 thigh volume (TV) cropped from CT DICOM\nimages. Cropped volumes were aligned with femur axis and resample in 2 mm\nvoxel-spacing. Proposed 2.5D DLNN consists of three 2D U-Net trained with\naxial, coronal and sagittal muscle slices respectively. A voting algorithm was\nused to combine the output of U-Nets to create final segmentation. 2.5D U-Net\nwas trained on PC with 38 TV and the remaining 10 TV were used to evaluate\nsegmentation accuracy of 10 classes within Thigh. The result segmentation of\nboth left and right thigh were de-cropped to original CT volume space. Finally,\nsegmentation accuracies were compared between proposed DLNN and 2D/3D U-Net.\nResults: Average segmentation DSC score accuracy of all classes with 2.5D U-Net\nas 91.18% and Average Surface distance (ASD) accuracy as 0.84 mm. We found,\nmean DSC score for 2D U-Net was 3.3% lower than the that of 2.5D U-Net and mean\nDSC score of 3D U-Net was 5.7% lower than that of 2.5D U-Net when trained with\nsame datasets. Conclusion: We achieved a faster computationally efficient and\nautomatic segmentation of thigh muscle into 11 classes with reasonable\naccuracy. Enables quantitative evaluation of muscle atrophy with disease\nprogression.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 02:30:31 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Haque", "Hasnine", ""], ["Hashimoto", "Masahiro", ""], ["Uetake", "Nozomu", ""], ["Jinzaki", "Masahiro", ""]]}, {"id": "1911.09257", "submitter": "Alexander Wong", "authors": "Andrew Hryniowski and Alexander Wong", "title": "DeepLABNet: End-to-end Learning of Deep Radial Basis Networks with Fully\n  Learnable Basis Functions", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From fully connected neural networks to convolutional neural networks, the\nlearned parameters within a neural network have been primarily relegated to the\nlinear parameters (e.g., convolutional filters). The non-linear functions\n(e.g., activation functions) have largely remained, with few exceptions in\nrecent years, parameter-less, static throughout training, and seen limited\nvariation in design. Largely ignored by the deep learning community, radial\nbasis function (RBF) networks provide an interesting mechanism for learning\nmore complex non-linear activation functions in addition to the linear\nparameters in a network. However, the interest in RBF networks has waned over\ntime due to the difficulty of integrating RBFs into more complex deep neural\nnetwork architectures in a tractable and stable manner. In this work, we\npresent a novel approach that enables end-to-end learning of deep RBF networks\nwith fully learnable activation basis functions in an automatic and tractable\nmanner. We demonstrate that our approach for enabling the use of learnable\nactivation basis functions in deep neural networks, which we will refer to as\nDeepLABNet, is an effective tool for automated activation function learning\nwithin complex network architectures.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 03:06:15 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Hryniowski", "Andrew", ""], ["Wong", "Alexander", ""]]}, {"id": "1911.09265", "submitter": "Guo-Jun Qi", "authors": "Xiao Wang, Daisuke Kihara, Jiebo Luo, and Guo-Jun Qi", "title": "EnAET: A Self-Trained framework for Semi-Supervised and Supervised\n  Learning with Ensemble Transformations", "comments": "10 pages, 3 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been successfully applied to many real-world\napplications. However, such successes rely heavily on large amounts of labeled\ndata that is expensive to obtain. Recently, many methods for semi-supervised\nlearning have been proposed and achieved excellent performance. In this study,\nwe propose a new EnAET framework to further improve existing semi-supervised\nmethods with self-supervised information. To our best knowledge, all current\nsemi-supervised methods improve performance with prediction consistency and\nconfidence ideas. We are the first to explore the role of {\\bf self-supervised}\nrepresentations in {\\bf semi-supervised} learning under a rich family of\ntransformations. Consequently, our framework can integrate the self-supervised\ninformation as a regularization term to further improve {\\it all} current\nsemi-supervised methods. In the experiments, we use MixMatch, which is the\ncurrent state-of-the-art method on semi-supervised learning, as a baseline to\ntest the proposed EnAET framework. Across different datasets, we adopt the same\nhyper-parameters, which greatly improves the generalization ability of the\nEnAET framework. Experiment results on different datasets demonstrate that the\nproposed EnAET framework greatly improves the performance of current\nsemi-supervised algorithms. Moreover, this framework can also improve {\\bf\nsupervised learning} by a large margin, including the extremely challenging\nscenarios with only 10 images per class. The code and experiment records are\navailable in \\url{https://github.com/maple-research-lab/EnAET}.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 03:20:48 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 16:15:05 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Wang", "Xiao", ""], ["Kihara", "Daisuke", ""], ["Luo", "Jiebo", ""], ["Qi", "Guo-Jun", ""]]}, {"id": "1911.09267", "submitter": "Yujun Shen", "authors": "Ceyuan Yang, Yujun Shen, Bolei Zhou", "title": "Semantic Hierarchy Emerges in Deep Generative Representations for Scene\n  Synthesis", "comments": "15 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of Generative Adversarial Networks (GANs) in image\nsynthesis, there lacks enough understanding on what generative models have\nlearned inside the deep generative representations and how photo-realistic\nimages are able to be composed of the layer-wise stochasticity introduced in\nrecent GANs. In this work, we show that highly-structured semantic hierarchy\nemerges as variation factors from synthesizing scenes from the generative\nrepresentations in state-of-the-art GAN models, like StyleGAN and BigGAN. By\nprobing the layer-wise representations with a broad set of semantics at\ndifferent abstraction levels, we are able to quantify the causality between the\nactivations and semantics occurring in the output image. Such a quantification\nidentifies the human-understandable variation factors learned by GANs to\ncompose scenes. The qualitative and quantitative results further suggest that\nthe generative representations learned by the GANs with layer-wise latent codes\nare specialized to synthesize different hierarchical semantics: the early\nlayers tend to determine the spatial layout and configuration, the middle\nlayers control the categorical objects, and the later layers finally render the\nscene attributes as well as color scheme. Identifying such a set of\nmanipulatable latent variation factors facilitates semantic scene manipulation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 03:26:15 GMT"}, {"version": "v2", "created": "Fri, 29 Nov 2019 02:12:56 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2020 05:49:15 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Yang", "Ceyuan", ""], ["Shen", "Yujun", ""], ["Zhou", "Bolei", ""]]}, {"id": "1911.09279", "submitter": "Yunlong Wang", "authors": "Guang Jiang, Mengzhen Shi, Ying Su, Pengcheng An, Brian Y. Lim,\n  Yunlong Wang", "title": "NaMemo: Enhancing Lecturers' Interpersonal Competence of Remembering\n  Students' Names", "comments": "DIS '20 Companion", "journal-ref": null, "doi": "10.1145/3393914.3395860", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Addressing students by their names helps a teacher to start building rapport\nwith students and thus facilitates their classroom participation. However, this\nbasic yet effective skill has become rather challenging for university\nlecturers, who have to handle large-sized (sometimes exceeding 100) groups in\ntheir daily teaching. To enhance lecturers' competence in delivering\ninterpersonal interaction, we developed NaMemo, a real-time name-indicating\nsystem based on a dedicated face-recognition pipeline. This paper presents the\nsystem design, the pilot feasibility test, and our plan for the following\nstudy, which aims to evaluate NaMemo's impacts on learning and teaching, as\nwell as to probe design implications including privacy considerations.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 04:13:55 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 04:10:58 GMT"}, {"version": "v3", "created": "Tue, 17 Mar 2020 01:07:42 GMT"}, {"version": "v4", "created": "Sun, 19 Apr 2020 06:45:07 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Jiang", "Guang", ""], ["Shi", "Mengzhen", ""], ["Su", "Ying", ""], ["An", "Pengcheng", ""], ["Lim", "Brian Y.", ""], ["Wang", "Yunlong", ""]]}, {"id": "1911.09287", "submitter": "Adam Dziedzic", "authors": "Adam Dziedzic and John Paparrizos and Sanjay Krishnan and Aaron Elmore\n  and Michael Franklin", "title": "Band-limited Training and Inference for Convolutional Neural Networks", "comments": "Published at International Conference on Machine Learning (ICML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convolutional layers are core building blocks of neural network\narchitectures. In general, a convolutional filter applies to the entire\nfrequency spectrum of the input data. We explore artificially constraining the\nfrequency spectra of these filters and data, called band-limiting, during\ntraining. The frequency domain constraints apply to both the feed-forward and\nback-propagation steps. Experimentally, we observe that Convolutional Neural\nNetworks (CNNs) are resilient to this compression scheme and results suggest\nthat CNNs learn to leverage lower-frequency components. In particular, we\nfound: (1) band-limited training can effectively control the resource usage\n(GPU and memory); (2) models trained with band-limited layers retain high\nprediction accuracy; and (3) requires no modification to existing training\nalgorithms or neural network architectures to use unlike other compression\nschemes.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 04:43:02 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Dziedzic", "Adam", ""], ["Paparrizos", "John", ""], ["Krishnan", "Sanjay", ""], ["Elmore", "Aaron", ""], ["Franklin", "Michael", ""]]}, {"id": "1911.09288", "submitter": "Tal Golan", "authors": "Tal Golan, Prashant C. Raju, Nikolaus Kriegeskorte", "title": "Controversial stimuli: pitting neural networks against each other as\n  models of human recognition", "comments": null, "journal-ref": "Proceedings of the National Academy of Sciences. Nov 2020,\n  201912334", "doi": "10.1073/pnas.1912334117", "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distinct scientific theories can make similar predictions. To adjudicate\nbetween theories, we must design experiments for which the theories make\ndistinct predictions. Here we consider the problem of comparing deep neural\nnetworks as models of human visual recognition. To efficiently compare models'\nability to predict human responses, we synthesize controversial stimuli: images\nfor which different models produce distinct responses. We applied this approach\nto two visual recognition tasks, handwritten digits (MNIST) and objects in\nsmall natural images (CIFAR-10). For each task, we synthesized controversial\nstimuli to maximize the disagreement among models which employed different\narchitectures and recognition algorithms. Human subjects viewed hundreds of\nthese stimuli, as well as natural examples, and judged the probability of\npresence of each digit/object category in each image. We quantified how\naccurately each model predicted the human judgments. The best performing models\nwere a generative Analysis-by-Synthesis model (based on variational\nautoencoders) for MNIST and a hybrid discriminative-generative Joint Energy\nModel for CIFAR-10. These DNNs, which model the distribution of images,\nperformed better than purely discriminative DNNs, which learn only to map\nimages to labels. None of the candidate models fully explained the human\nresponses. Controversial stimuli generalize the concept of adversarial\nexamples, obviating the need to assume a ground-truth model. Unlike natural\nimages, controversial stimuli are not constrained to the stimulus distribution\nmodels are trained on, thus providing severe out-of-distribution tests that\nreveal the models' inductive biases. Controversial stimuli therefore provide\npowerful probes of discrepancies between models and human perception.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 04:55:41 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 04:47:43 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Golan", "Tal", ""], ["Raju", "Prashant C.", ""], ["Kriegeskorte", "Nikolaus", ""]]}, {"id": "1911.09290", "submitter": "Zhao Kang", "authors": "Zhao Kang, Wangtao Zhou, Zhitong Zhao, Junming Shao, Meng Han, Zenglin\n  Xu", "title": "Large-scale Multi-view Subspace Clustering in Linear Time", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A plethora of multi-view subspace clustering (MVSC) methods have been\nproposed over the past few years. Researchers manage to boost clustering\naccuracy from different points of view. However, many state-of-the-art MVSC\nalgorithms, typically have a quadratic or even cubic complexity, are\ninefficient and inherently difficult to apply at large scales. In the era of\nbig data, the computational issue becomes critical. To fill this gap, we\npropose a large-scale MVSC (LMVSC) algorithm with linear order complexity.\nInspired by the idea of anchor graph, we first learn a smaller graph for each\nview. Then, a novel approach is designed to integrate those graphs so that we\ncan implement spectral clustering on a smaller graph. Interestingly, it turns\nout that our model also applies to single-view scenario. Extensive experiments\non various large-scale benchmark data sets validate the effectiveness and\nefficiency of our approach with respect to state-of-the-art clustering methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 05:10:29 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Kang", "Zhao", ""], ["Zhou", "Wangtao", ""], ["Zhao", "Zhitong", ""], ["Shao", "Junming", ""], ["Han", "Meng", ""], ["Xu", "Zenglin", ""]]}, {"id": "1911.09296", "submitter": "Ritwik Gupta", "authors": "Ritwik Gupta, Richard Hosfelt, Sandra Sajeev, Nirav Patel, Bryce\n  Goodman, Jigar Doshi, Eric Heim, Howie Choset, Matthew Gaston", "title": "xBD: A Dataset for Assessing Building Damage from Satellite Imagery", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present xBD, a new, large-scale dataset for the advancement of change\ndetection and building damage assessment for humanitarian assistance and\ndisaster recovery research. Natural disaster response requires an accurate\nunderstanding of damaged buildings in an affected region. Current response\nstrategies require in-person damage assessments within 24-48 hours of a\ndisaster. Massive potential exists for using aerial imagery combined with\ncomputer vision algorithms to assess damage and reduce the potential danger to\nhuman life. In collaboration with multiple disaster response agencies, xBD\nprovides pre- and post-event satellite imagery across a variety of disaster\nevents with building polygons, ordinal labels of damage level, and\ncorresponding satellite metadata. Furthermore, the dataset contains bounding\nboxes and labels for environmental factors such as fire, water, and smoke. xBD\nis the largest building damage assessment dataset to date, containing 850,736\nbuilding annotations across 45,362 km\\textsuperscript{2} of imagery.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 05:30:13 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Gupta", "Ritwik", ""], ["Hosfelt", "Richard", ""], ["Sajeev", "Sandra", ""], ["Patel", "Nirav", ""], ["Goodman", "Bryce", ""], ["Doshi", "Jigar", ""], ["Heim", "Eric", ""], ["Choset", "Howie", ""], ["Gaston", "Matthew", ""]]}, {"id": "1911.09298", "submitter": "Ligong Han", "authors": "Ligong Han, Ruijiang Gao, Mun Kim, Xin Tao, Bo Liu, Dimitris Metaxas", "title": "Robust Conditional GAN from Uncertainty-Aware Pairwise Comparisons", "comments": "Accepted for spotlight at AAAI-20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Conditional generative adversarial networks have shown exceptional generation\nperformance over the past few years. However, they require large numbers of\nannotations. To address this problem, we propose a novel generative adversarial\nnetwork utilizing weak supervision in the form of pairwise comparisons (PC-GAN)\nfor image attribute editing. In the light of Bayesian uncertainty estimation\nand noise-tolerant adversarial training, PC-GAN can estimate attribute rating\nefficiently and demonstrate robust performance in noise resistance. Through\nextensive experiments, we show both qualitatively and quantitatively that\nPC-GAN performs comparably with fully-supervised methods and outperforms\nunsupervised baselines.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 05:37:46 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 00:08:23 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Han", "Ligong", ""], ["Gao", "Ruijiang", ""], ["Kim", "Mun", ""], ["Tao", "Xin", ""], ["Liu", "Bo", ""], ["Metaxas", "Dimitris", ""]]}, {"id": "1911.09299", "submitter": "Bingyuan Liu", "authors": "Bingyuan Liu, Jiantao Zhang, Xiaoting Zhang, Wei Zhang, Chuanhui Yu,\n  Yuan Zhou", "title": "Furnishing Your Room by What You See: An End-to-End Furniture Set\n  Retrieval Framework with Rich Annotated Benchmark Dataset", "comments": "project website :\n  https://www.kujiale.com/festatic/furnitureSetRetrieval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding interior scenes has attracted enormous interest in computer\nvision community. However, few works focus on the understanding of furniture\nwithin the scenes and a large-scale dataset is also lacked to advance the\nfield. In this paper, we first fill the gap by presenting DeepFurniture, a\nrichly annotated large indoor scene dataset, including 24k indoor images, 170k\nfurniture instances and 20k unique furniture identities. On the dataset, we\nintroduce a new benchmark, named furniture set retrieval. Given an indoor photo\nas input, the task requires to detect all the furniture instances and search a\nmatched set of furniture identities. To address this challenging task, we\npropose a feature and context embedding based framework. It contains 3 major\ncontributions: (1) An improved Mask-RCNN model with an additional mask-based\nclassifier is introduced for better utilizing the mask information to relieve\nthe occlusion problems in furniture detection context. (2) A multi-task style\nSiamese network is proposed to train the feature embedding model for retrieval,\nwhich is composed of a classification subnet supervised by self-clustered\npseudo attributes and a verification subnet to estimate whether the input pair\nis matched. (3) In order to model the relationship of the furniture entities in\nan interior design, a context embedding model is employed to re-rank the\nretrieval results. Extensive experiments demonstrate the effectiveness of each\nmodule and the overall system.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 05:42:19 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 10:50:06 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Liu", "Bingyuan", ""], ["Zhang", "Jiantao", ""], ["Zhang", "Xiaoting", ""], ["Zhang", "Wei", ""], ["Yu", "Chuanhui", ""], ["Zhou", "Yuan", ""]]}, {"id": "1911.09301", "submitter": "Nishi Doshi", "authors": "Nishi Doshi, Gitam Shikhenawis, Suman K Mitra", "title": "Image Aesthetics Assessment using Multi Channel Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": "Paper ID 33", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image Aesthetics Assessment is one of the emerging domains in research. The\ndomain deals with classification of images into categories depending on the\nbasis of how pleasant they are for the users to watch. In this article, the\nfocus is on categorizing the images in high quality and low quality image. Deep\nconvolutional neural networks are used to classify the images. Instead of using\njust the raw image as input, different crops and saliency maps of the images\nare also used, as input to the proposed multi channel CNN architecture. The\nexperiments reported on widely used AVA database show improvement in the\naesthetic assessment performance over existing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 05:57:44 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Doshi", "Nishi", ""], ["Shikhenawis", "Gitam", ""], ["Mitra", "Suman K", ""]]}, {"id": "1911.09318", "submitter": "Hyunjong Park", "authors": "Hyunjong Park, Bumsub Ham", "title": "Relation Network for Person Re-identification", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (reID) aims at retrieving an image of the person of\ninterest from a set of images typically captured by multiple cameras. Recent\nreID methods have shown that exploiting local features describing body parts,\ntogether with a global feature of a person image itself, gives robust feature\nrepresentations, even in the case of missing body parts. However, using the\nindividual part-level features directly, without considering relations between\nbody parts, confuses differentiating identities of different persons having\nsimilar attributes in corresponding parts. To address this issue, we propose a\nnew relation network for person reID that considers relations between\nindividual body parts and the rest of them. Our model makes a single part-level\nfeature incorporate partial information of other body parts as well, supporting\nit to be more discriminative. We also introduce a global contrastive pooling\n(GCP) method to obtain a global feature of a person image. We propose to use\ncontrastive features for GCP to complement conventional max and averaging\npooling techniques. We show that our model outperforms the state of the art on\nthe Market1501, DukeMTMC-reID and CUHK03 datasets, demonstrating the\neffectiveness of our approach on discriminative person representations.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 07:21:11 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 06:37:10 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Park", "Hyunjong", ""], ["Ham", "Bumsub", ""]]}, {"id": "1911.09322", "submitter": "Minje Park", "authors": "Minje Park", "title": "Data Proxy Generation for Fast and Efficient Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the recent advances on Neural Architecture Search (NAS), it gains\npopularity in designing best networks for specific tasks. Although it shows\npromising results on many benchmarks and competitions, NAS still suffers from\nits demanding computation cost for searching high dimensional architectural\ndesign space, and this problem becomes even worse when we want to use a\nlarge-scale dataset. If we can make a reliable data proxy for NAS, the\nefficiency of NAS approaches increase accordingly. Our basic observation for\nmaking a data proxy is that each example in a specific dataset has a different\nimpact on NAS process and most of examples are redundant from a relative\naccuracy ranking perspective, which we should preserve when making a data\nproxy. We propose a systematic approach to measure the importance of each\nexample from this relative accuracy ranking point of view, and make a reliable\ndata proxy based on the statistics of training and testing examples. Our\nexperiment shows that we can preserve the almost same relative accuracy ranking\nbetween all possible network configurations even with 10-20$\\times$ smaller\ndata proxy.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 07:39:57 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Park", "Minje", ""]]}, {"id": "1911.09325", "submitter": "Yafeng Liu", "authors": "Liu Yafeng, Chen Tian, Liu Zhongyu, Zhang Lei, Hu Yanjun and Ding\n  Enjie", "title": "Simultaneous Implementation Features Extraction and Recognition Using\n  C3D Network for WiFi-based Human Activity Recognition", "comments": "11 pages, 8 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human actions recognition has attracted more and more people's attention.\nMany technology have been developed to express human action's features, such as\nimage, skeleton-based, and channel state information(CSI). Among them, on\naccount of CSI's easy to be equipped and undemanding for light, and it has\ngained more and more attention in some special scene. However, the relationship\nbetween CSI signal and human actions is very complex, and some preliminary work\nmust be done to make CSI features easy to understand for computer. Nowadays,\nmany work departed CSI-based features' action dealing into two parts. One part\nis for features extraction and dimension reduce, and the other part is for time\nseries problems. Some of them even omitted one of the two part work. Therefore,\nthe accuracies of current recognition systems are far from satisfactory. In\nthis paper, we propose a new deep learning based approach, i.e. C3D network and\nC3D network with attention mechanism, for human actions recognition using CSI\nsignals. This kind of network can make feature extraction from spatial\nconvolution and temporal convolution simultaneously, and through this network\nthe two part of CSI-based human actions recognition mentioned above can be\nrealized at the same time. The entire algorithm structure is simplified. The\nexperimental results show that our proposed C3D network is able to achieve the\nbest recognition performance for all activities when compared with some\nbenchmark approaches.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 07:45:46 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Yafeng", "Liu", ""], ["Tian", "Chen", ""], ["Zhongyu", "Liu", ""], ["Lei", "Zhang", ""], ["Yanjun", "Hu", ""], ["Enjie", "Ding", ""]]}, {"id": "1911.09326", "submitter": "Quang-Hieu Pham", "authors": "Quang-Hieu Pham, Mikaela Angelina Uy, Binh-Son Hua, Duc Thanh Nguyen,\n  Gemma Roig, Sai-Kit Yeung", "title": "LCD: Learned Cross-Domain Descriptors for 2D-3D Matching", "comments": "Accepted to AAAI 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel method to learn a local cross-domain\ndescriptor for 2D image and 3D point cloud matching. Our proposed method is a\ndual auto-encoder neural network that maps 2D and 3D input into a shared latent\nspace representation. We show that such local cross-domain descriptors in the\nshared embedding are more discriminative than those obtained from individual\ntraining in 2D and 3D domains. To facilitate the training process, we built a\nnew dataset by collecting $\\approx 1.4$ millions of 2D-3D correspondences with\nvarious lighting conditions and settings from publicly available RGB-D scenes.\nOur descriptor is evaluated in three main experiments: 2D-3D matching,\ncross-domain retrieval, and sparse-to-dense depth estimation. Experimental\nresults confirm the robustness of our approach as well as its competitive\nperformance not only in solving cross-domain tasks but also in being able to\ngeneralize to solve sole 2D and 3D tasks. Our dataset and code are released\npublicly at \\url{https://hkust-vgd.github.io/lcd}.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 07:56:13 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Pham", "Quang-Hieu", ""], ["Uy", "Mikaela Angelina", ""], ["Hua", "Binh-Son", ""], ["Nguyen", "Duc Thanh", ""], ["Roig", "Gemma", ""], ["Yeung", "Sai-Kit", ""]]}, {"id": "1911.09332", "submitter": "Muhammad Usman", "authors": "Shakeel Muhammad Ibrahim, Muhammad Sohail Ibrahim, Muhammad Usman,\n  Imran Naseem and Muhammad Moinuddin", "title": "Heart Segmentation From MRI Scans Using Convolutional Neural Network", "comments": "Accepted for oral presentation at 13th International Conference -\n  Mathematics, Actuarial, Computer Science & Statistics (MACS 13) at IoBM,\n  Karachi, Pakistan", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heart is one of the vital organs of human body. A minor dysfunction of heart\neven for a short time interval can be fatal, therefore, efficient monitoring of\nits physiological state is essential for the patients with cardiovascular\ndiseases. In the recent past, various computer assisted medical imaging systems\nhave been proposed for the segmentation of the organ of interest. However, for\nthe segmentation of heart using MRI, only few methods have been proposed each\nwith its own merits and demerits. For further advancement in this area of\nresearch, we analyze automated heart segmentation methods for magnetic\nresonance images. The analysis are based on deep learning methods that\nprocesses a full MR scan in a slice by slice fashion to predict desired mask\nfor heart region. We design two encoder decoder type fully convolutional neural\nnetwork models\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 08:20:48 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Ibrahim", "Shakeel Muhammad", ""], ["Ibrahim", "Muhammad Sohail", ""], ["Usman", "Muhammad", ""], ["Naseem", "Imran", ""], ["Moinuddin", "Muhammad", ""]]}, {"id": "1911.09338", "submitter": "Chuyuan Xiong", "authors": "Chuyuan Xiong, Deyuan Zhang, Tao Liu, Xiaoyong Du", "title": "Voice-Face Cross-modal Matching and Retrieval: A Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal associations between voice and face from a person can be learnt\nalgorithmically, which can benefit a lot of applications. The problem can be\ndefined as voice-face matching and retrieval tasks. Much research attention has\nbeen paid on these tasks recently. However, this research is still in the early\nstage. Test schemes based on random tuple mining tend to have low test\nconfidence. Generalization ability of models can not be evaluated by small\nscale datasets. Performance metrics on various tasks are scarce. A benchmark\nfor this problem needs to be established. In this paper, first, a framework\nbased on comprehensive studies is proposed for voice-face matching and\nretrieval. It achieves state-of-the-art performance with various performance\nmetrics on different tasks and with high test confidence on large scale\ndatasets, which can be taken as a baseline for the follow-up research. In this\nframework, a voice anchored L2-Norm constrained metric space is proposed, and\ncross-modal embeddings are learned with CNN-based networks and triplet loss in\nthe metric space. The embedding learning process can be more effective and\nefficient with this strategy. Different network structures of the framework and\nthe cross language transfer abilities of the model are also analyzed. Second, a\nvoice-face dataset (with 1.15M face data and 0.29M audio data) from Chinese\nspeakers is constructed, and a convenient and quality controllable dataset\ncollection tool is developed. The dataset and source code of the paper will be\npublished together with this paper.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 08:34:34 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2019 12:36:50 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Xiong", "Chuyuan", ""], ["Zhang", "Deyuan", ""], ["Liu", "Tao", ""], ["Du", "Xiaoyong", ""]]}, {"id": "1911.09345", "submitter": "Naveed Akhtar Dr.", "authors": "Nayyer Aafaq, Naveed Akhtar, Wei Liu, Ajmal Mian", "title": "Empirical Autopsy of Deep Video Captioning Frameworks", "comments": "09 pages, 05 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary deep learning based video captioning follows encoder-decoder\nframework. In encoder, visual features are extracted with 2D/3D Convolutional\nNeural Networks (CNNs) and a transformed version of those features is passed to\nthe decoder. The decoder uses word embeddings and a language model to map\nvisual features to natural language captions. Due to its composite nature, the\nencoder-decoder pipeline provides the freedom of multiple choices for each of\nits components, e.g the choices of CNNs models, feature transformations, word\nembeddings, and language models etc. Component selection can have drastic\neffects on the overall video captioning performance. However, current\nliterature is void of any systematic investigation in this regard. This article\nfills this gap by providing the first thorough empirical analysis of the role\nthat each major component plays in a contemporary video captioning pipeline. We\nperform extensive experiments by varying the constituent components of the\nvideo captioning framework, and quantify the performance gains that are\npossible by mere component selection. We use the popular MSVD dataset as the\ntest-bed, and demonstrate that substantial performance gains are possible by\ncareful selection of the constituent components without major changes to the\npipeline itself. These results are expected to provide guiding principles for\nfuture research in the fast growing direction of video captioning.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 08:47:36 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Aafaq", "Nayyer", ""], ["Akhtar", "Naveed", ""], ["Liu", "Wei", ""], ["Mian", "Ajmal", ""]]}, {"id": "1911.09349", "submitter": "Di Xie", "authors": "Jiaxu Chen and Jing Hao and Kai Chen and Di Xie and Shicai Yang and\n  Shiliang Pu", "title": "An End-to-End Audio Classification System based on Raw Waveforms and\n  Mix-Training Strategy", "comments": "InterSpeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio classification can distinguish different kinds of sounds, which is\nhelpful for intelligent applications in daily life. However, it remains a\nchallenging task since the sound events in an audio clip is probably multiple,\neven overlapping. This paper introduces an end-to-end audio classification\nsystem based on raw waveforms and mix-training strategy. Compared to\nhuman-designed features which have been widely used in existing research, raw\nwaveforms contain more complete information and are more appropriate for\nmulti-label classification. Taking raw waveforms as input, our network consists\nof two variants of ResNet structure which can learn a discriminative\nrepresentation. To explore the information in intermediate layers, a\nmulti-level prediction with attention structure is applied in our model.\nFurthermore, we design a mix-training strategy to break the performance\nlimitation caused by the amount of training data. Experiments show that the\nmean average precision of the proposed audio classification system on Audio Set\ndataset is 37.2%. Without using extra training data, our system exceeds the\nstate-of-the-art multi-level attention model.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 08:54:48 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Chen", "Jiaxu", ""], ["Hao", "Jing", ""], ["Chen", "Kai", ""], ["Xie", "Di", ""], ["Yang", "Shicai", ""], ["Pu", "Shiliang", ""]]}, {"id": "1911.09358", "submitter": "Mingtao Fu", "authors": "Yongchao Xu, Mingtao Fu, Qimeng Wang, Yukang Wang, Kai Chen, Gui-Song\n  Xia, Xiang Bai", "title": "Gliding vertex on the horizontal bounding box for multi-oriented object\n  detection", "comments": "Accepted by TPAMI 2020. The experiments of pedestrian detection are\n  updated as the benchmark has been changed", "journal-ref": null, "doi": "10.1109/TPAMI.2020.2974745", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection has recently experienced substantial progress. Yet, the\nwidely adopted horizontal bounding box representation is not appropriate for\nubiquitous oriented objects such as objects in aerial images and scene texts.\nIn this paper, we propose a simple yet effective framework to detect\nmulti-oriented objects. Instead of directly regressing the four vertices, we\nglide the vertex of the horizontal bounding box on each corresponding side to\naccurately describe a multi-oriented object. Specifically, We regress four\nlength ratios characterizing the relative gliding offset on each corresponding\nside. This may facilitate the offset learning and avoid the confusion issue of\nsequential label points for oriented objects. To further remedy the confusion\nissue for nearly horizontal objects, we also introduce an obliquity factor\nbased on area ratio between the object and its horizontal bounding box, guiding\nthe selection of horizontal or oriented detection for each object. We add these\nfive extra target variables to the regression head of faster R-CNN, which\nrequires ignorable extra computation time. Extensive experimental results\ndemonstrate that without bells and whistles, the proposed method achieves\nsuperior performances on multiple multi-oriented object detection benchmarks\nincluding object detection in aerial images, scene text detection, pedestrian\ndetection in fisheye images.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 09:28:14 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 07:09:37 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Xu", "Yongchao", ""], ["Fu", "Mingtao", ""], ["Wang", "Qimeng", ""], ["Wang", "Yukang", ""], ["Chen", "Kai", ""], ["Xia", "Gui-Song", ""], ["Bai", "Xiang", ""]]}, {"id": "1911.09375", "submitter": "Monika Sharma", "authors": "Monika Sharma, Shikha Gupta, Arindam Chowdhury, Lovekesh Vig", "title": "ChartNet: Visual Reasoning over Statistical Charts using MAC-Networks", "comments": null, "journal-ref": "International Joint Conference on Neural Networks (IJCNN) 2019", "doi": "10.1109/IJCNN.2019.8852427", "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the improvements in perception accuracies brought about via deep\nlearning, developing systems combining accurate visual perception with the\nability to reason over the visual percepts remains extremely challenging. A\nparticular application area of interest from an accessibility perspective is\nthat of reasoning over statistical charts such as bar and pie charts. To this\nend, we formulate the problem of reasoning over statistical charts as a\nclassification task using MAC-Networks to give answers from a predefined\nvocabulary of generic answers. Additionally, we enhance the capabilities of\nMAC-Networks to give chart-specific answers to open-ended questions by\nreplacing the classification layer by a regression layer to localize the\ntextual answers present over the images. We call our network ChartNet, and\ndemonstrate its efficacy on predicting both in vocabulary and out of vocabulary\nanswers. To test our methods, we generated our own dataset of statistical chart\nimages and corresponding question answer pairs. Results show that ChartNet\nconsistently outperform other state-of-the-art methods on reasoning over these\nquestions and may be a viable candidate for applications containing images of\nstatistical charts.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 10:03:25 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Sharma", "Monika", ""], ["Gupta", "Shikha", ""], ["Chowdhury", "Arindam", ""], ["Vig", "Lovekesh", ""]]}, {"id": "1911.09389", "submitter": "Yanting Pei", "authors": "Yanting Pei, Yaping Huang, Xingyuan Zhang", "title": "Classification-driven Single Image Dehazing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing dehazing algorithms often use hand-crafted features or\nConvolutional Neural Networks (CNN)-based methods to generate clear images\nusing pixel-level Mean Square Error (MSE) loss. The generated images generally\nhave better visual appeal, but not always have better performance for\nhigh-level vision tasks, e.g. image classification. In this paper, we\ninvestigate a new point of view in addressing this problem. Instead of focusing\nonly on achieving good quantitative performance on pixel-based metrics such as\nPeak Signal to Noise Ratio (PSNR), we also ensure that the dehazed image itself\ndoes not degrade the performance of the high-level vision tasks such as image\nclassification. To this end, we present an unified CNN architecture that\nincludes three parts: a dehazing sub-network (DNet), a classification-driven\nConditional Generative Adversarial Networks sub-network (CCGAN) and a\nclassification sub-network (CNet) related to image classification, which has\nbetter performance both on visual appeal and image classification. We conduct\ncomprehensive experiments on two challenging benchmark datasets for\nfine-grained and object classification: CUB-200-2011 and Caltech-256.\nExperimental results demonstrate that the proposed method outperforms many\nrecent state-of-the-art single image dehazing methods in terms of image\ndehazing metrics and classification accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 10:25:54 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Pei", "Yanting", ""], ["Huang", "Yaping", ""], ["Zhang", "Xingyuan", ""]]}, {"id": "1911.09401", "submitter": "Kai Xie", "authors": "Ying Wen, Kai Xie, Lianghua He", "title": "Segmenting Medical MRI via Recurrent Decoding Cell", "comments": "8 pages, 7 figures, AAAI-20", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The encoder-decoder networks are commonly used in medical image segmentation\ndue to their remarkable performance in hierarchical feature fusion. However,\nthe expanding path for feature decoding and spatial recovery does not consider\nthe long-term dependency when fusing feature maps from different layers, and\nthe universal encoder-decoder network does not make full use of the\nmulti-modality information to improve the network robustness especially for\nsegmenting medical MRI. In this paper, we propose a novel feature fusion unit\ncalled Recurrent Decoding Cell (RDC) which leverages convolutional RNNs to\nmemorize the long-term context information from the previous layers in the\ndecoding phase. An encoder-decoder network, named Convolutional Recurrent\nDecoding Network (CRDN), is also proposed based on RDC for segmenting\nmulti-modality medical MRI. CRDN adopts CNN backbone to encode image features\nand decode them hierarchically through a chain of RDCs to obtain the final\nhigh-resolution score map. The evaluation experiments on BrainWeb, MRBrainS and\nHVSMR datasets demonstrate that the introduction of RDC effectively improves\nthe segmentation accuracy as well as reduces the model size, and the proposed\nCRDN owns its robustness to image noise and intensity non-uniformity in medical\nMRI.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 10:46:42 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Wen", "Ying", ""], ["Xie", "Kai", ""], ["He", "Lianghua", ""]]}, {"id": "1911.09418", "submitter": "Yunteng Luan", "authors": "Yunteng Luan, Hanyu Zhao, Zhi Yang, Yafei Dai", "title": "MSD: Multi-Self-Distillation Learning via Multi-classifiers within Deep\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the development of neural networks, more and more deep neural networks are\nadopted in various tasks, such as image classification. However, as the huge\ncomputational overhead, these networks could not be applied on mobile devices\nor other low latency scenes. To address this dilemma, multi-classifier\nconvolutional network is proposed to allow faster inference via early\nclassifiers with the corresponding classifiers. These networks utilize\nsophisticated designing to increase the early classifier accuracy. However,\nnaively training the multi-classifier network could hurt the performance\n(accuracy) of deep neural networks as early classifiers throughout interfere\nwith the feature generation process.\n  In this paper, we propose a general training framework named\nmulti-self-distillation learning (MSD), which mining knowledge of different\nclassifiers within the same network and increase every classifier accuracy. Our\napproach can be applied not only to multi-classifier networks, but also modern\nCNNs (e.g., ResNet Series) augmented with additional side branch classifiers.\nWe use sampling-based branch augmentation technique to transform a\nsingle-classifier network into a multi-classifier network. This reduces the gap\nof capacity between different classifiers, and improves the effectiveness of\napplying MSD. Our experiments show that MSD improves the accuracy of various\nnetworks: enhancing the accuracy of every classifier significantly for existing\nmulti-classifier network (MSDNet), improving vanilla single-classifier networks\nwith internal classifiers with high accuracy, while also improving the final\naccuracy.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 11:35:50 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 08:52:14 GMT"}, {"version": "v3", "created": "Mon, 2 Dec 2019 12:04:32 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Luan", "Yunteng", ""], ["Zhao", "Hanyu", ""], ["Yang", "Zhi", ""], ["Dai", "Yafei", ""]]}, {"id": "1911.09428", "submitter": "Zhengyang Lu", "authors": "Zhengyang Lu, Ying Chen", "title": "Single Image Super Resolution based on a Modified U-net with Mixed\n  Gradient Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image super-resolution (SISR) is the task of inferring a\nhigh-resolution image from a single low-resolution image. Recent research on\nsuper-resolution has achieved great progress due to the development of deep\nconvolutional neural networks in the field of computer vision. Existing\nsuper-resolution reconstruction methods have high performances in the criterion\nof Mean Square Error (MSE) but most methods fail to reconstruct an image with\nshape edges. To solve this problem, the mixed gradient error, which is composed\nby MSE and a weighted mean gradient error, is proposed in this work and applied\nto a modified U-net network as the loss function. The modified U-net removes\nall batch normalization layers and one of the convolution layers in each block.\nThe operation reduces the number of parameters, and therefore accelerates the\nreconstruction. Compared with the existing image super-resolution algorithms,\nthe proposed reconstruction method has better performance and time consumption.\nThe experiments demonstrate that modified U-net network architecture with mixed\ngradient loss yields high-level results on three image datasets: SET14, BSD300,\nICDAR2003. Code is available online.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 12:02:38 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Lu", "Zhengyang", ""], ["Chen", "Ying", ""]]}, {"id": "1911.09435", "submitter": "Limin Wang", "authors": "Zhaoyang Liu, Donghao Luo, Yabiao Wang, Limin Wang, Ying Tai, Chengjie\n  Wang, Jilin Li, Feiyue Huang, Tong Lu", "title": "TEINet: Towards an Efficient Architecture for Video Recognition", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficiency is an important issue in designing video architectures for action\nrecognition. 3D CNNs have witnessed remarkable progress in action recognition\nfrom videos. However, compared with their 2D counterparts, 3D convolutions\noften introduce a large amount of parameters and cause high computational cost.\nTo relieve this problem, we propose an efficient temporal module, termed as\nTemporal Enhancement-and-Interaction (TEI Module), which could be plugged into\nthe existing 2D CNNs (denoted by TEINet). The TEI module presents a different\nparadigm to learn temporal features by decoupling the modeling of channel\ncorrelation and temporal interaction. First, it contains a Motion Enhanced\nModule (MEM) which is to enhance the motion-related features while suppress\nirrelevant information (e.g., background). Then, it introduces a Temporal\nInteraction Module (TIM) which supplements the temporal contextual information\nin a channel-wise manner. This two-stage modeling scheme is not only able to\ncapture temporal structure flexibly and effectively, but also efficient for\nmodel inference. We conduct extensive experiments to verify the effectiveness\nof TEINet on several benchmarks (e.g., Something-Something V1&V2, Kinetics,\nUCF101 and HMDB51). Our proposed TEINet can achieve a good recognition accuracy\non these datasets but still preserve a high efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 12:16:32 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Liu", "Zhaoyang", ""], ["Luo", "Donghao", ""], ["Wang", "Yabiao", ""], ["Wang", "Limin", ""], ["Tai", "Ying", ""], ["Wang", "Chengjie", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""], ["Lu", "Tong", ""]]}, {"id": "1911.09449", "submitter": "Xingxing Wei", "authors": "Zhipeng Wei, Jingjing Chen, Xingxing Wei, Linxi Jiang, Tat-Seng Chua,\n  Fengfeng Zhou, Yu-Gang Jiang", "title": "Heuristic Black-box Adversarial Attacks on Video Recognition Models", "comments": "AAAI-2020 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of attacking video recognition models in the black-box\nsetting, where the model information is unknown and the adversary can only make\nqueries to detect the predicted top-1 class and its probability. Compared with\nthe black-box attack on images, attacking videos is more challenging as the\ncomputation cost for searching the adversarial perturbations on a video is much\nhigher due to its high dimensionality. To overcome this challenge, we propose a\nheuristic black-box attack model that generates adversarial perturbations only\non the selected frames and regions. More specifically, a heuristic-based\nalgorithm is proposed to measure the importance of each frame in the video\ntowards generating the adversarial examples. Based on the frames' importance,\nthe proposed algorithm heuristically searches a subset of frames where the\ngenerated adversarial example has strong adversarial attack ability while keeps\nthe perturbations lower than the given bound. Besides, to further boost the\nattack efficiency, we propose to generate the perturbations only on the salient\nregions of the selected frames. In this way, the generated perturbations are\nsparse in both temporal and spatial domains. Experimental results of attacking\ntwo mainstream video recognition methods on the UCF-101 dataset and the HMDB-51\ndataset demonstrate that the proposed heuristic black-box adversarial attack\nmethod can significantly reduce the computation cost and lead to more than 28\\%\nreduction in query numbers for the untargeted attack on both datasets.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 13:04:33 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Wei", "Zhipeng", ""], ["Chen", "Jingjing", ""], ["Wei", "Xingxing", ""], ["Jiang", "Linxi", ""], ["Chua", "Tat-Seng", ""], ["Zhou", "Fengfeng", ""], ["Jiang", "Yu-Gang", ""]]}, {"id": "1911.09450", "submitter": "Haoli Bai", "authors": "Haoli Bai, Jiaxiang Wu, Irwin King, Michael Lyu", "title": "Few Shot Network Compression via Cross Distillation", "comments": "AAAI 2020", "journal-ref": "The Thirty-Fourth AAAI Conference on Artificial Intelligence\n  (AAAI), 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model compression has been widely adopted to obtain light-weighted deep\nneural networks. Most prevalent methods, however, require fine-tuning with\nsufficient training data to ensure accuracy, which could be challenged by\nprivacy and security issues. As a compromise between privacy and performance,\nin this paper we investigate few shot network compression: given few samples\nper class, how can we effectively compress the network with negligible\nperformance drop? The core challenge of few shot network compression lies in\nhigh estimation errors from the original network during inference, since the\ncompressed network can easily over-fits on the few training instances. The\nestimation errors could propagate and accumulate layer-wisely and finally\ndeteriorate the network output. To address the problem, we propose cross\ndistillation, a novel layer-wise knowledge distillation approach. By\ninterweaving hidden layers of teacher and student network, layer-wisely\naccumulated estimation errors can be effectively reduced.The proposed method\noffers a general framework compatible with prevalent network compression\ntechniques such as pruning. Extensive experiments on benchmark datasets\ndemonstrate that cross distillation can significantly improve the student\nnetwork's accuracy when only a few training instances are available.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 13:07:52 GMT"}, {"version": "v2", "created": "Sat, 2 May 2020 10:20:20 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Bai", "Haoli", ""], ["Wu", "Jiaxiang", ""], ["King", "Irwin", ""], ["Lyu", "Michael", ""]]}, {"id": "1911.09464", "submitter": "Xu Shen", "authors": "Jiwei Yang, Xu Shen, Jun Xing, Xinmei Tian, Houqiang Li, Bing Deng,\n  Jianqiang Huang, Xiansheng Hua", "title": "Quantization Networks", "comments": "10 pages, CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep neural networks are highly effective, their high computational\nand memory costs severely challenge their applications on portable devices. As\na consequence, low-bit quantization, which converts a full-precision neural\nnetwork into a low-bitwidth integer version, has been an active and promising\nresearch topic. Existing methods formulate the low-bit quantization of networks\nas an approximation or optimization problem. Approximation-based methods\nconfront the gradient mismatch problem, while optimization-based methods are\nonly suitable for quantizing weights and could introduce high computational\ncost in the training stage. In this paper, we propose a novel perspective of\ninterpreting and implementing neural network quantization by formulating\nlow-bit quantization as a differentiable non-linear function (termed\nquantization function). The proposed quantization function can be learned in a\nlossless and end-to-end manner and works for any weights and activations of\nneural networks in a simple and uniform way. Extensive experiments on image\nclassification and object detection tasks show that our quantization networks\noutperform the state-of-the-art methods. We believe that the proposed method\nwill shed new insights on the interpretation of neural network quantization.\nOur code is available at\nhttps://github.com/aliyun/alibabacloud-quantization-networks.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 13:44:03 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 02:37:31 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Yang", "Jiwei", ""], ["Shen", "Xu", ""], ["Xing", "Jun", ""], ["Tian", "Xinmei", ""], ["Li", "Houqiang", ""], ["Deng", "Bing", ""], ["Huang", "Jianqiang", ""], ["Hua", "Xiansheng", ""]]}, {"id": "1911.09509", "submitter": "Luiz Zanlorensi", "authors": "Luiz A. Zanlorensi, Diego R. Lucio, Alceu S. Britto Jr., Hugo\n  Proen\\c{c}a, David Menotti", "title": "Deep Representations for Cross-spectral Ocular Biometrics", "comments": "This paper is a postprint of a paper submitted to and accepted for\n  publication inIET Biometrics and is subject to Institution of Engineering and\n  Technology Copyright. The copy of the record is available at the IET Digital\n  Library", "journal-ref": null, "doi": "10.1049/iet-bmt.2019.0116", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major challenges in ocular biometrics is the cross-spectral\nscenario, i.e., how to match images acquired in different wavelengths\n(typically visible (VIS) against near-infrared (NIR)). This article designs and\nextensively evaluates cross-spectral ocular verification methods, for both the\nclosed and open-world settings, using well known deep learning representations\nbased on the iris and periocular regions. Using as inputs the bounding boxes of\nnon-normalized iris/periocular regions, we fine-tune Convolutional Neural\nNetwork(CNN) models (based either on VGG16 or ResNet-50 architectures),\noriginally trained for face recognition. Based on the experiments carried out\nin two publicly available cross-spectral ocular databases, we report results\nfor intra-spectral and cross-spectral scenarios, with the best performance\nbeing observed when fusing ResNet-50 deep representations from both the\nperiocular and iris regions. When compared to the state-of-the-art, we observed\nthat the proposed solution consistently reduces the Equal Error Rate(EER)\nvalues by 90% / 93% / 96% and 61% / 77% / 83% on the cross-spectral scenario\nand in the PolyU Bi-spectral and Cross-eye-cross-spectral datasets. Lastly, we\nevaluate the effect that the \"deepness\" factor of feature representations has\nin recognition effectiveness, and - based on a subjective analysis of the most\nproblematic pairwise comparisons - we point out further directions for this\nfield of research.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 14:54:46 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Zanlorensi", "Luiz A.", ""], ["Lucio", "Diego R.", ""], ["Britto", "Alceu S.", "Jr."], ["Proen\u00e7a", "Hugo", ""], ["Menotti", "David", ""]]}, {"id": "1911.09516", "submitter": "Songtao Liu", "authors": "Songtao Liu, Di Huang, Yunhong Wang", "title": "Learning Spatial Fusion for Single-Shot Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pyramidal feature representation is the common practice to address the\nchallenge of scale variation in object detection. However, the inconsistency\nacross different feature scales is a primary limitation for the single-shot\ndetectors based on feature pyramid. In this work, we propose a novel and data\ndriven strategy for pyramidal feature fusion, referred to as adaptively spatial\nfeature fusion (ASFF). It learns the way to spatially filter conflictive\ninformation to suppress the inconsistency, thus improving the scale-invariance\nof features, and introduces nearly free inference overhead. With the ASFF\nstrategy and a solid baseline of YOLOv3, we achieve the best speed-accuracy\ntrade-off on the MS COCO dataset, reporting 38.1% AP at 60 FPS, 42.4% AP at 45\nFPS and 43.9% AP at 29 FPS. The code is available at\nhttps://github.com/ruinmessi/ASFF\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 15:05:28 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 01:59:05 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Liu", "Songtao", ""], ["Huang", "Di", ""], ["Wang", "Yunhong", ""]]}, {"id": "1911.09518", "submitter": "Arjun Krishna Mr", "authors": "Arjun Krishna and A S Akil Arif Ibrahim", "title": "Video Segment Copy Detection Using Memory Constrained Hierarchical\n  Batch-Normalized LSTM Autoencoder", "comments": "Undergraduate Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we introduce a video hashing method for scalable video\nsegment copy detection. The objective of video segment copy detection is to\nfind the video (s) present in a large database, one of whose segments (cropped\nin time) is a (transformed) copy of the given query video. This transformation\nmay be temporal (for example frame dropping, change in frame rate) or spatial\n(brightness and contrast change, addition of noise etc.) in nature although the\nprimary focus of this report is detecting temporal attacks. The video hashing\nmethod proposed by us uses a deep learning neural network to learn variable\nlength binary hash codes for the entire video considering both temporal and\nspatial features into account. This is in contrast to most existing video\nhashing methods, as they use conventional image hashing techniques to obtain\nhash codes for a video after extracting features for every frame or certain key\nframes, in which case the temporal information present in the video is not\nexploited. Our hashing method is specifically resilient to time cropping making\nit extremely useful in video segment copy detection. Experimental results\nobtained on the large augmented dataset consisting of around 25,000 videos with\nsegment copies demonstrate the efficacy of our proposed video hashing method.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 15:00:02 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Krishna", "Arjun", ""], ["Ibrahim", "A S Akil Arif", ""]]}, {"id": "1911.09550", "submitter": "Hao Wang", "authors": "Hao Wang, Pu Lu, Hui Zhang, Mingkun Yang, Xiang Bai, Yongchao Xu,\n  Mengchao He, Yongpan Wang, Wenyu Liu", "title": "All You Need Is Boundary: Toward Arbitrary-Shaped Text Spotting", "comments": "Accepted to AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, end-to-end text spotting that aims to detect and recognize text\nfrom cluttered images simultaneously has received particularly growing interest\nin computer vision. Different from the existing approaches that formulate text\ndetection as bounding box extraction or instance segmentation, we localize a\nset of points on the boundary of each text instance. With the representation of\nsuch boundary points, we establish a simple yet effective scheme for end-to-end\ntext spotting, which can read the text of arbitrary shapes. Experiments on\nthree challenging datasets, including ICDAR2015, TotalText and COCO-Text\ndemonstrate that the proposed method consistently surpasses the\nstate-of-the-art in both scene text detection and end-to-end text recognition\ntasks.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 15:45:56 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Wang", "Hao", ""], ["Lu", "Pu", ""], ["Zhang", "Hui", ""], ["Yang", "Mingkun", ""], ["Bai", "Xiang", ""], ["Xu", "Yongchao", ""], ["He", "Mengchao", ""], ["Wang", "Yongpan", ""], ["Liu", "Wenyu", ""]]}, {"id": "1911.09579", "submitter": "Hefeng Wu", "authors": "Riquan Chen, Tianshui Chen, Xiaolu Hui, Hefeng Wu, Guanbin Li, Liang\n  Lin", "title": "Knowledge Graph Transfer Network for Few-Shot Recognition", "comments": "accepted by AAAI 2020 as oral paper", "journal-ref": null, "doi": "10.1609/aaai.v34i07.6630", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning aims to learn novel categories from very few samples given\nsome base categories with sufficient training samples. The main challenge of\nthis task is the novel categories are prone to dominated by color, texture,\nshape of the object or background context (namely specificity), which are\ndistinct for the given few training samples but not common for the\ncorresponding categories (see Figure 1). Fortunately, we find that transferring\ninformation of the correlated based categories can help learn the novel\nconcepts and thus avoid the novel concept being dominated by the specificity.\nBesides, incorporating semantic correlations among different categories can\neffectively regularize this information transfer. In this work, we represent\nthe semantic correlations in the form of structured knowledge graph and\nintegrate this graph into deep neural networks to promote few-shot learning by\na novel Knowledge Graph Transfer Network (KGTN). Specifically, by initializing\neach node with the classifier weight of the corresponding category, a\npropagation mechanism is learned to adaptively propagate node message through\nthe graph to explore node interaction and transfer classifier information of\nthe base categories to those of the novel ones. Extensive experiments on the\nImageNet dataset show significant performance improvement compared with current\nleading competitors. Furthermore, we construct an ImageNet-6K dataset that\ncovers larger scale categories, i.e, 6,000 categories, and experiments on this\ndataset further demonstrate the effectiveness of our proposed model. Our codes\nand models are available at https://github.com/MyChocer/KGTN .\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 16:16:22 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 13:06:09 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Chen", "Riquan", ""], ["Chen", "Tianshui", ""], ["Hui", "Xiaolu", ""], ["Wu", "Hefeng", ""], ["Li", "Guanbin", ""], ["Lin", "Liang", ""]]}, {"id": "1911.09599", "submitter": "Alexander Gomez Villa A. Gomez-Villa", "authors": "Alexander Gomez-Villa, Adrian Mart\\'in, Javier Vazquez-Corral, Jes\\'us\n  Malo, Marcelo Bertalm\\'io", "title": "Synthesizing Visual Illusions Using Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual illusions are a very useful tool for vision scientists, because they\nallow them to better probe the limits, thresholds and errors of the visual\nsystem. In this work we introduce the first ever framework to generate novel\nvisual illusions with an artificial neural network (ANN). It takes the form of\na generative adversarial network, with a generator of visual illusion\ncandidates and two discriminator modules, one for the inducer background and\nanother that decides whether or not the candidate is indeed an illusion. The\ngenerality of the model is exemplified by synthesizing illusions of different\ntypes, and validated with psychophysical experiments that corroborate that the\noutputs of our ANN are indeed visual illusions to human observers. Apart from\nsynthesizing new visual illusions, which may help vision researchers, the\nproposed model has the potential to open new ways to study the similarities and\ndifferences between ANN and human visual perception.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 16:49:43 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Gomez-Villa", "Alexander", ""], ["Mart\u00edn", "Adrian", ""], ["Vazquez-Corral", "Javier", ""], ["Malo", "Jes\u00fas", ""], ["Bertalm\u00edo", "Marcelo", ""]]}, {"id": "1911.09646", "submitter": "Luiz A. Zanlorensi", "authors": "Luiz A. Zanlorensi, Rayson Laroca, Eduardo Luz, Alceu S. Britto Jr.,\n  Luiz S. Oliveira, David Menotti", "title": "Ocular Recognition Databases and Competitions: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of the iris and periocular region as biometric traits has been\nextensively investigated, mainly due to the singularity of the iris features\nand the use of the periocular region when the image resolution is not\nsufficient to extract iris information. In addition to providing information\nabout an individual's identity, features extracted from these traits can also\nbe explored to obtain other information such as the individual's gender, the\ninfluence of drug use, the use of contact lenses, spoofing, among others. This\nwork presents a survey of the databases created for ocular recognition,\ndetailing their protocols and how their images were acquired. We also describe\nand discuss the most popular ocular recognition competitions (contests),\nhighlighting the submitted algorithms that achieved the best results using only\niris trait and also fusing iris and periocular region information. Finally, we\ndescribe some relevant works applying deep learning techniques to ocular\nrecognition and point out new challenges and future directions. Considering\nthat there are a large number of ocular databases, and each one is usually\ndesigned for a specific problem, we believe this survey can provide a broad\noverview of the challenges in ocular biometrics.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 18:09:37 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Zanlorensi", "Luiz A.", ""], ["Laroca", "Rayson", ""], ["Luz", "Eduardo", ""], ["Britto", "Alceu S.", "Jr."], ["Oliveira", "Luiz S.", ""], ["Menotti", "David", ""]]}, {"id": "1911.09649", "submitter": "Arda Senocak", "authors": "Arda Senocak, Tae-Hyun Oh, Junsik Kim, Ming-Hsuan Yang, In So Kweon", "title": "Learning to Localize Sound Sources in Visual Scenes: Analysis and\n  Applications", "comments": "To appear in TPAMI. arXiv admin note: substantial text overlap with\n  arXiv:1803.03849", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual events are usually accompanied by sounds in our daily lives. However,\ncan the machines learn to correlate the visual scene and sound, as well as\nlocalize the sound source only by observing them like humans? To investigate\nits empirical learnability, in this work we first present a novel unsupervised\nalgorithm to address the problem of localizing sound sources in visual scenes.\nIn order to achieve this goal, a two-stream network structure which handles\neach modality with attention mechanism is developed for sound source\nlocalization. The network naturally reveals the localized response in the scene\nwithout human annotation. In addition, a new sound source dataset is developed\nfor performance evaluation. Nevertheless, our empirical evaluation shows that\nthe unsupervised method generates false conclusions in some cases. Thereby, we\nshow that this false conclusion cannot be fixed without human prior knowledge\ndue to the well-known correlation and causality mismatch misconception. To fix\nthis issue, we extend our network to the supervised and semi-supervised network\nsettings via a simple modification due to the general architecture of our\ntwo-stream network. We show that the false conclusions can be effectively\ncorrected even with a small amount of supervision, i.e., semi-supervised setup.\nFurthermore, we present the versatility of the learned audio and visual\nembeddings on the cross-modal content alignment and we extend this proposed\nalgorithm to a new application, sound saliency based automatic camera view\npanning in 360-degree{\\deg} videos.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 07:29:33 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Senocak", "Arda", ""], ["Oh", "Tae-Hyun", ""], ["Kim", "Junsik", ""], ["Yang", "Ming-Hsuan", ""], ["Kweon", "In So", ""]]}, {"id": "1911.09652", "submitter": "Oluwafemi Azeez", "authors": "Oluwafemi Azeez", "title": "Unsupervised Domain Adaptation by Optical Flow Augmentation in Semantic\n  Segmentation", "comments": "arXiv admin note: text overlap with arXiv:1910.10369 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is expensive to generate real-life image labels and there is a domain gap\nbetween real-life and simulated images, hence a model trained on the latter\ncannot adapt to the former. Solving this can totally eliminate the need for\nlabeling real-life datasets completely. Class balanced self-training is one of\nthe existing techniques that attempt to reduce the domain gap. Moreover,\naugmenting RGB with flow maps has improved performance in simple semantic\nsegmentation and geometry is preserved across domains. Hence, by augmenting\nimages with dense optical flow map, domain adaptation in semantic segmentation\ncan be improved.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 07:36:45 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Azeez", "Oluwafemi", ""]]}, {"id": "1911.09659", "submitter": "Yandong Li", "authors": "Yunhui Guo, Yandong Li, Liqiang Wang, Tajana Rosing", "title": "AdaFilter: Adaptive Filter Fine-tuning for Deep Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing number of pre-trained deep neural network models.\nHowever, it is still unclear how to effectively use these models for a new\ntask. Transfer learning, which aims to transfer knowledge from source tasks to\na target task, is an effective solution to this problem. Fine-tuning is a\npopular transfer learning technique for deep neural networks where a few rounds\nof training are applied to the parameters of a pre-trained model to adapt them\nto a new task. Despite its popularity, in this paper, we show that fine-tuning\nsuffers from several drawbacks. We propose an adaptive fine-tuning approach,\ncalled AdaFilter, which selects only a part of the convolutional filters in the\npre-trained model to optimize on a per-example basis. We use a recurrent gated\nnetwork to selectively fine-tune convolutional filters based on the activations\nof the previous layer. We experiment with 7 public image classification\ndatasets and the results show that AdaFilter can reduce the average\nclassification error of the standard fine-tuning by 2.54%.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 18:39:01 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 00:30:19 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Guo", "Yunhui", ""], ["Li", "Yandong", ""], ["Wang", "Liqiang", ""], ["Rosing", "Tajana", ""]]}, {"id": "1911.09665", "submitter": "Cihang Xie", "authors": "Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan Yuille, Quoc\n  V. Le", "title": "Adversarial Examples Improve Image Recognition", "comments": "CVPR 2020, models are available at\n  https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are commonly viewed as a threat to ConvNets. Here we\npresent an opposite perspective: adversarial examples can be used to improve\nimage recognition models if harnessed in the right manner. We propose AdvProp,\nan enhanced adversarial training scheme which treats adversarial examples as\nadditional examples, to prevent overfitting. Key to our method is the usage of\na separate auxiliary batch norm for adversarial examples, as they have\ndifferent underlying distributions to normal examples.\n  We show that AdvProp improves a wide range of models on various image\nrecognition tasks and performs better when the models are bigger. For instance,\nby applying AdvProp to the latest EfficientNet-B7 [28] on ImageNet, we achieve\nsignificant improvements on ImageNet (+0.7%), ImageNet-C (+6.5%), ImageNet-A\n(+7.0%), Stylized-ImageNet (+4.8%). With an enhanced EfficientNet-B8, our\nmethod achieves the state-of-the-art 85.5% ImageNet top-1 accuracy without\nextra data. This result even surpasses the best model in [20] which is trained\nwith 3.5B Instagram images (~3000X more than ImageNet) and ~9.4X more\nparameters. Models are available at\nhttps://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 18:53:23 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 17:20:25 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Xie", "Cihang", ""], ["Tan", "Mingxing", ""], ["Gong", "Boqing", ""], ["Wang", "Jiang", ""], ["Yuille", "Alan", ""], ["Le", "Quoc V.", ""]]}, {"id": "1911.09676", "submitter": "Deepak Pathak", "authors": "Pratyusha Sharma, Deepak Pathak, Abhinav Gupta", "title": "Third-Person Visual Imitation Learning via Decoupled Hierarchical\n  Controller", "comments": "Accepted at NeurIPS 2019. Videos at\n  https://pathak22.github.io/hierarchical-imitation/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a generalized setup for learning from demonstration to build an\nagent that can manipulate novel objects in unseen scenarios by looking at only\na single video of human demonstration from a third-person perspective. To\naccomplish this goal, our agent should not only learn to understand the intent\nof the demonstrated third-person video in its context but also perform the\nintended task in its environment configuration. Our central insight is to\nenforce this structure explicitly during learning by decoupling what to achieve\n(intended task) from how to perform it (controller). We propose a hierarchical\nsetup where a high-level module learns to generate a series of first-person\nsub-goals conditioned on the third-person video demonstration, and a low-level\ncontroller predicts the actions to achieve those sub-goals. Our agent acts from\nraw image observations without any access to the full state information. We\nshow results on a real robotic platform using Baxter for the manipulation tasks\nof pouring and placing objects in a box. Project video and code are at\nhttps://pathak22.github.io/hierarchical-imitation/\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 18:59:29 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Sharma", "Pratyusha", ""], ["Pathak", "Deepak", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1911.09712", "submitter": "Jean Marie Uwabeza Vianney", "authors": "Jean Marie Uwabeza Vianney, Shubhra Aich, and Bingbing Liu", "title": "RefinedMPL: Refined Monocular PseudoLiDAR for 3D Object Detection in\n  Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we strive for solving the ambiguities arisen by the\nastoundingly high density of raw PseudoLiDAR for monocular 3D object detection\nfor autonomous driving. Without much computational overhead, we propose a\nsupervised and an unsupervised sparsification scheme of PseudoLiDAR prior to 3D\ndetection. Both the strategies assist the standard 3D detector gain better\nperformance over the raw PseudoLiDAR baseline using only ~5% of its points on\nthe KITTI object detection benchmark, thus making our monocular framework and\nLiDAR-based counterparts computationally equivalent (Figure 1). Moreover, our\narchitecture agnostic refinements provide state-of-the-art results on KITTI3D\ntest set for \"Car\" and \"Pedestrian\" categories with 54% relative improvement\nfor \"Pedestrian\". Finally, exploratory analysis is performed on the discrepancy\nbetween monocular and LiDAR-based 3D detection frameworks to guide future\nendeavours.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 19:21:32 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Vianney", "Jean Marie Uwabeza", ""], ["Aich", "Shubhra", ""], ["Liu", "Bingbing", ""]]}, {"id": "1911.09723", "submitter": "Erich Elsen", "authors": "Erich Elsen, Marat Dukhan, Trevor Gale, Karen Simonyan", "title": "Fast Sparse ConvNets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historically, the pursuit of efficient inference has been one of the driving\nforces behind research into new deep learning architectures and building\nblocks. Some recent examples include: the squeeze-and-excitation module,\ndepthwise separable convolutions in Xception, and the inverted bottleneck in\nMobileNet v2. Notably, in all of these cases, the resulting building blocks\nenabled not only higher efficiency, but also higher accuracy, and found wide\nadoption in the field. In this work, we further expand the arsenal of efficient\nbuilding blocks for neural network architectures; but instead of combining\nstandard primitives (such as convolution), we advocate for the replacement of\nthese dense primitives with their sparse counterparts. While the idea of using\nsparsity to decrease the parameter count is not new, the conventional wisdom is\nthat this reduction in theoretical FLOPs does not translate into real-world\nefficiency gains. We aim to correct this misconception by introducing a family\nof efficient sparse kernels for ARM and WebAssembly, which we open-source for\nthe benefit of the community as part of the XNNPACK library. Equipped with our\nefficient implementation of sparse primitives, we show that sparse versions of\nMobileNet v1, MobileNet v2 and EfficientNet architectures substantially\noutperform strong dense baselines on the efficiency-accuracy curve. On\nSnapdragon 835 our sparse networks outperform their dense equivalents by\n$1.3-2.4\\times$ -- equivalent to approximately one entire generation of\nMobileNet-family improvement. We hope that our findings will facilitate wider\nadoption of sparsity as a tool for creating efficient and accurate deep\nlearning architectures.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 19:48:14 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Elsen", "Erich", ""], ["Dukhan", "Marat", ""], ["Gale", "Trevor", ""], ["Simonyan", "Karen", ""]]}, {"id": "1911.09737", "submitter": "Saurabh Singh", "authors": "Saurabh Singh, Shankar Krishnan", "title": "Filter Response Normalization Layer: Eliminating Batch Dependence in the\n  Training of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Batch Normalization (BN) uses mini-batch statistics to normalize the\nactivations during training, introducing dependence between mini-batch\nelements. This dependency can hurt the performance if the mini-batch size is\ntoo small, or if the elements are correlated. Several alternatives, such as\nBatch Renormalization and Group Normalization (GN), have been proposed to\naddress this issue. However, they either do not match the performance of BN for\nlarge batches, or still exhibit degradation in performance for smaller batches,\nor introduce artificial constraints on the model architecture. In this paper we\npropose the Filter Response Normalization (FRN) layer, a novel combination of a\nnormalization and an activation function, that can be used as a replacement for\nother normalizations and activations. Our method operates on each activation\nchannel of each batch element independently, eliminating the dependency on\nother batch elements. Our method outperforms BN and other alternatives in a\nvariety of settings for all batch sizes. FRN layer performs $\\approx 0.7-1.0\\%$\nbetter than BN on top-1 validation accuracy with large mini-batch sizes for\nImagenet classification using InceptionV3 and ResnetV2-50 architectures.\nFurther, it performs $>1\\%$ better than GN on the same problem in the small\nmini-batch size regime. For object detection problem on COCO dataset, FRN layer\noutperforms all other methods by at least $0.3-0.5\\%$ in all batch size\nregimes.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 20:32:04 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 04:19:08 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Singh", "Saurabh", ""], ["Krishnan", "Shankar", ""]]}, {"id": "1911.09738", "submitter": "Siyuan Qiao", "authors": "Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, Alan Yuille", "title": "Rethinking Normalization and Elimination Singularity in Neural Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1903.10520", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study normalization methods for neural networks from the\nperspective of elimination singularity. Elimination singularities correspond to\nthe points on the training trajectory where neurons become consistently\ndeactivated. They cause degenerate manifolds in the loss landscape which will\nslow down training and harm model performances. We show that channel-based\nnormalizations (e.g. Layer Normalization and Group Normalization) are unable to\nguarantee a far distance from elimination singularities, in contrast with Batch\nNormalization which by design avoids models from getting too close to them. To\naddress this issue, we propose BatchChannel Normalization (BCN), which uses\nbatch knowledge to avoid the elimination singularities in the training of\nchannel-normalized models. Unlike Batch Normalization, BCN is able to run in\nboth large-batch and micro-batch training settings. The effectiveness of BCN is\nverified on many tasks, including image classification, object detection,\ninstance segmentation, and semantic segmentation. The code is here:\nhttps://github.com/joe-siyuan-qiao/Batch-Channel-Normalization.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 20:36:04 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Qiao", "Siyuan", ""], ["Wang", "Huiyu", ""], ["Liu", "Chenxi", ""], ["Shen", "Wei", ""], ["Yuille", "Alan", ""]]}, {"id": "1911.09753", "submitter": "Paul Hongsuck Seo", "authors": "Paul Hongsuck Seo, Piyush Sharma, Tomer Levinboim, Bohyung Han, Radu\n  Soricut", "title": "Reinforcing an Image Caption Generator Using Off-Line Human Feedback", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human ratings are currently the most accurate way to assess the quality of an\nimage captioning model, yet most often the only used outcome of an expensive\nhuman rating evaluation is a few overall statistics over the evaluation\ndataset. In this paper, we show that the signal from instance-level human\ncaption ratings can be leveraged to improve captioning models, even when the\namount of caption ratings is several orders of magnitude less than the caption\ntraining data. We employ a policy gradient method to maximize the human ratings\nas rewards in an off-policy reinforcement learning setting, where policy\ngradients are estimated by samples from a distribution that focuses on the\ncaptions in a caption ratings dataset. Our empirical evidence indicates that\nthe proposed method learns to generalize the human raters' judgments to a\npreviously unseen set of images, as judged by a different set of human judges,\nand additionally on a different, multi-dimensional side-by-side human\nevaluation procedure.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 21:26:28 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Seo", "Paul Hongsuck", ""], ["Sharma", "Piyush", ""], ["Levinboim", "Tomer", ""], ["Han", "Bohyung", ""], ["Soricut", "Radu", ""]]}, {"id": "1911.09781", "submitter": "Lu Jiang", "authors": "Lu Jiang, Di Huang, Mason Liu, Weilong Yang", "title": "Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels", "comments": "published at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Performing controlled experiments on noisy data is essential in understanding\ndeep learning across noise levels. Due to the lack of suitable datasets,\nprevious research has only examined deep learning on controlled synthetic label\nnoise, and real-world label noise has never been studied in a controlled\nsetting. This paper makes three contributions. First, we establish the first\nbenchmark of controlled real-world label noise from the web. This new benchmark\nenables us to study the web label noise in a controlled setting for the first\ntime. The second contribution is a simple but effective method to overcome both\nsynthetic and real noisy labels. We show that our method achieves the best\nresult on our dataset as well as on two public benchmarks (CIFAR and\nWebVision). Third, we conduct the largest study by far into understanding deep\nneural networks trained on noisy labels across different noise levels, noise\ntypes, network architectures, and training settings. The data and code are\nreleased at the following link: http://www.lujiang.info/cnlw.html\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 23:05:28 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 03:27:24 GMT"}, {"version": "v3", "created": "Thu, 27 Aug 2020 06:07:44 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Jiang", "Lu", ""], ["Huang", "Di", ""], ["Liu", "Mason", ""], ["Yang", "Weilong", ""]]}, {"id": "1911.09784", "submitter": "Yuqian Zhou", "authors": "Didan Deng, Zhaokang Chen, Yuqian Zhou, Bertram Shi", "title": "MIMAMO Net: Integrating Micro- and Macro-motion for Video Emotion\n  Recognition", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial-temporal feature learning is of vital importance for video emotion\nrecognition. Previous deep network structures often focused on macro-motion\nwhich extends over long time scales, e.g., on the order of seconds. We believe\nintegrating structures capturing information about both micro- and macro-motion\nwill benefit emotion prediction, because human perceive both micro- and\nmacro-expressions. In this paper, we propose to combine micro- and macro-motion\nfeatures to improve video emotion recognition with a two-stream recurrent\nnetwork, named MIMAMO (Micro-Macro-Motion) Net. Specifically, smaller and\nshorter micro-motions are analyzed by a two-stream network, while larger and\nmore sustained macro-motions can be well captured by a subsequent recurrent\nnetwork. Assigning specific interpretations to the roles of different parts of\nthe network enables us to make choice of parameters based on prior knowledge:\nchoices that turn out to be optimal. One of the important innovations in our\nmodel is the use of interframe phase differences rather than optical flow as\ninput to the temporal stream. Compared with the optical flow, phase differences\nrequire less computation and are more robust to illumination changes. Our\nproposed network achieves state of the art performance on two video emotion\ndatasets, the OMG emotion dataset and the Aff-Wild dataset. The most\nsignificant gains are for arousal prediction, for which motion information is\nintuitively more informative. Source code is available at\nhttps://github.com/wtomin/MIMAMO-Net.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 23:34:15 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Deng", "Didan", ""], ["Chen", "Zhaokang", ""], ["Zhou", "Yuqian", ""], ["Shi", "Bertram", ""]]}, {"id": "1911.09785", "submitter": "David Berthelot", "authors": "David Berthelot, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Kihyuk\n  Sohn, Han Zhang, Colin Raffel", "title": "ReMixMatch: Semi-Supervised Learning with Distribution Alignment and\n  Augmentation Anchoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We improve the recently-proposed \"MixMatch\" semi-supervised learning\nalgorithm by introducing two new techniques: distribution alignment and\naugmentation anchoring. Distribution alignment encourages the marginal\ndistribution of predictions on unlabeled data to be close to the marginal\ndistribution of ground-truth labels. Augmentation anchoring feeds multiple\nstrongly augmented versions of an input into the model and encourages each\noutput to be close to the prediction for a weakly-augmented version of the same\ninput. To produce strong augmentations, we propose a variant of AutoAugment\nwhich learns the augmentation policy while the model is being trained. Our new\nalgorithm, dubbed ReMixMatch, is significantly more data-efficient than prior\nwork, requiring between $5\\times$ and $16\\times$ less data to reach the same\naccuracy. For example, on CIFAR-10 with 250 labeled examples we reach $93.73\\%$\naccuracy (compared to MixMatch's accuracy of $93.58\\%$ with $4{,}000$ examples)\nand a median accuracy of $84.92\\%$ with just four labels per class. We make our\ncode and data open-source at https://github.com/google-research/remixmatch.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 23:44:25 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 23:14:46 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Berthelot", "David", ""], ["Carlini", "Nicholas", ""], ["Cubuk", "Ekin D.", ""], ["Kurakin", "Alex", ""], ["Sohn", "Kihyuk", ""], ["Zhang", "Han", ""], ["Raffel", "Colin", ""]]}, {"id": "1911.09800", "submitter": "Sonali Patil", "authors": "Sonali Patil, Tanmay Prakash, Bharath Comandur, and Avinash Kak", "title": "A Comparative Evaluation of SGM Variants (including a New Variant, tMGM)\n  for Dense Stereo Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal here is threefold: [1] To present a new dense-stereo matching\nalgorithm, tMGM, that by combining the hierarchical logic of tSGM with the\nsupport structure of MGM achieves 6-8\\% performance improvement over the\nbaseline SGM (these performance numbers are posted under tMGM-16 in the\nMiddlebury Benchmark V3 ); and [2] Through an exhaustive quantitative and\nqualitative comparative study, to compare how the major variants of the SGM\napproach to dense stereo matching, including the new tMGM, perform in the\npresence of: (a) illumination variations and shadows, (b) untextured or weakly\ntextured regions, (c) repetitive patterns in the scene in the presence of large\nstereo rectification errors. [3] To present a novel DEM-Sculpting approach for\nestimating initial disparity search bounds for multi-date satellite stereo\npairs. Based on our study, we have found that tMGM generally performs best with\nrespect to all these data conditions. Both tSGM and MGM improve the density of\nstereo disparity maps and combining the two in tMGM makes it possible to\naccurately estimate the disparities at a significant number of pixels that\nwould otherwise be declared invalid by SGM. The datasets we have used in our\ncomparative evaluation include the Middlebury2014, KITTI2015, and ETH3D\ndatasets and the satellite images over the San Fernando area from the MVS\nChallenge dataset.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 01:11:14 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Patil", "Sonali", ""], ["Prakash", "Tanmay", ""], ["Comandur", "Bharath", ""], ["Kak", "Avinash", ""]]}, {"id": "1911.09814", "submitter": "Ryo Yonetani", "authors": "Hiroaki Minoura and Ryo Yonetani and Mai Nishimura and Yoshitaka\n  Ushiku", "title": "Crowd Density Forecasting by Modeling Patch-based Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting human activities observed in videos is a long-standing challenge\nin computer vision, which leads to various real-world applications such as\nmobile robots, autonomous driving, and assistive systems. In this work, we\npresent a new visual forecasting task called crowd density forecasting. Given a\nvideo of a crowd captured by a surveillance camera, our goal is to predict how\nthat crowd will move in future frames. To address this task, we have developed\nthe patch-based density forecasting network (PDFN), which enables forecasting\nover a sequence of crowd density maps describing how crowded each location is\nin each video frame. PDFN represents a crowd density map based on spatially\noverlapping patches and learns density dynamics patch-wise in a compact latent\nspace. This enables us to model diverse and complex crowd density dynamics\nefficiently, even when the input video involves a variable number of crowds\nthat each move independently. Experimental results with several public datasets\ndemonstrate the effectiveness of our approach compared with state-of-the-art\nforecasting methods.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 02:18:30 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Minoura", "Hiroaki", ""], ["Yonetani", "Ryo", ""], ["Nishimura", "Mai", ""], ["Ushiku", "Yoshitaka", ""]]}, {"id": "1911.09816", "submitter": "Szu-Chi Chung", "authors": "Szu-Chi Chung, Shao-Hsuan Wang, Po-Yao Niu, Su-Yun Huang, Wei-Hau\n  Chang and I-Ping Tu", "title": "Two-stage dimension reduction for noisy high-dimensional images and\n  application to Cryogenic Electron Microscopy", "comments": "29 pages, 8 figures and 3 tables", "journal-ref": "Annals of Mathematical Sciences and Applications. Volume 5, Number\n  2, 283-316, 2020", "doi": "10.4310/AMSA.2020.v5.n2.a4", "report-no": null, "categories": "eess.IV cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Principal component analysis (PCA) is arguably the most widely used\ndimension-reduction method for vector-type data. When applied to a sample of\nimages, PCA requires vectorization of the image data, which in turn entails\nsolving an eigenvalue problem for the sample covariance matrix. We propose\nherein a two-stage dimension reduction (2SDR) method for image reconstruction\nfrom high-dimensional noisy image data. The first stage treats the image as a\nmatrix, which is a tensor of order 2, and uses multilinear principal component\nanalysis (MPCA) for matrix rank reduction and image denoising. The second stage\nvectorizes the reduced-rank matrix and achieves further dimension and noise\nreduction. Simulation studies demonstrate excellent performance of 2SDR, for\nwhich we also develop an asymptotic theory that establishes consistency of its\nrank selection. Applications to cryo-EM (cryogenic electronic microscopy),\nwhich has revolutionized structural biology, organic and medical chemistry,\ncellular and molecular physiology in the past decade, are also provided and\nillustrated with benchmark cryo-EM datasets. Connections to other\ncontemporaneous developments in image reconstruction and high-dimensional\nstatistical inference are also discussed.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 02:30:37 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 04:02:44 GMT"}, {"version": "v3", "created": "Wed, 10 Jun 2020 08:09:29 GMT"}, {"version": "v4", "created": "Sat, 27 Feb 2021 11:27:44 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Chung", "Szu-Chi", ""], ["Wang", "Shao-Hsuan", ""], ["Niu", "Po-Yao", ""], ["Huang", "Su-Yun", ""], ["Chang", "Wei-Hau", ""], ["Tu", "I-Ping", ""]]}, {"id": "1911.09817", "submitter": "Mingyang Zhang", "authors": "Mingyang Zhang, Xinyi Yu, Jingtao Rong, Linlin Ou, Weidong Zhang", "title": "Graph Pruning for Model Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous AutoML pruning works utilized individual layer features to\nautomatically prune filters. We analyze the correlation for two layers from\ndifferent blocks which have a short-cut structure. It is found that, in one\nblock, the deeper layer has many redundant filters which can be represented by\nfilters in the former layer so that it is necessary to take information from\nother layers into consideration in pruning. In this paper, a graph pruning\napproach is proposed, which views any deep model as a topology graph. Graph\nPruningNet based on the graph convolution network is designed to automatically\nextract neighboring information for each node. To extract features from various\ntopologies, Graph PruningNet is connected with Pruned Network by an individual\nfully connection layer for each node and jointly trained on a training dataset\nfrom scratch. Thus, we can obtain reasonable weights for any size of\nsub-network. We then search the best configuration of the Pruned Network by\nreinforcement learning. Different from previous work, we take the node features\nfrom well-trained Graph PruningNet, instead of the hand-craft features, as the\nstates in reinforcement learning. Compared with other AutoML pruning works, our\nmethod has achieved the state-of-the-art under same conditions on\nImageNet-2012. The code will be released on GitHub.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 02:32:15 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Zhang", "Mingyang", ""], ["Yu", "Xinyi", ""], ["Rong", "Jingtao", ""], ["Ou", "Linlin", ""], ["Zhang", "Weidong", ""]]}, {"id": "1911.09830", "submitter": "Tianyang Zhang", "authors": "Tianyang Zhang, Rui Ma", "title": "Identify the cells' nuclei based on the deep learning neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identify the cells' nuclei is the important point for most medical analyses.\nTo assist doctors finding the accurate cell' nuclei location automatically is\nhighly demanded in the clinical practice. Recently, fully convolutional neural\nnetwork (FCNs) serve as the back-bone in many image segmentation, like liver\nand tumer segmentation in medical field, human body block in technical filed.\nThe cells' nuclei identification task is also kind of image segmentation. To\nachieve this, we prefer to use deep learning algorithms. we construct three\ngeneral frameworks, one is Mask Region-based Convolutional Neural Network (Mask\nRCNN), which has the high performance in many image segmentations, one is\nU-net, which has the high generalization performance on small dataset and the\nother is DenseUNet, which is mixture network architecture with Dense Net and\nU-net. we compare the performance of these three frameworks. And we evaluated\nour method on the dataset of data science bowl 2018 challenge. For single model\nwithout any ensemble, they all have good performance.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 03:30:05 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Zhang", "Tianyang", ""], ["Ma", "Rui", ""]]}, {"id": "1911.09837", "submitter": "Jianyu Su", "authors": "Jianyu Su, Peter A. Beling, Rui Guo, Kyungtae Han", "title": "Graph Convolution Networks for Probabilistic Modeling of Driving\n  Acceleration", "comments": "Accepted by ITSC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to model and predict ego-vehicle's surrounding traffic is crucial\nfor autonomous pilots and intelligent driver-assistance systems. Acceleration\nprediction is important as one of the major components of traffic prediction.\nThis paper proposes novel approaches to the acceleration prediction problem. By\nrepresenting spatial relationships between vehicles with a graph model, we\nbuild a generalized acceleration prediction framework. This paper studies the\neffectiveness of proposed Graph Convolution Networks, which operate on graphs\npredicting the acceleration distribution for vehicles driving on highways. We\nfurther investigate prediction improvement through integrating of Recurrent\nNeural Networks to disentangle the temporal complexity inherent in the traffic\ndata. Results from simulation studies using comprehensive performance metrics\nsupport the conclusion that our proposed networks outperform state-of-the-art\nmethods in generating realistic trajectories over a prediction horizon.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 03:48:43 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 01:32:45 GMT"}, {"version": "v3", "created": "Thu, 7 May 2020 23:40:38 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Su", "Jianyu", ""], ["Beling", "Peter A.", ""], ["Guo", "Rui", ""], ["Han", "Kyungtae", ""]]}, {"id": "1911.09840", "submitter": "Mohammad Hamed Mozaffari", "authors": "M. Hamed Mozaffari and Won-Sook Lee", "title": "Real-time Ultrasound-enhanced Multimodal Imaging of Tongue using 3D\n  Printable Stabilizer System: A Deep Learning Approach", "comments": "12 figures, 1 table", "journal-ref": "Canadian Acoustics. 48, 1 (Mar. 2020)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite renewed awareness of the importance of articulation, it remains a\nchallenge for instructors to handle the pronunciation needs of language\nlearners. There are relatively scarce pedagogical tools for pronunciation\nteaching and learning. Unlike inefficient, traditional pronunciation\ninstructions like listening and repeating, electronic visual feedback (EVF)\nsystems such as ultrasound technology have been employed in new approaches.\nRecently, an ultrasound-enhanced multimodal method has been developed for\nvisualizing tongue movements of a language learner overlaid on the face-side of\nthe speaker's head. That system was evaluated for several language courses via\na blended learning paradigm at the university level. The result was asserted\nthat visualizing the articulator's system as biofeedback to language learners\nwill significantly improve articulation learning efficiency. In spite of the\nsuccessful usage of multimodal techniques for pronunciation training, it still\nrequires manual works and human manipulation. In this article, we aim to\ncontribute to this growing body of research by addressing difficulties of the\nprevious approaches by proposing a new comprehensive, automatic, real-time\nmultimodal pronunciation training system, benefits from powerful artificial\nintelligence techniques. The main objective of this research was to combine the\nadvantages of ultrasound technology, three-dimensional printing, and deep\nlearning algorithms to enhance the performance of previous systems. Our\npreliminary pedagogical evaluation of the proposed system revealed a\nsignificant improvement in flexibility, control, robustness, and autonomy.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 03:54:31 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Mozaffari", "M. Hamed", ""], ["Lee", "Won-Sook", ""]]}, {"id": "1911.09863", "submitter": "Ruturaj Gole", "authors": "Ruturaj Gole, Haixia Wu, Subho Ghose", "title": "Shape Detection In 2D Ultrasound Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound images are one of the most widely used techniques in clinical\nsettings to analyze and detect different organs for study or diagnoses of\ndiseases. The dependence on subjective opinions of experts such as radiologists\ncalls for an automatic recognition and detection system that can provide an\nobjective analysis. Previous work done on this topic is limited and can be\nclassified by the organ of interest. Hybrid neural networks, linear and\nlogistic regression models, 3D reconstructed models, and various machine\nlearning techniques have been used to solve complex problems such as detection\nof lesions and cancer. Our project aims to use Dual Path Networks (DPN) to\nsegment and detect shapes in ultrasound images taken from 3D printed models of\nthe liver. Further the DPN deep architectures could be coupled with Fully\nConvolutional Network (FCN) to refine the results. Data denoised with various\nfilters would be used to gauge how they fare against each other and provide the\nbest results. Small amount of dataset works with DPNs, and hence, that should\nbe appropriate for us as our dataset shall be limited in size. Moreover, the\nultrasound scans shall need to be taken from different orientations of the\nscanner with respect to the organ, such that the training dataset can\naccurately perform segmentation and shape detection.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 05:27:00 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Gole", "Ruturaj", ""], ["Wu", "Haixia", ""], ["Ghose", "Subho", ""]]}, {"id": "1911.09878", "submitter": "Sankaraganesh Jonna", "authors": "Arpit Bansal, Sankaraganesh Jonna, and Rajiv R.Sahay", "title": "PAG-Net: Progressive Attention Guided Depth Super-resolution Network", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a novel method for the challenging problem of\nguided depth map super-resolution, called PAGNet. It is based on residual dense\nnetworks and involves the attention mechanism to suppress the texture copying\nproblem arises due to improper guidance by RGB images. The attention module\nmainly involves providing the spatial attention to guidance image based on the\ndepth features. We evaluate the proposed trained models on test dataset and\nprovide comparisons with the state-of-the-art depth super-resolution methods.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 06:38:53 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Bansal", "Arpit", ""], ["Jonna", "Sankaraganesh", ""], ["Sahay", "Rajiv R.", ""]]}, {"id": "1911.09895", "submitter": "Mohammed Haroon Dupty", "authors": "Mohammed Haroon Dupty, Zhen Zhang, Wee Sun Lee", "title": "Visual Relationship Detection with Low Rank Non-Negative Tensor\n  Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of Visual Relationship Detection (VRD) which aims to\ndescribe the relationships between pairs of objects in the form of triplets of\n(subject, predicate, object). We observe that given a pair of bounding box\nproposals, objects often participate in multiple relations implying the\ndistribution of triplets is multimodal. We leverage the strong correlations\nwithin triplets to learn the joint distribution of triplet variables\nconditioned on the image and the bounding box proposals, doing away with the\nhitherto used independent distribution of triplets. To make learning the\ntriplet joint distribution feasible, we introduce a novel technique of learning\nconditional triplet distributions in the form of their normalized low rank\nnon-negative tensor decompositions. Normalized tensor decompositions take form\nof mixture distributions of discrete variables and thus are able to capture\nmultimodality. This allows us to efficiently learn higher order discrete\nmultimodal distributions and at the same time keep the parameter size\nmanageable. We further model the probability of selecting an object proposal\npair and include a relation triplet prior in our model. We show that each part\nof the model improves performance and the combination outperforms\nstate-of-the-art score on the Visual Genome (VG) and Visual Relationship\nDetection (VRD) datasets.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 07:23:02 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Dupty", "Mohammed Haroon", ""], ["Zhang", "Zhen", ""], ["Lee", "Wee Sun", ""]]}, {"id": "1911.09915", "submitter": "Zhengyuan Liu", "authors": "Zhengyuan Liu", "title": "Retinal Vessel Segmentation based on Fully Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The morphological attributes of retinal vessels, such as length, width,\ntortuosity and branching pattern and angles, play an important role in\ndiagnosis, screening, treatment, and evaluation of various cardiovascular and\nophthalmologic diseases such as diabetes, hypertension and arteriosclerosis.\nThe crucial step before extracting these morphological characteristics of\nretinal vessels from retinal fundus images is vessel segmentation. In this\nwork, we propose a method for retinal vessel segmentation based on fully\nconvolutional networks. Thousands of patches are extracted from each retinal\nimage and then fed into the network, and data argumentation is applied by\nrotating extracted patches. Two architectures of fully convolutional networks,\nU-Net and LadderNet, are used for vessel segmentation. The performance of our\nmethod is evaluated on three public datasets: DRIVE, STARE, and CHASE\\_DB1.\nExperimental results of our method show superior performance compared to recent\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 08:21:53 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Liu", "Zhengyuan", ""]]}, {"id": "1911.09918", "submitter": "Ruiqi Wang", "authors": "R.Q. Wang and Z.Q. Yuan and G.H. Chen", "title": "Simplified_edition_Multi-robot SLAM Multi-view Target Tracking based on\n  Panoramic Vision in Irregular Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to improve the precision of multi-robot SLAM multi-view target\ntracking process, a improved multi-robot SLAM multi-view target tracking\nalgorithm based on panoramic vision in irregular environment was put forward,\nadding an correction factor to renew the existing Extended Kalman Filter (EKF)\nmodel, obtaining new coordinates X and Y after twice iterations. The paper has\nbeen accepted by Computing and Visualization in Science and this is a\nsimplified version.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 08:36:05 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Wang", "R. Q.", ""], ["Yuan", "Z. Q.", ""], ["Chen", "G. H.", ""]]}, {"id": "1911.09929", "submitter": "Lewei Yao", "authors": "Lewei Yao, Hang Xu, Wei Zhang, Xiaodan Liang, Zhenguo Li", "title": "SM-NAS: Structural-to-Modular Neural Architecture Search for Object\n  Detection", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art object detection method is complicated with various\nmodules such as backbone, feature fusion neck, RPN and RCNN head, where each\nmodule may have different designs and structures. How to leverage the\ncomputational cost and accuracy trade-off for the structural combination as\nwell as the modular selection of multiple modules? Neural architecture search\n(NAS) has shown great potential in finding an optimal solution. Existing NAS\nworks for object detection only focus on searching better design of a single\nmodule such as backbone or feature fusion neck, while neglecting the balance of\nthe whole system. In this paper, we present a two-stage coarse-to-fine\nsearching strategy named Structural-to-Modular NAS (SM-NAS) for searching a\nGPU-friendly design of both an efficient combination of modules and better\nmodular-level architecture for object detection. Specifically, Structural-level\nsearching stage first aims to find an efficient combination of different\nmodules; Modular-level searching stage then evolves each specific module and\npushes the Pareto front forward to a faster task-specific network. We consider\na multi-objective search where the search space covers many popular designs of\ndetection methods. We directly search a detection backbone without pre-trained\nmodels or any proxy task by exploring a fast training from scratch strategy.\nThe resulting architectures dominate state-of-the-art object detection systems\nin both inference time and accuracy and demonstrate the effectiveness on\nmultiple detection datasets, e.g. halving the inference time with additional 1%\nmAP improvement compared to FPN and reaching 46% mAP with the similar inference\ntime of MaskRCNN.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 08:58:36 GMT"}, {"version": "v2", "created": "Sat, 30 Nov 2019 17:25:22 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Yao", "Lewei", ""], ["Xu", "Hang", ""], ["Zhang", "Wei", ""], ["Liang", "Xiaodan", ""], ["Li", "Zhenguo", ""]]}, {"id": "1911.09930", "submitter": "Yunfei Liu", "authors": "Yunfei Liu, Yu Li, Shaodi You and Feng Lu", "title": "Unsupervised Learning for Intrinsic Image Decomposition from a Single\n  Image", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intrinsic image decomposition, which is an essential task in computer vision,\naims to infer the reflectance and shading of the scene. It is challenging since\nit needs to separate one image into two components. To tackle this,\nconventional methods introduce various priors to constrain the solution, yet\nwith limited performance. Meanwhile, the problem is typically solved by\nsupervised learning methods, which is actually not an ideal solution since\nobtaining ground truth reflectance and shading for massive general natural\nscenes is challenging and even impossible. In this paper, we propose a novel\nunsupervised intrinsic image decomposition framework, which relies on neither\nlabeled training data nor hand-crafted priors. Instead, it directly learns the\nlatent feature of reflectance and shading from unsupervised and uncorrelated\ndata. To enable this, we explore the independence between reflectance and\nshading, the domain invariant content constraint and the physical constraint.\nExtensive experiments on both synthetic and real image datasets demonstrate\nconsistently superior performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 09:00:55 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 23:37:30 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Liu", "Yunfei", ""], ["Li", "Yu", ""], ["You", "Shaodi", ""], ["Lu", "Feng", ""]]}, {"id": "1911.09943", "submitter": "Hao Dong", "authors": "Guanqi Zhan, Yihao Zhao, Bingchan Zhao, Haoqi Yuan, Baoquan Chen, Hao\n  Dong", "title": "DLGAN: Disentangling Label-Specific Fine-Grained Features for Image\n  Manipulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown how disentangling images into content and feature\nspaces can provide controllable image translation/ manipulation. In this paper,\nwe propose a framework to enable utilizing discrete multi-labels to control\nwhich features to be disentangled, i.e., disentangling label-specific\nfine-grained features for image manipulation (dubbed DLGAN). By mapping the\ndiscrete label-specific attribute features into a continuous prior\ndistribution, we leverage the advantages of both discrete labels and reference\nimages to achieve image manipulation in a hybrid fashion. For example, given a\nface image dataset (e.g., CelebA) with multiple discrete fine-grained labels,\nwe can learn to smoothly interpolate a face image between black hair and blond\nhair through reference images while immediately controlling the gender and age\nthrough discrete input labels. To the best of our knowledge, this is the first\nwork that realizes such a hybrid manipulation within a single model. More\nimportantly, it is the first work to achieve image interpolation between two\ndifferent domains without requiring continuous labels as the supervision.\nQualitative and quantitative experiments demonstrate the effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 09:42:52 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 04:47:37 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Zhan", "Guanqi", ""], ["Zhao", "Yihao", ""], ["Zhao", "Bingchan", ""], ["Yuan", "Haoqi", ""], ["Chen", "Baoquan", ""], ["Dong", "Hao", ""]]}, {"id": "1911.09953", "submitter": "He-Feng Yin", "authors": "He-Feng Yin and Xiao-Jun Wu", "title": "Class-specific residual constraint non-negative representation for\n  pattern classification", "comments": "submitted to Journal of Electronic Imaging", "journal-ref": null, "doi": "10.1117/1.JEI.29.2.023014", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation based classification method (RBCM) remains one of the hottest\ntopics in the community of pattern recognition, and the recently proposed\nnon-negative representation based classification (NRC) achieved impressive\nrecognition results in various classification tasks. However, NRC ignores the\nrelationship between the coding and classification stages. Moreover, there is\nno regularization term other than the reconstruction error term in the\nformulation of NRC, which may result in unstable solution leading to\nmisclassification. To overcome these drawbacks of NRC, in this paper, we\npropose a class-specific residual constraint non-negative representation (CRNR)\nfor pattern classification. CRNR introduces a class-specific residual\nconstraint into the formulation of NRC, which encourages training samples from\ndifferent classes to competitively represent the test sample. Based on the\nproposed CRNR, we develop a CRNR based classifier (CRNRC) for pattern\nclassification. Experimental results on several benchmark datasets demonstrate\nthe superiority of CRNRC over conventional RBCM as well as the recently\nproposed NRC. Moreover, CRNRC works better or comparable to some\nstate-of-the-art deep approaches on diverse challenging pattern classification\ntasks. The source code of our proposed CRNRC is accessible at\nhttps://github.com/yinhefeng/CRNRC.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 10:03:14 GMT"}, {"version": "v2", "created": "Sun, 19 Jan 2020 06:49:11 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Yin", "He-Feng", ""], ["Wu", "Xiao-Jun", ""]]}, {"id": "1911.09960", "submitter": "Lior Wolf", "authors": "Barak Itkin, Lior Wolf, Nachum Dershowitz", "title": "Computational Ceramicology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Field archeologists are called upon to identify potsherds, for which purpose\nthey rely on their experience and on reference works. We have developed two\ncomplementary machine-learning tools to propose identifications based on images\ncaptured on site. One method relies on the shape of the fracture outline of a\nsherd; the other is based on decorative features. For the\noutline-identification tool, a novel deep-learning architecture was employed,\none that integrates shape information from points along the inner and outer\nsurfaces. The decoration classifier is based on relatively standard\narchitectures used in image recognition. In both cases, training the\nclassifiers required tackling challenges that arise when working with\nreal-world archeological data: paucity of labeled data; extreme imbalance\nbetween instances of the different categories; and the need to avoid neglecting\nrare classes and to take note of minute distinguishing features of some\nclasses. The scarcity of training data was overcome by using\nsynthetically-produced virtual potsherds and by employing multiple\ndata-augmentation techniques. A novel form of training loss allowed us to\novercome the problems caused by under-populated classes and non-homogeneous\ndistribution of discriminative features.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 10:32:52 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Itkin", "Barak", ""], ["Wolf", "Lior", ""], ["Dershowitz", "Nachum", ""]]}, {"id": "1911.09963", "submitter": "Pilhyeon Lee", "authors": "Pilhyeon Lee, Youngjung Uh, Hyeran Byun", "title": "Background Suppression Network for Weakly-supervised Temporal Action\n  Localization", "comments": "Accepted by the 34th AAAI Conference on Artificial Intelligence (AAAI\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-supervised temporal action localization is a very challenging problem\nbecause frame-wise labels are not given in the training stage while the only\nhint is video-level labels: whether each video contains action frames of\ninterest. Previous methods aggregate frame-level class scores to produce\nvideo-level prediction and learn from video-level action labels. This\nformulation does not fully model the problem in that background frames are\nforced to be misclassified as action classes to predict video-level labels\naccurately. In this paper, we design Background Suppression Network (BaS-Net)\nwhich introduces an auxiliary class for background and has a two-branch\nweight-sharing architecture with an asymmetrical training strategy. This\nenables BaS-Net to suppress activations from background frames to improve\nlocalization performance. Extensive experiments demonstrate the effectiveness\nof BaS-Net and its superiority over the state-of-the-art methods on the most\npopular benchmarks - THUMOS'14 and ActivityNet. Our code and the trained model\nare available at https://github.com/Pilhyeon/BaSNet-pytorch.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 10:39:57 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Lee", "Pilhyeon", ""], ["Uh", "Youngjung", ""], ["Byun", "Hyeran", ""]]}, {"id": "1911.09968", "submitter": "Yasin Almalioglu", "authors": "Yasin Almalioglu, Mehmet Turan, Alp Eren Sari, Muhamad Risqi U.\n  Saputra, Pedro P. B. de Gusm\\~ao, Andrew Markham, Niki Trigoni", "title": "SelfVIO: Self-Supervised Deep Monocular Visual-Inertial Odometry and\n  Depth Estimation", "comments": "15 pages, submitted to The IEEE Transactions on Robotics (T-RO)\n  journal, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, numerous supervised deep learning approaches requiring\nlarge amounts of labeled data have been proposed for visual-inertial odometry\n(VIO) and depth map estimation. To overcome the data limitation,\nself-supervised learning has emerged as a promising alternative, exploiting\nconstraints such as geometric and photometric consistency in the scene. In this\nstudy, we introduce a novel self-supervised deep learning-based VIO and depth\nmap recovery approach (SelfVIO) using adversarial training and self-adaptive\nvisual-inertial sensor fusion. SelfVIO learns to jointly estimate 6\ndegrees-of-freedom (6-DoF) ego-motion and a depth map of the scene from\nunlabeled monocular RGB image sequences and inertial measurement unit (IMU)\nreadings. The proposed approach is able to perform VIO without the need for IMU\nintrinsic parameters and/or the extrinsic calibration between the IMU and the\ncamera. estimation and single-view depth recovery network. We provide\ncomprehensive quantitative and qualitative evaluations of the proposed\nframework comparing its performance with state-of-the-art VIO, VO, and visual\nsimultaneous localization and mapping (VSLAM) approaches on the KITTI, EuRoC\nand Cityscapes datasets. Detailed comparisons prove that SelfVIO outperforms\nstate-of-the-art VIO approaches in terms of pose estimation and depth recovery,\nmaking it a promising approach among existing methods in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 10:51:09 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 13:37:41 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Almalioglu", "Yasin", ""], ["Turan", "Mehmet", ""], ["Sari", "Alp Eren", ""], ["Saputra", "Muhamad Risqi U.", ""], ["de Gusm\u00e3o", "Pedro P. B.", ""], ["Markham", "Andrew", ""], ["Trigoni", "Niki", ""]]}, {"id": "1911.09976", "submitter": "Xinshao Wang Mr", "authors": "Xinshao Wang, Elyor Kodirov, Yang Hua, Neil Robertson", "title": "Instance Cross Entropy for Deep Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loss functions play a crucial role in deep metric learning thus a variety of\nthem have been proposed. Some supervise the learning process by pairwise or\ntripletwise similarity constraints while others take advantage of structured\nsimilarity information among multiple data points. In this work, we approach\ndeep metric learning from a novel perspective. We propose instance cross\nentropy (ICE) which measures the difference between an estimated instance-level\nmatching distribution and its ground-truth one. ICE has three main appealing\nproperties. Firstly, similar to categorical cross entropy (CCE), ICE has clear\nprobabilistic interpretation and exploits structured semantic similarity\ninformation for learning supervision. Secondly, ICE is scalable to infinite\ntraining data as it learns on mini-batches iteratively and is independent of\nthe training set size. Thirdly, motivated by our relative weight analysis,\nseamless sample reweighting is incorporated. It rescales samples' gradients to\ncontrol the differentiation degree over training examples instead of truncating\nthem by sample mining. In addition to its simplicity and intuitiveness,\nextensive experiments on three real-world benchmarks demonstrate the\nsuperiority of ICE.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 11:12:48 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Wang", "Xinshao", ""], ["Kodirov", "Elyor", ""], ["Hua", "Yang", ""], ["Robertson", "Neil", ""]]}, {"id": "1911.09982", "submitter": "Ling Luo", "authors": "Ling Luo, Dingyu Xue, Xinglong Feng", "title": "HybridNetSeg: A Compact Hybrid Network for Retinal Vessel Segmentation", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of retinal vessel analysis methods based on image segmentation\nhave emerged in recent years. However, existing methods depend on cumbersome\nbackbones, such as VGG16 and ResNet-50, benefiting from their powerful feature\nextraction capabilities but suffering from high computational costs. In this\npaper, we propose a novel neural network (HybridNetSeg) dedicated to solving\nthis drawback while further improving overall performance. Considering\ndeformable convolution can extract complex and variable structural information,\nand larger kernel in mixed depthwise convolution makes contribution to higher\naccuracy. We have integrated these two modules and propose a Hybrid Convolution\nBlock (HCB) using the idea of heuristic learning. Inspired by the U-Net, we use\nHCB to replace a part of the common convolution of the U-Net encoder,\ndrastically reducing the parameter count to 0.71M while accelerating the\ninference process. Not only that, we also propose a multi-scale mixed loss\nmechanism. Extensive experiments on three major benchmark datasets demonstrate\nthe effectiveness of our proposed method\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 11:42:09 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Luo", "Ling", ""], ["Xue", "Dingyu", ""], ["Feng", "Xinglong", ""]]}, {"id": "1911.09989", "submitter": "May Hammad", "authors": "Menatallh Hammad, May Hammad, Mohamed Elshenawy", "title": "Characterizing the impact of using features extracted from pre-trained\n  models on the quality of video captioning sequence-to-sequence models", "comments": "Submitted to conference ICPRAI2020", "journal-ref": null, "doi": "10.1007/978-3-030-59830-3_21", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of video captioning, that is, the automatic generation of sentences\ndescribing a sequence of actions in a video, has attracted an increasing\nattention recently. The complex and high-dimensional representation of video\ndata makes it difficult for a typical encoder-decoder architectures to\nrecognize relevant features and encode them in a proper format. Video data\ncontains different modalities that can be recognized using a mix image, scene,\naction and audio features. In this paper, we characterize the different\nfeatures affecting video descriptions and explore the interactions among these\nfeatures and how they affect the final quality of a video representation.\nBuilding on existing encoder-decoder models that utilize limited range of video\ninformation, our comparisons show how the inclusion of multi-modal video\nfeatures can make a significant effect on improving the quality of generated\nstatements. The work is of special interest to scientists and practitioners who\nare using sequence-to-sequence models to generate video captions.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 12:06:19 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Hammad", "Menatallh", ""], ["Hammad", "May", ""], ["Elshenawy", "Mohamed", ""]]}, {"id": "1911.09996", "submitter": "Vacit Oguz Yazici", "authors": "Vacit Oguz Yazici, Abel Gonzalez-Garcia, Arnau Ramisa, Bartlomiej\n  Twardowski, Joost van de Weijer", "title": "Orderless Recurrent Models for Multi-label Classification", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNN) are popular for many computer vision tasks,\nincluding multi-label classification. Since RNNs produce sequential outputs,\nlabels need to be ordered for the multi-label classification task. Current\napproaches sort labels according to their frequency, typically ordering them in\neither rare-first or frequent-first. These imposed orderings do not take into\naccount that the natural order to generate the labels can change for each\nimage, e.g.\\ first the dominant object before summing up the smaller objects in\nthe image. Therefore, in this paper, we propose ways to dynamically order the\nground truth labels with the predicted label sequence. This allows for the\nfaster training of more optimal LSTM models for multi-label classification.\nAnalysis evidences that our method does not suffer from duplicate generation,\nsomething which is common for other models. Furthermore, it outperforms other\nCNN-RNN models, and we show that a standard architecture of an image encoder\nand language decoder trained with our proposed loss obtains the\nstate-of-the-art results on the challenging MS-COCO, WIDER Attribute and\nPA-100K and competitive results on NUS-WIDE.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 12:25:14 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 11:16:41 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2020 17:10:18 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Yazici", "Vacit Oguz", ""], ["Gonzalez-Garcia", "Abel", ""], ["Ramisa", "Arnau", ""], ["Twardowski", "Bartlomiej", ""], ["van de Weijer", "Joost", ""]]}, {"id": "1911.10003", "submitter": "He-Feng Yin", "authors": "He-Feng Yin, Xiao-Jun Wu and Su-Gen Chen", "title": "Locality Constraint Dictionary Learning with Support Vector for Pattern\n  Classification", "comments": "submitted to IEEE Access", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative dictionary learning (DDL) has recently gained significant\nattention due to its impressive performance in various pattern classification\ntasks. However, the locality of atoms is not fully explored in conventional DDL\napproaches which hampers their classification performance. In this paper, we\npropose a locality constraint dictionary learning with support vector\ndiscriminative term (LCDL-SV), in which the locality information is preserved\nby employing the graph Laplacian matrix of the learned dictionary. To jointly\nlearn a classifier during the training phase, a support vector discriminative\nterm is incorporated into the proposed objective function. Moreover, in the\nclassification stage, the identity of test data is jointly determined by the\nregularized residual and the learned multi-class support vector machine.\nFinally, the resulting optimization problem is solved by utilizing the\nalternative strategy. Experimental results on benchmark databases demonstrate\nthe superiority of our proposed method over previous dictionary learning\napproaches on both hand-crafted and deep features. The source code of our\nproposed LCDL-SV is accessible at https://github.com/yinhefeng/LCDL-SV\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 12:35:35 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Yin", "He-Feng", ""], ["Wu", "Xiao-Jun", ""], ["Chen", "Su-Gen", ""]]}, {"id": "1911.10022", "submitter": "Friso Heslinga", "authors": "Friso G. Heslinga, Josien P.W. Pluim, A.J.H.M. Houben, Miranda T.\n  Schram, Ronald M.A. Henry, Coen D.A. Stehouwer, Marleen J. van Greevenbroek,\n  Tos T.J.M. Berendschot, and Mitko Veta", "title": "Direct Classification of Type 2 Diabetes From Retinal Fundus Images in a\n  Population-based Sample From The Maastricht Study", "comments": "to be published in the proceeding of SPIE - Medical Imaging 2020, 6\n  pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Type 2 Diabetes (T2D) is a chronic metabolic disorder that can lead to\nblindness and cardiovascular disease. Information about early stage T2D might\nbe present in retinal fundus images, but to what extent these images can be\nused for a screening setting is still unknown. In this study, deep neural\nnetworks were employed to differentiate between fundus images from individuals\nwith and without T2D. We investigated three methods to achieve high\nclassification performance, measured by the area under the receiver operating\ncurve (ROC-AUC). A multi-target learning approach to simultaneously output\nretinal biomarkers as well as T2D works best (AUC = 0.746 [$\\pm$0.001]).\nFurthermore, the classification performance can be improved when images with\nhigh prediction uncertainty are referred to a specialist. We also show that the\ncombination of images of the left and right eye per individual can further\nimprove the classification performance (AUC = 0.758 [$\\pm$0.003]), using a\nsimple averaging approach. The results are promising, suggesting the\nfeasibility of screening for T2D from retinal fundus images.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 13:17:04 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Heslinga", "Friso G.", ""], ["Pluim", "Josien P. W.", ""], ["Houben", "A. J. H. M.", ""], ["Schram", "Miranda T.", ""], ["Henry", "Ronald M. A.", ""], ["Stehouwer", "Coen D. A.", ""], ["van Greevenbroek", "Marleen J.", ""], ["Berendschot", "Tos T. J. M.", ""], ["Veta", "Mitko", ""]]}, {"id": "1911.10024", "submitter": "Angelo Porrello", "authors": "Stefano Vincenzi, Angelo Porrello, Pietro Buzzega, Annamaria Conte,\n  Carla Ippoliti, Luca Candeloro, Alessio Di Lorenzo, Andrea Capobianco\n  Dondona, Simone Calderara", "title": "Spotting insects from satellites: modeling the presence of Culicoides\n  imicola through Deep CNNs", "comments": "8 pages, 2 figures. Accepted in the 15th International Conference on\n  SIGNAL IMAGE TECHNOLOGY & INTERNET BASED SYSTEMS (SITIS-2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, Vector-Borne Diseases (VBDs) raise a severe threat for public\nhealth, accounting for a considerable amount of human illnesses. Recently,\nseveral surveillance plans have been put in place for limiting the spread of\nsuch diseases, typically involving on-field measurements. Such a systematic and\neffective plan still misses, due to the high costs and efforts required for\nimplementing it. Ideally, any attempt in this field should consider the\ntriangle vectors-host-pathogen, which is strictly linked to the environmental\nand climatic conditions. In this paper, we exploit satellite imagery from\nSentinel-2 mission, as we believe they encode the environmental factors\nresponsible for the vector's spread. Our analysis - conducted in a data-driver\nfashion - couples spectral images with ground-truth information on the\nabundance of Culicoides imicola. In this respect, we frame our task as a binary\nclassification problem, underpinning Convolutional Neural Networks (CNNs) as\nbeing able to learn useful representation from multi-band images. Additionally,\nwe provide a multi-instance variant, aimed at extracting temporal patterns from\na short sequence of spectral images. Experiments show promising results,\nproviding the foundations for novel supportive tools, which could depict where\nsurveillance and prevention measures could be prioritized.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 13:17:19 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Vincenzi", "Stefano", ""], ["Porrello", "Angelo", ""], ["Buzzega", "Pietro", ""], ["Conte", "Annamaria", ""], ["Ippoliti", "Carla", ""], ["Candeloro", "Luca", ""], ["Di Lorenzo", "Alessio", ""], ["Dondona", "Andrea Capobianco", ""], ["Calderara", "Simone", ""]]}, {"id": "1911.10033", "submitter": "Adrian Lopez-Rodriguez", "authors": "Adrian Lopez Rodriguez, Krystian Mikolajczyk", "title": "Domain Adaptation for Object Detection via Style Consistency", "comments": "BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a domain adaptation approach for object detection. We introduce a\ntwo-step method: the first step makes the detector robust to low-level\ndifferences and the second step adapts the classifiers to changes in the\nhigh-level features. For the first step, we use a style transfer method for\npixel-adaptation of source images to the target domain. We find that enforcing\nlow distance in the high-level features of the object detector between the\nstyle transferred images and the source images improves the performance in the\ntarget domain. For the second step, we propose a robust pseudo labelling\napproach to reduce the noise in both positive and negative sampling.\nExperimental evaluation is performed using the detector SSD300 on PASCAL VOC\nextended with the dataset proposed in arxiv:1803.11365 where the target domain\nimages are of different styles. Our approach significantly improves the\nstate-of-the-art performance in this benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 13:31:20 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Rodriguez", "Adrian Lopez", ""], ["Mikolajczyk", "Krystian", ""]]}, {"id": "1911.10037", "submitter": "Dhananjai Chand", "authors": "Earnest Paul Ijjina, Dhananjai Chand, Savyasachi Gupta, Goutham K", "title": "Computer Vision-based Accident Detection in Traffic Surveillance", "comments": "Accepted in 10th ICCCNT 2019", "journal-ref": "10th ICCCNT, 2019, pp. 1-6", "doi": "10.1109/ICCCNT45670.2019.8944469", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision-based accident detection through video surveillance has\nbecome a beneficial but daunting task. In this paper, a neoteric framework for\ndetection of road accidents is proposed. The proposed framework capitalizes on\nMask R-CNN for accurate object detection followed by an efficient centroid\nbased object tracking algorithm for surveillance footage. The probability of an\naccident is determined based on speed and trajectory anomalies in a vehicle\nafter an overlap with other vehicles. The proposed framework provides a robust\nmethod to achieve a high Detection Rate and a low False Alarm Rate on general\nroad-traffic CCTV surveillance footage. This framework was evaluated on diverse\nconditions such as broad daylight, low visibility, rain, hail, and snow using\nthe proposed dataset. This framework was found effective and paves the way to\nthe development of general-purpose vehicular accident detection algorithms in\nreal-time.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 13:38:06 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Ijjina", "Earnest Paul", ""], ["Chand", "Dhananjai", ""], ["Gupta", "Savyasachi", ""], ["K", "Goutham", ""]]}, {"id": "1911.10059", "submitter": "Jonas Bialopetravi\\v{c}ius", "authors": "J. Bialopetravi\\v{c}ius, D. Narbutis", "title": "Deriving star cluster parameters with convolutional neural networks. II.\n  Extinction and cluster/background classification", "comments": "17 pages, 21 figures", "journal-ref": "A&A 633, A148 (2020)", "doi": "10.1051/0004-6361/201936185", "report-no": null, "categories": "astro-ph.GA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context. Convolutional neural networks (CNNs) have been established as the\ngo-to method for fast object detection and classification on natural images.\nThis opens the door for astrophysical parameter inference on the exponentially\nincreasing amount of sky survey data. Until now, star cluster analysis was\nbased on integral or resolved stellar photometry, which limits the amount of\ninformation that can be extracted from individual pixels of cluster images.\n  Aims. We aim to create a CNN capable of inferring star cluster evolutionary,\nstructural, and environmental parameters from multi-band images, as well to\ndemonstrate its capabilities in discriminating genuine clusters from galactic\nstellar backgrounds.\n  Methods. A CNN based on the deep residual network (ResNet) architecture was\ncreated and trained to infer cluster ages, masses, sizes, and extinctions, with\nrespect to the degeneracies between them. Mock clusters placed on M83 Hubble\nSpace Telescope (HST) images utilizing three photometric passbands (F336W,\nF438W, and F814W) were used. The CNN is also capable of predicting the\nlikelihood of a cluster's presence in an image, as well as quantifying its\nvisibility (signal-to-noise).\n  Results. The CNN was tested on mock images of artificial clusters and has\ndemonstrated reliable inference results for clusters of ages $\\lesssim$100 Myr,\nextinctions $A_V$ between 0 and 3 mag, masses between $3\\times10^3$ and\n$3\\times10^5$ ${\\rm M_\\odot}$, and sizes between 0.04 and 0.4 arcsec at the\ndistance of the M83 galaxy. Real M83 galaxy cluster parameter inference tests\nwere performed with objects taken from previous studies and have demonstrated\nconsistent results.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 14:21:01 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Bialopetravi\u010dius", "J.", ""], ["Narbutis", "D.", ""]]}, {"id": "1911.10082", "submitter": "Arushi Goel", "authors": "Arushi Goel, Basura Fernando, Thanh-Son Nguyen, and Hakan Bilen", "title": "Injecting Prior Knowledge into Image Caption Generation", "comments": "ECCV20 VIPriors Workshop; 14 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatically generating natural language descriptions from an image is a\nchallenging problem in artificial intelligence that requires a good\nunderstanding of the visual and textual signals and the correlations between\nthem. The state-of-the-art methods in image captioning struggles to approach\nhuman level performance, especially when data is limited. In this paper, we\npropose to improve the performance of the state-of-the-art image captioning\nmodels by incorporating two sources of prior knowledge: (i) a conditional\nlatent topic attention, that uses a set of latent variables (topics) as an\nanchor to generate highly probable words and, (ii) a regularization technique\nthat exploits the inductive biases in syntactic and semantic structure of\ncaptions and improves the generalization of image captioning models. Our\nexperiments validate that our method produces more human interpretable captions\nand also leads to significant improvements on the MSCOCO dataset in both the\nfull and low data regimes.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 15:22:34 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 14:19:51 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Goel", "Arushi", ""], ["Fernando", "Basura", ""], ["Nguyen", "Thanh-Son", ""], ["Bilen", "Hakan", ""]]}, {"id": "1911.10090", "submitter": "Matteo Poggi", "authors": "Filippo Aleotti, Matteo Poggi, Fabio Tosi, Stefano Mattoccia", "title": "Learning End-To-End Scene Flow by Distilling Single Tasks Knowledge", "comments": "Accepted to AAAI 2020. Project page:\n  https://vision.disi.unibo.it/~faleotti/dwarf.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene flow is a challenging task aimed at jointly estimating the 3D structure\nand motion of the sensed environment. Although deep learning solutions achieve\noutstanding performance in terms of accuracy, these approaches divide the whole\nproblem into standalone tasks (stereo and optical flow) addressing them with\nindependent networks. Such a strategy dramatically increases the complexity of\nthe training procedure and requires power-hungry GPUs to infer scene flow\nbarely at 1 FPS. Conversely, we propose DWARF, a novel and lightweight\narchitecture able to infer full scene flow jointly reasoning about depth and\noptical flow easily and elegantly trainable end-to-end from scratch. Moreover,\nsince ground truth images for full scene flow are scarce, we propose to\nleverage on the knowledge learned by networks specialized in stereo or flow,\nfor which much more data are available, to distill proxy annotations.\nExhaustive experiments show that i) DWARF runs at about 10 FPS on a single\nhigh-end GPU and about 1 FPS on NVIDIA Jetson TX2 embedded at KITTI resolution,\nwith moderate drop in accuracy compared to 10x deeper models, ii) learning from\nmany distilled samples is more effective than from the few, annotated ones\navailable. Code available at:\nhttps://github.com/FilippoAleotti/Dwarf-Tensorflow\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 15:38:14 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Aleotti", "Filippo", ""], ["Poggi", "Matteo", ""], ["Tosi", "Fabio", ""], ["Mattoccia", "Stefano", ""]]}, {"id": "1911.10091", "submitter": "Yucheng Zhu", "authors": "Yucheng Zhu, Yanrong Ji, Yueying Zhang, Linxin Xu, Aven Le Zhou,\n  Ellick Chan", "title": "Machine: The New Art Connoisseur", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The process of identifying and understanding art styles to discover artistic\ninfluences is essential to the study of art history. Traditionally, trained\nexperts review fine details of the works and compare them to other known works.\nTo automate and scale this task, we use several state-of-the-art CNN\narchitectures to explore how a machine may help perceive and quantify art\nstyles. This study explores: (1) How accurately can a machine classify art\nstyles? (2) What may be the underlying relationships among different styles and\nartists? To help answer the first question, our best-performing model using\nInception V3 achieves a 9-class classification accuracy of 88.35%, which\noutperforms the model in Elgammal et al.'s study by more than 20 percent.\nVisualizations using Grad-CAM heat maps confirm that the model correctly\nfocuses on the characteristic parts of paintings. To help address the second\nquestion, we conduct network analysis on the influences among styles and\nartists by extracting 512 features from the best-performing classification\nmodel. Through 2D and 3D T-SNE visualizations, we observe clear chronological\npatterns of development and separation among the art styles. The network\nanalysis also appears to show anticipated artist level connections from an art\nhistorical perspective. This technique appears to help identify some previously\nunknown linkages that may shed light upon new directions for further\nexploration by art historians. We hope that humans and machines working in\nconcert may bring new opportunities to the field.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 15:44:56 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 18:19:34 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Zhu", "Yucheng", ""], ["Ji", "Yanrong", ""], ["Zhang", "Yueying", ""], ["Xu", "Linxin", ""], ["Zhou", "Aven Le", ""], ["Chan", "Ellick", ""]]}, {"id": "1911.10097", "submitter": "Fangyu Liu", "authors": "Fangyu Liu, Rongtian Ye, Xun Wang, Shuaipeng Li", "title": "HAL: Improved Text-Image Matching by Mitigating Visual Semantic Hubs", "comments": "AAAI-20 (to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The hubness problem widely exists in high-dimensional embedding space and is\na fundamental source of error for cross-modal matching tasks. In this work, we\nstudy the emergence of hubs in Visual Semantic Embeddings (VSE) with\napplication to text-image matching. We analyze the pros and cons of two widely\nadopted optimization objectives for training VSE and propose a novel\nhubness-aware loss function (HAL) that addresses previous methods' defects.\nUnlike (Faghri et al.2018) which simply takes the hardest sample within a\nmini-batch, HAL takes all samples into account, using both local and global\nstatistics to scale up the weights of \"hubs\". We experiment our method with\nvarious configurations of model architectures and datasets. The method exhibits\nexceptionally good robustness and brings consistent improvement on the task of\ntext-image matching across all settings. Specifically, under the same model\narchitectures as (Faghri et al. 2018) and (Lee at al. 2018), by switching only\nthe learning objective, we report a maximum R@1improvement of 7.4% on MS-COCO\nand 8.3% on Flickr30k.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 15:51:08 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Liu", "Fangyu", ""], ["Ye", "Rongtian", ""], ["Wang", "Xun", ""], ["Li", "Shuaipeng", ""]]}, {"id": "1911.10115", "submitter": "Chiranjib Sur", "authors": "Chiranjib Sur", "title": "TPsgtR: Neural-Symbolic Tensor Product Scene-Graph-Triplet\n  Representation for Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Image captioning can be improved if the structure of the graphical\nrepresentations can be formulated with conceptual positional binding. In this\nwork, we have introduced a novel technique for caption generation using the\nneural-symbolic encoding of the scene-graphs, derived from regional visual\ninformation of the images and we call it Tensor Product Scene-Graph-Triplet\nRepresentation (TP$_{sgt}$R). While, most of the previous works concentrated on\nidentification of the object features in images, we introduce a neuro-symbolic\nembedding that can embed identified relationships among different regions of\nthe image into concrete forms, instead of relying on the model to compose for\nany/all combinations. These neural symbolic representation helps in better\ndefinition of the neural symbolic space for neuro-symbolic attention and can be\ntransformed to better captions. With this approach, we introduced two novel\narchitectures (TP$_{sgt}$R-TDBU and TP$_{sgt}$R-sTDBU) for comparison and\nexperiment result demonstrates that our approaches outperformed the other\nmodels, and generated captions are more comprehensive and natural.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 16:17:21 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Sur", "Chiranjib", ""]]}, {"id": "1911.10118", "submitter": "Karthik Gopinath", "authors": "Ran He, Karthik Gopinath, Christian Desrosiers and Herve Lombaert", "title": "Spectral Graph Transformer Networks for Brain Surface Parcellation", "comments": "Equal contribution of R. He and K. Gopinath", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of the brain surface modeled as a graph mesh is a challenging\ntask. Conventional deep learning approaches often rely on data lying in the\nEuclidean space. As an extension to irregular graphs, convolution operations\nare defined in the Fourier or spectral domain. This spectral domain is obtained\nby decomposing the graph Laplacian, which captures relevant shape information.\nHowever, the spectral decomposition across different brain graphs causes\ninconsistencies between the eigenvectors of individual spectral domains,\ncausing the graph learning algorithm to fail. Current spectral graph\nconvolution methods handle this variance by separately aligning the\neigenvectors to a reference brain in a slow iterative step. This paper presents\na novel approach for learning the transformation matrix required for aligning\nbrain meshes using a direct data-driven approach. Our alignment and graph\nprocessing method provides a fast analysis of brain surfaces. The novel\nSpectral Graph Transformer (SGT) network proposed in this paper uses very few\nrandomly sub-sampled nodes in the spectral domain to learn the alignment matrix\nfor multiple brain surfaces. We validate the use of this SGT network along with\na graph convolution network to perform cortical parcellation. Our method on 101\nmanually-labeled brain surfaces shows improved parcellation performance over a\nno-alignment strategy, gaining a significant speed (1400 fold) over traditional\niterative alignment approaches.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 16:18:07 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["He", "Ran", ""], ["Gopinath", "Karthik", ""], ["Desrosiers", "Christian", ""], ["Lombaert", "Herve", ""]]}, {"id": "1911.10127", "submitter": "Yao Yao None", "authors": "Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou,\n  Tian Fang, Long Quan", "title": "BlendedMVS: A Large-scale Dataset for Generalized Multi-view Stereo\n  Networks", "comments": "Accepted to CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning has recently achieved great success on multi-view stereo\n(MVS), limited training data makes the trained model hard to be generalized to\nunseen scenarios. Compared with other computer vision tasks, it is rather\ndifficult to collect a large-scale MVS dataset as it requires expensive active\nscanners and labor-intensive process to obtain ground truth 3D structures. In\nthis paper, we introduce BlendedMVS, a novel large-scale dataset, to provide\nsufficient training ground truth for learning-based MVS. To create the dataset,\nwe apply a 3D reconstruction pipeline to recover high-quality textured meshes\nfrom images of well-selected scenes. Then, we render these mesh models to color\nimages and depth maps. To introduce the ambient lighting information during\ntraining, the rendered color images are further blended with the input images\nto generate the training input. Our dataset contains over 17k high-resolution\nimages covering a variety of scenes, including cities, architectures,\nsculptures and small objects. Extensive experiments demonstrate that BlendedMVS\nendows the trained model with significantly better generalization ability\ncompared with other MVS datasets. The dataset and pretrained models are\navailable at \\url{https://github.com/YoYo000/BlendedMVS}.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 16:29:12 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 15:17:04 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Yao", "Yao", ""], ["Luo", "Zixin", ""], ["Li", "Shiwei", ""], ["Zhang", "Jingyang", ""], ["Ren", "Yufan", ""], ["Zhou", "Lei", ""], ["Fang", "Tian", ""], ["Quan", "Long", ""]]}, {"id": "1911.10129", "submitter": "Karthik Gopinath", "authors": "Karthik Gopinath, Christian Desrosiers, and Herve Lombaert", "title": "Learnable Pooling in Graph Convolution Networks for Brain Surface\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain surface analysis is essential to neuroscience, however, the complex\ngeometry of the brain cortex hinders computational methods for this task. The\ndifficulty arises from a discrepancy between 3D imaging data, which is\nrepresented in Euclidean space, and the non-Euclidean geometry of the\nhighly-convoluted brain surface. Recent advances in machine learning have\nenabled the use of neural networks for non-Euclidean spaces. These facilitate\nthe learning of surface data, yet pooling strategies often remain constrained\nto a single fixed-graph. This paper proposes a new learnable graph pooling\nmethod for processing multiple surface-valued data to output subject-based\ninformation. The proposed method innovates by learning an intrinsic aggregation\nof graph nodes based on graph spectral embedding. We illustrate the advantages\nof our approach with in-depth experiments on two large-scale benchmark\ndatasets. The flexibility of the pooling strategy is evaluated on four\ndifferent prediction tasks, namely, subject-sex classification, regression of\ncortical region sizes, classification of Alzheimer's disease stages, and brain\nage regression. Our experiments demonstrate the superiority of our learnable\npooling approach compared to other pooling techniques for graph convolution\nnetworks, with results improving the state-of-the-art in brain surface\nanalysis.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 16:34:58 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Gopinath", "Karthik", ""], ["Desrosiers", "Christian", ""], ["Lombaert", "Herve", ""]]}, {"id": "1911.10143", "submitter": "Taihong Xiao", "authors": "Taihong Xiao, Yi-Hsuan Tsai, Kihyuk Sohn, Manmohan Chandraker,\n  Ming-Hsuan Yang", "title": "Adversarial Learning of Privacy-Preserving and Task-Oriented\n  Representations", "comments": null, "journal-ref": "AAAI 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Data privacy has emerged as an important issue as data-driven deep learning\nhas been an essential component of modern machine learning systems. For\ninstance, there could be a potential privacy risk of machine learning systems\nvia the model inversion attack, whose goal is to reconstruct the input data\nfrom the latent representation of deep networks. Our work aims at learning a\nprivacy-preserving and task-oriented representation to defend against such\nmodel inversion attacks. Specifically, we propose an adversarial reconstruction\nlearning framework that prevents the latent representations decoded into\noriginal input data. By simulating the expected behavior of adversary, our\nframework is realized by minimizing the negative pixel reconstruction loss or\nthe negative feature reconstruction (i.e., perceptual distance) loss. We\nvalidate the proposed method on face attribute prediction, showing that our\nmethod allows protecting visual privacy with a small decrease in utility\nperformance. In addition, we show the utility-privacy trade-off with different\nchoices of hyperparameter for negative perceptual distance loss at training,\nallowing service providers to determine the right level of privacy-protection\nwith a certain utility performance. Moreover, we provide an extensive study\nwith different selections of features, tasks, and the data to further analyze\ntheir influence on privacy protection.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 17:06:28 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Xiao", "Taihong", ""], ["Tsai", "Yi-Hsuan", ""], ["Sohn", "Kihyuk", ""], ["Chandraker", "Manmohan", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1911.10150", "submitter": "Alex Lang", "authors": "Sourabh Vora, Alex H. Lang, Bassam Helou, and Oscar Beijbom", "title": "PointPainting: Sequential Fusion for 3D Object Detection", "comments": "11 pages, 6 figures, 8 tables. v1 is initial submission to CVPR 2020.\n  v2 is final version accepted for publication at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera and lidar are important sensor modalities for robotics in general and\nself-driving cars in particular. The sensors provide complementary information\noffering an opportunity for tight sensor-fusion. Surprisingly, lidar-only\nmethods outperform fusion methods on the main benchmark datasets, suggesting a\ngap in the literature. In this work, we propose PointPainting: a sequential\nfusion method to fill this gap. PointPainting works by projecting lidar points\ninto the output of an image-only semantic segmentation network and appending\nthe class scores to each point. The appended (painted) point cloud can then be\nfed to any lidar-only method. Experiments show large improvements on three\ndifferent state-of-the art methods, Point-RCNN, VoxelNet and PointPillars on\nthe KITTI and nuScenes datasets. The painted version of PointRCNN represents a\nnew state of the art on the KITTI leaderboard for the bird's-eye view detection\ntask. In ablation, we study how the effects of Painting depends on the quality\nand format of the semantic segmentation output, and demonstrate how latency can\nbe minimized through pipelining.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 17:19:50 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 17:17:18 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Vora", "Sourabh", ""], ["Lang", "Alex H.", ""], ["Helou", "Bassam", ""], ["Beijbom", "Oscar", ""]]}, {"id": "1911.10194", "submitter": "Bowen Cheng", "authors": "Bowen Cheng and Maxwell D. Collins and Yukun Zhu and Ting Liu and\n  Thomas S. Huang and Hartwig Adam and Liang-Chieh Chen", "title": "Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up\n  Panoptic Segmentation", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce Panoptic-DeepLab, a simple, strong, and fast\nsystem for panoptic segmentation, aiming to establish a solid baseline for\nbottom-up methods that can achieve comparable performance of two-stage methods\nwhile yielding fast inference speed. In particular, Panoptic-DeepLab adopts the\ndual-ASPP and dual-decoder structures specific to semantic, and instance\nsegmentation, respectively. The semantic segmentation branch is the same as the\ntypical design of any semantic segmentation model (e.g., DeepLab), while the\ninstance segmentation branch is class-agnostic, involving a simple instance\ncenter regression. As a result, our single Panoptic-DeepLab simultaneously\nranks first at all three Cityscapes benchmarks, setting the new state-of-art of\n84.2% mIoU, 39.0% AP, and 65.5% PQ on test set. Additionally, equipped with\nMobileNetV3, Panoptic-DeepLab runs nearly in real-time with a single 1025x2049\nimage (15.8 frames per second), while achieving a competitive performance on\nCityscapes (54.1 PQ% on test set). On Mapillary Vistas test set, our ensemble\nof six models attains 42.7% PQ, outperforming the challenge winner in 2018 by a\nhealthy margin of 1.5%. Finally, our Panoptic-DeepLab also performs on par with\nseveral top-down approaches on the challenging COCO dataset. For the first\ntime, we demonstrate a bottom-up approach could deliver state-of-the-art\nresults on panoptic segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 18:59:51 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 17:45:21 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2020 17:59:11 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Cheng", "Bowen", ""], ["Collins", "Maxwell D.", ""], ["Zhu", "Yukun", ""], ["Liu", "Ting", ""], ["Huang", "Thomas S.", ""], ["Adam", "Hartwig", ""], ["Chen", "Liang-Chieh", ""]]}, {"id": "1911.10248", "submitter": "Jisan Mahmud", "authors": "Jisan Mahmud, Rajat Vikram Singh, Peri Akiva, Spondon Kundu,\n  Kuan-Chuan Peng, Jan-Michael Frahm", "title": "ViewSynth: Learning Local Features from Depth using View Synthesis", "comments": "Accepted to BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of inexpensive commodity depth sensors has made\nkeypoint detection and matching in the depth image modality an important\nproblem in computer vision. Despite great improvements in recent RGB local\nfeature learning methods, adapting them directly in the depth modality leads to\nunsatisfactory performance. Most of these methods do not explicitly reason\nbeyond the visible pixels in the images. To address the limitations of these\nmethods, we propose a framework ViewSynth, to jointly learn: (1) viewpoint\ninvariant keypoint-descriptor from depth images using a proposed Contrastive\nMatching Loss, and (2) view synthesis of depth images from different viewpoints\nusing the proposed View Synthesis Module and View Synthesis Loss. By learning\nview synthesis, we explicitly encourage the feature extractor to encode\ninformation about not only the visible, but also the occluded parts of the\nscene. We demonstrate that in the depth modality, ViewSynth outperforms the\nstate-of-the-art depth and RGB local feature extraction techniques in the 3D\nkeypoint matching and camera localization tasks on the RGB-D datasets 7-Scenes,\nTUM RGBD and CoRBS in most scenarios. We also show the generalizability of\nViewSynth in 3D keypoint matching across different datasets.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 21:01:33 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 21:47:46 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 17:31:24 GMT"}, {"version": "v4", "created": "Tue, 1 Sep 2020 21:19:21 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Mahmud", "Jisan", ""], ["Singh", "Rajat Vikram", ""], ["Akiva", "Peri", ""], ["Kundu", "Spondon", ""], ["Peng", "Kuan-Chuan", ""], ["Frahm", "Jan-Michael", ""]]}, {"id": "1911.10249", "submitter": "Wadim Kehl", "authors": "Wadim Kehl, Federico Tombari, Slobodan Ilic, Nassir Navab", "title": "Real-Time 3D Model Tracking in Color and Depth on a Single CPU Core", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method to track 3D models in color and depth data. To this\nend, we introduce approximations that accelerate the state-of-the-art in\nregion-based tracking by an order of magnitude while retaining similar\naccuracy. Furthermore, we show how the method can be made more robust in the\npresence of depth data and consequently formulate a new joint contour and ICP\ntracking energy. We present better results than the state-of-the-art while\nbeing much faster then most other methods and achieving all of the above on a\nsingle CPU core.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 21:02:24 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Kehl", "Wadim", ""], ["Tombari", "Federico", ""], ["Ilic", "Slobodan", ""], ["Navab", "Nassir", ""]]}, {"id": "1911.10291", "submitter": "Wei-An Lin", "authors": "Wei-An Lin and Yogesh Balaji and Pouya Samangouei and Rama Chellappa", "title": "Invert and Defend: Model-based Approximate Inversion of Generative\n  Adversarial Networks for Secure Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring the latent variable generating a given test sample is a challenging\nproblem in Generative Adversarial Networks (GANs). In this paper, we propose\nInvGAN - a novel framework for solving the inference problem in GANs, which\ninvolves training an encoder network capable of inverting a pre-trained\ngenerator network without access to any training data. Under mild assumptions,\nwe theoretically show that using InvGAN, we can approximately invert the\ngenerations of any latent code of a trained GAN model. Furthermore, we\nempirically demonstrate the superiority of our inference scheme by quantitative\nand qualitative comparisons with other methods that perform a similar task. We\nalso show the effectiveness of our framework in the problem of adversarial\ndefenses where InvGAN can successfully be used as a projection-based defense\nmechanism. Additionally, we show how InvGAN can be used to implement\nreparameterization white-box attacks on projection-based defense mechanisms.\nExperimental validation on several benchmark datasets demonstrate the efficacy\nof our method in achieving improved performance on several white-box and\nblack-box attacks. Our code is available at\nhttps://github.com/yogeshbalaji/InvGAN.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 01:15:32 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Lin", "Wei-An", ""], ["Balaji", "Yogesh", ""], ["Samangouei", "Pouya", ""], ["Chellappa", "Rama", ""]]}, {"id": "1911.10301", "submitter": "He-Feng Yin", "authors": "He-Feng Yin, Xiao-Jun Wu, Josef Kittler and Zhen-Hua Feng", "title": "Learning a Representation with the Block-Diagonal Structure for Pattern\n  Classification", "comments": "accepted by Pattern Analysis and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse-representation-based classification (SRC) has been widely studied and\ndeveloped for various practical signal classification applications. However,\nthe performance of a SRC-based method is degraded when both the training and\ntest data are corrupted. To counteract this problem, we propose an approach\nthat learns Representation with Block-Diagonal Structure (RBDS) for robust\nimage recognition. To be more specific, we first introduce a regularization\nterm that captures the block-diagonal structure of the target representation\nmatrix of the training data. The resulting problem is then solved by an\noptimizer. Last, based on the learned representation, a simple yet effective\nlinear classifier is used for the classification task. The experimental results\nobtained on several benchmarking datasets demonstrate the efficacy of the\nproposed RBDS method.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 02:40:35 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Yin", "He-Feng", ""], ["Wu", "Xiao-Jun", ""], ["Kittler", "Josef", ""], ["Feng", "Zhen-Hua", ""]]}, {"id": "1911.10317", "submitter": "Pratik Kayal", "authors": "Davinder Singh, Naman Jain, Pranjali Jain, Pratik Kayal, Sudhakar\n  Kumawat, Nipun Batra", "title": "PlantDoc: A Dataset for Visual Plant Disease Detection", "comments": "5 Pages, 6 figures, 3 tables", "journal-ref": null, "doi": "10.1145/3371158.3371196", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  India loses 35% of the annual crop yield due to plant diseases. Early\ndetection of plant diseases remains difficult due to the lack of lab\ninfrastructure and expertise. In this paper, we explore the possibility of\ncomputer vision approaches for scalable and early plant disease detection. The\nlack of availability of sufficiently large-scale non-lab data set remains a\nmajor challenge for enabling vision based plant disease detection. Against this\nbackground, we present PlantDoc: a dataset for visual plant disease detection.\nOur dataset contains 2,598 data points in total across 13 plant species and up\nto 17 classes of diseases, involving approximately 300 human hours of effort in\nannotating internet scraped images. To show the efficacy of our dataset, we\nlearn 3 models for the task of plant disease classification. Our results show\nthat modelling using our dataset can increase the classification accuracy by up\nto 31%. We believe that our dataset can help reduce the entry barrier of\ncomputer vision techniques in plant disease detection.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 06:45:03 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Singh", "Davinder", ""], ["Jain", "Naman", ""], ["Jain", "Pranjali", ""], ["Kayal", "Pratik", ""], ["Kumawat", "Sudhakar", ""], ["Batra", "Nipun", ""]]}, {"id": "1911.10334", "submitter": "Xuan Liao", "authors": "Xuan Liao, Wenhao Li, Qisen Xu, Xiangfeng Wang, Bo Jin, Xiaoyun Zhang,\n  Ya Zhang and Yanfeng Wang", "title": "Iteratively-Refined Interactive 3D Medical Image Segmentation with\n  Multi-Agent Reinforcement Learning", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing automatic 3D image segmentation methods usually fail to meet the\nclinic use. Many studies have explored an interactive strategy to improve the\nimage segmentation performance by iteratively incorporating user hints.\nHowever, the dynamic process for successive interactions is largely ignored. We\nhere propose to model the dynamic process of iterative interactive image\nsegmentation as a Markov decision process (MDP) and solve it with reinforcement\nlearning (RL). Unfortunately, it is intractable to use single-agent RL for\nvoxel-wise prediction due to the large exploration space. To reduce the\nexploration space to a tractable size, we treat each voxel as an agent with a\nshared voxel-level behavior strategy so that it can be solved with multi-agent\nreinforcement learning. An additional advantage of this multi-agent model is to\ncapture the dependency among voxels for segmentation task. Meanwhile, to enrich\nthe information of previous segmentations, we reserve the prediction\nuncertainty in the state space of MDP and derive an adjustment action space\nleading to a more precise and finer segmentation. In addition, to improve the\nefficiency of exploration, we design a relative cross-entropy gain-based reward\nto update the policy in a constrained direction. Experimental results on\nvarious medical datasets have shown that our method significantly outperforms\nexisting state-of-the-art methods, with the advantage of fewer interactions and\na faster convergence.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 09:20:19 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Liao", "Xuan", ""], ["Li", "Wenhao", ""], ["Xu", "Qisen", ""], ["Wang", "Xiangfeng", ""], ["Jin", "Bo", ""], ["Zhang", "Xiaoyun", ""], ["Zhang", "Ya", ""], ["Wang", "Yanfeng", ""]]}, {"id": "1911.10335", "submitter": "Di Wu", "authors": "Di Wu, Chao Wang, Yong Wu and De-Shuang Huang", "title": "Attention Deep Model with Multi-Scale Deep Supervision for Person\n  Re-Identification", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, person re-identification (PReID) has become a hot topic in\ncomputer vision duo to it is an important part in intelligent surveillance.\nMany state-of-the-art PReID methods are attention-based or multi-scale feature\nlearning deep models. However, introducing attention mechanism may lead to some\nimportant feature information losing issue. Besides, most of the multi-scale\nmodels embedding the multi-scale feature learning block into the feature\nextraction deep network, which reduces the efficiency of inference network. To\naddress these issue, in this study, we introduce an attention deep architecture\nwith multi-scale deep supervision for PReID. Technically, we contribute a\nreverse attention block to complement the attention block, and a novel\nmulti-scale layer with deep supervision operator for training the backbone\nnetwork. The proposed block and operator are only used for training, and\ndiscard in test phase. Experiments have been performed on Market-1501,\nDukeMTMC-reID and CUHK03 datasets. All the experiment results show that the\nproposed model significantly outperforms the other competitive state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 09:27:53 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 09:01:52 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 08:08:09 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Wu", "Di", ""], ["Wang", "Chao", ""], ["Wu", "Yong", ""], ["Huang", "De-Shuang", ""]]}, {"id": "1911.10346", "submitter": "Zhe Zhang", "authors": "Zhe Zhang, Jie Tang, Gangshan Wu", "title": "Simple and Lightweight Human Pose Estimation", "comments": "results of inference speed corrected, github url added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on human pose estimation has achieved significant\nimprovement. However, most existing methods tend to pursue higher scores using\ncomplex architecture or computationally expensive models on benchmark datasets,\nignoring the deployment costs in practice. In this paper, we investigate the\nproblem of simple and lightweight human pose estimation. We first redesign a\nlightweight bottleneck block with two non-novel concepts: depthwise convolution\nand attention mechanism. And then, based on the lightweight block, we present a\nLightweight Pose Network (LPN) following the architecture design principles of\nSimpleBaseline. The model size (#Params) of our small network LPN-50 is only 9%\nof SimpleBaseline(ResNet50), and the computational complexity (FLOPs) is only\n11%. To give full play to the potential of our LPN and get more accurate\npredicted results, we also propose an iterative training strategy and a\nmodel-agnostic post-processing function Beta-Soft-Argmax. We empirically\ndemonstrate the effectiveness and efficiency of our methods on the benchmark\ndataset: the COCO keypoint detection dataset. Besides, we show the speed\nsuperiority of our lightweight network at inference time on a non-GPU platform.\nSpecifically, our LPN-50 can achieve 68.7 in AP score on the COCO test-dev set,\nwith only 2.7M parameters and 1.0 GFLOPs, while the inference speed is 17 FPS\non an Intel i7-8700K CPU machine.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 10:42:07 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 11:55:58 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Zhang", "Zhe", ""], ["Tang", "Jie", ""], ["Wu", "Gangshan", ""]]}, {"id": "1911.10352", "submitter": "Yashna Islam", "authors": "Md Abdul Mutalab Shaykat, Yashna Islam, Mohammad Ishtiaque Hossain", "title": "Shape Detection of Liver From 2D Ultrasound Images", "comments": "arXiv admin note: submission has been withdrawn by arXiv\n  administrators due to inappropriate overlap with external sources", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications of ultrasound images have expanded from fetal imaging to\nabdominal and cardiac diagnosis. Liver-being the largest gland in the body and\nresponsible for metabolic activities requires to be to be diagnosed and\ntherefore subject to utmost injury. Although, ultrasound imaging has developed\ninto three and four dimensions providing higher amount of information; it\nrequires highly trained medical staff due to the image complexity and\ndimensions it contain. Since 2D ultrasound images are still considered to be\nthe basis of clinical treatments,computer aided automated liver diagnosis is\nvery essential. Due to the limitations of ultrasound images, such as loss of\nresolution leading to speckle noise, it is difficult to detect shape of\norgans.In this project, we propose a shape detection method for liver in 2D\nUltrasound images. Then we compare the accuracies of the method for both noise\nand after noise removal.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 12:00:31 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Shaykat", "Md Abdul Mutalab", ""], ["Islam", "Yashna", ""], ["Hossain", "Mohammad Ishtiaque", ""]]}, {"id": "1911.10354", "submitter": "Kohei Uehara", "authors": "Kohei Uehara, Tatsuya Harada", "title": "Unsupervised Keyword Extraction for Full-sentence VQA", "comments": "EMNLP 2020 workshop: NLP Beyond Text (NLPBT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the majority of the existing Visual Question Answering (VQA) research, the\nanswers consist of short, often single words, as per instructions given to the\nannotators during dataset construction. This study envisions a VQA task for\nnatural situations, where the answers are more likely to be sentences rather\nthan single words. To bridge the gap between this natural VQA and existing VQA\napproaches, a novel unsupervised keyword extraction method is proposed. The\nmethod is based on the principle that the full-sentence answers can be\ndecomposed into two parts: one that contains new information answering the\nquestion (i.e., keywords), and one that contains information already included\nin the question. Discriminative decoders were designed to achieve such\ndecomposition, and the method was experimentally implemented on VQA datasets\ncontaining full-sentence answers. The results show that the proposed model can\naccurately extract the keywords without being given explicit annotations\ndescribing them.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 12:18:03 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 07:37:34 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2020 09:20:30 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Uehara", "Kohei", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1911.10360", "submitter": "Chaowei Fang", "authors": "Chaowei Fang, Guanbin Li, Chengwei Pan, Yiming Li, Yizhou Yu", "title": "Globally Guided Progressive Fusion Network for 3D Pancreas Segmentation", "comments": "MICCAI2019", "journal-ref": null, "doi": "10.1007/978-3-030-32245-8_24", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently 3D volumetric organ segmentation attracts much research interest in\nmedical image analysis due to its significance in computer aided diagnosis.\nThis paper aims to address the pancreas segmentation task in 3D computed\ntomography volumes. We propose a novel end-to-end network, Globally Guided\nProgressive Fusion Network, as an effective and efficient solution to\nvolumetric segmentation, which involves both global features and complicated 3D\ngeometric information. A progressive fusion network is devised to extract 3D\ninformation from a moderate number of neighboring slices and predict a\nprobability map for the segmentation of each slice. An independent branch for\nexcavating global features from downsampled slices is further integrated into\nthe network. Extensive experimental results demonstrate that our method\nachieves state-of-the-art performance on two pancreas datasets.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 12:51:44 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Fang", "Chaowei", ""], ["Li", "Guanbin", ""], ["Pan", "Chengwei", ""], ["Li", "Yiming", ""], ["Yu", "Yizhou", ""]]}, {"id": "1911.10364", "submitter": "Kenneth Co", "authors": "Kenneth T. Co, Luis Mu\\~noz-Gonz\\'alez, Leslie Kanthan, Ben Glocker,\n  Emil C. Lupu", "title": "Universal Adversarial Robustness of Texture and Shape-Biased Models", "comments": "Code available at: https://github.com/kenny-co/sgd-uap-torch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Increasing shape-bias in deep neural networks has been shown to improve\nrobustness to common corruptions and noise. In this paper we analyze the\nadversarial robustness of texture and shape-biased models to Universal\nAdversarial Perturbations (UAPs). We use UAPs to evaluate the robustness of DNN\nmodels with varying degrees of shape-based training. We find that shape-biased\nmodels do not markedly improve adversarial robustness, and we show that\nensembles of texture and shape-biased models can improve universal adversarial\nrobustness while maintaining strong performance.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 13:23:45 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 20:54:44 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2021 06:55:29 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Co", "Kenneth T.", ""], ["Mu\u00f1oz-Gonz\u00e1lez", "Luis", ""], ["Kanthan", "Leslie", ""], ["Glocker", "Ben", ""], ["Lupu", "Emil C.", ""]]}, {"id": "1911.10371", "submitter": "Lei Qi", "authors": "Pinzhuo Tian, Zhangkai Wu, Lei Qi, Lei Wang, Yinghuan Shi, Yang Gao", "title": "Differentiable Meta-learning Model for Few-shot Semantic Segmentation", "comments": "Accepted by AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address the annotation scarcity issue in some cases of semantic\nsegmentation, there have been a few attempts to develop the segmentation model\nin the few-shot learning paradigm. However, most existing methods only focus on\nthe traditional 1-way segmentation setting (i.e., one image only contains a\nsingle object). This is far away from practical semantic segmentation tasks\nwhere the K-way setting (K>1) is usually required by performing the accurate\nmulti-object segmentation. To deal with this issue, we formulate the few-shot\nsemantic segmentation task as a learning-based pixel classification problem and\npropose a novel framework called MetaSegNet based on meta-learning. In\nMetaSegNet, an architecture of embedding module consisting of the global and\nlocal feature branches is developed to extract the appropriate meta-knowledge\nfor the few-shot segmentation. Moreover, we incorporate a linear model into\nMetaSegNet as a base learner to directly predict the label of each pixel for\nthe multi-object segmentation. Furthermore, our MetaSegNet can be trained by\nthe episodic training mechanism in an end-to-end manner from scratch.\nExperiments on two popular semantic segmentation datasets, i.e., PASCAL VOC and\nCOCO, reveal the effectiveness of the proposed MetaSegNet in the K-way few-shot\nsemantic segmentation task.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 14:12:17 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Tian", "Pinzhuo", ""], ["Wu", "Zhangkai", ""], ["Qi", "Lei", ""], ["Wang", "Lei", ""], ["Shi", "Yinghuan", ""], ["Gao", "Yang", ""]]}, {"id": "1911.10373", "submitter": "Zhuo Feng", "authors": "Yongyu Wang, Zhiqiang Zhao, and Zhuo Feng", "title": "GRASPEL: Graph Spectral Learning at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning meaningful graphs from data plays important roles in many data\nmining and machine learning tasks, such as data representation and analysis,\ndimension reduction, data clustering, and visualization, etc. In this work, for\nthe first time, we present a highly-scalable spectral approach (GRASPEL) for\nlearning large graphs from data. By limiting the precision matrix to be a graph\nLaplacian, our approach aims to estimate ultra-sparse (tree-like) weighted\nundirected graphs and shows a clear connection with the prior graphical Lasso\nmethod. By interleaving the latest high-performance nearly-linear time spectral\nmethods for graph sparsification, coarsening and embedding, ultra-sparse yet\nspectrally-robust graphs can be learned by identifying and including the most\nspectrally-critical edges into the graph. Compared with prior state-of-the-art\ngraph learning approaches, GRASPEL is more scalable and allows substantially\nimproving computing efficiency and solution quality of a variety of data mining\nand machine learning applications, such as spectral clustering (SC), and\nt-Distributed Stochastic Neighbor Embedding (t-SNE). {For example, when\ncomparing with graphs constructed using existing methods, GRASPEL achieved the\nbest spectral clustering efficiency and accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 14:51:13 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 00:19:38 GMT"}, {"version": "v3", "created": "Tue, 28 Jul 2020 21:08:51 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Wang", "Yongyu", ""], ["Zhao", "Zhiqiang", ""], ["Feng", "Zhuo", ""]]}, {"id": "1911.10375", "submitter": "Tao Yu", "authors": "Tao Yu, Zongyu Guo, Xin Jin, Shilin Wu, Zhibo Chen, Weiping Li,\n  Zhizheng Zhang, Sen Liu", "title": "Region Normalization for Image Inpainting", "comments": "Accepted by AAAI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature Normalization (FN) is an important technique to help neural network\ntraining, which typically normalizes features across spatial dimensions. Most\nprevious image inpainting methods apply FN in their networks without\nconsidering the impact of the corrupted regions of the input image on\nnormalization, e.g. mean and variance shifts. In this work, we show that the\nmean and variance shifts caused by full-spatial FN limit the image inpainting\nnetwork training and we propose a spatial region-wise normalization named\nRegion Normalization (RN) to overcome the limitation. RN divides spatial pixels\ninto different regions according to the input mask, and computes the mean and\nvariance in each region for normalization. We develop two kinds of RN for our\nimage inpainting network: (1) Basic RN (RN-B), which normalizes pixels from the\ncorrupted and uncorrupted regions separately based on the original inpainting\nmask to solve the mean and variance shift problem; (2) Learnable RN (RN-L),\nwhich automatically detects potentially corrupted and uncorrupted regions for\nseparate normalization, and performs global affine transformation to enhance\ntheir fusion. We apply RN-B in the early layers and RN-L in the latter layers\nof the network respectively. Experiments show that our method outperforms\ncurrent state-of-the-art methods quantitatively and qualitatively. We further\ngeneralize RN to other inpainting networks and achieve consistent performance\nimprovements.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 15:16:36 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Yu", "Tao", ""], ["Guo", "Zongyu", ""], ["Jin", "Xin", ""], ["Wu", "Shilin", ""], ["Chen", "Zhibo", ""], ["Li", "Weiping", ""], ["Zhang", "Zhizheng", ""], ["Liu", "Sen", ""]]}, {"id": "1911.10414", "submitter": "Matan Atzmon", "authors": "Matan Atzmon and Yaron Lipman", "title": "SAL: Sign Agnostic Learning of Shapes from Raw Data", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, neural networks have been used as implicit representations for\nsurface reconstruction, modelling, learning, and generation. So far, training\nneural networks to be implicit representations of surfaces required training\ndata sampled from a ground-truth signed implicit functions such as signed\ndistance or occupancy functions, which are notoriously hard to compute.\n  In this paper we introduce Sign Agnostic Learning (SAL), a deep learning\napproach for learning implicit shape representations directly from raw,\nunsigned geometric data, such as point clouds and triangle soups.\n  We have tested SAL on the challenging problem of surface reconstruction from\nan un-oriented point cloud, as well as end-to-end human shape space learning\ndirectly from raw scans dataset, and achieved state of the art reconstructions\ncompared to current approaches. We believe SAL opens the door to many geometric\ndeep learning applications with real-world data, alleviating the usual\npainstaking, often manual pre-process.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 20:18:29 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 19:50:00 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Atzmon", "Matan", ""], ["Lipman", "Yaron", ""]]}, {"id": "1911.10415", "submitter": "Chen Ziwen", "authors": "Chen Ziwen, Wenxuan Wu, Zhongang Qi, Li Fuxin", "title": "Visualizing Point Cloud Classifiers by Curvature Smoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several networks that operate directly on point clouds have been\nproposed. There is significant utility in understanding their mechanisms to\nclassify point clouds, which can potentially help diagnosing these networks and\ndesigning better architectures. In this paper, we propose a novel approach to\nvisualize features important to the point cloud classifiers. Our approach is\nbased on smoothing curved areas on a point cloud. After prominent features were\nsmoothed, the resulting point cloud can be evaluated on the network to assess\nwhether the feature is important to the classifier. A technical contribution of\nthe paper is an approximated curvature smoothing algorithm, which can smoothly\ntransition from the original point cloud to one of constant curvature, such as\na uniform sphere. Based on the smoothing algorithm, we propose PCI-GOS (Point\nCloud Integrated-Gradients Optimized Saliency), a visualization technique that\ncan automatically find the minimal saliency map that covers the most important\nfeatures on a shape. Experiment results revealed insights into different point\ncloud classifiers.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 20:36:56 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 19:58:06 GMT"}, {"version": "v3", "created": "Tue, 1 Sep 2020 04:11:49 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Ziwen", "Chen", ""], ["Wu", "Wenxuan", ""], ["Qi", "Zhongang", ""], ["Fuxin", "Li", ""]]}, {"id": "1911.10417", "submitter": "Charles Huang", "authors": "Charles Huang, Masoud Badiei, Hyunseok Seo, Ming Ma, Xiaokun Liang,\n  Dante Capaldi, Michael Gensheimer, Lei Xing", "title": "Atlas Based Segmentations via Semi-Supervised Diffeomorphic\n  Registrations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Segmentation of organs-at-risk (OARs) is a bottleneck in current\nradiation oncology pipelines and is often time consuming and labor intensive.\nIn this paper, we propose an atlas-based semi-supervised registration algorithm\nto generate accurate segmentations of OARs for which there are ground truth\ncontours and rough segmentations of all other OARs in the atlas. To the best of\nour knowledge, this is the first study to use learning-based registration\nmethods for the segmentation of head and neck patients and demonstrate its\nutility in clinical applications. Methods: Our algorithm cascades rigid and\ndeformable deformation blocks, and takes on an atlas image (M), set of\natlas-space segmentations (S_A), and a patient image (F) as inputs, while\noutputting patient-space segmentations of all OARs defined on the atlas. We\ntrain our model on 475 CT images taken from public archives and Stanford RadOnc\nClinic (SROC), validate on 5 CT images from SROC, and test our model on 20 CT\nimages from SROC. Results: Our method outperforms current state of the art\nlearning-based registration algorithms and achieves an overall dice score of\n0.789 on our test set. Moreover, our method yields a performance comparable to\nmanual segmentation and supervised segmentation, while solving a much more\ncomplex registration problem. Whereas supervised segmentation methods only\nautomate the segmentation process for a select few number of OARs, we\ndemonstrate that our methods can achieve similar performance for OARs of\ninterest, while also providing segmentations for every other OAR on the\nprovided atlas. Conclusions: Our proposed algorithm has significant clinical\napplications and could help reduce the bottleneck for segmentation of head and\nneck OARs. Further, our results demonstrate that semi-supervised diffeomorphic\nregistration can be accurately applied to both registration and segmentation\nproblems.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 21:06:22 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Huang", "Charles", ""], ["Badiei", "Masoud", ""], ["Seo", "Hyunseok", ""], ["Ma", "Ming", ""], ["Liang", "Xiaokun", ""], ["Capaldi", "Dante", ""], ["Gensheimer", "Michael", ""], ["Xing", "Lei", ""]]}, {"id": "1911.10428", "submitter": "Juncai He", "authors": "Juncai He, Yuyan Chen, Lian Zhang, Jinchao Xu", "title": "Constrained Linear Data-feature Mapping for Image Classification", "comments": "15 page, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a constrained linear data-feature mapping model as\nan interpretable mathematical model for image classification using\nconvolutional neural network (CNN) such as the ResNet. From this viewpoint, we\nestablish the detailed connections in a technical level between the traditional\niterative schemes for constrained linear system and the architecture for the\nbasic blocks of ResNet. Under these connections, we propose some natural\nmodifications of ResNet type models which will have less parameters but still\nmaintain almost the same accuracy as these corresponding original models. Some\nnumerical experiments are shown to demonstrate the validity of this constrained\nlearning data-feature mapping assumption.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 22:39:47 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 15:27:07 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["He", "Juncai", ""], ["Chen", "Yuyan", ""], ["Zhang", "Lian", ""], ["Xu", "Jinchao", ""]]}, {"id": "1911.10435", "submitter": "Carlos M. Ortiz", "authors": "Brett Jefferson, Carlos Ortiz Marrero", "title": "Robust Assessment of Real-World Adversarial Examples", "comments": "updated title and abstract; minor edits; some reformatting; added\n  figure 3", "journal-ref": null, "doi": null, "report-no": "PNNL-SA-149173", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore rigorous, systematic, and controlled experimental evaluation of\nadversarial examples in the real world and propose a testing regimen for\nevaluation of real world adversarial objects. We show that for small scene/\nenvironmental perturbations, large adversarial performance differences exist.\nCurrent state of adversarial reporting exists largely as a frequency count over\na dynamic collections of scenes. Our work underscores the need for either a\nmore complete report or a score that incorporates scene changes and baseline\nperformance for models and environments tested by adversarial developers. We\nput forth a score that attempts to address the above issues in a\nstraight-forward exemplar application for multiple generated adversary\nexamples. We contribute the following: 1. a testbed for adversarial assessment,\n2. a score for adversarial examples, and 3. a collection of additional\nevaluations on testbed data.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 00:00:33 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2020 01:21:42 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Jefferson", "Brett", ""], ["Marrero", "Carlos Ortiz", ""]]}, {"id": "1911.10442", "submitter": "Eli (Omid) David", "authors": "Ido Faran, Nathan S. Netanyahu, Eli David, Maxim Shoshany, Fadi Kizel,\n  Jisung Geba Chang, Ronit Rud", "title": "Ground Truth Simulation for Deep Learning Classification of\n  Mid-Resolution Venus Images Via Unmixing of High-Resolution Hyperspectral\n  Fenix Data", "comments": null, "journal-ref": "IEEE International Geoscience and Remote Sensing Symposium\n  (IGARSS), pages 807-810, Yokohama, Japan, July 2019", "doi": "10.1109/IGARSS.2019.8900186", "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a deep neural network for classification constitutes a major problem\nin remote sensing due to the lack of adequate field data. Acquiring\nhigh-resolution ground truth (GT) by human interpretation is both\ncost-ineffective and inconsistent. We propose, instead, to utilize\nhigh-resolution, hyperspectral images for solving this problem, by unmixing\nthese images to obtain reliable GT for training a deep network. Specifically,\nwe simulate GT from high-resolution, hyperspectral FENIX images, and use it for\ntraining a convolutional neural network (CNN) for pixel-based classification.\nWe show how the model can be transferred successfully to classify new\nmid-resolution VENuS imagery.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 01:31:35 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Faran", "Ido", ""], ["Netanyahu", "Nathan S.", ""], ["David", "Eli", ""], ["Shoshany", "Maxim", ""], ["Kizel", "Fadi", ""], ["Chang", "Jisung Geba", ""], ["Rud", "Ronit", ""]]}, {"id": "1911.10444", "submitter": "Uday Kusupati", "authors": "Uday Kusupati, Shuo Cheng, Rui Chen, Hao Su", "title": "Normal Assisted Stereo Depth Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate stereo depth estimation plays a critical role in various 3D tasks in\nboth indoor and outdoor environments. Recently, learning-based multi-view\nstereo methods have demonstrated competitive performance with a limited number\nof views. However, in challenging scenarios, especially when building\ncross-view correspondences is hard, these methods still cannot produce\nsatisfying results. In this paper, we study how to leverage a normal estimation\nmodel and the predicted normal maps to improve the depth quality. We couple the\nlearning of a multi-view normal estimation module and a multi-view depth\nestimation module. In addition, we propose a novel consistency loss to train an\nindependent consistency module that refines the depths from depth/normal pairs.\nWe find that the joint learning can improve both the prediction of normal and\ndepth, and the accuracy & smoothness can be further improved by enforcing the\nconsistency. Experiments on MVS, SUN3D, RGBD, and Scenes11 demonstrate the\neffectiveness of our method and state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 02:16:57 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 04:17:49 GMT"}, {"version": "v3", "created": "Mon, 1 Jun 2020 00:34:08 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Kusupati", "Uday", ""], ["Cheng", "Shuo", ""], ["Chen", "Rui", ""], ["Su", "Hao", ""]]}, {"id": "1911.10448", "submitter": "Christian Wallraven", "authors": "Bjoern Browatzki and Christian Wallraven", "title": "3FabRec: Fast Few-shot Face alignment by Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current supervised methods for facial landmark detection require a large\namount of training data and may suffer from overfitting to specific datasets\ndue to the massive number of parameters. We introduce a semi-supervised method\nin which the crucial idea is to first generate implicit face knowledge from the\nlarge amounts of unlabeled images of faces available today. In a first,\ncompletely unsupervised stage, we train an adversarial autoencoder to\nreconstruct faces via a low-dimensional face embedding. In a second, supervised\nstage, we interleave the decoder with transfer layers to retask the generation\nof color images to the prediction of landmark heatmaps. Our framework (3FabRec)\nachieves state-of-the-art performance on several common benchmarks and, most\nimportantly, is able to maintain impressive accuracy on extremely small\ntraining sets down to as few as 10 images. As the interleaved layers only add a\nlow amount of parameters to the decoder, inference runs at several hundred FPS\non a GPU.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 02:38:17 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 05:54:33 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Browatzki", "Bjoern", ""], ["Wallraven", "Christian", ""]]}, {"id": "1911.10455", "submitter": "Anwesan Pal", "authors": "Anwesan Pal, Sayan Mondal and Henrik I. Christensen", "title": "Looking at the right stuff: Guided semantic-gaze for autonomous driving", "comments": "Paper accepted at CVPR-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, predicting driver's focus of attention has been a very\nactive area of research in the autonomous driving community. Unfortunately,\nexisting state-of-the-art techniques achieve this by relying only on human gaze\ninformation, thereby ignoring scene semantics. We propose a novel Semantics\nAugmented GazE (SAGE) detection approach that captures driving specific\ncontextual information, in addition to the raw gaze. Such a combined attention\nmechanism serves as a powerful tool to focus on the relevant regions in an\nimage frame in order to make driving both safe and efficient. Using this, we\ndesign a complete saliency prediction framework - SAGE-Net, which modifies the\ninitial prediction from SAGE by taking into account vital aspects such as\ndistance to objects (depth), ego vehicle speed, and pedestrian crossing intent.\nExhaustive experiments conducted through four popular saliency algorithms show\nthat on $\\mathbf{49/56\\text{ }(87.5\\%)}$ cases - considering both the overall\ndataset and crucial driving scenarios, SAGE outperforms existing techniques\nwithout any additional computational overhead during the training process. The\naugmented dataset along with the relevant code are available as part of the\nsupplementary material.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 04:06:44 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 18:21:21 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Pal", "Anwesan", ""], ["Mondal", "Sayan", ""], ["Christensen", "Henrik I.", ""]]}, {"id": "1911.10460", "submitter": "Shizhe Chen", "authors": "Shizhe Chen, Bei Liu, Jianlong Fu, Ruihua Song, Qin Jin, Pingping Lin,\n  Xiaoyu Qi, Chunting Wang and Jin Zhou", "title": "Neural Storyboard Artist: Visualizing Stories with Coherent Image\n  Sequences", "comments": "ACM MM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A storyboard is a sequence of images to illustrate a story containing\nmultiple sentences, which has been a key process to create different story\nproducts. In this paper, we tackle a new multimedia task of automatic\nstoryboard creation to facilitate this process and inspire human artists.\nInspired by the fact that our understanding of languages is based on our past\nexperience, we propose a novel inspire-and-create framework with a\nstory-to-image retriever that selects relevant cinematic images for inspiration\nand a storyboard creator that further refines and renders images to improve the\nrelevancy and visual consistency. The proposed retriever dynamically employs\ncontextual information in the story with hierarchical attentions and applies\ndense visual-semantic matching to accurately retrieve and ground images. The\ncreator then employs three rendering steps to increase the flexibility of\nretrieved images, which include erasing irrelevant regions, unifying styles of\nimages and substituting consistent characters. We carry out extensive\nexperiments on both in-domain and out-of-domain visual story datasets. The\nproposed model achieves better quantitative performance than the\nstate-of-the-art baselines for storyboard creation. Qualitative visualizations\nand user studies further verify that our approach can create high-quality\nstoryboards even for stories in the wild.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 05:06:41 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Chen", "Shizhe", ""], ["Liu", "Bei", ""], ["Fu", "Jianlong", ""], ["Song", "Ruihua", ""], ["Jin", "Qin", ""], ["Lin", "Pingping", ""], ["Qi", "Xiaoyu", ""], ["Wang", "Chunting", ""], ["Zhou", "Jin", ""]]}, {"id": "1911.10477", "submitter": "Jiancheng Yang", "authors": "Jiancheng Yang, Xiaoyang Huang, Yi He, Jingwei Xu, Canqian Yang,\n  Guozheng Xu, Bingbing Ni", "title": "Reinventing 2D Convolutions for 3D Images", "comments": "IEEE Journal of Biomedical and Health Informatics (IEEE JBHI). Code\n  is available at https://github.com/m3dv/ACSConv", "journal-ref": "IEEE Journal of Biomedical and Health Informatics (IEEE JBHI),\n  2021", "doi": "10.1109/JBHI.2021.3049452", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There have been considerable debates over 2D and 3D representation learning\non 3D medical images. 2D approaches could benefit from large-scale 2D\npretraining, whereas they are generally weak in capturing large 3D contexts. 3D\napproaches are natively strong in 3D contexts, however few publicly available\n3D medical dataset is large and diverse enough for universal 3D pretraining.\nEven for hybrid (2D + 3D) approaches, the intrinsic disadvantages within the 2D\n/ 3D parts still exist. In this study, we bridge the gap between 2D and 3D\nconvolutions by reinventing the 2D convolutions. We propose ACS\n(axial-coronal-sagittal) convolutions to perform natively 3D representation\nlearning, while utilizing the pretrained weights on 2D datasets. In ACS\nconvolutions, 2D convolution kernels are split by channel into three parts, and\nconvoluted separately on the three views (axial, coronal and sagittal) of 3D\nrepresentations. Theoretically, ANY 2D CNN (ResNet, DenseNet, or DeepLab) is\nable to be converted into a 3D ACS CNN, with pretrained weight of a same\nparameter size. Extensive experiments on several medical benchmarks (including\nclassification, segmentation and detection tasks) validate the consistent\nsuperiority of the pretrained ACS CNNs, over the 2D / 3D CNN counterparts with\n/ without pretraining. Even without pretraining, the ACS convolution can be\nused as a plug-and-play replacement of standard 3D convolution, with smaller\nmodel size and less computation.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 09:05:06 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 14:52:44 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2020 13:03:06 GMT"}, {"version": "v4", "created": "Mon, 4 Jan 2021 07:24:49 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Yang", "Jiancheng", ""], ["Huang", "Xiaoyang", ""], ["He", "Yi", ""], ["Xu", "Jingwei", ""], ["Yang", "Canqian", ""], ["Xu", "Guozheng", ""], ["Ni", "Bingbing", ""]]}, {"id": "1911.10492", "submitter": "Yi Tu", "authors": "Yi Tu, Li Niu, Weijie Zhao, Dawei Cheng, Liqing Zhang", "title": "Image Cropping with Composition and Saliency Aware Aesthetic Score Map", "comments": "Accepted by AAAI 20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aesthetic image cropping is a practical but challenging task which aims at\nfinding the best crops with the highest aesthetic quality in an image.\nRecently, many deep learning methods have been proposed to address this\nproblem, but they did not reveal the intrinsic mechanism of aesthetic\nevaluation. In this paper, we propose an interpretable image cropping model to\nunveil the mystery. For each image, we use a fully convolutional network to\nproduce an aesthetic score map, which is shared among all candidate crops\nduring crop-level aesthetic evaluation. Then, we require the aesthetic score\nmap to be both composition-aware and saliency-aware. In particular, the same\nregion is assigned with different aesthetic scores based on its relative\npositions in different crops. Moreover, a visually salient region is supposed\nto have more sensitive aesthetic scores so that our network can learn to place\nsalient objects at more proper positions. Such an aesthetic score map can be\nused to localize aesthetically important regions in an image, which sheds light\non the composition rules learned by our model. We show the competitive\nperformance of our model in the image cropping task on several benchmark\ndatasets, and also demonstrate its generality in real-world applications.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 10:18:32 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Tu", "Yi", ""], ["Niu", "Li", ""], ["Zhao", "Weijie", ""], ["Cheng", "Dawei", ""], ["Zhang", "Liqing", ""]]}, {"id": "1911.10496", "submitter": "Jiaxin Qi", "authors": "Jiaxin Qi, Yulei Niu, Jianqiang Huang, Hanwang Zhang", "title": "Two Causal Principles for Improving Visual Dialog", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper unravels the design tricks adopted by us, the champion team\nMReaL-BDAI, for Visual Dialog Challenge 2019: two causal principles for\nimproving Visual Dialog (VisDial). By \"improving\", we mean that they can\npromote almost every existing VisDial model to the state-of-the-art performance\non the leader-board. Such a major improvement is only due to our careful\ninspection on the causality behind the model and data, finding that the\ncommunity has overlooked two causalities in VisDial. Intuitively, Principle 1\nsuggests: we should remove the direct input of the dialog history to the answer\nmodel, otherwise a harmful shortcut bias will be introduced; Principle 2 says:\nthere is an unobserved confounder for history, question, and answer, leading to\nspurious correlations from training data. In particular, to remove the\nconfounder suggested in Principle 2, we propose several causal intervention\nalgorithms, which make the training fundamentally different from the\ntraditional likelihood estimation. Note that the two principles are\nmodel-agnostic, so they are applicable in any VisDial model. The code is\navailable at https://github.com/simpleshinobu/visdial-principles.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 10:35:35 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 17:09:34 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Qi", "Jiaxin", ""], ["Niu", "Yulei", ""], ["Huang", "Jianqiang", ""], ["Zhang", "Hanwang", ""]]}, {"id": "1911.10498", "submitter": "Lin Li", "authors": "Jing Huang, Hengfeng Miao, Lin Li, Yuanqiao Wen, Changshi Xiao", "title": "Deep Visual Waterline Detection within Inland Marine Environment", "comments": "9 pages, 3 figures, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Waterline usually plays as an important visual cue for maritime applications.\nHowever, the visual complexity of inland waterline presents a significant\nchallenge for the development of highly efficient computer vision algorithms\ntailored for waterline detection in a complicated inland water environment.\nThis paper attempts to find a solution to guarantee the effectiveness of\nwaterline detection for inland maritime applications with general digital\ncamera sensor. To this end, a general deep-learning-based paradigm applicable\nin variable inland waters, named DeepWL, is proposed, which concerns the\nefficiency of waterline detection simultaneously. Specifically, there are two\nnovel deep network models, named WLdetectNet and WLgenerateNet respectively,\ncooperating in the paradigm that afford a continuous waterline image-map\nestimation from a single captured video stream. Experimental results\ndemonstrate the effectiveness and superiority of the proposed approach via\nqualitative and quantitative assessment on the concerned performances.\nMoreover, due to its own generality, the proposed approach has the potential to\nbe applied to the waterline detection tasks of other water areas such as\ncoastal waters.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 10:50:52 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Huang", "Jing", ""], ["Miao", "Hengfeng", ""], ["Li", "Lin", ""], ["Wen", "Yuanqiao", ""], ["Xiao", "Changshi", ""]]}, {"id": "1911.10506", "submitter": "Riddhish Bhalodia", "authors": "Riddhish Bhalodia, Iain Lee, Shireen Elhabian", "title": "dpVAEs: Fixing Sample Generation for Regularized VAEs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised representation learning via generative modeling is a staple to\nmany computer vision applications in the absence of labeled data. Variational\nAutoencoders (VAEs) are powerful generative models that learn representations\nuseful for data generation. However, due to inherent challenges in the training\nobjective, VAEs fail to learn useful representations amenable for downstream\ntasks. Regularization-based methods that attempt to improve the representation\nlearning aspect of VAEs come at a price: poor sample generation. In this paper,\nwe explore this representation-generation trade-off for regularized VAEs and\nintroduce a new family of priors, namely decoupled priors, or dpVAEs, that\ndecouple the representation space from the generation space. This decoupling\nenables the use of VAE regularizers on the representation space without\nimpacting the distribution used for sample generation, and thereby reaping the\nrepresentation learning benefits of the regularizations without sacrificing the\nsample generation. dpVAE leverages invertible networks to learn a bijective\nmapping from an arbitrarily complex representation distribution to a simple,\ntractable, generative distribution. Decoupled priors can be adapted to the\nstate-of-the-art VAE regularizers without additional hyperparameter tuning. We\nshowcase the use of dpVAEs with different regularizers. Experiments on MNIST,\nSVHN, and CelebA demonstrate, quantitatively and qualitatively, that dpVAE\nfixes sample generation for regularized VAEs.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 11:31:39 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2020 03:54:34 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Bhalodia", "Riddhish", ""], ["Lee", "Iain", ""], ["Elhabian", "Shireen", ""]]}, {"id": "1911.10511", "submitter": "Xukai Xie", "authors": "Xukai Xie, Yuan Zhou, Sun-Yuan Kung", "title": "Exploiting Operation Importance for Differentiable Neural Architecture\n  Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, differentiable neural architecture search methods significantly\nreduce the search cost by constructing a super network and relax the\narchitecture representation by assigning architecture weights to the candidate\noperations. All the existing methods determine the importance of each operation\ndirectly by architecture weights. However, architecture weights cannot\naccurately reflect the importance of each operation; that is, the operation\nwith the highest weight might not related to the best performance. To alleviate\nthis deficiency, we propose a simple yet effective solution to neural\narchitecture search, termed as exploiting operation importance for effective\nneural architecture search (EoiNAS), in which a new indicator is proposed to\nfully exploit the operation importance and guide the model search. Based on\nthis new indicator, we propose a gradual operation pruning strategy to further\nimprove the search efficiency and accuracy. Experimental results have\ndemonstrated the effectiveness of the proposed method. Specifically, we achieve\nan error rate of 2.50\\% on CIFAR-10, which significantly outperforms\nstate-of-the-art methods. When transferred to ImageNet, it achieves the top-1\nerror of 25.6\\%, comparable to the state-of-the-art performance under the\nmobile setting.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 11:53:09 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Xie", "Xukai", ""], ["Zhou", "Yuan", ""], ["Kung", "Sun-Yuan", ""]]}, {"id": "1911.10520", "submitter": "Yuanbin Fu", "authors": "Yuanbin Fu and Jiayi Ma and Lin Ma and Xiaojie Guo", "title": "EDIT: Exemplar-Domain Aware Image-to-Image Translation", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image translation is to convert an image of the certain style to\nanother of the target style with the content preserved. A desired translator\nshould be capable to generate diverse results in a controllable (many-to-many)\nfashion. To this end, we design a novel generative adversarial network, namely\nexemplar-domain aware image-to-image translator (EDIT for short). The principle\nbehind is that, for images from multiple domains, the content features can be\nobtained by a uniform extractor, while (re-)stylization is achieved by mapping\nthe extracted features specifically to different purposes (domains and\nexemplars). The generator of our EDIT comprises of a part of blocks configured\nby shared parameters, and the rest by varied parameters exported by an\nexemplar-domain aware parameter network. In addition, a discriminator is\nequipped during the training phase to guarantee the output satisfying the\ndistribution of the target domain. Our EDIT can flexibly and effectively work\non multiple domains and arbitrary exemplars in a unified neat model. We conduct\nexperiments to show the efficacy of our design, and reveal its advances over\nother state-of-the-art methods both quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 12:40:52 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Fu", "Yuanbin", ""], ["Ma", "Jiayi", ""], ["Ma", "Lin", ""], ["Guo", "Xiaojie", ""]]}, {"id": "1911.10529", "submitter": "Jia Li", "authors": "Jia Li, Wen Su and Zengfu Wang", "title": "Simple Pose: Rethinking and Improving a Bottom-up Approach for\n  Multi-Person Pose Estimation", "comments": "Accepted by AAAI 2020 (the Thirty-Fourth AAAI Conference on\n  Artificial Intelligence)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We rethink a well-know bottom-up approach for multi-person pose estimation\nand propose an improved one. The improved approach surpasses the baseline\nsignificantly thanks to (1) an intuitional yet more sensible representation,\nwhich we refer to as body parts to encode the connection information between\nkeypoints, (2) an improved stacked hourglass network with attention mechanisms,\n(3) a novel focal L2 loss which is dedicated to hard keypoint and keypoint\nassociation (body part) mining, and (4) a robust greedy keypoint assignment\nalgorithm for grouping the detected keypoints into individual poses. Our\napproach not only works straightforwardly but also outperforms the baseline by\nabout 15% in average precision and is comparable to the state of the art on the\nMS-COCO test-dev dataset. The code and pre-trained models are publicly\navailable online.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 13:51:38 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Li", "Jia", ""], ["Su", "Wen", ""], ["Wang", "Zengfu", ""]]}, {"id": "1911.10531", "submitter": "Ruicong Xu", "authors": "Ruicong Xu, Li Niu, Jianfu Zhang, Liqing Zhang", "title": "A Proposal-based Approach for Activity Image-to-Video Retrieval", "comments": "The Thirty-Fourth AAAI Conference on Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activity image-to-video retrieval task aims to retrieve videos containing the\nsimilar activity as the query image, which is a challenging task because videos\ngenerally have many background segments irrelevant to the activity. In this\npaper, we utilize R-C3D model to represent a video by a bag of activity\nproposals, which can filter out background segments to some extent. However,\nthere are still noisy proposals in each bag. Thus, we propose an Activity\nProposal-based Image-to-Video Retrieval (APIVR) approach, which incorporates\nmulti-instance learning into cross-modal retrieval framework to address the\nproposal noise issue. Specifically, we propose a Graph Multi-Instance Learning\n(GMIL) module with graph convolutional layer, and integrate this module with\nclassification loss, adversarial loss, and triplet loss in our cross-modal\nretrieval framework. Moreover, we propose geometry-aware triplet loss based on\npoint-to-subspace distance to preserve the structural information of activity\nproposals. Extensive experiments on three widely-used datasets verify the\neffectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 14:03:21 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Xu", "Ruicong", ""], ["Niu", "Li", ""], ["Zhang", "Jianfu", ""], ["Zhang", "Liqing", ""]]}, {"id": "1911.10535", "submitter": "Fan Yang", "authors": "Fan Yang, Feiran Li, Yang Wu, Sakriani Sakti, and Satoshi Nakamura", "title": "Using Panoramic Videos for Multi-person Localization and Tracking in a\n  3D Panoramic Coordinate", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D panoramic multi-person localization and tracking are prominent in many\napplications, however, conventional methods using LiDAR equipment could be\neconomically expensive and also computationally inefficient due to the\nprocessing of point cloud data. In this work, we propose an effective and\nefficient approach at a low cost. First, we obtain panoramic videos with four\nnormal cameras. Then, we transform human locations from a 2D panoramic image\ncoordinate to a 3D panoramic camera coordinate using camera geometry and human\nbio-metric property (i.e., height). Finally, we generate 3D tracklets by\nassociating human appearance and 3D trajectory. We verify the effectiveness of\nour method on three datasets including a new one built by us, in terms of 3D\nsingle-view multi-person localization, 3D single-view multi-person tracking,\nand 3D panoramic multi-person localization and tracking. Our code and dataset\nare available at \\url{https://github.com/fandulu/MPLT}.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 14:40:57 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 01:31:37 GMT"}, {"version": "v3", "created": "Fri, 7 Feb 2020 00:58:10 GMT"}, {"version": "v4", "created": "Thu, 13 Feb 2020 03:24:08 GMT"}, {"version": "v5", "created": "Sun, 8 Mar 2020 02:30:58 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Yang", "Fan", ""], ["Li", "Feiran", ""], ["Wu", "Yang", ""], ["Sakti", "Sakriani", ""], ["Nakamura", "Satoshi", ""]]}, {"id": "1911.10538", "submitter": "Ori Nizan Mr", "authors": "Ori Nizan and Ayellet Tal", "title": "Breaking the cycle -- Colleagues are all you need", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel approach to performing image-to-image translation\nbetween unpaired domains. Rather than relying on a cycle constraint, our method\ntakes advantage of collaboration between various GANs. This results in a\nmulti-modal method, in which multiple optional and diverse images are produced\nfor a given image. Our model addresses some of the shortcomings of classical\nGANs: (1) It is able to remove large objects, such as glasses. (2) Since it\ndoes not need to support the cycle constraint, no irrelevant traces of the\ninput are left on the generated image. (3) It manages to translate between\ndomains that require large shape modifications. Our results are shown to\noutperform those generated by state-of-the-art methods for several challenging\napplications on commonly-used datasets, both qualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 14:43:45 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 20:23:38 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Nizan", "Ori", ""], ["Tal", "Ayellet", ""]]}, {"id": "1911.10544", "submitter": "Bo Jiang", "authors": "Bo Jiang, Xixi Wang, Jin Tang", "title": "AttKGCN: Attribute Knowledge Graph Convolutional Network for Person\n  Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative feature representation of person image is important for person\nre-identification (Re-ID) task. Recently, attributes have been demonstrated\nbeneficially in guiding for learning more discriminative feature\nrepresentations for Re-ID. As attributes normally co-occur in person images, it\nis desirable to model the attribute dependencies to improve the attribute\nprediction and thus Re-ID results. In this paper, we propose to model these\nattribute dependencies via a novel attribute knowledge graph (AttKG), and\npropose a novel Attribute Knowledge Graph Convolutional Network (AttKGCN) to\nsolve Re-ID problem. AttKGCN integrates both attribute prediction and Re-ID\nlearning together in a unified end-to-end framework which can boost their\nperformances, respectively. AttKGCN first builds a directed attribute KG whose\nnodes denote attributes and edges encode the co-occurrence relationships of\ndifferent attributes. Then, AttKGCN learns a set of inter-dependent attribute\nclassifiers which are combined with person visual descriptors for attribute\nprediction. Finally, AttKGCN integrates attribute description and deeply visual\nrepresentation together to construct a more discriminative feature\nrepresentation for Re-ID task. Extensive experiments on several benchmark\ndatasets demonstrate the effectiveness of AttKGCN on attribute prediction and\nRe-ID tasks.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 14:57:43 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Jiang", "Bo", ""], ["Wang", "Xixi", ""], ["Tang", "Jin", ""]]}, {"id": "1911.10566", "submitter": "Yuan-Gen Wang", "authors": "Fu-Zhao Ou, Yuan-Gen Wang, Jin Li, Guopu Zhu, Sam Kwong", "title": "Controllable List-wise Ranking for Universal No-reference Image Quality\n  Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  No-reference image quality assessment (NR-IQA) has received increasing\nattention in the IQA community since reference image is not always available.\nReal-world images generally suffer from various types of distortion.\nUnfortunately, existing NR-IQA methods do not work with all types of\ndistortion. It is a challenging task to develop universal NR-IQA that has the\nability of evaluating all types of distorted images. In this paper, we propose\na universal NR-IQA method based on controllable list-wise ranking (CLRIQA).\nFirst, to extend the authentically distorted image dataset, we present an\nimaging-heuristic approach, in which the over-underexposure is formulated as an\ninverse of Weber-Fechner law, and fusion strategy and probabilistic compression\nare adopted, to generate the degraded real-world images. These degraded images\nare label-free yet associated with quality ranking information. We then design\na controllable list-wise ranking function by limiting rank range and\nintroducing an adaptive margin to tune rank interval. Finally, the extended\ndataset and controllable list-wise ranking function are used to pre-train a\nCNN. Moreover, in order to obtain an accurate prediction model, we take\nadvantage of the original dataset to further fine-tune the pre-trained network.\nExperiments evaluated on four benchmark datasets (i.e. LIVE, CSIQ, TID2013, and\nLIVE-C) show that the proposed CLRIQA improves the state of the art by over 9%\nin terms of overall performance. The code and model are publicly available at\nhttps://github.com/GZHU-Image-Lab/CLRIQA.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 16:25:05 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 04:56:00 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Ou", "Fu-Zhao", ""], ["Wang", "Yuan-Gen", ""], ["Li", "Jin", ""], ["Zhu", "Guopu", ""], ["Kwong", "Sam", ""]]}, {"id": "1911.10572", "submitter": "Yongzhe Yan", "authors": "Yongzhe Yan, Stefan Duffner, Priyanka Phutane, Anthony Berthelier,\n  Christophe Blanc, Christophe Garcia and Thierry Chateau", "title": "2D Wasserstein Loss for Robust Facial Landmark Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent performance of facial landmark detection has been significantly\nimproved by using deep Convolutional Neural Networks (CNNs), especially the\nHeatmap Regression Models (HRMs). Although their performance on common\nbenchmark datasets has reached a high level, the robustness of these models\nstill remains a challenging problem in the practical use under noisy conditions\nof realistic environments. Contrary to most existing work focusing on the\ndesign of new models, we argue that improving the robustness requires\nrethinking many other aspects, including the use of datasets, the format of\nlandmark annotation, the evaluation metric as well as the training and\ndetection algorithm itself. In this paper, we propose a novel method for robust\nfacial landmark detection, using a loss function based on the 2D Wasserstein\ndistance combined with a new landmark coordinate sampling relying on the\nbarycenter of the individual probability distributions. Our method can be\nplugged-and-play on most state-of-the-art HRMs with neither additional\ncomplexity nor structural modifications of the models. Further, with the large\nperformance increase, we found that current evaluation metrics can no longer\nfully reflect the robustness of these models. Therefore, we propose several\nimprovements to the standard evaluation protocol. Extensive experimental\nresults on both traditional evaluation metrics and our evaluation metrics\ndemonstrate that our approach significantly improves the robustness of\nstate-of-the-art facial landmark detection models.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 16:56:10 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 00:26:32 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Yan", "Yongzhe", ""], ["Duffner", "Stefan", ""], ["Phutane", "Priyanka", ""], ["Berthelier", "Anthony", ""], ["Blanc", "Christophe", ""], ["Garcia", "Christophe", ""], ["Chateau", "Thierry", ""]]}, {"id": "1911.10575", "submitter": "Mohamed Zahran", "authors": "Ahmad El Sallab, Ibrahim Sobh, Mohamed Zahran, Mohamed Shawky", "title": "Unsupervised Neural Sensor Models for Synthetic LiDAR Data Augmentation", "comments": "Accepted in Machine learning for Autonomous Driving NeurIPS 2019\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Data scarcity is a bottleneck to machine learning-based perception modules,\nusually tackled by augmenting real data with synthetic data from simulators.\nRealistic models of the vehicle perception sensors are hard to formulate in\nclosed form, and at the same time, they require the existence of paired data to\nbe learned. In this work, we propose two unsupervised neural sensor models\nbased on unpaired domain translations with CycleGANs and Neural Style Transfer\ntechniques. We employ CARLA as the simulation environment to obtain simulated\nLiDAR point clouds, together with their annotations for data augmentation, and\nwe use KITTI dataset as the real LiDAR dataset from which we learn the\nrealistic sensor model mapping. Moreover, we provide a framework for data\naugmentation and evaluation of the developed sensor models, through extrinsic\nobject detection task evaluation using YOLO network adapted to provide oriented\nbounding boxes for LiDAR Bird-eye-View projected point clouds. Evaluation is\nperformed on unseen real LiDAR frames from KITTI dataset, with different\namounts of simulated data augmentation using the two proposed approaches,\nshowing improvement of 6% mAP for the object detection task, in favor of the\naugmenting LiDAR point clouds adapted with the proposed neural sensor models\nover the raw simulated LiDAR.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 17:29:50 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Sallab", "Ahmad El", ""], ["Sobh", "Ibrahim", ""], ["Zahran", "Mohamed", ""], ["Shawky", "Mohamed", ""]]}, {"id": "1911.10576", "submitter": "Yongzhe Yan", "authors": "Yongzhe Yan, Stefan Duffner, Priyanka Phutane, Anthony Berthelier,\n  Christophe Blanc, Christophe Garcia and Thierry Chateau", "title": "Facial Landmark Correlation Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a facial landmark position correlation analysis as well as its\napplications. Although numerous facial landmark detection methods have been\npresented in the literature, few of them explicitly take into account the\ninherent relationship among landmarks. To reveal and interpret this\nrelationship, we propose to analyze landmark correlation by using Canonical\nCorrelation Analysis~(CCA). We experimentally show that the dense facial\nlandmark annotations in current benchmarks are strongly correlated. We propose\ntwo applications based on this analysis. First, by analyzing the landmark\ncorrelation, we gain some interesting insights into the predictions of\ndifferent landmark detection models (including random forests model and CNN\nmodels). We also demonstrate how CNNs progressively learn to predict facial\nlandmarks. Second, we propose a few-shot learning method that allows to\nconsiderably reduce the manual effort for dense landmark annotation.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 17:30:06 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 00:04:47 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Yan", "Yongzhe", ""], ["Duffner", "Stefan", ""], ["Phutane", "Priyanka", ""], ["Berthelier", "Anthony", ""], ["Blanc", "Christophe", ""], ["Garcia", "Christophe", ""], ["Chateau", "Thierry", ""]]}, {"id": "1911.10581", "submitter": "Filippos Kokkinos", "authors": "Filippos Kokkinos, Ioannis Marras, Matteo Maggioni, Gregory Slabaugh,\n  Stefanos Zafeiriou", "title": "Pixel Adaptive Filtering Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art methods for computer vision rely heavily on the translation\nequivariance and spatial sharing properties of convolutional layers without\nexplicitly taking into consideration the input content. Modern techniques\nemploy deep sophisticated architectures in order to circumvent this issue. In\nthis work, we propose a Pixel Adaptive Filtering Unit (PAFU) which introduces a\ndifferentiable kernel selection mechanism paired with a discrete, learnable and\ndecorrelated group of kernels to allow for content-based spatial adaptation.\nFirst, we demonstrate the applicability of the technique in applications where\nruntime is of importance. Next, we employ PAFU in deep neural networks as a\nreplacement of standard convolutional layers to enhance the original\narchitectures with spatially varying computations to achieve considerable\nperformance improvements. Finally, diverse and extensive experimentation\nprovides strong empirical evidence in favor of the proposed content-adaptive\nprocessing scheme across different image processing and high-level computer\nvision tasks.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 17:41:15 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Kokkinos", "Filippos", ""], ["Marras", "Ioannis", ""], ["Maggioni", "Matteo", ""], ["Slabaugh", "Gregory", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1911.10594", "submitter": "Dipan Pal", "authors": "Dipan K. Pal, Sreena Nallamothu, Marios Savvides", "title": "Towards a Hypothesis on Visual Transformation based Self-Supervision", "comments": "Draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the first qualitative hypothesis characterizing the behavior of\nvisual transformation based self-supervision, called the VTSS hypothesis. Given\na dataset upon which a self-supervised task is performed while predicting\ninstantiations of a transformation, the hypothesis states that if the predicted\ninstantiations of the transformations are already present in the dataset, then\nthe representation learned will be less useful. The hypothesis was derived by\nobserving a key constraint in the application of self-supervision using a\nparticular transformation. This constraint, which we term the transformation\nconflict for this paper, forces a network learn degenerative features thereby\nreducing the usefulness of the representation. The VTSS hypothesis helps us\nidentify transformations that have the potential to be effective as a\nself-supervision task. Further, it helps to generally predict whether a\nparticular transformation based self-supervision technique would be effective\nor not for a particular dataset. We provide extensive evaluations on CIFAR 10,\nCIFAR 100, SVHN and FMNIST confirming the hypothesis and the trends it\npredicts. We also propose novel cost-effective self-supervision techniques\nbased on translation and scale, which when combined with rotation outperforms\nall transformations applied individually. Overall, this paper aims to shed\nlight on the phenomenon of visual transformation based self-supervision.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 19:27:35 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 03:51:46 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Pal", "Dipan K.", ""], ["Nallamothu", "Sreena", ""], ["Savvides", "Marios", ""]]}, {"id": "1911.10600", "submitter": "Sameeksha Katoch", "authors": "Sameeksha Katoch, Kowshik Thopalli, Jayaraman J. Thiagarajan, Pavan\n  Turaga, Andreas Spanias", "title": "Invenio: Discovering Hidden Relationships Between Tasks/Domains Using\n  Structured Meta Learning", "comments": "Semantic structure development for tasks/domains essential for\n  efficient knowledge transfer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting known semantic relationships between fine-grained tasks is\ncritical to the success of recent model agnostic approaches. These approaches\noften rely on meta-optimization to make a model robust to systematic task or\ndomain shifts. However, in practice, the performance of these methods can\nsuffer, when there are no coherent semantic relationships between the tasks (or\ndomains). We present Invenio, a structured meta-learning algorithm to infer\nsemantic similarities between a given set of tasks and to provide insights into\nthe complexity of transferring knowledge between different tasks. In contrast\nto existing techniques such as Task2Vec and Taskonomy, which measure\nsimilarities between pre-trained models, our approach employs a novel\nself-supervised learning strategy to discover these relationships in the\ntraining loop and at the same time utilizes them to update task-specific models\nin the meta-update step. Using challenging task and domain databases, under\nfew-shot learning settings, we show that Invenio can discover intricate\ndependencies between tasks or domains, and can provide significant gains over\nexisting approaches in terms of generalization performance. The learned\nsemantic structure between tasks/domains from Invenio is interpretable and can\nbe used to construct meaningful priors for tasks or domains.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 20:01:19 GMT"}, {"version": "v2", "created": "Sat, 1 Feb 2020 20:21:57 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Katoch", "Sameeksha", ""], ["Thopalli", "Kowshik", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Turaga", "Pavan", ""], ["Spanias", "Andreas", ""]]}, {"id": "1911.10608", "submitter": "Manpreet Singh Minhas", "authors": "Manpreet Singh Minhas, John Zelek", "title": "AnoNet: Weakly Supervised Anomaly Detection in Textured Surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Humans can easily detect a defect (anomaly) because it is different or\nsalient when compared to the surface it resides on. Today, manual human visual\ninspection is still the norm because it is difficult to automate anomaly\ndetection. Neural networks are a useful tool that can teach a machine to find\ndefects. However, they require a lot of training examples to learn what a\ndefect is and it is tedious and expensive to get these samples. We tackle the\nproblem of teaching a network with a low number of training samples with a\nsystem we call AnoNet. AnoNet's architecture is similar to CompactCNN with the\nexceptions that (1) it is a fully convolutional network and does not use\nstrided convolution; (2) it is shallow and compact which minimizes over-fitting\nby design; (3) the compact design constrains the size of intermediate features\nwhich allows training to be done without image downsizing; (4) the model\nfootprint is low making it suitable for edge computation; and (5) the anomaly\ncan be detected and localized despite the weak labelling. AnoNet learns to\ndetect the underlying shape of the anomalies despite the weak annotation as\nwell as preserves the spatial localization of the anomaly. Pre-seeding AnoNet\nwith an engineered filter bank initialization technique reduces the total\nsamples required for training and also achieves state-of-the-art performance.\nCompared to the CompactCNN, AnoNet achieved a massive 94% reduction of network\nparameters from 1.13 million to 64 thousand parameters. Experiments were\nconducted on four data-sets and results were compared against CompactCNN and\nDeepLabv3. AnoNet improved the performance on an average across all data-sets\nby 106% to an F1 score of 0.98 and by 13% to an AUROC value of 0.942. AnoNet\ncan learn from a limited number of images. For one of the data-sets, AnoNet\nlearnt to detect anomalies after a single pass through just 53 training images.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 21:05:35 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Minhas", "Manpreet Singh", ""], ["Zelek", "John", ""]]}, {"id": "1911.10614", "submitter": "Yihui He", "authors": "Yihui He and Jianren Wang", "title": "Deep Mixture Density Network for Probabilistic Object Detection", "comments": "IROS 2020 oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mistakes/uncertainties in object detection could lead to catastrophes when\ndeploying robots in the real world. In this paper, we measure the uncertainties\nof object localization to minimize this kind of risk. Uncertainties emerge upon\nchallenging cases like occlusion. The bounding box borders of an occluded\nobject can have multiple plausible configurations. We propose a deep\nmultivariate mixture of Gaussians model for probabilistic object detection. The\ncovariances help to learn the relationship between the borders, and the mixture\ncomponents potentially learn different configurations of an occluded part.\nQuantitatively, our model improves the AP of the baselines by 3.9% and 1.4% on\nCrowdHuman and MS-COCO respectively with almost no computational or memory\noverhead. Qualitatively, our model enjoys explainability since the resulting\ncovariance matrices and the mixture components help measure uncertainties.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 21:35:22 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 14:22:08 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["He", "Yihui", ""], ["Wang", "Jianren", ""]]}, {"id": "1911.10617", "submitter": "Claas Flint", "authors": "Claas Flint, Katharina F\\\"orster, Sophie A. Koser, Carsten Konrad,\n  Pienie Zwitserlood, Klaus Berger, Marco Hermesdorf, Tilo Kircher, Igor\n  Nenadic, Axel Krug, Bernhard T. Baune, Katharina Dohm, Ronny Redlich, Nils\n  Opel, Volker Arolt, Tim Hahn, Xiaoyi Jiang, Udo Dannlowski, Dominik Grotegerd", "title": "Biological sex classification with structural MRI data shows increased\n  misclassification in transgender women", "comments": "Content adapted to the publication at Neuropsychopharmacology", "journal-ref": "Neuropsychopharmacology 45 (2020) 1758-1765", "doi": "10.1038/s41386-020-0666-3", "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transgender individuals (TIs) show brain structural alterations that differ\nfrom their biological sex as well as their perceived gender. To substantiate\nevidence that the brain structure of TIs differs from male and female, we use a\ncombined multivariate and univariate approach. Gray matter segments resulting\nfrom voxel-based morphometry preprocessing of $N = 1753$ cisgender (CG) healthy\nparticipants were used to train ($N=1402$) and validate (20 % hold-out; $N =\n351$) a support-vector machine classifying the biological sex. As a second\nvalidation, we classified $N = 1104$ patients with depression. A third\nvalidation was performed using the matched CG sample of the transgender women\n(TWs) application-sample. Subsequently, the classifier was applied to $N = 26$\nTWs. Finally, we compared brain volumes of CG-men, women and TW-pre/post\ntreatment (cross-sex hormone treatment) in a univariate analysis controlling\nfor sexual orientation, age and total brain volume. The application of our\nbiological sex classifier to the transgender sample resulted in a significantly\nlower true positive rate (TPR) (TPR-male = 56.0 %). The TPR did not differ\nbetween CG-individuals with (TPR-male = 86.9 %) and without depression\n(TPR-male = 88.5 %). The univariate analysis of the transgender\napplication-sample revealed that TW-pre/post treatment show brain structural\ndifferences from CG-women and CG-men in the putamen and insula, as well as the\nwhole-brain analysis. Our results support the hypothesis that brain structure\nin TW differs from brain structure of their biological sex (male) as well as\ntheir perceived gender (female). This finding substantiates evidence that TIs\nshow specific brain structural alterations leading to a different pattern of\nbrain structure than CG-individuals.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 21:50:55 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 17:47:39 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Flint", "Claas", ""], ["F\u00f6rster", "Katharina", ""], ["Koser", "Sophie A.", ""], ["Konrad", "Carsten", ""], ["Zwitserlood", "Pienie", ""], ["Berger", "Klaus", ""], ["Hermesdorf", "Marco", ""], ["Kircher", "Tilo", ""], ["Nenadic", "Igor", ""], ["Krug", "Axel", ""], ["Baune", "Bernhard T.", ""], ["Dohm", "Katharina", ""], ["Redlich", "Ronny", ""], ["Opel", "Nils", ""], ["Arolt", "Volker", ""], ["Hahn", "Tim", ""], ["Jiang", "Xiaoyi", ""], ["Dannlowski", "Udo", ""], ["Grotegerd", "Dominik", ""]]}, {"id": "1911.10621", "submitter": "Samet Demir", "authors": "Samet Demir, Hasan Ferit Eniser, Alper Sen", "title": "DeepSmartFuzzer: Reward Guided Test Generation For Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing Deep Neural Network (DNN) models has become more important than ever\nwith the increasing usage of DNN models in safety-critical domains such as\nautonomous cars. The traditional approach of testing DNNs is to create a test\nset, which is a random subset of the dataset about the problem of interest.\nThis kind of approach is not enough for testing most of the real-world\nscenarios since these traditional test sets do not include corner cases, while\na corner case input is generally considered to introduce erroneous behaviors.\nRecent works on adversarial input generation, data augmentation, and\ncoverage-guided fuzzing (CGF) have provided new ways to extend traditional test\nsets. Among those, CGF aims to produce new test inputs by fuzzing existing ones\nto achieve high coverage on a test adequacy criterion (i.e. coverage\ncriterion). Given that the subject test adequacy criterion is a\nwell-established one, CGF can potentially find error inducing inputs for\ndifferent underlying reasons. In this paper, we propose a novel CGF solution\nfor structural testing of DNNs. The proposed fuzzer employs Monte Carlo Tree\nSearch to drive the coverage-guided search in the pursuit of achieving high\ncoverage. Our evaluation shows that the inputs generated by our method result\nin higher coverage than the inputs produced by the previously introduced\ncoverage-guided fuzzing techniques.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 22:18:54 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Demir", "Samet", ""], ["Eniser", "Hasan Ferit", ""], ["Sen", "Alper", ""]]}, {"id": "1911.10636", "submitter": "Vincenzo Liguori", "authors": "Vincenzo Liguori", "title": "Pyramid Vector Quantization and Bit Level Sparsity in Weights for\n  Efficient Neural Networks Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses three basic blocks for the inference of convolutional\nneural networks (CNNs). Pyramid Vector Quantization (PVQ) is discussed as an\neffective quantizer for CNNs weights resulting in highly sparse and\ncompressible networks. Properties of PVQ are exploited for the elimination of\nmultipliers during inference while maintaining high performance. The result is\nthen extended to any other quantized weights. The Tiny Yolo v3 CNN is used to\ncompare such basic blocks.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 23:03:19 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Liguori", "Vincenzo", ""]]}, {"id": "1911.10657", "submitter": "Kwang Moo Yi", "authors": "Teaghan O'Briain, Kyong Hwan Jin, Hongyoon Choi, Erika Chin, Magdalena\n  Bazalova-Carter, Kwang Moo Yi", "title": "Reducing the Human Effort in Developing PET-CT Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to reduce the tedious nature of developing and evaluating methods for\naligning PET-CT scans from multiple patient visits. Current methods for\nregistration rely on correspondences that are created manually by medical\nexperts with 3D manipulation, or assisted alignments done by utilizing mutual\ninformation across CT scans that may not be consistent when transferred to the\nPET images. Instead, we propose to label multiple key points across several 2D\nslices, which we then fit a key curve to. This removes the need for creating\nmanual alignments in 3D and makes the labelling process easier. We use these\nkey curves to define an error metric for the alignments that can be computed\nefficiently. While our metric is non-differentiable, we further show that we\ncan utilize it during the training of our deep model via a novel method.\nSpecifically, instead of relying on detailed geometric labels -- e.g., manual\n3D alignments -- we use synthetically generated deformations of real data. To\nincorporate robustness to changes that occur between visits other than\ngeometric changes, we enforce consistency across visits in the deep network's\ninternal representations. We demonstrate the potential of our method via\nqualitative and quantitative experiments.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 01:36:33 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["O'Briain", "Teaghan", ""], ["Jin", "Kyong Hwan", ""], ["Choi", "Hongyoon", ""], ["Chin", "Erika", ""], ["Bazalova-Carter", "Magdalena", ""], ["Yi", "Kwang Moo", ""]]}, {"id": "1911.10672", "submitter": "Dongxu Wei", "authors": "Dongxu Wei, Xiaowei Xu, Haibin Shen, Kejie Huang", "title": "GAC-GAN: A General Method for Appearance-Controllable Human Video Motion\n  Transfer", "comments": "paper under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human video motion transfer has a wide range of applications in multimedia,\ncomputer vision and graphics. Recently, due to the rapid development of\nGenerative Adversarial Networks (GANs), there has been significant progress in\nthe field. However, almost all existing GAN-based works are prone to address\nthe mapping from human motions to video scenes, with scene appearances are\nencoded individually in the trained models. Therefore, each trained model can\nonly generate videos with a specific scene appearance, new models are required\nto be trained to generate new appearances. Besides, existing works lack the\ncapability of appearance control. For example, users have to provide video\nrecords of wearing new clothes or performing in new backgrounds to enable\nclothes or background changing in their synthetic videos, which greatly limits\nthe application flexibility. In this paper, we propose GAC-GAN, a general\nmethod for appearance-controllable human video motion transfer. To enable\ngeneral-purpose appearance synthesis, we propose to include appearance\ninformation in the conditioning inputs. Thus, once trained, our model can\ngenerate new appearances by altering the input appearance information. To\nachieve appearance control, we first obtain the appearance-controllable\nconditioning inputs and then utilize a two-stage GAC-GAN to generate the\ncorresponding appearance-controllable outputs, where we utilize an ACGAN loss\nand a shadow extraction module for output foreground and background appearance\ncontrol respectively. We further build a solo dance dataset containing a large\nnumber of dance videos for training and evaluation. Experimental results show\nthat, our proposed GAC-GAN can not only support appearance-controllable human\nvideo motion transfer but also achieve higher video quality than state-of-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 02:56:47 GMT"}, {"version": "v2", "created": "Sun, 9 Feb 2020 15:52:47 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Wei", "Dongxu", ""], ["Xu", "Xiaowei", ""], ["Shen", "Haibin", ""], ["Huang", "Kejie", ""]]}, {"id": "1911.10676", "submitter": "Chaoqin Huang", "authors": "Chaoqin Huang, Fei Ye, Jinkun Cao, Maosen Li, Ya Zhang, Cewu Lu", "title": "Attribute Restoration Framework for Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent advances in deep neural networks, anomaly detection in\nmultimedia has received much attention in the computer vision community. While\nreconstruction-based methods have recently shown great promise for anomaly\ndetection, the information equivalence among input and supervision for\nreconstruction tasks can not effectively force the network to learn semantic\nfeature embeddings. We here propose to break this equivalence by erasing\nselected attributes from the original data and reformulate it as a restoration\ntask, where the normal and the anomalous data are expected to be\ndistinguishable based on restoration errors. Through forcing the network to\nrestore the original image, the semantic feature embeddings related to the\nerased attributes are learned by the network. During testing phases, because\nanomalous data are restored with the attribute learned from the normal data,\nthe restoration error is expected to be large. Extensive experiments have\ndemonstrated that the proposed method significantly outperforms several\nstate-of-the-arts on multiple benchmark datasets, especially on ImageNet,\nincreasing the AUROC of the top-performing baseline by 10.1%. We also evaluate\nour method on a real-world anomaly detection dataset MVTec AD and a video\nanomaly detection dataset ShanghaiTech.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 03:06:43 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 10:23:33 GMT"}, {"version": "v3", "created": "Sat, 12 Dec 2020 07:50:28 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Huang", "Chaoqin", ""], ["Ye", "Fei", ""], ["Cao", "Jinkun", ""], ["Li", "Maosen", ""], ["Zhang", "Ya", ""], ["Lu", "Cewu", ""]]}, {"id": "1911.10683", "submitter": "Xu Zhong", "authors": "Xu Zhong, Elaheh ShafieiBavani, Antonio Jimeno Yepes", "title": "Image-based table recognition: data, model, and evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Important information that relates to a specific topic in a document is often\norganized in tabular format to assist readers with information retrieval and\ncomparison, which may be difficult to provide in natural language. However,\ntabular data in unstructured digital documents, e.g., Portable Document Format\n(PDF) and images, are difficult to parse into structured machine-readable\nformat, due to complexity and diversity in their structure and style. To\nfacilitate image-based table recognition with deep learning, we develop the\nlargest publicly available table recognition dataset PubTabNet\n(https://github.com/ibm-aur-nlp/PubTabNet), containing 568k table images with\ncorresponding structured HTML representation. PubTabNet is automatically\ngenerated by matching the XML and PDF representations of the scientific\narticles in PubMed Central Open Access Subset (PMCOA). We also propose a novel\nattention-based encoder-dual-decoder (EDD) architecture that converts images of\ntables into HTML code. The model has a structure decoder which reconstructs the\ntable structure and helps the cell decoder to recognize cell content. In\naddition, we propose a new Tree-Edit-Distance-based Similarity (TEDS) metric\nfor table recognition, which more appropriately captures multi-hop cell\nmisalignment and OCR errors than the pre-established metric. The experiments\ndemonstrate that the EDD model can accurately recognize complex tables solely\nrelying on the image representation, outperforming the state-of-the-art by 9.7%\nabsolute TEDS score.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 03:25:03 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 00:18:15 GMT"}, {"version": "v3", "created": "Tue, 7 Jan 2020 21:18:43 GMT"}, {"version": "v4", "created": "Mon, 2 Mar 2020 22:27:21 GMT"}, {"version": "v5", "created": "Wed, 4 Mar 2020 02:38:07 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Zhong", "Xu", ""], ["ShafieiBavani", "Elaheh", ""], ["Yepes", "Antonio Jimeno", ""]]}, {"id": "1911.10686", "submitter": "Hejia Zhang", "authors": "Hejia Zhang and Stefanos Nikolaidis", "title": "Robot Learning and Execution of Collaborative Manipulation Plans from\n  YouTube Cooking Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People often watch videos on the web to learn how to cook new recipes,\nassemble furniture or repair a computer. We wish to enable robots with the very\nsame capability. This is challenging; there is a large variation in\nmanipulation actions and some videos even involve multiple persons, who\ncollaborate by sharing and exchanging objects and tools. Furthermore, the\nlearned representations need to be general enough to be transferable to robotic\nsystems. On the other hand, previous work has shown that the space of human\nmanipulation actions has a linguistic, hierarchical structure that relates\nactions to manipulated objects and tools. Building upon this theory of language\nfor action, we propose a framework for understanding and executing demonstrated\naction sequences from full-length, unconstrained cooking videos on the web. The\nframework takes as input a cooking video annotated with object labels and\nbounding boxes, and outputs a collaborative manipulation action plan for one or\nmore robotic arms. We demonstrate performance of the system in a standardized\ndataset of 100 YouTube cooking videos, as well as in three full-length Youtube\nvideos that include collaborative actions between two participants. We\nadditionally propose an open-source platform for executing the learned plans in\na simulation environment as well as with an actual robotic arm.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 03:41:00 GMT"}, {"version": "v2", "created": "Sun, 22 Dec 2019 21:50:39 GMT"}, {"version": "v3", "created": "Mon, 3 Feb 2020 08:39:06 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Zhang", "Hejia", ""], ["Nikolaidis", "Stefanos", ""]]}, {"id": "1911.10688", "submitter": "Zhenyue Qin", "authors": "Zhenyue Qin and Dongwoo Kim and Tom Gedeon", "title": "Rethinking Softmax with Cross-Entropy: Neural Network Classifier as\n  Mutual Information Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mutual information is widely applied to learn latent representations of\nobservations, whilst its implication in classification neural networks remain\nto be better explained. We show that optimising the parameters of\nclassification neural networks with softmax cross-entropy is equivalent to\nmaximising the mutual information between inputs and labels under the balanced\ndata assumption. Through experiments on synthetic and real datasets, we show\nthat softmax cross-entropy can estimate mutual information approximately. When\napplied to image classification, this relation helps approximate the point-wise\nmutual information between an input image and a label without modifying the\nnetwork structure. To this end, we propose infoCAM, informative class\nactivation map, which highlights regions of the input image that are the most\nrelevant to a given label based on differences in information. The activation\nmap helps localise the target object in an input image. Through experiments on\nthe semi-supervised object localisation task with two real-world datasets, we\nevaluate the effectiveness of our information-theoretic approach.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 03:44:34 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 11:46:40 GMT"}, {"version": "v3", "created": "Sun, 29 Mar 2020 12:53:51 GMT"}, {"version": "v4", "created": "Thu, 17 Sep 2020 04:47:27 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Qin", "Zhenyue", ""], ["Kim", "Dongwoo", ""], ["Gedeon", "Tom", ""]]}, {"id": "1911.10692", "submitter": "Mei Wang", "authors": "Mei Wang, Weihong Deng", "title": "Mitigate Bias in Face Recognition using Skewness-Aware Reinforcement\n  Learning", "comments": null, "journal-ref": "CVPR 2020: 9322-9331", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Racial equality is an important theme of international human rights law, but\nit has been largely obscured when the overall face recognition accuracy is\npursued blindly. More facts indicate racial bias indeed degrades the fairness\nof recognition system and the error rates on non-Caucasians are usually much\nhigher than Caucasians. To encourage fairness, we introduce the idea of\nadaptive margin to learn balanced performance for different races based on\nlarge margin losses. A reinforcement learning based race balance network\n(RL-RBN) is proposed. We formulate the process of finding the optimal margins\nfor non-Caucasians as a Markov decision process and employ deep Q-learning to\nlearn policies for an agent to select appropriate margin by approximating the\nQ-value function. Guided by the agent, the skewness of feature scatter between\nraces can be reduced. Besides, we provide two ethnicity aware training\ndatasets, called BUPT-Globalface and BUPT-Balancedface dataset, which can be\nutilized to study racial bias from both data and algorithm aspects. Extensive\nexperiments on RFW database show that RL-RBN successfully mitigates racial bias\nand learns more balanced performance for different races.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 03:58:11 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Wang", "Mei", ""], ["Deng", "Weihong", ""]]}, {"id": "1911.10695", "submitter": "Yuzhe Yang", "authors": "Minghao Guo, Yuzhe Yang, Rui Xu, Ziwei Liu, Dahua Lin", "title": "When NAS Meets Robustness: In Search of Robust Architectures against\n  Adversarial Attacks", "comments": "CVPR 2020. First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in adversarial attacks uncover the intrinsic vulnerability of\nmodern deep neural networks. Since then, extensive efforts have been devoted to\nenhancing the robustness of deep networks via specialized learning algorithms\nand loss functions. In this work, we take an architectural perspective and\ninvestigate the patterns of network architectures that are resilient to\nadversarial attacks. To obtain the large number of networks needed for this\nstudy, we adopt one-shot neural architecture search, training a large network\nfor once and then finetuning the sub-networks sampled therefrom. The sampled\narchitectures together with the accuracies they achieve provide a rich basis\nfor our study. Our \"robust architecture Odyssey\" reveals several valuable\nobservations: 1) densely connected patterns result in improved robustness; 2)\nunder computational budget, adding convolution operations to direct connection\nedge is effective; 3) flow of solution procedure (FSP) matrix is a good\nindicator of network robustness. Based on these observations, we discover a\nfamily of robust architectures (RobNets). On various datasets, including CIFAR,\nSVHN, Tiny-ImageNet, and ImageNet, RobNets exhibit superior robustness\nperformance to other widely used architectures. Notably, RobNets substantially\nimprove the robust accuracy (~5% absolute gains) under both white-box and\nblack-box attacks, even with fewer parameter numbers. Code is available at\nhttps://github.com/gmh14/RobNets.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 04:14:02 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 06:07:55 GMT"}, {"version": "v3", "created": "Thu, 26 Mar 2020 01:37:40 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Guo", "Minghao", ""], ["Yang", "Yuzhe", ""], ["Xu", "Rui", ""], ["Liu", "Ziwei", ""], ["Lin", "Dahua", ""]]}, {"id": "1911.10713", "submitter": "Jinlu Liu", "authors": "Jinlu Liu and Liang Song and Yongqiang Qin", "title": "Prototype Rectification for Few-Shot Learning", "comments": "ECCV 2020 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning requires to recognize novel classes with scarce labeled\ndata. Prototypical network is useful in existing researches, however, training\non narrow-size distribution of scarce data usually tends to get biased\nprototypes. In this paper, we figure out two key influencing factors of the\nprocess: the intra-class bias and the cross-class bias. We then propose a\nsimple yet effective approach for prototype rectification in transductive\nsetting. The approach utilizes label propagation to diminish the intra-class\nbias and feature shifting to diminish the cross-class bias. We also conduct\ntheoretical analysis to derive its rationality as well as the lower bound of\nthe performance. Effectiveness is shown on three few-shot benchmarks. Notably,\nour approach achieves state-of-the-art performance on both miniImageNet (70.31%\non 1-shot and 81.89% on 5-shot) and tieredImageNet (78.74% on 1-shot and 86.92%\non 5-shot).\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 06:04:29 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 08:39:00 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2020 13:21:07 GMT"}, {"version": "v4", "created": "Mon, 13 Jul 2020 06:59:51 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Liu", "Jinlu", ""], ["Song", "Liang", ""], ["Qin", "Yongqiang", ""]]}, {"id": "1911.10714", "submitter": "Zhichao Fu", "authors": "Zhichao Fu, Yu Kong, Yingbin Zheng, Hao Ye, Wenxin Hu, Jing Yang,\n  Liang He", "title": "Cascaded Detail-Preserving Networks for Super-Resolution of Document\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accuracy of OCR is usually affected by the quality of the input document\nimage and different kinds of marred document images hamper the OCR results.\nAmong these scenarios, the low-resolution image is a common and challenging\ncase. In this paper, we propose the cascaded networks for document image\nsuper-resolution. Our model is composed by the Detail-Preserving Networks with\nsmall magnification. The loss function with perceptual terms is designed to\nsimultaneously preserve the original patterns and enhance the edge of the\ncharacters. These networks are trained with the same architecture and different\nparameters and then assembled into a pipeline model with a larger\nmagnification. The low-resolution images can upscale gradually by passing\nthrough each Detail-Preserving Network until the final high-resolution images.\nThrough extensive experiments on two scanning document image datasets, we\ndemonstrate that the proposed approach outperforms recent state-of-the-art\nimage super-resolution methods, and combining it with standard OCR system lead\nto signification improvements on the recognition results.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 06:19:41 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Fu", "Zhichao", ""], ["Kong", "Yu", ""], ["Zheng", "Yingbin", ""], ["Ye", "Hao", ""], ["Hu", "Wenxin", ""], ["Yang", "Jing", ""], ["He", "Liang", ""]]}, {"id": "1911.10727", "submitter": "Takumi Saikawa", "authors": "Takumi Saikawa, Quan Huu Cap, Satoshi Kagiwada, Hiroyuki Uga, Hitoshi\n  Iyatomi", "title": "AOP: An Anti-overfitting Pretreatment for Practical Image-based Plant\n  Diagnosis", "comments": "To appear in the IEEE BigData 2019 Workshop on Big Food and Nutrition\n  Data Management and Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image-based plant diagnosis, clues related to diagnosis are often unclear,\nand the other factors such as image backgrounds often have a significant impact\non the final decision. As a result, overfitting due to latent similarities in\nthe dataset often occurs, and the diagnostic performance on real unseen data\n(e,g. images from other farms) is usually dropped significantly. However, this\nproblem has not been sufficiently explored, since many systems have shown\nexcellent diagnostic performance due to the bias caused by the similarities in\nthe dataset. In this study, we investigate this problem with experiments using\nmore than 50,000 images of cucumber leaves, and propose an anti-overfitting\npretreatment (AOP) for realizing practical image-based plant diagnosis systems.\nThe AOP detects the area of interest (leaf, fruit etc.) and performs brightness\ncalibration as a preprocessing step. The experimental results demonstrate that\nour AOP can improve the accuracy of diagnosis for unknown test images from\ndifferent farms by 12.2% in a practical setting.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 06:59:08 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Saikawa", "Takumi", ""], ["Cap", "Quan Huu", ""], ["Kagiwada", "Satoshi", ""], ["Uga", "Hiroyuki", ""], ["Iyatomi", "Hitoshi", ""]]}, {"id": "1911.10729", "submitter": "Jingru Yi", "authors": "Pengxiang Wu and Chao Chen and Jingru Yi and Dimitris Metaxas", "title": "Point Cloud Processing via Recurrent Set Encoding", "comments": "AAAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new permutation-invariant network for 3D point cloud processing.\nOur network is composed of a recurrent set encoder and a convolutional feature\naggregator. Given an unordered point set, the encoder firstly partitions its\nambient space into parallel beams. Points within each beam are then modeled as\na sequence and encoded into subregional geometric features by a shared\nrecurrent neural network (RNN). The spatial layout of the beams is regular, and\nthis allows the beam features to be further fed into an efficient 2D\nconvolutional neural network (CNN) for hierarchical feature aggregation. Our\nnetwork is effective at spatial feature learning, and competes favorably with\nthe state-of-the-arts (SOTAs) on a number of benchmarks. Meanwhile, it is\nsignificantly more efficient compared to the SOTAs.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 07:04:40 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Wu", "Pengxiang", ""], ["Chen", "Chao", ""], ["Yi", "Jingru", ""], ["Metaxas", "Dimitris", ""]]}, {"id": "1911.10737", "submitter": "Liangchen Liu", "authors": "Liangchen Liu, Louis Ly, Colin Macdonald, and Yen-Hsi Richard Tsai", "title": "Nearest Neighbor Sampling of Point Sets using Random Rays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for the sampling, compression, and analysis of\ndistributions of point sets and other geometric objects embedded in Euclidean\nspaces. A set of randomly selected rays are projected onto their closest points\nin the data set, forming the ray signature. From the signature, statistical\ninformation about the data set, as well as certain geometrical information, can\nbe extracted, independent of the ray set. We present promising results from\n\"RayNN\", a neural network for the classification of point clouds based on ray\nsignatures.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 07:31:54 GMT"}, {"version": "v2", "created": "Fri, 29 Nov 2019 20:09:42 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Liu", "Liangchen", ""], ["Ly", "Louis", ""], ["Macdonald", "Colin", ""], ["Tsai", "Yen-Hsi Richard", ""]]}, {"id": "1911.10739", "submitter": "Ikki Kishida", "authors": "Ikki Kishida and Hideki Nakayama", "title": "Empirical Study of Easy and Hard Examples in CNN Training", "comments": "Accepted to ICONIP 2019", "journal-ref": "ICONIP 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) generalize well despite their massive size and\ncapability of memorizing all examples. There is a hypothesis that DNNs start\nlearning from simple patterns and the hypothesis is based on the existence of\nexamples that are consistently well-classified at the early training stage\n(i.e., easy examples) and examples misclassified (i.e., hard examples). Easy\nexamples are the evidence that DNNs start learning from specific patterns and\nthere is a consistent learning process. It is important to know how DNNs learn\npatterns and obtain generalization ability, however, properties of easy and\nhard examples are not thoroughly investigated (e.g., contributions to\ngeneralization and visual appearances). In this work, we study the similarities\nof easy and hard examples respectively for different Convolutional Neural\nNetwork (CNN) architectures, assessing how those examples contribute to\ngeneralization. Our results show that easy examples are visually similar to\neach other and hard examples are visually diverse, and both examples are\nlargely shared across different CNN architectures. Moreover, while hard\nexamples tend to contribute more to generalization than easy examples, removing\na large number of easy examples leads to poor generalization. By analyzing\nthose results, we hypothesize that biases in a dataset and Stochastic Gradient\nDescent (SGD) are the reasons why CNNs have consistent easy and hard examples.\nFurthermore, we show that large scale classification datasets can be\nefficiently compressed by using easiness proposed in this work.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 07:32:04 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Kishida", "Ikki", ""], ["Nakayama", "Hideki", ""]]}, {"id": "1911.10751", "submitter": "Yang Liu", "authors": "Yang Liu, Zhaoyang Lu, Jing Li, Tao Yang, Chao Yao", "title": "Deep Image-to-Video Adaptation and Fusion Networks for Action\n  Recognition", "comments": "Accepted by IEEE Transactions on Image Processing, codes can be found\n  at https://yangliu9208.github.io/DIVAFN/", "journal-ref": null, "doi": "10.1109/TIP.2019.2957930", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep learning methods for action recognition in videos require a\nlarge number of labeled videos for training, which is labor-intensive and\ntime-consuming. For the same action, the knowledge learned from different media\ntypes, e.g., videos and images, may be related and complementary. However, due\nto the domain shifts and heterogeneous feature representations between videos\nand images, the performance of classifiers trained on images may be\ndramatically degraded when directly deployed to videos. In this paper, we\npropose a novel method, named Deep Image-to-Video Adaptation and Fusion\nNetworks (DIVAFN), to enhance action recognition in videos by transferring\nknowledge from images using video keyframes as a bridge. The DIVAFN is a\nunified deep learning model, which integrates domain-invariant representations\nlearning and cross-modal feature fusion into a unified optimization framework.\nSpecifically, we design an efficient cross-modal similarities metric to reduce\nthe modality shift among images, keyframes and videos. Then, we adopt an\nautoencoder architecture, whose hidden layer is constrained to be the semantic\nrepresentations of the action class names. In this way, when the autoencoder is\nadopted to project the learned features from different domains to the same\nspace, more compact, informative and discriminative representations can be\nobtained. Finally, the concatenation of the learned semantic feature\nrepresentations from these three autoencoders are used to train the classifier\nfor action recognition in videos. Comprehensive experiments on four real-world\ndatasets show that our method outperforms some state-of-the-art domain\nadaptation and action recognition methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 08:02:17 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Liu", "Yang", ""], ["Lu", "Zhaoyang", ""], ["Li", "Jing", ""], ["Yang", "Tao", ""], ["Yao", "Chao", ""]]}, {"id": "1911.10752", "submitter": "Shan An", "authors": "Shan An, Guangfu Che, Fangru Zhou, Xianglong Liu, Xin Ma and Yu Chen", "title": "Fast and Incremental Loop Closure Detection Using Proximity Graphs", "comments": "8 pages, 6 figures, IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual loop closure detection, which can be considered as an image retrieval\ntask, is an important problem in SLAM (Simultaneous Localization and Mapping)\nsystems. The frequently used bag-of-words (BoW) models can achieve high\nprecision and moderate recall. However, the requirement for lower time costs\nand fewer memory costs for mobile robot applications is not well satisfied. In\nthis paper, we propose a novel loop closure detection framework titled `FILD'\n(Fast and Incremental Loop closure Detection), which focuses on an on-line and\nincremental graph vocabulary construction for fast loop closure detection. The\nglobal and local features of frames are extracted using the Convolutional\nNeural Networks (CNN) and SURF on the GPU, which guarantee extremely fast\nextraction speeds. The graph vocabulary construction is based on one type of\nproximity graph, named Hierarchical Navigable Small World (HNSW) graphs, which\nis modified to adapt to this specific application. In addition, this process is\ncoupled with a novel strategy for real-time geometrical verification, which\nonly keeps binary hash codes and significantly saves on memory usage. Extensive\nexperiments on several publicly available datasets show that the proposed\napproach can achieve fairly good recall at 100\\% precision compared to other\nstate-of-the-art methods. The source code can be downloaded at\nhttps://github.com/AnshanTJU/FILD for further studies.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 08:03:59 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["An", "Shan", ""], ["Che", "Guangfu", ""], ["Zhou", "Fangru", ""], ["Liu", "Xianglong", ""], ["Ma", "Xin", ""], ["Chen", "Yu", ""]]}, {"id": "1911.10771", "submitter": "Rui Shao", "authors": "Rui Shao, Xiangyuan Lan, Pong C. Yuen", "title": "Regularized Fine-grained Meta Face Anti-spoofing", "comments": "Accepted by AAAI 2020. Codes are available at\n  https://github.com/rshaojimmy/AAAI2020-RFMetaFAS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face presentation attacks have become an increasingly critical concern when\nface recognition is widely applied. Many face anti-spoofing methods have been\nproposed, but most of them ignore the generalization ability to unseen attacks.\nTo overcome the limitation, this work casts face anti-spoofing as a domain\ngeneralization (DG) problem, and attempts to address this problem by developing\na new meta-learning framework called Regularized Fine-grained Meta-learning. To\nlet our face anti-spoofing model generalize well to unseen attacks, the\nproposed framework trains our model to perform well in the simulated domain\nshift scenarios, which is achieved by finding generalized learning directions\nin the meta-learning process. Specifically, the proposed framework incorporates\nthe domain knowledge of face anti-spoofing as the regularization so that\nmeta-learning is conducted in the feature space regularized by the supervision\nof domain knowledge. This enables our model more likely to find generalized\nlearning directions with the regularized meta-learning for face anti-spoofing\ntask. Besides, to further enhance the generalization ability of our model, the\nproposed framework adopts a fine-grained learning strategy that simultaneously\nconducts meta-learning in a variety of domain shift scenarios in each\niteration. Extensive experiments on four public datasets validate the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 08:55:46 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Shao", "Rui", ""], ["Lan", "Xiangyuan", ""], ["Yuen", "Pong C.", ""]]}, {"id": "1911.10773", "submitter": "Yitong Yan", "authors": "Yitong Yan, Chuangchuang Liu, Changyou Chen, Xianfang Sun, Longcun\n  Jin, and Xiang Zhou", "title": "Fine-grained Attention and Feature-sharing Generative Adversarial\n  Networks for Single Image Super-Resolution", "comments": "15 pages, 14 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional super-resolution methods that aim to minimize the mean square\nerror usually produce the images with over-smoothed and blurry edges, due to\nthe lose of high-frequency details. In this paper, we propose two novel\ntechniques in the generative adversarial networks to produce photo-realistic\nimages for image super-resolution. Firstly, instead of producing a single score\nto discriminate images between real and fake, we propose a variant, called\nFine-grained Attention Generative Adversarial Network for image\nsuper-resolution (FASRGAN), to discriminate each pixel between real and fake.\nFASRGAN adopts a Unet-like network as the discriminator with two outputs: an\nimage score and an image score map. The score map has the same spatial size as\nthe HR/SR images, serving as the fine-grained attention to represent the degree\nof reconstruction difficulty for each pixel. Secondly, instead of using\ndifferent networks for the generator and the discriminator in the SR problem,\nwe use a feature-sharing network (Fs-SRGAN) for both the generator and the\ndiscriminator. By network sharing, certain information is shared between the\ngenerator and the discriminator, which in turn can improve the ability of\nproducing high-quality images. Quantitative and visual comparisons with the\nstate-of-the-art methods on the benchmark datasets demonstrate the superiority\nof our methods. The application of super-resolution images to object\nrecognition further proves that the proposed methods endow the power to\nreconstruction capabilities and the excellent super-resolution effects.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 09:03:15 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 14:21:22 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Yan", "Yitong", ""], ["Liu", "Chuangchuang", ""], ["Chen", "Changyou", ""], ["Sun", "Xianfang", ""], ["Jin", "Longcun", ""], ["Zhou", "Xiang", ""]]}, {"id": "1911.10782", "submitter": "Weizhe Liu", "authors": "Weizhe Liu, Mathieu Salzmann, Pascal Fua", "title": "Estimating People Flows to Better Count Them in Crowded Scenes", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern methods for counting people in crowded scenes rely on deep networks to\nestimate people densities in individual images. As such, only very few take\nadvantage of temporal consistency in video sequences, and those that do only\nimpose weak smoothness constraints across consecutive frames.\n  In this paper, we advocate estimating people flows across image locations\nbetween consecutive images and inferring the people densities from these flows\ninstead of directly regressing. This enables us to impose much stronger\nconstraints encoding the conservation of the number of people. As a result, it\nsignificantly boosts performance without requiring a more complex architecture.\nFurthermore, it also enables us to exploit the correlation between people flow\nand optical flow to further improve the results.\n  We will demonstrate that we consistently outperform state-of-the-art methods\non five benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 09:34:40 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 06:32:20 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2020 16:42:46 GMT"}, {"version": "v4", "created": "Wed, 15 Jul 2020 16:59:04 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Liu", "Weizhe", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""]]}, {"id": "1911.10796", "submitter": "Shihua Zhang", "authors": "Chihao Zhang, Kuo Gai and Shihua Zhang", "title": "Matrix Normal PCA for Interpretable Dimension Reduction and Graphical\n  Noise Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is one of the most widely used dimension\nreduction and multivariate statistical techniques. From a probabilistic\nperspective, PCA seeks a low-dimensional representation of data in the presence\nof independent identical Gaussian noise. Probabilistic PCA (PPCA) and its\nvariants have been extensively studied for decades. Most of them assume the\nunderlying noise follows a certain independent identical distribution. However,\nthe noise in the real world is usually complicated and structured. To address\nthis challenge, some variants of PCA for data with non-IID noise have been\nproposed. However, most of the existing methods only assume that the noise is\ncorrelated in the feature space while there may exist two-way structured noise.\nTo this end, we propose a powerful and intuitive PCA method (MN-PCA) through\nmodeling the graphical noise by the matrix normal distribution, which enables\nus to explore the structure of noise in both the feature space and the sample\nspace. MN-PCA obtains a low-rank representation of data and the structure of\nnoise simultaneously. And it can be explained as approximating data over the\ngeneralized Mahalanobis distance. We develop two algorithms to solve this\nmodel: one maximizes the regularized likelihood, the other exploits the\nWasserstein distance, which is more robust. Extensive experiments on various\ndata demonstrate their effectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 09:59:33 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 15:01:38 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Zhang", "Chihao", ""], ["Gai", "Kuo", ""], ["Zhang", "Shihua", ""]]}, {"id": "1911.10807", "submitter": "Jinlu Liu", "authors": "Liang Song and Jinlu Liu and Yongqiang Qin", "title": "Generalized Adaptation for Few-Shot Learning", "comments": "We found a bug in the script in reexamining some of this work. We\n  decide to withdraw for further modification", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Few-Shot Learning research works have two stages: pre-training base\nmodel and adapting to novel model. In this paper, we propose to use closed-form\nbase learner, which constrains the adapting stage with pre-trained base model\nto get better generalized novel model. Following theoretical analysis proves\nits rationality as well as indication of how to train a well-generalized base\nmodel. We then conduct experiments on four benchmarks and achieve\nstate-of-the-art performance in all cases. Notably, we achieve the accuracy of\n87.75% on 5-shot miniImageNet which approximately outperforms existing methods\nby 10%.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 10:18:37 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 11:04:06 GMT"}, {"version": "v3", "created": "Tue, 2 Jun 2020 06:24:39 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Song", "Liang", ""], ["Liu", "Jinlu", ""], ["Qin", "Yongqiang", ""]]}, {"id": "1911.10810", "submitter": "Yinglong Wang", "authors": "Yinglong Wang, Chao Ma, Bing Zeng", "title": "Deep Image Deraining Via Intrinsic Rainy Image Priors and Multi-scale\n  Auxiliary Decoding", "comments": "11 figures, 5 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different rain models and novel network structures have been proposed to\nremove rain streaks from single rainy images. In this work, we bring attention\nto the intrinsic priors and multi-scale features of the rainy images, and\ndevelop several intrinsic loss functions to train a CNN deraining network. We\nfirst study the sparse priors of rainy images, which have been verified to\npreserve unbroken edges in image decomposition. However, its mathematical\nformulation usually leads to an intractable solution, we propose quasi-sparsity\npriors to decrease complexity, so that our network can be trained under the\nsupervision of sparse properties of rainy images. Quasi-sparsity supervises\nnetwork training in different gradient domain which is still ill-posed to\ndecompose a rainy image into rain layer and background layer. We develop\nanother $L_1$ loss based on the intrinsic low-value property of rain layer to\nrestore image contents together with the commonly-used $L_1$ similarity loss.\nMulti-scale features are further explored via a multi-scale auxiliary decoding\nstructure to show which kinds of features contribute the most to the deraining\ntask, and the corresponding multi-scale auxiliary loss improves the deraining\nperformance further. In our network, more efficient group convolution and\nfeature sharing are utilized to obtain an one order of magnitude improvement in\nnetwork running speed. The proposed deraining method performs favorably against\nstate-of-the-art deraining approaches.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 10:20:15 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Wang", "Yinglong", ""], ["Ma", "Chao", ""], ["Zeng", "Bing", ""]]}, {"id": "1911.10819", "submitter": "Maxim Berman", "authors": "Maxim Berman and Matthew B. Blaschko", "title": "Discriminative training of conditional random fields with probably\n  submodular constraints", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problems of segmentation, denoising, registration and 3D reconstruction are\noften addressed with the graph cut algorithm. However, solving an unconstrained\ngraph cut problem is NP-hard. For tractable optimization, pairwise potentials\nhave to fulfill the submodularity inequality. In our learning paradigm,\npairwise potentials are created as the dot product of a learned vector w with\npositive feature vectors. In order to constrain such a model to remain\ntractable, previous approaches have enforced the weight vector to be positive\nfor pairwise potentials in which the labels differ, and set pairwise potentials\nto zero in the case that the label remains the same. Such constraints are\nsufficient to guarantee that the resulting pairwise potentials satisfy the\nsubmodularity inequality. However, we show that such an approach unnecessarily\nrestricts the capacity of the learned models. Guaranteeing submodularity for\nall possible inputs, no matter how improbable, reduces inference error to\neffectively zero, but increases model error. In contrast, we relax the\nrequirement of guaranteed submodularity to solutions that are probably\napproximately submodular. We show that the conceptually simple strategy of\nenforcing submodularity on the training examples guarantees with low sample\ncomplexity that test images will also yield submodular pairwise potentials.\nResults are presented in the binary and muticlass settings, showing substantial\nimprovement from the resulting increased model capacity.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 10:38:05 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Berman", "Maxim", ""], ["Blaschko", "Matthew B.", ""]]}, {"id": "1911.10862", "submitter": "Hanlin Chen", "authors": "Hanlin Chen, Li'an Zhuo, Baochang Zhang, Xiawu Zheng, Jianzhuang Liu,\n  David Doermann, Rongrong Ji", "title": "Binarized Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) can have a significant impact in computer\nvision by automatically designing optimal neural network architectures for\nvarious tasks. A variant, binarized neural architecture search (BNAS), with a\nsearch space of binarized convolutions, can produce extremely compressed\nmodels. Unfortunately, this area remains largely unexplored. BNAS is more\nchallenging than NAS due to the learning inefficiency caused by optimization\nrequirements and the huge architecture space. To address these issues, we\nintroduce channel sampling and operation space reduction into a differentiable\nNAS to significantly reduce the cost of searching. This is accomplished through\na performance-based strategy used to abandon less potential operations. Two\noptimization methods for binarized neural networks are used to validate the\neffectiveness of our BNAS. Extensive experiments demonstrate that the proposed\nBNAS achieves a performance comparable to NAS on both CIFAR and ImageNet\ndatabases. An accuracy of $96.53\\%$ vs. $97.22\\%$ is achieved on the CIFAR-10\ndataset, but with a significantly compressed model, and a $40\\%$ faster search\nthan the state-of-the-art PC-DARTS.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 12:25:05 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 11:50:28 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Chen", "Hanlin", ""], ["Zhuo", "Li'an", ""], ["Zhang", "Baochang", ""], ["Zheng", "Xiawu", ""], ["Liu", "Jianzhuang", ""], ["Doermann", "David", ""], ["Ji", "Rongrong", ""]]}, {"id": "1911.10868", "submitter": "Marin Toromanoff", "authors": "Marin Toromanoff, Emilie Wirbel, Fabien Moutarde", "title": "End-to-End Model-Free Reinforcement Learning for Urban Driving using\n  Implicit Affordances", "comments": "Accepted at main conference of CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement Learning (RL) aims at learning an optimal behavior policy from\nits own experiments and not rule-based control methods. However, there is no RL\nalgorithm yet capable of handling a task as difficult as urban driving. We\npresent a novel technique, coined implicit affordances, to effectively leverage\nRL for urban driving thus including lane keeping, pedestrians and vehicles\navoidance, and traffic light detection. To our knowledge we are the first to\npresent a successful RL agent handling such a complex task especially regarding\nthe traffic light detection. Furthermore, we have demonstrated the\neffectiveness of our method by winning the Camera Only track of the CARLA\nchallenge.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 12:34:26 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 14:44:13 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Toromanoff", "Marin", ""], ["Wirbel", "Emilie", ""], ["Moutarde", "Fabien", ""]]}, {"id": "1911.10873", "submitter": "Marc Aubreville", "authors": "Marc Aubreville, Christof A. Bertram, Samir Jabari, Christian Marzahl,\n  Robert Klopfleisch, Andreas Maier", "title": "Learning New Tricks from Old Dogs -- Inter-Species, Inter-Tissue Domain\n  Adaptation for Mitotic Figure Assessment", "comments": "5 pages, submission to BVM 2020", "journal-ref": "Bildverarbeitung f\\\"ur die Medizin 2020. Informatik Aktuell. p.\n  1-7", "doi": "10.1007/978-3-658-29267-6_1", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For histopathological tumor assessment, the count of mitotic figures per area\nis an important part of prognostication. Algorithmic approaches - such as for\nmitotic figure identification - have significantly improved in recent times,\npotentially allowing for computer-augmented or fully automatic screening\nsystems in the future. This trend is further supported by whole slide scanning\nmicroscopes becoming available in many pathology labs and could soon become a\nstandard imaging tool.\n  For an application in broader fields of such algorithms, the availability of\nmitotic figure data sets of sufficient size for the respective tissue type and\nspecies is an important precondition, that is, however, rarely met. While\nalgorithmic performance climbed steadily for e.g. human mammary carcinoma,\nthanks to several challenges held in the field, for most tumor types, data sets\nare not available.\n  In this work, we assess domain transfer of mitotic figure recognition using\ndomain adversarial training on four data sets, two from dogs and two from\nhumans. We were able to show that domain adversarial training considerably\nimproves accuracy when applying mitotic figure classification learned from the\ncanine on the human data sets (up to +12.8% in accuracy) and is thus a helpful\nmethod to transfer knowledge from existing data sets to new tissue types and\nspecies.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 12:45:33 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Aubreville", "Marc", ""], ["Bertram", "Christof A.", ""], ["Jabari", "Samir", ""], ["Marzahl", "Christian", ""], ["Klopfleisch", "Robert", ""], ["Maier", "Andreas", ""]]}, {"id": "1911.10891", "submitter": "Ali Shahin Shamsabadi", "authors": "Ali Shahin Shamsabadi, Ricardo Sanchez-Matilla, Andrea Cavallaro", "title": "ColorFool: Semantic Adversarial Colorization", "comments": "Conference on Computer Vision and Pattern Recognition (CVPR2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks that generate small L_p-norm perturbations to mislead\nclassifiers have limited success in black-box settings and with unseen\nclassifiers. These attacks are also not robust to defenses that use denoising\nfilters and to adversarial training procedures. Instead, adversarial attacks\nthat generate unrestricted perturbations are more robust to defenses, are\ngenerally more successful in black-box settings and are more transferable to\nunseen classifiers. However, unrestricted perturbations may be noticeable to\nhumans. In this paper, we propose a content-based black-box adversarial attack\nthat generates unrestricted perturbations by exploiting image semantics to\nselectively modify colors within chosen ranges that are perceived as natural by\nhumans. We show that the proposed approach, ColorFool, outperforms in terms of\nsuccess rate, robustness to defense frameworks and transferability, five\nstate-of-the-art adversarial attacks on two different tasks, scene and object\nclassification, when attacking three state-of-the-art deep neural networks\nusing three standard datasets. The source code is available at\nhttps://github.com/smartcameras/ColorFool.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 13:10:29 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2020 09:40:31 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Shamsabadi", "Ali Shahin", ""], ["Sanchez-Matilla", "Ricardo", ""], ["Cavallaro", "Andrea", ""]]}, {"id": "1911.10927", "submitter": "Denys Rozumnyi", "authors": "Denys Rozumnyi, Jan Kotera, Filip Sroubek, Jiri Matas", "title": "Sub-frame Appearance and 6D Pose Estimation of Fast Moving Objects", "comments": null, "journal-ref": "2020 IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR)", "doi": "10.1109/CVPR42600.2020.00681", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method that tracks fast moving objects, mainly non-uniform\nspherical, in full 6 degrees of freedom, estimating simultaneously their 3D\nmotion trajectory, 3D pose and object appearance changes with a time step that\nis a fraction of the video frame exposure time. The sub-frame object\nlocalization and appearance estimation allows realistic temporal\nsuper-resolution and precise shape estimation. The method, called TbD-3D\n(Tracking by Deblatting in 3D) relies on a novel reconstruction algorithm which\nsolves a piece-wise deblurring and matting problem. The 3D rotation is\nestimated by minimizing the reprojection error. As a second contribution, we\npresent a new challenging dataset with fast moving objects that change their\nappearance and distance to the camera. High speed camera recordings with zero\nlag between frame exposures were used to generate videos with different frame\nrates annotated with ground-truth trajectory and pose.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 14:13:25 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Rozumnyi", "Denys", ""], ["Kotera", "Jan", ""], ["Sroubek", "Filip", ""], ["Matas", "Jiri", ""]]}, {"id": "1911.10949", "submitter": "Rundi Wu", "authors": "Rundi Wu, Yixin Zhuang, Kai Xu, Hao Zhang, Baoquan Chen", "title": "PQ-NET: A Generative Part Seq2Seq Network for 3D Shapes", "comments": "Accepted to CVPR 2020. Code available at\n  https://github.com/ChrisWu1997/PQ-NET", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce PQ-NET, a deep neural network which represents and generates 3D\nshapes via sequential part assembly. The input to our network is a 3D shape\nsegmented into parts, where each part is first encoded into a feature\nrepresentation using a part autoencoder. The core component of PQ-NET is a\nsequence-to-sequence or Seq2Seq autoencoder which encodes a sequence of part\nfeatures into a latent vector of fixed size, and the decoder reconstructs the\n3D shape, one part at a time, resulting in a sequential assembly. The latent\nspace formed by the Seq2Seq encoder encodes both part structure and fine part\ngeometry. The decoder can be adapted to perform several generative tasks\nincluding shape autoencoding, interpolation, novel shape generation, and\nsingle-view 3D reconstruction, where the generated shapes are all composed of\nmeaningful parts.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 14:43:05 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 10:16:05 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2020 01:38:40 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Wu", "Rundi", ""], ["Zhuang", "Yixin", ""], ["Xu", "Kai", ""], ["Zhang", "Hao", ""], ["Chen", "Baoquan", ""]]}, {"id": "1911.10967", "submitter": "Miao Liu", "authors": "Miao Liu, Siyu Tang, Yin Li, James Rehg", "title": "Forecasting Human-Object Interaction: Joint Prediction of Motor\n  Attention and Actions in First Person Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the challenging task of anticipating human-object interaction in\nfirst person videos. Most existing methods ignore how the camera wearer\ninteracts with the objects, or simply consider body motion as a separate\nmodality. In contrast, we observe that the international hand movement reveals\ncritical information about the future activity. Motivated by this, we adopt\nintentional hand movement as a future representation and propose a novel deep\nnetwork that jointly models and predicts the egocentric hand motion,\ninteraction hotspots and future action. Specifically, we consider the future\nhand motion as the motor attention, and model this attention using latent\nvariables in our deep model. The predicted motor attention is further used to\ncharacterise the discriminative spatial-temporal visual features for predicting\nactions and interaction hotspots. We present extensive experiments\ndemonstrating the benefit of the proposed joint model. Importantly, our model\nproduces new state-of-the-art results for action anticipation on both EGTEA\nGaze+ and the EPIC-Kitchens datasets. Our project page is available at\nhttps://aptx4869lm.github.io/ForecastingHOI/\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 15:10:20 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 01:58:19 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Liu", "Miao", ""], ["Tang", "Siyu", ""], ["Li", "Yin", ""], ["Rehg", "James", ""]]}, {"id": "1911.10979", "submitter": "Yong-Goo Shin", "authors": "Yong-Goo Shin, Yoon-Jae Yeo, and Sung-Jea Ko", "title": "Simple yet Effective Way for Improving the Performance of GAN", "comments": "Accepted to IEEE transactions on neural networks and learning systems", "journal-ref": null, "doi": "10.1109/TNNLS.2020.3045000", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In adversarial learning, discriminator often fails to guide the generator\nsuccessfully since it distinguishes between real and generated images using\nsilly or non-robust features. To alleviate this problem, this brief presents a\nsimple but effective way that improves the performance of generative\nadversarial network (GAN) without imposing the training overhead or modifying\nthe network architectures of existing methods. The proposed method employs a\nnovel cascading rejection (CR) module for discriminator, which extracts\nmultiple non-overlapped features in an iterative manner using the vector\nrejection operation. Since the extracted diverse features prevent the\ndiscriminator from concentrating on non-meaningful features, the discriminator\ncan guide the generator effectively to produce the images that are more similar\nto the real images. In addition, since the proposed CR module requires only a\nfew simple vector operations, it can be readily applied to existing frameworks\nwith marginal training overheads. Quantitative evaluations on various datasets\nincluding CIFAR-10, CelebA, CelebA-HQ, LSUN, and tiny-ImageNet confirm that the\nproposed method significantly improves the performance of GAN and conditional\nGAN in terms of Frechet inception distance (FID) indicating the diversity and\nvisual appearance of the generated images.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 10:31:19 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 03:50:17 GMT"}, {"version": "v3", "created": "Sun, 10 May 2020 04:33:33 GMT"}, {"version": "v4", "created": "Tue, 19 Jan 2021 16:19:15 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Shin", "Yong-Goo", ""], ["Yeo", "Yoon-Jae", ""], ["Ko", "Sung-Jea", ""]]}, {"id": "1911.10989", "submitter": "Valeriya Pronina", "authors": "Valeriya Pronina, Filippos Kokkinos, Dmitry V. Dylov, Stamatios\n  Lefkimmiatis", "title": "Microscopy Image Restoration with Deep Wiener-Kolmogorov filters", "comments": "Updated version", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microscopy is a powerful visualization tool in biology, enabling the study of\ncells, tissues, and the fundamental biological processes; yet, the observed\nimages typically suffer from blur and background noise. In this work, we\npropose a unifying framework of algorithms for Gaussian image deblurring and\ndenoising. These algorithms are based on deep learning techniques for the\ndesign of learnable regularizers integrated into the Wiener-Kolmogorov filter.\nOur extensive experimentation line showcases that the proposed approach\nachieves a superior quality of image reconstruction and surpasses the solutions\nthat rely either on deep learning or on optimization schemes alone. Augmented\nwith the variance stabilizing transformation, the proposed reconstruction\npipeline can also be successfully applied to the problem of Poisson image\ndeblurring, surpassing the state-of-the-art methods. Moreover, several variants\nof the proposed framework demonstrate competitive performance at low\ncomputational complexity, which is of high importance for real-time imaging\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 15:36:05 GMT"}, {"version": "v2", "created": "Wed, 25 Dec 2019 08:50:47 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 09:15:55 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Pronina", "Valeriya", ""], ["Kokkinos", "Filippos", ""], ["Dylov", "Dmitry V.", ""], ["Lefkimmiatis", "Stamatios", ""]]}, {"id": "1911.11010", "submitter": "Andrey Savchenko", "authors": "Andrey V. Savchenko", "title": "Event Recognition with Automatic Album Detection based on Sequential\n  Processing, Neural Attention and Image Captioning", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a new formulation of event recognition task is examined: it is\nrequired to predict event categories in a gallery of images, for which albums\n(groups of photos corresponding to a single event) are unknown. We propose the\nnovel two-stage approach. At first, features are extracted in each photo using\nthe pre-trained convolutional neural network. These features are classified\nindividually. The scores of the classifier are used to group sequential photos\ninto several clusters. Finally, the features of photos in each group are\naggregated into a single descriptor using neural attention mechanism. This\nalgorithm is optionally extended to improve the accuracy for classification of\neach image in an album. In contrast to conventional fine-tuning of\nconvolutional neural networks (CNN) we proposed to use image captioning, i.e.,\ngenerative model that converts images to textual descriptions. They are one-hot\nencoded and summarized into sparse feature vector suitable for learning of\narbitrary classifier. Experimental study with Photo Event Collection and\nMulti-Label Curation of Flickr Events Dataset demonstrates that our approach is\n9-20% more accurate than event recognition on single photos. Moreover, proposed\nmethod has 13-16% lower error rate than classification of groups of photos\nobtained with hierarchical clustering. It is experimentally shown that the\nimage captions trained on Conceptual Captions dataset can be classified more\naccurately than the features from object detector, though they both are\nobviously not as rich as the CNN-based features. However, it is possible to\ncombine our approach with conventional CNNs in an ensemble to provide the\nstate-of-the-art results for several event datasets.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 15:58:18 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 05:44:26 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Savchenko", "Andrey V.", ""]]}, {"id": "1911.11028", "submitter": "Dongdong Chen", "authors": "Dongdong Chen, Mike E. Davies", "title": "Deep Decomposition Learning for Inverse Imaging Problems", "comments": "To appear in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is emerging as a new paradigm for solving inverse imaging\nproblems. However, the deep learning methods often lack the assurance of\ntraditional physics-based methods due to the lack of physical information\nconsiderations in neural network training and deploying. The appropriate\nsupervision and explicit calibration by the information of the physic model can\nenhance the neural network learning and its practical performance. In this\npaper, inspired by the geometry that data can be decomposed by two components\nfrom the null-space of the forward operator and the range space of its\npseudo-inverse, we train neural networks to learn the two components and\ntherefore learn the decomposition, i.e. we explicitly reformulate the neural\nnetwork layers as learning range-nullspace decomposition functions with\nreference to the layer inputs, instead of learning unreferenced functions. We\nempirically show that the proposed framework demonstrates superior performance\nover recent deep residual learning, unrolled learning and nullspace learning on\ntasks including compressive sensing medical imaging and natural image\nsuper-resolution. Our code is available at\nhttps://github.com/edongdongchen/DDN.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 16:24:13 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 01:36:23 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 21:47:03 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Chen", "Dongdong", ""], ["Davies", "Mike E.", ""]]}, {"id": "1911.11033", "submitter": "Mehmet Ozgur Turkoglu", "authors": "Mehmet Ozgur Turkoglu, Stefano D'Aronco, Jan Dirk Wegner, Konrad\n  Schindler", "title": "Gating Revisited: Deep Multi-layer RNNs That Can Be Trained", "comments": "To appear in TPAMI (accepted March 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new STAckable Recurrent cell (STAR) for recurrent neural\nnetworks (RNNs), which has fewer parameters than widely used LSTM and GRU while\nbeing more robust against vanishing or exploding gradients. Stacking recurrent\nunits into deep architectures suffers from two major limitations: (i) many\nrecurrent cells (e.g., LSTMs) are costly in terms of parameters and computation\nresources; and (ii) deep RNNs are prone to vanishing or exploding gradients\nduring training. We investigate the training of multi-layer RNNs and examine\nthe magnitude of the gradients as they propagate through the network in the\n\"vertical\" direction. We show that, depending on the structure of the basic\nrecurrent unit, the gradients are systematically attenuated or amplified. Based\non our analysis we design a new type of gated cell that better preserves\ngradient magnitude. We validate our design on a large number of sequence\nmodelling tasks and demonstrate that the proposed STAR cell allows to build and\ntrain deeper recurrent architectures, ultimately leading to improved\nperformance while being computationally more efficient.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 16:35:51 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 08:22:12 GMT"}, {"version": "v3", "created": "Sat, 28 Nov 2020 18:07:40 GMT"}, {"version": "v4", "created": "Sat, 6 Mar 2021 12:26:10 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Turkoglu", "Mehmet Ozgur", ""], ["D'Aronco", "Stefano", ""], ["Wegner", "Jan Dirk", ""], ["Schindler", "Konrad", ""]]}, {"id": "1911.11079", "submitter": "Meisam Rakhshanfar", "authors": "Meisam Rakhshanfar", "title": "Radius Adaptive Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) is widely used in computer vision\napplications. In the networks that deal with images, CNNs are the most\ntime-consuming layer of the networks. Usually, the solution to address the\ncomputation cost is to decrease the number of trainable parameters. This\nsolution usually comes with the cost of dropping the accuracy. Another problem\nwith this technique is that usually the cost of memory access is not taken into\naccount which results in insignificant speedup gain. The number of operations\nand memory access in a standard convolution layer is independent of the input\ncontent, which makes it limited for certain accelerations. We propose a simple\nmodification to a standard convolution to bridge this gap. We propose an\nadaptive convolution that adopts different kernel sizes (or radii) based on the\ncontent. The network can learn and select the proper radius based on the input\ncontent in a soft decision manner. Our proposed radius-adaptive convolutional\nneural network (RACNN) has a similar number of weights to a standard one, yet,\nresults show it can reach higher speeds. The code has been made available at:\nhttps://github.com/meisamrf/racnn.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 17:35:04 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Rakhshanfar", "Meisam", ""]]}, {"id": "1911.11081", "submitter": "Ashkan Khakzar", "authors": "Ashkan Khakzar, Soroosh Baselizadeh, Saurabh Khanduja, Christian\n  Rupprecht, Seong Tae Kim, Nassir Navab", "title": "Improving Feature Attribution through Input-specific Network Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attributing the output of a neural network to the contribution of given input\nelements is a way of shedding light on the black-box nature of neural networks.\nDue to the complexity of current network architectures, current gradient-based\nattribution methods provide very noisy or coarse results. We propose to prune a\nneural network for a given single input to keep only neurons that highly\ncontribute to the prediction. We show that by input-specific pruning, network\ngradients change from reflecting local (noisy) importance information to global\nimportance. Our proposed method is efficient and generates fine-grained\nattribution maps. We further provide a theoretical justification of the pruning\napproach relating it to perturbations and validate it through a novel\nexperimental setup. Our method is evaluated by multiple benchmarks: sanity\nchecks, pixel perturbation, and Remove-and-Retrain (ROAR). These benchmarks\nevaluate the method from different perspectives and our method performs better\nthan other methods across all evaluations.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 17:40:37 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 17:39:42 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Khakzar", "Ashkan", ""], ["Baselizadeh", "Soroosh", ""], ["Khanduja", "Saurabh", ""], ["Rupprecht", "Christian", ""], ["Kim", "Seong Tae", ""], ["Navab", "Nassir", ""]]}, {"id": "1911.11098", "submitter": "Kaichun Mo", "authors": "Kaichun Mo, Paul Guerrero, Li Yi, Hao Su, Peter Wonka, Niloy Mitra,\n  Leonidas J. Guibas", "title": "StructEdit: Learning Structural Shape Variations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to encode differences in the geometry and (topological) structure of\nthe shapes of ordinary objects is key to generating semantically plausible\nvariations of a given shape, transferring edits from one shape to another, and\nmany other applications in 3D content creation. The common approach of encoding\nshapes as points in a high-dimensional latent feature space suggests treating\nshape differences as vectors in that space. Instead, we treat shape differences\nas primary objects in their own right and propose to encode them in their own\nlatent space. In a setting where the shapes themselves are encoded in terms of\nfine-grained part hierarchies, we demonstrate that a separate encoding of shape\ndeltas or differences provides a principled way to deal with inhomogeneities in\nthe shape space due to different combinatorial part structures, while also\nallowing for compactness in the representation, as well as edit abstraction and\ntransfer. Our approach is based on a conditional variational autoencoder for\nencoding and decoding shape deltas, conditioned on a source shape. We\ndemonstrate the effectiveness and robustness of our approach in multiple shape\nmodification and generation tasks, and provide comparison and ablation studies\non the PartNet dataset, one of the largest publicly available 3D datasets.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 18:08:04 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Mo", "Kaichun", ""], ["Guerrero", "Paul", ""], ["Yi", "Li", ""], ["Su", "Hao", ""], ["Wonka", "Peter", ""], ["Mitra", "Niloy", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "1911.11111", "submitter": "Lin Zhang", "authors": "Lin Zhang", "title": "Phase Contrast Microscopy Cell PopulationSegmentation: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Phase contrast microscopy (PCM) has been widely used in biomedicine research,\nwhich allows users to observe objectives without staining or killing them. One\nimportant related research is to employ PCM to monitor live cells. How to\nsegment cell populations in obtained PCM images gains more and more attention\nas its a critical step for downstream applications, such as cell tracking, cell\nclassification and others. Many papers have been published to deal with this\nproblem from different perspectives. In this paper we aim to present a\ncomprehensive review on the development of PCM cell population segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 18:20:40 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Zhang", "Lin", ""]]}, {"id": "1911.11130", "submitter": "Shangzhe Wu", "authors": "Shangzhe Wu, Christian Rupprecht, Andrea Vedaldi", "title": "Unsupervised Learning of Probably Symmetric Deformable 3D Objects from\n  Images in the Wild", "comments": "CVPR 2020 Oral. Project page: https://elliottwu.com/projects/unsup3d/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to learn 3D deformable object categories from raw\nsingle-view images, without external supervision. The method is based on an\nautoencoder that factors each input image into depth, albedo, viewpoint and\nillumination. In order to disentangle these components without supervision, we\nuse the fact that many object categories have, at least in principle, a\nsymmetric structure. We show that reasoning about illumination allows us to\nexploit the underlying object symmetry even if the appearance is not symmetric\ndue to shading. Furthermore, we model objects that are probably, but not\ncertainly, symmetric by predicting a symmetry probability map, learned\nend-to-end with the other components of the model. Our experiments show that\nthis method can recover very accurately the 3D shape of human faces, cat faces\nand cars from single-view images, without any supervision or a prior shape\nmodel. On benchmarks, we demonstrate superior accuracy compared to another\nmethod that uses supervision at the level of 2D image correspondences.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 18:56:12 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 03:57:25 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Wu", "Shangzhe", ""], ["Rupprecht", "Christian", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1911.11132", "submitter": "Dan Hendrycks", "authors": "Dan Hendrycks and Steven Basart and Mantas Mazeika and Mohammadreza\n  Mostajabi and Jacob Steinhardt and Dawn Song", "title": "Scaling Out-of-Distribution Detection for Real-World Settings", "comments": "StreetHazards dataset and code are available at\n  https://github.com/hendrycks/anomaly-seg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting out-of-distribution examples is important for safety-critical\nmachine learning applications such as medical screening and self-driving cars.\nHowever, existing research mainly focuses on simple small-scale settings. To\nset the stage for more realistic out-of-distribution detection, we depart from\nsmall-scale settings and explore large-scale multiclass and multi-label\nsettings with high-resolution images and hundreds of classes. To make future\nwork in real-world settings possible, we also create a new benchmark for\nanomaly segmentation by introducing the Combined Anomalous Object Segmentation\nbenchmark. Our novel benchmark combines two datasets for anomaly segmentation\nthat incorporate both realism and anomaly diversity. Using both real images and\nthose from a simulated driving environment, we ensure the background context\nand a wide variety of anomalous objects are naturally integrated, unlike\nbefore. We conduct extensive experiments in these more realistic settings for\nout-of-distribution detection and find that a surprisingly simple detector\nbased on the maximum logit outperforms prior methods in all the large-scale\nmulti-class, multi-label, and segmentation tasks we consider, establishing a\nnew baseline for future work. These results, along with our new anomaly\nsegmentation benchmark, open the door to future research in out-of-distribution\ndetection.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 18:58:23 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 07:24:14 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Hendrycks", "Dan", ""], ["Basart", "Steven", ""], ["Mazeika", "Mantas", ""], ["Mostajabi", "Mohammadreza", ""], ["Steinhardt", "Jacob", ""], ["Song", "Dawn", ""]]}, {"id": "1911.11134", "submitter": "Utku Evci", "authors": "Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, Erich Elsen", "title": "Rigging the Lottery: Making All Tickets Winners", "comments": "Published in Proceedings of the 37th International Conference on\n  Machine Learning. Code can be found in github.com/google-research/rigl", "journal-ref": "Proceedings of the 37th International Conference on Machine\n  Learning (2020) 471-481", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many applications require sparse neural networks due to space or inference\ntime restrictions. There is a large body of work on training dense networks to\nyield sparse networks for inference, but this limits the size of the largest\ntrainable sparse model to that of the largest trainable dense model. In this\npaper we introduce a method to train sparse neural networks with a fixed\nparameter count and a fixed computational cost throughout training, without\nsacrificing accuracy relative to existing dense-to-sparse training methods. Our\nmethod updates the topology of the sparse network during training by using\nparameter magnitudes and infrequent gradient calculations. We show that this\napproach requires fewer floating-point operations (FLOPs) to achieve a given\nlevel of accuracy compared to prior techniques. We demonstrate state-of-the-art\nsparse training results on a variety of networks and datasets, including\nResNet-50, MobileNets on Imagenet-2012, and RNNs on WikiText-103. Finally, we\nprovide some insights into why allowing the topology to change during the\noptimization can overcome local minima encountered when the topology remains\nstatic. Code used in our work can be found in github.com/google-research/rigl.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 18:58:53 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2020 20:13:36 GMT"}, {"version": "v3", "created": "Fri, 23 Jul 2021 14:12:42 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Evci", "Utku", ""], ["Gale", "Trevor", ""], ["Menick", "Jacob", ""], ["Castro", "Pablo Samuel", ""], ["Elsen", "Erich", ""]]}, {"id": "1911.11136", "submitter": "Chaowei Fang", "authors": "Chaowei Fang, Guanbin Li, Xiaoguang Han, Yizhou Yu", "title": "Self-Enhanced Convolutional Network for Facial Video Hallucination", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2955640", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a domain-specific super-resolution problem, facial image hallucination has\nenjoyed a series of breakthroughs thanks to the advances of deep convolutional\nneural networks. However, the direct migration of existing methods to video is\nstill difficult to achieve good performance due to its lack of alignment and\nconsistency modelling in temporal domain. Taking advantage of high inter-frame\ndependency in videos, we propose a self-enhanced convolutional network for\nfacial video hallucination. It is implemented by making full usage of preceding\nsuper-resolved frames and a temporal window of adjacent low-resolution frames.\nSpecifically, the algorithm first obtains the initial high-resolution inference\nof each frame by taking into consideration a sequence of consecutive\nlow-resolution inputs through temporal consistency modelling. It further\nrecurrently exploits the reconstructed results and intermediate features of a\nsequence of preceding frames to improve the initial super-resolution of the\ncurrent frame by modelling the coherence of structural facial features across\nframes. Quantitative and qualitative evaluations demonstrate the superiority of\nthe proposed algorithm against state-of-the-art methods. Moreover, our\nalgorithm also achieves excellent performance in the task of general video\nsuper-resolution in a single-shot setting.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 01:06:50 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Fang", "Chaowei", ""], ["Li", "Guanbin", ""], ["Han", "Xiaoguang", ""], ["Yu", "Yizhou", ""]]}, {"id": "1911.11170", "submitter": "Ilchae Jung", "authors": "Ilchae Jung, Kihyun You, Hyeonwoo Noh, Minsu Cho, Bohyung Han", "title": "Real-Time Object Tracking via Meta-Learning: Efficient Model Adaptation\n  and One-Shot Channel Pruning", "comments": "9 pages, 5 figures, AAAI 2020 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel meta-learning framework for real-time object tracking with\nefficient model adaptation and channel pruning. Given an object tracker, our\nframework learns to fine-tune its model parameters in only a few iterations of\ngradient-descent during tracking while pruning its network channels using the\ntarget ground-truth at the first frame. Such a learning problem is formulated\nas a meta-learning task, where a meta-tracker is trained by updating its\nmeta-parameters for initial weights, learning rates, and pruning masks through\ncarefully designed tracking simulations. The integrated meta-tracker greatly\nimproves tracking performance by accelerating the convergence of online\nlearning and reducing the cost of feature computation. Experimental evaluation\non the standard datasets demonstrates its outstanding accuracy and speed\ncompared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 19:09:01 GMT"}, {"version": "v2", "created": "Sun, 1 Dec 2019 10:17:53 GMT"}, {"version": "v3", "created": "Wed, 4 Dec 2019 08:38:46 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Jung", "Ilchae", ""], ["You", "Kihyun", ""], ["Noh", "Hyeonwoo", ""], ["Cho", "Minsu", ""], ["Han", "Bohyung", ""]]}, {"id": "1911.11177", "submitter": "Yair Movshovitz-Attias", "authors": "Elad Eban, Yair Movshovitz-Attias, Hao Wu, Mark Sandler, Andrew Poon,\n  Yerlan Idelbayev, Miguel A. Carreira-Perpinan", "title": "Structured Multi-Hashing for Model Compression", "comments": "Elad and Yair contributed equally to the paper. They jointly proposed\n  the idea of structured-multi-hashing. Elad: Wrote most of the code and ran\n  most of the experiments Yair: Main contributor to the manuscript Hao: Coding\n  and experiments Yerlan: Coding and experiments Miguel: advised Yerlan about\n  optimization and model compression Mark:experiments Andrew: experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of deep neural networks (DNNs), state-of-the-art models\nare too large to deploy on low-resource devices or common server configurations\nin which multiple models are held in memory. Model compression methods address\nthis limitation by reducing the memory footprint, latency, or energy\nconsumption of a model with minimal impact on accuracy. We focus on the task of\nreducing the number of learnable variables in the model. In this work we\ncombine ideas from weight hashing and dimensionality reductions resulting in a\nsimple and powerful structured multi-hashing method based on matrix products\nthat allows direct control of model size of any deep network and is trained\nend-to-end. We demonstrate the strength of our approach by compressing models\nfrom the ResNet, EfficientNet, and MobileNet architecture families. Our method\nallows us to drastically decrease the number of variables while maintaining\nhigh accuracy. For instance, by applying our approach to EfficentNet-B4 (16M\nparameters) we reduce it to to the size of B0 (5M parameters), while gaining\nover 3% in accuracy over B0 baseline. On the commonly used benchmark CIFAR10 we\nreduce the ResNet32 model by 75% with no loss in quality, and are able to do a\n10x compression while still achieving above 90% accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 19:21:25 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Eban", "Elad", ""], ["Movshovitz-Attias", "Yair", ""], ["Wu", "Hao", ""], ["Sandler", "Mark", ""], ["Poon", "Andrew", ""], ["Idelbayev", "Yerlan", ""], ["Carreira-Perpinan", "Miguel A.", ""]]}, {"id": "1911.11206", "submitter": "Dave Epstein", "authors": "Dave Epstein, Boyuan Chen, Carl Vondrick", "title": "Oops! Predicting Unintentional Action in Video", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From just a short glance at a video, we can often tell whether a person's\naction is intentional or not. Can we train a model to recognize this? We\nintroduce a dataset of in-the-wild videos of unintentional action, as well as a\nsuite of tasks for recognizing, localizing, and anticipating its onset. We\ntrain a supervised neural network as a baseline and analyze its performance\ncompared to human consistency on the tasks. We also investigate self-supervised\nrepresentations that leverage natural signals in our dataset, and show the\neffectiveness of an approach that uses the intrinsic speed of video to perform\ncompetitively with highly-supervised pretraining. However, a significant gap\nbetween machine and human performance remains. The project website is available\nat https://oops.cs.columbia.edu\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 20:15:31 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Epstein", "Dave", ""], ["Chen", "Boyuan", ""], ["Vondrick", "Carl", ""]]}, {"id": "1911.11209", "submitter": "Naofumi Tomita", "authors": "Naofumi Tomita, Steven Jiang, Matthew E. Maeder and Saeed Hassanpour", "title": "Automatic Post-Stroke Lesion Segmentation on MR Images using 3D Residual\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate the feasibility and performance of deep\nresidual neural networks for volumetric segmentation of irreversibly damaged\nbrain tissue lesions on T1-weighted MRI scans for chronic stroke patients. A\ntotal of 239 T1-weighted MRI scans of chronic ischemic stroke patients from a\npublic dataset were retrospectively analyzed by 3D deep convolutional\nsegmentation models with residual learning, using a novel zoom-in&out strategy.\nDice similarity coefficient (DSC), Average symmetric surface distance (ASSD),\nand Hausdorff distance (HD) of the identified lesions were measured by using\nthe manual tracing of lesions as the reference standard. Bootstrapping was\nemployed for all metrics to estimate 95% confidence intervals. The models were\nassessed on the test set of 31 scans. The average DSC was 0.64 (0.51-0.76) with\na median of 0.78. ASSD and HD were 3.6 mm (1.7-6.2 mm) and 20.4 mm (10.0-33.3\nmm), respectively. To the best of our knowledge, this performance is the\nhighest achieved on this public dataset. The latest deep learning architecture\nand techniques were applied for 3D segmentation on MRI scans and demonstrated\nto be effective for volumetric segmentation of chronic ischemic stroke lesions.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 20:20:58 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Tomita", "Naofumi", ""], ["Jiang", "Steven", ""], ["Maeder", "Matthew E.", ""], ["Hassanpour", "Saeed", ""]]}, {"id": "1911.11219", "submitter": "Chang Xiao", "authors": "Chang Xiao and Changxi Zheng", "title": "One Man's Trash is Another Man's Treasure: Resisting Adversarial\n  Examples by Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern image classification systems are often built on deep neural networks,\nwhich suffer from adversarial examples--images with deliberately crafted,\nimperceptible noise to mislead the network's classification. To defend against\nadversarial examples, a plausible idea is to obfuscate the network's gradient\nwith respect to the input image. This general idea has inspired a long line of\ndefense methods. Yet, almost all of them have proven vulnerable. We revisit\nthis seemingly flawed idea from a radically different perspective. We embrace\nthe omnipresence of adversarial examples and the numerical procedure of\ncrafting them, and turn this harmful attacking process into a useful defense\nmechanism. Our defense method is conceptually simple: before feeding an input\nimage for classification, transform it by finding an adversarial example on a\npre-trained external model. We evaluate our method against a wide range of\npossible attacks. On both CIFAR-10 and Tiny ImageNet datasets, our method is\nsignificantly more robust than state-of-the-art methods. Particularly, in\ncomparison to adversarial training, our method offers lower training cost as\nwell as stronger robustness.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 20:33:59 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 21:10:06 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Xiao", "Chang", ""], ["Zheng", "Changxi", ""]]}, {"id": "1911.11227", "submitter": "Jan Bedna\\v{r}\\'ik", "authors": "Jan Bednarik, Shaifali Parashar, Erhan Gundogdu, Mathieu Salzmann,\n  Pascal Fua", "title": "Shape Reconstruction by Learning Differentiable Surface Representations", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models that produce point clouds have emerged as a powerful tool\nto represent 3D surfaces, and the best current ones rely on learning an\nensemble of parametric representations. Unfortunately, they offer no control\nover the deformations of the surface patches that form the ensemble and thus\nfail to prevent them from either overlapping or collapsing into single points\nor lines. As a consequence, computing shape properties such as surface normals\nand curvatures becomes difficult and unreliable.\n  In this paper, we show that we can exploit the inherent differentiability of\ndeep networks to leverage differential surface properties during training so as\nto prevent patch collapse and strongly reduce patch overlap. Furthermore, this\nlets us reliably compute quantities such as surface normals and curvatures. We\nwill demonstrate on several tasks that this yields more accurate surface\nreconstructions than the state-of-the-art methods in terms of normals\nestimation and amount of collapsed and overlapped patches.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 20:51:18 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Bednarik", "Jan", ""], ["Parashar", "Shaifali", ""], ["Gundogdu", "Erhan", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""]]}, {"id": "1911.11230", "submitter": "Chenxi Liu", "authors": "Michelle Shu, Chenxi Liu, Weichao Qiu, Alan Yuille", "title": "Identifying Model Weakness with Adversarial Examiner", "comments": "To appear in AAAI-20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are usually evaluated according to the average case\nperformance on the test set. However, this is not always ideal, because in some\nsensitive domains (e.g. autonomous driving), it is the worst case performance\nthat matters more. In this paper, we are interested in systematic exploration\nof the input data space to identify the weakness of the model to be evaluated.\nWe propose to use an adversarial examiner in the testing stage. Different from\nthe existing strategy to always give the same (distribution of) test data, the\nadversarial examiner will dynamically select the next test data to hand out\nbased on the testing history so far, with the goal being to undermine the\nmodel's performance. This sequence of test data not only helps us understand\nthe current model, but also serves as constructive feedback to help improve the\nmodel in the next iteration. We conduct experiments on ShapeNet object\nclassification. We show that our adversarial examiner can successfully put more\nemphasis on the weakness of the model, preventing performance estimates from\nbeing overly optimistic.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 21:04:49 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Shu", "Michelle", ""], ["Liu", "Chenxi", ""], ["Qiu", "Weichao", ""], ["Yuille", "Alan", ""]]}, {"id": "1911.11236", "submitter": "Bo Yang", "authors": "Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua\n  Wang, Niki Trigoni, Andrew Markham", "title": "RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds", "comments": "CVPR 2020 Oral. Code and data are available at:\n  https://github.com/QingyongHu/RandLA-Net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of efficient semantic segmentation for large-scale 3D\npoint clouds. By relying on expensive sampling techniques or computationally\nheavy pre/post-processing steps, most existing approaches are only able to be\ntrained and operate over small-scale point clouds. In this paper, we introduce\nRandLA-Net, an efficient and lightweight neural architecture to directly infer\nper-point semantics for large-scale point clouds. The key to our approach is to\nuse random point sampling instead of more complex point selection approaches.\nAlthough remarkably computation and memory efficient, random sampling can\ndiscard key features by chance. To overcome this, we introduce a novel local\nfeature aggregation module to progressively increase the receptive field for\neach 3D point, thereby effectively preserving geometric details. Extensive\nexperiments show that our RandLA-Net can process 1 million points in a single\npass with up to 200X faster than existing approaches. Moreover, our RandLA-Net\nclearly surpasses state-of-the-art approaches for semantic segmentation on two\nlarge-scale benchmarks Semantic3D and SemanticKITTI.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 21:15:52 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 23:57:02 GMT"}, {"version": "v3", "created": "Fri, 1 May 2020 21:29:02 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Hu", "Qingyong", ""], ["Yang", "Bo", ""], ["Xie", "Linhai", ""], ["Rosa", "Stefano", ""], ["Guo", "Yulan", ""], ["Wang", "Zhihua", ""], ["Trigoni", "Niki", ""], ["Markham", "Andrew", ""]]}, {"id": "1911.11237", "submitter": "Didac Suris Coll-Vinent", "authors": "D\\'idac Sur\\'is, Dave Epstein, Heng Ji, Shih-Fu Chang, Carl Vondrick", "title": "Learning to Learn Words from Visual Scenes", "comments": "26 pages, 12 figures", "journal-ref": "European Conference on Computer Vision (ECCV), 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language acquisition is the process of learning words from the surrounding\nscene. We introduce a meta-learning framework that learns how to learn word\nrepresentations from unconstrained scenes. We leverage the natural\ncompositional structure of language to create training episodes that cause a\nmeta-learner to learn strong policies for language acquisition. Experiments on\ntwo datasets show that our approach is able to more rapidly acquire novel words\nas well as more robustly generalize to unseen compositions, significantly\noutperforming established baselines. A key advantage of our approach is that it\nis data efficient, allowing representations to be learned from scratch without\nlanguage pre-training. Visualizations and analysis suggest visual information\nhelps our approach learn a rich cross-modal representation from minimal\nexamples. Project webpage is available at https://expert.cs.columbia.edu/\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 21:19:31 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 21:19:04 GMT"}, {"version": "v3", "created": "Sun, 12 Jul 2020 21:19:49 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Sur\u00eds", "D\u00eddac", ""], ["Epstein", "Dave", ""], ["Ji", "Heng", ""], ["Chang", "Shih-Fu", ""], ["Vondrick", "Carl", ""]]}, {"id": "1911.11238", "submitter": "Ganesh Sundaramoorthi", "authors": "Ganesh Sundaramoorthi, Timothy E. Wang", "title": "Translation Insensitive CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem that state-of-the-art Convolution Neural Networks\n(CNN) classifiers are not invariant to small shifts. The problem can be solved\nby the removal of sub-sampling operations such as stride and max pooling, but\nat a cost of severely degraded training and test efficiency. We present a novel\nusage of Gaussian-Hermite basis to efficiently approximate arbitrary filters\nwithin the CNN framework to obtain translation invariance. This is shown to be\ninvariant to small shifts, and preserves the efficiency of training. Further,\nto improve efficiency in memory usage as well as computational speed, we show\nthat it is still possible to sub-sample with this approach and retain a weaker\nform of invariance that we call \\emph{translation insensitivity}, which leads\nto stability with respect to shifts. We prove these claims analytically and\nempirically. Our analytic methods further provide a framework for understanding\nany architecture in terms of translation insensitivity, and provide guiding\nprinciples for design.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 21:22:06 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Sundaramoorthi", "Ganesh", ""], ["Wang", "Timothy E.", ""]]}, {"id": "1911.11250", "submitter": "Tobias Schlosser", "authors": "Tobias Schlosser, Frederik Beuth, Michael Friedrich, and Danny Kowerko", "title": "A Novel Visual Fault Detection and Classification System for\n  Semiconductor Manufacturing Using Stacked Hybrid Convolutional Neural\n  Networks", "comments": "Accepted for: 2019 IEEE 24th International Conference on Emerging\n  Technologies and Factory Automation (ETFA); the latest versions of this\n  contribution cover minor typo corrections", "journal-ref": null, "doi": "10.1109/ETFA.2019.8869311", "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated visual inspection in the semiconductor industry aims to detect and\nclassify manufacturing defects utilizing modern image processing techniques.\nWhile an earliest possible detection of defect patterns allows quality control\nand automation of manufacturing chains, manufacturers benefit from an increased\nyield and reduced manufacturing costs. Since classical image processing systems\nare limited in their ability to detect novel defect patterns, and machine\nlearning approaches often involve a tremendous amount of computational effort,\nthis contribution introduces a novel deep neural network based hybrid approach.\nUnlike classical deep neural networks, a multi-stage system allows the\ndetection and classification of the finest structures in pixel size within\nhigh-resolution imagery. Consisting of stacked hybrid convolutional neural\nnetworks (SH-CNN) and inspired by current approaches of visual attention, the\nrealized system draws the focus over the level of detail from its structures to\nmore task-relevant areas of interest. The results of our test environment show\nthat the SH-CNN outperforms current approaches of learning-based automated\nvisual inspection, whereas a distinction depending on the level of detail\nenables the elimination of defect patterns in earlier stages of the\nmanufacturing process.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 21:58:28 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2019 16:48:41 GMT"}, {"version": "v3", "created": "Sun, 26 Jan 2020 20:00:12 GMT"}, {"version": "v4", "created": "Fri, 1 Jan 2021 23:30:21 GMT"}, {"version": "v5", "created": "Thu, 18 Mar 2021 23:22:01 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Schlosser", "Tobias", ""], ["Beuth", "Frederik", ""], ["Friedrich", "Michael", ""], ["Kowerko", "Danny", ""]]}, {"id": "1911.11251", "submitter": "Tobias Schlosser", "authors": "Tobias Schlosser, Michael Friedrich, and Danny Kowerko", "title": "Hexagonal Image Processing in the Context of Machine Learning:\n  Conception of a Biologically Inspired Hexagonal Deep Learning Framework", "comments": "Accepted for: 2019 18th IEEE International Conference on Machine\n  Learning and Applications (ICMLA); the latest versions of this contribution\n  cover minor typo corrections", "journal-ref": null, "doi": "10.1109/ICMLA.2019.00300", "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the human visual perception system, hexagonal image processing in\nthe context of machine learning deals with the development of image processing\nsystems that combine the advantages of evolutionary motivated structures based\non biological models. While conventional state-of-the-art image processing\nsystems of recording and output devices almost exclusively utilize square\narranged methods, their hexagonal counterparts offer a number of key advantages\nthat can benefit both researchers and users. This contribution serves as a\ngeneral application-oriented approach the synthesis of the therefore designed\nhexagonal image processing framework, called Hexnet, the processing steps of\nhexagonal image transformation, and dependent methods. The results of our\ncreated test environment show that the realized framework surpasses current\napproaches of hexagonal image processing systems, while hexagonal artificial\nneural networks can benefit from the implemented hexagonal architecture. As\nhexagonal lattice format based deep neural networks, also called H-DNN, can be\ncompared to their square counterparts by transforming classical square lattice\nbased data sets into their hexagonal representation, they can also result in a\nreduction of trainable parameters as well as result in increased training and\ntest rates.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 21:58:31 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2019 16:48:44 GMT"}, {"version": "v3", "created": "Sun, 26 Jan 2020 20:00:15 GMT"}, {"version": "v4", "created": "Tue, 24 Mar 2020 13:50:32 GMT"}, {"version": "v5", "created": "Wed, 25 Mar 2020 22:59:06 GMT"}, {"version": "v6", "created": "Fri, 1 Jan 2021 23:30:21 GMT"}, {"version": "v7", "created": "Thu, 18 Mar 2021 23:21:56 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Schlosser", "Tobias", ""], ["Friedrich", "Michael", ""], ["Kowerko", "Danny", ""]]}, {"id": "1911.11263", "submitter": "Wei Shen", "authors": "Hao Ding, Siyuan Qiao, Alan Yuille, Wei Shen", "title": "Deeply Shape-guided Cascade for Instance Segmentation", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key to a successful cascade architecture for precise instance\nsegmentation is to fully leverage the relationship between bounding box\ndetection and mask segmentation across multiple stages. Although modern\ninstance segmentation cascades achieve leading performance, they mainly make\nuse of a unidirectional relationship, i.e., mask segmentation can benefit from\niteratively refined bounding box detection. In this paper, we investigate an\nalternative direction, i.e., how to take the advantage of precise mask\nsegmentation for bounding box detection in a cascade architecture. We propose a\nDeeply Shape-guided Cascade (DSC) for instance segmentation, which iteratively\nimposes the shape guidances extracted from mask prediction at the previous\nstage on bounding box detection at current stage. It forms a bi-directional\nrelationship between the two tasks by introducing three key components: (1)\nInitial shape guidance: A mask-supervised Region Proposal Network (mPRN) with\nthe ability to generate class-agnostic masks; (2) Explicit shape guidance: A\nmask-guided region-of-interest (RoI) feature extractor, which employs mask\nsegmentation at previous stage to focus feature extraction at current stage\nwithin a region aligned well with the shape of the instance-of-interest rather\nthan a rectangular RoI; (3) Implicit shape guidance: A feature fusion operation\nwhich feeds intermediate mask features at previous stage to the bounding box\nhead at current stage. Experimental results show that DSC outperforms the\nstate-of-the-art instance segmentation cascade, Hybrid Task Cascade (HTC), by a\nlarge margin and achieves 51.8 box AP and 45.5 mask AP on COCO test-dev. The\ncode is released at: https://github.com/hding2455/DSC.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 22:44:46 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 03:40:58 GMT"}, {"version": "v3", "created": "Tue, 29 Dec 2020 02:56:04 GMT"}, {"version": "v4", "created": "Sat, 27 Mar 2021 08:24:03 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Ding", "Hao", ""], ["Qiao", "Siyuan", ""], ["Yuille", "Alan", ""], ["Shen", "Wei", ""]]}, {"id": "1911.11288", "submitter": "Sergey Zakharov", "authors": "Sergey Zakharov, Wadim Kehl, Arjun Bhargava, Adrien Gaidon", "title": "Autolabeling 3D Objects with Differentiable Rendering of SDF Shape\n  Priors", "comments": "CVPR 2020 (Oral). 8 pages + supplementary material. The first two\n  authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an automatic annotation pipeline to recover 9D cuboids and 3D\nshapes from pre-trained off-the-shelf 2D detectors and sparse LIDAR data. Our\nautolabeling method solves an ill-posed inverse problem by considering learned\nshape priors and optimizing geometric and physical parameters. To address this\nchallenging problem, we apply a novel differentiable shape renderer to signed\ndistance fields (SDF), leveraged together with normalized object coordinate\nspaces (NOCS). Initially trained on synthetic data to predict shape and\ncoordinates, our method uses these predictions for projective and geometric\nalignment over real samples. Moreover, we also propose a curriculum learning\nstrategy, iteratively retraining on samples of increasing difficulty in\nsubsequent self-improving annotation rounds. Our experiments on the KITTI3D\ndataset show that we can recover a substantial amount of accurate cuboids, and\nthat these autolabels can be used to train 3D vehicle detectors with\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 00:11:49 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 15:44:47 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Zakharov", "Sergey", ""], ["Kehl", "Wadim", ""], ["Bhargava", "Arjun", ""], ["Gaidon", "Adrien", ""]]}, {"id": "1911.11293", "submitter": "Terrell Mundhenk", "authors": "T. Nathan Mundhenk and Barry Y. Chen and Gerald Friedland", "title": "Efficient Saliency Maps for Explainable AI", "comments": "In submission to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.PF cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an explainable AI saliency map method for use with deep\nconvolutional neural networks (CNN) that is much more efficient than popular\nfine-resolution gradient methods. It is also quantitatively similar or better\nin accuracy. Our technique works by measuring information at the end of each\nnetwork scale which is then combined into a single saliency map. We describe\nhow saliency measures can be made more efficient by exploiting Saliency Map\nOrder Equivalence. We visualize individual scale/layer contributions by using a\nLayer Ordered Visualization of Information. This provides an interesting\ncomparison of scale information contributions within the network not provided\nby other saliency map methods. Using our method instead of Guided Backprop,\ncoarse-resolution class activation methods such as Grad-CAM and Grad-CAM++ seem\nto yield demonstrably superior results without sacrificing speed. This will\nmake fine-resolution saliency methods feasible on resource limited platforms\nsuch as robots, cell phones, low-cost industrial devices, astronomy and\nsatellite imagery.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 00:32:23 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 21:20:06 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Mundhenk", "T. Nathan", ""], ["Chen", "Barry Y.", ""], ["Friedland", "Gerald", ""]]}, {"id": "1911.11294", "submitter": "Jianwen Xie", "authors": "Jianwen Xie, Ruiqi Gao, Zilong Zheng, Song-Chun Zhu, Ying Nian Wu", "title": "Motion-Based Generator Model: Unsupervised Disentanglement of\n  Appearance, Trackable and Intrackable Motions in Dynamic Patterns", "comments": null, "journal-ref": "The Thirty-Fourth AAAI Conference on Artificial Intelligence 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic patterns are characterized by complex spatial and motion patterns.\nUnderstanding dynamic patterns requires a disentangled representational model\nthat separates the factorial components. A commonly used model for dynamic\npatterns is the state space model, where the state evolves over time according\nto a transition model and the state generates the observed image frames\naccording to an emission model. To model the motions explicitly, it is natural\nfor the model to be based on the motions or the displacement fields of the\npixels. Thus in the emission model, we let the hidden state generate the\ndisplacement field, which warps the trackable component in the previous image\nframe to generate the next frame while adding a simultaneously emitted residual\nimage to account for the change that cannot be explained by the deformation.\nThe warping of the previous image is about the trackable part of the change of\nimage frame, while the residual image is about the intrackable part of the\nimage. We use a maximum likelihood algorithm to learn the model that iterates\nbetween inferring latent noise vectors that drive the transition model and\nupdating the parameters given the inferred latent vectors. Meanwhile we adopt a\nregularization term to penalize the norms of the residual images to encourage\nthe model to explain the change of image frames by trackable motion. Unlike\nexisting methods on dynamic patterns, we learn our model in unsupervised\nsetting without ground truth displacement fields. In addition, our model\ndefines a notion of intrackability by the separation of warped component and\nresidual component in each image frame. We show that our method can synthesize\nrealistic dynamic pattern, and disentangling appearance, trackable and\nintrackable motions. The learned models are useful for motion transfer, and it\nis natural to adopt it to define and measure intrackability of a dynamic\npattern.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 00:32:34 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Xie", "Jianwen", ""], ["Gao", "Ruiqi", ""], ["Zheng", "Zilong", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1911.11306", "submitter": "Hyunjun Eun", "authors": "Hyunjun Eun, Sumin Lee, Jinyoung Moon, Jongyoul Park, Chanho Jung,\n  Changick Kim", "title": "SRG: Snippet Relatedness-based Temporal Action Proposal Generator", "comments": "To appear in TCSVT", "journal-ref": null, "doi": "10.1109/TCSVT.2019.2953187", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent temporal action proposal generation approaches have suggested\nintegrating segment- and snippet score-based methodologies to produce proposals\nwith high recall and accurate boundaries. In this paper, different from such a\nhybrid strategy, we focus on the potential of the snippet score-based approach.\nSpecifically, we propose a new snippet score-based method, named Snippet\nRelatedness-based Generator (SRG), with a novel concept of \"snippet\nrelatedness\". Snippet relatedness represents which snippets are related to a\nspecific action instance. To effectively learn this snippet relatedness, we\npresent \"pyramid non-local operations\" for locally and globally capturing\nlong-range dependencies among snippets. By employing these components, SRG\nfirst produces a 2D relatedness score map that enables the generation of\nvarious temporal intervals reliably covering most action instances with high\noverlap. Then, SRG evaluates the action confidence scores of these temporal\nintervals and refines their boundaries to obtain temporal action proposals. On\nTHUMOS-14 and ActivityNet-1.3 datasets, SRG outperforms state-of-the-art\nmethods for temporal action proposal generation. Furthermore, compared to\ncompeting proposal generators, SRG leads to significant improvements in\ntemporal action detection.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 01:59:54 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 06:05:01 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Eun", "Hyunjun", ""], ["Lee", "Sumin", ""], ["Moon", "Jinyoung", ""], ["Park", "Jongyoul", ""], ["Jung", "Chanho", ""], ["Kim", "Changick", ""]]}, {"id": "1911.11308", "submitter": "Runzhong Wang", "authors": "Runzhong Wang, Junchi Yan and Xiaokang Yang", "title": "Neural Graph Matching Network: Learning Lawler's Quadratic Assignment\n  Problem with Extension to Hypergraph and Multiple-graph Matching", "comments": "Accepted by TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph matching involves combinatorial optimization based on edge-to-edge\naffinity matrix, which can be generally formulated as Lawler's Quadratic\nAssignment Problem (QAP). This paper presents a QAP network directly learning\nwith the affinity matrix (equivalently the association graph) whereby the\nmatching problem is translated into a constrained vertex classification task.\nThe association graph is learned by an embedding network for vertex\nclassification, followed by Sinkhorn normalization and a cross-entropy loss for\nend-to-end learning. We further improve the embedding model on association\ngraph by introducing Sinkhorn based matching-aware constraint, as well as dummy\nnodes to deal with unequal sizes of graphs. To our best knowledge, this is one\nof the first network to directly learn with the general Lawler's QAP. In\ncontrast, recent deep matching methods focus on the learning of node/edge\nfeatures in two graphs respectively. We also show how to extend our network to\nhypergraph matching, and matching of multiple graphs. Experimental results on\nboth synthetic graphs and real-world images show its effectiveness. For pure\nQAP tasks on synthetic data and QAPLIB benchmark, our method can perform\ncompetitively and even surpass state-of-the-art graph matching and QAP solvers\nwith notable less time cost. We provide a project homepage at\nhttp://thinklab.sjtu.edu.cn/project/NGM/index.html.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 02:06:57 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 17:16:22 GMT"}, {"version": "v3", "created": "Thu, 6 May 2021 17:18:34 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Wang", "Runzhong", ""], ["Yan", "Junchi", ""], ["Yang", "Xiaokang", ""]]}, {"id": "1911.11312", "submitter": "Fangneng Zhan", "authors": "Fangneng Zhan, Changgong Zhang", "title": "Spatial-Aware GAN for Unsupervised Person Re-identification", "comments": "Accepted to ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent person re-identification research has achieved great success by\nlearning from a large number of labeled person images. On the other hand, the\nlearned models often experience significant performance drops when applied to\nimages collected in a different environment. Unsupervised domain adaptation\n(UDA) has been investigated to mitigate this constraint, but most existing\nsystems adapt images at pixel level only and ignore obvious discrepancies at\nspatial level. This paper presents an innovative UDA-based person\nre-identification network that is capable of adapting images at both spatial\nand pixel levels simultaneously. A novel disentangled cycle-consistency loss is\ndesigned which guides the learning of spatial-level and pixel-level adaptation\nin a collaborative manner. In addition, a novel multi-modal mechanism is\nincorporated which is capable of generating images of different geometry views\nand augmenting training images effectively. Extensive experiments over a number\nof public datasets show that the proposed UDA network achieves superior person\nre-identification performance as compared with the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 02:56:34 GMT"}, {"version": "v2", "created": "Sat, 9 Jan 2021 14:46:40 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Zhan", "Fangneng", ""], ["Zhang", "Changgong", ""]]}, {"id": "1911.11314", "submitter": "Wuyang Chen", "authors": "Ye Yuan, Wuyang Chen, Tianlong Chen, Yang Yang, Zhou Ren, Zhangyang\n  Wang and Gang Hua", "title": "Calibrated Domain-Invariant Learning for Highly Generalizable Large\n  Scale Re-Identification", "comments": "WACV 2020 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world applications, such as city-scale traffic monitoring and\ncontrol, requires large-scale re-identification. However, previous ReID methods\noften failed to address two limitations in existing ReID benchmarks, i.e., low\nspatiotemporal coverage and sample imbalance. Notwithstanding their\ndemonstrated success in every single benchmark, they have difficulties in\ngeneralizing to unseen environments. As a result, these methods are less\napplicable in a large-scale setting due to poor generalization. In seek for a\nhighly generalizable large-scale ReID method, we present an adversarial domain\ninvariant feature learning framework (ADIN) that explicitly learns to separate\nidentity-related features from challenging variations, where for the first time\n\"free\" annotations in ReID data such as video timestamp and camera index are\nutilized. Furthermore, we find that the imbalance of nuisance classes\njeopardizes the adversarial training, and for mitigation we propose a\ncalibrated adversarial loss that is attentive to nuisance distribution.\nExperiments on existing large-scale person vehicle ReID datasets demonstrate\nthat ADIN learns more robust and generalizable representations, as evidenced by\nits outstanding direct transfer performance across datasets, which is a\ncriterion that can better measure the generalizability of large-scale ReID\nmethods/\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 03:08:28 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 02:28:50 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Yuan", "Ye", ""], ["Chen", "Wuyang", ""], ["Chen", "Tianlong", ""], ["Yang", "Yang", ""], ["Ren", "Zhou", ""], ["Wang", "Zhangyang", ""], ["Hua", "Gang", ""]]}, {"id": "1911.11319", "submitter": "Yao Zhou", "authors": "Pingchuan Ma, Yao Zhou, Yu Lu, Wei Zhang", "title": "Learning Efficient Video Representation with Video Shuffle Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D CNN shows its strong ability in learning spatiotemporal representation in\nrecent video recognition tasks. However, inflating 2D convolution to 3D\ninevitably introduces additional computational costs, making it cumbersome in\npractical deployment. We consider whether there is a way to equip the\nconventional 2D convolution with temporal vision no requiring expanding its\nkernel. To this end, we propose the video shuffle, a parameter-free plug-in\ncomponent that efficiently reallocates the inputs of 2D convolution so that its\nreceptive field can be extended to the temporal dimension. In practical, video\nshuffle firstly divides each frame feature into multiple groups and then\naggregate the grouped features via temporal shuffle operation. This allows the\nfollowing 2D convolution aggregate the global spatiotemporal features. The\nproposed video shuffle can be flexibly inserted into popular 2D CNNs, forming\nthe Video Shuffle Networks (VSN). With a simple yet efficient implementation,\nVSN performs surprisingly well on temporal modeling benchmarks. In experiments,\nVSN not only gains non-trivial improvements on Kinetics and Moments in Time,\nbut also achieves state-of-the-art performance on Something-Something-V1,\nSomething-Something-V2 datasets.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 03:52:47 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Ma", "Pingchuan", ""], ["Zhou", "Yao", ""], ["Lu", "Yu", ""], ["Zhang", "Wei", ""]]}, {"id": "1911.11323", "submitter": "Jing Zhang", "authors": "Yang Wang and Yang Cao and Zheng-Jun Zha and Jing Zhang and Zhiwei\n  Xiong and Wei Zhang and Feng Wu", "title": "Progressive Retinex: Mutually Reinforced Illumination-Noise Perception\n  Network for Low Light Image Enhancement", "comments": "The 27th ACM International Conference on Multimedia (MM'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrast enhancement and noise removal are coupled problems for low-light\nimage enhancement. The existing Retinex based methods do not take the coupling\nrelation into consideration, resulting in under or over-smoothing of the\nenhanced images. To address this issue, this paper presents a novel progressive\nRetinex framework, in which illumination and noise of low-light image are\nperceived in a mutually reinforced manner, leading to noise reduction low-light\nenhancement results. Specifically, two fully pointwise convolutional neural\nnetworks are devised to model the statistical regularities of ambient light and\nimage noise respectively, and to leverage them as constraints to facilitate the\nmutual learning process. The proposed method not only suppresses the\ninterference caused by the ambiguity between tiny textures and image noises,\nbut also greatly improves the computational efficiency. Moreover, to solve the\nproblem of insufficient training data, we propose an image synthesis strategy\nbased on camera imaging model, which generates color images corrupted by\nillumination-dependent noises. Experimental results on both synthetic and real\nlow-light images demonstrate the superiority of our proposed approaches against\nthe State-Of-The-Art (SOTA) low-light enhancement methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 03:56:45 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Wang", "Yang", ""], ["Cao", "Yang", ""], ["Zha", "Zheng-Jun", ""], ["Zhang", "Jing", ""], ["Xiong", "Zhiwei", ""], ["Zhang", "Wei", ""], ["Wu", "Feng", ""]]}, {"id": "1911.11341", "submitter": "Quan Huu Cap", "authors": "Quan Huu Cap, Hiroki Tani, Hiroyuki Uga, Satoshi Kagiwada and Hitoshi\n  Iyatomi", "title": "Super-Resolution for Practical Automated Plant Disease Diagnosis System", "comments": "Published as a conference paper at CISS 2019, Baltimore, MD, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated plant diagnosis using images taken from a distance is often\ninsufficient in resolution and degrades diagnostic accuracy since the important\nexternal characteristics of symptoms are lost. In this paper, we first propose\nan effective pre-processing method for improving the performance of automated\nplant disease diagnosis systems using super-resolution techniques. We\ninvestigate the efficiency of two different super-resolution methods by\ncomparing the disease diagnostic performance on the practical original\nhigh-resolution, low-resolution, and super-resolved cucumber images. Our method\ngenerates super-resolved images that look very close to natural images with\n4$\\times$ upscaling factors and is capable of recovering the lost detailed\nsymptoms, largely boosting the diagnostic performance. Our model improves the\ndisease classification accuracy by 26.9% over the bicubic interpolation method\nof 65.6% and shows a small gap (3% lower) between the original result of 95.5%.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 05:03:03 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Cap", "Quan Huu", ""], ["Tani", "Hiroki", ""], ["Uga", "Hiroyuki", ""], ["Kagiwada", "Satoshi", ""], ["Iyatomi", "Hitoshi", ""]]}, {"id": "1911.11344", "submitter": "Bhavan Jasani", "authors": "Bhavan Jasani, Afshaan Mazagonwalla", "title": "Skeleton based Zero Shot Action Recognition in Joint Pose-Language\n  Semantic Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How does one represent an action? How does one describe an action that we\nhave never seen before? Such questions are addressed by the Zero Shot Learning\nparadigm, where a model is trained on only a subset of classes and is evaluated\non its ability to correctly classify an example from a class it has never seen\nbefore. In this work, we present a body pose based zero shot action recognition\nnetwork and demonstrate its performance on the NTU RGB-D dataset. Our model\nlearns to jointly encapsulate visual similarities based on pose features of the\naction performer as well as similarities in the natural language descriptions\nof the unseen action class names. We demonstrate how this pose-language\nsemantic space encodes knowledge which allows our model to correctly predict\nactions not seen during training.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 05:10:47 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Jasani", "Bhavan", ""], ["Mazagonwalla", "Afshaan", ""]]}, {"id": "1911.11351", "submitter": "Mingda Wu", "authors": "Mingda Wu, Di Huang, Yuanfang Guo, Yunhong Wang", "title": "Distraction-Aware Feature Learning for Human Attribute Recognition via\n  Coarse-to-Fine Attention Mechanism", "comments": "8 pages, 5 figures, accepted by AAAI-20 as an oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Human Attribute Recognition (HAR) has become a hot topic due to its\nscientific challenges and application potentials, where localizing attributes\nis a crucial stage but not well handled. In this paper, we propose a novel deep\nlearning approach to HAR, namely Distraction-aware HAR (Da-HAR). It enhances\ndeep CNN feature learning by improving attribute localization through a\ncoarse-to-fine attention mechanism. At the coarse step, a self-mask block is\nbuilt to roughly discriminate and reduce distractions, while at the fine step,\na masked attention branch is applied to further eliminate irrelevant regions.\nThanks to this mechanism, feature learning is more accurate, especially when\nheavy occlusions and complex backgrounds exist. Extensive experiments are\nconducted on the WIDER-Attribute and RAP databases, and state-of-the-art\nresults are achieved, demonstrating the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 05:49:52 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Wu", "Mingda", ""], ["Huang", "Di", ""], ["Guo", "Yuanfang", ""], ["Wang", "Yunhong", ""]]}, {"id": "1911.11357", "submitter": "Samaneh Azadi", "authors": "Samaneh Azadi, Michael Tschannen, Eric Tzeng, Sylvain Gelly, Trevor\n  Darrell, Mario Lucic", "title": "Semantic Bottleneck Scene Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coupling the high-fidelity generation capabilities of label-conditional image\nsynthesis methods with the flexibility of unconditional generative models, we\npropose a semantic bottleneck GAN model for unconditional synthesis of complex\nscenes. We assume pixel-wise segmentation labels are available during training\nand use them to learn the scene structure. During inference, our model first\nsynthesizes a realistic segmentation layout from scratch, then synthesizes a\nrealistic scene conditioned on that layout. For the former, we use an\nunconditional progressive segmentation generation network that captures the\ndistribution of realistic semantic scene layouts. For the latter, we use a\nconditional segmentation-to-image synthesis network that captures the\ndistribution of photo-realistic images conditioned on the semantic layout. When\ntrained end-to-end, the resulting model outperforms state-of-the-art generative\nmodels in unsupervised image synthesis on two challenging domains in terms of\nthe Frechet Inception Distance and user-study evaluations. Moreover, we\ndemonstrate the generated segmentation maps can be used as additional training\ndata to strongly improve recent segmentation-to-image synthesis networks.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 06:01:09 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Azadi", "Samaneh", ""], ["Tschannen", "Michael", ""], ["Tzeng", "Eric", ""], ["Gelly", "Sylvain", ""], ["Darrell", "Trevor", ""], ["Lucic", "Mario", ""]]}, {"id": "1911.11378", "submitter": "Manraj Singh Grover", "authors": "Osaid Rehman Nasir, Shailesh Kumar Jha, Manraj Singh Grover, Yi Yu,\n  Ajit Kumar, Rajiv Ratn Shah", "title": "Text2FaceGAN: Face Generation from Fine Grained Textual Descriptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.MM eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Powerful generative adversarial networks (GAN) have been developed to\nautomatically synthesize realistic images from text. However, most existing\ntasks are limited to generating simple images such as flowers from captions. In\nthis work, we extend this problem to the less addressed domain of face\ngeneration from fine-grained textual descriptions of face, e.g., \"A person has\ncurly hair, oval face, and mustache\". We are motivated by the potential of\nautomated face generation to impact and assist critical tasks such as criminal\nface reconstruction. Since current datasets for the task are either very small\nor do not contain captions, we generate captions for images in the CelebA\ndataset by creating an algorithm to automatically convert a list of attributes\nto a set of captions. We then model the highly multi-modal problem of text to\nface generation as learning the conditional distribution of faces (conditioned\non text) in same latent space. We utilize the current state-of-the-art GAN\n(DC-GAN with GAN-CLS loss) for learning conditional multi-modality. The\npresence of more fine-grained details and variable length of the captions makes\nthe problem easier for a user but more difficult to handle compared to the\nother text-to-image tasks. We flipped the labels for real and fake images and\nadded noise in discriminator. Generated images for diverse textual descriptions\nshow promising results. In the end, we show how the widely used inceptions\nscore is not a good metric to evaluate the performance of generative models\nused for synthesizing faces from text.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 07:37:47 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Nasir", "Osaid Rehman", ""], ["Jha", "Shailesh Kumar", ""], ["Grover", "Manraj Singh", ""], ["Yu", "Yi", ""], ["Kumar", "Ajit", ""], ["Shah", "Rajiv Ratn", ""]]}, {"id": "1911.11379", "submitter": "Abdolreza Rashno Dr.", "authors": "Sadegh Fadaei and Abdolreza Rashno and Elyas Rashno", "title": "Content-based image retrieval speedup", "comments": null, "journal-ref": "5th Conference on Signal Processing and Intelligent Systems\n  (ICSPIS2019)", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-based image retrieval (CBIR) is a task of retrieving images from\ntheir contents. Since retrieval process is a time-consuming task in large image\ndatabases, acceleration methods can be very useful. This paper presents a novel\nmethod to speed up CBIR systems. In the proposed method, first Zernike moments\nare extracted from query image and an interval is calculated for that query.\nImages in database which are out of the interval are ignored in retrieval\nprocess. Therefore, a database reduction occurs before retrieval which leads to\nspeed up. It is shown that in reduced database, relevant images to query image\nare preserved and irrelevant images are throwed away. Therefore, the proposed\nmethod speed up retrieval process and preserve CBIR accuracy, simultaneously.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 07:40:06 GMT"}, {"version": "v2", "created": "Sun, 22 Dec 2019 14:12:35 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Fadaei", "Sadegh", ""], ["Rashno", "Abdolreza", ""], ["Rashno", "Elyas", ""]]}, {"id": "1911.11384", "submitter": "Qiao Liu", "authors": "Qiao Liu, Xin Li, Zhenyu He, Nana Fan, Di Yuan, Wei Liu, Yonsheng\n  Liang", "title": "Multi-Task Driven Feature Models for Thermal Infrared Tracking", "comments": "Thirty-Fourth AAAI Conference on Artifical Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep Thermal InfraRed (TIR) trackers usually use the feature models\nof RGB trackers for representation. However, these feature models learned on\nRGB images are neither effective in representing TIR objects nor taking\nfine-grained TIR information into consideration. To this end, we develop a\nmulti-task framework to learn the TIR-specific discriminative features and\nfine-grained correlation features for TIR tracking. Specifically, we first use\nan auxiliary classification network to guide the generation of TIR-specific\ndiscriminative features for distinguishing the TIR objects belonging to\ndifferent classes. Second, we design a fine-grained aware module to capture\nmore subtle information for distinguishing the TIR objects belonging to the\nsame class. These two kinds of features complement each other and recognize TIR\nobjects in the levels of inter-class and intra-class respectively. These two\nfeature models are learned using a multi-task matching framework and are\njointly optimized on the TIR tracking task. In addition, we develop a\nlarge-scale TIR training dataset to train the network for adapting the model to\nthe TIR domain. Extensive experimental results on three benchmarks show that\nthe proposed algorithm achieves a relative gain of 10% over the baseline and\nperforms favorably against the state-of-the-art methods. Codes and the proposed\nTIR dataset are available at {https://github.com/QiaoLiuHit/MMNet}.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 07:50:37 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Liu", "Qiao", ""], ["Li", "Xin", ""], ["He", "Zhenyu", ""], ["Fan", "Nana", ""], ["Yuan", "Di", ""], ["Liu", "Wei", ""], ["Liang", "Yonsheng", ""]]}, {"id": "1911.11390", "submitter": "Van-Quang Nguyen", "authors": "Van-Quang Nguyen, Masanori Suganuma, and Takayuki Okatani", "title": "Efficient Attention Mechanism for Visual Dialog that can Handle All the\n  Interactions between Multiple Inputs", "comments": "Accepted to ECCV 2020, 14 pages. Slight change in title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been a primary concern in recent studies of vision and language tasks\nto design an effective attention mechanism dealing with interactions between\nthe two modalities. The Transformer has recently been extended and applied to\nseveral bi-modal tasks, yielding promising results. For visual dialog, it\nbecomes necessary to consider interactions between three or more inputs, i.e.,\nan image, a question, and a dialog history, or even its individual dialog\ncomponents. In this paper, we present a neural architecture named Light-weight\nTransformer for Many Inputs (LTMI) that can efficiently deal with all the\ninteractions between multiple such inputs in visual dialog. It has a block\nstructure similar to the Transformer and employs the same design of attention\ncomputation, whereas it has only a small number of parameters, yet has\nsufficient representational power for the purpose. Assuming a standard setting\nof visual dialog, a layer built upon the proposed attention block has less than\none-tenth of parameters as compared with its counterpart, a natural Transformer\nextension. The experimental results on the VisDial datasets validate the\neffectiveness of the proposed approach, showing improvements of the best NDCG\nscore on the VisDial v1.0 dataset from 57.59 to 60.92 with a single model, from\n64.47 to 66.53 with ensemble models, and even to 74.88 with additional\nfinetuning. Our implementation code is available at\nhttps://github.com/davidnvq/visdial.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 08:10:02 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 14:10:12 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Nguyen", "Van-Quang", ""], ["Suganuma", "Masanori", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1911.11393", "submitter": "Jin Xie", "authors": "Jin Xie, Longfei Wang, Paula Webster, Yang Yao, Jiayao Sun, Shuo Wang\n  and Huihui Zhou", "title": "A Two-stream End-to-End Deep Learning Network for Recognizing Atypical\n  Visual Attention in Autism Spectrum Disorder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye movements have been widely investigated to study the atypical visual\nattention in Autism Spectrum Disorder (ASD). The majority of these studies have\nbeen focused on limited eye movement features by statistical comparisons\nbetween ASD and Typically Developing (TD) groups, which make it difficult to\naccurately separate ASD from TD at the individual level. The deep learning\ntechnology has been highly successful in overcoming this issue by automatically\nextracting features important for classification through a data-driven learning\nprocess. However, there is still a lack of end-to-end deep learning framework\nfor recognition of abnormal attention in ASD. In this study, we developed a\nnovel two-stream deep learning network for this recognition based on 700 images\nand corresponding eye movement patterns of ASD and TD, and obtained an accuracy\nof 0.95, which was higher than the previous state-of-the-art. We next\ncharacterized contributions to the classification at the single image level and\nnon-linearly integration of this single image level information during the\nclassification. Moreover, we identified a group of pixel-level visual features\nwithin these images with greater impacts on the classification. Together, this\ntwo-stream deep learning network provides us a novel and powerful tool to\nrecognize and understand abnormal visual attention in ASD.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 08:19:47 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Xie", "Jin", ""], ["Wang", "Longfei", ""], ["Webster", "Paula", ""], ["Yao", "Yang", ""], ["Sun", "Jiayao", ""], ["Wang", "Shuo", ""], ["Zhou", "Huihui", ""]]}, {"id": "1911.11394", "submitter": "Yang Yang", "authors": "Yang Yang, Xiaojie Guo, Jiayi Ma, Lin Ma and Haibin Ling", "title": "LaFIn: Generative Landmark Guided Face Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is challenging to inpaint face images in the wild, due to the large\nvariation of appearance, such as different poses, expressions and occlusions. A\ngood inpainting algorithm should guarantee the realism of output, including the\ntopological structure among eyes, nose and mouth, as well as the attribute\nconsistency on pose, gender, ethnicity, expression, etc. This paper studies an\neffective deep learning based strategy to deal with these issues, which\ncomprises of a facial landmark predicting subnet and an image inpainting\nsubnet. Concretely, given partial observation, the landmark predictor aims to\nprovide the structural information (e.g. topological relationship and\nexpression) of incomplete faces, while the inpaintor is to generate plausible\nappearance (e.g. gender and ethnicity) conditioned on the predicted landmarks.\nExperiments on the CelebA-HQ and CelebA datasets are conducted to reveal the\nefficacy of our design and, to demonstrate its superiority over\nstate-of-the-art alternatives both qualitatively and quantitatively. In\naddition, we assume that high-quality completed faces together with their\nlandmarks can be utilized as augmented data to further improve the performance\nof (any) landmark predictor, which is corroborated by experimental results on\nthe 300W and WFLW datasets.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 08:20:50 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Yang", "Yang", ""], ["Guo", "Xiaojie", ""], ["Ma", "Jiayi", ""], ["Ma", "Lin", ""], ["Ling", "Haibin", ""]]}, {"id": "1911.11419", "submitter": "Kekai Sheng", "authors": "Kekai Sheng, Weiming Dong, Menglei Chai, Guohui Wang, Peng Zhou,\n  Feiyue Huang, Bao-Gang Hu, Rongrong Ji, Chongyang Ma", "title": "Revisiting Image Aesthetic Assessment via Self-Supervised Feature\n  Learning", "comments": "AAAI Conference on Artificial Intelligence, 2020, accepted", "journal-ref": "Proceedings of AAAI Conference on Articial Intelligence 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual aesthetic assessment has been an active research field for decades.\nAlthough latest methods have achieved promising performance on benchmark\ndatasets, they typically rely on a large number of manual annotations including\nboth aesthetic labels and related image attributes. In this paper, we revisit\nthe problem of image aesthetic assessment from the self-supervised feature\nlearning perspective. Our motivation is that a suitable feature representation\nfor image aesthetic assessment should be able to distinguish different\nexpert-designed image manipulations, which have close relationships with\nnegative aesthetic effects. To this end, we design two novel pretext tasks to\nidentify the types and parameters of editing operations applied to synthetic\ninstances. The features from our pretext tasks are then adapted for a one-layer\nlinear classifier to evaluate the performance in terms of binary aesthetic\nclassification. We conduct extensive quantitative experiments on three\nbenchmark datasets and demonstrate that our approach can faithfully extract\naesthetics-aware features and outperform alternative pretext schemes. Moreover,\nwe achieve comparable results to state-of-the-art supervised methods that use\n10 million labels from ImageNet.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 09:41:04 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Sheng", "Kekai", ""], ["Dong", "Weiming", ""], ["Chai", "Menglei", ""], ["Wang", "Guohui", ""], ["Zhou", "Peng", ""], ["Huang", "Feiyue", ""], ["Hu", "Bao-Gang", ""], ["Ji", "Rongrong", ""], ["Ma", "Chongyang", ""]]}, {"id": "1911.11431", "submitter": "Alma Eguizabal", "authors": "Alma Eguizabal, Peter J. Schreier, J\\\"urgen Schmidt", "title": "Procrustes registration of two-dimensional statistical shape models\n  without correspondences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical shape models are a useful tool in image processing and computer\nvision. A Procrustres registration of the contours of the same shape is\ntypically perform to align the training samples to learn the statistical shape\nmodel. A Procrustes registration between two contours with known\ncorrespondences is straightforward. However, these correspondences are not\ngenerally available. Manually placed landmarks are often used for\ncorrespondence in the design of statistical shape models. However, determining\nmanual landmarks on the contours is time-consuming and often error-prone. One\nsolution to simultaneously find correspondence and registration is the\nIterative Closest Point (ICP) algorithm. However, ICP requires an initial\nposition of the contours that is close to registration, and it is not robust\nagainst outliers. We propose a new strategy, based on Dynamic Time Warping,\nthat efficiently solves the Procrustes registration problem without\ncorrespondences. We study the registration performance in a collection of\ndifferent shape data sets and show that our technique outperforms competing\ntechniques based on the ICP approach. Our strategy is applied to an ensemble of\ncontours of the same shape as an extension of the generalized Procrustes\nanalysis accounting for a lack of correspondence.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 10:01:28 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 09:47:38 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Eguizabal", "Alma", ""], ["Schreier", "Peter J.", ""], ["Schmidt", "J\u00fcrgen", ""]]}, {"id": "1911.11433", "submitter": "Anush Sankaran", "authors": "Ameya Prabhu, Riddhiman Dasgupta, Anush Sankaran, Srikanth\n  Tamilselvam, Senthil Mani", "title": "\"You might also like this model\": Data Driven Approach for Recommending\n  Deep Learning Models for Unknown Image Datasets", "comments": "NeurIPS 2019, New in ML Group", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an unknown (new) classification dataset, choosing an appropriate deep\nlearning architecture is often a recursive, time-taking, and laborious process.\nIn this research, we propose a novel technique to recommend a suitable\narchitecture from a repository of known models. Further, we predict the\nperformance accuracy of the recommended architecture on the given unknown\ndataset, without the need for training the model. We propose a model encoder\napproach to learn a fixed length representation of deep learning architectures\nalong with its hyperparameters, in an unsupervised fashion. We manually curate\na repository of image datasets with corresponding known deep learning models\nand show that the predicted accuracy is a good estimator of the actual\naccuracy. We discuss the implications of the proposed approach for three\nbenchmark images datasets and also the challenges in using the approach for\ntext modality. To further increase the reproducibility of the proposed\napproach, the entire implementation is made publicly available along with the\ntrained models.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 10:01:35 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 20:45:57 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Prabhu", "Ameya", ""], ["Dasgupta", "Riddhiman", ""], ["Sankaran", "Anush", ""], ["Tamilselvam", "Srikanth", ""], ["Mani", "Senthil", ""]]}, {"id": "1911.11445", "submitter": "Jun Wei", "authors": "Jun Wei, Shuhui Wang, Qingming Huang", "title": "F3Net: Fusion, Feedback and Focus for Salient Object Detection", "comments": "Accepted by AAAI2020, https://github.com/weijun88/F3Net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of existing salient object detection models have achieved great progress\nby aggregating multi-level features extracted from convolutional neural\nnetworks. However, because of the different receptive fields of different\nconvolutional layers, there exists big differences between features generated\nby these layers. Common feature fusion strategies (addition or concatenation)\nignore these differences and may cause suboptimal solutions. In this paper, we\npropose the F3Net to solve above problem, which mainly consists of cross\nfeature module (CFM) and cascaded feedback decoder (CFD) trained by minimizing\na new pixel position aware loss (PPA). Specifically, CFM aims to selectively\naggregate multi-level features. Different from addition and concatenation, CFM\nadaptively selects complementary components from input features before fusion,\nwhich can effectively avoid introducing too much redundant information that may\ndestroy the original features. Besides, CFD adopts a multi-stage feedback\nmechanism, where features closed to supervision will be introduced to the\noutput of previous layers to supplement them and eliminate the differences\nbetween features. These refined features will go through multiple similar\niterations before generating the final saliency maps. Furthermore, different\nfrom binary cross entropy, the proposed PPA loss doesn't treat pixels equally,\nwhich can synthesize the local structure information of a pixel to guide the\nnetwork to focus more on local details. Hard pixels from boundaries or\nerror-prone parts will be given more attention to emphasize their importance.\nF3Net is able to segment salient object regions accurately and provide clear\nlocal details. Comprehensive experiments on five benchmark datasets demonstrate\nthat F3Net outperforms state-of-the-art approaches on six evaluation metrics.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 10:41:35 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Wei", "Jun", ""], ["Wang", "Shuhui", ""], ["Huang", "Qingming", ""]]}, {"id": "1911.11449", "submitter": "Ruiqi Lu", "authors": "Ruiqi Lu, Huimin Ma", "title": "Occluded Pedestrian Detection with Visible IoU and Box Sign Predictor", "comments": "The 26th IEEE International Conference on Image Processing (ICIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a robust classifier and an accurate box regressor are difficult for\noccluded pedestrian detection. Traditionally adopted Intersection over Union\n(IoU) measurement does not consider the occluded region of the object and leads\nto improper training samples. To address such issue, a modification called\nvisible IoU is proposed in this paper to explicitly incorporate the visible\nratio in selecting samples. Then a newly designed box sign predictor is placed\nin parallel with box regressor to separately predict the moving direction of\ntraining samples. It leads to higher localization accuracy by introducing sign\nprediction loss during training and sign refining in testing. Following these\nnovelties, we obtain state-of-the-art performance on CityPersons benchmark for\noccluded pedestrian detection.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 10:52:19 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Lu", "Ruiqi", ""], ["Ma", "Huimin", ""]]}, {"id": "1911.11462", "submitter": "Mengmeng Xu", "authors": "Mengmeng Xu, Chen Zhao, David S. Rojas, Ali Thabet and Bernard Ghanem", "title": "G-TAD: Sub-Graph Localization for Temporal Action Detection", "comments": "Accepted by CVPR2020. 8 pages, 9 figures, 2 pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal action detection is a fundamental yet challenging task in video\nunderstanding. Video context is a critical cue to effectively detect actions,\nbut current works mainly focus on temporal context, while neglecting semantic\ncontext as well as other important context properties. In this work, we propose\na graph convolutional network (GCN) model to adaptively incorporate multi-level\nsemantic context into video features and cast temporal action detection as a\nsub-graph localization problem. Specifically, we formulate video snippets as\ngraph nodes, snippet-snippet correlations as edges, and actions associated with\ncontext as target sub-graphs. With graph convolution as the basic operation, we\ndesign a GCN block called GCNeXt, which learns the features of each node by\naggregating its context and dynamically updates the edges in the graph. To\nlocalize each sub-graph, we also design an SGAlign layer to embed each\nsub-graph into the Euclidean space. Extensive experiments show that G-TAD is\ncapable of finding effective video context without extra supervision and\nachieves state-of-the-art performance on two detection benchmarks. On\nActivityNet-1.3, it obtains an average mAP of 34.09%; on THUMOS14, it reaches\n51.6% at IoU@0.5 when combined with a proposal processing method. G-TAD code is\npublicly available at https://github.com/frostinassiky/gtad.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 11:27:09 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 21:48:21 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Xu", "Mengmeng", ""], ["Zhao", "Chen", ""], ["Rojas", "David S.", ""], ["Thabet", "Ali", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1911.11484", "submitter": "Weizhe Liu", "authors": "Weizhe Liu, Mathieu Salzmann, Pascal Fua", "title": "Using Depth for Pixel-Wise Detection of Adversarial Attacks in Crowd\n  Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art methods for counting people in crowded scenes rely on deep\nnetworks to estimate crowd density. While effective, deep learning approaches\nare vulnerable to adversarial attacks, which, in a crowd-counting context, can\nlead to serious security issues. However, attack and defense mechanisms have\nbeen virtually unexplored in regression tasks, let alone for crowd density\nestimation.\n  In this paper, we investigate the effectiveness of existing attack strategies\non crowd-counting networks, and introduce a simple yet effective pixel-wise\ndetection mechanism. It builds on the intuition that, when attacking a\nmultitask network, in our case estimating crowd density and scene depth, both\noutputs will be perturbed, and thus the second one can be used for detection\npurposes. We will demonstrate that this significantly outperforms heuristic and\nuncertainty-based strategies.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 12:12:34 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 07:20:12 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Liu", "Weizhe", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""]]}, {"id": "1911.11502", "submitter": "Ya Zhao", "authors": "Ya Zhao, Rui Xu, Xinchao Wang, Peng Hou, Haihong Tang, Mingli Song", "title": "Hearing Lips: Improving Lip Reading by Distilling Speech Recognizers", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lip reading has witnessed unparalleled development in recent years thanks to\ndeep learning and the availability of large-scale datasets. Despite the\nencouraging results achieved, the performance of lip reading, unfortunately,\nremains inferior to the one of its counterpart speech recognition, due to the\nambiguous nature of its actuations that makes it challenging to extract\ndiscriminant features from the lip movement videos. In this paper, we propose a\nnew method, termed as Lip by Speech (LIBS), of which the goal is to strengthen\nlip reading by learning from speech recognizers. The rationale behind our\napproach is that the features extracted from speech recognizers may provide\ncomplementary and discriminant clues, which are formidable to be obtained from\nthe subtle movements of the lips, and consequently facilitate the training of\nlip readers. This is achieved, specifically, by distilling multi-granularity\nknowledge from speech recognizers to lip readers. To conduct this cross-modal\nknowledge distillation, we utilize an efficacious alignment scheme to handle\nthe inconsistent lengths of the audios and videos, as well as an innovative\nfiltering strategy to refine the speech recognizer's prediction. The proposed\nmethod achieves the new state-of-the-art performance on the CMLR and LRS2\ndatasets, outperforming the baseline by a margin of 7.66% and 2.75% in\ncharacter error rate, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 13:05:07 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Zhao", "Ya", ""], ["Xu", "Rui", ""], ["Wang", "Xinchao", ""], ["Hou", "Peng", ""], ["Tang", "Haihong", ""], ["Song", "Mingli", ""]]}, {"id": "1911.11512", "submitter": "Sheng Yi", "authors": "Sheng Yi and Xi Li and Huimin Ma", "title": "WSOD with PSNet and Box Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised object detection(WSOD) task uses only image-level\nannotations to train object detection task. WSOD does not require\ntime-consuming instance-level annotations, so the study of this task has\nattracted more and more attention. Previous weakly supervised object detection\nmethods iteratively update detectors and pseudo-labels, or use feature-based\nmask-out methods. Most of these methods do not generate complete and accurate\nproposals, often only the most discriminative parts of the object, or too many\nbackground areas. To solve this problem, we added the box regression module to\nthe weakly supervised object detection network and proposed a proposal scoring\nnetwork (PSNet) to supervise it. The box regression module modifies proposal to\nimprove the IoU of proposal and ground truth. PSNet scores the proposal output\nfrom the box regression network and utilize the score to improve the box\nregression module. In addition, we take advantage of the PRS algorithm for\ngenerating a more accurate pseudo label to train the box regression module.\nUsing these methods, we train the detector on the PASCAL VOC 2007 and 2012 and\nobtain significantly improved results.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 13:20:54 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Yi", "Sheng", ""], ["Li", "Xi", ""], ["Ma", "Huimin", ""]]}, {"id": "1911.11530", "submitter": "Zhang Chen", "authors": "Zhang Chen, Anpei Chen, Guli Zhang, Chengyuan Wang, Yu Ji, Kiriakos N.\n  Kutulakos, Jingyi Yu", "title": "A Neural Rendering Framework for Free-Viewpoint Relighting", "comments": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2020", "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel Relightable Neural Renderer (RNR) for simultaneous view\nsynthesis and relighting using multi-view image inputs. Existing neural\nrendering (NR) does not explicitly model the physical rendering process and\nhence has limited capabilities on relighting. RNR instead models image\nformation in terms of environment lighting, object intrinsic attributes, and\nlight transport function (LTF), each corresponding to a learnable component. In\nparticular, the incorporation of a physically based rendering process not only\nenables relighting but also improves the quality of view synthesis.\nComprehensive experiments on synthetic and real data show that RNR provides a\npractical and effective solution for conducting free-viewpoint relighting.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 13:46:16 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 15:55:08 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Chen", "Zhang", ""], ["Chen", "Anpei", ""], ["Zhang", "Guli", ""], ["Wang", "Chengyuan", ""], ["Ji", "Yu", ""], ["Kutulakos", "Kiriakos N.", ""], ["Yu", "Jingyi", ""]]}, {"id": "1911.11534", "submitter": "Yixin Zhuang", "authors": "Siyan Dong, Songyin Wu, Yixin Zhuang, Kai Xu, Shanghang Zhang, Baoquan\n  Chen", "title": "Decoupling Features and Coordinates for Few-shot RGB Relocalization", "comments": "Siyan Dong and Songyin Wu contributed equally to this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-scene model adaption is crucial for camera relocalization in real\nscenarios. It is often preferable that a pre-learned model can be fast adapted\nto a novel scene with as few training samples as possible. The existing\nstate-of-the-art approaches, however, can hardly support such few-shot scene\nadaption due to the entangling of image feature extraction and scene coordinate\nregression. To address this issue, we approach camera relocalization with a\ndecoupled solution where feature extraction, coordinate regression, and pose\nestimation are performed separately. Our key insight is that feature encoder\nused for coordinate regression should be learned by removing the distracting\nfactor of coordinate systems, such that feature encoder is learned from\nmultiple scenes for general feature representation and more important,\nview-insensitive capability. With this feature prior, and combined with a\ncoordinate regressor, few-shot observations in a new scene are much easier to\nconnect with the 3D world than the one with existing integrated solution.\nExperiments have shown the superiority of our approach compared to the\nstate-of-the-art methods, producing higher accuracy on several scenes with\ndiverse visual appearance and viewpoint distribution.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 13:57:39 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 17:49:36 GMT"}, {"version": "v3", "created": "Tue, 4 Aug 2020 10:29:36 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Dong", "Siyan", ""], ["Wu", "Songyin", ""], ["Zhuang", "Yixin", ""], ["Xu", "Kai", ""], ["Zhang", "Shanghang", ""], ["Chen", "Baoquan", ""]]}, {"id": "1911.11544", "submitter": "Rameen Abdal", "authors": "Rameen Abdal, Yipeng Qin, Peter Wonka", "title": "Image2StyleGAN++: How to Edit the Embedded Images?", "comments": "CVPR 2020 \" For the video, visit https://youtu.be/yd5WczbFt68 \"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose Image2StyleGAN++, a flexible image editing framework with many\napplications. Our framework extends the recent Image2StyleGAN in three ways.\nFirst, we introduce noise optimization as a complement to the $W^+$ latent\nspace embedding. Our noise optimization can restore high-frequency features in\nimages and thus significantly improves the quality of reconstructed images,\ne.g. a big increase of PSNR from 20 dB to 45 dB. Second, we extend the global\n$W^+$ latent space embedding to enable local embeddings. Third, we combine\nembedding with activation tensor manipulation to perform high-quality local\nedits along with global semantic edits on images. Such edits motivate various\nhigh-quality image editing applications, e.g. image reconstruction, image\ninpainting, image crossover, local style transfer, image editing using\nscribbles, and attribute level feature transfer. Examples of the edited images\nare shown across the paper for visual inspection.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 14:08:28 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 00:38:59 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Abdal", "Rameen", ""], ["Qin", "Yipeng", ""], ["Wonka", "Peter", ""]]}, {"id": "1911.11554", "submitter": "Sicheng Zhao", "authors": "Sicheng Zhao, Guangzhi Wang, Shanghang Zhang, Yang Gu, Yaxian Li,\n  Zhichao Song, Pengfei Xu, Runbo Hu, Hua Chai, Kurt Keutzer", "title": "Multi-source Distilling Domain Adaptation", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks suffer from performance decay when there is domain shift\nbetween the labeled source domain and unlabeled target domain, which motivates\nthe research on domain adaptation (DA). Conventional DA methods usually assume\nthat the labeled data is sampled from a single source distribution. However, in\npractice, labeled data may be collected from multiple sources, while naive\napplication of the single-source DA algorithms may lead to suboptimal\nsolutions. In this paper, we propose a novel multi-source distilling domain\nadaptation (MDDA) network, which not only considers the different distances\namong multiple sources and the target, but also investigates the different\nsimilarities of the source samples to the target ones. Specifically, the\nproposed MDDA includes four stages: (1) pre-train the source classifiers\nseparately using the training data from each source; (2) adversarially map the\ntarget into the feature space of each source respectively by minimizing the\nempirical Wasserstein distance between source and target; (3) select the source\ntraining samples that are closer to the target to fine-tune the source\nclassifiers; and (4) classify each encoded target feature by corresponding\nsource classifier, and aggregate different predictions using respective domain\nweight, which corresponds to the discrepancy between each source and target.\nExtensive experiments are conducted on public DA benchmarks, and the results\ndemonstrate that the proposed MDDA significantly outperforms the\nstate-of-the-art approaches. Our source code is released at:\nhttps://github.com/daoyuan98/MDDA.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 19:30:15 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 18:21:22 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Zhao", "Sicheng", ""], ["Wang", "Guangzhi", ""], ["Zhang", "Shanghang", ""], ["Gu", "Yang", ""], ["Li", "Yaxian", ""], ["Song", "Zhichao", ""], ["Xu", "Pengfei", ""], ["Hu", "Runbo", ""], ["Chai", "Hua", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1911.11582", "submitter": "Panhe Feng", "authors": "Panhe Feng, Xuejing Kang, Lizhu Ye, Lei Zhu, Chunpeng Li, Anlong Ming", "title": "DDNet: Dual-path Decoder Network for Occlusion Relationship Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occlusion relationship reasoning based on convolution neural networks\nconsists of two subtasks: occlusion boundary extraction and occlusion\norientation inference. Due to the essential differences between the two\nsubtasks in the feature expression at the higher and lower stages, it is\nchallenging to carry on them simultaneously in one network. To address this\nissue, we propose a novel Dual-path Decoder Network, which uniformly extracts\nocclusion information at higher stages and separates into two paths to recover\nboundary and occlusion orientation respectively in lower stages. Besides,\nconsidering the restriction of occlusion orientation presentation to occlusion\norientation learning, we design a new orthogonal representation for occlusion\norientation and proposed the Orthogonal Orientation Regression loss which can\nget rid of the unfitness between occlusion representation and learning and\nfurther prompt the occlusion orientation learning. Finally, we apply a\nmulti-scale loss together with our proposed orientation regression loss to\nguide the boundary and orientation path learning respectively. Experiments\ndemonstrate that our proposed method achieves state-of-the-art results on PIOD\nand BSDS ownership datasets.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 14:47:28 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Feng", "Panhe", ""], ["Kang", "Xuejing", ""], ["Ye", "Lizhu", ""], ["Zhu", "Lei", ""], ["Li", "Chunpeng", ""], ["Ming", "Anlong", ""]]}, {"id": "1911.11612", "submitter": "Mahdi Kalayeh", "authors": "Mahdi M. Kalayeh and Mubarak Shah", "title": "On Symbiosis of Attribute Prediction and Semantic Segmentation", "comments": "Accepted for publication in PAMI. arXiv admin note: substantial text\n  overlap with arXiv:1704.08740", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to employ semantic segmentation to improve\nperson-related attribute prediction. The core idea lies in the fact that the\nprobability of an attribute to appear in an image is far from being uniform in\nthe spatial domain. We build our attribute prediction model jointly with a deep\nsemantic segmentation network. This harnesses the localization cues learned by\nthe semantic segmentation to guide the attention of the attribute prediction to\nthe regions where different attributes naturally show up. Therefore, in\naddition to prediction, we are able to localize the attributes despite merely\nhaving access to image-level labels (weak supervision) during training. We\nfirst propose semantic segmentation-based pooling and gating, respectively\ndenoted as SSP and SSG. In the former, the estimated segmentation masks are\nused to pool the final activations of the attribute prediction network, from\nmultiple semantically homogeneous regions. In SSG, the same idea is applied to\nthe intermediate layers of the network. SSP and SSG, while effective, impose\nheavy memory utilization since each channel of the activations is pooled/gated\nwith all the semantic segmentation masks. To circumvent this, we propose\nSymbiotic Augmentation (SA), where we learn only one mask per activation\nchannel. SA allows the model to either pick one, or combine (weighted\nsuperposition) multiple semantic maps, in order to generate the proper mask for\neach channel. SA simultaneously applies the same mechanism to the reverse\nproblem by leveraging output logits of attribute prediction to guide the\nsemantic segmentation task. We evaluate our proposed methods for facial\nattributes on CelebA and LFWA datasets, while benchmarking WIDER Attribute and\nBerkeley Attributes of People for whole body attributes. Our proposed methods\nachieve superior results compared to the previous works.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 20:51:05 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Kalayeh", "Mahdi M.", ""], ["Shah", "Mubarak", ""]]}, {"id": "1911.11616", "submitter": "Yantao Lu", "authors": "Yantao Lu, Yunhan Jia, Jianyu Wang, Bai Li, Weiheng Chai, Lawrence\n  Carin, Senem Velipasalar", "title": "Enhancing Cross-task Black-Box Transferability of Adversarial Examples\n  with Dispersion Reduction", "comments": "arXiv admin note: substantial text overlap with arXiv:1905.03333", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are known to be vulnerable to carefully crafted adversarial\nexamples, and these malicious samples often transfer, i.e., they remain\nadversarial even against other models. Although great efforts have been delved\ninto the transferability across models, surprisingly, less attention has been\npaid to the cross-task transferability, which represents the real-world\ncybercriminal's situation, where an ensemble of different defense/detection\nmechanisms need to be evaded all at once. In this paper, we investigate the\ntransferability of adversarial examples across a wide range of real-world\ncomputer vision tasks, including image classification, object detection,\nsemantic segmentation, explicit content detection, and text detection. Our\nproposed attack minimizes the ``dispersion'' of the internal feature map, which\novercomes existing attacks' limitation of requiring task-specific loss\nfunctions and/or probing a target model. We conduct evaluation on open source\ndetection and segmentation models as well as four different computer vision\ntasks provided by Google Cloud Vision (GCV) APIs, to show how our approach\noutperforms existing attacks by degrading performance of multiple CV tasks by a\nlarge margin with only modest perturbations linf=16.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 23:08:17 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Lu", "Yantao", ""], ["Jia", "Yunhan", ""], ["Wang", "Jianyu", ""], ["Li", "Bai", ""], ["Chai", "Weiheng", ""], ["Carin", "Lawrence", ""], ["Velipasalar", "Senem", ""]]}, {"id": "1911.11619", "submitter": "In Kyu Park", "authors": "Andre Ivan, Williem, In Kyu Park", "title": "Joint Spatial and Angular Super-Resolution from a Single Image", "comments": "arXiv admin note: substantial text overlap with arXiv:1903.12364", "journal-ref": "IEEE Access, vol. 8, June 2020 page(s): 112562-112573", "doi": "10.1109/ACCESS.2020.3002921", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Synthesizing a densely sampled light field from a single image is highly\nbeneficial for many applications. Moreover, jointly solving both angular and\nspatial super-resolution problem also introduces new possibilities in light\nfield imaging. The conventional method relies on physical-based rendering and a\nsecondary network to solve the angular super-resolution problem. In addition,\npixel-based loss limits the network capability to infer scene geometry\nglobally. In this paper, we show that both super-resolution problems can be\nsolved jointly from a single image by proposing a single end-to-end deep neural\nnetwork that does not require a physical-based approach. Two novel loss\nfunctions based on known light field domain knowledge are proposed to enable\nthe network to preserve the spatio-angular consistency between sub-aperture\nimages. Experimental results show that the proposed model successfully\nsynthesizes dense high resolution light field and it outperforms the\nstate-of-the-art method in both quantitative and qualitative criteria. The\nmethod can be generalized to arbitrary scenes, rather than focusing on a\nparticular subject. The synthesized light field can be used for various\napplications, such as depth estimation and refocusing.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 02:35:10 GMT"}, {"version": "v2", "created": "Fri, 29 Nov 2019 08:38:47 GMT"}, {"version": "v3", "created": "Sat, 27 Jun 2020 07:10:14 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Ivan", "Andre", ""], ["Williem", "", ""], ["Park", "In Kyu", ""]]}, {"id": "1911.11620", "submitter": "Jonathan Connell", "authors": "Jonathan Connell", "title": "Teaching Perception", "comments": "arXiv admin note: text overlap with arXiv:1911.09782", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visual world is very rich and generally too complex to perceive in its\nentirety. Yet only certain features are typically required to adequately\nperform some task in a given situation. Rather than hardwire-in decisions about\nwhen and what to sense, this paper describes a robotic system whose behavioral\npolicy can be set by verbal instructions it receives. These capabilities are\ndemonstrated in an associated video showing the fully implemented system\nguiding the perception of a physical robot in simple scenario. The structure\nand functioning of the underlying natural language based symbolic reasoning\nsystem is also discussed.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 23:46:37 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Connell", "Jonathan", ""]]}, {"id": "1911.11634", "submitter": "Chunlei Liu", "authors": "Chunlei Liu, Wenrui Ding, Yuan Hu, Baochang Zhang, Jianzhuang Liu,\n  Guodong Guo", "title": "GBCNs: Genetic Binary Convolutional Networks for Enhancing the\n  Performance of 1-bit DCNNs", "comments": "We withdraw it for error in Fig. 4. Yuan Hu, who is supervised by\n  Chunlei Liu, forgot to set \"tensorboard\" for exp. Chunlei knew this and they\n  thought ok regress several recorded real points on old data for Fig. 4. Max.\n  accuracy is right. But made error due to insufficient communication. Hu and\n  Liu are responsible for it. Other authors were not told details. We will\n  correct code github.com/liuchunlei0430", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training 1-bit deep convolutional neural networks (DCNNs) is one of the most\nchallenging problems in computer vision, because it is much easier to get\ntrapped into local minima than conventional DCNNs. The reason lies in that the\nbinarized kernels and activations of 1-bit DCNNs cause a significant accuracy\nloss and training inefficiency. To address this problem, we propose Genetic\nBinary Convolutional Networks (GBCNs) to optimize 1-bit DCNNs, by introducing a\nnew balanced Genetic Algorithm (BGA) to improve the representational ability in\nan end-to-end framework. The BGA method is proposed to modify the binary\nprocess of GBCNs to alleviate the local minima problem, which can significantly\nimprove the performance of 1-bit DCNNs. We develop a new BGA module that is\ngeneric and flexible, and can be easily incorporated into existing DCNNs, such\nasWideResNets and ResNets. Extensive experiments on the object classification\ntasks (CIFAR, ImageNet) validate the effectiveness of the proposed method. To\nhighlight, our method shows strong generalization on the object recognition\ntask, i.e., face recognition, facial and person re-identification.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 06:44:20 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 17:20:50 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Liu", "Chunlei", ""], ["Ding", "Wenrui", ""], ["Hu", "Yuan", ""], ["Zhang", "Baochang", ""], ["Liu", "Jianzhuang", ""], ["Guo", "Guodong", ""]]}, {"id": "1911.11680", "submitter": "Xi Yin", "authors": "Xi Yin, Ying Tai, Yuge Huang, Xiaoming Liu", "title": "FAN: Feature Adaptation Network for Surveillance Face Recognition and\n  Normalization", "comments": null, "journal-ref": "ACCV2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies face recognition (FR) and normalization in surveillance\nimagery. Surveillance FR is a challenging problem that has great values in law\nenforcement. Despite recent progress in conventional FR, less effort has been\ndevoted to surveillance FR. To bridge this gap, we propose a Feature Adaptation\nNetwork (FAN) to jointly perform surveillance FR and normalization. Our face\nnormalization mainly acts on the aspect of image resolution, closely related to\nface super-resolution. However, previous face super-resolution methods require\npaired training data with pixel-to-pixel correspondence, which is typically\nunavailable between real-world low-resolution and high-resolution faces. FAN\ncan leverage both paired and unpaired data as we disentangle the features into\nidentity and non-identity components and adapt the distribution of the identity\nfeatures, which breaks the limit of current face super-resolution methods. We\nfurther propose a random scale augmentation scheme to learn resolution robust\nidentity features, with advantages over previous fixed scale augmentation.\nExtensive experiments on LFW, WIDER FACE, QUML-SurvFace and SCface datasets\nhave shown the effectiveness of our method on surveillance FR and\nnormalization.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 16:29:27 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 00:32:02 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Yin", "Xi", ""], ["Tai", "Ying", ""], ["Huang", "Yuge", ""], ["Liu", "Xiaoming", ""]]}, {"id": "1911.11686", "submitter": "Ying Huang", "authors": "Ying Huang, Jiankai Zhuang, Zengchang Qin", "title": "Multi-Level Network for High-Speed Multi-Person Pose Estimation", "comments": "5 pages, published at ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In multi-person pose estimation, the left/right joint type discrimination is\nalways a hard problem because of the similar appearance. Traditionally, we\nsolve this problem by stacking multiple refinement modules to increase\nnetwork's receptive fields and capture more global context, which can also\nincrease a great amount of computation. In this paper, we propose a Multi-level\nNetwork (MLN) that learns to aggregate features from lower-level (left/right\ninformation), upper-level (localization information), joint-limb level\n(complementary information) and global-level (context) information for\ndiscrimination of joint type. Through feature reuse and its intra-relation, MLN\ncan attain comparable performance to other conventional methods while runtime\nspeed retains at 42.2 FPS.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 16:42:46 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Huang", "Ying", ""], ["Zhuang", "Jiankai", ""], ["Qin", "Zengchang", ""]]}, {"id": "1911.11702", "submitter": "Miguel Fabian Romero Rondon", "authors": "Miguel Fabian Romero Rondon, Lucile Sassatelli, Ramon Aparicio Pardo,\n  Frederic Precioso", "title": "Revisiting Deep Architectures for Head Motion Prediction in 360{\\deg}\n  Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider predicting the user's head motion in 360-degree videos, with 2\nmodalities only: the past user's positions and the video content (not knowing\nother users' traces). We make two main contributions. First, we re-examine\nexisting deep-learning approaches for this problem and identify hidden flaws\nfrom a thorough root-cause analysis. Second, from the results of this analysis,\nwe design a new proposal establishing state-of-the-art performance. First,\nre-assessing the existing methods that use both modalities, we obtain the\nsurprising result that they all perform worse than baselines using the user's\ntrajectory only. A root-cause analysis of the metrics, datasets and neural\narchitectures shows in particular that (i) the content can inform the\nprediction for horizons longer than 2 to 3 sec. (existing methods consider\nshorter horizons), and that (ii) to compete with the baselines, it is necessary\nto have a recurrent unit dedicated to process the positions, but this is not\nsufficient. Second, from a re-examination of the problem supported with the\nconcept of Structural-RNN, we design a new deep neural architecture, named\nTRACK. TRACK achieves state-of-the-art performance on all considered datasets\nand prediction horizons, outperforming competitors by up to 20 percent on\nfocus-type videos and horizons 2-5 seconds. The entire framework (codes and\ndatasets) is online and received an ACM reproducibility badge.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 17:13:00 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 14:07:32 GMT"}, {"version": "v3", "created": "Wed, 14 Apr 2021 16:13:35 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Rondon", "Miguel Fabian Romero", ""], ["Sassatelli", "Lucile", ""], ["Pardo", "Ramon Aparicio", ""], ["Precioso", "Frederic", ""]]}, {"id": "1911.11705", "submitter": "Kuo-Shiuan Peng", "authors": "Kuo-Shiuan Peng and Gregory Ditzler and Jerzy Rozenblit", "title": "Edge-Guided Occlusion Fading Reduction for a Light-Weighted\n  Self-Supervised Monocular Depth Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised monocular depth estimation methods generally suffer the\nocclusion fading issue due to the lack of supervision by the per pixel ground\ntruth. Although a post-processing method was proposed by Godard et. al. to\nreduce the occlusion fading, the compensated results have a severe halo effect.\nIn this paper, we propose a novel Edge-Guided post-processing to reduce the\nocclusion fading issue for self-supervised monocular depth estimation. We\nfurther introduce Atrous Spatial Pyramid Pooling (ASPP) into the network to\nreduce the computational costs and improve the inference performance. The\nproposed ASPP-based network is lighter, faster, and better than current\ncommonly used depth estimation networks. This light-weight network only needs\n8.1 million parameters and can achieve up to 40 frames per second for\n$256\\times512$ input in the inference stage using a single nVIDIA GTX1080 GPU.\nThe proposed network also outperforms the current state-of-the-art on the KITTI\nbenchmarks. The ASPP-based network and Edge-Guided post-processing produce\nbetter results either quantitatively and qualitatively than the competitors.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 17:19:19 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Peng", "Kuo-Shiuan", ""], ["Ditzler", "Gregory", ""], ["Rozenblit", "Jerzy", ""]]}, {"id": "1911.11744", "submitter": "Simon Stepputtis", "authors": "Simon Stepputtis, Joseph Campbell, Mariano Phielipp, Chitta Baral,\n  Heni Ben Amor", "title": "Imitation Learning of Robot Policies by Combining Language, Vision and\n  Demonstration", "comments": "Accepted to the NeurIPS 2019 Workshop on Robot Learning: Control and\n  Interaction in the Real World, Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a novel end-to-end imitation learning approach which\ncombines natural language, vision, and motion information to produce an\nabstract representation of a task, which in turn is used to synthesize specific\nmotion controllers at run-time. This multimodal approach enables generalization\nto a wide variety of environmental conditions and allows an end-user to direct\na robot policy through verbal communication. We empirically validate our\napproach with an extensive set of simulations and show that it achieves a high\ntask success rate over a variety of conditions while remaining amenable to\nprobabilistic interpretability.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 18:27:51 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Stepputtis", "Simon", ""], ["Campbell", "Joseph", ""], ["Phielipp", "Mariano", ""], ["Baral", "Chitta", ""], ["Amor", "Heni Ben", ""]]}, {"id": "1911.11751", "submitter": "Gyanendra Sharma", "authors": "Gyanendra Sharma, Richard J Radke", "title": "Multi-person Spatial Interaction in a Large Immersive Display Using\n  Smartphones as Touchpads", "comments": "8 pages with references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a multi-user interaction interface for a large\nimmersive space that supports simultaneous screen interactions by combining (1)\nuser input via personal smartphones and Bluetooth microphones, (2) spatial\ntracking via an overhead array of Kinect sensors, and (3) WebSocket interfaces\nto a webpage running on the large screen. Users are automatically, dynamically\nassigned personal and shared screen sub-spaces based on their tracked location\nwith respect to the screen, and use a webpage on their personal smartphone for\ntouchpad-type input. We report user experiments using our interaction framework\nthat involve image selection and placement tasks, with the ultimate goal of\nrealizing display-wall environments as viable, interactive workspaces with\nnatural multimodal interfaces.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 18:39:24 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Sharma", "Gyanendra", ""], ["Radke", "Richard J", ""]]}, {"id": "1911.11758", "submitter": "Yuheng Li", "authors": "Yuheng Li, Krishna Kumar Singh, Utkarsh Ojha, Yong Jae Lee", "title": "MixNMatch: Multifactor Disentanglement and Encoding for Conditional\n  Image Generation", "comments": "CVPR 2020 camera ready", "journal-ref": "CVPR 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MixNMatch, a conditional generative model that learns to\ndisentangle and encode background, object pose, shape, and texture from real\nimages with minimal supervision, for mix-and-match image generation. We build\nupon FineGAN, an unconditional generative model, to learn the desired\ndisentanglement and image generator, and leverage adversarial joint image-code\ndistribution matching to learn the latent factor encoders. MixNMatch requires\nbounding boxes during training to model background, but requires no other\nsupervision. Through extensive experiments, we demonstrate MixNMatch's ability\nto accurately disentangle, encode, and combine multiple factors for\nmix-and-match image generation, including sketch2color, cartoon2img, and\nimg2gif applications. Our code/models/demo can be found at\nhttps://github.com/Yuheng-Li/MixNMatch\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 18:49:39 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 06:17:57 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2020 17:56:13 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Li", "Yuheng", ""], ["Singh", "Krishna Kumar", ""], ["Ojha", "Utkarsh", ""], ["Lee", "Yong Jae", ""]]}, {"id": "1911.11759", "submitter": "Xiuye Gu", "authors": "Xiuye Gu, Weixin Luo, Michael S. Ryoo, Yong Jae Lee", "title": "Password-conditioned Anonymization and Deanonymization with Face\n  Identity Transformers", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cameras are prevalent in our daily lives, and enable many useful systems\nbuilt upon computer vision technologies such as smart cameras and home robots\nfor service applications. However, there is also an increasing societal concern\nas the captured images/videos may contain privacy-sensitive information (e.g.,\nface identity). We propose a novel face identity transformer which enables\nautomated photo-realistic password-based anonymization as well as\ndeanonymization of human faces appearing in visual data. Our face identity\ntransformer is trained to (1) remove face identity information after\nanonymization, (2) make the recovery of the original face possible when given\nthe correct password, and (3) return a wrong--but photo-realistic--face given a\nwrong password. Extensive experiments show that our approach enables multimodal\npassword-conditioned face anonymizations and deanonymizations, without\nsacrificing privacy compared to existing anonymization approaches.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 18:50:53 GMT"}, {"version": "v2", "created": "Fri, 29 Nov 2019 20:42:15 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2020 13:10:01 GMT"}, {"version": "v4", "created": "Wed, 30 Sep 2020 15:52:50 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Gu", "Xiuye", ""], ["Luo", "Weixin", ""], ["Ryoo", "Michael S.", ""], ["Lee", "Yong Jae", ""]]}, {"id": "1911.11763", "submitter": "Paul-Edouard Sarlin", "authors": "Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, Andrew\n  Rabinovich", "title": "SuperGlue: Learning Feature Matching with Graph Neural Networks", "comments": "Oral at CVPR 2020, with appendix and link to publicly available code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces SuperGlue, a neural network that matches two sets of\nlocal features by jointly finding correspondences and rejecting non-matchable\npoints. Assignments are estimated by solving a differentiable optimal transport\nproblem, whose costs are predicted by a graph neural network. We introduce a\nflexible context aggregation mechanism based on attention, enabling SuperGlue\nto reason about the underlying 3D scene and feature assignments jointly.\nCompared to traditional, hand-designed heuristics, our technique learns priors\nover geometric transformations and regularities of the 3D world through\nend-to-end training from image pairs. SuperGlue outperforms other learned\napproaches and achieves state-of-the-art results on the task of pose estimation\nin challenging real-world indoor and outdoor environments. The proposed method\nperforms matching in real-time on a modern GPU and can be readily integrated\ninto modern SfM or SLAM systems. The code and trained weights are publicly\navailable at https://github.com/magicleap/SuperGluePretrainedNetwork.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 18:57:21 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 16:49:25 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Sarlin", "Paul-Edouard", ""], ["DeTone", "Daniel", ""], ["Malisiewicz", "Tomasz", ""], ["Rabinovich", "Andrew", ""]]}, {"id": "1911.11776", "submitter": "Takuhiro Kaneko", "authors": "Takuhiro Kaneko, Tatsuya Harada", "title": "Noise Robust Generative Adversarial Networks", "comments": "Accepted to CVPR 2020. Project page:\n  https://takuhirok.github.io/NR-GAN/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are neural networks that learn data\ndistributions through adversarial training. In intensive studies, recent GANs\nhave shown promising results for reproducing training images. However, in spite\nof noise, they reproduce images with fidelity. As an alternative, we propose a\nnovel family of GANs called noise robust GANs (NR-GANs), which can learn a\nclean image generator even when training images are noisy. In particular,\nNR-GANs can solve this problem without having complete noise information (e.g.,\nthe noise distribution type, noise amount, or signal-noise relationship). To\nachieve this, we introduce a noise generator and train it along with a clean\nimage generator. However, without any constraints, there is no incentive to\ngenerate an image and noise separately. Therefore, we propose distribution and\ntransformation constraints that encourage the noise generator to capture only\nthe noise-specific components. In particular, considering such constraints\nunder different assumptions, we devise two variants of NR-GANs for\nsignal-independent noise and three variants of NR-GANs for signal-dependent\nnoise. On three benchmark datasets, we demonstrate the effectiveness of NR-GANs\nin noise robust image generation. Furthermore, we show the applicability of\nNR-GANs in image denoising. Our code is available at\nhttps://github.com/takuhirok/NR-GAN/.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 18:42:54 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 16:54:37 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Kaneko", "Takuhiro", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1911.11789", "submitter": "Yawar Siddiqui", "authors": "Yawar Siddiqui, Julien Valentin, Matthias Nie{\\ss}ner", "title": "ViewAL: Active Learning with Viewpoint Entropy for Semantic Segmentation", "comments": "CVPR2020, Video: https://youtu.be/tAGdx2j-X_g", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose ViewAL, a novel active learning strategy for semantic segmentation\nthat exploits viewpoint consistency in multi-view datasets. Our core idea is\nthat inconsistencies in model predictions across viewpoints provide a very\nreliable measure of uncertainty and encourage the model to perform well\nirrespective of the viewpoint under which objects are observed. To incorporate\nthis uncertainty measure, we introduce a new viewpoint entropy formulation,\nwhich is the basis of our active learning strategy. In addition, we propose\nuncertainty computations on a superpixel level, which exploits inherently\nlocalized signal in the segmentation task, directly lowering the annotation\ncosts. This combination of viewpoint entropy and the use of superpixels allows\nto efficiently select samples that are highly informative for improving the\nnetwork. We demonstrate that our proposed active learning strategy not only\nyields the best-performing models for the same amount of required labeled data,\nbut also significantly reduces labeling effort. For instance, our method\nachieves 95% of maximum achievable network performance using only 7%, 17%, and\n24% labeled data on SceneNet-RGBD, ScanNet, and Matterport3D, respectively. On\nthese datasets, the best state-of-the-art method achieves the same performance\nwith 14%, 27% and 33% labeled data. Finally, we demonstrate that labeling using\nsuperpixels yields the same quality of ground-truth compared to labeling whole\nimages, but requires 25% less time.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 19:00:16 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 18:00:31 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Siddiqui", "Yawar", ""], ["Valentin", "Julien", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1911.11808", "submitter": "Yang Jiao", "authors": "Yang Jiao, Mo Weng, Mei Yang", "title": "Multi-Object Portion Tracking in 4D Fluorescence Microscopy Imagery with\n  Deep Feature Maps", "comments": "10 pages, 8 figures, CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D fluorescence microscopy of living organisms has increasingly become an\nessential and powerful tool in biomedical research and diagnosis. An exploding\namount of imaging data has been collected, whereas efficient and effective\ncomputational tools to extract information from them are still lagging behind.\nThis is largely due to the challenges in analyzing biological data. Interesting\nbiological structures are not only small, but are often morphologically\nirregular and highly dynamic. Although tracking cells in live organisms has\nbeen studied for years, existing tracking methods for cells are not effective\nin tracking subcellular structures, such as protein complexes, which feature in\ncontinuous morphological changes including split and merge, in addition to fast\nmigration and complex motion. In this paper, we first define the problem of\nmulti-object portion tracking to model the protein object tracking process. A\nmulti-object tracking method with portion matching is proposed based on 3D\nsegmentation results. The proposed method distills deep feature maps from deep\nnetworks, then recognizes and matches object portions using an extended search.\nExperimental results confirm that the proposed method achieves 2.96% higher on\nconsistent tracking accuracy and 35.48% higher on event identification accuracy\nthan the state-of-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 19:46:41 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Jiao", "Yang", ""], ["Weng", "Mo", ""], ["Yang", "Mei", ""]]}, {"id": "1911.11822", "submitter": "Jean-Philippe Mercier", "authors": "Jean-Philippe Mercier, Mathieu Garon, Philippe Gigu\\`ere and\n  Jean-Fran\\c{c}ois Lalonde", "title": "Deep Template-based Object Instance Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the focus in the object detection literature has been on the problem\nof identifying the bounding box of a particular class of object in an image.\nYet, in contexts such as robotics and augmented reality, it is often necessary\nto find a specific object instance---a unique toy or a custom industrial part\nfor example---rather than a generic object class. Here, applications can\nrequire a rapid shift from one object instance to another, thus requiring fast\nturnaround which affords little-to-no training time. What is more, gathering a\ndataset and training a model for every new object instance to be detected can\nbe an expensive and time-consuming process. In this context, we propose a\ngeneric 2D object instance detection approach that uses example viewpoints of\nthe target object at test time to retrieve its 2D location in RGB images,\nwithout requiring any additional training (i.e. fine-tuning) step. To this end,\nwe present an end-to-end architecture that extracts global and local\ninformation of the object from its viewpoints. The global information is used\nto tune early filters in the backbone while local viewpoints are correlated\nwith the input image. Our method offers an improvement of almost 30 mAP over\nthe previous template matching methods on the challenging Occluded Linemod\ndataset (overall mAP of 50.7). Our experiments also show that our single\ngeneric model (not trained on any of the test objects) yields detection results\nthat are on par with approaches that are trained specifically on the target\nobjects.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 20:38:26 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 17:39:00 GMT"}, {"version": "v3", "created": "Sun, 15 Nov 2020 03:26:20 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Mercier", "Jean-Philippe", ""], ["Garon", "Mathieu", ""], ["Gigu\u00e8re", "Philippe", ""], ["Lalonde", "Jean-Fran\u00e7ois", ""]]}, {"id": "1911.11834", "submitter": "Zeyu Wang", "authors": "Zeyu Wang, Klint Qinami, Ioannis Christos Karakozis, Kyle Genova, Prem\n  Nair, Kenji Hata, Olga Russakovsky", "title": "Towards Fairness in Visual Recognition: Effective Strategies for Bias\n  Mitigation", "comments": "To appear in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision models learn to perform a task by capturing relevant\nstatistics from training data. It has been shown that models learn spurious\nage, gender, and race correlations when trained for seemingly unrelated tasks\nlike activity recognition or image captioning. Various mitigation techniques\nhave been presented to prevent models from utilizing or learning such biases.\nHowever, there has been little systematic comparison between these techniques.\nWe design a simple but surprisingly effective visual recognition benchmark for\nstudying bias mitigation. Using this benchmark, we provide a thorough analysis\nof a wide range of techniques. We highlight the shortcomings of popular\nadversarial training approaches for bias mitigation, propose a simple but\nsimilarly effective alternative to the inference-time Reducing Bias\nAmplification method of Zhao et al., and design a domain-independent training\ntechnique that outperforms all other methods. Finally, we validate our findings\non the attribute classification task in the CelebA dataset, where attribute\npresence is known to be correlated with the gender of people in the image, and\ndemonstrate that the proposed technique is effective at mitigating real-world\ngender bias.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 21:02:30 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 12:56:20 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Wang", "Zeyu", ""], ["Qinami", "Klint", ""], ["Karakozis", "Ioannis Christos", ""], ["Genova", "Kyle", ""], ["Nair", "Prem", ""], ["Hata", "Kenji", ""], ["Russakovsky", "Olga", ""]]}, {"id": "1911.11854", "submitter": "Erfan Ebrahim Esfahani", "authors": "Erfan Ebrahim Esfahani and Alireza Hosseini", "title": "Compressed MRI Reconstruction Exploiting a Rotation-Invariant Total\n  Variation Discretization", "comments": null, "journal-ref": null, "doi": "10.1016/j.mri.2020.03.008", "report-no": null, "categories": "eess.IV cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the first-order method of Malitsky and Pock, we propose a new\nvariational framework for compressed MR image reconstruction which introduces\nthe application of a rotation-invariant discretization of total variation\nfunctional into MR imaging while exploiting BM3D frame as a sparsifying\ntransform. In the first step, we provide theoretical and numerical analysis\nestablishing the exceptional rotation-invariance property of this total\nvariation functional and observe its superiority over other well-known\nvariational regularization terms in both upright and rotated imaging setups.\nThereupon, the proposed MRI reconstruction model is presented as a constrained\noptimization problem, however, we do not use conventional ADMM-type algorithms\ndesigned for constrained problems to obtain a solution, but rather we tailor\nthe linesearch-equipped method of Malitsky and Pock to our model, which was\noriginally proposed for unconstrained problems. As attested by numerical\nexperiments, this framework significantly outperforms various state-of-the-art\nalgorithms from variational methods to adaptive and learning approaches and in\nparticular, it eliminates the stagnating behavior of a previous work on\nBM3D-MRI which compromised the solution beyond a certain iteration.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 22:05:21 GMT"}, {"version": "v2", "created": "Sat, 7 Dec 2019 10:52:50 GMT"}, {"version": "v3", "created": "Fri, 21 Feb 2020 21:47:06 GMT"}, {"version": "v4", "created": "Sun, 19 Apr 2020 10:25:32 GMT"}, {"version": "v5", "created": "Tue, 21 Apr 2020 07:03:20 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Esfahani", "Erfan Ebrahim", ""], ["Hosseini", "Alireza", ""]]}, {"id": "1911.11872", "submitter": "Manu Goyal", "authors": "Manu Goyal, Thomas Knackstedt, Shaofeng Yan, and Saeed Hassanpour", "title": "Artificial Intelligence-Based Image Classification for Diagnosis of Skin\n  Cancer: Challenges and Opportunities", "comments": "AI Skin Cancer", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been great interest in developing Artificial Intelligence\n(AI) enabled computer-aided diagnostics solutions for the diagnosis of skin\ncancer. With the increasing incidence of skin cancers, low awareness among a\ngrowing population, and a lack of adequate clinical expertise and services,\nthere is an immediate need for AI systems to assist clinicians in this domain.\nA large number of skin lesion datasets are available publicly, and researchers\nhave developed AI-based image classification solutions, particularly deep\nlearning algorithms, to distinguish malignant skin lesions from benign lesions\nin different image modalities such as dermoscopic, clinical, and histopathology\nimages. Despite the various claims of AI systems achieving higher accuracy than\ndermatologists in the classification of different skin lesions, these AI\nsystems are still in the very early stages of clinical application in terms of\nbeing ready to aid clinicians in the diagnosis of skin cancers. In this review,\nwe discuss advancements in the digital image-based AI solutions for the\ndiagnosis of skin cancer, along with some challenges and future opportunities\nto improve these AI systems to support dermatologists and enhance their ability\nto diagnose skin cancer.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 22:47:34 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 17:17:23 GMT"}, {"version": "v3", "created": "Sat, 20 Jun 2020 18:17:48 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Goyal", "Manu", ""], ["Knackstedt", "Thomas", ""], ["Yan", "Shaofeng", ""], ["Hassanpour", "Saeed", ""]]}, {"id": "1911.11893", "submitter": "Yunhao Ba", "authors": "Pradyumna Chari, Chinmay Talegaonkar, Yunhao Ba and Achuta Kadambi", "title": "Visual Physics: Discovering Physical Laws from Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we teach a machine to discover the laws of physics from video\nstreams. We assume no prior knowledge of physics, beyond a temporal stream of\nbounding boxes. The problem is very difficult because a machine must learn not\nonly a governing equation (e.g. projectile motion) but also the existence of\ngoverning parameters (e.g. velocities). We evaluate our ability to discover\nphysical laws on videos of elementary physical phenomena, such as projectile\nmotion or circular motion. These elementary tasks have textbook governing\nequations and enable ground truth verification of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 00:34:38 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Chari", "Pradyumna", ""], ["Talegaonkar", "Chinmay", ""], ["Ba", "Yunhao", ""], ["Kadambi", "Achuta", ""]]}, {"id": "1911.11897", "submitter": "Hao Tang", "authors": "Hao Tang and Hong Liu and Dan Xu and Philip H.S. Torr and Nicu Sebe", "title": "AttentionGAN: Unpaired Image-to-Image Translation using Attention-Guided\n  Generative Adversarial Networks", "comments": "An extended version of a paper published in IJCNN2019. arXiv admin\n  note: substantial text overlap with arXiv:1903.12296", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art methods in the unpaired image-to-image translation are\ncapable of learning a mapping from a source domain to a target domain with\nunpaired image data. Though the existing methods have achieved promising\nresults, they still produce unsatisfied artifacts, being able to convert\nlow-level information while limited in transforming high-level semantics of\ninput images. One possible reason is that generators do not have the ability to\nperceive the most discriminative semantic parts between the source and target\ndomains, thus making the generated images low quality. In this paper, we\npropose a new Attention-Guided Generative Adversarial Networks (AttentionGAN)\nfor the unpaired image-to-image translation task. AttentionGAN can identify the\nmost discriminative semantic objects and minimize changes of unwanted parts for\nsemantic manipulation problems without using extra data and models. The\nattention-guided generators in AttentionGAN are able to produce attention masks\nvia a built-in attention mechanism, and then fuse the generation output with\nthe attention masks to obtain high-quality target images. Accordingly, we also\ndesign a novel attention-guided discriminator which only considers attended\nregions. Extensive experiments are conducted on several generative tasks,\ndemonstrating that the proposed model is effective to generate sharper and more\nrealistic images compared with existing competitive models. The source code for\nthe proposed AttentionGAN is available at\nhttps://github.com/Ha0Tang/AttentionGAN.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 00:53:27 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 23:39:13 GMT"}, {"version": "v3", "created": "Sat, 28 Dec 2019 03:53:51 GMT"}, {"version": "v4", "created": "Wed, 12 Feb 2020 20:31:35 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Tang", "Hao", ""], ["Liu", "Hong", ""], ["Xu", "Dan", ""], ["Torr", "Philip H. S.", ""], ["Sebe", "Nicu", ""]]}, {"id": "1911.11903", "submitter": "Subhayan Mukherjee", "authors": "Subhayan Mukherjee, Giuseppe Valenzise, Irene Cheng", "title": "Potential of deep features for opinion-unaware, distortion-unaware,\n  no-reference image quality assessment", "comments": "International Conference on Smart Multimedia (Springer), 16-18\n  December 2019, San Diego, California, USA", "journal-ref": null, "doi": "10.1007/978-3-030-54407-2_8", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image Quality Assessment algorithms predict a quality score for a pristine or\ndistorted input image, such that it correlates with human opinion. Traditional\nmethods required a non-distorted \"reference\" version of the input image to\ncompare with, in order to predict this score. However, recent \"No-reference\"\nmethods circumvent this requirement by modelling the distribution of clean\nimage features, thereby making them more suitable for practical use. However,\nmajority of such methods either use hand-crafted features or require training\non human opinion scores (supervised learning), which are difficult to obtain\nand standardise. We explore the possibility of using deep features instead,\nparticularly, the encoded (bottleneck) feature maps of a Convolutional\nAutoencoder neural network architecture. Also, we do not train the network on\nsubjective scores (unsupervised learning). The primary requirements for an IQA\nmethod are monotonic increase in predicted scores with increasing degree of\ninput image distortion, and consistent ranking of images with the same\ndistortion type and content, but different distortion levels. Quantitative\nexperiments using the Pearson, Kendall and Spearman correlation scores on a\ndiverse set of images show that our proposed method meets the above\nrequirements better than the state-of-art method (which uses hand-crafted\nfeatures) for three types of distortions: blurring, noise and compression\nartefacts. This demonstrates the potential for future research in this\nrelatively unexplored sub-area within IQA.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 01:10:56 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Mukherjee", "Subhayan", ""], ["Valenzise", "Giuseppe", ""], ["Cheng", "Irene", ""]]}, {"id": "1911.11907", "submitter": "Kai Han", "authors": "Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, Chang Xu", "title": "GhostNet: More Features from Cheap Operations", "comments": "CVPR 2020. Code is available at\n  https://github.com/huawei-noah/ghostnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deploying convolutional neural networks (CNNs) on embedded devices is\ndifficult due to the limited memory and computation resources. The redundancy\nin feature maps is an important characteristic of those successful CNNs, but\nhas rarely been investigated in neural architecture design. This paper proposes\na novel Ghost module to generate more feature maps from cheap operations. Based\non a set of intrinsic feature maps, we apply a series of linear transformations\nwith cheap cost to generate many ghost feature maps that could fully reveal\ninformation underlying intrinsic features. The proposed Ghost module can be\ntaken as a plug-and-play component to upgrade existing convolutional neural\nnetworks. Ghost bottlenecks are designed to stack Ghost modules, and then the\nlightweight GhostNet can be easily established. Experiments conducted on\nbenchmarks demonstrate that the proposed Ghost module is an impressive\nalternative of convolution layers in baseline models, and our GhostNet can\nachieve higher recognition performance (e.g. $75.7\\%$ top-1 accuracy) than\nMobileNetV3 with similar computational cost on the ImageNet ILSVRC-2012\nclassification dataset. Code is available at\nhttps://github.com/huawei-noah/ghostnet\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 01:36:42 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 06:20:33 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Han", "Kai", ""], ["Wang", "Yunhe", ""], ["Tian", "Qi", ""], ["Guo", "Jianyuan", ""], ["Xu", "Chunjing", ""], ["Xu", "Chang", ""]]}, {"id": "1911.11916", "submitter": "Francis Baek", "authors": "Francis Baek, Somin Park, Hyoungkwan Kim", "title": "Data Augmentation Using Adversarial Training for Construction-Equipment\n  Classification", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based construction-site image analysis has recently made great\nprogress with regard to accuracy and speed, but it requires a large amount of\ndata. Acquiring sufficient amount of labeled construction-image data is a\nprerequisite for deep learning-based construction-image recognition and\nrequires considerable time and effort. In this paper, we propose a \"data\naugmentation\" scheme based on generative adversarial networks (GANs) for\nconstruction-equipment classification. The proposed method combines a GAN and\nadditional \"adversarial training\" to stably perform \"data augmentation\" for\nconstruction equipment. The \"data augmentation\" was verified via binary\nclassification experiments involving excavator images, and the average accuracy\nimprovement was 4.094%. In the experiment, three image sizes (32-32-3, 64-64-3,\nand 128-128-3) and 120, 240, and 480 training samples were used to demonstrate\nthe robustness of the proposed method. These results demonstrated that the\nproposed method can effectively and reliably generate construction-equipment\nimages and train deep learning-based classifiers for construction equipment.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 02:16:53 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Baek", "Francis", ""], ["Park", "Somin", ""], ["Kim", "Hyoungkwan", ""]]}, {"id": "1911.11924", "submitter": "Heng Yang", "authors": "Heng Yang and Luca Carlone", "title": "In Perfect Shape: Certifiably Optimal 3D Shape Reconstruction from 2D\n  Landmarks", "comments": "Camera-ready, CVPR 2020. 18 pages, 5 figures, 1 table", "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  2020", "doi": null, "report-no": null, "categories": "cs.CV cs.RO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of 3D shape reconstruction from 2D landmarks extracted\nin a single image. We adopt the 3D deformable shape model and formulate the\nreconstruction as a joint optimization of the camera pose and the linear shape\nparameters. Our first contribution is to apply Lasserre's hierarchy of convex\nSums-of-Squares (SOS) relaxations to solve the shape reconstruction problem and\nshow that the SOS relaxation of minimum order 2 empirically solves the original\nnon-convex problem exactly. Our second contribution is to exploit the structure\nof the polynomial in the objective function and find a reduced set of basis\nmonomials for the SOS relaxation that significantly decreases the size of the\nresulting semidefinite program (SDP) without compromising its accuracy. These\ntwo contributions, to the best of our knowledge, lead to the first certifiably\noptimal solver for 3D shape reconstruction, that we name Shape*. Our third\ncontribution is to add an outlier rejection layer to Shape* using a truncated\nleast squares (TLS) robust cost function and leveraging graduated non-convexity\nto solve TLS without initialization. The result is a robust reconstruction\nalgorithm, named Shape#, that tolerates a large amount of outlier measurements.\nWe evaluate the performance of Shape* and Shape# in both simulated and real\nexperiments, showing that Shape* outperforms local optimization and previous\nconvex relaxation techniques, while Shape# achieves state-of-the-art\nperformance and is robust against 70% outliers in the FG3DCar dataset.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 02:49:38 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2020 17:20:41 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Yang", "Heng", ""], ["Carlone", "Luca", ""]]}, {"id": "1911.11929", "submitter": "Yueh-Hua Wu", "authors": "Chien-Yao Wang, Hong-Yuan Mark Liao, I-Hau Yeh, Yueh-Hua Wu, Ping-Yang\n  Chen, and Jun-Wei Hsieh", "title": "CSPNet: A New Backbone that can Enhance Learning Capability of CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have enabled state-of-the-art approaches to achieve\nincredible results on computer vision tasks such as object detection. However,\nsuch success greatly relies on costly computation resources, which hinders\npeople with cheap devices from appreciating the advanced technology. In this\npaper, we propose Cross Stage Partial Network (CSPNet) to mitigate the problem\nthat previous works require heavy inference computations from the network\narchitecture perspective. We attribute the problem to the duplicate gradient\ninformation within network optimization. The proposed networks respect the\nvariability of the gradients by integrating feature maps from the beginning and\nthe end of a network stage, which, in our experiments, reduces computations by\n20% with equivalent or even superior accuracy on the ImageNet dataset, and\nsignificantly outperforms state-of-the-art approaches in terms of AP50 on the\nMS COCO object detection dataset. The CSPNet is easy to implement and general\nenough to cope with architectures based on ResNet, ResNeXt, and DenseNet.\nSource code is at https://github.com/WongKinYiu/CrossStagePartialNetworks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 03:15:27 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Wang", "Chien-Yao", ""], ["Liao", "Hong-Yuan Mark", ""], ["Yeh", "I-Hau", ""], ["Wu", "Yueh-Hua", ""], ["Chen", "Ping-Yang", ""], ["Hsieh", "Jun-Wei", ""]]}, {"id": "1911.11938", "submitter": "T.S. Jayram", "authors": "T.S. Jayram and Vincent Marois and Tomasz Kornuta and Vincent Albouy\n  and Emre Sevgen and Ahmet S. Ozcan", "title": "Transfer Learning in Visual and Relational Reasoning", "comments": "18 pages; more baseline comparisons; additional clarifications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning has become the de facto standard in computer vision and\nnatural language processing, especially where labeled data is scarce. Accuracy\ncan be significantly improved by using pre-trained models and subsequent\nfine-tuning. In visual reasoning tasks, such as image question answering,\ntransfer learning is more complex. In addition to transferring the capability\nto recognize visual features, we also expect to transfer the system's ability\nto reason. Moreover, for video data, temporal reasoning adds another dimension.\nIn this work, we formalize these unique aspects of transfer learning and\npropose a theoretical framework for visual reasoning, exemplified by the\nwell-established CLEVR and COG datasets. Furthermore, we introduce a new,\nend-to-end differentiable recurrent model (SAMNet), which shows\nstate-of-the-art accuracy and better performance in transfer learning on both\ndatasets. The improved performance of SAMNet stems from its capability to\ndecouple the abstract multi-step reasoning from the length of the sequence and\nits selective attention enabling to store only the question-relevant objects in\nthe external memory.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 03:54:15 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2020 04:26:42 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Jayram", "T. S.", ""], ["Marois", "Vincent", ""], ["Kornuta", "Tomasz", ""], ["Albouy", "Vincent", ""], ["Sevgen", "Emre", ""], ["Ozcan", "Ahmet S.", ""]]}, {"id": "1911.11943", "submitter": "Choi Sungik", "authors": "Sungik Choi, Sae-Young Chung", "title": "Novelty Detection Via Blurring", "comments": "ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional out-of-distribution (OOD) detection schemes based on variational\nautoencoder or Random Network Distillation (RND) have been observed to assign\nlower uncertainty to the OOD than the target distribution. In this work, we\ndiscover that such conventional novelty detection schemes are also vulnerable\nto the blurred images. Based on the observation, we construct a novel RND-based\nOOD detector, SVD-RND, that utilizes blurred images during training. Our\ndetector is simple, efficient at test time, and outperforms baseline OOD\ndetectors in various domains. Further results show that SVD-RND learns better\ntarget distribution representation than the baseline RND algorithm. Finally,\nSVD-RND combined with geometric transform achieves near-perfect detection\naccuracy on the CelebA dataset.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 04:10:18 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 07:36:13 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 06:39:00 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Choi", "Sungik", ""], ["Chung", "Sae-Young", ""]]}, {"id": "1911.11946", "submitter": "Pratik Vaishnavi", "authors": "Pratik Vaishnavi, Tianji Cong, Kevin Eykholt, Atul Prakash, Amir\n  Rahmati", "title": "Can Attention Masks Improve Adversarial Robustness?", "comments": "Version presented at AAAI-20 workshop on Engineering Dependable and\n  Secure Machine Learning Systems (EDSMLS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are known to be susceptible to adversarial\nexamples. Adversarial examples are maliciously crafted inputs that are designed\nto fool a model, but appear normal to human beings. Recent work has shown that\npixel discretization can be used to make classifiers for MNIST highly robust to\nadversarial examples. However, pixel discretization fails to provide\nsignificant protection on more complex datasets. In this paper, we take the\nfirst step towards reconciling these contrary findings. Focusing on the\nobservation that discrete pixelization in MNIST makes the background completely\nblack and foreground completely white, we hypothesize that the important\nproperty for increasing robustness is the elimination of image background using\nattention masks before classifying an object. To examine this hypothesis, we\ncreate foreground attention masks for two different datasets, GTSRB and\nMS-COCO. Our initial results suggest that using attention mask leads to\nimproved robustness. On the adversarially trained classifiers, we see an\nadversarial robustness increase of over 20% on MS-COCO.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 04:26:35 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2019 22:55:53 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Vaishnavi", "Pratik", ""], ["Cong", "Tianji", ""], ["Eykholt", "Kevin", ""], ["Prakash", "Atul", ""], ["Rahmati", "Amir", ""]]}, {"id": "1911.11960", "submitter": "Joel Ruben Antony Moniz", "authors": "Joel Ruben Antony Moniz, Eunsu Kang, Barnab\\'as P\\'oczos", "title": "LucidDream: Controlled Temporally-Consistent DeepDream on Videos", "comments": "Workshop on Machine Learning for Creativity and Design, NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we aim to propose a set of techniques to improve the\ncontrollability and aesthetic appeal when DeepDream, which uses a pre-trained\nneural network to modify images by hallucinating objects into them, is applied\nto videos. In particular, we demonstrate a simple modification that improves\ncontrol over the class of object that DeepDream is induced to hallucinate. We\nalso show that the flickering artifacts which frequently appear when DeepDream\nis applied on videos can be mitigated by the use of an additional temporal\nconsistency loss term.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 05:29:36 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Moniz", "Joel Ruben Antony", ""], ["Kang", "Eunsu", ""], ["P\u00f3czos", "Barnab\u00e1s", ""]]}, {"id": "1911.11961", "submitter": "Haichao Shi", "authors": "Xiao-Yu Zhang, Changsheng Li, Haichao Shi, Xiaobin Zhu, Peng Li, Jing\n  Dong", "title": "AdapNet: Adaptability Decomposing Encoder-Decoder Network for Weakly\n  Supervised Action Recognition and Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The point process is a solid framework to model sequential data, such as\nvideos, by exploring the underlying relevance. As a challenging problem for\nhigh-level video understanding, weakly supervised action recognition and\nlocalization in untrimmed videos has attracted intensive research attention.\nKnowledge transfer by leveraging the publicly available trimmed videos as\nexternal guidance is a promising attempt to make up for the coarse-grained\nvideo-level annotation and improve the generalization performance. However,\nunconstrained knowledge transfer may bring about irrelevant noise and\njeopardize the learning model. This paper proposes a novel adaptability\ndecomposing encoder-decoder network to transfer reliable knowledge between\ntrimmed and untrimmed videos for action recognition and localization via\nbidirectional point process modeling, given only video-level annotations. By\ndecomposing the original features into domain-adaptable and domain-specific\nones based on their adaptability, trimmed-untrimmed knowledge transfer can be\nsafely confined within a more coherent subspace. An encoder-decoder based\nstructure is carefully designed and jointly optimized to facilitate effective\naction classification and temporal localization. Extensive experiments are\nconducted on two benchmark datasets (i.e., THUMOS14 and ActivityNet1.3), and\nexperimental results clearly corroborate the efficacy of our method.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 05:29:47 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Zhang", "Xiao-Yu", ""], ["Li", "Changsheng", ""], ["Shi", "Haichao", ""], ["Zhu", "Xiaobin", ""], ["Li", "Peng", ""], ["Dong", "Jing", ""]]}, {"id": "1911.11970", "submitter": "Domingo Mery", "authors": "Domingo Mery and Florencia Valdes", "title": "Graph Representation for Face Analysis in Image Collections", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an image collection of a social event with a huge number of pictures,\nit is very useful to have tools that can be used to analyze how the individuals\n--that are present in the collection-- interact with each other. In this paper,\nwe propose an optimal graph representation that is based on the `connectivity'\nof them. The connectivity of a pair of subjects gives a score that represents\nhow `connected' they are. It is estimated based on co-occurrence, closeness,\nfacial expressions, and the orientation of the head when they are looking to\neach other. In our proposed graph, the nodes represent the subjects of the\ncollection, and the edges correspond to their connectivities. The location of\nthe nodes is estimated according to their connectivity (the closer the nodes,\nthe more connected are the subjects). Finally, we developed a graphical user\ninterface in which we can click onto the nodes (or the edges) to display the\ncorresponding images of the collection in which the subject of the nodes (or\nthe connected subjects) are present. We present relevant results by analyzing a\nwedding celebration, a sitcom video, a volleyball game and images extracted\nfrom Twitter given a hashtag. We believe that this tool can be very helpful to\ndetect the existing social relations in an image collection.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 06:02:05 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Mery", "Domingo", ""], ["Valdes", "Florencia", ""]]}, {"id": "1911.11981", "submitter": "Yue Wang", "authors": "Yue Wang, Yuke Li, James H. Elder, Runmin Wu, Huchuan Lu", "title": "Class-Conditional Domain Adaptation on Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is an important sub-task for many applications, but\npixel-level ground truth labeling is costly and there is a tendency to overfit\nthe training data, limiting generalization. Unsupervised domain adaptation can\npotentially address these problems, allowing systems trained on labelled\ndatasets from one or more source domains (including less expensive synthetic\ndomains) to be adapted to novel target domains. The conventional approach is to\nautomatically align the representational distributions of source and target\ndomains. One limitation of this approach is that it tends to disadvantage lower\nprobability classes. We address this problem by introducing a Class-Conditional\nDomain Adaptation method (CCDA). It includes a class-conditional multi-scale\ndiscriminator and the class-conditional loss. This novel CCDA method encourages\nthe network to shift the domain in a class-conditional manner, and it equalizes\nloss over classes. We evaluate our CCDA method on two transfer tasks and\ndemonstrate performance comparable to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 06:38:28 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 01:53:08 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Wang", "Yue", ""], ["Li", "Yuke", ""], ["Elder", "James H.", ""], ["Wu", "Runmin", ""], ["Lu", "Huchuan", ""]]}, {"id": "1911.11985", "submitter": "Ruiqi Lu", "authors": "Ruiqi Lu, Huimin Ma", "title": "Semantic Head Enhanced Pedestrian Detection in a Crowd", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian detection in the crowd is a challenging task because of\nintra-class occlusion. More prior information is needed for the detector to be\nrobust against it. Human head area is naturally a strong cue because of its\nstable appearance, visibility and relative location to body. Inspired by it, we\nadopt an extra branch to conduct semantic head detection in parallel with\ntraditional body branch. Instead of manually labeling the head regions, we use\nweak annotations inferred directly from body boxes, which is named as `semantic\nhead'. In this way, the head detection is formulated into using a special part\nof labeled box to detect the corresponding part of human body, which\nsurprisingly improves the performance and robustness to occlusion. Moreover,\nthe head-body alignment structure is explicitly explored by introducing\nAlignment Loss, which functions in a self-supervised manner. Based on these, we\npropose the head-body alignment net (HBAN) in this work, which aims to enhance\npedestrian detection by fully utilizing the human head prior. Comprehensive\nevaluations are conducted to demonstrate the effectiveness of HBAN on\nCityPersons dataset.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 06:52:04 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Lu", "Ruiqi", ""], ["Ma", "Huimin", ""]]}, {"id": "1911.11988", "submitter": "Craig Atkinson", "authors": "Craig Atkinson, Brendan McCane, Lech Szymanski, Anthony Robins", "title": "GRIm-RePR: Prioritising Generating Important Features for\n  Pseudo-Rehearsal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pseudo-rehearsal allows neural networks to learn a sequence of tasks without\nforgetting how to perform in earlier tasks. Preventing forgetting is achieved\nby introducing a generative network which can produce data from previously seen\ntasks so that it can be rehearsed along side learning the new task. This has\nbeen found to be effective in both supervised and reinforcement learning. Our\ncurrent work aims to further prevent forgetting by encouraging the generator to\naccurately generate features important for task retention. More specifically,\nthe generator is improved by introducing a second discriminator into the\nGenerative Adversarial Network which learns to classify between real and fake\nitems from the intermediate activation patterns that they produce when fed\nthrough a continual learning agent. Using Atari 2600 games, we experimentally\nfind that improving the generator can considerably reduce catastrophic\nforgetting compared to the standard pseudo-rehearsal methods used in deep\nreinforcement learning. Furthermore, we propose normalising the Q-values taught\nto the long-term system as we observe this substantially reduces catastrophic\nforgetting by minimising the interference between tasks' reward functions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 07:06:03 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Atkinson", "Craig", ""], ["McCane", "Brendan", ""], ["Szymanski", "Lech", ""], ["Robins", "Anthony", ""]]}, {"id": "1911.11999", "submitter": "Guoxian Song", "authors": "Guoxian Song, Jianmin Zheng, Jianfei Cai, Tat-Jen Cham", "title": "Recovering Facial Reflectance and Geometry from Multi-view Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the problem of estimating shapes and diffuse reflectances of human\nfaces from images has been extensively studied, there is relatively less work\ndone on recovering the specular albedo. This paper presents a lightweight\nsolution for inferring photorealistic facial reflectance and geometry. Our\nsystem processes video streams from two views of a subject, and outputs two\nreflectance maps for diffuse and specular albedos, as well as a vector map of\nsurface normals. A model-based optimization approach is used, consisting of the\nthree stages of multi-view face model fitting, facial reflectance inference and\nfacial geometry refinement. Our approach is based on a novel formulation built\nupon the 3D morphable model (3DMM) for representing 3D textured faces in\nconjunction with the Blinn-Phong reflection model. It has the advantage of\nrequiring only a simple setup with two video streams, and is able to exploit\nthe interaction between the diffuse and specular reflections across multiple\nviews as well as time frames. As a result, the method is able to reliably\nrecover high-fidelity facial reflectance and geometry, which facilitates\nvarious applications such as generating photorealistic facial images under new\nviewpoints or illumination conditions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 07:50:23 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Song", "Guoxian", ""], ["Zheng", "Jianmin", ""], ["Cai", "Jianfei", ""], ["Cham", "Tat-Jen", ""]]}, {"id": "1911.12012", "submitter": "Shilin Zhu", "authors": "Shuo Cheng, Zexiang Xu, Shilin Zhu, Zhuwen Li, Li Erran Li, Ravi\n  Ramamoorthi, Hao Su", "title": "Deep Stereo using Adaptive Thin Volume Representation with Uncertainty\n  Awareness", "comments": "Accepted to CVPR 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Uncertainty-aware Cascaded Stereo Network (UCS-Net) for 3D\nreconstruction from multiple RGB images. Multi-view stereo (MVS) aims to\nreconstruct fine-grained scene geometry from multi-view images. Previous\nlearning-based MVS methods estimate per-view depth using plane sweep volumes\nwith a fixed depth hypothesis at each plane; this generally requires densely\nsampled planes for desired accuracy, and it is very hard to achieve\nhigh-resolution depth. In contrast, we propose adaptive thin volumes (ATVs); in\nan ATV, the depth hypothesis of each plane is spatially varying, which adapts\nto the uncertainties of previous per-pixel depth predictions. Our UCS-Net has\nthree stages: the first stage processes a small standard plane sweep volume to\npredict low-resolution depth; two ATVs are then used in the following stages to\nrefine the depth with higher resolution and higher accuracy. Our ATV consists\nof only a small number of planes; yet, it efficiently partitions local depth\nranges within learned small intervals. In particular, we propose to use\nvariance-based uncertainty estimates to adaptively construct ATVs; this\ndifferentiable process introduces reasonable and fine-grained spatial\npartitioning. Our multi-stage framework progressively subdivides the vast scene\nspace with increasing depth resolution and precision, which enables scene\nreconstruction with high completeness and accuracy in a coarse-to-fine fashion.\nWe demonstrate that our method achieves superior performance compared with\nstate-of-the-art benchmarks on various challenging datasets.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 08:14:52 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 23:09:41 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Cheng", "Shuo", ""], ["Xu", "Zexiang", ""], ["Zhu", "Shilin", ""], ["Li", "Zhuwen", ""], ["Li", "Li Erran", ""], ["Ramamoorthi", "Ravi", ""], ["Su", "Hao", ""]]}, {"id": "1911.12018", "submitter": "Bang Yang", "authors": "Bang Yang, Yuexian Zou, Fenglin Liu, Can Zhang", "title": "Non-Autoregressive Coarse-to-Fine Video Captioning", "comments": "9 pages, 6 figures, to be published in AAAI2021. Our code is\n  available at\n  https://github.com/yangbang18/Non-Autoregressive-Video-Captioning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is encouraged to see that progress has been made to bridge videos and\nnatural language. However, mainstream video captioning methods suffer from slow\ninference speed due to the sequential manner of autoregressive decoding, and\nprefer generating generic descriptions due to the insufficient training of\nvisual words (e.g., nouns and verbs) and inadequate decoding paradigm. In this\npaper, we propose a non-autoregressive decoding based model with a\ncoarse-to-fine captioning procedure to alleviate these defects. In\nimplementations, we employ a bi-directional self-attention based network as our\nlanguage model for achieving inference speedup, based on which we decompose the\ncaptioning procedure into two stages, where the model has different focuses.\nSpecifically, given that visual words determine the semantic correctness of\ncaptions, we design a mechanism of generating visual words to not only promote\nthe training of scene-related words but also capture relevant details from\nvideos to construct a coarse-grained sentence \"template\". Thereafter, we devise\ndedicated decoding algorithms that fill in the \"template\" with suitable words\nand modify inappropriate phrasing via iterative refinement to obtain a\nfine-grained description. Extensive experiments on two mainstream video\ncaptioning benchmarks, i.e., MSVD and MSR-VTT, demonstrate that our approach\nachieves state-of-the-art performance, generates diverse descriptions, and\nobtains high inference efficiency. Our code is available at\nhttps://github.com/yangbang18/Non-Autoregressive-Video-Captioning.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 08:36:41 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 13:13:27 GMT"}, {"version": "v3", "created": "Tue, 10 Dec 2019 08:29:22 GMT"}, {"version": "v4", "created": "Sun, 5 Jul 2020 22:19:04 GMT"}, {"version": "v5", "created": "Mon, 14 Dec 2020 13:21:51 GMT"}, {"version": "v6", "created": "Wed, 24 Mar 2021 05:43:53 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Yang", "Bang", ""], ["Zou", "Yuexian", ""], ["Liu", "Fenglin", ""], ["Zhang", "Can", ""]]}, {"id": "1911.12028", "submitter": "Konstantin Bulatov", "authors": "Olga Petrova and Konstantin Bulatov and Vladimir L. Arlazarov", "title": "Methods of Weighted Combination for Text Field Recognition in a Video\n  Stream", "comments": "6 pages, 4 figures, 1 table, accepted and presented at International\n  Conference on Machine Vision 2019 (ICMV 2019)", "journal-ref": "Proc. SPIE 11433 ICMV-2019 (2020), 114332L", "doi": "10.1117/12.2559378", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to a noticeable expansion of document recognition applicability, there is\na high demand for recognition on mobile devices. A mobile camera, unlike a\nscanner, cannot always ensure the absence of various image distortions,\ntherefore the task of improving the recognition precision is relevant. The\nadvantage of mobile devices over scanners is the ability to use video stream\ninput, which allows to get multiple images of a recognized document. Despite\nthis, not enough attention is currently paid to the issue of combining\nrecognition results obtained from different frames when using video stream\ninput. In this paper we propose a weighted text string recognition results\ncombination method and weighting criteria, and provide experimental data for\nverifying their validity and effectiveness. Based on the obtained results, it\nis concluded that the use of such weighted combination is appropriate for\nimproving the quality of the video stream recognition result.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 08:48:54 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Petrova", "Olga", ""], ["Bulatov", "Konstantin", ""], ["Arlazarov", "Vladimir L.", ""]]}, {"id": "1911.12036", "submitter": "Hui Tang", "authors": "Hui Tang, Kui Jia", "title": "Discriminative Adversarial Domain Adaptation", "comments": "18 pages, 10 figures, 12 tables, accepted by AAAI-20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given labeled instances on a source domain and unlabeled ones on a target\ndomain, unsupervised domain adaptation aims to learn a task classifier that can\nwell classify target instances. Recent advances rely on domain-adversarial\ntraining of deep networks to learn domain-invariant features. However, due to\nan issue of mode collapse induced by the separate design of task and domain\nclassifiers, these methods are limited in aligning the joint distributions of\nfeature and category across domains. To overcome it, we propose a novel\nadversarial learning method termed Discriminative Adversarial Domain Adaptation\n(DADA). Based on an integrated category and domain classifier, DADA has a novel\nadversarial objective that encourages a mutually inhibitory relation between\ncategory and domain predictions for any input instance. We show that under\npractical conditions, it defines a minimax game that can promote the joint\ndistribution alignment. Except for the traditional closed set domain\nadaptation, we also extend DADA for extremely challenging problem settings of\npartial and open set domain adaptation. Experiments show the efficacy of our\nproposed methods and we achieve the new state of the art for all the three\nsettings on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 09:19:16 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 03:49:04 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Tang", "Hui", ""], ["Jia", "Kui", ""]]}, {"id": "1911.12037", "submitter": "Yunzhong Hou", "authors": "Yunzhong Hou, Liang Zheng, Zhongdao Wang, Shengjin Wang", "title": "Locality Aware Appearance Metric for Multi-Target Multi-Camera Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-target multi-camera tracking (MTMCT) systems track targets across\ncameras. Due to the continuity of target trajectories, tracking systems usually\nrestrict their data association within a local neighborhood. In single camera\ntracking, local neighborhood refers to consecutive frames; in multi-camera\ntracking, it refers to neighboring cameras that the target may appear\nsuccessively. For similarity estimation, tracking systems often adopt\nappearance features learned from the re-identification (re-ID) perspective.\nDifferent from tracking, re-ID usually does not have access to the trajectory\ncues that can limit the search space to a local neighborhood. Due to its global\nmatching property, the re-ID perspective requires to learn global appearance\nfeatures. We argue that the mismatch between the local matching procedure in\ntracking and the global nature of re-ID appearance features may compromise\nMTMCT performance.\n  To fit the local matching procedure in MTMCT, in this work, we introduce\nlocality aware appearance metric (LAAM). Specifically, we design an\nintra-camera metric for single camera tracking, and an inter-camera metric for\nmulti-camera tracking. Both metrics are trained with data pairs sampled from\ntheir corresponding local neighborhoods, as opposed to global sampling in the\nre-ID perspective. We show that the locally learned metrics can be successfully\napplied on top of several globally learned re-ID features. With the proposed\nmethod, we report new state-of-the-art performance on the DukeMTMC dataset, and\na substantial improvement on the CityFlow dataset.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 09:22:32 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Hou", "Yunzhong", ""], ["Zheng", "Liang", ""], ["Wang", "Zhongdao", ""], ["Wang", "Shengjin", ""]]}, {"id": "1911.12044", "submitter": "Zhongfan Jia", "authors": "Zhongfan Jia, Chenglong Bao, Kaisheng Ma", "title": "Exploring Frequency Domain Interpretation of Convolutional Neural\n  Networks", "comments": "The main conclusion of this paper is ambiguous and cannot be fully\n  supported by the experiments. We decide to withdraw this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many existing interpretation methods of convolutional neural networks (CNNs)\nmainly analyze in spatial domain, yet model interpretability in frequency\ndomain has been rarely studied. To the best of our knowledge, there is no study\non the interpretation of modern CNNs from the perspective of the frequency\nproportion of filters. In this work, we analyze the frequency properties of\nfilters in the first layer as it is the entrance of information and relatively\nmore convenient for analysis. By controlling the proportion of different\nfrequency filters in the training stage, the network classification accuracy\nand model robustness is evaluated and our results reveal that it has a great\nimpact on the robustness to common corruptions. Moreover, a learnable\nmodulation of frequency proportion with perturbation in power spectrum is\nproposed from the perspective of frequency domain. Experiments on CIFAR-10-C\nshow 10.97% average robustness gains for ResNet-18 with negligible natural\naccuracy degradation.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 09:41:39 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 14:07:30 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Jia", "Zhongfan", ""], ["Bao", "Chenglong", ""], ["Ma", "Kaisheng", ""]]}, {"id": "1911.12051", "submitter": "Ping-Yang Chen", "authors": "Ping-Yang Chen, Jun-Wei Hsieh, Chien-Yao Wang, Hong-Yuan Mark Liao,\n  and Munkhjargal Gochoo", "title": "Residual Bi-Fusion Feature Pyramid Network for Accurate Single-shot\n  Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art (SoTA) models have improved the accuracy of object detection\nwith a large margin via a FP (feature pyramid). FP is a top-down aggregation to\ncollect semantically strong features to improve scale invariance in both\ntwo-stage and one-stage detectors. However, this top-down pathway cannot\npreserve accurate object positions due to the shift-effect of pooling. Thus,\nthe advantage of FP to improve detection accuracy will disappear when more\nlayers are used. The original FP lacks a bottom-up pathway to offset the lost\ninformation from lower-layer feature maps. It performs well in large-sized\nobject detection but poor in small-sized object detection. A new structure\n\"residual feature pyramid\" is proposed in this paper. It is bidirectional to\nfuse both deep and shallow features towards more effective and robust detection\nfor both small-sized and large-sized objects. Due to the \"residual\" nature, it\ncan be easily trained and integrated to different backbones (even deeper or\nlighter) than other bi-directional methods. One important property of this\nresidual FP is: accuracy improvement is still found even if more layers are\nadopted. Extensive experiments on VOC and MS COCO datasets showed the proposed\nmethod achieved the SoTA results for highly-accurate and efficient object\ndetection..\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 09:53:49 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 06:54:49 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Chen", "Ping-Yang", ""], ["Hsieh", "Jun-Wei", ""], ["Wang", "Chien-Yao", ""], ["Liao", "Hong-Yuan Mark", ""], ["Gochoo", "Munkhjargal", ""]]}, {"id": "1911.12053", "submitter": "Haoyu He", "authors": "Haoyu He, Jing Zhang, Qiming Zhang, Dacheng Tao", "title": "Grapy-ML: Graph Pyramid Mutual Learning for Cross-dataset Human Parsing", "comments": "Accepted as an oral paper in AAAI2020. 9 pages, 4 figures.\n  https://www.aaai.org/Papers/AAAI/2020GB/AAAI-HeH.2317.pdf", "journal-ref": "AAAI 2020", "doi": "10.1609/aaai.v34i07.6728", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human parsing, or human body part semantic segmentation, has been an active\nresearch topic due to its wide potential applications. In this paper, we\npropose a novel GRAph PYramid Mutual Learning (Grapy-ML) method to address the\ncross-dataset human parsing problem, where the annotations are at different\ngranularities. Starting from the prior knowledge of the human body hierarchical\nstructure, we devise a graph pyramid module (GPM) by stacking three levels of\ngraph structures from coarse granularity to fine granularity subsequently. At\neach level, GPM utilizes the self-attention mechanism to model the correlations\nbetween context nodes. Then, it adopts a top-down mechanism to progressively\nrefine the hierarchical features through all the levels. GPM also enables\nefficient mutual learning. Specifically, the network weights of the first two\nlevels are shared to exchange the learned coarse-granularity information across\ndifferent datasets. By making use of the multi-granularity labels, Grapy-ML\nlearns a more discriminative feature representation and achieves\nstate-of-the-art performance, which is demonstrated by extensive experiments on\nthe three popular benchmarks, e.g. CIHP dataset. The source code is publicly\navailable at https://github.com/Charleshhy/Grapy-ML.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 09:59:25 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["He", "Haoyu", ""], ["Zhang", "Jing", ""], ["Zhang", "Qiming", ""], ["Tao", "Dacheng", ""]]}, {"id": "1911.12069", "submitter": "Davide Cozzolino", "authors": "Davide Cozzolino and Justus Thies and Andreas R\\\"ossler and Matthias\n  Nie{\\ss}ner and Luisa Verdoliva", "title": "SpoC: Spoofing Camera Fingerprints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the fast progress in synthetic media generation, creating realistic\nfalse images has become very easy. Such images can be used to wrap \"rich\" fake\nnews with enhanced credibility, spawning a new wave of high-impact, high-risk\nmisinformation campaigns. Therefore, there is a fast-growing interest in\nreliable detectors of manipulated media. The most powerful detectors, to date,\nrely on the subtle traces left by any device on all images acquired by it. In\nparticular, due to proprietary in-camera processes, like demosaicing or\ncompression, each camera model leaves trademark traces that can be exploited\nfor forensic analyses. The absence or distortion of such traces in the target\nimage is a strong hint of manipulation. In this paper, we challenge such\ndetectors to gain better insight into their vulnerabilities. This is an\nimportant study in order to build better forgery detectors able to face\nmalicious attacks. Our proposal consists of a GAN-based approach that injects\ncamera traces into synthetic images. Given a GAN-generated image, we insert the\ntraces of a specific camera model into it and deceive state-of-the-art\ndetectors into believing the image was acquired by that model. Likewise, we\ndeceive independent detectors of synthetic GAN images into believing the image\nis real. Experiments prove the effectiveness of the proposed method in a wide\narray of conditions. Moreover, no prior information on the attacked detectors\nis needed, but only sample images from the target camera.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 10:41:19 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 09:51:53 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Cozzolino", "Davide", ""], ["Thies", "Justus", ""], ["R\u00f6ssler", "Andreas", ""], ["Nie\u00dfner", "Matthias", ""], ["Verdoliva", "Luisa", ""]]}, {"id": "1911.12101", "submitter": "Keke Tang", "authors": "Keke Tang, Peng Song, Yuexin Ma, Zhaoquan Gu, Yu Su, Zhihong Tian,\n  Wenping Wang", "title": "Decision Propagation Networks for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-level (e.g., semantic) features encoded in the latter layers of\nconvolutional neural networks are extensively exploited for image\nclassification, leaving low-level (e.g., color) features in the early layers\nunderexplored. In this paper, we propose a novel Decision Propagation Module\n(DPM) to make an intermediate decision that could act as category-coherent\nguidance extracted from early layers, and then propagate it to the latter\nlayers. Therefore, by stacking a collection of DPMs into a classification\nnetwork, the generated Decision Propagation Network is explicitly formulated as\nto progressively encode more discriminative features guided by the decision,\nand then refine the decision based on the new generated features layer by\nlayer. Comprehensive results on four publicly available datasets validate DPM\ncould bring significant improvements for existing classification networks with\nminimal additional computational cost and is superior to the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 12:13:06 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Tang", "Keke", ""], ["Song", "Peng", ""], ["Ma", "Yuexin", ""], ["Gu", "Zhaoquan", ""], ["Su", "Yu", ""], ["Tian", "Zhihong", ""], ["Wang", "Wenping", ""]]}, {"id": "1911.12104", "submitter": "Jie Yang", "authors": "Jie Yang, Yu-Kai Wang, Xin Yao, Chin-Teng Lin", "title": "Adaptive Initialization Method for K-means Algorithm", "comments": "22 pages, 2 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The K-means algorithm is a widely used clustering algorithm that offers\nsimplicity and efficiency. However, the traditional K-means algorithm uses the\nrandom method to determine the initial cluster centers, which make clustering\nresults prone to local optima and then result in worse clustering performance.\nMany initialization methods have been proposed, but none of them can\ndynamically adapt to datasets with various characteristics. In our previous\nresearch, an initialization method for K-means based on hybrid distance was\nproposed, and this algorithm can adapt to datasets with different\ncharacteristics. However, it has the following drawbacks: (a) When calculating\ndensity, the threshold cannot be uniquely determined, resulting in unstable\nresults. (b) Heavily depending on adjusting the parameter, the parameter must\nbe adjusted five times to obtain better clustering results. (c) The time\ncomplexity of the algorithm is quadratic, which is difficult to apply to large\ndatasets. In the current paper, we proposed an adaptive initialization method\nfor the K-means algorithm (AIMK) to improve our previous work. AIMK can not\nonly adapt to datasets with various characteristics but also obtain better\nclustering results within two interactions. In addition, we then leverage\nrandom sampling in AIMK, which is named as AIMK-RS, to reduce the time\ncomplexity. AIMK-RS is easily applied to large and high-dimensional datasets.\nWe compared AIMK and AIMK-RS with 10 different algorithms on 16 normal and six\nextra-large datasets. The experimental results show that AIMK and AIMK-RS\noutperform the current initialization methods and several well-known clustering\nalgorithms. Furthermore, AIMK-RS can significantly reduce the complexity of\napplying it to extra-large datasets with high dimensions. The time complexity\nof AIMK-RS is O(n).\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 12:27:00 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Yang", "Jie", ""], ["Wang", "Yu-Kai", ""], ["Yao", "Xin", ""], ["Lin", "Chin-Teng", ""]]}, {"id": "1911.12110", "submitter": "Xin-Yu Zhang", "authors": "Xin-Yu Zhang, Le Zhang, Zao-Yi Zheng, Yun Liu, Jia-Wang Bian,\n  Ming-Ming Cheng", "title": "AdaSample: Adaptive Sampling of Hard Positives for Descriptor Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Triplet loss has been widely employed in a wide range of computer vision\ntasks, including local descriptor learning. The effectiveness of the triplet\nloss heavily relies on the triplet selection, in which a common practice is to\nfirst sample intra-class patches (positives) from the dataset for batch\nconstruction and then mine in-batch negatives to form triplets. For\nhigh-informativeness triplet collection, researchers mostly focus on mining\nhard negatives in the second stage, while paying relatively less attention to\nconstructing informative batches. To alleviate this issue, we propose\nAdaSample, an adaptive online batch sampler, in this paper. Specifically, hard\npositives are sampled based on their informativeness. In this way, we formulate\na hardness-aware positive mining pipeline within a novel maximum loss\nminimization training protocol. The efficacy of the proposed method is\nevaluated on several standard benchmarks, where it demonstrates a significant\nand consistent performance gain on top of the existing strong baselines.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 12:38:08 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Zhang", "Xin-Yu", ""], ["Zhang", "Le", ""], ["Zheng", "Zao-Yi", ""], ["Liu", "Yun", ""], ["Bian", "Jia-Wang", ""], ["Cheng", "Ming-Ming", ""]]}, {"id": "1911.12116", "submitter": "David M\\\"unch", "authors": "Vanessa Buhrmester, David M\\\"unch, Michael Arens", "title": "Analysis of Explainers of Black Box Deep Neural Networks for Computer\n  Vision: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning is a state-of-the-art technique to make inference on extensive\nor complex data. As a black box model due to their multilayer nonlinear\nstructure, Deep Neural Networks are often criticized to be non-transparent and\ntheir predictions not traceable by humans. Furthermore, the models learn from\nartificial datasets, often with bias or contaminated discriminating content.\nThrough their increased distribution, decision-making algorithms can contribute\npromoting prejudge and unfairness which is not easy to notice due to lack of\ntransparency. Hence, scientists developed several so-called explanators or\nexplainers which try to point out the connection between input and output to\nrepresent in a simplified way the inner structure of machine learning black\nboxes. In this survey we differ the mechanisms and properties of explaining\nsystems for Deep Neural Networks for Computer Vision tasks. We give a\ncomprehensive overview about taxonomy of related studies and compare several\nsurvey papers that deal with explainability in general. We work out the\ndrawbacks and gaps and summarize further research ideas.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 12:58:52 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Buhrmester", "Vanessa", ""], ["M\u00fcnch", "David", ""], ["Arens", "Michael", ""]]}, {"id": "1911.12126", "submitter": "Xiangxiang Chu", "authors": "Xiangxiang Chu and Tianbao Zhou and Bo Zhang and Jixiang Li", "title": "Fair DARTS: Eliminating Unfair Advantages in Differentiable Architecture\n  Search", "comments": "Accepted to ECCV 2020, camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentiable Architecture Search (DARTS) is now a widely disseminated\nweight-sharing neural architecture search method. However, it suffers from\nwell-known performance collapse due to an inevitable aggregation of skip\nconnections. In this paper, we first disclose that its root cause lies in an\nunfair advantage in exclusive competition. Through experiments, we show that if\neither of two conditions is broken, the collapse disappears. Thereby, we\npresent a novel approach called Fair DARTS where the exclusive competition is\nrelaxed to be collaborative. Specifically, we let each operation's\narchitectural weight be independent of others. Yet there is still an important\nissue of discretization discrepancy. We then propose a zero-one loss to push\narchitectural weights towards zero or one, which approximates an expected\nmulti-hot solution. Our experiments are performed on two mainstream search\nspaces, and we derive new state-of-the-art results on CIFAR-10 and ImageNet.\nOur code is available on https://github.com/xiaomi-automl/fairdarts .\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 13:10:25 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 11:31:52 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2020 02:37:59 GMT"}, {"version": "v4", "created": "Thu, 16 Jul 2020 01:16:52 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Chu", "Xiangxiang", ""], ["Zhou", "Tianbao", ""], ["Zhang", "Bo", ""], ["Li", "Jixiang", ""]]}, {"id": "1911.12148", "submitter": "Ke Yang", "authors": "Ke Yang and Dongsheng Li and Yong Dou", "title": "Towards Precise End-to-end Weakly Supervised Object Detection Network", "comments": "accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is challenging for weakly supervised object detection network to precisely\npredict the positions of the objects, since there are no instance-level\ncategory annotations. Most existing methods tend to solve this problem by using\na two-phase learning procedure, i.e., multiple instance learning detector\nfollowed by a fully supervised learning detector with bounding-box regression.\nBased on our observation, this procedure may lead to local minima for some\nobject categories. In this paper, we propose to jointly train the two phases in\nan end-to-end manner to tackle this problem. Specifically, we design a single\nnetwork with both multiple instance learning and bounding-box regression\nbranches that share the same backbone. Meanwhile, a guided attention module\nusing classification loss is added to the backbone for effectively extracting\nthe implicit location information in the features. Experimental results on\npublic datasets show that our method achieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 13:34:38 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Yang", "Ke", ""], ["Li", "Dongsheng", ""], ["Dou", "Yong", ""]]}, {"id": "1911.12159", "submitter": "H\\'ector Andrade-Loarca", "authors": "H\\'ector Andrade-Loarca, Gitta Kutyniok and Ozan \\\"Oktem", "title": "Shearlets as Feature Extractor for Semantic Edge Detection: The\n  Model-Based and Data-Driven Realm", "comments": "30 pages, 12 figures. To appear in Proceedings of the Royal Society.\n  Mathematical, physical and engineering sciences", "journal-ref": null, "doi": "10.1098/rspa.2019.0841", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic edge detection has recently gained a lot of attention as an image\nprocessing task, mainly due to its wide range of real-world applications. This\nis based on the fact that edges in images contain most of the semantic\ninformation. Semantic edge detection involves two tasks, namely pure edge\ndetecion and edge classification. Those are in fact fundamentally distinct in\nterms of the level of abstraction that each task requires, which is known as\nthe distracted supervision paradox that limits the possible performance of a\nsupervised model in semantic edge detection. In this work, we will present a\nnovel hybrid method to avoid the distracted supervision paradox and achieve\nhigh-performance in semantic edge detection. Our approach is based on a\ncombination of the model-based concept of shearlets, which provides probably\noptimally sparse approximations of a model-class of images, and the data-driven\nmethod of a suitably designed convolutional neural netwok. Finally, we present\nseveral applications such as tomographic reconstruction and show that our\napproach signifiantly outperforms former methods, thereby indicating the value\nof such hybrid methods for the area in biomedical imaging.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 14:05:26 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Andrade-Loarca", "H\u00e9ctor", ""], ["Kutyniok", "Gitta", ""], ["\u00d6ktem", "Ozan", ""]]}, {"id": "1911.12161", "submitter": "David Zimmerer", "authors": "David Zimmerer, Jens Petersen, Klaus Maier-Hein", "title": "High- and Low-level image component decomposition using VAEs for\n  improved reconstruction and anomaly detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Auto-Encoders have often been used for unsupervised pretraining,\nfeature extraction and out-of-distribution and anomaly detection in the medical\nfield. However, VAEs often lack the ability to produce sharp images and learn\nhigh-level features. We propose to alleviate these issues by adding a new\nbranch to conditional hierarchical VAEs. This enforces a division between\nhigher-level and lower-level features. Despite the additional computational\noverhead compared to a normal VAE it results in sharper and better\nreconstructions and can capture the data distribution similarly well (indicated\nby a similar or slightly better OoD detection performance).\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 14:08:25 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Zimmerer", "David", ""], ["Petersen", "Jens", ""], ["Maier-Hein", "Klaus", ""]]}, {"id": "1911.12170", "submitter": "Milan Aggarwal", "authors": "Mausoom Sarkar, Milan Aggarwal, Arneh Jain, Hiresh Gupta, Balaji\n  Krishnamurthy", "title": "Document Structure Extraction using Prior based High Resolution\n  Hierarchical Semantic Segmentation", "comments": "This work has been accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structure extraction from document images has been a long-standing research\ntopic due to its high impact on a wide range of practical applications. In this\npaper, we share our findings on employing a hierarchical semantic segmentation\nnetwork for this task of structure extraction. We propose a prior based deep\nhierarchical CNN network architecture that enables document structure\nextraction using very high resolution(1800 x 1000) images. We divide the\ndocument image into overlapping horizontal strips such that the network\nsegments a strip and uses its prediction mask as prior for predicting the\nsegmentation of the subsequent strip. We perform experiments establishing the\neffectiveness of our strip based network architecture through ablation methods\nand comparison with low-resolution variations. Further, to demonstrate our\nnetwork's capabilities, we train it on only one type of documents (Forms) and\nachieve state-of-the-art results over other general document datasets. We\nintroduce our new human-annotated forms dataset and show that our method\nsignificantly outperforms different segmentation baselines on this dataset in\nextracting hierarchical structures. Our method is currently being used in\nAdobe's AEM Forms for automated conversion of paper and PDF forms to modern\nHTML based forms.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 14:18:02 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 14:53:19 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Sarkar", "Mausoom", ""], ["Aggarwal", "Milan", ""], ["Jain", "Arneh", ""], ["Gupta", "Hiresh", ""], ["Krishnamurthy", "Balaji", ""]]}, {"id": "1911.12207", "submitter": "Jiayun Wang", "authors": "Jiayun Wang, Yubei Chen, Rudrasis Chakraborty, Stella X. Yu", "title": "Orthogonal Convolutional Neural Networks", "comments": "To appear in CVPR 2020, project page: http://pwang.pw/ocnn.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks are hindered by training instability and\nfeature redundancy towards further performance improvement. A promising\nsolution is to impose orthogonality on convolutional filters.\n  We develop an efficient approach to impose filter orthogonality on a\nconvolutional layer based on the doubly block-Toeplitz matrix representation of\nthe convolutional kernel instead of using the common kernel orthogonality\napproach, which we show is only necessary but not sufficient for ensuring\northogonal convolutions.\n  Our proposed orthogonal convolution requires no additional parameters and\nlittle computational overhead. This method consistently outperforms the kernel\northogonality alternative on a wide range of tasks such as image classification\nand inpainting under supervised, semi-supervised and unsupervised settings.\nFurther, it learns more diverse and expressive features with better training\nstability, robustness, and generalization. Our code is publicly available at\nhttps://github.com/samaonline/Orthogonal-Convolutional-Neural-Networks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 15:04:26 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 07:22:49 GMT"}, {"version": "v3", "created": "Wed, 8 Apr 2020 04:58:47 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Wang", "Jiayun", ""], ["Chen", "Yubei", ""], ["Chakraborty", "Rudrasis", ""], ["Yu", "Stella X.", ""]]}, {"id": "1911.12236", "submitter": "Jesus Zarzar", "authors": "Jesus Zarzar, Silvio Giancola, Bernard Ghanem", "title": "PointRGCN: Graph Convolution Networks for 3D Vehicles Detection\n  Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In autonomous driving pipelines, perception modules provide a visual\nunderstanding of the surrounding road scene. Among the perception tasks,\nvehicle detection is of paramount importance for a safe driving as it\nidentifies the position of other agents sharing the road. In our work, we\npropose PointRGCN: a graph-based 3D object detection pipeline based on graph\nconvolutional networks (GCNs) which operates exclusively on 3D LiDAR point\nclouds. To perform more accurate 3D object detection, we leverage a graph\nrepresentation that performs proposal feature and context aggregation. We\nintegrate residual GCNs in a two-stage 3D object detection pipeline, where 3D\nobject proposals are refined using a novel graph representation. In particular,\nR-GCN is a residual GCN that classifies and regresses 3D proposals, and C-GCN\nis a contextual GCN that further refines proposals by sharing contextual\ninformation between multiple proposals. We integrate our refinement modules\ninto a novel 3D detection pipeline, PointRGCN, and achieve state-of-the-art\nperformance on the easy difficulty for the bird eye view detection task.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 15:51:49 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Zarzar", "Jesus", ""], ["Giancola", "Silvio", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1911.12239", "submitter": "Florian Jug", "authors": "Mangal Prakash, Tim-Oliver Buchholz, Manan Lalit, Pavel Tomancak,\n  Florian Jug, Alexander Krull", "title": "Leveraging Self-supervised Denoising for Image Segmentation", "comments": "accepted at ISBI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) has arguably emerged as the method of choice for the\ndetection and segmentation of biological structures in microscopy images.\nHowever, DL typically needs copious amounts of annotated training data that is\nfor biomedical projects typically not available and excessively expensive to\ngenerate. Additionally, tasks become harder in the presence of noise, requiring\neven more high-quality training data. Hence, we propose to use denoising\nnetworks to improve the performance of other DL-based image segmentation\nmethods. More specifically, we present ideas on how state-of-the-art\nself-supervised CARE networks can improve cell/nuclei segmentation in\nmicroscopy data. Using two state-of-the-art baseline methods, U-Net and\nStarDist, we show that our ideas consistently improve the quality of resulting\nsegmentations, especially when only limited training data for noisy micrographs\nare available.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 15:56:27 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 14:27:59 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2020 09:15:05 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Prakash", "Mangal", ""], ["Buchholz", "Tim-Oliver", ""], ["Lalit", "Manan", ""], ["Tomancak", "Pavel", ""], ["Jug", "Florian", ""], ["Krull", "Alexander", ""]]}, {"id": "1911.12249", "submitter": "Asket Kaur", "authors": "Asket Kaur, Navya Rao, Tanya Joon", "title": "Literature Review of Action Recognition in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The literature review presented below on Action Recognition in the wild is\nthe in-depth study of Research Papers. Action Recognition problem in the\nuntrimmed videos is a challenging task and most of the papers have tackled this\nproblem using hand-crafted features with shallow learning techniques and\nsophisticated end-to-end deep learning techniques.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 16:14:14 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Kaur", "Asket", ""], ["Rao", "Navya", ""], ["Joon", "Tanya", ""]]}, {"id": "1911.12287", "submitter": "Giannis Daras", "authors": "Giannis Daras, Augustus Odena, Han Zhang, Alexandros G. Dimakis", "title": "Your Local GAN: Designing Two Dimensional Local Attention Mechanisms for\n  Generative Models", "comments": "Added TFRC, tensorflow-gan acknowledgements. Changed \"Ablation Study\"\n  to \"Ablation Studies\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new local sparse attention layer that preserves\ntwo-dimensional geometry and locality. We show that by just replacing the dense\nattention layer of SAGAN with our construction, we obtain very significant FID,\nInception score and pure visual improvements. FID score is improved from\n$18.65$ to $15.94$ on ImageNet, keeping all other parameters the same. The\nsparse attention patterns that we propose for our new layer are designed using\na novel information theoretic criterion that uses information flow graphs. We\nalso present a novel way to invert Generative Adversarial Networks with\nattention. Our method extracts from the attention layer of the discriminator a\nsaliency map, which we use to construct a new loss function for the inversion.\nThis allows us to visualize the newly introduced attention heads and show that\nthey indeed capture interesting aspects of two-dimensional geometry of real\nimages.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 17:03:16 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 18:30:38 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Daras", "Giannis", ""], ["Odena", "Augustus", ""], ["Zhang", "Han", ""], ["Dimakis", "Alexandros G.", ""]]}, {"id": "1911.12291", "submitter": "Florian Jug", "authors": "Mangal Prakash, Manan Lalit, Pavel Tomancak, Alexander Krull, Florian\n  Jug", "title": "Fully Unsupervised Probabilistic Noise2Void", "comments": "Accepted at ISBI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image denoising is the first step in many biomedical image analysis pipelines\nand Deep Learning (DL) based methods are currently best performing. A new\ncategory of DL methods such as Noise2Void or Noise2Self can be used fully\nunsupervised, requiring nothing but the noisy data. However, this comes at the\nprice of reduced reconstruction quality. The recently proposed Probabilistic\nNoise2Void (PN2V) improves results, but requires an additional noise model for\nwhich calibration data needs to be acquired. Here, we present improvements to\nPN2V that (i) replace histogram based noise models by parametric noise models,\nand (ii) show how suitable noise models can be created even in the absence of\ncalibration data. This is a major step since it actually renders PN2V fully\nunsupervised. We demonstrate that all proposed improvements are not only\nacademic but indeed relevant.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 17:11:59 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 08:52:52 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Prakash", "Mangal", ""], ["Lalit", "Manan", ""], ["Tomancak", "Pavel", ""], ["Krull", "Alexander", ""], ["Jug", "Florian", ""]]}, {"id": "1911.12317", "submitter": "Yang Liu", "authors": "Yang Liu, Pietro Perona, Markus Meister", "title": "PanDA: Panoptic Data Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recently proposed panoptic segmentation task presents a significant\nchallenge of image understanding with computer vision by unifying semantic\nsegmentation and instance segmentation tasks. In this paper we present an\nefficient and novel panoptic data augmentation (PanDA) method which operates\nexclusively in pixel space, requires no additional data or training, and is\ncomputationally cheap to implement. By retraining original state-of-the-art\nmodels on PanDA augmented datasets generated with a single frozen set of\nparameters, we show robust performance gains in panoptic segmentation, instance\nsegmentation, as well as detection across models, backbones, dataset domains,\nand scales. Finally, the effectiveness of unrealistic-looking training images\nsynthesized by PanDA suggest that one should rethink the need for image realism\nfor efficient data augmentation.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 17:52:00 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2020 01:43:14 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Liu", "Yang", ""], ["Perona", "Pietro", ""], ["Meister", "Markus", ""]]}, {"id": "1911.12330", "submitter": "Daniel Mas Montserrat", "authors": "Daniel Mas Montserrat, Jianhang Chen, Qian Lin, Jan P. Allebach,\n  Edward J. Delp", "title": "Multi-View Matching Network for 6D Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications that interact with the real world such as augmented reality or\nrobot manipulation require a good understanding of the location and pose of the\nsurrounding objects. In this paper, we present a new approach to estimate the 6\nDegree of Freedom (DoF) or 6D pose of objects from a single RGB image. Our\napproach can be paired with an object detection and segmentation method to\nestimate, refine and track the pose of the objects by matching the input image\nwith rendered images.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 18:16:45 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Montserrat", "Daniel Mas", ""], ["Chen", "Jianhang", ""], ["Lin", "Qian", ""], ["Allebach", "Jan P.", ""], ["Delp", "Edward J.", ""]]}, {"id": "1911.12354", "submitter": "Alessio Xompero", "authors": "Alessio Xompero, Ricardo Sanchez-Matilla, Apostolos Modas, Pascal\n  Frossard, Andrea Cavallaro", "title": "Multi-view shape estimation of transparent containers", "comments": "Accepted to International Conference on Acoustic, Speech, and Signal\n  Processing (ICASSP); 5 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 3D localisation of an object and the estimation of its properties, such\nas shape and dimensions, are challenging under varying degrees of transparency\nand lighting conditions. In this paper, we propose a method for jointly\nlocalising container-like objects and estimating their dimensions using two\nwide-baseline, calibrated RGB cameras. Under the assumption of circular\nsymmetry along the vertical axis, we estimate the dimensions of an object with\na generative 3D sampling model of sparse circumferences, iterative shape\nfitting and image re-projection to verify the sampling hypotheses in each\ncamera using semantic segmentation masks. We evaluate the proposed method on a\nnovel dataset of objects with different degrees of transparency and captured\nunder different backgrounds and illumination conditions. Our method, which is\nbased on RGB images only, outperforms in terms of localisation success and\ndimension estimation accuracy a deep-learning based approach that uses depth\nmaps.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 18:55:20 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 18:31:37 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Xompero", "Alessio", ""], ["Sanchez-Matilla", "Ricardo", ""], ["Modas", "Apostolos", ""], ["Frossard", "Pascal", ""], ["Cavallaro", "Andrea", ""]]}, {"id": "1911.12361", "submitter": "Jennifer J. Sun", "authors": "Jennifer J. Sun, Ting Liu, Gautam Prasad", "title": "GLA in MediaEval 2018 Emotional Impact of Movies Task", "comments": "MediaEval 2018, 29-31 October 2018, Sophia Antipolis, France. This\n  work is presented at the workshop in MediaEval 2018 for the Emotional Impact\n  of Movies Task", "journal-ref": null, "doi": null, "report-no": "urn:nbn:de:0074-2283-7", "categories": "cs.CV cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visual and audio information from movies can evoke a variety of emotions\nin viewers. Towards a better understanding of viewer impact, we present our\nmethods for the MediaEval 2018 Emotional Impact of Movies Task to predict the\nexpected valence and arousal continuously in movies. This task, using the\nLIRIS-ACCEDE dataset, enables researchers to compare different approaches for\npredicting viewer impact from movies. Our approach leverages image, audio, and\nface based features computed using pre-trained neural networks. These features\nwere computed over time and modeled using a gated recurrent unit (GRU) based\nnetwork followed by a mixture of experts model to compute multiclass\npredictions. We smoothed these predictions using a Butterworth filter for our\nfinal result. Our method enabled us to achieve top performance in three\nevaluation metrics in the MediaEval 2018 task.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 18:59:55 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Sun", "Jennifer J.", ""], ["Liu", "Ting", ""], ["Prasad", "Gautam", ""]]}, {"id": "1911.12362", "submitter": "Haitian Zheng", "authors": "Haitian Zheng, Haofu Liao, Lele Chen, Wei Xiong, Tianlang Chen, Jiebo\n  Luo", "title": "Example-Guided Scene Image Synthesis using Masked Spatial-Channel\n  Attention and Patch-Based Self-Supervision", "comments": "13 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Example-guided image synthesis has been recently attempted to synthesize an\nimage from a semantic label map and an exemplary image. In the task, the\nadditional exemplary image serves to provide style guidance that controls the\nappearance of the synthesized output. Despite the controllability advantage,\nthe previous models are designed on datasets with specific and roughly aligned\nobjects. In this paper, we tackle a more challenging and general task, where\nthe exemplar is an arbitrary scene image that is semantically unaligned to the\ngiven label map. To this end, we first propose a new Masked Spatial-Channel\nAttention (MSCA) module which models the correspondence between two\nunstructured scenes via cross-attention. Next, we propose an end-to-end network\nfor joint global and local feature alignment and synthesis. In addition, we\npropose a novel patch-based self-supervision scheme to enable training.\nExperiments on the large-scale CCOO-stuff dataset show significant improvements\nover existing methods. Moreover, our approach provides interpretability and can\nbe readily extended to other tasks including style and spatial interpolation or\nextrapolation, as well as other content manipulation.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 15:14:24 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Zheng", "Haitian", ""], ["Liao", "Haofu", ""], ["Chen", "Lele", ""], ["Xiong", "Wei", ""], ["Chen", "Tianlang", ""], ["Luo", "Jiebo", ""]]}, {"id": "1911.12377", "submitter": "Federico Landi", "authors": "Federico Landi, Lorenzo Baraldi, Marcella Cornia, Massimiliano\n  Corsini, Rita Cucchiara", "title": "Perceive, Transform, and Act: Multi-Modal Attention Networks for\n  Vision-and-Language Navigation", "comments": "A revised version of this paper is currently under consideration at\n  Computer Vision and Image Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-and-Language Navigation (VLN) is a challenging task in which an agent\nneeds to follow a language-specified path to reach a target destination. In\nthis paper, we strive for the creation of an agent able to tackle three key\nissues: multi-modality, long-term dependencies, and adaptability towards\ndifferent locomotive settings. To that end, we devise \"Perceive, Transform, and\nAct\" (PTA): a fully-attentive VLN architecture that leaves the recurrent\napproach behind and the first Transformer-like architecture incorporating three\ndifferent modalities - natural language, images, and discrete actions for the\nagent control. In particular, we adopt an early fusion strategy to merge\nlingual and visual information efficiently in our encoder. We then propose to\nrefine the decoding phase with a late fusion extension between the agent's\nhistory of actions and the perception modalities. We experimentally validate\nour model on two datasets and two different action settings. PTA surpasses\nprevious state-of-the-art architectures for low-level VLN on R2R and achieves\nthe first place for both setups in the recently proposed R4R benchmark. Our\ncode is publicly available at\nhttps://github.com/aimagelab/perceive-transform-and-act.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 19:00:24 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 07:30:16 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Landi", "Federico", ""], ["Baraldi", "Lorenzo", ""], ["Cornia", "Marcella", ""], ["Corsini", "Massimiliano", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1911.12387", "submitter": "Alireza Borjali", "authors": "Alireza Borjali, Antonia F. Chen, Orhun K. Muratoglu, Mohammad A.\n  Morid, Kartik M. Varadarajan", "title": "Detecting total hip replacement prosthesis design on preoperative\n  radiographs using deep convolutional neural network", "comments": null, "journal-ref": null, "doi": "10.1002/jor.24617", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the design of a failed implant is a key step in preoperative\nplanning of revision total joint arthroplasty. Manual identification of the\nimplant design from radiographic images is time consuming and prone to error.\nFailure to identify the implant design preoperatively can lead to increased\noperating room time, more complex surgery, increased blood loss, increased bone\nloss, increased recovery time, and overall increased healthcare costs. In this\nstudy, we present a novel, fully automatic and interpretable approach to\nidentify the design of total hip replacement (THR) implants from plain\nradiographs using deep convolutional neural network (CNN). CNN achieved 100%\naccuracy in identification of three commonly used THR implant designs. Such CNN\ncan be used to automatically identify the design of a failed THR implant\npreoperatively in just a few seconds, saving time and improving the\nidentification accuracy. This can potentially improve patient outcomes, free\npractitioners time, and reduce healthcare costs.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 19:22:33 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Borjali", "Alireza", ""], ["Chen", "Antonia F.", ""], ["Muratoglu", "Orhun K.", ""], ["Morid", "Mohammad A.", ""], ["Varadarajan", "Kartik M.", ""]]}, {"id": "1911.12408", "submitter": "Wenxuan Wu", "authors": "Wenxuan Wu, Zhiyuan Wang, Zhuwen Li, Wei Liu, Li Fuxin", "title": "PointPWC-Net: A Coarse-to-Fine Network for Supervised and\n  Self-Supervised Scene Flow Estimation on 3D Point Clouds", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel end-to-end deep scene flow model, called PointPWC-Net, on\n3D point clouds in a coarse-to-fine fashion. Flow computed at the coarse level\nis upsampled and warped to a finer level, enabling the algorithm to accommodate\nfor large motion without a prohibitive search space. We introduce novel cost\nvolume, upsampling, and warping layers to efficiently handle 3D point cloud\ndata. Unlike traditional cost volumes that require exhaustively computing all\nthe cost values on a high-dimensional grid, our point-based formulation\ndiscretizes the cost volume onto input 3D points, and a PointConv operation\nefficiently computes convolutions on the cost volume. Experiment results on\nFlyingThings3D outperform the state-of-the-art by a large margin. We further\nexplore novel self-supervised losses to train our model and achieve comparable\nresults to state-of-the-art trained with supervised loss. Without any\nfine-tuning, our method also shows great generalization ability on KITTI Scene\nFlow 2015 dataset, outperforming all previous methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 20:29:33 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 03:43:04 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Wu", "Wenxuan", ""], ["Wang", "Zhiyuan", ""], ["Li", "Zhuwen", ""], ["Liu", "Wei", ""], ["Fuxin", "Li", ""]]}, {"id": "1911.12409", "submitter": "Eli Shlizerman", "authors": "Kun Su, Xiulong Liu and Eli Shlizerman", "title": "PREDICT & CLUSTER: Unsupervised Skeleton Based Action Recognition", "comments": "See video at: https://www.youtube.com/watch?v=-dcCFUBRmwE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel system for unsupervised skeleton-based action recognition.\nGiven inputs of body keypoints sequences obtained during various movements, our\nsystem associates the sequences with actions. Our system is based on an\nencoder-decoder recurrent neural network, where the encoder learns a separable\nfeature representation within its hidden states formed by training the model to\nperform prediction task. We show that according to such unsupervised training\nthe decoder and the encoder self-organize their hidden states into a feature\nspace which clusters similar movements into the same cluster and distinct\nmovements into distant clusters. Current state-of-the-art methods for action\nrecognition are strongly supervised, i.e., rely on providing labels for\ntraining. Unsupervised methods have been proposed, however, they require camera\nand depth inputs (RGB+D) at each time step. In contrast, our system is fully\nunsupervised, does not require labels of actions at any stage, and can operate\nwith body keypoints input only. Furthermore, the method can perform on various\ndimensions of body keypoints (2D or 3D) and include additional cues describing\nmovements. We evaluate our system on three extensive action recognition\nbenchmarks with different number of actions and examples. Our results\noutperform prior unsupervised skeleton-based methods, unsupervised RGB+D based\nmethods on cross-view tests and while being unsupervised have similar\nperformance to supervised skeleton-based action recognition.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 20:34:54 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Su", "Kun", ""], ["Liu", "Xiulong", ""], ["Shlizerman", "Eli", ""]]}, {"id": "1911.12423", "submitter": "Ximeng Sun", "authors": "Ximeng Sun, Rameswar Panda, Rogerio Feris, Kate Saenko", "title": "AdaShare: Learning What To Share For Efficient Deep Multi-Task Learning", "comments": "Neurips 2020 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning is an open and challenging problem in computer vision.\nThe typical way of conducting multi-task learning with deep neural networks is\neither through handcrafted schemes that share all initial layers and branch out\nat an adhoc point, or through separate task-specific networks with an\nadditional feature sharing/fusion mechanism. Unlike existing methods, we\npropose an adaptive sharing approach, called AdaShare, that decides what to\nshare across which tasks to achieve the best recognition accuracy, while taking\nresource efficiency into account. Specifically, our main idea is to learn the\nsharing pattern through a task-specific policy that selectively chooses which\nlayers to execute for a given task in the multi-task network. We efficiently\noptimize the task-specific policy jointly with the network weights, using\nstandard back-propagation. Experiments on several challenging and diverse\nbenchmark datasets with a variable number of tasks well demonstrate the\nefficacy of our approach over state-of-the-art methods. Project page:\nhttps://cs-people.bu.edu/sunxm/AdaShare/project.html.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 21:07:25 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 21:32:28 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Sun", "Ximeng", ""], ["Panda", "Rameswar", ""], ["Feris", "Rogerio", ""], ["Saenko", "Kate", ""]]}, {"id": "1911.12425", "submitter": "Eu Wern Teh", "authors": "Eu Wern Teh and Graham W. Taylor", "title": "Learning with less data via Weakly Labeled Patch Classification in\n  Digital Pathology", "comments": "To appear in IEEE International Symposium on Biomedical Imaging\n  (ISBI) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Digital Pathology (DP), labeled data is generally very scarce due to the\nrequirement that medical experts provide annotations. We address this issue by\nlearning transferable features from weakly labeled data, which are collected\nfrom various parts of the body and are organized by non-medical experts. In\nthis paper, we show that features learned from such weakly labeled datasets are\nindeed transferable and allow us to achieve highly competitive patch\nclassification results on the colorectal cancer (CRC) dataset [1] and the\nPatchCamelyon (PCam) dataset [2] while using an order of magnitude less labeled\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 21:19:09 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 15:18:26 GMT"}, {"version": "v3", "created": "Wed, 22 Jan 2020 02:20:39 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Teh", "Eu Wern", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1911.12448", "submitter": "Chenchen Zhu", "authors": "Chenchen Zhu, Fangyi Chen, Zhiqiang Shen, Marios Savvides", "title": "Soft Anchor-Point Object Detection", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, anchor-free detection methods have been through great progress. The\nmajor two families, anchor-point detection and key-point detection, are at\nopposite edges of the speed-accuracy trade-off, with anchor-point detectors\nhaving the speed advantage. In this work, we boost the performance of the\nanchor-point detector over the key-point counterparts while maintaining the\nspeed advantage. To achieve this, we formulate the detection problem from the\nanchor point's perspective and identify ineffective training as the main\nproblem. Our key insight is that anchor points should be optimized jointly as a\ngroup both within and across feature pyramid levels. We propose a simple yet\neffective training strategy with soft-weighted anchor points and soft-selected\npyramid levels to address the false attention issue within each pyramid level\nand the feature selection issue across all the pyramid levels, respectively. To\nevaluate the effectiveness, we train a single-stage anchor-free detector called\nSoft Anchor-Point Detector (SAPD). Experiments show that our concise SAPD\npushes the envelope of speed/accuracy trade-off to a new level, outperforming\nrecent state-of-the-art anchor-free and anchor-based detectors. Without bells\nand whistles, our best model can achieve a single-model single-scale AP of\n47.4% on COCO.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 22:26:20 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 00:09:56 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Zhu", "Chenchen", ""], ["Chen", "Fangyi", ""], ["Shen", "Zhiqiang", ""], ["Savvides", "Marios", ""]]}, {"id": "1911.12451", "submitter": "Ali Borji", "authors": "Ali Borji, Seyed Mehdi Iranmanesh", "title": "Empirical Upper Bound in Object Detection and More", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object detection remains as one of the most notorious open problems in\ncomputer vision. Despite large strides in accuracy in recent years, modern\nobject detectors have started to saturate on popular benchmarks raising the\nquestion of how far we can reach with deep learning tools and tricks. Here, by\nemploying 2 state-of-the-art object detection benchmarks, and analyzing more\nthan 15 models over 4 large scale datasets, we I) carefully determine the\nupperbound in AP, which is 91.6% on VOC (test2007), 78.2% on COCO (val2017),\nand 58.9% on OpenImages V4 (validation), regardless of the IOU. These numbers\nare much better than the mAP of the best model1 (47.9% on VOC, and 46.9% on\nCOCO; IOUs=.5:.95), II) characterize the sources of errors in object detectors,\nin a novel and intuitive way, and find that classification error (confusion\nwith other classes and misses) explains the largest fraction of errors and\nweighs more than localization and duplicate errors, and III) analyze the\ninvariance properties of models when surrounding context of an object is\nremoved, when an object is placed in an incongruent background, and when images\nare blurred or flipped vertically. We find that models generate boxes on empty\nregions and that context is more important for detecting small objects than\nlarger ones. Our work taps into the tight relationship between recognition and\ndetection and offers insights for building better models.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 22:40:41 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 23:08:23 GMT"}, {"version": "v3", "created": "Tue, 17 Dec 2019 03:22:13 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Borji", "Ali", ""], ["Iranmanesh", "Seyed Mehdi", ""]]}, {"id": "1911.12465", "submitter": "Tao Hu", "authors": "Tao Hu, Zhizhong Han, Matthias Zwicker", "title": "3D Shape Completion with Multi-view Consistent Inference", "comments": "Accepted to AAAI 2020 as oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D shape completion is important to enable machines to perceive the complete\ngeometry of objects from partial observations. To address this problem,\nview-based methods have been presented. These methods represent shapes as\nmultiple depth images, which can be back-projected to yield corresponding 3D\npoint clouds, and they perform shape completion by learning to complete each\ndepth image using neural networks. While view-based methods lead to\nstate-of-the-art results, they currently do not enforce geometric consistency\namong the completed views during the inference stage. To resolve this issue, we\npropose a multi-view consistent inference technique for 3D shape completion,\nwhich we express as an energy minimization problem including a data term and a\nregularization term. We formulate the regularization term as a consistency loss\nthat encourages geometric consistency among multiple views, while the data term\nguarantees that the optimized views do not drift away too much from a learned\nshape descriptor. Experimental results demonstrate that our method completes\nshapes more accurately than previous techniques.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 00:01:52 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Hu", "Tao", ""], ["Han", "Zhizhong", ""], ["Zwicker", "Matthias", ""]]}, {"id": "1911.12467", "submitter": "Mateusz Kozi\\'nski", "authors": "Leonardo Citraro and Mateusz Kozi\\'nski and Pascal Fua", "title": "Towards Reliable Evaluation of Road Network Reconstructions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing performance measures rank delineation algorithms inconsistently,\nwhich makes it difficult to decide which one is best in any given situation. We\nshow that these inconsistencies stem from design flaws that make the metrics\ninsensitive to whole classes of errors. To provide more reliable evaluation, we\ndesign three new metrics that are far more consistent even though they use very\ndifferent approaches to comparing ground-truth and reconstructed road networks.\nWe use both synthetic and real data to demonstrate this and advocate the use of\nthese corrected metrics as a tool to gauge future progress.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 00:16:16 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Citraro", "Leonardo", ""], ["Kozi\u0144ski", "Mateusz", ""], ["Fua", "Pascal", ""]]}, {"id": "1911.12476", "submitter": "Shaoli Huang", "authors": "Mingjiang Liang, Shaoli Huang, Shirui Pan, Mingming Gong and Wei Liu", "title": "Learning Multi-level Weight-centric Features for Few-shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning is currently enjoying a considerable resurgence of\ninterest, aided by the recent advance of deep learning. Contemporary approaches\nbased on weight-generation scheme delivers a straightforward and flexible\nsolution to the problem. However, they did not fully consider both the\nrepresentation power for unseen categories and weight generation capacity in\nfeature learning, making it a significant performance bottleneck. This paper\nproposes a multi-level weight-centric feature learning to give full play to\nfeature extractor's dual roles in few-shot learning. Our proposed method\nconsists of two essential techniques: a weight-centric training strategy to\nimprove the features' prototype-ability and a multi-level feature incorporating\na mid- and relation-level information. The former increases the feasibility of\nconstructing a discriminative decision boundary based on a few samples.\nSimultaneously, the latter helps improve the transferability for characterizing\nnovel classes and preserve classification capability for base classes. We\nextensively evaluate our approach to low-shot classification benchmarks.\nExperiments demonstrate our proposed method significantly outperforms its\ncounterparts in both standard and generalized settings and using different\nnetwork backbones.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 01:22:59 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 05:58:52 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Liang", "Mingjiang", ""], ["Huang", "Shaoli", ""], ["Pan", "Shirui", ""], ["Gong", "Mingming", ""], ["Liu", "Wei", ""]]}, {"id": "1911.12491", "submitter": "Jangho Kim", "authors": "Jangho Kim, Yash Bhalgat, Jinwon Lee, Chirag Patel, Nojun Kwak", "title": "QKD: Quantization-aware Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantization and Knowledge distillation (KD) methods are widely used to\nreduce memory and power consumption of deep neural networks (DNNs), especially\nfor resource-constrained edge devices. Although their combination is quite\npromising to meet these requirements, it may not work as desired. It is mainly\nbecause the regularization effect of KD further diminishes the already reduced\nrepresentation power of a quantized model. To address this short-coming, we\npropose Quantization-aware Knowledge Distillation (QKD) wherein quantization\nand KD are care-fully coordinated in three phases. First, Self-studying (SS)\nphase fine-tunes a quantized low-precision student network without KD to obtain\na good initialization. Second, Co-studying (CS) phase tries to train a teacher\nto make it more quantizaion-friendly and powerful than a fixed teacher.\nFinally, Tutoring (TU) phase transfers knowledge from the trained teacher to\nthe student. We extensively evaluate our method on ImageNet and CIFAR-10/100\ndatasets and show an ablation study on networks with both standard and\ndepthwise-separable convolutions. The proposed QKD outperformed existing\nstate-of-the-art methods (e.g., 1.3% improvement on ResNet-18 with W4A4, 2.6%\non MobileNetV2 with W4A4). Additionally, QKD could recover the full-precision\naccuracy at as low as W3A3 quantization on ResNet and W6A6 quantization on\nMobilenetV2.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 02:27:27 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Kim", "Jangho", ""], ["Bhalgat", "Yash", ""], ["Lee", "Jinwon", ""], ["Patel", "Chirag", ""], ["Kwak", "Nojun", ""]]}, {"id": "1911.12501", "submitter": "Shaoli Huang", "authors": "Sanjeev Sharma, Shaoli Huang, and Dacheng Tao", "title": "An End-to-end Framework for Unconstrained Monocular 3D Hand Pose\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the challenging problem of unconstrained 3D hand pose\nestimation using monocular RGB images. Most of the existing approaches assume\nsome prior knowledge of hand (such as hand locations and side information) is\navailable for 3D hand pose estimation. This restricts their use in\nunconstrained environments. We, therefore, present an end-to-end framework that\nrobustly predicts hand prior information and accurately infers 3D hand pose by\nlearning ConvNet models while only using keypoint annotations. To achieve\nrobustness, the proposed framework uses a novel keypoint-based method to\nsimultaneously predict hand regions and side labels, unlike existing methods\nthat suffer from background color confusion caused by using segmentation or\ndetection-based technology. Moreover, inspired by the biological structure of\nthe human hand, we introduce two geometric constraints directly into the 3D\ncoordinates prediction that further improves its performance in a\nweakly-supervised training. Experimental results show that our proposed\nframework not only performs robustly on unconstrained setting, but also\noutperforms the state-of-art methods on standard benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 02:55:21 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Sharma", "Sanjeev", ""], ["Huang", "Shaoli", ""], ["Tao", "Dacheng", ""]]}, {"id": "1911.12507", "submitter": "Thuong Nguyen Canh", "authors": "Thuong, Nguyen Canh, Chien, Trinh Van", "title": "Error Resilient Deep Compressive Sensing", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive sensing (CS) is an emerging sampling technology that enables\nreconstructing signals from a subset of measurements and even corrupted\nmeasurements. Deep learning-based compressive sensing (DCS) has improved CS\nperformance while maintaining a fast reconstruction but requires a training\nnetwork for each measurement rate. Also, concerning the transmission scheme of\nmeasurement lost, DCS cannot recover the original signal. Thereby, it fails to\nmaintain the error-resilient property. In this work, we proposed a robust deep\nreconstruction network to preserve the error-resilient property under the\nassumption of random measurement lost. Measurement lost layer is proposed to\nsimulate the measurement lost in an end-to-end framework.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 03:16:39 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Thuong", "", ""], ["Canh", "Nguyen", ""], ["Chien", "", ""], ["Van", "Trinh", ""]]}, {"id": "1911.12509", "submitter": "Lei Shi", "authors": "Lei Shi, Yifan Zhang, Jian Cheng, Hanqing Lu", "title": "Action Recognition via Pose-Based Graph Convolutional Networks with\n  Intermediate Dense Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Pose-based action recognition has drawn considerable attention recently.\nExisting methods exploit the joint positions to extract the body-part features\nfrom the activation map of the convolutional networks to assist human action\nrecognition. However, these features are simply concatenated or max-pooled in\nprevious works. The structured correlations among the body parts, which are\nessential for understanding complex human actions, are not fully exploited. To\naddress the problem, we propose a pose-based graph convolutional network\n(PGCN), which encodes the body-part features into a human-based spatiotemporal\ngraph, and explicitly models their correlations with a novel light-weight\nadaptive graph convolutional module to produce a highly discriminative\nrepresentation for human action recognition. Besides, we discover that the\nbackbone network tends to identify patterns from the most discriminative areas\nof the input regardless of the others. Thus the features pooled by the joint\npositions from other areas are less informative, which consequently hampers the\nperformance of the followed aggregation process for recognizing actions. To\nalleviate this issue, we introduce a simple intermediate dense supervision\nmechanism for the backbone network, which adequately addresses the problem with\nno extra computation cost during inference. We evaluate the proposed approach\non three popular benchmarks for pose-based action recognition tasks, i.e.,\nSub-JHMDB, PennAction and NTU-RGBD, where our approach significantly\noutperforms state-of-the-arts without the bells and whistles.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 03:28:50 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Shi", "Lei", ""], ["Zhang", "Yifan", ""], ["Cheng", "Jian", ""], ["Lu", "Hanqing", ""]]}, {"id": "1911.12512", "submitter": "Xinyang Jiang", "authors": "Xinyang Jiang, Yifei Gong, Xiaowei Guo, Qize Yang, Feiyue Huang,\n  Weishi Zheng, Feng Zheng, Xing Sun", "title": "Rethinking Temporal Fusion for Video-based Person Re-identification on\n  Semantic and Time Aspect", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the research interest of person re-identification (ReID) has\ngradually turned to video-based methods, which acquire a person representation\nby aggregating frame features of an entire video. However, existing video-based\nReID methods do not consider the semantic difference brought by the outputs of\ndifferent network stages, which potentially compromises the information\nrichness of the person features. Furthermore, traditional methods ignore\nimportant relationship among frames, which causes information redundancy in\nfusion along the time axis. To address these issues, we propose a novel general\ntemporal fusion framework to aggregate frame features on both semantic aspect\nand time aspect. As for the semantic aspect, a multi-stage fusion network is\nexplored to fuse richer frame features at multiple semantic levels, which can\neffectively reduce the information loss caused by the traditional single-stage\nfusion. While, for the time axis, the existing intra-frame attention method is\nimproved by adding a novel inter-frame attention module, which effectively\nreduces the information redundancy in temporal fusion by taking the\nrelationship among frames into consideration. The experimental results show\nthat our approach can effectively improve the video-based re-identification\naccuracy, achieving the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 03:35:57 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Jiang", "Xinyang", ""], ["Gong", "Yifei", ""], ["Guo", "Xiaowei", ""], ["Yang", "Qize", ""], ["Huang", "Feiyue", ""], ["Zheng", "Weishi", ""], ["Zheng", "Feng", ""], ["Sun", "Xing", ""]]}, {"id": "1911.12514", "submitter": "Wojciech Michal Matkowski", "authors": "Wojciech Michal Matkowski, Tingting Chai and Adams Wai Kin Kong", "title": "Palmprint Recognition in Uncontrolled and Uncooperative Environment", "comments": "Accepted in the IEEE Transactions on Information Forensics and\n  Security", "journal-ref": null, "doi": "10.1109/TIFS.2019.2945183", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online palmprint recognition and latent palmprint identification are two\nbranches of palmprint studies. The former uses middle-resolution images\ncollected by a digital camera in a well-controlled or contact-based environment\nwith user cooperation for commercial applications and the latter uses\nhigh-resolution latent palmprints collected in crime scenes for forensic\ninvestigation. However, these two branches do not cover some palmprint images\nwhich have the potential for forensic investigation. Due to the prevalence of\nsmartphone and consumer camera, more evidence is in the form of digital images\ntaken in uncontrolled and uncooperative environment, e.g., child pornographic\nimages and terrorist images, where the criminals commonly hide or cover their\nface. However, their palms can be observable. To study palmprint identification\non images collected in uncontrolled and uncooperative environment, a new\npalmprint database is established and an end-to-end deep learning algorithm is\nproposed. The new database named NTU Palmprints from the Internet (NTU-PI-v1)\ncontains 7881 images from 2035 palms collected from the Internet. The proposed\nalgorithm consists of an alignment network and a feature extraction network and\nis end-to-end trainable. The proposed algorithm is compared with the\nstate-of-the-art online palmprint recognition methods and evaluated on three\npublic contactless palmprint databases, IITD, CASIA, and PolyU and two new\ndatabases, NTU-PI-v1 and NTU contactless palmprint database. The experimental\nresults showed that the proposed algorithm outperforms the existing palmprint\nrecognition methods.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 03:38:32 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Matkowski", "Wojciech Michal", ""], ["Chai", "Tingting", ""], ["Kong", "Adams Wai Kin", ""]]}, {"id": "1911.12517", "submitter": "Wen Wang", "authors": "Wen Wang, Lijun Du, Yinxing Gao, Yanzhou Su, Feng Wang, Jian Cheng", "title": "A Discriminative Learned CNN Embedding for Remote Sensing Image Scene\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a discriminatively learned CNN embedding is proposed for remote\nsensing image scene classification. Our proposed siamese network simultaneously\ncomputes the classification loss function and the metric learning loss function\nof the two input images. Specifically, for the classification loss, we use the\nstandard cross-entropy loss function to predict the classes of the images. For\nthe metric learning loss, our siamese network learns to map the intra-class and\ninter-class input pairs to a feature space where intra-class inputs are close\nand inter-class inputs are separated by a margin. Concretely, for remote\nsensing image scene classification, we would like to map images from the same\nscene to feature vectors that are close, and map images from different scenes\nto feature vectors that are widely separated. Experiments are conducted on\nthree different remote sensing image datasets to evaluate the effectiveness of\nour proposed approach. The results demonstrate that the proposed method\nachieves an excellent classification performance.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 03:51:57 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 08:52:39 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Wang", "Wen", ""], ["Du", "Lijun", ""], ["Gao", "Yinxing", ""], ["Su", "Yanzhou", ""], ["Wang", "Feng", ""], ["Cheng", "Jian", ""]]}, {"id": "1911.12527", "submitter": "Kang Zhou", "authors": "Kang Zhou, Shenghua Gao, Jun Cheng, Zaiwang Gu, Huazhu Fu, Zhi Tu,\n  Jianlong Yang, Yitian Zhao, Jiang Liu", "title": "Sparse-GAN: Sparsity-constrained Generative Adversarial Network for\n  Anomaly Detection in Retinal OCT Image", "comments": "Accepted to ISBI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of convolutional neural network, deep learning has shown\nits success for retinal disease detection from optical coherence tomography\n(OCT) images. However, deep learning often relies on large scale labelled data\nfor training, which is oftentimes challenging especially for disease with low\noccurrence. Moreover, a deep learning system trained from data-set with one or\na few diseases is unable to detect other unseen diseases, which limits the\npractical usage of the system in disease screening. To address the limitation,\nwe propose a novel anomaly detection framework termed Sparsity-constrained\nGenerative Adversarial Network (Sparse-GAN) for disease screening where only\nhealthy data are available in the training set. The contributions of Sparse-GAN\nare two-folds: 1) The proposed Sparse-GAN predicts the anomalies in latent\nspace rather than image-level; 2) Sparse-GAN is constrained by a novel Sparsity\nRegularization Net. Furthermore, in light of the role of lesions for disease\nscreening, we present to leverage on an anomaly activation map to show the\nheatmap of lesions. We evaluate our proposed Sparse-GAN on a publicly available\ndataset, and the results show that the proposed method outperforms the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 04:46:48 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 17:31:31 GMT"}, {"version": "v3", "created": "Mon, 3 Feb 2020 15:27:32 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Zhou", "Kang", ""], ["Gao", "Shenghua", ""], ["Cheng", "Jun", ""], ["Gu", "Zaiwang", ""], ["Fu", "Huazhu", ""], ["Tu", "Zhi", ""], ["Yang", "Jianlong", ""], ["Zhao", "Yitian", ""], ["Liu", "Jiang", ""]]}, {"id": "1911.12528", "submitter": "Istvan Fehervari", "authors": "Istvan Fehervari, Avinash Ravichandran, Srikar Appalaraju", "title": "Unbiased Evaluation of Deep Metric Learning Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep metric learning (DML) is a popular approach for images retrieval,\nsolving verification (same or not) problems and addressing open set\nclassification. Arguably, the most common DML approach is with triplet loss,\ndespite significant advances in the area of DML. Triplet loss suffers from\nseveral issues such as collapse of the embeddings, high sensitivity to sampling\nschemes and more importantly a lack of performance when compared to more modern\nmethods. We attribute this adoption to a lack of fair comparisons between\nvarious methods and the difficulty in adopting them for novel problem\nstatements. In this paper, we perform an unbiased comparison of the most\npopular DML baseline methods under same conditions and more importantly, not\nobfuscating any hyper parameter tuning or adjustment needed to favor a\nparticular method. We find, that under equal conditions several older methods\nperform significantly better than previously believed. In fact, our unified\nimplementation of 12 recently introduced DML algorithms achieve state-of-the\nart performance on CUB200, CAR196, and Stanford Online products datasets which\nestablishes a new set of baselines for future DML research. The codebase and\nall tuned hyperparameters will be open-sourced for reproducibility and to serve\nas a source of benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 04:54:14 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Fehervari", "Istvan", ""], ["Ravichandran", "Avinash", ""], ["Appalaraju", "Srikar", ""]]}, {"id": "1911.12529", "submitter": "Hwann-Tzong Chen", "authors": "Ting-I Hsieh, Yi-Chen Lo, Hwann-Tzong Chen, Tyng-Luh Liu", "title": "One-Shot Object Detection with Co-Attention and Co-Excitation", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to tackle the challenging problem of one-shot object\ndetection. Given a query image patch whose class label is not included in the\ntraining data, the goal of the task is to detect all instances of the same\nclass in a target image. To this end, we develop a novel {\\em co-attention and\nco-excitation} (CoAE) framework that makes contributions in three key technical\naspects. First, we propose to use the non-local operation to explore the\nco-attention embodied in each query-target pair and yield region proposals\naccounting for the one-shot situation. Second, we formulate a\nsqueeze-and-co-excitation scheme that can adaptively emphasize correlated\nfeature channels to help uncover relevant proposals and eventually the target\nobjects. Third, we design a margin-based ranking loss for implicitly learning a\nmetric to predict the similarity of a region proposal to the underlying query,\nno matter its class label is seen or unseen in training. The resulting model is\ntherefore a two-stage detector that yields a strong baseline on both VOC and\nMS-COCO under one-shot setting of detecting objects from both seen and\nnever-seen classes. Codes are available at\nhttps://github.com/timy90022/One-Shot-Object-Detection.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 05:14:23 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Hsieh", "Ting-I", ""], ["Lo", "Yi-Chen", ""], ["Chen", "Hwann-Tzong", ""], ["Liu", "Tyng-Luh", ""]]}, {"id": "1911.12546", "submitter": "Christopher Ren", "authors": "Christopher X. Ren, Amanda Ziemann, Alice M.S. Durieux, James Theiler", "title": "Cycle-Consistent Adversarial Networks for Realistic Pervasive Change\n  Generation in Remote Sensing Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a new method of generating realistic pervasive changes\nin the context of evaluating the effectiveness of change detection algorithms\nin controlled settings. The method, a cycle-consistent adversarial network\n(CycleGAN), requires low quantities of training data to generate realistic\nchanges. Here we show an application of CycleGAN in creating realistic\nsnow-covered scenes of multispectral Sentinel-2 imagery, and demonstrate how\nthese images can be used as a test bed for anomalous change detection\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 06:03:18 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 00:13:16 GMT"}, {"version": "v3", "created": "Fri, 15 May 2020 16:18:04 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Ren", "Christopher X.", ""], ["Ziemann", "Amanda", ""], ["Durieux", "Alice M. S.", ""], ["Theiler", "James", ""]]}, {"id": "1911.12552", "submitter": "Ye Lin", "authors": "Ye Lin, Keren Fu, Shenggui Ling, Cheng Peng", "title": "Unsupervised Many-to-Many Image-to-Image Translation Across Multiple\n  Domains", "comments": "13 pages, 14 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised multi-domain image-to-image translation aims to synthesis images\namong multiple domains without labeled data, which is more general and\ncomplicated than one-to-one image mapping. However, existing methods mainly\nfocus on reducing the large costs of modeling and do not pay enough attention\nto the quality of generated images. In some target domains, their translation\nresults may not be expected or even it has model collapse. To improve the image\nquality, we propose an effective many-to-many mapping framework for\nunsupervised multi-domain image-to-image translation. There are two key aspects\nin our method. The first is a proposed many-to-many architecture with only one\ndomain-shared encoder and several domain-specialized decoders to effectively\nand simultaneously translate images across multiple domains. The second is two\nproposed constraints extended from one-to-one mappings to further help improve\nthe generation. All the evaluations demonstrate our framework is superior to\nexisting methods and provides an effective solution for multi-domain\nimage-to-image translation.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 06:39:35 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 02:20:43 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Lin", "Ye", ""], ["Fu", "Keren", ""], ["Ling", "Shenggui", ""], ["Peng", "Cheng", ""]]}, {"id": "1911.12588", "submitter": "Wei Li", "authors": "Rong Zhang, Wei Li, Peng Wang, Chenye Guan, Jin Fang, Yuhang Song,\n  Jinhui Yu, Baoquan Chen, Weiwei Xu, Ruigang Yang", "title": "AutoRemover: Automatic Object Removal for Autonomous Driving Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the need for photo-realistic simulation in autonomous driving,\nin this paper we present a video inpainting algorithm \\emph{AutoRemover},\ndesigned specifically for generating street-view videos without any moving\nobjects. In our setup we have two challenges: the first is the shadow, shadows\nare usually unlabeled but tightly coupled with the moving objects. The second\nis the large ego-motion in the videos. To deal with shadows, we build up an\nautonomous driving shadow dataset and design a deep neural network to detect\nshadows automatically. To deal with large ego-motion, we take advantage of the\nmulti-source data, in particular the 3D data, in autonomous driving. More\nspecifically, the geometric relationship between frames is incorporated into an\ninpainting deep neural network to produce high-quality structurally consistent\nvideo output. Experiments show that our method outperforms other\nstate-of-the-art (SOTA) object removal algorithms, reducing the RMSE by over\n$19\\%$.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 08:29:41 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Zhang", "Rong", ""], ["Li", "Wei", ""], ["Wang", "Peng", ""], ["Guan", "Chenye", ""], ["Fang", "Jin", ""], ["Song", "Yuhang", ""], ["Yu", "Jinhui", ""], ["Chen", "Baoquan", ""], ["Xu", "Weiwei", ""], ["Yang", "Ruigang", ""]]}, {"id": "1911.12597", "submitter": "Luca Caltagirone", "authors": "Luca Caltagirone, Lennart Svensson, Mattias Wahde, Martin Sanfridson", "title": "Lidar-Camera Co-Training for Semi-Supervised Road Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in the field of machine learning and computer vision have\nenabled the development of fast and accurate road detectors. Commonly such\nsystems are trained within a supervised learning paradigm where both an input\nsensor's data and the corresponding ground truth label must be provided. The\ntask of generating labels is commonly carried out by human annotators and it is\nnotoriously time consuming and expensive. In this work, it is shown that a\nsemi-supervised approach known as co-training can provide significant F1-score\naverage improvements compared to supervised learning. In co-training, two\nclassifiers acting on different views of the data cooperatively improve each\nother's performance by leveraging unlabeled examples. Depending on the amount\nof labeled data used, the improvements ranged from 1.12 to 6.10 percentage\npoints for a camera-based road detector and from 1.04 to 8.14 percentage points\nfor a lidar-based road detector. Lastly, the co-training algorithm is validated\non the KITTI road benchmark, achieving high performance using only 36 labeled\ntraining examples together with several thousands unlabeled ones.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 08:52:27 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Caltagirone", "Luca", ""], ["Svensson", "Lennart", ""], ["Wahde", "Mattias", ""], ["Sanfridson", "Martin", ""]]}, {"id": "1911.12641", "submitter": "Damian Kaliroff", "authors": "Damian Kaliroff and Guy Gilboa", "title": "PhIT-Net: Photo-consistent Image Transform for Robust Illumination\n  Invariant Matching", "comments": "Modified title. Added figures in section 3 for better understanding\n  of the general concept. Added table summarizing graphs. New paper format (two\n  columns)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new and completely data-driven approach for generating a\nphoto-consistent image transform. We show that simple classical algorithms\nwhich operate in the transform domain become extremely resilient to\nillumination changes. This considerably improves matching accuracy,\noutperforming the use of state-of-the-art invariant representations as well as\nnew matching methods based on deep features. The transform is obtained by\ntraining a neural network with a specialized triplet loss, designed to\nemphasize actual scene changes while attenuating illumination changes. The\ntransform yields an illumination invariant representation, structured as an\nimage map, which is highly flexible and can be easily used for various tasks.\nWe point out that the utility of our method is not restricted to handling\nillumination invariance, and that it may be applied for generating\nrepresentations which are invariant to additional types of nuisance, undesired,\nimage variants\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 10:55:55 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 15:20:30 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 12:02:28 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Kaliroff", "Damian", ""], ["Gilboa", "Guy", ""]]}, {"id": "1911.12667", "submitter": "Humam Alwassel", "authors": "Humam Alwassel, Dhruv Mahajan, Bruno Korbar, Lorenzo Torresani,\n  Bernard Ghanem, Du Tran", "title": "Self-Supervised Learning by Cross-Modal Audio-Video Clustering", "comments": "Accepted to NeurIPS 2020 (spotlight presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual and audio modalities are highly correlated, yet they contain different\ninformation. Their strong correlation makes it possible to predict the\nsemantics of one from the other with good accuracy. Their intrinsic differences\nmake cross-modal prediction a potentially more rewarding pretext task for\nself-supervised learning of video and audio representations compared to\nwithin-modality learning. Based on this intuition, we propose Cross-Modal Deep\nClustering (XDC), a novel self-supervised method that leverages unsupervised\nclustering in one modality (e.g., audio) as a supervisory signal for the other\nmodality (e.g., video). This cross-modal supervision helps XDC utilize the\nsemantic correlation and the differences between the two modalities. Our\nexperiments show that XDC outperforms single-modality clustering and other\nmulti-modal variants. XDC achieves state-of-the-art accuracy among\nself-supervised methods on multiple video and audio benchmarks. Most\nimportantly, our video model pretrained on large-scale unlabeled data\nsignificantly outperforms the same model pretrained with full-supervision on\nImageNet and Kinetics for action recognition on HMDB51 and UCF101. To the best\nof our knowledge, XDC is the first self-supervised learning method that\noutperforms large-scale fully-supervised pretraining for action recognition on\nthe same architecture.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 12:17:36 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 19:06:14 GMT"}, {"version": "v3", "created": "Mon, 26 Oct 2020 14:02:35 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Alwassel", "Humam", ""], ["Mahajan", "Dhruv", ""], ["Korbar", "Bruno", ""], ["Torresani", "Lorenzo", ""], ["Ghanem", "Bernard", ""], ["Tran", "Du", ""]]}, {"id": "1911.12675", "submitter": "Xu Shen", "authors": "Xu Shen, Xinmei Tian, Tongliang Liu, Fang Xu and Dacheng Tao", "title": "Continuous Dropout", "comments": "Accepted by TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout has been proven to be an effective algorithm for training robust deep\nnetworks because of its ability to prevent overfitting by avoiding the\nco-adaptation of feature detectors. Current explanations of dropout include\nbagging, naive Bayes, regularization, and sex in evolution. According to the\nactivation patterns of neurons in the human brain, when faced with different\nsituations, the firing rates of neurons are random and continuous, not binary\nas current dropout does. Inspired by this phenomenon, we extend the traditional\nbinary dropout to continuous dropout. On the one hand, continuous dropout is\nconsiderably closer to the activation characteristics of neurons in the human\nbrain than traditional binary dropout. On the other hand, we demonstrate that\ncontinuous dropout has the property of avoiding the co-adaptation of feature\ndetectors, which suggests that we can extract more independent feature\ndetectors for model averaging in the test stage. We introduce the proposed\ncontinuous dropout to a feedforward neural network and comprehensively compare\nit with binary dropout, adaptive dropout, and DropConnect on MNIST, CIFAR-10,\nSVHN, NORB, and ILSVRC-12. Thorough experiments demonstrate that our method\nperforms better in preventing the co-adaptation of feature detectors and\nimproves test performance. The code is available at:\nhttps://github.com/jasonustc/caffe-multigpu/tree/dropout.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 12:37:48 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Shen", "Xu", ""], ["Tian", "Xinmei", ""], ["Liu", "Tongliang", ""], ["Xu", "Fang", ""], ["Tao", "Dacheng", ""]]}, {"id": "1911.12676", "submitter": "Maximilian Jaritz", "authors": "Maximilian Jaritz, Tuan-Hung Vu, Raoul de Charette, \\'Emilie Wirbel,\n  Patrick P\\'erez", "title": "xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic\n  Segmentation", "comments": "Accepted at CVPR 2020. For a demo video, see http://tiny.cc/xmuda", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised Domain Adaptation (UDA) is crucial to tackle the lack of\nannotations in a new domain. There are many multi-modal datasets, but most UDA\napproaches are uni-modal. In this work, we explore how to learn from\nmulti-modality and propose cross-modal UDA (xMUDA) where we assume the presence\nof 2D images and 3D point clouds for 3D semantic segmentation. This is\nchallenging as the two input spaces are heterogeneous and can be impacted\ndifferently by domain shift. In xMUDA, modalities learn from each other through\nmutual mimicking, disentangled from the segmentation objective, to prevent the\nstronger modality from adopting false predictions from the weaker one. We\nevaluate on new UDA scenarios including day-to-night, country-to-country and\ndataset-to-dataset, leveraging recent autonomous driving datasets. xMUDA brings\nlarge improvements over uni-modal UDA on all tested scenarios, and is\ncomplementary to state-of-the-art UDA techniques. Code is available at\nhttps://github.com/valeoai/xmuda.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 12:38:05 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 19:24:04 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Jaritz", "Maximilian", ""], ["Vu", "Tuan-Hung", ""], ["de Charette", "Raoul", ""], ["Wirbel", "\u00c9milie", ""], ["P\u00e9rez", "Patrick", ""]]}, {"id": "1911.12682", "submitter": "Xu Shen", "authors": "Xu Shen, Xinmei Tian, Shaoyan Sun, Dacheng Tao", "title": "Patch Reordering: a Novel Way to Achieve Rotation and Translation\n  Invariance in Convolutional Neural Networks", "comments": "Accepted AAAI17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have demonstrated state-of-the-art\nperformance on many visual recognition tasks. However, the combination of\nconvolution and pooling operations only shows invariance to small local\nlocation changes in meaningful objects in input. Sometimes, such networks are\ntrained using data augmentation to encode this invariance into the parameters,\nwhich restricts the capacity of the model to learn the content of these\nobjects. A more efficient use of the parameter budget is to encode rotation or\ntranslation invariance into the model architecture, which relieves the model\nfrom the need to learn them. To enable the model to focus on learning the\ncontent of objects other than their locations, we propose to conduct patch\nranking of the feature maps before feeding them into the next layer. When patch\nranking is combined with convolution and pooling operations, we obtain\nconsistent representations despite the location of meaningful objects in input.\nWe show that the patch ranking module improves the performance of the CNN on\nmany benchmark tasks, including MNIST digit recognition, large-scale image\nrecognition, and image retrieval. The code is available at\nhttps://github.com//jasonustc/caffe-multigpu/tree/TICNN .\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 12:49:57 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Shen", "Xu", ""], ["Tian", "Xinmei", ""], ["Sun", "Shaoyan", ""], ["Tao", "Dacheng", ""]]}, {"id": "1911.12688", "submitter": "Giulia Orr\\`u", "authors": "Giulia Orr\\`u, Gian Luca Marcialis, Fabio Roli", "title": "A novel classification-selection approach for the self updating of\n  template-based face recognition systems", "comments": "This is an original manuscript of an article published by Elsevier in\n  Pattern Recognition on 27 November 2019. Available online:\n  https://doi.org/10.1016/j.patcog.2019.107121", "journal-ref": null, "doi": "10.1016/j.patcog.2019.107121", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The boosting on the need of security notably increased the amount of possible\nfacial recognition applications, especially due to the success of the Internet\nof Things (IoT) paradigm. However, although handcrafted and deep\nlearning-inspired facial features reached a significant level of compactness\nand expressive power, the facial recognition performance still suffers from\nintra-class variations such as ageing, facial expressions, lighting changes,\nand pose. These variations cannot be captured in a single acquisition and\nrequire multiple acquisitions of long duration, which are expensive and need a\nhigh level of collaboration from the users. Among others, self-update\nalgorithms have been proposed in order to mitigate these problems.\nSelf-updating aims to add novel templates to the users' gallery among the\ninputs submitted during system operations. Consequently, computational\ncomplexity and storage space tend to be among the critical requirements of\nthese algorithms. The present paper deals with the above problems by a novel\ntemplate-based self-update algorithm, able to keep over time the expressive\npower of a limited set of templates stored in the system database. The\nrationale behind the proposed approach is in the working hypothesis that a\ndominating mode characterises the features' distribution given the client.\nTherefore, the key point is to select the best templates around that mode. We\npropose two methods, which are tested on systems based on handcrafted features\nand deep-learning-inspired autoencoders at the state-of-the-art. Three\nbenchmark data sets are used. Experimental results confirm that, by effective\nand compact feature sets which can support our working hypothesis, the proposed\nclassification-selection approaches overcome the problem of manual updating\nand, in case, stringent computational requirements.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 12:58:17 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Orr\u00f9", "Giulia", ""], ["Marcialis", "Gian Luca", ""], ["Roli", "Fabio", ""]]}, {"id": "1911.12706", "submitter": "Michael Werman", "authors": "Danail Brezov and Michael Werman", "title": "Cameras Viewing Cameras Geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A basic problem in computer vision is to understand the structure of a\nreal-world scene given several images of it. Here we study several theoretical\naspects of the intra multi-view geometry of calibrated cameras when all that\nthey can reliably recognize is each other. With the proliferation of wearable\ncameras, autonomous vehicles and drones, the geometry of these multiple cameras\nis a timely and relevant problem to study.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 13:40:06 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Brezov", "Danail", ""], ["Werman", "Michael", ""]]}, {"id": "1911.12709", "submitter": "Theodora Kontogianni", "authors": "Theodora Kontogianni, Michael Gygli, Jasper Uijlings, Vittorio Ferrari", "title": "Continuous Adaptation for Interactive Object Segmentation by Learning\n  from Corrections", "comments": "ECCV 2020 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In interactive object segmentation a user collaborates with a computer vision\nmodel to segment an object. Recent works employ convolutional neural networks\nfor this task: Given an image and a set of corrections made by the user as\ninput, they output a segmentation mask. These approaches achieve strong\nperformance by training on large datasets but they keep the model parameters\nunchanged at test time. Instead, we recognize that user corrections can serve\nas sparse training examples and we propose a method that capitalizes on that\nidea to update the model parameters on-the-fly to the data at hand. Our\napproach enables the adaptation to a particular object and its background, to\ndistributions shifts in a test set, to specific object classes, and even to\nlarge domain changes, where the imaging modality changes between training and\ntesting. We perform extensive experiments on 8 diverse datasets and show:\nCompared to a model with frozen parameters, our method reduces the required\ncorrections (i) by 9%-30% when distribution shifts are small between training\nand testing; (ii) by 12%-44% when specializing to a specific class; (iii) and\nby 60% and 77% when we completely change domain between training and testing.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 13:43:54 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 16:21:08 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2020 15:42:01 GMT"}, {"version": "v4", "created": "Sun, 8 Nov 2020 15:55:14 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Kontogianni", "Theodora", ""], ["Gygli", "Michael", ""], ["Uijlings", "Jasper", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1911.12721", "submitter": "Jaeyoung Yoo", "authors": "Jaeyoung Yoo, Hojun Lee, Inseop Chung, Geonseok Seo, Nojun Kwak", "title": "Density-based Object Detection: Learning Bounding Boxes without Ground\n  Truth Assignment", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-object detection using neural networks, most methods train a network\nbased on ground truth assignment, which makes the training too heuristic and\ncomplicated. In this paper, we reformulate the multi-object detection task as a\nproblem of density estimation of bounding boxes. Instead of using a\nground-truth-assignment-based method, we train a network by estimating the\nprobability density of bounding boxes in an input image using a mixture model.\nFor this purpose, we propose a novel network for object detection called\nMixture Density Object Detector (MDOD), and the corresponding objective\nfunction for the density-estimation-based training. Unlike the\nground-truth-assignment-based methods, our proposed method gets rid of the\ncumbersome processes of matching between ground truth boxes and their\npredictions as well as the heuristic anchor design. It is also free from the\nproblem of foreground-background imbalance. We applied MDOD to MS COCO dataset.\nOur proposed method not only deals with multi-object detection problems in a\nnew approach, but also improves detection performances through MDOD. Code will\nbe available.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 14:08:55 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 11:24:56 GMT"}, {"version": "v3", "created": "Sun, 4 Oct 2020 12:37:30 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Yoo", "Jaeyoung", ""], ["Lee", "Hojun", ""], ["Chung", "Inseop", ""], ["Seo", "Geonseok", ""], ["Kwak", "Nojun", ""]]}, {"id": "1911.12739", "submitter": "Mingyu Ding", "authors": "Mingyu Ding, Zhe Wang, Bolei Zhou, Jianping Shi, Zhiwu Lu, Ping Luo", "title": "Every Frame Counts: Joint Learning of Video Segmentation and Optical\n  Flow", "comments": "Published in AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge for video semantic segmentation is the lack of labeled\ndata. In most benchmark datasets, only one frame of a video clip is annotated,\nwhich makes most supervised methods fail to utilize information from the rest\nof the frames. To exploit the spatio-temporal information in videos, many\nprevious works use pre-computed optical flows, which encode the temporal\nconsistency to improve the video segmentation. However, the video segmentation\nand optical flow estimation are still considered as two separate tasks. In this\npaper, we propose a novel framework for joint video semantic segmentation and\noptical flow estimation. Semantic segmentation brings semantic information to\nhandle occlusion for more robust optical flow estimation, while the\nnon-occluded optical flow provides accurate pixel-level temporal\ncorrespondences to guarantee the temporal consistency of the segmentation.\nMoreover, our framework is able to utilize both labeled and unlabeled frames in\nthe video through joint training, while no additional calculation is required\nin inference. Extensive experiments show that the proposed model makes the\nvideo semantic segmentation and optical flow estimation benefit from each other\nand outperforms existing methods under the same settings in both tasks.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 15:01:35 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Ding", "Mingyu", ""], ["Wang", "Zhe", ""], ["Zhou", "Bolei", ""], ["Shi", "Jianping", ""], ["Lu", "Zhiwu", ""], ["Luo", "Ping", ""]]}, {"id": "1911.12747", "submitter": "Joon Son Chung", "authors": "Triantafyllos Afouras, Joon Son Chung, Andrew Zisserman", "title": "ASR is all you need: cross-modal distillation for lip reading", "comments": "ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this work is to train strong models for visual speech recognition\nwithout requiring human annotated ground truth data. We achieve this by\ndistilling from an Automatic Speech Recognition (ASR) model that has been\ntrained on a large-scale audio-only corpus. We use a cross-modal distillation\nmethod that combines Connectionist Temporal Classification (CTC) with a\nframe-wise cross-entropy loss. Our contributions are fourfold: (i) we show that\nground truth transcriptions are not necessary to train a lip reading system;\n(ii) we show how arbitrary amounts of unlabelled video data can be leveraged to\nimprove performance; (iii) we demonstrate that distillation significantly\nspeeds up training; and, (iv) we obtain state-of-the-art results on the\nchallenging LRS2 and LRS3 datasets for training only on publicly available\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 15:15:27 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 06:53:12 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Afouras", "Triantafyllos", ""], ["Chung", "Joon Son", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1911.12763", "submitter": "Niall Twomey", "authors": "Mikhail Fain, Niall Twomey, Andrey Ponikar, Ryan Fox and Danushka\n  Bollegala", "title": "Dividing and Conquering Cross-Modal Recipe Retrieval: from Nearest\n  Neighbours Baselines to SoTA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel non-parametric method for cross-modal recipe retrieval\nwhich is applied on top of precomputed image and text embeddings. By combining\nour method with standard approaches for building image and text encoders,\ntrained independently with a self-supervised classification objective, we\ncreate a baseline model which outperforms most existing methods on a\nchallenging image-to-recipe task. We also use our method for comparing image\nand text encoders trained using different modern approaches, thus addressing\nthe issues hindering the development of novel methods for cross-modal recipe\nretrieval. We demonstrate how to use the insights from model comparison and\nextend our baseline model with standard triplet loss that improves\nstate-of-the-art on the Recipe1M dataset by a large margin, while using only\nprecomputed features and with much less complexity than existing methods.\nFurther, our approach readily generalizes beyond recipe retrieval to other\nchallenging domains, achieving state-of-the-art performance on Politics and\nGoodNews cross-modal retrieval tasks.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 16:00:09 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 15:32:46 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Fain", "Mikhail", ""], ["Twomey", "Niall", ""], ["Ponikar", "Andrey", ""], ["Fox", "Ryan", ""], ["Bollegala", "Danushka", ""]]}, {"id": "1911.12780", "submitter": "Colin Paterson", "authors": "Colin Paterson, Radu Calinescu and Chiara Picardi", "title": "Detection and Mitigation of Rare Subclasses in Deep Neural Network\n  Classifiers", "comments": "8 pages, 7 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regions of high-dimensional input spaces that are underrepresented in\ntraining datasets reduce machine-learnt classifier performance, and may lead to\ncorner cases and unwanted bias for classifiers used in decision making systems.\nWhen these regions belong to otherwise well-represented classes, their presence\nand negative impact are very hard to identify. We propose an approach for the\ndetection and mitigation of such rare subclasses in deep neural network\nclassifiers. The new approach is underpinned by an easy-to-compute commonality\nmetric that supports the detection of rare subclasses, and comprises methods\nfor reducing the impact of these subclasses during both model training and\nmodel exploitation. We demonstrate our approach using two well-known datasets,\nMNIST's handwritten digits and Kaggle's cats/dogs, identifying rare subclasses\nand producing models which compensate for subclass rarity. In addition we\ndemonstrate how our run-time approach increases the ability of users to\nidentify samples likely to be misclassified at run-time.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 16:41:35 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 15:06:42 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Paterson", "Colin", ""], ["Calinescu", "Radu", ""], ["Picardi", "Chiara", ""]]}, {"id": "1911.12796", "submitter": "Kaidi Xu", "authors": "Shaokai Ye, Kailu Wu, Mu Zhou, Yunfei Yang, Sia huat Tan, Kaidi Xu,\n  Jiebo Song, Chenglong Bao, Kaisheng Ma", "title": "Light-weight Calibrator: a Separable Component for Unsupervised Domain\n  Adaptation", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing domain adaptation methods aim at learning features that can be\ngeneralized among domains. These methods commonly require to update source\nclassifier to adapt to the target domain and do not properly handle the trade\noff between the source domain and the target domain. In this work, instead of\ntraining a classifier to adapt to the target domain, we use a separable\ncomponent called data calibrator to help the fixed source classifier recover\ndiscrimination power in the target domain, while preserving the source domain's\nperformance. When the difference between two domains is small, the source\nclassifier's representation is sufficient to perform well in the target domain\nand outperforms GAN-based methods in digits. Otherwise, the proposed method can\nleverage synthetic images generated by GANs to boost performance and achieve\nstate-of-the-art performance in digits datasets and driving scene semantic\nsegmentation. Our method empirically reveals that certain intriguing hints,\nwhich can be mitigated by adversarial attack to domain discriminators, are one\nof the sources for performance degradation under the domain shift.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 17:18:03 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 14:12:02 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Ye", "Shaokai", ""], ["Wu", "Kailu", ""], ["Zhou", "Mu", ""], ["Yang", "Yunfei", ""], ["Tan", "Sia huat", ""], ["Xu", "Kaidi", ""], ["Song", "Jiebo", ""], ["Bao", "Chenglong", ""], ["Ma", "Kaisheng", ""]]}, {"id": "1911.12801", "submitter": "Ziyun Wang", "authors": "Ziyun Wang", "title": "Motion Equivariance OF Event-based Camera Data with the Temporal\n  Normalization Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we focus on using convolution neural networks (CNN) to perform\nobject recognition on the event data. In object recognition, it is important\nfor a neural network to be robust to the variations of the data during testing.\nFor traditional cameras, translations are well handled because CNNs are\nnaturally equivariant to translations. However, because event cameras record\nthe change of light intensity of an image, the geometric shape of event volumes\nwill not only depend on the objects but also on their relative motions with\nrespect to the camera. The deformation of the events caused by motions causes\nthe CNN to be less robust to unseen motions during inference. To address this\nproblem, we would like to explore the equivariance property of CNNs, a\nwell-studied area that demonstrates to produce predictable deformation of\nfeatures under certain transformations of the input image.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 17:26:36 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Wang", "Ziyun", ""]]}, {"id": "1911.12836", "submitter": "Paul Voigtlaender", "authors": "Paul Voigtlaender, Jonathon Luiten, Philip H.S. Torr, and Bastian\n  Leibe", "title": "Siam R-CNN: Visual Tracking by Re-Detection", "comments": "CVPR 2020 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Siam R-CNN, a Siamese re-detection architecture which unleashes\nthe full power of two-stage object detection approaches for visual object\ntracking. We combine this with a novel tracklet-based dynamic programming\nalgorithm, which takes advantage of re-detections of both the first-frame\ntemplate and previous-frame predictions, to model the full history of both the\nobject to be tracked and potential distractor objects. This enables our\napproach to make better tracking decisions, as well as to re-detect tracked\nobjects after long occlusion. Finally, we propose a novel hard example mining\nstrategy to improve Siam R-CNN's robustness to similar looking objects. Siam\nR-CNN achieves the current best performance on ten tracking benchmarks, with\nespecially strong results for long-term tracking. We make our code and models\navailable at www.vision.rwth-aachen.de/page/siamrcnn.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 19:21:34 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 11:09:54 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Voigtlaender", "Paul", ""], ["Luiten", "Jonathon", ""], ["Torr", "Philip H. S.", ""], ["Leibe", "Bastian", ""]]}, {"id": "1911.12850", "submitter": "Basel Alyafi", "authors": "Basel Alyafi, Oliver Diaz, Joan C Vilanova, Javier del Riego, Robert\n  Marti", "title": "Quality analysis of DCGAN-generated mammography lesions", "comments": "Abstract accepted in the International Workshop Breast Imaging IWBI\n  (2020), 4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image synthesis has gained a great focus recently, especially after\nthe introduction of Generative Adversarial Networks (GANs). GANs have been used\nwidely to provide anatomically-plausible and diverse samples for augmentation\nand other applications, including segmentation and super resolution. In our\nprevious work, Deep Convolutional GANs were used to generate synthetic\nmammogram lesions, masses mainly, that could enhance the classification\nperformance in imbalanced datasets. In this new work, a deeper investigation\nwas carried out to explore other aspects of the generated images evaluation,\ni.e., realism, feature space distribution, and observers studies. t-Stochastic\nNeighbor Embedding (t-SNE) was used to reduce the dimensionality of real and\nfake images to enable 2D visualisations. Additionally, two expert radiologists\nperformed a realism-evaluation study. Visualisations showed that the generated\nimages have a similar feature distribution of the real ones, avoiding outliers.\nMoreover, Receiver Operating Characteristic (ROC) curve showed that the\nradiologists could not, in many cases, distinguish between synthetic and real\nlesions, giving 48% and 61% accuracies in a balanced sample set.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 20:11:19 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 11:40:09 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Alyafi", "Basel", ""], ["Diaz", "Oliver", ""], ["Vilanova", "Joan C", ""], ["del Riego", "Javier", ""], ["Marti", "Robert", ""]]}, {"id": "1911.12861", "submitter": "Peihao Zhu", "authors": "Peihao Zhu, Rameen Abdal, Yipeng Qin, Peter Wonka", "title": "SEAN: Image Synthesis with Semantic Region-Adaptive Normalization", "comments": "Accepted as a CVPR 2020 oral paper. The interactive demo is available\n  at https://youtu.be/0Vbj9xFgoUw", "journal-ref": null, "doi": "10.1109/CVPR42600.2020.00515", "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose semantic region-adaptive normalization (SEAN), a simple but\neffective building block for Generative Adversarial Networks conditioned on\nsegmentation masks that describe the semantic regions in the desired output\nimage. Using SEAN normalization, we can build a network architecture that can\ncontrol the style of each semantic region individually, e.g., we can specify\none style reference image per region. SEAN is better suited to encode,\ntransfer, and synthesize style than the best previous method in terms of\nreconstruction quality, variability, and visual quality. We evaluate SEAN on\nmultiple datasets and report better quantitative metrics (e.g. FID, PSNR) than\nthe current state of the art. SEAN also pushes the frontier of interactive\nimage editing. We can interactively edit images by changing segmentation masks\nor the style for any given region. We can also interpolate styles from two\nreference images per region.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 20:54:35 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 13:47:09 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Zhu", "Peihao", ""], ["Abdal", "Rameen", ""], ["Qin", "Yipeng", ""], ["Wonka", "Peter", ""]]}, {"id": "1911.12870", "submitter": "Veniamin Morgenshtern I.", "authors": "Matthias Sonntag and Veniamin I. Morgenshtern", "title": "Region segmentation via deep learning and convex optimization", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method to segment regions in three-dimensional\npoint clouds. We assume that (i) the shape and the number of regions in the\npoint cloud are not known and (ii) the point cloud may be noisy. The method\nconsists of two steps. In the first step we use a deep neural network to\npredict the probability that a pair of small patches from the point cloud\nbelongs to the same region. In the second step, we use a convex-optimization\nbased method to improve the predictions of the network by enforcing consistency\nconstraints. We evaluate the accuracy of our method on a custom dataset of\nconvex polyhedra, where the regions correspond to the faces of the polyhedra.\nThe method can be seen as a robust and flexible alternative to the famous\nregion growing segmentation algorithm. All reported results are reproducible\nand come with easy to use code that could serve as a baseline for future\nresearch.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 21:42:21 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Sonntag", "Matthias", ""], ["Morgenshtern", "Veniamin I.", ""]]}, {"id": "1911.12885", "submitter": "Shi Qiu", "authors": "Shi Qiu, Saeed Anwar and Nick Barnes", "title": "Geometric Back-projection Network for Point Cloud Classification", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the basic task of point cloud analysis, classification is fundamental but\nalways challenging. To address some unsolved problems of existing methods, we\npropose a network that captures geometric features of point clouds for better\nrepresentations. To achieve this, on the one hand, we enrich the geometric\ninformation of points in low-level 3D space explicitly. On the other hand, we\napply CNN-based structures in high-level feature spaces to learn local\ngeometric context implicitly. Specifically, we leverage an idea of\nerror-correcting feedback structure to capture the local features of point\nclouds comprehensively. Furthermore, an attention module based on channel\naffinity assists the feature map to avoid possible redundancy by emphasizing\nits distinct channels. The performance on both synthetic and real-world point\nclouds datasets demonstrate the superiority and applicability of our network.\nComparing with other state-of-the-art methods, our approach balances accuracy\nand efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 22:37:06 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 02:54:28 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2020 06:23:35 GMT"}, {"version": "v4", "created": "Thu, 3 Sep 2020 12:29:50 GMT"}, {"version": "v5", "created": "Tue, 13 Apr 2021 05:57:13 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Qiu", "Shi", ""], ["Anwar", "Saeed", ""], ["Barnes", "Nick", ""]]}, {"id": "1911.12886", "submitter": "Kumar Abhishek", "authors": "Weina Jin, Mostafa Fatehi, Kumar Abhishek, Mayur Mallya, Brian Toyota,\n  and Ghassan Hamarneh", "title": "Artificial Intelligence in Glioma Imaging: Challenges and Advances", "comments": "31 pages, 6 figures. Accepted for publication in the Journal of\n  Neural Engineering", "journal-ref": null, "doi": "10.1088/1741-2552/ab8131", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Primary brain tumors including gliomas continue to pose significant\nmanagement challenges to clinicians. While the presentation, the pathology, and\nthe clinical course of these lesions are variable, the initial investigations\nare usually similar. Patients who are suspected to have a brain tumor will be\nassessed with computed tomography (CT) and magnetic resonance imaging (MRI).\nThe imaging findings are used by neurosurgeons to determine the feasibility of\nsurgical resection and plan such an undertaking. Imaging studies are also an\nindispensable tool in tracking tumor progression or its response to treatment.\nAs these imaging studies are non-invasive, relatively cheap and accessible to\npatients, there have been many efforts over the past two decades to increase\nthe amount of clinically-relevant information that can be extracted from brain\nimaging. Most recently, artificial intelligence (AI) techniques have been\nemployed to segment and characterize brain tumors, as well as to detect\nprogression or treatment-response. However, the clinical utility of such\nendeavours remains limited due to challenges in data collection and annotation,\nmodel training, and the reliability of AI-generated information.\n  We provide a review of recent advances in addressing the above challenges.\nFirst, to overcome the challenge of data paucity, different image imputation\nand synthesis techniques along with annotation collection efforts are\nsummarized. Next, various training strategies are presented to meet multiple\ndesiderata, such as model performance, generalization ability, data privacy\nprotection, and learning with sparse annotations. Finally, standardized\nperformance evaluation and model interpretability methods have been reviewed.\nWe believe that these technical approaches will facilitate the development of a\nfully-functional AI tool in the clinical care of patients with gliomas.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 22:40:56 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 02:47:22 GMT"}, {"version": "v3", "created": "Fri, 10 Apr 2020 10:03:23 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Jin", "Weina", ""], ["Fatehi", "Mostafa", ""], ["Abhishek", "Kumar", ""], ["Mallya", "Mayur", ""], ["Toyota", "Brian", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "1911.12889", "submitter": "Hanwen Kang", "authors": "Hanwen Kang and Chao Chen", "title": "Fruit Detection, Segmentation and 3D Visualisation of Environments in\n  Apple Orchards", "comments": "17 pages, 7 figures", "journal-ref": "Computers and Electronics in Agriculture 171 (2020) 105302", "doi": "10.1016/j.compag.2020.105302", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic harvesting of fruits in orchards is a challenging task, since high\ndensity and overlapping of fruits and branches can heavily impact the success\nrate of robotic harvesting. Therefore, the vision system is demanded to provide\ncomprehensive information of the working environment to guide the manipulator\nand gripping system to successful detach the target fruits. In this study, a\ndeep learning based one-stage detector DaSNet-V2 is developed to perform the\nmulti-task vision sensing in the working environment of apple orchards.\nDaSNet-V2 combines the detection and instance segmentation of fruits and\nsemantic segmentation of branch into a single network architecture. Meanwhile,\na light-weight backbone network LW-net is utilised in the DaSNet-V2 model to\nimprove the computational efficiency of the model. In the experiment, DaSNet-V2\nis tested and evaluated on the RGB-D images of the orchard. From the experiment\nresults, DaSNet-V2 with lightweight backbone achieves 0.844, 0.858, and 0.795\non the F 1 score of the detection, and mean intersection of union on the\ninstance segmentation of fruits and semantic segmentation of branches,\nrespectively. To provide a direct-viewing of the working environment in\norchards, the obtained sensing results are illustrated by 3D visualisation .\nThe robustness and efficiency of the DaSNet-V2 in detection and segmentation\nare validated by the experiments in the real-environment of apple orchard.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 22:49:48 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Kang", "Hanwen", ""], ["Chen", "Chao", ""]]}, {"id": "1911.12903", "submitter": "Renee Su", "authors": "Renee Su and Rong Chen", "title": "Land Cover Change Detection via Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a change detection method that identifies land cover\nchanges from aerial imagery, using semantic segmentation, a machine learning\napproach. We present a land cover classification training pipeline with Deeplab\nv3+, state-of-the-art semantic segmentation technology, including data\npreparation, model training for seven land cover types, and model exporting\nmodules. In the land cover change detection system, the inputs are images\nretrieved from Google Earth at the same location but from different times. The\nsystem then predicts semantic segmentation results on these images using the\ntrained model and calculates the land cover class percentage for each input\nimage. We see an improvement in the accuracy of the land cover semantic\nsegmentation model, with a mean IoU of 0.756 compared to 0.433, as reported in\nthe DeepGlobe land cover classification challenge. The land cover change\ndetection system that leverages the state-of-the-art semantic segmentation\ntechnology is proposed and can be used for deforestation analysis, land\nmanagement, and urban planning.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 23:54:36 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Su", "Renee", ""], ["Chen", "Rong", ""]]}, {"id": "1911.12906", "submitter": "Kenichiro Tanaka", "authors": "Kenichiro Tanaka, Yasuhiro Mukaigawa, Achuta Kadambi", "title": "Enhancing Passive Non-Line-of-Sight Imaging Using Polarization Cues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method of passive non-line-of-sight (NLOS) imaging\nusing polarization cues. A key observation is that the oblique light has a\ndifferent polarimetric signal. It turns out this effect is due to the\npolarization axis rotation, a phenomena which can be used to better condition\nthe light transport matrix for non-line-of-sight imaging. Our analysis and\nresults show that the use of a polarizer in front of the camera is not only a\nseparate technique, but it can be seen as an enhancement technique for more\nadvanced forms of passive NLOS imaging. For example, this paper shows that\npolarization can enhance passive NLOS imaging both with and without occluders.\nIn all tested cases, despite the light attenuation from polarization optics,\nrecovery of the occluded images is improved.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 00:22:31 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Tanaka", "Kenichiro", ""], ["Mukaigawa", "Yasuhiro", ""], ["Kadambi", "Achuta", ""]]}, {"id": "1911.12911", "submitter": "Zhiyuan Hu", "authors": "Ziqi Pang, Zhiyuan Hu, Pavel Tokmakov, Yu-Xiong Wang and Martial\n  Hebert", "title": "Unlocking the Full Potential of Small Data with Diverse Supervision", "comments": "Learning from Limited and Imperfect Data (L2ID) Workshop @ CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtually all of deep learning literature relies on the assumption of large\namounts of available training data. Indeed, even the majority of few-shot\nlearning methods rely on a large set of \"base classes\" for pretraining. This\nassumption, however, does not always hold. For some tasks, annotating a large\nnumber of classes can be infeasible, and even collecting the images themselves\ncan be a challenge in some scenarios. In this paper, we study this problem and\ncall it \"Small Data\" setting, in contrast to \"Big Data\". To unlock the full\npotential of small data, we propose to augment the models with annotations for\nother related tasks, thus increasing their generalization abilities. In\nparticular, we use the richly annotated scene parsing dataset ADE20K to\nconstruct our realistic Long-tail Recognition with Diverse Supervision (LRDS)\nbenchmark by splitting the object categories into head and tail based on their\ndistribution. Following the standard few-shot learning protocol, we use the\nhead classes for representation learning and the tail classes for evaluation.\nMoreover, we further subsample the head categories and images to generate two\nnovel settings which we call \"Scarce-Class\" and \"Scarce-Image\", respectively\ncorresponding to the shortage of samples for rare classes and training images.\nFinally, we analyze the effect of applying various additional supervision\nsources under the proposed settings. Our experiments demonstrate that densely\nlabeling a small set of images can indeed largely remedy the small data\nconstraints.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 00:56:06 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 14:05:25 GMT"}, {"version": "v3", "created": "Mon, 26 Apr 2021 23:24:04 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Pang", "Ziqi", ""], ["Hu", "Zhiyuan", ""], ["Tokmakov", "Pavel", ""], ["Wang", "Yu-Xiong", ""], ["Hebert", "Martial", ""]]}, {"id": "1911.12914", "submitter": "Dohyung Kim MR", "authors": "Junghyup Lee, Dohyung Kim, Wonkyung Lee, Jean Ponce, Bumsub Ham", "title": "Learning Semantic Correspondence Exploiting an Object-level Prior", "comments": "Accepted to TPAMI. arXiv admin note: substantial text overlap with\n  arXiv:1904.01810", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of semantic correspondence, that is, establishing a\ndense flow field between images depicting different instances of the same\nobject or scene category. We propose to use images annotated with binary\nforeground masks and subjected to synthetic geometric deformations to train a\nconvolutional neural network (CNN) for this task. Using these masks as part of\nthe supervisory signal provides an object-level prior for the semantic\ncorrespondence task and offers a good compromise between semantic flow methods,\nwhere the amount of training data is limited by the cost of manually selecting\npoint correspondences, and semantic alignment ones, where the regression of a\nsingle global geometric transformation between images may be sensitive to\nimage-specific details such as background clutter. We propose a new CNN\narchitecture, dubbed SFNet, which implements this idea. It leverages a new and\ndifferentiable version of the argmax function for end-to-end training, with a\nloss that combines mask and flow consistency with smoothness terms.\nExperimental results demonstrate the effectiveness of our approach, which\nsignificantly outperforms the state of the art on standard benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 01:13:11 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 06:29:40 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Lee", "Junghyup", ""], ["Kim", "Dohyung", ""], ["Lee", "Wonkyung", ""], ["Ponce", "Jean", ""], ["Ham", "Bumsub", ""]]}, {"id": "1911.12950", "submitter": "Jin Chen", "authors": "Kaihua Zhang, Jin Chen, Bo Liu, Qingshan Liu", "title": "Deep Object Co-segmentation via Spatial-Semantic Network Modulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object co-segmentation is to segment the shared objects in multiple relevant\nimages, which has numerous applications in computer vision. This paper presents\na spatial and semantic modulated deep network framework for object\nco-segmentation. A backbone network is adopted to extract multi-resolution\nimage features. With the multi-resolution features of the relevant images as\ninput, we design a spatial modulator to learn a mask for each image. The\nspatial modulator captures the correlations of image feature descriptors via\nunsupervised learning. The learned mask can roughly localize the shared\nforeground object while suppressing the background. For the semantic modulator,\nwe model it as a supervised image classification task. We propose a\nhierarchical second-order pooling module to transform the image features for\nclassification use. The outputs of the two modulators manipulate the\nmulti-resolution features by a shift-and-scale operation so that the features\nfocus on segmenting co-object regions. The proposed model is trained end-to-end\nwithout any intricate post-processing. Extensive experiments on four image\nco-segmentation benchmark datasets demonstrate the superior accuracy of the\nproposed method compared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 04:40:30 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Zhang", "Kaihua", ""], ["Chen", "Jin", ""], ["Liu", "Bo", ""], ["Liu", "Qingshan", ""]]}, {"id": "1911.12983", "submitter": "Mohammad Mahfujur Rahman", "authors": "Mohammad Mahfujur Rahman, Clinton Fookes, Mahsa Baktashmotlagh, Sridha\n  Sridharan", "title": "Correlation-aware Adversarial Domain Adaptation and Generalization", "comments": "Preprint submitted to Pattern Recognition, Accepted in Pattern\n  Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation (DA) and domain generalization (DG) have emerged as a\nsolution to the domain shift problem where the distribution of the source and\ntarget data is different. The task of DG is more challenging than DA as the\ntarget data is totally unseen during the training phase in DG scenarios. The\ncurrent state-of-the-art employs adversarial techniques, however, these are\nrarely considered for the DG problem. Furthermore, these approaches do not\nconsider correlation alignment which has been proven highly beneficial for\nminimizing domain discrepancy. In this paper, we propose a correlation-aware\nadversarial DA and DG framework where the features of the source and target\ndata are minimized using correlation alignment along with adversarial learning.\nIncorporating the correlation alignment module along with adversarial learning\nhelps to achieve a more domain agnostic model due to the improved ability to\nreduce domain discrepancy with unlabeled target data more effectively.\nExperiments on benchmark datasets serve as evidence that our proposed method\nyields improved state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 07:22:15 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Rahman", "Mohammad Mahfujur", ""], ["Fookes", "Clinton", ""], ["Baktashmotlagh", "Mahsa", ""], ["Sridharan", "Sridha", ""]]}, {"id": "1911.12989", "submitter": "Junpeng Zhang", "authors": "Junpeng Zhang, Xiuping Jia, Jiankun Hu and Jocelyn Chanussot", "title": "Online Structured Sparsity-based Moving Object Detection from Satellite\n  Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the recent developments in computer vision, low-rank and\nstructured sparse matrix decomposition can be potentially be used for extract\nmoving objects in satellite videos. This set of approaches seeks for rank\nminimization on the background that typically requires batch-based optimization\nover a sequence of frames, which causes delays in processing and limits their\napplications. To remedy this delay, we propose an Online Low-rank and\nStructured Sparse Decomposition (O-LSD). O-LSD reformulates the batch-based\nlow-rank matrix decomposition with the structured sparse penalty to its\nequivalent frame-wise separable counterpart, which then defines a stochastic\noptimization problem for online subspace basis estimation. In order to promote\nonline processing, O-LSD conducts the foreground and background separation and\nthe subspace basis update alternatingly for every frame in a video. We also\nshow the convergence of O-LSD theoretically. Experimental results on two\nsatellite videos demonstrate the performance of O-LSD in term of accuracy and\ntime consumption is comparable with the batch-based approaches with\nsignificantly reduced delay in processing.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 07:54:14 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 04:07:44 GMT"}, {"version": "v3", "created": "Tue, 10 Dec 2019 10:56:11 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Zhang", "Junpeng", ""], ["Jia", "Xiuping", ""], ["Hu", "Jiankun", ""], ["Chanussot", "Jocelyn", ""]]}, {"id": "1911.12990", "submitter": "Jihun Yun", "authors": "Jung Hyun Lee, Jihun Yun, Sung Ju Hwang, Eunho Yang", "title": "Semi-Relaxed Quantization with DropBits: Training Low-Bit Neural\n  Networks via Bit-wise Regularization", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network quantization, which aims to reduce the bit-lengths of the network\nweights and activations, has emerged as one of the key ingredients to reduce\nthe size of neural networks for their deployments to resource-limited devices.\nIn order to overcome the nature of transforming continuous activations and\nweights to discrete ones, recent study called Relaxed Quantization (RQ)\n[Louizos et al. 2019] successfully employ the popular Gumbel-Softmax that\nallows this transformation with efficient gradient-based optimization. However,\nRQ with this Gumbel-Softmax relaxation still suffers from bias-variance\ntrade-off depending on the temperature parameter of Gumbel-Softmax. To resolve\nthe issue, we propose a novel method, Semi-Relaxed Quantization (SRQ) that uses\nmulti-class straight-through estimator to effectively reduce the bias and\nvariance, along with a new regularization technique, DropBits that replaces\ndropout regularization to randomly drop the bits instead of neurons to further\nreduce the bias of the multi-class straight-through estimator in SRQ. As a\nnatural extension of DropBits, we further introduce the way of learning\nheterogeneous quantization levels to find proper bit-length for each layer\nusing DropBits. We experimentally validate our method on various benchmark\ndatasets and network architectures, and also support the quantized lottery\nticket hypothesis: learning heterogeneous quantization levels outperforms the\ncase using the same but fixed quantization levels from scratch.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 07:58:43 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 07:35:34 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Lee", "Jung Hyun", ""], ["Yun", "Jihun", ""], ["Hwang", "Sung Ju", ""], ["Yang", "Eunho", ""]]}, {"id": "1911.12993", "submitter": "Sethu Hareesh Kolluru", "authors": "Sethu Hareesh Kolluru", "title": "Investigations on the inference optimization techniques and their impact\n  on multiple hardware platforms for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, the task of pixel-wise semantic segmentation in the context of\nself-driving with a goal to reduce the inference time is explored. Fully\nConvolutional Network (FCN-8s, FCN-16s, and FCN-32s) with a VGG16 encoder\narchitecture and skip connections is trained and validated on the Cityscapes\ndataset. Numerical investigations are carried out for several inference\noptimization techniques built into TensorFlow and TensorRT to quantify their\nimpact on the inference time and network size. Finally, the trained network is\nported on to an embedded platform (Nvidia Jetson TX1) and the inference time,\nas well as the total energy consumed for inference across hardware platforms,\nare compared.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 08:08:28 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Kolluru", "Sethu Hareesh", ""]]}, {"id": "1911.13008", "submitter": "Wenpeng Li", "authors": "Wenpeng Li, Yongli Sun, Jinjun Wang, Han Xu, Xiangru Yang, Long Cui", "title": "Collaborative Attention Network for Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Jointly utilizing global and local features to improve model accuracy is\nbecoming a popular approach for the person re-identification (ReID) problem,\nbecause previous works using global features alone have very limited capacity\nat extracting discriminative local patterns in the obtained feature\nrepresentation. Existing works that attempt to collect local patterns either\nexplicitly slice the global feature into several local pieces in a handcrafted\nway, or apply the attention mechanism to implicitly infer the importance of\ndifferent local regions. In this paper, we show that by explicitly learning the\nimportance of small local parts and part combinations, we can further improve\nthe final feature representation for Re-ID. Specifically, we first separate the\nglobal feature into multiple local slices at different scale with a proposed\nmulti-branch structure. Then we introduce the Collaborative Attention Network\n(CAN) to automatically learn the combination of features from adjacent slices.\nIn this way, the combination keeps the intrinsic relation between adjacent\nfeatures across local regions and scales, without losing information by\npartitioning the global features. Experiment results on several widely-used\npublic datasets including Market-1501, DukeMTMC-ReID and CUHK03 prove that the\nproposed method outperforms many existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 09:18:20 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 06:46:22 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Li", "Wenpeng", ""], ["Sun", "Yongli", ""], ["Wang", "Jinjun", ""], ["Xu", "Han", ""], ["Yang", "Xiangru", ""], ["Cui", "Long", ""]]}, {"id": "1911.13038", "submitter": "Krishna Kanth Nakka", "authors": "Krishna Kanth Nakka and Mathieu Salzmann", "title": "Indirect Local Attacks for Context-aware Semantic Segmentation Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep networks have achieved impressive semantic segmentation\nperformance, in particular thanks to their use of larger contextual\ninformation. In this paper, we show that the resulting networks are sensitive\nnot only to global attacks, where perturbations affect the entire input image,\nbut also to indirect local attacks where perturbations are confined to a small\nimage region that does not overlap with the area that we aim to fool. To this\nend, we introduce several indirect attack strategies, including adaptive local\nattacks, aiming to find the best image location to perturb, and universal local\nattacks. Furthermore, we propose attack detection techniques both for the\nglobal image level and to obtain a pixel-wise localization of the fooled\nregions. Our results are unsettling: Because they exploit a larger context,\nmore accurate semantic segmentation networks are more sensitive to indirect\nlocal attacks.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 10:28:11 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 09:21:00 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Nakka", "Krishna Kanth", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "1911.13044", "submitter": "Todor Davchev", "authors": "Todor Davchev, Michael Burke, Subramanian Ramamoorthy", "title": "Learning Structured Representations of Spatial and Interactive Dynamics\n  for Trajectory Prediction in Crowded Scenes", "comments": null, "journal-ref": "IEEE Robotics and Automation Letters 2021", "doi": "10.1109/LRA.2020.3047778", "report-no": null, "categories": "cs.LG cs.CV cs.MA cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context plays a significant role in the generation of motion for dynamic\nagents in interactive environments. This work proposes a modular method that\nutilises a learned model of the environment for motion prediction. This\nmodularity explicitly allows for unsupervised adaptation of trajectory\nprediction models to unseen environments and new tasks by relying on unlabelled\nimage data only. We model both the spatial and dynamic aspects of a given\nenvironment alongside the per agent motions. This results in more informed\nmotion prediction and allows for performance comparable to the\nstate-of-the-art. We highlight the model's prediction capability using a\nbenchmark pedestrian prediction problem and a robot manipulation task and show\nthat we can transfer the predictor across these tasks in a completely\nunsupervised way. The proposed approach allows for robust and label efficient\nforward modelling, and relaxes the need for full model re-training in new\nenvironments.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 10:42:10 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 10:56:41 GMT"}, {"version": "v3", "created": "Fri, 17 Jan 2020 11:34:06 GMT"}, {"version": "v4", "created": "Sat, 16 May 2020 09:29:35 GMT"}, {"version": "v5", "created": "Sun, 16 Aug 2020 18:47:40 GMT"}, {"version": "v6", "created": "Sat, 2 Jan 2021 13:24:01 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Davchev", "Todor", ""], ["Burke", "Michael", ""], ["Ramamoorthy", "Subramanian", ""]]}, {"id": "1911.13053", "submitter": "Changlin Li", "authors": "Changlin Li, Jiefeng Peng, Liuchun Yuan, Guangrun Wang, Xiaodan Liang,\n  Liang Lin, Xiaojun Chang", "title": "Blockwisely Supervised Neural Architecture Search with Knowledge\n  Distillation", "comments": "To be appear in CVPR 2020. We achieve a state-of-the-art 78.4% top-1\n  accuracy on ImageNet in a mobile setting, which is about a 2.1% gain over\n  EfficientNet-B0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS), aiming at automatically designing network\narchitectures by machines, is hoped and expected to bring about a new\nrevolution in machine learning. Despite these high expectation, the\neffectiveness and efficiency of existing NAS solutions are unclear, with some\nrecent works going so far as to suggest that many existing NAS solutions are no\nbetter than random architecture selection. The inefficiency of NAS solutions\nmay be attributed to inaccurate architecture evaluation. Specifically, to speed\nup NAS, recent works have proposed under-training different candidate\narchitectures in a large search space concurrently by using shared network\nparameters; however, this has resulted in incorrect architecture ratings and\nfurthered the ineffectiveness of NAS.\n  In this work, we propose to modularize the large search space of NAS into\nblocks to ensure that the potential candidate architectures are fully trained;\nthis reduces the representation shift caused by the shared parameters and leads\nto the correct rating of the candidates. Thanks to the block-wise search, we\ncan also evaluate all of the candidate architectures within a block. Moreover,\nwe find that the knowledge of a network model lies not only in the network\nparameters but also in the network architecture. Therefore, we propose to\ndistill the neural architecture (DNA) knowledge from a teacher model as the\nsupervision to guide our block-wise architecture search, which significantly\nimproves the effectiveness of NAS. Remarkably, the capacity of our searched\narchitecture has exceeded the teacher model, demonstrating the practicability\nand scalability of our method. Finally, our method achieves a state-of-the-art\n78.4\\% top-1 accuracy on ImageNet in a mobile setting, which is about a 2.1\\%\ngain over EfficientNet-B0. All of our searched models along with the evaluation\ncode are available online.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 11:00:30 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 06:08:31 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Li", "Changlin", ""], ["Peng", "Jiefeng", ""], ["Yuan", "Liuchun", ""], ["Wang", "Guangrun", ""], ["Liang", "Xiaodan", ""], ["Lin", "Liang", ""], ["Chang", "Xiaojun", ""]]}, {"id": "1911.13061", "submitter": "Duncan Watson-Parris", "authors": "Duncan Watson-Parris, Samuel Sutherland, Matthew Christensen, Anthony\n  Caterini, Dino Sejdinovic, Philip Stier", "title": "Detecting anthropogenic cloud perturbations with deep learning", "comments": "Awarded Best Paper and Spotlight Oral at Climate Change: How Can AI\n  Help? (Workshop) at International Conference on Machine Learning (ICML), Long\n  Beach, California, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the most pressing questions in climate science is that of the effect\nof anthropogenic aerosol on the Earth's energy balance. Aerosols provide the\n`seeds' on which cloud droplets form, and changes in the amount of aerosol\navailable to a cloud can change its brightness and other physical properties\nsuch as optical thickness and spatial extent. Clouds play a critical role in\nmoderating global temperatures and small perturbations can lead to significant\namounts of cooling or warming. Uncertainty in this effect is so large it is not\ncurrently known if it is negligible, or provides a large enough cooling to\nlargely negate present-day warming by CO2. This work uses deep convolutional\nneural networks to look for two particular perturbations in clouds due to\nanthropogenic aerosol and assess their properties and prevalence, providing\nvaluable insights into their climatic effects.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 11:22:48 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Watson-Parris", "Duncan", ""], ["Sutherland", "Samuel", ""], ["Christensen", "Matthew", ""], ["Caterini", "Anthony", ""], ["Sejdinovic", "Dino", ""], ["Stier", "Philip", ""]]}, {"id": "1911.13073", "submitter": "Nupur Kumari", "authors": "Mayank Singh, Nupur Kumari, Puneet Mangla, Abhishek Sinha, Vineeth N\n  Balasubramanian, Balaji Krishnamurthy", "title": "Attributional Robustness Training using Input-Gradient Spatial Alignment", "comments": "ECCV 2020, Code at\n  https://github.com/nupurkmr9/Attributional-Robustness", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretability is an emerging area of research in trustworthy machine\nlearning. Safe deployment of machine learning system mandates that the\nprediction and its explanation be reliable and robust. Recently, it has been\nshown that the explanations could be manipulated easily by adding visually\nimperceptible perturbations to the input while keeping the model's prediction\nintact. In this work, we study the problem of attributional robustness (i.e.\nmodels having robust explanations) by showing an upper bound for attributional\nvulnerability in terms of spatial correlation between the input image and its\nexplanation map. We propose a training methodology that learns robust features\nby minimizing this upper bound using soft-margin triplet loss. Our methodology\nof robust attribution training (\\textit{ART}) achieves the new state-of-the-art\nattributional robustness measure by a margin of $\\approx$ 6-18 $\\%$ on several\nstandard datasets, ie. SVHN, CIFAR-10 and GTSRB. We further show the utility of\nthe proposed robust training technique (\\textit{ART}) in the downstream task of\nweakly supervised object localization by achieving the new state-of-the-art\nperformance on CUB-200 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 12:08:41 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 07:55:31 GMT"}, {"version": "v3", "created": "Tue, 10 Dec 2019 07:20:28 GMT"}, {"version": "v4", "created": "Sat, 18 Jul 2020 16:07:35 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Singh", "Mayank", ""], ["Kumari", "Nupur", ""], ["Mangla", "Puneet", ""], ["Sinha", "Abhishek", ""], ["Balasubramanian", "Vineeth N", ""], ["Krishnamurthy", "Balaji", ""]]}, {"id": "1911.13077", "submitter": "Kazuya Nishimura", "authors": "Kazuya Nishimura, Dai Fei Elmer Ker, Ryoma Bise", "title": "Weakly Supervised Cell Instance Segmentation by Propagating from\n  Detection Response", "comments": "9 pages, 3 figures, Accepted in MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cell shape analysis is important in biomedical research. Deep learning\nmethods may perform to segment individual cells if they use sufficient training\ndata that the boundary of each cell is annotated. However, it is very\ntime-consuming for preparing such detailed annotation for many cell culture\nconditions. In this paper, we propose a weakly supervised method that can\nsegment individual cell regions who touch each other with unclear boundaries in\ndense conditions without the training data for cell regions. We demonstrated\nthe efficacy of our method using several data-set including multiple cell types\ncaptured by several types of microscopy. Our method achieved the highest\naccuracy compared with several conventional methods. In addition, we\ndemonstrated that our method can perform without any annotation by using\nfluorescence images that cell nuclear were stained as training data. Code is\npublicly available in \"https://github.com/naivete5656/WSISPDR\".\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 12:29:15 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Nishimura", "Kazuya", ""], ["Ker", "Dai Fei Elmer", ""], ["Bise", "Ryoma", ""]]}, {"id": "1911.13114", "submitter": "Jules Simon", "authors": "Jules Simon, Guillaume-Alexandre Bilodeau, David Steele, Harshad\n  Mahadik", "title": "Color inference from semantic labeling for person search in videos", "comments": "8 pages, 7 figures ICIAR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an explainable model to generate semantic color labels for person\nsearch. In this context, persons are described from their semantic parts, such\nas hat, shirt, etc. Person search consists in looking for people based on these\ndescriptions. In this work, we aim to improve the accuracy of color labels for\npeople. Our goal is to handle the high variability of human perception.\nExisting solutions are based on hand-crafted features or learnt features that\nare not explainable. Moreover most of them only focus on a limited set of\ncolors. We propose a method based on binary search trees and a large\npeer-labelled color name dataset. This allows us to synthesize the human\nperception of colors. Using semantic segmentation and our color labeling\nmethod, we label segments of pedestrians with their associated colors. We\nevaluate our solution on person search on datasets such as PCN, and show a\nprecision as high as 80.4%.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 14:07:08 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 22:06:36 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Simon", "Jules", ""], ["Bilodeau", "Guillaume-Alexandre", ""], ["Steele", "David", ""], ["Mahadik", "Harshad", ""]]}, {"id": "1911.13125", "submitter": "Florent Nageotte", "authors": "Paolo Cabras (ICube), Florent Nageotte (ICube), Philippe Zanne\n  (ICube), Christophe Doignon (ICube)", "title": "An adaptive and fully automatic method for estimating the 3D position of\n  bendable instruments using endoscopic images", "comments": "The International Journal of Medical Robotics and Computer Assisted\n  Surgery, John Wiley & Sons, Inc., 2017", "journal-ref": null, "doi": "10.1002/rcs.1812", "report-no": null, "categories": "eess.IV cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background. Flexible bendable instruments are key tools for performing\nsurgical endoscopy. Being able to measure the 3D position of such instruments\ncan be useful for various tasks, such as controlling automatically robotized\ninstruments and analyzing motions. Methods. We propose an automatic method to\ninfer the 3D pose of a single bending section instrument, using only the images\nprovided by a monocular camera embedded at the tip of the endoscope. The\nproposed method relies on colored markers attached onto the bending section.\nThe image of the instrument is segmented using a graph-based method and the\ncorners of the markers are extracted by detecting the color transition along\nB{\\'e}zier curves fitted on edge points. These features are accurately located\nand then used to estimate the 3D pose of the instrument using an adaptive model\nthat allows to take into account the mechanical play between the instrument and\nits housing channel. Results. The feature extraction method provides good\nlocalization of markers corners with images of in vivo environment despite\nsensor saturation due to strong lighting. The RMS error on the estimation of\nthe tip position of the instrument for laboratory experiments was 2.1, 1.96,\n3.18 mm in the x, y and z directions respectively. Qualitative analysis in the\ncase of in vivo images shows the ability to correctly estimate the 3D position\nof the instrument tip during real motions. Conclusions. The proposed method\nprovides an automatic and accurate estimation of the 3D position of the tip of\na bendable instrument in realistic conditions, where standard approaches fail.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 14:40:13 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Cabras", "Paolo", "", "ICube"], ["Nageotte", "Florent", "", "ICube"], ["Zanne", "Philippe", "", "ICube"], ["Doignon", "Christophe", "", "ICube"]]}, {"id": "1911.13135", "submitter": "Gabriel Turinici", "authors": "Gabriel Turinici (CEREMADE, Universit\\'e Paris Dauphine - PSL)", "title": "Radon Sobolev Variational Auto-Encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The quality of generative models (such as Generative adversarial networks and\nVariational Auto-Encoders) depends heavily on the choice of a good probability\ndistance. However some popular metrics like the Wasserstein or the Sliced\nWasserstein distances, the Jensen-Shannon divergence, the Kullback-Leibler\ndivergence, lack convenient properties such as (geodesic) convexity, fast\nevaluation and so on. To address these shortcomings, we introduce a class of\ndistances that have built-in convexity. We investigate the relationship with\nsome known paradigms (sliced distances - a synonym for Radon distances -,\nreproducing kernel Hilbert spaces, energy distances). The distances are shown\nto possess fast implementations and are included in an adapted Variational\nAuto-Encoder termed Radon Sobolev Variational Auto-Encoder (RS-VAE) which\nproduces high quality results on standard generative datasets.\n  Keywords: Variational Auto-Encoder; Generative model; Sobolev spaces; Radon\nSobolev Variational Auto-Encoder;\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 15:02:28 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 17:00:16 GMT"}, {"version": "v3", "created": "Wed, 14 Apr 2021 18:08:35 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Turinici", "Gabriel", "", "CEREMADE, Universit\u00e9 Paris Dauphine - PSL"]]}, {"id": "1911.13162", "submitter": "Alexander Preuhs", "authors": "Alexander Preuhs, Michael Manhart, Philipp Roser, Bernhard Stimpel,\n  Christopher Syben, Marios Psychogios, Markus Kowarschik, Andreas Maier", "title": "Deep autofocus with cone-beam CT consistency constraint", "comments": "Accepted at BVM 2020, review score under Top-6 of the conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High quality reconstruction with interventional C-arm cone-beam computed\ntomography (CBCT) requires exact geometry information. If the geometry\ninformation is corrupted, e. g., by unexpected patient or system movement, the\nmeasured signal is misplaced in the backprojection operation. With prolonged\nacquisition times of interventional C-arm CBCT the likelihood of rigid patient\nmotion increases. To adapt the backprojection operation accordingly, a motion\nestimation strategy is necessary. Recently, a novel learning-based approach was\nproposed, capable of compensating motions within the acquisition plane. We\nextend this method by a CBCT consistency constraint, which was proven to be\nefficient for motions perpendicular to the acquisition plane. By the\nsynergistic combination of these two measures, in and out-plane motion is well\ndetectable, achieving an average artifact suppression of 93 [percent]. This\noutperforms the entropy-based state-of-the-art autofocus measure which achieves\non average an artifact suppression of 54 [percent].\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 15:54:38 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 12:17:44 GMT"}, {"version": "v3", "created": "Wed, 4 Dec 2019 21:43:50 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Preuhs", "Alexander", ""], ["Manhart", "Michael", ""], ["Roser", "Philipp", ""], ["Stimpel", "Bernhard", ""], ["Syben", "Christopher", ""], ["Psychogios", "Marios", ""], ["Kowarschik", "Markus", ""], ["Maier", "Andreas", ""]]}, {"id": "1911.13168", "submitter": "Mehrdad Noori", "authors": "Sina Mohammadi, Mehrdad Noori, Ali Bahri, Sina Ghofrani Majelan,\n  Mohammad Havaei", "title": "CAGNet: Content-Aware Guidance for Salient Object Detection", "comments": "25 pages, 10 figures, 5 tables, Accepted by Elsevier, Pattern\n  Recognition", "journal-ref": "Pattern Recognition, Volume 103, 2020, 107303, ISSN 0031-3203", "doi": "10.1016/j.patcog.2020.107303", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beneficial from Fully Convolutional Neural Networks (FCNs), saliency\ndetection methods have achieved promising results. However, it is still\nchallenging to learn effective features for detecting salient objects in\ncomplicated scenarios, in which i) non-salient regions may have \"salient-like\"\nappearance; ii) the salient objects may have different-looking regions. To\nhandle these complex scenarios, we propose a Feature Guide Network which\nexploits the nature of low-level and high-level features to i) make foreground\nand background regions more distinct and suppress the non-salient regions which\nhave \"salient-like\" appearance; ii) assign foreground label to\ndifferent-looking salient regions. Furthermore, we utilize a Multi-scale\nFeature Extraction Module (MFEM) for each level of abstraction to obtain\nmulti-scale contextual information. Finally, we design a loss function which\noutperforms the widely-used Cross-entropy loss. By adopting four different\npre-trained models as the backbone, we prove that our method is very general\nwith respect to the choice of the backbone model. Experiments on five\nchallenging datasets demonstrate that our method achieves the state-of-the-art\nperformance in terms of different evaluation metrics. Additionally, our\napproach contains fewer parameters than the existing ones, does not need any\npost-processing, and runs fast at a real-time speed of 28 FPS when processing a\n480 x 480 image.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 16:03:47 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2020 20:11:23 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Mohammadi", "Sina", ""], ["Noori", "Mehrdad", ""], ["Bahri", "Ali", ""], ["Majelan", "Sina Ghofrani", ""], ["Havaei", "Mohammad", ""]]}, {"id": "1911.13169", "submitter": "Agnieszka Szczotka", "authors": "Agnieszka Barbara Szczotka, Dzhoshkun Ismail Shakir, DanieleRavi,\n  Matthew J. Clarkson, Stephen P. Pereira, Tom Vercauteren", "title": "Learning from Irregularly Sampled Data for Endomicroscopy\n  Super-resolution: A Comparative Study of Sparse and Dense Approaches", "comments": null, "journal-ref": null, "doi": "10.1007/s11548-020-02170-7", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Probe-based Confocal Laser Endomicroscopy (pCLE) enables performing\nan optical biopsy, providing real-time microscopic images, via a probe. pCLE\nprobes consist of multiple optical fibres arranged in a bundle, which taken\ntogether generate signals in an irregularly sampled pattern. Current pCLE\nreconstruction is based on interpolating irregular signals onto an over-sampled\nCartesian grid, using a naive linear interpolation. It was shown that\nConvolutional Neural Networks (CNNs) could improve pCLE image quality. Although\nclassical CNNs were applied to pCLE, input data were limited to reconstructed\nimages in contrast to irregular data produced by pCLE. Methods: We compare pCLE\nreconstruction and super-resolution (SR) methods taking irregularly sampled or\nreconstructed pCLE images as input. We also propose to embed a Nadaraya-Watson\n(NW) kernel regression into the CNN framework as a novel trainable CNN layer.\nUsing the NW layer and exemplar-based super-resolution, we design an NWNetSR\narchitecture that allows for reconstructing high-quality pCLE images directly\nfrom the irregularly sampled input data. We created synthetic sparse pCLE\nimages to evaluate our methodology. Results: The results were validated through\nan image quality assessment based on a combination of the following metrics:\nPeak signal-to-noise ratio, the Structural Similarity Index. Conclusion: Both\ndense and sparse CNNs outperform the reconstruction method currently used in\nthe clinic. The main contributions of our study are a comparison of sparse and\ndense approach in pCLE image reconstruction, implementing trainable generalised\nNW kernel regression, and adaptation of synthetic data for training pCLE SR.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 16:04:38 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Szczotka", "Agnieszka Barbara", ""], ["Shakir", "Dzhoshkun Ismail", ""], ["DanieleRavi", "", ""], ["Clarkson", "Matthew J.", ""], ["Pereira", "Stephen P.", ""], ["Vercauteren", "Tom", ""]]}, {"id": "1911.13173", "submitter": "Brendan Ruff", "authors": "Brendan Ruff and Taylor Beck and Joscha Bach", "title": "Mean Shift Rejection: Training Deep Neural Networks Without Minibatch\n  Statistics or Normalization", "comments": "under review at ECAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks are known to be unstable during training\nat high learning rate unless normalization techniques are employed. Normalizing\nweights or activations allows the use of higher learning rates, resulting in\nfaster convergence and higher test accuracy. Batch normalization requires\nminibatch statistics that approximate the dataset statistics but this incurs\nadditional compute and memory costs and causes a communication bottleneck for\ndistributed training. Weight normalization and initialization-only schemes do\nnot achieve comparable test accuracy.\n  We introduce a new understanding of the cause of training instability and\nprovide a technique that is independent of normalization and minibatch\nstatistics. Our approach treats training instability as a spatial common mode\nsignal which is suppressed by placing the model on a channel-wise zero-mean\nisocline that is maintained throughout training. Firstly, we apply channel-wise\nzero-mean initialization of filter kernels with overall unity kernel magnitude.\nAt each training step we modify the gradients of spatial kernels so that their\nweighted channel-wise mean is subtracted in order to maintain the common mode\nrejection condition. This prevents the onset of mean shift. This new technique\nallows direct training of the test graph so that training and test models are\nidentical. We also demonstrate that injecting random noise throughout the\nnetwork during training improves generalization. This is based on the idea\nthat, as a side effect, batch normalization performs deep data augmentation by\ninjecting minibatch noise due to the weakness of the dataset approximation.\n  Our technique achieves higher accuracy compared to batch normalization and\nfor the first time shows that minibatches and normalization are unnecessary for\nstate-of-the-art training.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 16:19:00 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Ruff", "Brendan", ""], ["Beck", "Taylor", ""], ["Bach", "Joscha", ""]]}, {"id": "1911.13175", "submitter": "Sean Moran", "authors": "Sean Moran, Steven McDonagh, Gregory Slabaugh", "title": "CURL: Neural Curve Layers for Global Image Enhancement", "comments": "Accepted to ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to adjust global image properties such as colour,\nsaturation, and luminance using human-interpretable image enhancement curves,\ninspired by the Photoshop curves tool. Our method, dubbed neural CURve Layers\n(CURL), is designed as a multi-colour space neural retouching block trained\njointly in three different colour spaces (HSV, CIELab, RGB) guided by a novel\nmulti-colour space loss. The curves are fully differentiable and are trained\nend-to-end for different computer vision problems including photo enhancement\n(RGB-to-RGB) and as part of the image signal processing pipeline for image\nformation (RAW-to-RGB). To demonstrate the effectiveness of CURL we combine\nthis global image transformation block with a pixel-level (local) image\nmulti-scale encoder-decoder backbone network. In an extensive experimental\nevaluation we show that CURL produces state-of-the-art image quality versus\nrecently proposed deep learning approaches in both objective and perceptual\nmetrics, setting new state-of-the-art performance on multiple public datasets.\nOur code is publicly available at: https://github.com/sjmoran/CURL.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 16:20:05 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 16:18:59 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 13:50:27 GMT"}, {"version": "v4", "created": "Fri, 23 Oct 2020 07:42:56 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Moran", "Sean", ""], ["McDonagh", "Steven", ""], ["Slabaugh", "Gregory", ""]]}, {"id": "1911.13225", "submitter": "Shaohui Liu", "authors": "Shaohui Liu, Yinda Zhang, Songyou Peng, Boxin Shi, Marc Pollefeys,\n  Zhaopeng Cui", "title": "DIST: Rendering Deep Implicit Signed Distance Function with\n  Differentiable Sphere Tracing", "comments": "Camera-ready version to appear in CVPR 2020. Project page:\n  http://b1ueber2y.me/projects/DIST-Renderer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a differentiable sphere tracing algorithm to bridge the gap\nbetween inverse graphics methods and the recently proposed deep learning based\nimplicit signed distance function. Due to the nature of the implicit function,\nthe rendering process requires tremendous function queries, which is\nparticularly problematic when the function is represented as a neural network.\nWe optimize both the forward and backward passes of our rendering layer to make\nit run efficiently with affordable memory consumption on a commodity graphics\ncard. Our rendering method is fully differentiable such that losses can be\ndirectly computed on the rendered 2D observations, and the gradients can be\npropagated backwards to optimize the 3D geometry. We show that our rendering\nmethod can effectively reconstruct accurate 3D shapes from various inputs, such\nas sparse depth and multi-view images, through inverse optimization. With the\ngeometry based reasoning, our 3D shape prediction methods show excellent\ngeneralization capability and robustness against various noises.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 17:27:46 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 07:19:07 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Liu", "Shaohui", ""], ["Zhang", "Yinda", ""], ["Peng", "Songyou", ""], ["Shi", "Boxin", ""], ["Pollefeys", "Marc", ""], ["Cui", "Zhaopeng", ""]]}, {"id": "1911.13237", "submitter": "Tianyuan Zhang", "authors": "Tianyuan Zhang, Bichen Wu, Xin Wang, Joseph Gonzalez, Kurt Keutzer", "title": "Domain-Aware Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks with more parameters and FLOPs have higher capacity and\ngeneralize better to diverse domains. But to be deployed on edge devices, the\nmodel's complexity has to be constrained due to limited compute resource. In\nthis work, we propose a method to improve the model capacity without increasing\ninference-time complexity. Our method is based on an assumption of data\nlocality: for an edge device, within a short period of time, the input data to\nthe device are sampled from a single domain with relatively low diversity.\nTherefore, it is possible to utilize a specialized, low-complexity model to\nachieve good performance in that input domain. To leverage this, we propose\nDomain-aware Dynamic Network (DDN), which is a high-capacity dynamic network in\nwhich each layer contains multiple weights. During inference, based on the\ninput domain, DDN dynamically combines those weights into one single weight\nthat specializes in the given domain. This way, DDN can keep the inference-time\ncomplexity low but still maintain a high capacity. Experiments show that\nwithout increasing the parameters, FLOPs, and actual latency, DDN achieves up\nto 2.6\\% higher AP50 than a static network on the BDD100K object-detection\nbenchmark.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 12:00:58 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Zhang", "Tianyuan", ""], ["Wu", "Bichen", ""], ["Wang", "Xin", ""], ["Gonzalez", "Joseph", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1911.13239", "submitter": "Wenyan Cong", "authors": "Wenyan Cong, Jianfu Zhang, Li Niu, Liu Liu, Zhixin Ling, Weiyuan Li,\n  Liqing Zhang", "title": "DoveNet: Deep Image Harmonization via Domain Verification", "comments": "Accepted by CVPR2020. arXiv admin note: text overlap with\n  arXiv:1908.10526", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image composition is an important operation in image processing, but the\ninconsistency between foreground and background significantly degrades the\nquality of composite image. Image harmonization, aiming to make the foreground\ncompatible with the background, is a promising yet challenging task. However,\nthe lack of high-quality publicly available dataset for image harmonization\ngreatly hinders the development of image harmonization techniques. In this\nwork, we contribute an image harmonization dataset iHarmony4 by generating\nsynthesized composite images based on COCO (resp., Adobe5k, Flickr, day2night)\ndataset, leading to our HCOCO (resp., HAdobe5k, HFlickr, Hday2night)\nsub-dataset. Moreover, we propose a new deep image harmonization method DoveNet\nusing a novel domain verification discriminator, with the insight that the\nforeground needs to be translated to the same domain as background. Extensive\nexperiments on our constructed dataset demonstrate the effectiveness of our\nproposed method. Our dataset and code are available at\nhttps://github.com/bcmi/Image_Harmonization_Datasets.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 07:14:50 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2020 03:56:44 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2020 08:38:11 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Cong", "Wenyan", ""], ["Zhang", "Jianfu", ""], ["Niu", "Li", ""], ["Liu", "Liu", ""], ["Ling", "Zhixin", ""], ["Li", "Weiyuan", ""], ["Zhang", "Liqing", ""]]}, {"id": "1911.13251", "submitter": "Jiangtong Li", "authors": "Jiangtong Li and Zhixin Ling and Li Niu and Liqing Zhang", "title": "Zero-Shot Sketch-Based Image Retrieval with Structure-aware Asymmetric\n  Disentanglement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of Sketch-Based Image Retrieval (SBIR) is using free-hand sketches\nto retrieve images of the same category from a natural image gallery. However,\nSBIR requires all test categories to be seen during training, which cannot be\nguaranteed in real-world applications. So we investigate more challenging\nZero-Shot SBIR (ZS-SBIR), in which test categories do not appear in the\ntraining stage. After realizing that sketches mainly contain structure\ninformation while images contain additional appearance information, we attempt\nto achieve structure-aware retrieval via asymmetric disentanglement.For this\npurpose, we propose our STRucture-aware Asymmetric Disentanglement (STRAD)\nmethod, in which image features are disentangled into structure features and\nappearance features while sketch features are only projected to structure\nspace. Through disentangling structure and appearance space, bi-directional\ndomain translation is performed between the sketch domain and the image domain.\nExtensive experiments demonstrate that our STRAD method remarkably outperforms\nstate-of-the-art methods on three large-scale benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 17:43:45 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 02:13:30 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Li", "Jiangtong", ""], ["Ling", "Zhixin", ""], ["Niu", "Li", ""], ["Zhang", "Liqing", ""]]}, {"id": "1911.13269", "submitter": "Michael Tarasiou", "authors": "Michail Tarasiou, Stefanos Zafeiriou", "title": "Extracting deep local features to detect manipulated images of human\n  faces", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in computer vision and machine learning have made it\npossible to create realistic manipulated videos of human faces, raising the\nissue of ensuring adequate protection against the malevolent effects unlocked\nby such capabilities. In this paper we propose local image features that are\nshared across manipulated regions are the key element for the automatic\ndetection of manipulated face images. We also design a lightweight architecture\nwith the correct structural biases for extracting such features and derive a\nmultitask training scheme that consistently outperforms image class supervision\nalone. The trained networks achieve state-of-the-art results in the\nFaceForensics++ dataset using significantly reduced number of parameters and\nare shown to work well in detecting fully generated face images.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 18:10:36 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 11:24:54 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Tarasiou", "Michail", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1911.13270", "submitter": "Andrew Gambardella", "authors": "Andrew Gambardella, At{\\i}l{\\i}m G\\\"une\\c{s} Baydin, Philip H. S. Torr", "title": "Transflow Learning: Repurposing Flow Models Without Retraining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that deep generative models have a rich latent space, and\nthat it is possible to smoothly manipulate their outputs by traversing this\nlatent space. Recently, architectures have emerged that allow for more complex\nmanipulations, such as making an image look as though it were from a different\nclass, or painted in a certain style. These methods typically require large\namounts of training in order to learn a single class of manipulations. We\npresent Transflow Learning, a method for transforming a pre-trained generative\nmodel so that its outputs more closely resemble data that we provide\nafterwards. In contrast to previous methods, Transflow Learning does not\nrequire any training at all, and instead warps the probability distribution\nfrom which we sample latent vectors using Bayesian inference. Transflow\nLearning can be used to solve a wide variety of tasks, such as neural style\ntransfer and few-shot classification.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 18:14:53 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 14:09:04 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Gambardella", "Andrew", ""], ["Baydin", "At\u0131l\u0131m G\u00fcne\u015f", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1911.13271", "submitter": "Kang Yeol Kim", "authors": "Wonwoong Cho, Kangyeol Kim, Eungyeup Kim, Hyunwoo J. Kim, Jaegul Choo", "title": "Unpaired Image Translation via Adaptive Convolution-based Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disentangling content and style information of an image has played an\nimportant role in recent success in image translation. In this setting, how to\ninject given style into an input image containing its own content is an\nimportant issue, but existing methods followed relatively simple approaches,\nleaving room for improvement especially when incorporating significant style\nchanges. In response, we propose an advanced normalization technique based on\nadaptive convolution (AdaCoN), in order to properly impose style information\ninto the content of an input image. In detail, after locally standardizing the\ncontent representation in a channel-wise manner, AdaCoN performs adaptive\nconvolution where the convolution filter weights are dynamically estimated\nusing the encoded style representation. The flexibility of AdaCoN can handle\ncomplicated image translation tasks involving significant style changes. Our\nqualitative and quantitative experiments demonstrate the superiority of our\nproposed method against various existing approaches that inject the style into\nthe content.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 18:16:03 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Cho", "Wonwoong", ""], ["Kim", "Kangyeol", ""], ["Kim", "Eungyeup", ""], ["Kim", "Hyunwoo J.", ""], ["Choo", "Jaegul", ""]]}, {"id": "1911.13273", "submitter": "Alireza Mehrtash", "authors": "Alireza Mehrtash, William M. Wells III, Clare M. Tempany, Purang\n  Abolmaesumi, Tina Kapur", "title": "Confidence Calibration and Predictive Uncertainty Estimation for Deep\n  Medical Image Segmentation", "comments": "Journal of IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": "10.1109/TMI.2020.3006437", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully convolutional neural networks (FCNs), and in particular U-Nets, have\nachieved state-of-the-art results in semantic segmentation for numerous medical\nimaging applications. Moreover, batch normalization and Dice loss have been\nused successfully to stabilize and accelerate training. However, these networks\nare poorly calibrated i.e. they tend to produce overconfident predictions both\nin correct and erroneous classifications, making them unreliable and hard to\ninterpret. In this paper, we study predictive uncertainty estimation in FCNs\nfor medical image segmentation. We make the following contributions: 1) We\nsystematically compare cross entropy loss with Dice loss in terms of\nsegmentation quality and uncertainty estimation of FCNs; 2) We propose model\nensembling for confidence calibration of the FCNs trained with batch\nnormalization and Dice loss; 3) We assess the ability of calibrated FCNs to\npredict segmentation quality of structures and detect out-of-distribution test\nexamples. We conduct extensive experiments across three medical image\nsegmentation applications of the brain, the heart, and the prostate to evaluate\nour contributions. The results of this study offer considerable insight into\nthe predictive uncertainty estimation and out-of-distribution detection in\nmedical image segmentation and provide practical recipes for confidence\ncalibration. Moreover, we consistently demonstrate that model ensembling\nimproves confidence calibration.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 18:20:26 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 19:33:15 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Mehrtash", "Alireza", ""], ["Wells", "William M.", "III"], ["Tempany", "Clare M.", ""], ["Abolmaesumi", "Purang", ""], ["Kapur", "Tina", ""]]}, {"id": "1911.13287", "submitter": "Feihu Zhang", "authors": "Feihu Zhang, Xiaojuan Qi, Ruigang Yang, Victor Prisacariu, Benjamin\n  Wah, Philip Torr", "title": "Domain-invariant Stereo Matching Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art stereo matching networks have difficulties in generalizing\nto new unseen environments due to significant domain differences, such as\ncolor, illumination, contrast, and texture. In this paper, we aim at designing\na domain-invariant stereo matching network (DSMNet) that generalizes well to\nunseen scenes. To achieve this goal, we propose i) a novel \"domain\nnormalization\" approach that regularizes the distribution of learned\nrepresentations to allow them to be invariant to domain differences, and ii) a\ntrainable non-local graph-based filter for extracting robust structural and\ngeometric representations that can further enhance domain-invariant\ngeneralizations. When trained on synthetic data and generalized to real test\nsets, our model performs significantly better than all state-of-the-art models.\nIt even outperforms some deep learning models (e.g. MC-CNN) fine-tuned with\ntest-domain data.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 18:41:26 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Zhang", "Feihu", ""], ["Qi", "Xiaojuan", ""], ["Yang", "Ruigang", ""], ["Prisacariu", "Victor", ""], ["Wah", "Benjamin", ""], ["Torr", "Philip", ""]]}, {"id": "1911.13299", "submitter": "Vivek Ramanujan", "authors": "Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi,\n  Mohammad Rastegari", "title": "What's Hidden in a Randomly Weighted Neural Network?", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a neural network is synonymous with learning the values of the\nweights. By contrast, we demonstrate that randomly weighted neural networks\ncontain subnetworks which achieve impressive performance without ever training\nthe weight values. Hidden in a randomly weighted Wide ResNet-50 we show that\nthere is a subnetwork (with random weights) that is smaller than, but matches\nthe performance of a ResNet-34 trained on ImageNet. Not only do these\n\"untrained subnetworks\" exist, but we provide an algorithm to effectively find\nthem. We empirically show that as randomly weighted neural networks with fixed\nweights grow wider and deeper, an \"untrained subnetwork\" approaches a network\nwith learned weights in accuracy. Our code and pretrained models are available\nat https://github.com/allenai/hidden-networks.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 18:56:53 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 01:30:39 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Ramanujan", "Vivek", ""], ["Wortsman", "Mitchell", ""], ["Kembhavi", "Aniruddha", ""], ["Farhadi", "Ali", ""], ["Rastegari", "Mohammad", ""]]}]