[{"id": "1504.00028", "submitter": "Zhangyang Wang", "authors": "Zhangyang Wang, Jianchao Yang, Hailin Jin, Eli Shechtman, Aseem\n  Agarwala, Jonathan Brandt, Thomas S. Huang", "title": "Real-World Font Recognition Using Deep Network and Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address a challenging fine-grain classification problem: recognizing a\nfont style from an image of text. In this task, it is very easy to generate\nlots of rendered font examples but very hard to obtain real-world labeled\nimages. This real-to-synthetic domain gap caused poor generalization to new\nreal data in previous methods (Chen et al. (2014)). In this paper, we refer to\nConvolutional Neural Networks, and use an adaptation technique based on a\nStacked Convolutional Auto-Encoder that exploits unlabeled real-world images\ncombined with synthetic data. The proposed method achieves an accuracy of\nhigher than 80% (top-5) on a real-world dataset.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 20:30:00 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Wang", "Zhangyang", ""], ["Yang", "Jianchao", ""], ["Jin", "Hailin", ""], ["Shechtman", "Eli", ""], ["Agarwala", "Aseem", ""], ["Brandt", "Jonathan", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1504.00045", "submitter": "Yongxin Yang", "authors": "Zhiyuan Shi, Yongxin Yang, Timothy M. Hospedales and Tao Xiang", "title": "Weakly Supervised Learning of Objects, Attributes and their Associations", "comments": "14 pages, Accepted to ECCV 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When humans describe images they tend to use combinations of nouns and\nadjectives, corresponding to objects and their associated attributes\nrespectively. To generate such a description automatically, one needs to model\nobjects, attributes and their associations. Conventional methods require strong\nannotation of object and attribute locations, making them less scalable. In\nthis paper, we model object-attribute associations from weakly labelled images,\nsuch as those widely available on media sharing sites (e.g. Flickr), where only\nimage-level labels (either object or attributes) are given, without their\nlocations and associations. This is achieved by introducing a novel weakly\nsupervised non-parametric Bayesian model. Once learned, given a new image, our\nmodel can describe the image, including objects, attributes and their\nassociations, as well as their locations and segmentation. Extensive\nexperiments on benchmark datasets demonstrate that our weakly supervised model\nperforms at par with strongly supervised models on tasks such as image\ndescription and retrieval based on object-attribute associations.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 21:18:18 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Shi", "Zhiyuan", ""], ["Yang", "Yongxin", ""], ["Hospedales", "Timothy M.", ""], ["Xiang", "Tao", ""]]}, {"id": "1504.00325", "submitter": "C. Lawrence Zitnick", "authors": "Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh\n  Gupta, Piotr Dollar, C. Lawrence Zitnick", "title": "Microsoft COCO Captions: Data Collection and Evaluation Server", "comments": "arXiv admin note: text overlap with arXiv:1411.4952", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe the Microsoft COCO Caption dataset and evaluation\nserver. When completed, the dataset will contain over one and a half million\ncaptions describing over 330,000 images. For the training and validation\nimages, five independent human generated captions will be provided. To ensure\nconsistency in evaluation of automatic caption generation algorithms, an\nevaluation server is used. The evaluation server receives candidate captions\nand scores them using several popular metrics, including BLEU, METEOR, ROUGE\nand CIDEr. Instructions for using the evaluation server are provided.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 18:13:43 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2015 20:21:16 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Chen", "Xinlei", ""], ["Fang", "Hao", ""], ["Lin", "Tsung-Yi", ""], ["Vedantam", "Ramakrishna", ""], ["Gupta", "Saurabh", ""], ["Dollar", "Piotr", ""], ["Zitnick", "C. Lawrence", ""]]}, {"id": "1504.00430", "submitter": "Hanyang Peng", "authors": "Hanyang Peng, Yong Fan", "title": "Direct l_(2,p)-Norm Learning for Feature Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel sparse learning based feature selection\nmethod that directly optimizes a large margin linear classification model\nsparsity with l_(2,p)-norm (0 < p < 1)subject to data-fitting constraints,\nrather than using the sparsity as a regularization term. To solve the direct\nsparsity optimization problem that is non-smooth and non-convex when 0<p<1, we\nprovide an efficient iterative algorithm with proved convergence by converting\nit to a convex and smooth optimization problem at every iteration step. The\nproposed algorithm has been evaluated based on publicly available datasets, and\nextensive comparison experiments have demonstrated that our algorithm could\nachieve feature selection performance competitive to state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 02:16:39 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Peng", "Hanyang", ""], ["Fan", "Yong", ""]]}, {"id": "1504.00580", "submitter": "Piotr Gawron jr.", "authors": "Mateusz Ostaszewski and Przemys{\\l}aw Sadowski and Piotr Gawron", "title": "Quantum image classification using principal component analysis", "comments": "9 pages", "journal-ref": "Theoretical and Applied Informatics, Vol. 27, No. 1, pp. 1-12\n  (2015)", "doi": "10.20904/271001", "report-no": null, "categories": "quant-ph cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel quantum algorithm for classification of images. The\nalgorithm is constructed using principal component analysis and von Neuman\nquantum measurements. In order to apply the algorithm we present a new quantum\nrepresentation of grayscale images.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 14:53:51 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Ostaszewski", "Mateusz", ""], ["Sadowski", "Przemys\u0142aw", ""], ["Gawron", "Piotr", ""]]}, {"id": "1504.00593", "submitter": "Emanuele Olivetti", "authors": "Emanuele Olivetti, Thien Bao Nguyen, Paolo Avesani", "title": "The Approximation of the Dissimilarity Projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion magnetic resonance imaging (dMRI) data allow to reconstruct the 3D\npathways of axons within the white matter of the brain as a tractography. The\nanalysis of tractographies has drawn attention from the machine learning and\npattern recognition communities providing novel challenges such as finding an\nappropriate representation space for the data. Many of the current learning\nalgorithms require the input to be from a vectorial space. This requirement\ncontrasts with the intrinsic nature of the tractography because its basic\nelements, called streamlines or tracks, have different lengths and different\nnumber of points and for this reason they cannot be directly represented in a\ncommon vectorial space. In this work we propose the adoption of the\ndissimilarity representation which is an Euclidean embedding technique defined\nby selecting a set of streamlines called prototypes and then mapping any new\nstreamline to the vector of distances from prototypes. We investigate the\ndegree of approximation of this projection under different prototype selection\npolicies and prototype set sizes in order to characterise its use on\ntractography data. Additionally we propose the use of a scalable approximation\nof the most effective prototype selection policy that provides fast and\naccurate dissimilarity approximations of complete tractographies.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 15:47:46 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Olivetti", "Emanuele", ""], ["Nguyen", "Thien Bao", ""], ["Avesani", "Paolo", ""]]}, {"id": "1504.00641", "submitter": "Ankit Patel", "authors": "Ankit B. Patel, Tan Nguyen and Richard G. Baraniuk", "title": "A Probabilistic Theory of Deep Learning", "comments": "56 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": "Rice University Electrical and Computer Engineering Dept. Technical\n  Report No 2015-1", "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A grand challenge in machine learning is the development of computational\nalgorithms that match or outperform humans in perceptual inference tasks that\nare complicated by nuisance variation. For instance, visual object recognition\ninvolves the unknown object position, orientation, and scale in object\nrecognition while speech recognition involves the unknown voice pronunciation,\npitch, and speed. Recently, a new breed of deep learning algorithms have\nemerged for high-nuisance inference tasks that routinely yield pattern\nrecognition systems with near- or super-human capabilities. But a fundamental\nquestion remains: Why do they work? Intuitions abound, but a coherent framework\nfor understanding, analyzing, and synthesizing deep learning architectures has\nremained elusive. We answer this question by developing a new probabilistic\nframework for deep learning based on the Deep Rendering Model: a generative\nprobabilistic model that explicitly captures latent nuisance variation. By\nrelaxing the generative model to a discriminative one, we can recover two of\nthe current leading deep learning systems, deep convolutional neural networks\nand random decision forests, providing insights into their successes and\nshortcomings, as well as a principled route to their improvement.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 18:38:38 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Patel", "Ankit B.", ""], ["Nguyen", "Tan", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1504.00702", "submitter": "Sergey Levine", "authors": "Sergey Levine, Chelsea Finn, Trevor Darrell, Pieter Abbeel", "title": "End-to-End Training of Deep Visuomotor Policies", "comments": "updating with revisions for JMLR final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy search methods can allow robots to learn control policies for a wide\nrange of tasks, but practical applications of policy search often require\nhand-engineered components for perception, state estimation, and low-level\ncontrol. In this paper, we aim to answer the following question: does training\nthe perception and control systems jointly end-to-end provide better\nperformance than training each component separately? To this end, we develop a\nmethod that can be used to learn policies that map raw image observations\ndirectly to torques at the robot's motors. The policies are represented by deep\nconvolutional neural networks (CNNs) with 92,000 parameters, and are trained\nusing a partially observed guided policy search method, which transforms policy\nsearch into supervised learning, with supervision provided by a simple\ntrajectory-centric reinforcement learning method. We evaluate our method on a\nrange of real-world manipulation tasks that require close coordination between\nvision and control, such as screwing a cap onto a bottle, and present simulated\ncomparisons to a range of prior policy search methods.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 22:23:51 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2015 22:46:23 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2015 04:33:01 GMT"}, {"version": "v4", "created": "Mon, 7 Dec 2015 16:39:49 GMT"}, {"version": "v5", "created": "Tue, 19 Apr 2016 01:33:13 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Levine", "Sergey", ""], ["Finn", "Chelsea", ""], ["Darrell", "Trevor", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1504.00905", "submitter": "Jose Lopez", "authors": "Jose A. Lopez, Octavia Camps, Mario Sznaier", "title": "Robust Anomaly Detection Using Semidefinite Programming", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach, based on polynomial optimization and the\nmethod of moments, to the problem of anomaly detection. The proposed technique\nonly requires information about the statistical moments of the normal-state\ndistribution of the features of interest and compares favorably with existing\napproaches (such as Parzen windows and 1-class SVM). In addition, it provides a\nsuccinct description of the normal state. Thus, it leads to a substantial\nsimplification of the the anomaly detection problem when working with higher\ndimensional datasets.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 18:20:36 GMT"}, {"version": "v2", "created": "Sat, 30 May 2015 15:58:36 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Lopez", "Jose A.", ""], ["Camps", "Octavia", ""], ["Sznaier", "Mario", ""]]}, {"id": "1504.00923", "submitter": "Fred Richardson", "authors": "Fred Richardson, Douglas Reynolds, Najim Dehak", "title": "A Unified Deep Neural Network for Speaker and Language Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learned feature representations and sub-phoneme posteriors from Deep Neural\nNetworks (DNNs) have been used separately to produce significant performance\ngains for speaker and language recognition tasks. In this work we show how\nthese gains are possible using a single DNN for both speaker and language\nrecognition. The unified DNN approach is shown to yield substantial performance\nimprovements on the the 2013 Domain Adaptation Challenge speaker recognition\ntask (55% reduction in EER for the out-of-domain condition) and on the NIST\n2011 Language Recognition Evaluation (48% reduction in EER for the 30s test\ncondition).\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 19:57:06 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Richardson", "Fred", ""], ["Reynolds", "Douglas", ""], ["Dehak", "Najim", ""]]}, {"id": "1504.00976", "submitter": "Ankit Parekh", "authors": "Ankit Parekh, Ivan W. Selesnick", "title": "Convex Denoising using Non-Convex Tight Frame Regularization", "comments": "5 pages, 6 figures", "journal-ref": "IEEE Signal Processing Letters, 22(10):1786-1790, Oct. 2015", "doi": "10.1109/LSP.2015.2432095", "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of signal denoising using a sparse\ntight-frame analysis prior. The L1 norm has been extensively used as a\nregularizer to promote sparsity; however, it tends to under-estimate non-zero\nvalues of the underlying signal. To more accurately estimate non-zero values,\nwe propose the use of a non-convex regularizer, chosen so as to ensure\nconvexity of the objective function. The convexity of the objective function is\nensured by constraining the parameter of the non-convex penalty. We use ADMM to\nobtain a solution and show how to guarantee that ADMM converges to the global\noptimum of the objective function. We illustrate the proposed method for 1D and\n2D signal denoising.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2015 03:28:01 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 15:55:05 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Parekh", "Ankit", ""], ["Selesnick", "Ivan W.", ""]]}, {"id": "1504.00983", "submitter": "Chen Sun", "authors": "Chen Sun and Sanketh Shetty and Rahul Sukthankar and Ram Nevatia", "title": "Temporal Localization of Fine-Grained Actions in Videos by Domain\n  Transfer from Web Images", "comments": "Camera ready version for ACM Multimedia 2015", "journal-ref": null, "doi": "10.1145/2733373.2806226", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of fine-grained action localization from temporally\nuntrimmed web videos. We assume that only weak video-level annotations are\navailable for training. The goal is to use these weak labels to identify\ntemporal segments corresponding to the actions, and learn models that\ngeneralize to unconstrained web videos. We find that web images queried by\naction names serve as well-localized highlights for many actions, but are\nnoisily labeled. To solve this problem, we propose a simple yet effective\nmethod that takes weak video labels and noisy image labels as input, and\ngenerates localized action frames as output. This is achieved by cross-domain\ntransfer between video frames and web images, using pre-trained deep\nconvolutional neural networks. We then use the localized action frames to train\naction recognition models with long short-term memory networks. We collect a\nfine-grained sports action data set FGA-240 of more than 130,000 YouTube\nvideos. It has 240 fine-grained actions under 85 sports activities. Convincing\nresults are shown on the FGA-240 data set, as well as the THUMOS 2014\nlocalization data set with untrimmed training videos.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2015 05:40:55 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2015 07:04:34 GMT"}], "update_date": "2015-08-05", "authors_parsed": [["Sun", "Chen", ""], ["Shetty", "Sanketh", ""], ["Sukthankar", "Rahul", ""], ["Nevatia", "Ram", ""]]}, {"id": "1504.01013", "submitter": "Chunhua Shen", "authors": "Guosheng Lin, Chunhua Shen, Anton van dan Hengel, Ian Reid", "title": "Efficient piecewise training of deep structured models for semantic\n  segmentation", "comments": "Appearing in IEEE Conf. Computer Vision and Pattern Recognition\n  (CVPR) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in semantic image segmentation have mostly been achieved by\ntraining deep convolutional neural networks (CNNs). We show how to improve\nsemantic segmentation through the use of contextual information; specifically,\nwe explore `patch-patch' context between image regions, and `patch-background'\ncontext. For learning from the patch-patch context, we formulate Conditional\nRandom Fields (CRFs) with CNN-based pairwise potential functions to capture\nsemantic correlations between neighboring patches. Efficient piecewise training\nof the proposed deep structured model is then applied to avoid repeated\nexpensive CRF inference for back propagation. For capturing the\npatch-background context, we show that a network design with traditional\nmulti-scale image input and sliding pyramid pooling is effective for improving\nperformance. Our experimental results set new state-of-the-art performance on a\nnumber of popular semantic segmentation datasets, including NYUDv2, PASCAL VOC\n2012, PASCAL-Context, and SIFT-flow. In particular, we achieve an\nintersection-over-union score of 78.0 on the challenging PASCAL VOC 2012\ndataset.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2015 14:26:23 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2015 02:05:01 GMT"}, {"version": "v3", "created": "Wed, 9 Mar 2016 03:07:34 GMT"}, {"version": "v4", "created": "Mon, 6 Jun 2016 00:26:44 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Lin", "Guosheng", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van dan", ""], ["Reid", "Ian", ""]]}, {"id": "1504.01025", "submitter": "Zhihan Lv", "authors": "Zhihan Lv, Liangbing Feng, Shengzhong Feng, Haibo Li", "title": "Preprint Extending Touch-less Interaction on Vision Based Wearable\n  Device", "comments": "This is the preprint version of our paper on IEEE Virtual Reality\n  Conference 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the preprint version of our paper on IEEE Virtual Reality Conference\n2015. A touch-less interaction technology on vision based wearable device is\ndesigned and evaluated. Users interact with the application with dynamic\nhands/feet gestures in front of the camera. Several proof-of-concept prototypes\nwith eleven dynamic gestures are developed based on the touch-less interaction.\nAt last, a comparing user study evaluation is proposed to demonstrate the\nusability of the touch-less approach, as well as the impact on user's emotion,\nrunning on a wearable framework or Google Glass.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2015 17:12:19 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2015 09:14:35 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Lv", "Zhihan", ""], ["Feng", "Liangbing", ""], ["Feng", "Shengzhong", ""], ["Li", "Haibo", ""]]}, {"id": "1504.01052", "submitter": "Gregor Ehrensperger", "authors": "Gregor Ehrensperger, Alexander Ostermann and Felix Schwitzer", "title": "Fast algorithms for morphological operations using run-length encoded\n  binary images", "comments": "17 pages, 2 figures. Submitted to Elsevier (Pattern Recognition). For\n  the associated source code, see\n  https://numerical-analysis.uibk.ac.at/g.ehrensperger", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents innovative algorithms to efficiently compute erosions and\ndilations of run-length encoded (RLE) binary images with arbitrary shaped\nstructuring elements. An RLE image is given by a set of runs, where a run is a\nhorizontal concatenation of foreground pixels. The proposed algorithms extract\nthe skeleton of the structuring element and build distance tables of the input\nimage, which are storing the distance to the next background pixel on the left\nand right hand sides. This information is then used to speed up the\ncalculations of the erosion and dilation operator by enabling the use of\ntechniques which allow to skip the analysis of certain pixels whenever a hit or\nmiss occurs. Additionally the input image gets trimmed during the preprocessing\nsteps on the base of two primitive criteria. Experimental results show the\nadvantages over other algorithms. The source code of our algorithms is\navailable in C++.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2015 20:51:43 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Ehrensperger", "Gregor", ""], ["Ostermann", "Alexander", ""], ["Schwitzer", "Felix", ""]]}, {"id": "1504.01124", "submitter": "Amit K C", "authors": "Amit Kumar K.C. and Laurent Jacques and Christophe De Vleeschouwer", "title": "Discriminative and Efficient Label Propagation on Complementary Graphs\n  for Multi-Object Tracking", "comments": "15 pages, 6 figures, submitted to PAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of detections, detected at each time instant independently, we\ninvestigate how to associate them across time. This is done by propagating\nlabels on a set of graphs, each graph capturing how either the spatio-temporal\nor the appearance cues promote the assignment of identical or distinct labels\nto a pair of detections. The graph construction is motivated by a locally\nlinear embedding of the detection features. Interestingly, the neighborhood of\na node in appearance graph is defined to include all the nodes for which the\nappearance feature is available (even if they are temporally distant). This\ngives our framework the uncommon ability to exploit the appearance features\nthat are available only sporadically. Once the graphs have been defined,\nmulti-object tracking is formulated as the problem of finding a label\nassignment that is consistent with the constraints captured each graph, which\nresults into a difference of convex (DC) program. We propose to decompose the\nglobal objective function into node-wise sub-problems. This not only allows a\ncomputationally efficient solution, but also supports an incremental and\nscalable construction of the graph, thereby making the framework applicable to\nlarge graphs and practical tracking scenarios. Moreover, it opens the\npossibility of parallel implementation.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2015 13:56:00 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2015 09:46:32 GMT"}, {"version": "v3", "created": "Tue, 1 Dec 2015 17:14:36 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["C.", "Amit Kumar K.", ""], ["Jacques", "Laurent", ""], ["De Vleeschouwer", "Christophe", ""]]}, {"id": "1504.01220", "submitter": "Xiaodan Liang", "authors": "Si Liu and Xiaodan Liang and Luoqi Liu and Xiaohui Shen and Jianchao\n  Yang and Changsheng Xu and Liang Lin and Xiaochun Cao and Shuicheng Yan", "title": "Matching-CNN Meets KNN: Quasi-Parametric Human Parsing", "comments": "This manuscript is the accepted version for CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both parametric and non-parametric approaches have demonstrated encouraging\nperformances in the human parsing task, namely segmenting a human image into\nseveral semantic regions (e.g., hat, bag, left arm, face). In this work, we aim\nto develop a new solution with the advantages of both methodologies, namely\nsupervision from annotated data and the flexibility to use newly annotated\n(possibly uncommon) images, and present a quasi-parametric human parsing model.\nUnder the classic K Nearest Neighbor (KNN)-based nonparametric framework, the\nparametric Matching Convolutional Neural Network (M-CNN) is proposed to predict\nthe matching confidence and displacements of the best matched region in the\ntesting image for a particular semantic region in one KNN image. Given a\ntesting image, we first retrieve its KNN images from the\nannotated/manually-parsed human image corpus. Then each semantic region in each\nKNN image is matched with confidence to the testing image using M-CNN, and the\nmatched regions from all KNN images are further fused, followed by a superpixel\nsmoothing procedure to obtain the ultimate human parsing result. The M-CNN\ndiffers from the classic CNN in that the tailored cross image matching filters\nare introduced to characterize the matching between the testing image and the\nsemantic region of a KNN image. The cross image matching filters are defined at\ndifferent convolutional layers, each aiming to capture a particular range of\ndisplacements. Comprehensive evaluations over a large dataset with 7,700\nannotated human images well demonstrate the significant performance gain from\nthe quasi-parametric model over the state-of-the-arts, for the human parsing\ntask.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 07:20:02 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Liu", "Si", ""], ["Liang", "Xiaodan", ""], ["Liu", "Luoqi", ""], ["Shen", "Xiaohui", ""], ["Yang", "Jianchao", ""], ["Xu", "Changsheng", ""], ["Lin", "Liang", ""], ["Cao", "Xiaochun", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1504.01420", "submitter": "Sunil Kumar Kopparapu Dr", "authors": "Sunil Kopparapu and Devanuj and Akhilesh Srivastava and P.V.S. Rao", "title": "Knowledge driven Offline to Online Script Conversion", "comments": "4 pages, 5 figures, KBCS 2004", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of offline to online script conversion is a challenging and an\nill-posed problem. The interest in offline to online conversion exists because\nthere are a plethora of robust algorithms in online script literature which can\nnot be used on offline scripts. In this paper, we propose a method, based on\nheuristics, to extract online script information from offline bitmap image. We\nshow the performance of the proposed method on a real sample signature offline\nimage, whose online information is known.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 21:27:45 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Kopparapu", "Sunil", ""], ["Devanuj", "", ""], ["Srivastava", "Akhilesh", ""], ["Rao", "P. V. S.", ""]]}, {"id": "1504.01441", "submitter": "Orazio Gallo", "authors": "Orazio Gallo (1), Alejandro Troccoli (1), Jun Hu (1 and 2), Kari Pulli\n  (1 and 3), Jan Kautz (1) ((1) NVIDIA, (2) Duke University, (3) Light)", "title": "Locally Non-rigid Registration for Mobile HDR Photography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image registration for stack-based HDR photography is challenging. If not\nproperly accounted for, camera motion and scene changes result in artifacts in\nthe composite image. Unfortunately, existing methods to address this problem\nare either accurate, but too slow for mobile devices, or fast, but prone to\nfailing. We propose a method that fills this void: our approach is extremely\nfast---under 700ms on a commercial tablet for a pair of 5MP images---and\nprevents the artifacts that arise from insufficient registration quality.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 00:29:54 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2015 00:33:20 GMT"}, {"version": "v3", "created": "Tue, 5 May 2015 00:15:00 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Gallo", "Orazio", "", "NVIDIA"], ["Troccoli", "Alejandro", "", "NVIDIA"], ["Hu", "Jun", "", "1 and 2"], ["Pulli", "Kari", "", "1 and 3"], ["Kautz", "Jan", "", "NVIDIA"]]}, {"id": "1504.01476", "submitter": "Sunil Kumar Kopparapu Dr", "authors": "Lajish V. L. and Sunil Kumar Kopparapu", "title": "Mobile Phone Based Vehicle License Plate Recognition for Road Policing", "comments": "7 pages; PReMI Experiential Workshop, Delhi", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identity of a vehicle is done through the vehicle license plate by traffic\npolice in general. Au- tomatic vehicle license plate recognition has several\napplications in intelligent traffic management systems. The security situation\nacross the globe and particularly in India demands a need to equip the traffic\npolice with a system that enables them to get instant details of a vehicle. The\nsystem should be easy to use, should be mobile, and work 24 x 7. In this paper,\nwe describe a mobile phone based, client-server architected, license plate\nrecognition system. While we use the state of the art image processing and\npattern recognition algorithms tuned for Indian conditions to automatically\nrecognize non-uniform license plates, the main contribution is in creating an\nend to end usable solution. The client application runs on a mobile device and\na server application, with access to vehicle information database, is hosted\ncentrally. The solution enables capture of license plate image captured by the\nphone camera and passes to the server; on the server the license plate number\nis recognized; the data associated with the number plate is then sent back to\nthe mobile device, instantaneously. We describe the end to end system\narchitecture in detail. A working prototype of the proposed system has been\nimplemented in the lab environment.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 05:25:42 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["L.", "Lajish V.", ""], ["Kopparapu", "Sunil Kumar", ""]]}, {"id": "1504.01488", "submitter": "Sunil Kumar Kopparapu Dr", "authors": "Sunil Kumar Kopparapu, Lajish VL", "title": "On-line Handwritten Devanagari Character Recognition using Fuzzy\n  Directional Features", "comments": "6 pages; 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new feature set for use in the recognition of on-line\nhandwritten Devanagari script based on Fuzzy Directional Features. Experiments\nare conducted for the automatic recognition of isolated handwritten character\nprimitives (sub-character units). Initially we describe the proposed feature\nset, called the Fuzzy Directional Features (FDF) and then show how these\nfeatures can be effectively utilized for writer independent character\nrecognition. Experimental results show that FDF set perform well for writer\nindependent data set at stroke level recognition. The main contribution of this\npaper is the introduction of a novel feature set and establish experimentally\nits ability in recognition of handwritten Devanagari script.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 06:31:58 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Kopparapu", "Sunil Kumar", ""], ["VL", "Lajish", ""]]}, {"id": "1504.01492", "submitter": "Chunhua Shen", "authors": "Peng Wang, Chunhua Shen, Anton van den Hengel", "title": "Efficient SDP Inference for Fully-connected CRFs Based on Low-rank\n  Decomposition", "comments": "15 pages. A conference version of this work appears in Proc. IEEE\n  Conference on Computer Vision and Pattern Recognition, 2015", "journal-ref": null, "doi": "10.1109/CVPR.2015.7298942", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional Random Fields (CRF) have been widely used in a variety of\ncomputer vision tasks. Conventional CRFs typically define edges on neighboring\nimage pixels, resulting in a sparse graph such that efficient inference can be\nperformed. However, these CRFs fail to model long-range contextual\nrelationships. Fully-connected CRFs have thus been proposed. While there are\nefficient approximate inference methods for such CRFs, usually they are\nsensitive to initialization and make strong assumptions. In this work, we\ndevelop an efficient, yet general algorithm for inference on fully-connected\nCRFs. The algorithm is based on a scalable SDP algorithm and the low- rank\napproximation of the similarity/kernel matrix. The core of the proposed\nalgorithm is a tailored quasi-Newton method that takes advantage of the\nlow-rank matrix approximation when solving the specialized SDP dual problem.\nExperiments demonstrate that our method can be applied on fully-connected CRFs\nthat cannot be solved previously, such as pixel-level image co-segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 06:43:50 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Wang", "Peng", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1504.01502", "submitter": "Tony Lindeberg", "authors": "Tony Lindeberg", "title": "Separable time-causal and time-recursive spatio-temporal receptive\n  fields", "comments": "12 pages, 2 figures, 2 tables. arXiv admin note: substantial text\n  overlap with arXiv:1404.2037", "journal-ref": "Proc SSVM 2015: Scale-Space and Variational Methods for Computer\n  Vision, Springer LNCS vol 9087, pages 90-102, 2015", "doi": "10.1007/978-3-319-18461-6_8", "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an improved model and theory for time-causal and time-recursive\nspatio-temporal receptive fields, obtained by a combination of Gaussian\nreceptive fields over the spatial domain and first-order integrators or\nequivalently truncated exponential filters coupled in cascade over the temporal\ndomain. Compared to previous spatio-temporal scale-space formulations in terms\nof non-enhancement of local extrema or scale invariance, these receptive fields\nare based on different scale-space axiomatics over time by ensuring\nnon-creation of new local extrema or zero-crossings with increasing temporal\nscale. Specifically, extensions are presented about parameterizing the\nintermediate temporal scale levels, analysing the resulting temporal dynamics\nand transferring the theory to a discrete implementation in terms of recursive\nfilters over time.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 07:29:54 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Lindeberg", "Tony", ""]]}, {"id": "1504.01515", "submitter": "Paris Giampouras", "authors": "Paris Giampouras, Konstantinos Themelis, Athanasios Rontogiannis and\n  Konstantinos Koutroumbas", "title": "Simultaneously sparse and low-rank abundance matrix estimation for\n  hyperspectral image unmixing", "comments": "30 pages, 9 figures", "journal-ref": null, "doi": "10.1109/TGRS.2016.2551327", "report-no": null, "categories": "cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a plethora of applications dealing with inverse problems, e.g. in image\nprocessing, social networks, compressive sensing, biological data processing\netc., the signal of interest is known to be structured in several ways at the\nsame time. This premise has recently guided the research to the innovative and\nmeaningful idea of imposing multiple constraints on the parameters involved in\nthe problem under study. For instance, when dealing with problems whose\nparameters form sparse and low-rank matrices, the adoption of suitably combined\nconstraints imposing sparsity and low-rankness, is expected to yield\nsubstantially enhanced estimation results. In this paper, we address the\nspectral unmixing problem in hyperspectral images. Specifically, two novel\nunmixing algorithms are introduced, in an attempt to exploit both spatial\ncorrelation and sparse representation of pixels lying in homogeneous regions of\nhyperspectral images. To this end, a novel convex mixed penalty term is first\ndefined consisting of the sum of the weighted $\\ell_1$ and the weighted nuclear\nnorm of the abundance matrix corresponding to a small area of the image\ndetermined by a sliding square window. This penalty term is then used to\nregularize a conventional quadratic cost function and impose simultaneously\nsparsity and row-rankness on the abundance matrix. The resulting regularized\ncost function is minimized by a) an incremental proximal sparse and low-rank\nunmixing algorithm and b) an algorithm based on the alternating minimization\nmethod of multipliers (ADMM). The effectiveness of the proposed algorithms is\nillustrated in experiments conducted both on simulated and real data.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 08:23:45 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2015 16:53:41 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Giampouras", "Paris", ""], ["Themelis", "Konstantinos", ""], ["Rontogiannis", "Athanasios", ""], ["Koutroumbas", "Konstantinos", ""]]}, {"id": "1504.01561", "submitter": "Zuxuan Wu", "authors": "Zuxuan Wu, Xi Wang, Yu-Gang Jiang, Hao Ye, Xiangyang Xue", "title": "Modeling Spatial-Temporal Clues in a Hybrid Deep Learning Framework for\n  Video Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifying videos according to content semantics is an important problem\nwith a wide range of applications. In this paper, we propose a hybrid deep\nlearning framework for video classification, which is able to model static\nspatial information, short-term motion, as well as long-term temporal clues in\nthe videos. Specifically, the spatial and the short-term motion features are\nextracted separately by two Convolutional Neural Networks (CNN). These two\ntypes of CNN-based features are then combined in a regularized feature fusion\nnetwork for classification, which is able to learn and utilize feature\nrelationships for improved performance. In addition, Long Short Term Memory\n(LSTM) networks are applied on top of the two features to further model\nlonger-term temporal clues. The main contribution of this work is the hybrid\nlearning framework that can model several important aspects of the video data.\nWe also show that (1) combining the spatial and the short-term motion features\nin the regularized fusion network is better than direct classification and\nfusion using the CNN with a softmax layer, and (2) the sequence-based LSTM is\nhighly complementary to the traditional classification strategy without\nconsidering the temporal frame orders. Extensive experiments are conducted on\ntwo popular and challenging benchmarks, the UCF-101 Human Actions and the\nColumbia Consumer Videos (CCV). On both benchmarks, our framework achieves\nto-date the best reported performance: $91.3\\%$ on the UCF-101 and $83.5\\%$ on\nthe CCV.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 11:53:46 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Wu", "Zuxuan", ""], ["Wang", "Xi", ""], ["Jiang", "Yu-Gang", ""], ["Ye", "Hao", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1504.01639", "submitter": "Marc Bola\\~nos", "authors": "Marc Bola\\~nos and Petia Radeva", "title": "Ego-Object Discovery", "comments": "9 pages, 13 figures, Submitted to: Image and Vision Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lifelogging devices are spreading faster everyday. This growth can represent\ngreat benefits to develop methods for extraction of meaningful information\nabout the user wearing the device and his/her environment. In this paper, we\npropose a semi-supervised strategy for easily discovering objects relevant to\nthe person wearing a first-person camera. Given an egocentric video/images\nsequence acquired by the camera, our algorithm uses both the appearance\nextracted by means of a convolutional neural network and an object refill\nmethodology that allows to discover objects even in case of small amount of\nobject appearance in the collection of images. An SVM filtering strategy is\napplied to deal with the great part of the False Positive object candidates\nfound by most of the state of the art object detectors. We validate our method\non a new egocentric dataset of 4912 daily images acquired by 4 persons as well\nas on both PASCAL 2012 and MSRC datasets. We obtain for all of them results\nthat largely outperform the state of the art approach. We make public both the\nEDUB dataset and the algorithm code.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 15:23:22 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2015 09:19:48 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Bola\u00f1os", "Marc", ""], ["Radeva", "Petia", ""]]}, {"id": "1504.01716", "submitter": "Brody Huval", "authors": "Brody Huval, Tao Wang, Sameep Tandon, Jeff Kiske, Will Song, Joel\n  Pazhayampallil, Mykhaylo Andriluka, Pranav Rajpurkar, Toki Migimatsu, Royce\n  Cheng-Yue, Fernando Mujica, Adam Coates, Andrew Y. Ng", "title": "An Empirical Evaluation of Deep Learning on Highway Driving", "comments": "Added a video for lane detection", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous groups have applied a variety of deep learning techniques to\ncomputer vision problems in highway perception scenarios. In this paper, we\npresented a number of empirical evaluations of recent deep learning advances.\nComputer vision, combined with deep learning, has the potential to bring about\na relatively inexpensive, robust solution to autonomous driving. To prepare\ndeep learning for industry uptake and practical applications, neural networks\nwill require large data sets that represent all possible driving environments\nand scenarios. We collect a large data set of highway data and apply deep\nlearning and computer vision algorithms to problems such as car and lane\ndetection. We show how existing convolutional neural networks (CNNs) can be\nused to perform lane and vehicle detection while running at frame rates\nrequired for a real-time system. Our results lend credence to the hypothesis\nthat deep learning holds promise for autonomous driving.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 19:41:59 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2015 19:53:22 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2015 01:27:14 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Huval", "Brody", ""], ["Wang", "Tao", ""], ["Tandon", "Sameep", ""], ["Kiske", "Jeff", ""], ["Song", "Will", ""], ["Pazhayampallil", "Joel", ""], ["Andriluka", "Mykhaylo", ""], ["Rajpurkar", "Pranav", ""], ["Migimatsu", "Toki", ""], ["Cheng-Yue", "Royce", ""], ["Mujica", "Fernando", ""], ["Coates", "Adam", ""], ["Ng", "Andrew Y.", ""]]}, {"id": "1504.01753", "submitter": "Xida Chen", "authors": "Xida Chen, Steve Sutphen, Paul Macoun, Yee-Hong Yang", "title": "Design and Implementation of a 3D Undersea Camera System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the design and development of an undersea camera\nsystem. The goal of our system is to provide a 3D model of the undersea habitat\nin a long-term continuous manner. The most important feature of our system is\nthe use of multiple cameras and multiple projectors, which is able to provide\naccurate 3D models with an accuracy of a millimeter. By introducing projectors\nin our system, we can use many different structured light methods for different\ntasks. There are two main advantages comparing our system with using ROVs or\nAUVs. First, our system can provide continuous monitoring of the undersea\nhabitat. Second, our system has a low hardware cost. Comparing to existing\ndeployed camera systems, the advantage of our system is that it can provide\naccurate 3D models and provides opportunities for future development of\ninnovative algorithms for undersea research.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 21:05:48 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Chen", "Xida", ""], ["Sutphen", "Steve", ""], ["Macoun", "Paul", ""], ["Yang", "Yee-Hong", ""]]}, {"id": "1504.01777", "submitter": "Junbin Gao Professor", "authors": "Yanfeng Sun and Junbin Gao and Xia Hong and Bamdev Mishra and Baocai\n  Yin", "title": "Heterogeneous Tensor Decomposition for Clustering via Manifold\n  Optimization", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensors or multiarray data are generalizations of matrices. Tensor clustering\nhas become a very important research topic due to the intrinsically rich\nstructures in real-world multiarray datasets. Subspace clustering based on\nvectorizing multiarray data has been extensively researched. However,\nvectorization of tensorial data does not exploit complete structure\ninformation. In this paper, we propose a subspace clustering algorithm without\nadopting any vectorization process. Our approach is based on a novel\nheterogeneous Tucker decomposition model. In contrast to existing techniques,\nwe propose a new clustering algorithm that alternates between different modes\nof the proposed heterogeneous tensor model. All but the last mode have\nclosed-form updates. Updating the last mode reduces to optimizing over the\nso-called multinomial manifold, for which we investigate second order\nRiemannian geometry and propose a trust-region algorithm. Numerical experiments\nshow that our proposed algorithm compete effectively with state-of-the-art\nclustering algorithms that are based on tensor factorization.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 23:18:34 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2015 02:53:10 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Sun", "Yanfeng", ""], ["Gao", "Junbin", ""], ["Hong", "Xia", ""], ["Mishra", "Bamdev", ""], ["Yin", "Baocai", ""]]}, {"id": "1504.01800", "submitter": "A.  Ben Hamza", "authors": "Mohammed Khader and A. Ben Hamza", "title": "A Multicomponent Approach to Nonrigid Registration of Diffusion Tensor\n  Images", "comments": "19 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nonrigid registration approach for diffusion tensor images using\na multicomponent information-theoretic measure. Explicit orientation\noptimization is enabled by incorporating tensor reorientation, which is\nnecessary for wrapping diffusion tensor images. Experimental results on\ndiffusion tensor images indicate the feasibility of the proposed approach and a\nmuch better performance compared to the affine registration method based on\nmutual information in terms of registration accuracy in the presence of\ngeometric distortion.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 02:02:55 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2015 12:27:57 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Khader", "Mohammed", ""], ["Hamza", "A. Ben", ""]]}, {"id": "1504.01806", "submitter": "Junbin Gao Professor", "authors": "Boyue Wang and Yongli Hu and Junbin Gao and Yanfeng Sun and Baocai Yin", "title": "Kernelized Low Rank Representation on Grassmann Manifolds", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low rank representation (LRR) has recently attracted great interest due to\nits pleasing efficacy in exploring low-dimensional subspace structures embedded\nin data. One of its successful applications is subspace clustering which means\ndata are clustered according to the subspaces they belong to. In this paper, at\na higher level, we intend to cluster subspaces into classes of subspaces. This\nis naturally described as a clustering problem on Grassmann manifold. The\nnovelty of this paper is to generalize LRR on Euclidean space onto an LRR model\non Grassmann manifold in a uniform kernelized framework. The new methods have\nmany applications in computer vision tasks. Several clustering experiments are\nconducted on handwritten digit images, dynamic textures, human face clips and\ntraffic scene sequences. The experimental results show that the proposed\nmethods outperform a number of state-of-the-art subspace clustering methods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 02:37:49 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Wang", "Boyue", ""], ["Hu", "Yongli", ""], ["Gao", "Junbin", ""], ["Sun", "Yanfeng", ""], ["Yin", "Baocai", ""]]}, {"id": "1504.01807", "submitter": "Junbin Gao Professor", "authors": "Boyue Wang and Yongli Hu and Junbin Gao and Yanfeng Sun and Baocai Yin", "title": "Low Rank Representation on Grassmann Manifolds: An Extrinsic Perspective", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer vision algorithms employ subspace models to represent data. The\nLow-rank representation (LRR) has been successfully applied in subspace\nclustering for which data are clustered according to their subspace structures.\nThe possibility of extending LRR on Grassmann manifold is explored in this\npaper. Rather than directly embedding Grassmann manifold into a symmetric\nmatrix space, an extrinsic view is taken by building the self-representation of\nLRR over the tangent space of each Grassmannian point. A new algorithm for\nsolving the proposed Grassmannian LRR model is designed and implemented.\nSeveral clustering experiments are conducted on handwritten digits dataset,\ndynamic texture video clips and YouTube celebrity face video data. The\nexperimental results show our method outperforms a number of existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 02:38:04 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Wang", "Boyue", ""], ["Hu", "Yongli", ""], ["Gao", "Junbin", ""], ["Sun", "Yanfeng", ""], ["Yin", "Baocai", ""]]}, {"id": "1504.01883", "submitter": "Narmada Naik", "authors": "Narmada Naik and G.N Rathna", "title": "Robust real time face recognition and tracking on gpu using fusion of\n  rgb and depth image", "comments": null, "journal-ref": null, "doi": "10.5121/csit.2015.50601", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a real-time face recognition system using kinect sensor.\nThe algorithm is implemented on GPU using opencl and significant speed\nimprovements are observed. We use kinect depth image to increase the robustness\nand reduce computational cost of conventional LBP based face recognition. The\nmain objective of this paper was to perform robust, high speed fusion based\nface recognition and tracking. The algorithm is mainly composed of three steps.\nFirst step is to detect all faces in the video using viola jones algorithm. The\nsecond step is online database generation using a tracking window on the face.\nA modified LBP feature vector is calculated using fusion information from depth\nand greyscale image on GPU. This feature vector is used to train a svm\nclassifier. Third step involves recognition of multiple faces based on our\nmodified feature vector.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 09:34:30 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Naik", "Narmada", ""], ["Rathna", "G. N", ""]]}, {"id": "1504.01920", "submitter": "Zuxuan Wu", "authors": "Hao Ye, Zuxuan Wu, Rui-Wei Zhao, Xi Wang, Yu-Gang Jiang, Xiangyang Xue", "title": "Evaluating Two-Stream CNN for Video Classification", "comments": "ACM ICMR'15", "journal-ref": null, "doi": "10.1145/2671188.2749406", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos contain very rich semantic information. Traditional hand-crafted\nfeatures are known to be inadequate in analyzing complex video semantics.\nInspired by the huge success of the deep learning methods in analyzing image,\naudio and text data, significant efforts are recently being devoted to the\ndesign of deep nets for video analytics. Among the many practical needs,\nclassifying videos (or video clips) based on their major semantic categories\n(e.g., \"skiing\") is useful in many applications. In this paper, we conduct an\nin-depth study to investigate important implementation options that may affect\nthe performance of deep nets on video classification. Our evaluations are\nconducted on top of a recent two-stream convolutional neural network (CNN)\npipeline, which uses both static frames and motion optical flows, and has\ndemonstrated competitive performance against the state-of-the-art methods. In\norder to gain insights and to arrive at a practical guideline, many important\noptions are studied, including network architectures, model fusion, learning\nparameters and the final prediction methods. Based on the evaluations, very\ncompetitive results are attained on two popular video classification\nbenchmarks. We hope that the discussions and conclusions from this work can\nhelp researchers in related fields to quickly set up a good basis for further\ninvestigations along this very promising direction.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 11:29:21 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Ye", "Hao", ""], ["Wu", "Zuxuan", ""], ["Zhao", "Rui-Wei", ""], ["Wang", "Xi", ""], ["Jiang", "Yu-Gang", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1504.01942", "submitter": "Laura Leal Taix\\'e", "authors": "Laura Leal-Taix\\'e and Anton Milan and Ian Reid and Stefan Roth and\n  Konrad Schindler", "title": "MOTChallenge 2015: Towards a Benchmark for Multi-Target Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent past, the computer vision community has developed centralized\nbenchmarks for the performance evaluation of a variety of tasks, including\ngeneric object and pedestrian detection, 3D reconstruction, optical flow,\nsingle-object short-term tracking, and stereo estimation. Despite potential\npitfalls of such benchmarks, they have proved to be extremely helpful to\nadvance the state of the art in the respective area. Interestingly, there has\nbeen rather limited work on the standardization of quantitative benchmarks for\nmultiple target tracking. One of the few exceptions is the well-known PETS\ndataset, targeted primarily at surveillance applications. Despite being widely\nused, it is often applied inconsistently, for example involving using different\nsubsets of the available data, different ways of training the models, or\ndiffering evaluation scripts. This paper describes our work toward a novel\nmultiple object tracking benchmark aimed to address such issues. We discuss the\nchallenges of creating such a framework, collecting existing and new data,\ngathering state-of-the-art methods to be tested on the datasets, and finally\ncreating a unified evaluation system. With MOTChallenge we aim to pave the way\ntoward a unified evaluation framework for a more meaningful quantification of\nmulti-target tracking.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 12:56:38 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Leal-Taix\u00e9", "Laura", ""], ["Milan", "Anton", ""], ["Reid", "Ian", ""], ["Roth", "Stefan", ""], ["Schindler", "Konrad", ""]]}, {"id": "1504.01954", "submitter": "Heider Ali Mr", "authors": "Heider K. Ali and Anthony Whitehead", "title": "Image Subset Selection Using Gabor Filters and Neural Networks", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An automatic method for the selection of subsets of images, both modern and\nhistoric, out of a set of landmark large images collected from the Internet is\npresented in this paper. This selection depends on the extraction of dominant\nfeatures using Gabor filtering. Features are selected carefully from a\npreliminary image set and fed into a neural network as a training data. The\nmethod collects a large set of raw landmark images containing modern and\nhistoric landmark images and non-landmark images. The method then processes\nthese images to classify them as landmark and non-landmark images. The\nclassification performance highly depends on the number of candidate features\nof the landmark.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 13:22:13 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Ali", "Heider K.", ""], ["Whitehead", "Anthony", ""]]}, {"id": "1504.01989", "submitter": "Tyng-Luh Liu", "authors": "Jyh-Jing Hwang and Tyng-Luh Liu", "title": "Pixel-wise Deep Learning for Contour Detection", "comments": "2 pages. arXiv admin note: substantial text overlap with\n  arXiv:1412.6857", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of contour detection via per-pixel classifications of\nedge point. To facilitate the process, the proposed approach leverages with\nDenseNet, an efficient implementation of multiscale convolutional neural\nnetworks (CNNs), to extract an informative feature vector for each pixel and\nuses an SVM classifier to accomplish contour detection. In the experiment of\ncontour detection, we look into the effectiveness of combining per-pixel\nfeatures from different CNN layers and verify their performance on BSDS500.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 14:44:20 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Hwang", "Jyh-Jing", ""], ["Liu", "Tyng-Luh", ""]]}, {"id": "1504.02164", "submitter": "Xiangru Li", "authors": "Xiangru Li, Yu Lu, Georges Comte, Ali Luo, Yongheng Zhao, Yongjun Wang", "title": "Linearly Supporting Feature Extraction For Automated Estimation Of\n  Stellar Atmospheric Parameters", "comments": "21 pages, 7 figures, 8 tables, The Astrophysical Journal Supplement\n  Series (accepted for publication)", "journal-ref": "ApJS, 2015, 218(1): 3", "doi": "10.1088/0067-0049/218/1/3", "report-no": null, "categories": "astro-ph.SR astro-ph.IM cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We describe a scheme to extract linearly supporting (LSU) features from\nstellar spectra to automatically estimate the atmospheric parameters $T_{eff}$,\nlog$~g$, and [Fe/H]. \"Linearly supporting\" means that the atmospheric\nparameters can be accurately estimated from the extracted features through a\nlinear model. The successive steps of the process are as follow: first,\ndecompose the spectrum using a wavelet packet (WP) and represent it by the\nderived decomposition coefficients; second, detect representative spectral\nfeatures from the decomposition coefficients using the proposed method Least\nAbsolute Shrinkage and Selection Operator (LARS)$_{bs}$; third, estimate the\natmospheric parameters $T_{eff}$, log$~g$, and [Fe/H] from the detected\nfeatures using a linear regression method. One prominent characteristic of this\nscheme is its ability to evaluate quantitatively the contribution of each\ndetected feature to the atmospheric parameter estimate and also to trace back\nthe physical significance of that feature. This work also shows that the\nusefulness of a component depends on both wavelength and frequency. The\nproposed scheme has been evaluated on both real spectra from the Sloan Digital\nSky Survey (SDSS)/SEGUE and synthetic spectra calculated from Kurucz's NEWODF\nmodels. On real spectra, we extracted 23 features to estimate $T_{eff}$, 62\nfeatures for log$~g$, and 68 features for [Fe/H]. Test consistencies between\nour estimates and those provided by the Spectroscopic Sarameter Pipeline of\nSDSS show that the mean absolute errors (MAEs) are 0.0062 dex for log$~T_{eff}$\n(83 K for $T_{eff}$), 0.2345 dex for log$~g$, and 0.1564 dex for [Fe/H]. For\nthe synthetic spectra, the MAE test accuracies are 0.0022 dex for log$~T_{eff}$\n(32 K for $T_{eff}$), 0.0337 dex for log$~g$, and 0.0268 dex for [Fe/H].\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 01:09:13 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2015 01:17:04 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Li", "Xiangru", ""], ["Lu", "Yu", ""], ["Comte", "Georges", ""], ["Luo", "Ali", ""], ["Zhao", "Yongheng", ""], ["Wang", "Yongjun", ""]]}, {"id": "1504.02174", "submitter": "P. Christopher Staecker", "authors": "Laurence Boxer, P. Christopher Staecker", "title": "Connectivity Preserving Multivalued Functions in Digital Topology", "comments": "small changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study connectivity preserving multivalued functions between digital\nimages. This notion generalizes that of continuous multivalued functions\nstudied mostly in the setting of the digital plane $Z^2$. We show that\nconnectivity preserving multivalued functions, like continuous multivalued\nfunctions, are appropriate models for digital morpholological operations.\nConnectivity preservation, unlike continuity, is preserved by compositions, and\ngeneralizes easily to higher dimensions and arbitrary adjacency relations.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 02:05:02 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2015 15:48:27 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Boxer", "Laurence", ""], ["Staecker", "P. Christopher", ""]]}, {"id": "1504.02206", "submitter": "Ming Yan", "authors": "Fang Li and Stanley Osher and Jing Qin and Ming Yan", "title": "A Multiphase Image Segmentation Based on Fuzzy Membership Functions and\n  L1-norm Fidelity", "comments": "28 pages, 8 figures, 3 tables", "journal-ref": "Journal of Scientific Computing, 69 (2016), 82-106", "doi": "10.1007/s10915-016-0183-z", "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a variational multiphase image segmentation model\nbased on fuzzy membership functions and L1-norm fidelity. Then we apply the\nalternating direction method of multipliers to solve an equivalent problem. All\nthe subproblems can be solved efficiently. Specifically, we propose a fast\nmethod to calculate the fuzzy median. Experimental results and comparisons show\nthat the L1-norm based method is more robust to outliers such as impulse noise\nand keeps better contrast than its L2-norm counterpart. Theoretically, we prove\nthe existence of the minimizer and analyze the convergence of the algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 07:21:02 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2016 21:06:37 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Li", "Fang", ""], ["Osher", "Stanley", ""], ["Qin", "Jing", ""], ["Yan", "Ming", ""]]}, {"id": "1504.02235", "submitter": "Gowri Rajasekaran", "authors": "R. Gowri, R. Rathipriya", "title": "Extraction of Protein Sequence Motif Information using PSO K-Means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The main objective of the paper is to find the motif information.The\nfunctionalities of the proteins are ideally found from their motif information\nwhich is extracted using various techniques like clustering with k-means,\nhybrid k-means, self-organising maps, etc., in the literature. In this work\nprotein sequence information is extracted using optimised k-means algorithm.\nThe particle swarm optimisation technique is one of the frequently used\noptimisation method. In the current work the PSO k-means is used for motif\ninformation extraction. This paper also deals with the comparison between the\nmotif information obtained from clusters and biclustersusing PSO k-means\nalgorithm. The motif information acquired is based on the structure homogeneity\nof the protein sequence.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 09:53:44 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Gowri", "R.", ""], ["Rathipriya", "R.", ""]]}, {"id": "1504.02340", "submitter": "Wongun Choi", "authors": "Wongun Choi", "title": "Near-Online Multi-target Tracking with Aggregated Local Flow Descriptor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the two key aspects of multiple target tracking\nproblem: 1) designing an accurate affinity measure to associate detections and\n2) implementing an efficient and accurate (near) online multiple target\ntracking algorithm. As the first contribution, we introduce a novel Aggregated\nLocal Flow Descriptor (ALFD) that encodes the relative motion pattern between a\npair of temporally distant detections using long term interest point\ntrajectories (IPTs). Leveraging on the IPTs, the ALFD provides a robust\naffinity measure for estimating the likelihood of matching detections\nregardless of the application scenarios. As another contribution, we present a\nNear-Online Multi-target Tracking (NOMT) algorithm. The tracking problem is\nformulated as a data-association between targets and detections in a temporal\nwindow, that is performed repeatedly at every frame. While being efficient,\nNOMT achieves robustness via integrating multiple cues including ALFD metric,\ntarget dynamics, appearance similarity, and long term trajectory regularization\ninto the model. Our ablative analysis verifies the superiority of the ALFD\nmetric over the other conventional affinity metrics. We run a comprehensive\nexperimental evaluation on two challenging tracking datasets, KITTI and MOT\ndatasets. The NOMT method combined with ALFD metric achieves the best accuracy\nin both datasets with significant margins (about 10% higher MOTA) over the\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 14:57:32 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Choi", "Wongun", ""]]}, {"id": "1504.02351", "submitter": "Yongxin Yang", "authors": "Guosheng Hu, Yongxin Yang, Dong Yi, Josef Kittler, William Christmas,\n  Stan Z. Li and Timothy Hospedales", "title": "When Face Recognition Meets with Deep Learning: an Evaluation of\n  Convolutional Neural Networks for Face Recognition", "comments": "7 pages, 4 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning, in particular Convolutional Neural Network (CNN), has achieved\npromising results in face recognition recently. However, it remains an open\nquestion: why CNNs work well and how to design a 'good' architecture. The\nexisting works tend to focus on reporting CNN architectures that work well for\nface recognition rather than investigate the reason. In this work, we conduct\nan extensive evaluation of CNN-based face recognition systems (CNN-FRS) on a\ncommon ground to make our work easily reproducible. Specifically, we use public\ndatabase LFW (Labeled Faces in the Wild) to train CNNs, unlike most existing\nCNNs trained on private databases. We propose three CNN architectures which are\nthe first reported architectures trained using LFW data. This paper\nquantitatively compares the architectures of CNNs and evaluate the effect of\ndifferent implementation choices. We identify several useful properties of\nCNN-FRS. For instance, the dimensionality of the learned features can be\nsignificantly reduced without adverse effect on face recognition accuracy. In\naddition, traditional metric learning method exploiting CNN-learned features is\nevaluated. Experiments show two crucial factors to good CNN-FRS performance are\nthe fusion of multiple CNNs and metric learning. To make our work reproducible,\nsource code and models will be made publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 15:27:49 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Hu", "Guosheng", ""], ["Yang", "Yongxin", ""], ["Yi", "Dong", ""], ["Kittler", "Josef", ""], ["Christmas", "William", ""], ["Li", "Stan Z.", ""], ["Hospedales", "Timothy", ""]]}, {"id": "1504.02356", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Eva Mohedano, Amaia Salvador, Sergi Porta, Xavier Gir\\'o-i-Nieto,\n  Graham Healy, Kevin McGuinness, Noel O'Connor and Alan F. Smeaton", "title": "Exploring EEG for Object Detection and Retrieval", "comments": "This preprint is the full version of a short paper accepted in the\n  ACM International Conference on Multimedia Retrieval (ICMR) 2015 (Shanghai,\n  China)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the potential for using Brain Computer Interfaces (BCI)\nas a relevance feedback mechanism in content-based image retrieval. We\ninvestigate if it is possible to capture useful EEG signals to detect if\nrelevant objects are present in a dataset of realistic and complex images. We\nperform several experiments using a rapid serial visual presentation (RSVP) of\nimages at different rates (5Hz and 10Hz) on 8 users with different degrees of\nfamiliarization with BCI and the dataset. We then use the feedback from the BCI\nand mouse-based interfaces to retrieve localized objects in a subset of TRECVid\nimages. We show that it is indeed possible to detect such objects in complex\nimages and, also, that users with previous knowledge on the dataset or\nexperience with the RSVP outperform others. When the users have limited time to\nannotate the images (100 seconds in our experiments) both interfaces are\ncomparable in performance. Comparing our best users in a retrieval task, we\nfound that EEG-based relevance feedback outperforms mouse-based feedback. The\nrealistic and complex image dataset differentiates our work from previous\nstudies on EEG for image retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 15:43:52 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Mohedano", "Eva", ""], ["Salvador", "Amaia", ""], ["Porta", "Sergi", ""], ["Gir\u00f3-i-Nieto", "Xavier", ""], ["Healy", "Graham", ""], ["McGuinness", "Kevin", ""], ["O'Connor", "Noel", ""], ["Smeaton", "Alan F.", ""]]}, {"id": "1504.02398", "submitter": "Dorian Galvez-Lopez", "authors": "Dorian G\\'alvez-L\\'opez, Marta Salas, Juan D. Tard\\'os, J. M. M.\n  Montiel", "title": "Real-time Monocular Object SLAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a real-time object-based SLAM system that leverages the largest\nobject database to date. Our approach comprises two main components: 1) a\nmonocular SLAM algorithm that exploits object rigidity constraints to improve\nthe map and find its real scale, and 2) a novel object recognition algorithm\nbased on bags of binary words, which provides live detections with a database\nof 500 3D objects. The two components work together and benefit each other: the\nSLAM algorithm accumulates information from the observations of the objects,\nanchors object features to especial map landmarks and sets constrains on the\noptimization. At the same time, objects partially or fully located within the\nmap are used as a prior to guide the recognition algorithm, achieving higher\nrecall. We evaluate our proposal on five real environments showing improvements\non the accuracy of the map and efficiency with respect to other\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 17:46:19 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["G\u00e1lvez-L\u00f3pez", "Dorian", ""], ["Salas", "Marta", ""], ["Tard\u00f3s", "Juan D.", ""], ["Montiel", "J. M. M.", ""]]}, {"id": "1504.02437", "submitter": "Ruiqi Guo", "authors": "Ruiqi Guo, Chuhang Zou and Derek Hoiem", "title": "Predicting Complete 3D Models of Indoor Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One major goal of vision is to infer physical models of objects, surfaces,\nand their layout from sensors. In this paper, we aim to interpret indoor scenes\nfrom one RGBD image. Our representation encodes the layout of walls, which must\nconform to a Manhattan structure but is otherwise flexible, and the layout and\nextent of objects, modeled with CAD-like 3D shapes. We represent both the\nvisible and occluded portions of the scene, producing a complete 3D parse. Such\na scene interpretation is useful for robotics and visual reasoning, but\ndifficult to produce due to the well-known challenge of segmentation, the high\ndegree of occlusion, and the diversity of objects in indoor scene. We take a\ndata-driven approach, generating sets of potential object regions, matching to\nregions in training images, and transferring and aligning associated 3D models\nwhile encouraging fit to observations and overall consistency. We demonstrate\nencouraging results on the NYU v2 dataset and highlight a variety of\ninteresting directions for future work.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 19:25:33 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 05:46:56 GMT"}, {"version": "v3", "created": "Fri, 18 Aug 2017 01:55:57 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Guo", "Ruiqi", ""], ["Zou", "Chuhang", ""], ["Hoiem", "Derek", ""]]}, {"id": "1504.02485", "submitter": "Xingchao Peng", "authors": "Xingchao Peng, Baochen Sun, Karim Ali, and Kate Saenko", "title": "What Do Deep CNNs Learn About Objects?", "comments": "2 pages workshop paper. arXiv admin note: substantial text overlap\n  with arXiv:1412.7122", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks learn extremely powerful image\nrepresentations, yet most of that power is hidden in the millions of deep-layer\nparameters. What exactly do these parameters represent? Recent work has started\nto analyse CNN representations, finding that, e.g., they are invariant to some\n2D transformations Fischer et al. (2014), but are confused by particular types\nof image noise Nguyen et al. (2014). In this work, we delve deeper and ask: how\ninvariant are CNNs to object-class variations caused by 3D shape, pose, and\nphotorealism?\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 20:02:03 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Peng", "Xingchao", ""], ["Sun", "Baochen", ""], ["Ali", "Karim", ""], ["Saenko", "Kate", ""]]}, {"id": "1504.02518", "submitter": "Rostislav Goroshin", "authors": "Ross Goroshin, Joan Bruna, Jonathan Tompson, David Eigen, Yann LeCun", "title": "Unsupervised Feature Learning from Temporal Data", "comments": "arXiv admin note: substantial text overlap with arXiv:1412.6056", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art classification and detection algorithms rely on\nsupervised training. In this work we study unsupervised feature learning in the\ncontext of temporally coherent video data. We focus on feature learning from\nunlabeled video data, using the assumption that adjacent video frames contain\nsemantically similar information. This assumption is exploited to train a\nconvolutional pooling auto-encoder regularized by slowness and sparsity. We\nestablish a connection between slow feature learning to metric learning and\nshow that the trained encoder can be used to define a more temporally and\nsemantically coherent metric.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 23:26:26 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2015 23:08:30 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Goroshin", "Ross", ""], ["Bruna", "Joan", ""], ["Tompson", "Jonathan", ""], ["Eigen", "David", ""], ["LeCun", "Yann", ""]]}, {"id": "1504.02531", "submitter": "Zhimin Gao", "authors": "Zhimin Gao, Lei Wang, Luping Zhou, Jianjia Zhang", "title": "HEp-2 Cell Image Classification with Deep Convolutional Neural Networks", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient Human Epithelial-2 (HEp-2) cell image classification can facilitate\nthe diagnosis of many autoimmune diseases. This paper presents an automatic\nframework for this classification task, by utilizing the deep convolutional\nneural networks (CNNs) which have recently attracted intensive attention in\nvisual recognition. This paper elaborates the important components of this\nframework, discusses multiple key factors that impact the efficiency of\ntraining a deep CNN, and systematically compares this framework with the\nwell-established image classification models in the literature. Experiments on\nbenchmark datasets show that i) the proposed framework can effectively\noutperform existing models by properly applying data augmentation; ii) our\nCNN-based framework demonstrates excellent adaptability across different\ndatasets, which is highly desirable for classification under varying laboratory\nsettings. Our system is ranked high in the cell image classification\ncompetition hosted by ICPR 2014.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 01:58:17 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 01:12:12 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Gao", "Zhimin", ""], ["Wang", "Lei", ""], ["Zhou", "Luping", ""], ["Zhang", "Jianjia", ""]]}, {"id": "1504.02648", "submitter": "Tony Lindeberg", "authors": "Tony Lindeberg", "title": "Time-causal and time-recursive spatio-temporal receptive fields", "comments": "39 pages, 12 figures, 5 tables in Journal of Mathematical Imaging and\n  Vision, published online Dec 2015", "journal-ref": "Journal of Mathematical Imaging and Vision, 55(1): 50-88, 2016", "doi": "10.1007/s10851-015-0613-9", "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an improved model and theory for time-causal and time-recursive\nspatio-temporal receptive fields, based on a combination of Gaussian receptive\nfields over the spatial domain and first-order integrators or equivalently\ntruncated exponential filters coupled in cascade over the temporal domain.\n  Compared to previous spatio-temporal scale-space formulations in terms of\nnon-enhancement of local extrema or scale invariance, these receptive fields\nare based on different scale-space axiomatics over time by ensuring\nnon-creation of new local extrema or zero-crossings with increasing temporal\nscale. Specifically, extensions are presented about (i) parameterizing the\nintermediate temporal scale levels, (ii) analysing the resulting temporal\ndynamics, (iii) transferring the theory to a discrete implementation, (iv)\ncomputing scale-normalized spatio-temporal derivative expressions for\nspatio-temporal feature detection and (v) computational modelling of receptive\nfields in the lateral geniculate nucleus (LGN) and the primary visual cortex\n(V1) in biological vision.\n  We show that by distributing the intermediate temporal scale levels according\nto a logarithmic distribution, we obtain much faster temporal response\nproperties (shorter temporal delays) compared to a uniform distribution.\nSpecifically, these kernels converge very rapidly to a limit kernel possessing\ntrue self-similar scale-invariant properties over temporal scales, thereby\nallowing for true scale invariance over variations in the temporal scale,\nalthough the underlying temporal scale-space representation is based on a\ndiscretized temporal scale parameter.\n  We show how scale-normalized temporal derivatives can be defined for these\ntime-causal scale-space kernels and how the composed theory can be used for\ncomputing basic types of scale-normalized spatio-temporal derivative\nexpressions in a computationally efficient manner.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 12:06:27 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2015 15:50:45 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Lindeberg", "Tony", ""]]}, {"id": "1504.02756", "submitter": "Lorenzo Livi", "authors": "Lorenzo Livi, Alireza Sadeghian, Hamid Sadeghian", "title": "Discrimination and characterization of Parkinsonian rest tremors by\n  analyzing long-term correlations and multifractal signatures", "comments": "10 pages, 41 references", "journal-ref": null, "doi": "10.1109/TBME.2016.2515760", "report-no": null, "categories": "physics.med-ph cs.CV physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze 48 signals of rest tremor velocity related to 12\ndistinct subjects affected by Parkinson's disease. The subjects belong to two\ndifferent groups, formed by four and eight subjects with, respectively, high-\nand low-amplitude rest tremors. Each subject is tested in four settings, given\nby combining the use of deep brain stimulation and L-DOPA medication. We\ndevelop two main feature-based representations of such signals, which are\nobtained by considering (i) the long-term correlations and multifractal\nproperties, and (ii) the power spectra. The feature-based representations are\ninitially utilized for the purpose of characterizing the subjects under\ndifferent settings. In agreement with previous studies, we show that deep brain\nstimulation does not significantly characterize neither of the two groups,\nregardless of the adopted representation. On the other hand, the medication\neffect yields statistically significant differences in both high- and\nlow-amplitude tremor groups. We successively test several different instances\nof the two feature-based representations of the signals in the setting of\nsupervised classification and (nonlinear) feature transformation. We consider\nthree different classification problems, involving the recognition of (i) the\npresence of medication, (ii) the use of deep brain stimulation, and (iii) the\nmembership to the high- and low-amplitude tremor groups. Classification results\nshow that the use of medication can be discriminated with higher accuracy,\nconsidering many of the feature-based representations. Notably, we show that\nthe best results are obtained with a parsimonious, two-dimensional\nrepresentation encoding the long-term correlations and multifractal character\nof the signals.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 18:34:27 GMT"}, {"version": "v2", "created": "Fri, 15 May 2015 17:20:26 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Livi", "Lorenzo", ""], ["Sadeghian", "Alireza", ""], ["Sadeghian", "Hamid", ""]]}, {"id": "1504.02762", "submitter": "Kevin Moon", "authors": "Kevin R. Moon, Veronique Delouille, Jimmy J. Li, Ruben De Visscher,\n  Fraser Watson, Alfred O. Hero III", "title": "Image patch analysis of sunspots and active regions. II. Clustering via\n  matrix factorization", "comments": "Accepted for publication in the Journal of Space Weather and Space\n  Climate (SWSC). 33 pages, 12 figures", "journal-ref": "Journal of Space Weather and Space Climate, Vol. 6, A3 (2016)", "doi": "10.1051/swsc/2015043", "report-no": null, "categories": "astro-ph.SR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Separating active regions that are quiet from potentially eruptive ones is a\nkey issue in Space Weather applications. Traditional classification schemes\nsuch as Mount Wilson and McIntosh have been effective in relating an active\nregion large scale magnetic configuration to its ability to produce eruptive\nevents. However, their qualitative nature prevents systematic studies of an\nactive region's evolution for example. We introduce a new clustering of active\nregions that is based on the local geometry observed in Line of Sight\nmagnetogram and continuum images. We use a reduced-dimension representation of\nan active region that is obtained by factoring the corresponding data matrix\ncomprised of local image patches. Two factorizations can be compared via the\ndefinition of appropriate metrics on the resulting factors. The distances\nobtained from these metrics are then used to cluster the active regions. We\nfind that these metrics result in natural clusterings of active regions. The\nclusterings are related to large scale descriptors of an active region such as\nits size, its local magnetic field distribution, and its complexity as measured\nby the Mount Wilson classification scheme. We also find that including data\nfocused on the neutral line of an active region can result in an increased\ncorrespondence between our clustering results and other active region\ndescriptors such as the Mount Wilson classifications and the $R$ value. We\nprovide some recommendations for which metrics, matrix factorization\ntechniques, and regions of interest to use to study active regions.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 19:03:26 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2015 16:16:23 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Moon", "Kevin R.", ""], ["Delouille", "Veronique", ""], ["Li", "Jimmy J.", ""], ["De Visscher", "Ruben", ""], ["Watson", "Fraser", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1504.02763", "submitter": "Filipe Condessa", "authors": "Filipe Condessa, Jelena Kovacevic, Jose Bioucas-Dias", "title": "Performance measures for classification systems with rejection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers with rejection are essential in real-world applications where\nmisclassifications and their effects are critical. However, if no problem\nspecific cost function is defined, there are no established measures to assess\nthe performance of such classifiers. We introduce a set of desired properties\nfor performance measures for classifiers with rejection, based on which we\npropose a set of three performance measures for the evaluation of the\nperformance of classifiers with rejection that satisfy the desired properties.\nThe nonrejected accuracy measures the ability of the classifier to accurately\nclassify nonrejected samples; the classification quality measures the correct\ndecision making of the classifier with rejector; and the rejection quality\nmeasures the ability to concentrate all misclassified samples onto the set of\nrejected samples. From the measures, we derive the concept of relative\noptimality that allows us to connect the measures to a family of cost functions\nthat take into account the trade-off between rejection and misclassification.\nWe illustrate the use of the proposed performance measures on classifiers with\nrejection applied to synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 19:15:39 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2016 11:29:13 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Condessa", "Filipe", ""], ["Kovacevic", "Jelena", ""], ["Bioucas-Dias", "Jose", ""]]}, {"id": "1504.02764", "submitter": "Roozbeh Mottaghi", "authors": "Roozbeh Mottaghi, Yu Xiang, Silvio Savarese", "title": "A Coarse-to-Fine Model for 3D Pose Estimation and Sub-category\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the fact that object detection, 3D pose estimation, and sub-category\nrecognition are highly correlated tasks, they are usually addressed\nindependently from each other because of the huge space of parameters. To\njointly model all of these tasks, we propose a coarse-to-fine hierarchical\nrepresentation, where each level of the hierarchy represents objects at a\ndifferent level of granularity. The hierarchical representation prevents\nperformance loss, which is often caused by the increase in the number of\nparameters (as we consider more tasks to model), and the joint modelling\nenables resolving ambiguities that exist in independent modelling of these\ntasks. We augment PASCAL3D+ dataset with annotations for these tasks and show\nthat our hierarchical model is effective in joint modelling of object\ndetection, 3D pose estimation, and sub-category recognition.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 19:18:59 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Mottaghi", "Roozbeh", ""], ["Xiang", "Yu", ""], ["Savarese", "Silvio", ""]]}, {"id": "1504.02789", "submitter": "Ashesh Jain", "authors": "Ashesh Jain, Hema S. Koppula, Bharad Raghavan, Shane Soh, Ashutosh\n  Saxena", "title": "Car that Knows Before You Do: Anticipating Maneuvers via Learning\n  Temporal Driving Models", "comments": "ICCV 2015, http://brain4cars.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced Driver Assistance Systems (ADAS) have made driving safer over the\nlast decade. They prepare vehicles for unsafe road conditions and alert drivers\nif they perform a dangerous maneuver. However, many accidents are unavoidable\nbecause by the time drivers are alerted, it is already too late. Anticipating\nmaneuvers beforehand can alert drivers before they perform the maneuver and\nalso give ADAS more time to avoid or prepare for the danger.\n  In this work we anticipate driving maneuvers a few seconds before they occur.\nFor this purpose we equip a car with cameras and a computing device to capture\nthe driving context from both inside and outside of the car. We propose an\nAutoregressive Input-Output HMM to model the contextual information alongwith\nthe maneuvers. We evaluate our approach on a diverse data set with 1180 miles\nof natural freeway and city driving and show that we can anticipate maneuvers\n3.5 seconds before they occur with over 80\\% F1-score in real-time.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 20:52:40 GMT"}, {"version": "v2", "created": "Sat, 19 Sep 2015 05:30:47 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Jain", "Ashesh", ""], ["Koppula", "Hema S.", ""], ["Raghavan", "Bharad", ""], ["Soh", "Shane", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1504.02840", "submitter": "Ahmad Pahlavan Tafti", "authors": "Ahmad Pahlavan Tafti, Hamid Hassannia, Zeyun Yu", "title": "siftservice.com - Turning a Computer Vision algorithm into a World Wide\n  Web Service", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image features detection and description is a longstanding topic in computer\nvision and pattern recognition areas. The Scale Invariant Feature Transform\n(SIFT) is probably the most popular and widely demanded feature descriptor\nwhich facilitates a variety of computer vision applications such as image\nregistration, object tracking, image forgery detection, and 3D surface\nreconstruction. This work introduces a Software as a Service (SaaS) based\nimplementation of the SIFT algorithm which is freely available at\nhttp://siftservice.com for any academic, educational and research purposes. The\nservice provides application-to-application interaction and aims Rapid\nApplication Development (RAD) and also fast prototyping for computer vision\nstudents and researchers all around the world. An Internet connection is all\nthey need!\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2015 05:47:09 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Tafti", "Ahmad Pahlavan", ""], ["Hassannia", "Hamid", ""], ["Yu", "Zeyun", ""]]}, {"id": "1504.02856", "submitter": "Arabinda Dash", "authors": "Arabinda Dash and Sujaya Kumar Sathua", "title": "High Density Noise Removal by Cascading Algorithms", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": "10.1109/ACCT.2015.100", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An advanced non-linear cascading filter algorithm for the removal of high\ndensity salt and pepper noise from the digital images is proposed. The proposed\nmethod consists of two stages. The first stage Decision base Median Filter\n(DMF) acts as the preliminary noise removal algorithm. The second stage is\neither Modified Decision Base Partial Trimmed Global Mean Filter (MDBPTGMF) or\nModified Decision Based Unsymmetric Trimmed Median Filter (MDBUTMF) which is\nused to remove the remaining noise and enhance the image quality. The DMF\nalgorithm performs well at low noise density but it fails to remove the noise\nat medium and high level. The MDBPTGMF and MDUTMF have excellent performance at\nlow, medium and high noise density but these reduce the image quality and blur\nthe image at high noise level. So the basic idea behind this paper is to\ncombine the advantages of the filters used in both the stages to remove the\nSalt and Pepper noise and enhance the image quality at all the noise density\nlevel. The proposed method is tested against different gray scale images and it\ngives better Mean Absolute Error (MAE), Peak Signal to Noise Ratio (PSNR) and\nImage Enhancement Factor (IEF) than the Adaptive Median Filter (AMF), Decision\nBase Unsymmetric Trimmed Median Filter (DBUTMF), Modified Decision Base\nUnsymmetric Trimmed Median Filter (MDBUTMF) and Decision Base Partial Trimmed\nGlobal Mean Filter (DBPTGMF).\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2015 10:21:56 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Dash", "Arabinda", ""], ["Sathua", "Sujaya Kumar", ""]]}, {"id": "1504.02863", "submitter": "Andreas Bulling", "authors": "Xucong Zhang, Yusuke Sugano, Mario Fritz, Andreas Bulling", "title": "Appearance-Based Gaze Estimation in the Wild", "comments": null, "journal-ref": null, "doi": "10.1109/CVPR.2015.7299081", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Appearance-based gaze estimation is believed to work well in real-world\nsettings, but existing datasets have been collected under controlled laboratory\nconditions and methods have been not evaluated across multiple datasets. In\nthis work we study appearance-based gaze estimation in the wild. We present the\nMPIIGaze dataset that contains 213,659 images we collected from 15 participants\nduring natural everyday laptop use over more than three months. Our dataset is\nsignificantly more variable than existing ones with respect to appearance and\nillumination. We also present a method for in-the-wild appearance-based gaze\nestimation using multimodal convolutional neural networks that significantly\noutperforms state-of-the art methods in the most challenging cross-dataset\nevaluation. We present an extensive evaluation of several state-of-the-art\nimage-based gaze estimation algorithms on three current datasets, including our\nown. This evaluation provides clear insights and allows us to identify key\nresearch challenges of gaze estimation in the wild.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2015 11:52:33 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Zhang", "Xucong", ""], ["Sugano", "Yusuke", ""], ["Fritz", "Mario", ""], ["Bulling", "Andreas", ""]]}, {"id": "1504.03083", "submitter": "Xiaodong He", "authors": "Xiaodong He, Rupesh Srivastava, Jianfeng Gao, Li Deng", "title": "Joint Learning of Distributed Representations for Images and Texts", "comments": "This is a previous tech report of a part of the work of\n  arXiv:1411.4952. In order to avoid confusion, we'd like to withdraw this\n  report from arXiv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report provides extra details of the deep multimodal\nsimilarity model (DMSM) which was proposed in (Fang et al. 2015,\narXiv:1411.4952). The model is trained via maximizing global semantic\nsimilarity between images and their captions in natural language using the\npublic Microsoft COCO database, which consists of a large set of images and\ntheir corresponding captions. The learned representations attempt to capture\nthe combination of various visual concepts and cues.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 07:36:08 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2015 17:24:00 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["He", "Xiaodong", ""], ["Srivastava", "Rupesh", ""], ["Gao", "Jianfeng", ""], ["Deng", "Li", ""]]}, {"id": "1504.03106", "submitter": "Carlo Ciliberto", "authors": "Carlo Ciliberto, Lorenzo Rosasco and Silvia Villa", "title": "Learning Multiple Visual Tasks while Discovering their Structure", "comments": "19 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning is a natural approach for computer vision applications\nthat require the simultaneous solution of several distinct but related\nproblems, e.g. object detection, classification, tracking of multiple agents,\nor denoising, to name a few. The key idea is that exploring task relatedness\n(structure) can lead to improved performances.\n  In this paper, we propose and study a novel sparse, non-parametric approach\nexploiting the theory of Reproducing Kernel Hilbert Spaces for vector-valued\nfunctions. We develop a suitable regularization framework which can be\nformulated as a convex optimization problem, and is provably solvable using an\nalternating minimization approach. Empirical tests show that the proposed\nmethod compares favorably to state of the art techniques and further allows to\nrecover interpretable structures, a problem of interest in its own right.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 09:27:23 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Ciliberto", "Carlo", ""], ["Rosasco", "Lorenzo", ""], ["Villa", "Silvia", ""]]}, {"id": "1504.03154", "submitter": "Carlo Ciliberto", "authors": "Giulia Pasquale, Carlo Ciliberto, Francesca Odone, Lorenzo Rosasco and\n  Lorenzo Natale", "title": "Real-world Object Recognition with Off-the-shelf Deep Conv Nets: How\n  Many Objects can iCub Learn?", "comments": "18 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to visually recognize objects is a fundamental skill for robotics\nsystems. Indeed, a large variety of tasks involving manipulation, navigation or\ninteraction with other agents, deeply depends on the accurate understanding of\nthe visual scene. Yet, at the time being, robots are lacking good visual\nperceptual systems, which often become the main bottleneck preventing the use\nof autonomous agents for real-world applications.\n  Lately in computer vision, systems that learn suitable visual representations\nand based on multi-layer deep convolutional networks are showing remarkable\nperformance in tasks such as large-scale visual recognition and image\nretrieval. To this regard, it is natural to ask whether such remarkable\nperformance would generalize also to the robotic setting.\n  In this paper we investigate such possibility, while taking further steps in\ndeveloping a computational vision system to be embedded on a robotic platform,\nthe iCub humanoid robot. In particular, we release a new dataset ({\\sc\niCubWorld28}) that we use as a benchmark to address the question: {\\it how many\nobjects can iCub recognize?} Our study is developed in a learning framework\nwhich reflects the typical visual experience of a humanoid robot like the iCub.\nExperiments shed interesting insights on the strength and weaknesses of current\ncomputer vision approaches applied in real robotic settings.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 12:45:09 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2015 05:56:01 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Pasquale", "Giulia", ""], ["Ciliberto", "Carlo", ""], ["Odone", "Francesca", ""], ["Rosasco", "Lorenzo", ""], ["Natale", "Lorenzo", ""]]}, {"id": "1504.03285", "submitter": "Filip Radenovi\\'c", "authors": "Filip Radenovic, Herve Jegou, Ondrej Chum", "title": "Multiple Measurements and Joint Dimensionality Reduction for Large Scale\n  Image Search with Short Vectors - Extended Version", "comments": "Extended version of the ICMR 2015 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the construction of a short-vector (128D) image\nrepresentation for large-scale image and particular object retrieval. In\nparticular, the method of joint dimensionality reduction of multiple\nvocabularies is considered. We study a variety of vocabulary generation\ntechniques: different k-means initializations, different descriptor\ntransformations, different measurement regions for descriptor extraction. Our\nextensive evaluation shows that different combinations of vocabularies, each\npartitioning the descriptor space in a different yet complementary manner,\nresults in a significant performance improvement, which exceeds the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 18:17:12 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Radenovic", "Filip", ""], ["Jegou", "Herve", ""], ["Chum", "Ondrej", ""]]}, {"id": "1504.03293", "submitter": "Yuting Zhang", "authors": "Yuting Zhang, Kihyuk Sohn, Ruben Villegas, Gang Pan, Honglak Lee", "title": "Improving Object Detection with Deep Convolutional Networks via Bayesian\n  Optimization and Structured Prediction", "comments": "CVPR 2015", "journal-ref": null, "doi": "10.1109/CVPR.2015.7298621", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection systems based on the deep convolutional neural network (CNN)\nhave recently made ground- breaking advances on several object detection\nbenchmarks. While the features learned by these high-capacity neural networks\nare discriminative for categorization, inaccurate localization is still a major\nsource of error for detection. Building upon high-capacity CNN architectures,\nwe address the localization problem by 1) using a search algorithm based on\nBayesian optimization that sequentially proposes candidate regions for an\nobject bounding box, and 2) training the CNN with a structured loss that\nexplicitly penalizes the localization inaccuracy. In experiments, we\ndemonstrated that each of the proposed methods improves the detection\nperformance over the baseline method on PASCAL VOC 2007 and 2012 datasets.\nFurthermore, two methods are complementary and significantly outperform the\nprevious state-of-the-art when combined.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 18:50:51 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2016 18:27:32 GMT"}, {"version": "v3", "created": "Thu, 14 Jan 2016 04:11:45 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Zhang", "Yuting", ""], ["Sohn", "Kihyuk", ""], ["Villegas", "Ruben", ""], ["Pan", "Gang", ""], ["Lee", "Honglak", ""]]}, {"id": "1504.03315", "submitter": "Saurabh Agarwal", "authors": "Saurabh Agarwal, Punit Kumar Johari", "title": "A Novel Approach to Develop a New Hybrid Technique for Trademark Image\n  Retrieval", "comments": "12 Pages, International Journal on Information Theory (IJIT),Vol.3,\n  No.4, October 2014", "journal-ref": null, "doi": "10.5121/ijit.2014.3403", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trademark Image Retrieval is playing a vital role as a part of CBIR System.\nTrademark is of great significance because it carries the status value of any\ncompany. To retrieve such a fake or copied trademark we design a retrieval\nsystem which is based on hybrid techniques. It contains a mixture of two\ndifferent feature vector which combined together to give a suitable retrieval\nsystem. In the proposed system we extract the corner feature which is applied\non an edge pixel image. This feature is used to extract the relevant image and\nto more purify the result we apply other feature which is the invariant moment\nfeature. From the experimental result we conclude that the system is 85 percent\nefficient.\n", "versions": [{"version": "v1", "created": "Sat, 8 Nov 2014 02:36:18 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Agarwal", "Saurabh", ""], ["Johari", "Punit Kumar", ""]]}, {"id": "1504.03409", "submitter": "Hao  Wu", "authors": "Hao Wu, Yi Wan", "title": "Clustering Assisted Fundamental Matrix Estimation", "comments": "12 pages, 8 figures, 3 tables, Second International Conference on\n  Computer Science and Information Technology (COSIT 2015) March 21~22, 2015,\n  Geneva, Switzerland", "journal-ref": null, "doi": "10.5121/csit.2015.50604", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision, the estimation of the fundamental matrix is a basic\nproblem that has been extensively studied. The accuracy of the estimation\nimposes a significant influence on subsequent tasks such as the camera\ntrajectory determination and 3D reconstruction. In this paper we propose a new\nmethod for fundamental matrix estimation that makes use of clustering a group\nof 4D vectors. The key insight is the observation that among the 4D vectors\nconstructed from matching pairs of points obtained from the SIFT algorithm,\nwell-defined cluster points tend to be reliable inliers suitable for\nfundamental matrix estimation. Based on this, we utilizes a recently proposed\nefficient clustering method through density peaks seeking and propose a new\nclustering assisted method. Experimental results show that the proposed\nalgorithm is faster and more accurate than currently commonly used methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 03:13:40 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Wu", "Hao", ""], ["Wan", "Yi", ""]]}, {"id": "1504.03410", "submitter": "Hanjiang Lai", "authors": "Hanjiang Lai, Yan Pan, Ye Liu and Shuicheng Yan", "title": "Simultaneous Feature Learning and Hash Coding with Deep Neural Networks", "comments": "This paper has been accepted to IEEE International Conference on\n  Pattern Recognition and Computer Vision (CVPR), 2015", "journal-ref": null, "doi": "10.1109/CVPR.2015.7298947", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity-preserving hashing is a widely-used method for nearest neighbour\nsearch in large-scale image retrieval tasks. For most existing hashing methods,\nan image is first encoded as a vector of hand-engineering visual features,\nfollowed by another separate projection or quantization step that generates\nbinary codes. However, such visual feature vectors may not be optimally\ncompatible with the coding process, thus producing sub-optimal hashing codes.\nIn this paper, we propose a deep architecture for supervised hashing, in which\nimages are mapped into binary codes via carefully designed deep neural\nnetworks. The pipeline of the proposed deep architecture consists of three\nbuilding blocks: 1) a sub-network with a stack of convolution layers to produce\nthe effective intermediate image features; 2) a divide-and-encode module to\ndivide the intermediate image features into multiple branches, each encoded\ninto one hash bit; and 3) a triplet ranking loss designed to characterize that\none image is more similar to the second image than to the third one. Extensive\nevaluations on several benchmark image datasets show that the proposed\nsimultaneous feature learning and hash coding pipeline brings substantial\nimprovements over other state-of-the-art supervised or unsupervised hashing\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 03:14:24 GMT"}], "update_date": "2019-08-17", "authors_parsed": [["Lai", "Hanjiang", ""], ["Pan", "Yan", ""], ["Liu", "Ye", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1504.03439", "submitter": "Dai-Gyoung Kim", "authors": "Zahid Hussain Shamsi, Hyun Sook Oh and Dai-Gyoung Kim", "title": "Image Denoising Using Low Rank Minimization With Modified Noise\n  Estimation", "comments": "4 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the application of low rank minimization to image denoising has\nshown remarkable denoising results which are equivalent or better than those of\nthe existing state-of-the-art algorithms. However, due to iterative nature of\nlow rank optimization, estimation of residual noise is an essential requirement\nafter each iteration. Currently, this noise is estimated by using the filtered\nnoise in the previous iteration without considering the geometric structure of\nthe given image. This estimate may be affected in the presence of moderate and\nsevere levels of noise. To obtain a more reliable estimate of residual noise,\nwe propose a modified algorithm (GWNNM) which includes the contribution of the\ngeometric structure of an image to the existing noise estimation. Furthermore,\nthe proposed algorithm exploits the difference of large and small singular\nvalues to enhance the edges and textures during the denoising process.\nConsequently, the proposed modifications achieve significant improvements in\nthe denoising results of the existing low rank optimization algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 07:18:48 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2015 11:30:32 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Shamsi", "Zahid Hussain", ""], ["Oh", "Hyun Sook", ""], ["Kim", "Dai-Gyoung", ""]]}, {"id": "1504.03504", "submitter": "Fang Wang", "authors": "Fang Wang, Le Kang, Yi Li", "title": "Sketch-based 3D Shape Retrieval using Convolutional Neural Networks", "comments": "CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrieving 3D models from 2D human sketches has received considerable\nattention in the areas of graphics, image retrieval, and computer vision.\nAlmost always in state of the art approaches a large amount of \"best views\" are\ncomputed for 3D models, with the hope that the query sketch matches one of\nthese 2D projections of 3D models using predefined features.\n  We argue that this two stage approach (view selection -- matching) is\npragmatic but also problematic because the \"best views\" are subjective and\nambiguous, which makes the matching inputs obscure. This imprecise nature of\nmatching further makes it challenging to choose features manually. Instead of\nrelying on the elusive concept of \"best views\" and the hand-crafted features,\nwe propose to define our views using a minimalism approach and learn features\nfor both sketches and views. Specifically, we drastically reduce the number of\nviews to only two predefined directions for the whole dataset. Then, we learn\ntwo Siamese Convolutional Neural Networks (CNNs), one for the views and one for\nthe sketches. The loss function is defined on the within-domain as well as the\ncross-domain similarities. Our experiments on three benchmark datasets\ndemonstrate that our method is significantly better than state of the art\napproaches, and outperforms them in all conventional metrics.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 11:55:45 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Wang", "Fang", ""], ["Kang", "Le", ""], ["Li", "Yi", ""]]}, {"id": "1504.03522", "submitter": "Lukas Neumann", "authors": "Luk\\'a\\v{s} Neumann, Ji\\v{r}\\'i Matas", "title": "Efficient Scene Text Localization and Recognition with Local Character\n  Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An unconstrained end-to-end text localization and recognition method is\npresented. The method detects initial text hypothesis in a single pass by an\nefficient region-based method and subsequently refines the text hypothesis\nusing a more robust local text model, which deviates from the common assumption\nof region-based methods that all characters are detected as connected\ncomponents.\n  Additionally, a novel feature based on character stroke area estimation is\nintroduced. The feature is efficiently computed from a region distance map, it\nis invariant to scaling and rotations and allows to efficiently detect text\nregions regardless of what portion of text they capture.\n  The method runs in real time and achieves state-of-the-art text localization\nand recognition results on the ICDAR 2013 Robust Reading dataset.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 12:42:56 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Neumann", "Luk\u00e1\u0161", ""], ["Matas", "Ji\u0159\u00ed", ""]]}, {"id": "1504.03573", "submitter": "Marcus A. Brubaker", "authors": "Marcus A. Brubaker, Ali Punjani and David J. Fleet", "title": "Building Proteins in a Day: Efficient 3D Molecular Reconstruction", "comments": "To be presented at IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2015", "journal-ref": null, "doi": "10.1109/CVPR.2015.7298929", "report-no": null, "categories": "cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering the 3D atomic structure of molecules such as proteins and viruses\nis a fundamental research problem in biology and medicine. Electron\nCryomicroscopy (Cryo-EM) is a promising vision-based technique for structure\nestimation which attempts to reconstruct 3D structures from 2D images. This\npaper addresses the challenging problem of 3D reconstruction from 2D Cryo-EM\nimages. A new framework for estimation is introduced which relies on modern\nstochastic optimization techniques to scale to large datasets. We also\nintroduce a novel technique which reduces the cost of evaluating the objective\nfunction during optimization by over five orders or magnitude. The net result\nis an approach capable of estimating 3D molecular structure from large scale\ndatasets in about a day on a single workstation.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 14:56:17 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Brubaker", "Marcus A.", ""], ["Punjani", "Ali", ""], ["Fleet", "David J.", ""]]}, {"id": "1504.03641", "submitter": "Sergey Zagoruyko", "authors": "Sergey Zagoruyko and Nikos Komodakis", "title": "Learning to Compare Image Patches via Convolutional Neural Networks", "comments": "CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show how to learn directly from image data (i.e., without\nresorting to manually-designed features) a general similarity function for\ncomparing image patches, which is a task of fundamental importance for many\ncomputer vision problems. To encode such a function, we opt for a CNN-based\nmodel that is trained to account for a wide variety of changes in image\nappearance. To that end, we explore and study multiple neural network\narchitectures, which are specifically adapted to this task. We show that such\nan approach can significantly outperform the state-of-the-art on several\nproblems and benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 17:53:51 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Zagoruyko", "Sergey", ""], ["Komodakis", "Nikos", ""]]}, {"id": "1504.03707", "submitter": "Bo Xin", "authors": "Bo Xin, Yuan Tian, Yizhou Wang and Wen Gao", "title": "Background Subtraction via Generalized Fused Lasso Foreground Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background Subtraction (BS) is one of the key steps in video analysis. Many\nbackground models have been proposed and achieved promising performance on\npublic data sets. However, due to challenges such as illumination change,\ndynamic background etc. the resulted foreground segmentation often consists of\nholes as well as background noise. In this regard, we consider generalized\nfused lasso regularization to quest for intact structured foregrounds. Together\nwith certain assumptions about the background, such as the low-rank assumption\nor the sparse-composition assumption (depending on whether pure background\nframes are provided), we formulate BS as a matrix decomposition problem using\nregularization terms for both the foreground and background matrices. Moreover,\nunder the proposed formulation, the two generally distinctive background\nassumptions can be solved in a unified manner. The optimization was carried out\nvia applying the augmented Lagrange multiplier (ALM) method in such a way that\na fast parametric-flow algorithm is used for updating the foreground matrix.\nExperimental results on several popular BS data sets demonstrate the advantage\nof the proposed model compared to state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 20:25:27 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Xin", "Bo", ""], ["Tian", "Yuan", ""], ["Wang", "Yizhou", ""], ["Gao", "Wen", ""]]}, {"id": "1504.03810", "submitter": "Smitha M.L.", "authors": "B.H. Shekar, Smitha M.L.", "title": "Text Localization in Video Using Multiscale Weber's Local Descriptor", "comments": "IEEE SPICES, 2015", "journal-ref": null, "doi": "10.1109/SPICES.2015.7091559", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach for detecting the text present in\nvideos and scene images based on the Multiscale Weber's Local Descriptor\n(MWLD). Given an input video, the shots are identified and the key frames are\nextracted based on their spatio-temporal relationship. From each key frame, we\ndetect the local region information using WLD with different radius and\nneighborhood relationship of pixel values and hence obtained intensity enhanced\nkey frames at multiple scales. These multiscale WLD key frames are merged\ntogether and then the horizontal gradients are computed using morphological\noperations. The obtained results are then binarized and the false positives are\neliminated based on geometrical properties. Finally, we employ connected\ncomponent analysis and morphological dilation operation to determine the text\nregions that aids in text localization. The experimental results obtained on\npublicly available standard Hua, Horizontal-1 and Horizontal-2 video dataset\nillustrate that the proposed method can accurately detect and localize texts of\nvarious sizes, fonts and colors in videos.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 07:56:05 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Shekar", "B. H.", ""], ["L.", "Smitha M.", ""]]}, {"id": "1504.03811", "submitter": "Meng-Che Chuang", "authors": "Meng-Che Chuang, Jenq-Neng Hwang, Kresimir Williams, Richard Towler", "title": "Tracking Live Fish from Low-Contrast and Low-Frame-Rate Stereo Videos", "comments": "14 pages, 14 figures, 6 tables", "journal-ref": "IEEE Trans. on Circuits and Systems for Video Technology, vol. 25,\n  no. 1, pp.167-179, Jan. 2015", "doi": "10.1109/TCSVT.2014.2357093", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-extractive fish abundance estimation with the aid of visual analysis has\ndrawn increasing attention. Unstable illumination, ubiquitous noise and low\nframe rate video capturing in the underwater environment, however, make\nconventional tracking methods unreliable. In this paper, we present a multiple\nfish tracking system for low-contrast and low-frame-rate stereo videos with the\nuse of a trawl-based underwater camera system. An automatic fish segmentation\nalgorithm overcomes the low-contrast issues by adopting a histogram\nbackprojection approach on double local-thresholded images to ensure an\naccurate segmentation on the fish shape boundaries. Built upon a reliable\nfeature-based object matching method, a multiple-target tracking algorithm via\na modified Viterbi data association is proposed to overcome the poor motion\ncontinuity and frequent entrance/exit of fish targets under low-frame-rate\nscenarios. In addition, a computationally efficient block-matching approach\nperforms successful stereo matching, which enables an automatic fish-body tail\ncompensation to greatly reduce segmentation error and allows for an accurate\nfish length measurement. Experimental results show that an effective and\nreliable tracking performance for multiple live fish with underwater stereo\ncameras is achieved.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 08:05:14 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Chuang", "Meng-Che", ""], ["Hwang", "Jenq-Neng", ""], ["Williams", "Kresimir", ""], ["Towler", "Richard", ""]]}, {"id": "1504.03834", "submitter": "Pornchai Phukpattaranont Dr", "authors": "Pornchai Phukpattaranont", "title": "Comparisons of wavelet functions in QRS signal to noise ratio\n  enhancement and detection accuracy", "comments": "16 pages, 8 figures, Article submitted to Journal of the Korean\n  Physical Society for considering of publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare the capability of wavelet functions used for noise removal in\npreprocessing step of a QRS detection algorithm in the electrocardiogram (ECG)\nsignal. The QRS signal to noise ratio enhancement and the detection accuracy of\neach wavelet function are evaluated using three measures: (1) the ratio of the\nmaximum beat amplitude to the minimum beat amplitude (RMM), (2) the mean of\nabsolute of time error (MATE), and (3) the figure of merit (FOM). Three wavelet\nfunctions from previous well-known publications are explored, i.e., Bior1.3,\nDb10, and Mexican hat wavelet functions. Results evaluated with the ECG signal\nfrom MIT-BIH arrhythmia database show that the Mexican hat wavelet function is\nbetter than the others. While the scale 8 of Mexican hat wavelet function can\nprovide the best enhancement in QRS signal to noise ratio, the scale 4 of\nMexican hat wavelet function can provide the best detection accuracy. These\nresults may be combined and may enable the use of a single fixed threshold for\nall ECG records leading to the reduction in computational complexity of the QRS\ndetection algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 09:22:31 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2015 04:35:37 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Phukpattaranont", "Pornchai", ""]]}, {"id": "1504.03871", "submitter": "Saeed Reza Kheradpisheh", "authors": "Saeed Reza Kheradpisheh, Mohammad Ganjtabesh, Timoth\\'ee Masquelier", "title": "Bio-inspired Unsupervised Learning of Visual Features Leads to Robust\n  Invariant Object Recognition", "comments": null, "journal-ref": "Neurocomputing 205 (2016) 382-392", "doi": "10.1016/j.neucom.2016.04.029", "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Retinal image of surrounding objects varies tremendously due to the changes\nin position, size, pose, illumination condition, background context, occlusion,\nnoise, and nonrigid deformations. But despite these huge variations, our visual\nsystem is able to invariantly recognize any object in just a fraction of a\nsecond. To date, various computational models have been proposed to mimic the\nhierarchical processing of the ventral visual pathway, with limited success.\nHere, we show that the association of both biologically inspired network\narchitecture and learning rule significantly improves the models' performance\nwhen facing challenging invariant object recognition problems. Our model is an\nasynchronous feedforward spiking neural network. When the network is presented\nwith natural images, the neurons in the entry layers detect edges, and the most\nactivated ones fire first, while neurons in higher layers are equipped with\nspike timing-dependent plasticity. These neurons progressively become selective\nto intermediate complexity visual features appropriate for object\ncategorization. The model is evaluated on 3D-Object and ETH-80 datasets which\nare two benchmarks for invariant object recognition, and is shown to outperform\nstate-of-the-art models, including DeepConvNet and HMAX. This demonstrates its\nability to accurately recognize different instances of multiple object classes\neven under various appearance conditions (different views, scales, tilts, and\nbackgrounds). Several statistical analysis techniques are used to show that our\nmodel extracts class specific and highly informative features.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 11:47:21 GMT"}, {"version": "v2", "created": "Sun, 3 May 2015 12:40:59 GMT"}, {"version": "v3", "created": "Tue, 28 Jun 2016 10:54:22 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Kheradpisheh", "Saeed Reza", ""], ["Ganjtabesh", "Mohammad", ""], ["Masquelier", "Timoth\u00e9e", ""]]}, {"id": "1504.03967", "submitter": "Holger Roth", "authors": "Holger R. Roth, Amal Farag, Le Lu, Evrim B. Turkbey, and Ronald M.\n  Summers", "title": "Deep convolutional networks for pancreas segmentation in CT imaging", "comments": "SPIE Medical Imaging conference, Orlando, FL, USA: SPIE Proceedings |\n  Volume 9413 | Classification", "journal-ref": "Proc. SPIE 9413, Medical Imaging 2015: Image Processing, 94131G\n  (20 March 2015)", "doi": "10.1117/12.2081420", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Automatic organ segmentation is an important prerequisite for many\ncomputer-aided diagnosis systems. The high anatomical variability of organs in\nthe abdomen, such as the pancreas, prevents many segmentation methods from\nachieving high accuracies when compared to other segmentation of organs like\nthe liver, heart or kidneys. Recently, the availability of large annotated\ntraining sets and the accessibility of affordable parallel computing resources\nvia GPUs have made it feasible for \"deep learning\" methods such as\nconvolutional networks (ConvNets) to succeed in image classification tasks.\nThese methods have the advantage that used classification features are trained\ndirectly from the imaging data. We present a fully-automated bottom-up method\nfor pancreas segmentation in computed tomography (CT) images of the abdomen.\nThe method is based on hierarchical coarse-to-fine classification of local\nimage regions (superpixels). Superpixels are extracted from the abdominal\nregion using Simple Linear Iterative Clustering (SLIC). An initial probability\nresponse map is generated, using patch-level confidences and a two-level\ncascade of random forest classifiers, from which superpixel regions with\nprobabilities larger 0.5 are retained. These retained superpixels serve as a\nhighly sensitive initial input of the pancreas and its surroundings to a\nConvNet that samples a bounding box around each superpixel at different scales\n(and random non-rigid deformations at training time) in order to assign a more\ndistinct probability of each superpixel region being pancreas or not. We\nevaluate our method on CT images of 82 patients (60 for training, 2 for\nvalidation, and 20 for testing). Using ConvNets we achieve average Dice scores\nof 68%+-10% (range, 43-80%) in testing. This shows promise for accurate\npancreas segmentation, using a deep learning approach and compares favorably to\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 16:55:46 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Roth", "Holger R.", ""], ["Farag", "Amal", ""], ["Lu", "Le", ""], ["Turkbey", "Evrim B.", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1504.04003", "submitter": "Holger Roth", "authors": "Holger R. Roth, Christopher T. Lee, Hoo-Chang Shin, Ari Seff, Lauren\n  Kim, Jianhua Yao, Le Lu, Ronald M. Summers", "title": "Anatomy-specific classification of medical images using deep\n  convolutional nets", "comments": "Presented at: 2015 IEEE International Symposium on Biomedical\n  Imaging, April 16-19, 2015, New York Marriott at Brooklyn Bridge, NY, USA", "journal-ref": "Biomedical Imaging (ISBI), 2015 IEEE 12th International Symposium\n  on Year: 2015 Pages: 101 - 104", "doi": "10.1109/ISBI.2015.7163826", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Automated classification of human anatomy is an important prerequisite for\nmany computer-aided diagnosis systems. The spatial complexity and variability\nof anatomy throughout the human body makes classification difficult. \"Deep\nlearning\" methods such as convolutional networks (ConvNets) outperform other\nstate-of-the-art methods in image classification tasks. In this work, we\npresent a method for organ- or body-part-specific anatomical classification of\nmedical images acquired using computed tomography (CT) with ConvNets. We train\na ConvNet, using 4,298 separate axial 2D key-images to learn 5 anatomical\nclasses. Key-images were mined from a hospital PACS archive, using a set of\n1,675 patients. We show that a data augmentation approach can help to enrich\nthe data set and improve classification performance. Using ConvNets and data\naugmentation, we achieve anatomy-specific classification error of 5.9 % and\narea-under-the-curve (AUC) values of an average of 0.998 in testing. We\ndemonstrate that deep learning can be used to train very reliable and accurate\nclassifiers that could initialize further computer-aided diagnosis.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 19:55:27 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Roth", "Holger R.", ""], ["Lee", "Christopher T.", ""], ["Shin", "Hoo-Chang", ""], ["Seff", "Ari", ""], ["Kim", "Lauren", ""], ["Yao", "Jianhua", ""], ["Lu", "Le", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1504.04085", "submitter": "Aswin Sankaranarayanan", "authors": "Huaijin Chen, M. Salman Asif, Aswin C. Sankaranarayanan, and Ashok\n  Veeraraghavan", "title": "FPA-CS: Focal Plane Array-based Compressive Imaging in Short-wave\n  Infrared", "comments": "appears in IEEE Conf. Computer Vision and Pattern Recognition, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cameras for imaging in short and mid-wave infrared spectra are significantly\nmore expensive than their counterparts in visible imaging. As a result,\nhigh-resolution imaging in those spectrum remains beyond the reach of most\nconsumers. Over the last decade, compressive sensing (CS) has emerged as a\npotential means to realize inexpensive short-wave infrared cameras. One\napproach for doing this is the single-pixel camera (SPC) where a single\ndetector acquires coded measurements of a high-resolution image. A\ncomputational reconstruction algorithm is then used to recover the image from\nthese coded measurements. Unfortunately, the measurement rate of a SPC is\ninsufficient to enable imaging at high spatial and temporal resolutions.\n  We present a focal plane array-based compressive sensing (FPA-CS)\narchitecture that achieves high spatial and temporal resolutions. The idea is\nto use an array of SPCs that sense in parallel to increase the measurement\nrate, and consequently, the achievable spatio-temporal resolution of the\ncamera. We develop a proof-of-concept prototype in the short-wave infrared\nusing a sensor with 64$\\times$ 64 pixels; the prototype provides a 4096$\\times$\nincrease in the measurement rate compared to the SPC and achieves a megapixel\nresolution at video rate using CS techniques.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 02:29:20 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Chen", "Huaijin", ""], ["Asif", "M. Salman", ""], ["Sankaranarayanan", "Aswin C.", ""], ["Veeraraghavan", "Ashok", ""]]}, {"id": "1504.04090", "submitter": "Stephen Tierney", "authors": "Stephen Tierney, Yi Guo, Junbin Gao", "title": "Segmentation of Subspaces in Sequential Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Ordered Subspace Clustering (OSC) to segment data drawn from a\nsequentially ordered union of subspaces. Similar to Sparse Subspace Clustering\n(SSC) we formulate the problem as one of finding a sparse representation but\ninclude an additional penalty term to take care of sequential data. We test our\nmethod on data drawn from infrared hyper spectral, video and motion capture\ndata. Experiments show that our method, OSC, outperforms the state of the art\nmethods: Spatial Subspace Clustering (SpatSC), Low-Rank Representation (LRR)\nand SSC.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 03:18:28 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Tierney", "Stephen", ""], ["Guo", "Yi", ""], ["Gao", "Junbin", ""]]}, {"id": "1504.04343", "submitter": "Ce Zhang", "authors": "Stefan Hadjis, Firas Abuzaid, Ce Zhang, Christopher R\\'e", "title": "Caffe con Troll: Shallow Ideas to Speed Up Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Caffe con Troll (CcT), a fully compatible end-to-end version of\nthe popular framework Caffe with rebuilt internals. We built CcT to examine the\nperformance characteristics of training and deploying general-purpose\nconvolutional neural networks across different hardware architectures. We find\nthat, by employing standard batching optimizations for CPU training, we achieve\na 4.5x throughput improvement over Caffe on popular networks like CaffeNet.\nMoreover, with these improvements, the end-to-end training time for CNNs is\ndirectly proportional to the FLOPS delivered by the CPU, which enables us to\nefficiently train hybrid CPU-GPU systems for CNNs.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 19:11:08 GMT"}, {"version": "v2", "created": "Tue, 26 May 2015 20:12:33 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Hadjis", "Stefan", ""], ["Abuzaid", "Firas", ""], ["Zhang", "Ce", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1504.04531", "submitter": "Nicolas Dobigeon", "authors": "Laetitia Loncan, Luis B. Almeida, Jos\\'e M. Bioucas-Dias, Xavier\n  Briottet, Jocelyn Chanussot, Nicolas Dobigeon, Sophie Fabre, Wenzhi Liao,\n  Giorgio A. Licciardi, Miguel Sim\\~oes, Jean-Yves Tourneret, Miguel A.\n  Veganzones, Gemine Vivone, Qi Wei and Naoto Yokoya", "title": "Hyperspectral pansharpening: a review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pansharpening aims at fusing a panchromatic image with a multispectral one,\nto generate an image with the high spatial resolution of the former and the\nhigh spectral resolution of the latter. In the last decade, many algorithms\nhave been presented in the literature for pansharpening using multispectral\ndata. With the increasing availability of hyperspectral systems, these methods\nare now being adapted to hyperspectral images. In this work, we compare new\npansharpening techniques designed for hyperspectral data with some of the state\nof the art methods for multispectral pansharpening, which have been adapted for\nhyperspectral data. Eleven methods from different classes (component\nsubstitution, multiresolution analysis, hybrid, Bayesian and matrix\nfactorization) are analyzed. These methods are applied to three datasets and\ntheir effectiveness and robustness are evaluated with widely used performance\nindicators. In addition, all the pansharpening techniques considered in this\npaper have been implemented in a MATLAB toolbox that is made available to the\ncommunity.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2015 15:07:11 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Loncan", "Laetitia", ""], ["Almeida", "Luis B.", ""], ["Bioucas-Dias", "Jos\u00e9 M.", ""], ["Briottet", "Xavier", ""], ["Chanussot", "Jocelyn", ""], ["Dobigeon", "Nicolas", ""], ["Fabre", "Sophie", ""], ["Liao", "Wenzhi", ""], ["Licciardi", "Giorgio A.", ""], ["Sim\u00f5es", "Miguel", ""], ["Tourneret", "Jean-Yves", ""], ["Veganzones", "Miguel A.", ""], ["Vivone", "Gemine", ""], ["Wei", "Qi", ""], ["Yokoya", "Naoto", ""]]}, {"id": "1504.04548", "submitter": "Simone Bianco", "authors": "Simone Bianco, Claudio Cusano, Raimondo Schettini", "title": "Color Constancy Using CNNs", "comments": "Accepted at DeepVision: Deep Learning in Computer Vision 2015 (CVPR\n  2015 workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we describe a Convolutional Neural Network (CNN) to accurately\npredict the scene illumination. Taking image patches as input, the CNN works in\nthe spatial domain without using hand-crafted features that are employed by\nmost previous methods. The network consists of one convolutional layer with max\npooling, one fully connected layer and three output nodes. Within the network\nstructure, feature learning and regression are integrated into one optimization\nprocess, which leads to a more effective model for estimating scene\nillumination. This approach achieves state-of-the-art performance on a standard\ndataset of RAW images. Preliminary experiments on images with spatially varying\nillumination demonstrate the stability of the local illuminant estimation\nability of our CNN.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2015 15:51:07 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Bianco", "Simone", ""], ["Cusano", "Claudio", ""], ["Schettini", "Raimondo", ""]]}, {"id": "1504.04651", "submitter": "Sunpreet Arora", "authors": "Anil K. Jain, Sunpreet S. Arora, Lacey Best-Rowden, Kai Cao, Prem\n  Sewak Sudhish and Anjoo Bhatnagar", "title": "Biometrics for Child Vaccination and Welfare: Persistence of Fingerprint\n  Recognition for Infants and Toddlers", "comments": "Michigan State University Technical Report", "journal-ref": null, "doi": null, "report-no": "MSU-CSE-15-7", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a number of emerging applications requiring biometric recognition of\nchildren (e.g., tracking child vaccination schedules, identifying missing\nchildren and preventing newborn baby swaps in hospitals), investigating the\ntemporal stability of biometric recognition accuracy for children is important.\nThe persistence of recognition accuracy of three of the most commonly used\nbiometric traits (fingerprints, face and iris) has been investigated for\nadults. However, persistence of biometric recognition accuracy has not been\nstudied systematically for children in the age group of 0-4 years. Given that\nvery young children are often uncooperative and do not comprehend or follow\ninstructions, in our opinion, among all biometric modalities, fingerprints are\nthe most viable for recognizing children. This is primarily because it is\neasier to capture fingerprints of young children compared to other biometric\ntraits, e.g., iris, where a child needs to stare directly towards the camera to\ninitiate iris capture. In this report, we detail our initiative to investigate\nthe persistence of fingerprint recognition for children in the age group of 0-4\nyears. Based on preliminary results obtained for the data collected in the\nfirst phase of our study, use of fingerprints for recognition of 0-4 year-old\nchildren appears promising.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2015 22:19:53 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Jain", "Anil K.", ""], ["Arora", "Sunpreet S.", ""], ["Best-Rowden", "Lacey", ""], ["Cao", "Kai", ""], ["Sudhish", "Prem Sewak", ""], ["Bhatnagar", "Anjoo", ""]]}, {"id": "1504.04660", "submitter": "Neal Hurlburt", "authors": "Neal Hurlburt and Steve Jaffey", "title": "A spectral optical flow method for determining velocities from digital\n  imagery", "comments": "12 pages, 5 figures. Submitted to Earth Science Informatics", "journal-ref": "Earth Science Informatics: Volume 8, Issue 4 (2015), Page 959-965", "doi": "10.1007/s12145-015-0224-4", "report-no": null, "categories": "cs.CV astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for determining surface flows from solar images based\nupon optical flow techniques. We apply the method to sets of images obtained by\na variety of solar imagers to assess its performance. The {\\tt opflow3d}\nprocedure is shown to extract accurate velocity estimates when provided perfect\ntest data and quickly generates results consistent with completely distinct\nmethods when applied on global scales. We also validate it in detail by\ncomparing it to an established method when applied to high-resolution datasets\nand find that it provides comparable results without the need to tune, filter\nor otherwise preprocess the images before its application.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2015 23:44:20 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Hurlburt", "Neal", ""], ["Jaffey", "Steve", ""]]}, {"id": "1504.04763", "submitter": "David Novotny", "authors": "David Novotn\\'y, Diane Larlus, Florent Perronnin, Andrea Vedaldi", "title": "Understanding the Fisher Vector: a multimodal part model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fisher Vectors and related orderless visual statistics have demonstrated\nexcellent performance in object detection, sometimes superior to established\napproaches such as the Deformable Part Models. However, it remains unclear how\nthese models can capture complex appearance variations using visual codebooks\nof limited sizes and coarse geometric information. In this work, we propose to\ninterpret Fisher-Vector-based object detectors as part-based models. Through\nthe use of several visualizations and experiments, we show that this is a\nuseful insight to explain the good performance of the model. Furthermore, we\nreveal for the first time several interesting properties of the FV, including\nits ability to work well using only a small subset of input patches and visual\nwords. Finally, we discuss the relation of the FV and DPM detectors, pointing\nout differences and commonalities between them.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 21:12:41 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Novotn\u00fd", "David", ""], ["Larlus", "Diane", ""], ["Perronnin", "Florent", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1504.04792", "submitter": "Jianxin Wu", "authors": "Jianxin Wu, Bin-Bin Gao, and Guoqing Liu", "title": "Visual Recognition Using Directional Distribution Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision, an entity such as an image or video is often represented\nas a set of instance vectors, which can be SIFT, motion, or deep learning\nfeature vectors extracted from different parts of that entity. Thus, it is\nessential to design efficient and effective methods to compare two sets of\ninstance vectors. Existing methods such as FV, VLAD or Super Vectors have\nachieved excellent results. However, this paper shows that these methods are\ndesigned based on a generative perspective, and a discriminative method can be\nmore effective in categorizing images or videos. The proposed D3\n(discriminative distribution distance) method effectively compares two sets as\ntwo distributions, and proposes a directional total variation distance (DTVD)\nto measure how separated are they. Furthermore, a robust classifier-based\nmethod is proposed to estimate DTVD robustly. The D3 method is evaluated in\naction and image recognition tasks and has achieved excellent accuracy and\nspeed. D3 also has a synergy with FV. The combination of D3 and FV has\nadvantages over D3, FV, and VLAD.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2015 04:55:59 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2015 00:37:20 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Wu", "Jianxin", ""], ["Gao", "Bin-Bin", ""], ["Liu", "Guoqing", ""]]}, {"id": "1504.04871", "submitter": "Sukrit Shankar", "authors": "Sukrit Shankar, Vikas K. Garg, Roberto Cipolla", "title": "DEEP-CARVING: Discovering Visual Attributes by Carving Deep Neural Nets", "comments": "10 pages, 8 figures, CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the approaches for discovering visual attributes in images demand\nsignificant supervision, which is cumbersome to obtain. In this paper, we aim\nto discover visual attributes in a weakly supervised setting that is commonly\nencountered with contemporary image search engines. Deep Convolutional Neural\nNetworks (CNNs) have enjoyed remarkable success in vision applications\nrecently. However, in a weakly supervised scenario, widely used CNN training\nprocedures do not learn a robust model for predicting multiple attribute labels\nsimultaneously. The primary reason is that the attributes highly co-occur\nwithin the training data. To ameliorate this limitation, we propose\nDeep-Carving, a novel training procedure with CNNs, that helps the net\nefficiently carve itself for the task of multiple attribute prediction. During\ntraining, the responses of the feature maps are exploited in an ingenious way\nto provide the net with multiple pseudo-labels (for training images) for\nsubsequent iterations. The process is repeated periodically after a fixed\nnumber of iterations, and enables the net carve itself iteratively for\nefficiently disentangling features. Additionally, we contribute a\nnoun-adjective pairing inspired Natural Scenes Attributes Dataset to the\nresearch community, CAMIT - NSAD, containing a number of co-occurring\nattributes within a noun category. We describe, in detail, salient aspects of\nthis dataset. Our experiments on CAMIT-NSAD and the SUN Attributes Dataset,\nwith weak supervision, clearly demonstrate that the Deep-Carved CNNs\nconsistently achieve considerable improvement in the precision of attribute\nprediction over popular baseline methods.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2015 18:56:52 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Shankar", "Sukrit", ""], ["Garg", "Vikas K.", ""], ["Cipolla", "Roberto", ""]]}, {"id": "1504.04923", "submitter": "Chunhua Shen", "authors": "Ruizhi Qiao, Lingqiao Liu, Chunhua Shen, and Anton von den Hengel", "title": "Learning discriminative trajectorylet detector sets for accurate\n  skeleton-based action recognition", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The introduction of low-cost RGB-D sensors has promoted the research in\nskeleton-based human action recognition. Devising a representation suitable for\ncharacterising actions on the basis of noisy skeleton sequences remains a\nchallenge, however. We here provide two insights into this challenge. First, we\nshow that the discriminative information of a skeleton sequence usually resides\nin a short temporal interval and we propose a simple-but-effective local\ndescriptor called trajectorylet to capture the static and kinematic information\nwithin this interval. Second, we further propose to encode each trajectorylet\nwith a discriminative trajectorylet detector set which is selected from a large\nnumber of candidate detectors trained through exemplar-SVMs. The action-level\nrepresentation is obtained by pooling trajectorylet encodings. Evaluating on\nstandard datasets acquired from the Kinect sensor, it is demonstrated that our\nmethod obtains superior results over existing approaches under various\nexperimental setups.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 02:41:03 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Qiao", "Ruizhi", ""], ["Liu", "Lingqiao", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton von den", ""]]}, {"id": "1504.04943", "submitter": "Yu Zhang", "authors": "Yu Zhang and Xiu-shen Wei and Jianxin Wu and Jianfei Cai and Jiangbo\n  Lu and Viet-Anh Nguyen and Minh N. Do", "title": "Weakly Supervised Fine-Grained Image Categorization", "comments": null, "journal-ref": "An extended version in IEEE Trans Image Processing, 25(4), 2016:\n  pp. 1713-1725", "doi": "10.1109/TIP.2016.2531289", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we categorize fine-grained images without using any object /\npart annotation neither in the training nor in the testing stage, a step\ntowards making it suitable for deployments. Fine-grained image categorization\naims to classify objects with subtle distinctions. Most existing works heavily\nrely on object / part detectors to build the correspondence between object\nparts by using object or object part annotations inside training images. The\nneed for expensive object annotations prevents the wide usage of these methods.\nInstead, we propose to select useful parts from multi-scale part proposals in\nobjects, and use them to compute a global image representation for\ncategorization. This is specially designed for the annotation-free fine-grained\ncategorization task, because useful parts have shown to play an important role\nin existing annotation-dependent works but accurate part detectors can be\nhardly acquired. With the proposed image representation, we can further detect\nand visualize the key (most discriminative) parts in objects of different\nclasses. In the experiment, the proposed annotation-free method achieves better\naccuracy than that of state-of-the-art annotation-free and most existing\nannotation-dependent methods on two challenging datasets, which shows that it\nis not always necessary to use accurate object / part annotations in\nfine-grained image categorization.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 05:58:21 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Zhang", "Yu", ""], ["Wei", "Xiu-shen", ""], ["Wu", "Jianxin", ""], ["Cai", "Jianfei", ""], ["Lu", "Jiangbo", ""], ["Nguyen", "Viet-Anh", ""], ["Do", "Minh N.", ""]]}, {"id": "1504.05035", "submitter": "Wangmeng Zuo", "authors": "Xiaohe Wu, Wangmeng Zuo, Yuanyuan Zhu, Liang Lin", "title": "F-SVM: Combination of Feature Transformation and SVM Learning via Convex\n  Relaxation", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalization error bound of support vector machine (SVM) depends on the\nratio of radius and margin, while standard SVM only considers the maximization\nof the margin but ignores the minimization of the radius. Several approaches\nhave been proposed to integrate radius and margin for joint learning of feature\ntransformation and SVM classifier. However, most of them either require the\nform of the transformation matrix to be diagonal, or are non-convex and\ncomputationally expensive. In this paper, we suggest a novel approximation for\nthe radius of minimum enclosing ball (MEB) in feature space, and then propose a\nconvex radius-margin based SVM model for joint learning of feature\ntransformation and SVM classifier, i.e., F-SVM. An alternating minimization\nmethod is adopted to solve the F-SVM model, where the feature transformation is\nupdatedvia gradient descent and the classifier is updated by employing the\nexisting SVM solver. By incorporating with kernel principal component analysis,\nF-SVM is further extended for joint learning of nonlinear transformation and\nclassifier. Experimental results on the UCI machine learning datasets and the\nLFW face datasets show that F-SVM outperforms the standard SVM and the existing\nradius-margin based SVMs, e.g., RMM, R-SVM+ and R-SVM+{\\mu}.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 12:36:50 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Wu", "Xiaohe", ""], ["Zuo", "Wangmeng", ""], ["Zhu", "Yuanyuan", ""], ["Lin", "Liang", ""]]}, {"id": "1504.05133", "submitter": "Joe Yue-Hei Ng", "authors": "Joe Yue-Hei Ng, Fan Yang, Larry S. Davis", "title": "Exploiting Local Features from Deep Networks for Image Retrieval", "comments": "CVPR DeepVision Workshop 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have been successfully applied to image\nclassification tasks. When these same networks have been applied to image\nretrieval, the assumption has been made that the last layers would give the\nbest performance, as they do in classification. We show that for instance-level\nimage retrieval, lower layers often perform better than the last layers in\nconvolutional neural networks. We present an approach for extracting\nconvolutional features from different layers of the networks, and adopt VLAD\nencoding to encode features into a single vector for each image. We investigate\nthe effect of different layers and scales of input images on the performance of\nconvolutional features using the recent deep networks OxfordNet and GoogLeNet.\nExperiments demonstrate that intermediate layers or higher layers with finer\nscales produce better results for image retrieval, compared to the last layer.\nWhen using compressed 128-D VLAD descriptors, our method obtains\nstate-of-the-art results and outperforms other VLAD and CNN based approaches on\ntwo out of three test datasets. Our work provides guidance for transferring\ndeep networks trained on image classification to image retrieval tasks.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 17:41:46 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2015 03:36:25 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Ng", "Joe Yue-Hei", ""], ["Yang", "Fan", ""], ["Davis", "Larry S.", ""]]}, {"id": "1504.05137", "submitter": "Helio M. de Oliveira", "authors": "V.V. Vermehren Valenzuela, R.D. Lins, H.M. de Oliveira", "title": "Application of Enhanced-2D-CWT in Topographic Images for Mapping\n  Landslide Risk Areas", "comments": "8 pages, 8 figures; Lecture Notes in Computer Science LNCS 7950,\n  pp.380-388, 2013 Springer-Verlag, Heidelberg ISBN: 978-3-642-39093-7", "journal-ref": null, "doi": "10.1007/978-3-642-39094-4_43", "report-no": null, "categories": "cs.CV physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been lately a number of catastrophic events of landslides and\nmudslides in the mountainous region of Rio de Janeiro, Brazil. Those were\ncaused by intense rain in localities where there was unplanned occupation of\nslopes of hills and mountains. Thus, it became imperative creating an inventory\nof landslide risk areas in densely populated cities. This work presents a way\nof demarcating risk areas by using the bidimensional Continuous Wavelet\nTransform (2D-CWT) applied to high resolution topographic images of the\nmountainous region of Rio de Janeiro.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 14:02:22 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Valenzuela", "V. V. Vermehren", ""], ["Lins", "R. D.", ""], ["de Oliveira", "H. M.", ""]]}, {"id": "1504.05241", "submitter": "Hong Zhang Hong Zhang", "authors": "Yi Hou, Hong Zhang, Shilin Zhou", "title": "Convolutional Neural Network-Based Image Representation for Visual Loop\n  Closure Detection", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNN) have recently been shown in many\ncomputer vision and pattern recog- nition applications to outperform by a\nsignificant margin state- of-the-art solutions that use traditional\nhand-crafted features. However, this impressive performance is yet to be fully\nexploited in robotics. In this paper, we focus one specific problem that can\nbenefit from the recent development of the CNN technology, i.e., we focus on\nusing a pre-trained CNN model as a method of generating an image representation\nappropriate for visual loop closure detection in SLAM (simultaneous\nlocalization and mapping). We perform a comprehensive evaluation of the outputs\nat the intermediate layers of a CNN as image descriptors, in comparison with\nstate-of-the-art image descriptors, in terms of their ability to match images\nfor detecting loop closures. The main conclusions of our study include: (a)\nCNN-based image representations perform comparably to state-of-the-art hand-\ncrafted competitors in environments without significant lighting change, (b)\nthey outperform state-of-the-art competitors when lighting changes\nsignificantly, and (c) they are also significantly faster to extract than the\nstate-of-the-art hand-crafted features even on a conventional CPU and are two\norders of magnitude faster on an entry-level GPU.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 21:43:45 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Hou", "Yi", ""], ["Zhang", "Hong", ""], ["Zhou", "Shilin", ""]]}, {"id": "1504.05277", "submitter": "Jianxin Wu", "authors": "Bin-Bin Gao and Xiu-Shen Wei and Jianxin Wu and Weiyao Lin", "title": "Deep Spatial Pyramid: The Devil is Once Again in the Details", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show that by carefully making good choices for various\ndetailed but important factors in a visual recognition framework using deep\nlearning features, one can achieve a simple, efficient, yet highly accurate\nimage classification system. We first list 5 important factors, based on both\nexisting researches and ideas proposed in this paper. These important detailed\nfactors include: 1) $\\ell_2$ matrix normalization is more effective than\nunnormalized or $\\ell_2$ vector normalization, 2) the proposed natural deep\nspatial pyramid is very effective, and 3) a very small $K$ in Fisher Vectors\nsurprisingly achieves higher accuracy than normally used large $K$ values.\nAlong with other choices (convolutional activations and multiple scales), the\nproposed DSP framework is not only intuitive and efficient, but also achieves\nexcellent classification accuracy on many benchmark datasets. For example,\nDSP's accuracy on SUN397 is 59.78%, significantly higher than previous\nstate-of-the-art (53.86%).\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 02:13:44 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2015 02:20:26 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Gao", "Bin-Bin", ""], ["Wei", "Xiu-Shen", ""], ["Wu", "Jianxin", ""], ["Lin", "Weiyao", ""]]}, {"id": "1504.05298", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Ognjen Arandjelovic, Duc-Son Pham, Svetha Venkatesh", "title": "Viewpoint distortion compensation in practical surveillance systems", "comments": "International Conference on Multimedia & Expo, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our aim is to estimate the perspective-effected geometric distortion of a\nscene from a video feed. In contrast to all previous work we wish to achieve\nthis using from low-level, spatio-temporally local motion features used in\ncommercial semi-automatic surveillance systems. We: (i) describe a dense\nalgorithm which uses motion features to estimate the perspective distortion at\neach image locus and then polls all such local estimates to arrive at the\nglobally best estimate, (ii) present an alternative coarse algorithm which\nsubdivides the image frame into blocks, and uses motion features to derive\nblock-specific motion characteristics and constrain the relationships between\nthese characteristics, with the perspective estimate emerging as a result of a\nglobal optimization scheme, and (iii) report the results of an evaluation using\nnine large sets acquired using existing close-circuit television (CCTV)\ncameras. Our findings demonstrate that both of the proposed methods are\nsuccessful, their accuracy matching that of human labelling using complete\nvisual data.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 04:08:39 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Arandjelovic", "Ognjen", ""], ["Pham", "Duc-Son", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1504.05299", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Ognjen Arandjelovic, Duc-Son Pham, Svetha Venkatesh", "title": "Groupwise registration of aerial images", "comments": "International Joint Conference on Artificial Intelligence, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the task of time separated aerial image registration.\nThe ability to solve this problem accurately and reliably is important for a\nvariety of subsequent image understanding applications. The principal challenge\nlies in the extent and nature of transient appearance variation that a land\narea can undergo, such as that caused by the change in illumination conditions,\nseasonal variations, or the occlusion by non-persistent objects (people, cars).\nOur work introduces several novelties: (i) unlike all previous work on aerial\nimage registration, we approach the problem using a set-based paradigm; (ii) we\nshow how local, pair-wise constraints can be used to enforce a globally good\nregistration using a constraints graph structure; (iii) we show how a simple\nholistic representation derived from raw aerial images can be used as a basic\nbuilding block of the constraints graph in a manner which achieves both high\nregistration accuracy and speed. We demonstrate: (i) that the proposed method\noutperforms the state-of-the-art for pair-wise registration already, achieving\ngreater accuracy and reliability, while at the same time reducing the\ncomputational cost of the task; and (ii) that the increase in the number of\navailable images in a set consistently reduces the average registration error.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 04:14:02 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Arandjelovic", "Ognjen", ""], ["Pham", "Duc-Son", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1504.05302", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Ognjen Arandjelovic, Duc-Son Pham, Svetha Venkatesh", "title": "The adaptable buffer algorithm for high quantile estimation in\n  non-stationary data streams", "comments": "International Joint Conference on Neural Networks, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to estimate a particular quantile of a distribution is an important\nproblem which frequently arises in many computer vision and signal processing\napplications. For example, our work was motivated by the requirements of many\nsemi-automatic surveillance analytics systems which detect abnormalities in\nclose-circuit television (CCTV) footage using statistical models of low-level\nmotion features. In this paper we specifically address the problem of\nestimating the running quantile of a data stream with non-stationary\nstochasticity when the memory for storing observations is limited. We make\nseveral major contributions: (i) we derive an important theoretical result\nwhich shows that the change in the quantile of a stream is constrained\nregardless of the stochastic properties of data, (ii) we describe a set of\nhigh-level design goals for an effective estimation algorithm that emerge as a\nconsequence of our theoretical findings, (iii) we introduce a novel algorithm\nwhich implements the aforementioned design goals by retaining a sample of data\nvalues in a manner adaptive to changes in the distribution of data and\nprogressively narrowing down its focus in the periods of quasi-stationary\nstochasticity, and (iv) we present a comprehensive evaluation of the proposed\nalgorithm and compare it with the existing methods in the literature on both\nsynthetic data sets and three large `real-world' streams acquired in the course\nof operation of an existing commercial surveillance system. Our findings\nconvincingly demonstrate that the proposed method is highly successful and\nvastly outperforms the existing alternatives, especially when the target\nquantile is high valued and the available buffer capacity severely limited.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 04:42:58 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Arandjelovic", "Ognjen", ""], ["Pham", "Duc-Son", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1504.05308", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Ognjen Arandjelovic", "title": "Automatic Face Recognition from Video", "comments": "Doctor of Philosophy (PhD) dissertation, University of Cambridge,\n  2007", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this work is to automatically recognize faces from video\nsequences in a realistic, unconstrained setup in which illumination conditions\nare extreme and greatly changing, viewpoint and user motion pattern have a wide\nvariability, and video input is of low quality. At the centre of focus are face\nappearance manifolds: this thesis presents a significant advance of their\nunderstanding and application in the sphere of face recognition. The two main\ncontributions are the Generic Shape-Illumination Manifold recognition algorithm\nand the Anisotropic Manifold Space clustering. The Generic Shape-Illumination\nManifold is evaluated on a large data corpus acquired in real-world conditions\nand its performance is shown to greatly exceed that of state-of-the-art methods\nin the literature and the best performing commercial software. Empirical\nevaluation of the Anisotropic Manifold Space clustering on a popular situation\ncomedy is also described with excellent preliminary results.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 05:10:41 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Arandjelovic", "Ognjen", ""]]}, {"id": "1504.05369", "submitter": "Dan Zecha", "authors": "Dan Zecha and Rainer Lienhart", "title": "Key-Pose Prediction in Cyclic Human Motion", "comments": "Accepted at WACV 2015, 8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of estimating innercyclic time intervals\nwithin repetitive motion sequences of top-class swimmers in a swimming channel.\nInterval limits are given by temporal occurrences of key-poses, i.e.\ndistinctive postures of the body. A key-pose is defined by means of only one or\ntwo specific features of the complete posture. It is often difficult to detect\nsuch subtle features directly. We therefore propose the following method: Given\nthat we observe the swimmer from the side, we build a pictorial structure of\nposelets to robustly identify random support poses within the regular motion of\na swimmer. We formulate a maximum likelihood model which predicts a key-pose\ngiven the occurrences of multiple support poses within one stroke. The maximum\nlikelihood can be extended with prior knowledge about the temporal location of\na key-pose in order to improve the prediction recall. We experimentally show\nthat our models reliably and robustly detect key-poses with a high precision\nand that their performance can be improved by extending the framework with\nadditional camera views.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 10:09:43 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Zecha", "Dan", ""], ["Lienhart", "Rainer", ""]]}, {"id": "1504.05451", "submitter": "Jing Yang", "authors": "Qingshan Liu, Jing Yang, Kaihua Zhang, Yi Wu", "title": "Adaptive Compressive Tracking via Online Vector Boosting Feature\n  Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the compressive tracking (CT) method has attracted much attention\ndue to its high efficiency, but it cannot well deal with the large scale target\nappearance variations due to its data-independent random projection matrix that\nresults in less discriminative features. To address this issue, in this paper\nwe propose an adaptive CT approach, which selects the most discriminative\nfeatures to design an effective appearance model. Our method significantly\nimproves CT in three aspects: Firstly, the most discriminative features are\nselected via an online vector boosting method. Secondly, the object\nrepresentation is updated in an effective online manner, which preserves the\nstable features while filtering out the noisy ones. Finally, a simple and\neffective trajectory rectification approach is adopted that can make the\nestimated location more accurate. Extensive experiments on the CVPR2013\ntracking benchmark demonstrate the superior performance of our algorithm\ncompared over state-of-the-art tracking algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 14:55:07 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2015 01:27:08 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Liu", "Qingshan", ""], ["Yang", "Jing", ""], ["Zhang", "Kaihua", ""], ["Wu", "Yi", ""]]}, {"id": "1504.05524", "submitter": "Dan Oneata", "authors": "Heng Wang, Dan Oneata, Jakob Verbeek, Cordelia Schmid", "title": "A robust and efficient video representation for action recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a state-of-the-art video representation and applies it\nto efficient action recognition and detection. We first propose to improve the\npopular dense trajectory features by explicit camera motion estimation. More\nspecifically, we extract feature point matches between frames using SURF\ndescriptors and dense optical flow. The matches are used to estimate a\nhomography with RANSAC. To improve the robustness of homography estimation, a\nhuman detector is employed to remove outlier matches from the human body as\nhuman motion is not constrained by the camera. Trajectories consistent with the\nhomography are considered as due to camera motion, and thus removed. We also\nuse the homography to cancel out camera motion from the optical flow. This\nresults in significant improvement on motion-based HOF and MBH descriptors. We\nfurther explore the recent Fisher vector as an alternative feature encoding\napproach to the standard bag-of-words histogram, and consider different ways to\ninclude spatial layout information in these encodings. We present a large and\nvaried set of evaluations, considering (i) classification of short basic\nactions on six datasets, (ii) localization of such actions in feature-length\nmovies, and (iii) large-scale recognition of complex events. We find that our\nimproved trajectory features significantly outperform previous dense\ntrajectories, and that Fisher vectors are superior to bag-of-words encodings\nfor video recognition tasks. In all three tasks, we show substantial\nimprovements over the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 17:44:07 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Wang", "Heng", ""], ["Oneata", "Dan", ""], ["Verbeek", "Jakob", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1504.05623", "submitter": "Michael Greminger", "authors": "Michael A. Greminger", "title": "Median and Mode Ellipse Parameterization for Robust Contour Fitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problems that require the parameterization of closed contours arise\nfrequently in computer vision applications. This article introduces a new curve\nparameterization algorithm that is able to fit a closed curve to a set of\npoints while being robust to the presence of outliers and occlusions in the\ndata. This robustness property makes this algorithm applicable to computer\nvision applications where misclassification of features may lead to outliers.\nThe algorithm starts by fitting ellipses to numerous five point subsets from\nthe source data. The closed curve is parameterized by determining the median\nperimeter of the set of ellipses. The resulting curve is not an ellipse,\nallowing arbitrary closed contours to be parameterized. The use of the modal\nperimeter rather than the median perimeter is also explored. A detailed\ncomparison is made between the proposed curve fitting algorithm and existing\nrobust ellipse fitting algorithms. Finally, the utility of the algorithm for\ncomputer vision applications is demonstrated through the parameterization of\nthe boundary of fuel droplets during combustion. The performance of the\nproposed algorithm and the performance of existing algorithms are compared to a\nground truth segmentation of the fuel droplet images, which demonstrates\nimproved performance for both area quantification and edge deviation.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 00:12:07 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Greminger", "Michael A.", ""]]}, {"id": "1504.05632", "submitter": "Zhangyang Wang", "authors": "Zhangyang Wang, Yingzhen Yang, Zhaowen Wang, Shiyu Chang, Wei Han,\n  Jianchao Yang, and Thomas S. Huang", "title": "Self-Tuned Deep Super Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been successfully applied to image super resolution (SR).\nIn this paper, we propose a deep joint super resolution (DJSR) model to exploit\nboth external and self similarities for SR. A Stacked Denoising Convolutional\nAuto Encoder (SDCAE) is first pre-trained on external examples with proper data\naugmentations. It is then fine-tuned with multi-scale self examples from each\ninput, where the reliability of self examples is explicitly taken into account.\nWe also enhance the model performance by sub-model training and selection. The\nDJSR model is extensively evaluated and compared with state-of-the-arts, and\nshow noticeable performance improvements both quantitatively and perceptually\non a wide range of images.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 02:01:36 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Wang", "Zhangyang", ""], ["Yang", "Yingzhen", ""], ["Wang", "Zhaowen", ""], ["Chang", "Shiyu", ""], ["Han", "Wei", ""], ["Yang", "Jianchao", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1504.05776", "submitter": "Nelly Pustelnik", "authors": "Nelly Pustelnik, Herwig Wendt, Patrice Abry, Nicolas Dobigeon", "title": "Combining local regularity estimation and total variation optimization\n  for scale-free texture segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture segmentation constitutes a standard image processing task, crucial to\nmany applications. The present contribution focuses on the particular subset of\nscale-free textures and its originality resides in the combination of three key\ningredients: First, texture characterization relies on the concept of local\nregularity ; Second, estimation of local regularity is based on new multiscale\nquantities referred to as wavelet leaders ; Third, segmentation from local\nregularity faces a fundamental bias variance trade-off: In nature, local\nregularity estimation shows high variability that impairs the detection of\nchanges, while a posteriori smoothing of regularity estimates precludes from\nlocating correctly changes. Instead, the present contribution proposes several\nvariational problem formulations based on total variation and proximal\nresolutions that effectively circumvent this trade-off. Estimation and\nsegmentation performance for the proposed procedures are quantified and\ncompared on synthetic as well as on real-world textures.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 13:01:12 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2016 13:58:32 GMT"}, {"version": "v3", "created": "Fri, 24 Jun 2016 08:22:00 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Pustelnik", "Nelly", ""], ["Wendt", "Herwig", ""], ["Abry", "Patrice", ""], ["Dobigeon", "Nicolas", ""]]}, {"id": "1504.05809", "submitter": "Xianbiao Qi", "authors": "Xianbiao Qi, Guoying Zhao, Linlin Shen, Qingquan Li, Matti Pietikainen", "title": "LOAD: Local Orientation Adaptive Descriptor for Texture and Material\n  Classification", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel local feature, called Local Orientation\nAdaptive Descriptor (LOAD), to capture regional texture in an image. In LOAD,\nwe proposed to define point description on an Adaptive Coordinate System (ACS),\nadopt a binary sequence descriptor to capture relationships between one point\nand its neighbors and use multi-scale strategy to enhance the discriminative\npower of the descriptor. The proposed LOAD enjoys not only discriminative power\nto capture the texture information, but also has strong robustness to\nillumination variation and image rotation. Extensive experiments on benchmark\ndata sets of texture classification and real-world material recognition show\nthat the proposed LOAD yields the state-of-the-art performance. It is worth to\nmention that we achieve a 65.4\\% classification accuracy-- which is, to the\nbest of our knowledge, the highest record by far --on Flickr Material Database\nby using a single feature. Moreover, by combining LOAD with the feature\nextracted by Convolutional Neural Networks (CNN), we obtain significantly\nbetter performance than both the LOAD and CNN. This result confirms that the\nLOAD is complementary to the learning-based features.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 13:59:49 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Qi", "Xianbiao", ""], ["Zhao", "Guoying", ""], ["Shen", "Linlin", ""], ["Li", "Qingquan", ""], ["Pietikainen", "Matti", ""]]}, {"id": "1504.05843", "submitter": "Hao Yang Mr", "authors": "Hao Yang, Joey Tianyi Zhou, Yu Zhang, Bin-Bin Gao, Jianxin Wu, Jianfei\n  Cai", "title": "Exploit Bounding Box Annotations for Multi-label Object Recognition", "comments": "Accepted in CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have shown great performance as general\nfeature representations for object recognition applications. However, for\nmulti-label images that contain multiple objects from different categories,\nscales and locations, global CNN features are not optimal. In this paper, we\nincorporate local information to enhance the feature discriminative power. In\nparticular, we first extract object proposals from each image. With each image\ntreated as a bag and object proposals extracted from it treated as instances,\nwe transform the multi-label recognition problem into a multi-class\nmulti-instance learning problem. Then, in addition to extracting the typical\nCNN feature representation from each proposal, we propose to make use of\nground-truth bounding box annotations (strong labels) to add another level of\nlocal information by using nearest-neighbor relationships of local regions to\nform a multi-view pipeline. The proposed multi-view multi-instance framework\nutilizes both weak and strong labels effectively, and more importantly it has\nthe generalization ability to even boost the performance of unseen categories\nby partial strong labels from other categories. Our framework is extensively\ncompared with state-of-the-art hand-crafted feature based methods and CNN based\nmethods on two multi-label benchmark datasets. The experimental results\nvalidate the discriminative power and the generalization ability of the\nproposed framework. With strong labels, our framework is able to achieve\nstate-of-the-art results in both datasets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 15:01:29 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2016 09:44:35 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Yang", "Hao", ""], ["Zhou", "Joey Tianyi", ""], ["Zhang", "Yu", ""], ["Gao", "Bin-Bin", ""], ["Wu", "Jianxin", ""], ["Cai", "Jianfei", ""]]}, {"id": "1504.06036", "submitter": "Andrew  Brustolin", "authors": "Andrew F. C. Brustolin", "title": "Edge Detection Based on Global and Local Parameters of the Image", "comments": "13 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an edge detection method based on global and local\nparameters of the image, which produces satisfactory results on the edge\ndetection of complex images and has a simple structure for execution. The local\nand global parameters of the image are arithmetic means and standard\ndeviations, the former acquired from a three sized window representing five\npixels, the latter acquired from the entire row or column. We obtain the\ndifferences of grayscale intensities between two adjacent pixels and the sum of\nthe modulus of these differences from the horizontal and vertical scans of the\nimage. Using these obtained values, we calculate the local and global\nparameters. After the gathering of the local and global parameters, we compare\neach sum of the modulus of differences with its own local and global parameter.\nIn the case of the comparison is true, the consecutive pixel to the modulus sum\nof differences index is marked as an edge. We present the results of the tests\nwith grayscale images using different parameters and discuss the advantages and\ndisadvantages of each parameter value and algorithm structure chosen on the\nedge processing. There is a comparison of results between this papers detector\nand Canny, where we evaluate the quality of the presented detector.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 04:11:41 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Brustolin", "Andrew F. C.", ""]]}, {"id": "1504.06055", "submitter": "Naiyan Wang", "authors": "Naiyan Wang, Jianping Shi, Dit-Yan Yeung, Jiaya Jia", "title": "Understanding and Diagnosing Visual Tracking Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several benchmark datasets for visual tracking research have been proposed in\nrecent years. Despite their usefulness, whether they are sufficient for\nunderstanding and diagnosing the strengths and weaknesses of different trackers\nremains questionable. To address this issue, we propose a framework by breaking\na tracker down into five constituent parts, namely, motion model, feature\nextractor, observation model, model updater, and ensemble post-processor. We\nthen conduct ablative experiments on each component to study how it affects the\noverall result. Surprisingly, our findings are discrepant with some common\nbeliefs in the visual tracking research community. We find that the feature\nextractor plays the most important role in a tracker. On the other hand,\nalthough the observation model is the focus of many studies, we find that it\noften brings no significant improvement. Moreover, the motion model and model\nupdater contain many details that could affect the result. Also, the ensemble\npost-processor can improve the result substantially when the constituent\ntrackers have high diversity. Based on our findings, we put together some very\nelementary building blocks to give a basic tracker which is competitive in\nperformance to the state-of-the-art trackers. We believe our framework can\nprovide a solid baseline when conducting controlled experiments for visual\ntracking research.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 06:37:29 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Wang", "Naiyan", ""], ["Shi", "Jianping", ""], ["Yeung", "Dit-Yan", ""], ["Jia", "Jiaya", ""]]}, {"id": "1504.06063", "submitter": "Lin Ma", "authors": "Lin Ma, Zhengdong Lu, Lifeng Shang, Hang Li", "title": "Multimodal Convolutional Neural Networks for Matching Image and Sentence", "comments": "Accepted by ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose multimodal convolutional neural networks (m-CNNs)\nfor matching image and sentence. Our m-CNN provides an end-to-end framework\nwith convolutional architectures to exploit image representation, word\ncomposition, and the matching relations between the two modalities. More\nspecifically, it consists of one image CNN encoding the image content, and one\nmatching CNN learning the joint representation of image and sentence. The\nmatching CNN composes words to different semantic fragments and learns the\ninter-modal relations between image and the composed fragments at different\nlevels, thus fully exploit the matching relations between image and sentence.\nExperimental results on benchmark databases of bidirectional image and sentence\nretrieval demonstrate that the proposed m-CNNs can effectively capture the\ninformation necessary for image and sentence matching. Specifically, our\nproposed m-CNNs for bidirectional image and sentence retrieval on Flickr30K and\nMicrosoft COCO databases achieve the state-of-the-art performances.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 07:10:13 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2015 01:47:05 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2015 08:09:54 GMT"}, {"version": "v4", "created": "Tue, 21 Jul 2015 07:30:53 GMT"}, {"version": "v5", "created": "Sat, 29 Aug 2015 09:35:09 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Ma", "Lin", ""], ["Lu", "Zhengdong", ""], ["Shang", "Lifeng", ""], ["Li", "Hang", ""]]}, {"id": "1504.06066", "submitter": "Kaiming He", "authors": "Shaoqing Ren, Kaiming He, Ross Girshick, Xiangyu Zhang, Jian Sun", "title": "Object Detection Networks on Convolutional Feature Maps", "comments": "To appear in TPAMI; substantial re-writing over the original post at\n  arXiv of April 2015. COCO competition results included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most object detectors contain two important components: a feature extractor\nand an object classifier. The feature extractor has rapidly evolved with\nsignificant research efforts leading to better deep convolutional\narchitectures. The object classifier, however, has not received much attention\nand many recent systems (like SPPnet and Fast/Faster R-CNN) use simple\nmulti-layer perceptrons. This paper demonstrates that carefully designing deep\nnetworks for object classification is just as important. We experiment with\nregion-wise classifier networks that use shared, region-independent\nconvolutional features. We call them \"Networks on Convolutional feature maps\"\n(NoCs). We discover that aside from deep feature maps, a deep and convolutional\nper-region classifier is of particular importance for object detection, whereas\nlatest superior image classification models (such as ResNets and GoogLeNets) do\nnot directly lead to good detection accuracy without using such a per-region\nclassifier. We show by experiments that despite the effective ResNets and\nFaster R-CNN systems, the design of NoCs is an essential element for the\n1st-place winning entries in ImageNet and MS COCO challenges 2015.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 07:15:10 GMT"}, {"version": "v2", "created": "Wed, 17 Aug 2016 15:51:13 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Ren", "Shaoqing", ""], ["He", "Kaiming", ""], ["Girshick", "Ross", ""], ["Zhang", "Xiangyu", ""], ["Sun", "Jian", ""]]}, {"id": "1504.06103", "submitter": "Tomas Vojir", "authors": "Tomas Vojir, Jiri Matas, Jana Noskova", "title": "Online Adaptive Hidden Markov Model for Multi-Tracker Fusion", "comments": "27 pages, 9 figures, submitted to CVIU journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel method for visual object tracking called\nHMMTxD. The method fuses observations from complementary out-of-the box\ntrackers and a detector by utilizing a hidden Markov model whose latent states\ncorrespond to a binary vector expressing the failure of individual trackers.\nThe Markov model is trained in an unsupervised way, relying on an online\nlearned detector to provide a source of tracker-independent information for a\nmodified Baum- Welch algorithm that updates the model w.r.t. the partially\nannotated data.\n  We show the effectiveness of the proposed method on combination of two and\nthree tracking algorithms. The performance of HMMTxD is evaluated on two\nstandard benchmarks (CVPR2013 and VOT) and on a rich collection of 77 publicly\navailable sequences. The HMMTxD outperforms the state-of-the-art, often\nsignificantly, on all datasets in almost all criteria.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 09:34:59 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 14:52:18 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Vojir", "Tomas", ""], ["Matas", "Jiri", ""], ["Noskova", "Jana", ""]]}, {"id": "1504.06133", "submitter": "Anguelos Nicolaou", "authors": "Anguelos Nicolaou, Andrew D. Bagdanov, Marcus Liwicki, Dimosthenis\n  Karatzas", "title": "Sparse Radial Sampling LBP for Writer Identification", "comments": "Submitted to the 13th International Conference on Document Analysis\n  and Recognition (ICDAR 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the use of Sparse Radial Sampling Local Binary\nPatterns, a variant of Local Binary Patterns (LBP) for text-as-texture\nclassification. By adapting and extending the standard LBP operator to the\nparticularities of text we get a generic text-as-texture classification scheme\nand apply it to writer identification. In experiments on CVL and ICDAR 2013\ndatasets, the proposed feature-set demonstrates State-Of-the-Art (SOA)\nperformance. Among the SOA, the proposed method is the only one that is based\non dense extraction of a single local feature descriptor. This makes it fast\nand applicable at the earliest stages in a DIA pipeline without the need for\nsegmentation, binarization, or extraction of multiple features.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 11:51:53 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Nicolaou", "Anguelos", ""], ["Bagdanov", "Andrew D.", ""], ["Liwicki", "Marcus", ""], ["Karatzas", "Dimosthenis", ""]]}, {"id": "1504.06151", "submitter": "Nauman Shahid", "authors": "Nauman Shahid, Vassilis Kalofolias, Xavier Bresson, Michael Bronstein\n  and Pierre Vandergheynst", "title": "Robust Principal Component Analysis on Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Component Analysis (PCA) is the most widely used tool for linear\ndimensionality reduction and clustering. Still it is highly sensitive to\noutliers and does not scale well with respect to the number of data samples.\nRobust PCA solves the first issue with a sparse penalty term. The second issue\ncan be handled with the matrix factorization model, which is however\nnon-convex. Besides, PCA based clustering can also be enhanced by using a graph\nof data similarity. In this article, we introduce a new model called \"Robust\nPCA on Graphs\" which incorporates spectral graph regularization into the Robust\nPCA framework. Our proposed model benefits from 1) the robustness of principal\ncomponents to occlusions and missing values, 2) enhanced low-rank recovery, 3)\nimproved clustering property due to the graph smoothness assumption on the\nlow-rank matrix, and 4) convexity of the resulting optimization problem.\nExtensive experiments on 8 benchmark, 3 video and 2 artificial datasets with\ncorruptions clearly reveal that our model outperforms 10 other state-of-the-art\nmodels in its clustering and low-rank recovery tasks.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 12:39:40 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Shahid", "Nauman", ""], ["Kalofolias", "Vassilis", ""], ["Bresson", "Xavier", ""], ["Bronstein", "Michael", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1504.06201", "submitter": "Gedas Bertasius", "authors": "Gedas Bertasius, Jianbo Shi and Lorenzo Torresani", "title": "High-for-Low and Low-for-High: Efficient Boundary Detection from Deep\n  Object Features and its Applications to High-Level Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the current boundary detection systems rely exclusively on low-level\nfeatures, such as color and texture. However, perception studies suggest that\nhumans employ object-level reasoning when judging if a particular pixel is a\nboundary. Inspired by this observation, in this work we show how to predict\nboundaries by exploiting object-level features from a pretrained\nobject-classification network. Our method can be viewed as a \"High-for-Low\"\napproach where high-level object features inform the low-level boundary\ndetection process. Our model achieves state-of-the-art performance on an\nestablished boundary detection benchmark and it is efficient to run.\n  Additionally, we show that due to the semantic nature of our boundaries we\ncan use them to aid a number of high-level vision tasks. We demonstrate that\nusing our boundaries we improve the performance of state-of-the-art methods on\nthe problems of semantic boundary labeling, semantic segmentation and object\nproposal generation. We can view this process as a \"Low-for-High\" scheme, where\nlow-level boundaries aid high-level vision tasks.\n  Thus, our contributions include a boundary detection system that is accurate,\nefficient, generalizes well to multiple datasets, and is also shown to improve\nexisting state-of-the-art high-level vision methods on three distinct tasks.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 14:35:12 GMT"}, {"version": "v2", "created": "Fri, 1 May 2015 13:46:18 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2015 17:48:23 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Bertasius", "Gedas", ""], ["Shi", "Jianbo", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "1504.06206", "submitter": "Lu\\'is Pinto", "authors": "Isabel N. Figueiredo, Carlos Leal, Lu\\'is Pinto, Pedro N. Figueiredo,\n  Richard Tsai", "title": "An Elastic Image Registration Approach for Wireless Capsule Endoscope\n  Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless Capsule Endoscope (WCE) is an innovative imaging device that permits\nphysicians to examine all the areas of the Gastrointestinal (GI) tract. It is\nespecially important for the small intestine, where traditional invasive\nendoscopies cannot reach. Although WCE represents an extremely important\nadvance in medical imaging, a major drawback that remains unsolved is the WCE\nprecise location in the human body during its operating time. This is mainly\ndue to the complex physiological environment and the inherent capsule effects\nduring its movement. When an abnormality is detected, in the WCE images,\nmedical doctors do not know precisely where this abnormality is located\nrelative to the intestine and therefore they can not proceed efficiently with\nthe appropriate therapy. The primary objective of the present paper is to give\na contribution to WCE localization, using image-based methods. The main focus\nof this work is on the description of a multiscale elastic image registration\napproach, its experimental application on WCE videos, and comparison with a\nmultiscale affine registration. The proposed approach includes registrations\nthat capture both rigid-like and non-rigid deformations, due respectively to\nthe rigid-like WCE movement and the elastic deformation of the small intestine\noriginated by the GI peristaltic movement. Under this approach a qualitative\ninformation about the WCE speed can be obtained, as well as the WCE location\nand orientation via projective geometry. The results of the experimental tests\nwith real WCE video frames show the good performance of the proposed approach,\nwhen elastic deformations of the small intestine are involved in successive\nframes, and its superiority with respect to a multiscale affine image\nregistration, which accounts for rigid-like deformations only and discards\nelastic deformations.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 14:41:07 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Figueiredo", "Isabel N.", ""], ["Leal", "Carlos", ""], ["Pinto", "Lu\u00eds", ""], ["Figueiredo", "Pedro N.", ""], ["Tsai", "Richard", ""]]}, {"id": "1504.06243", "submitter": "Weiyao Lin", "authors": "Yang Shen, Weiyao Lin, Junchi Yan, Mingliang Xu, Jianxin Wu, Jingdong\n  Wang", "title": "Person Re-identification with Correspondence Structure Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of handling spatial misalignments due to\ncamera-view changes or human-pose variations in person re-identification. We\nfirst introduce a boosting-based approach to learn a correspondence structure\nwhich indicates the patch-wise matching probabilities between images from a\ntarget camera pair. The learned correspondence structure can not only capture\nthe spatial correspondence pattern between cameras but also handle the\nviewpoint or human-pose variation in individual images. We further introduce a\nglobal-based matching process. It integrates a global matching constraint over\nthe learned correspondence structure to exclude cross-view misalignments during\nthe image patch matching process, hence achieving a more reliable matching\nscore between images. Experimental results on various datasets demonstrate the\neffectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 16:24:43 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Shen", "Yang", ""], ["Lin", "Weiyao", ""], ["Yan", "Junchi", ""], ["Xu", "Mingliang", ""], ["Wu", "Jianxin", ""], ["Wang", "Jingdong", ""]]}, {"id": "1504.06266", "submitter": "Hamid Tizhoosh", "authors": "Ahmed Othman, Hamid R. Tizhoosh, Farzad Khalvati", "title": "Evolving Fuzzy Image Segmentation with Self-Configuration", "comments": "Benchmark data (35 breast ultrasound images with gold standard\n  segments) available; 11 pages, 4 algorithms, 6 figures, 5 tables;", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Current image segmentation techniques usually require that the user tune\nseveral parameters in order to obtain maximum segmentation accuracy, a\ncomputationally inefficient approach, especially when a large number of images\nmust be processed sequentially in daily practice. The use of evolving fuzzy\nsystems for designing a method that automatically adjusts parameters to segment\nmedical images according to the quality expectation of expert users has been\nproposed recently (Evolving fuzzy image segmentation EFIS). However, EFIS\nsuffers from a few limitations when used in practice mainly due to some fixed\nparameters. For instance, EFIS depends on auto-detection of the object of\ninterest for feature calculation, a task that is highly application-dependent.\nThis shortcoming limits the applicability of EFIS, which was proposed with the\nultimate goal of offering a generic but adjustable segmentation scheme. In this\npaper, a new version of EFIS is proposed to overcome these limitations. The new\nEFIS, called self-configuring EFIS (SC-EFIS), uses available training data to\nself-estimate the parameters that are fixed in EFIS. As well, the proposed\nSC-EFIS relies on a feature selection process that does not require\nauto-detection of an ROI. The proposed SC-EFIS was evaluated using the same\nsegmentation algorithms and the same dataset as for EFIS. The results show that\nSC-EFIS can provide the same results as EFIS but with a higher level of\nautomation.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 17:23:09 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Othman", "Ahmed", ""], ["Tizhoosh", "Hamid R.", ""], ["Khalvati", "Farzad", ""]]}, {"id": "1504.06375", "submitter": "Saining Xie", "authors": "Saining Xie and Zhuowen Tu", "title": "Holistically-Nested Edge Detection", "comments": "v2 Add appendix A for updated results (ODS=0.790) on BSDS-500 in a\n  new experiment setting. Fix typos and reorganize formulations. Add Table 2 to\n  discuss the role of deep supervision. Add links to publicly available\n  repository for code, models and data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new edge detection algorithm that tackles two important issues\nin this long-standing vision problem: (1) holistic image training and\nprediction; and (2) multi-scale and multi-level feature learning. Our proposed\nmethod, holistically-nested edge detection (HED), performs image-to-image\nprediction by means of a deep learning model that leverages fully convolutional\nneural networks and deeply-supervised nets. HED automatically learns rich\nhierarchical representations (guided by deep supervision on side responses)\nthat are important in order to approach the human ability resolve the\nchallenging ambiguity in edge and object boundary detection. We significantly\nadvance the state-of-the-art on the BSD500 dataset (ODS F-score of .782) and\nthe NYU Depth dataset (ODS F-score of .746), and do so with an improved speed\n(0.4 second per image) that is orders of magnitude faster than some recent\nCNN-based edge detection algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 02:12:15 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2015 02:15:38 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Xie", "Saining", ""], ["Tu", "Zhuowen", ""]]}, {"id": "1504.06378", "submitter": "James Supancic III", "authors": "James Steven Supancic III, Gregory Rogez, Yi Yang, Jamie Shotton, Deva\n  Ramanan", "title": "Depth-based hand pose estimation: methods, data, and challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand pose estimation has matured rapidly in recent years. The introduction of\ncommodity depth sensors and a multitude of practical applications have spurred\nnew advances. We provide an extensive analysis of the state-of-the-art,\nfocusing on hand pose estimation from a single depth frame. To do so, we have\nimplemented a considerable number of systems, and will release all software and\nevaluation code. We summarize important conclusions here: (1) Pose estimation\nappears roughly solved for scenes with isolated hands. However, methods still\nstruggle to analyze cluttered scenes where hands may be interacting with nearby\nobjects and surfaces. To spur further progress we introduce a challenging new\ndataset with diverse, cluttered scenes. (2) Many methods evaluate themselves\nwith disparate criteria, making comparisons difficult. We define a consistent\nevaluation criteria, rigorously motivated by human experiments. (3) We\nintroduce a simple nearest-neighbor baseline that outperforms most existing\nsystems. This implies that most systems do not generalize beyond their training\nsets. This also reinforces the under-appreciated point that training data is as\nimportant as the model itself. We conclude with directions for future progress.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 02:37:37 GMT"}, {"version": "v2", "created": "Wed, 6 May 2015 20:31:57 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Supancic", "James Steven", "III"], ["Rogez", "Gregory", ""], ["Yang", "Yi", ""], ["Shotton", "Jamie", ""], ["Ramanan", "Deva", ""]]}, {"id": "1504.06434", "submitter": "Jasper Uijlings", "authors": "Jasper Uijlings and Vittorio Ferrari", "title": "Situational Object Boundary Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intuitively, the appearance of true object boundaries varies from image to\nimage. Hence the usual monolithic approach of training a single boundary\npredictor and applying it to all images regardless of their content is bound to\nbe suboptimal. In this paper we therefore propose situational object boundary\ndetection: We first define a variety of situations and train a specialized\nobject boundary detector for each of them using [Dollar and Zitnick 2013]. Then\ngiven a test image, we classify it into these situations using its context,\nwhich we model by global image appearance. We apply the corresponding\nsituational object boundary detectors, and fuse them based on the\nclassification probabilities. In experiments on ImageNet, Microsoft COCO, and\nPascal VOC 2012 segmentation we show that our situational object boundary\ndetection gives significant improvements over a monolithic approach.\nAdditionally, our method substantially outperforms [Hariharan et al. 2011] on\nsemantic contour detection on their SBD dataset.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 09:15:33 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Uijlings", "Jasper", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1504.06507", "submitter": "Michael Baltaxe", "authors": "Michael Baltaxe, Peter Meer, Michael Lindenbaum", "title": "Local Variation as a Statistical Hypothesis Test", "comments": null, "journal-ref": null, "doi": "10.1007/s11263-015-0855-4", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of image oversegmentation is to divide an image into several pieces,\neach of which should ideally be part of an object. One of the simplest and yet\nmost effective oversegmentation algorithms is known as local variation (LV)\n(Felzenszwalb and Huttenlocher 2004). In this work, we study this algorithm and\nshow that algorithms similar to LV can be devised by applying different\nstatistical models and decisions, thus providing further theoretical\njustification and a well-founded explanation for the unexpected high\nperformance of the LV approach. Some of these algorithms are based on\nstatistics of natural images and on a hypothesis testing decision; we denote\nthese algorithms probabilistic local variation (pLV). The best pLV algorithm,\nwhich relies on censored estimation, presents state-of-the-art results while\nkeeping the same computational complexity of the LV algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 13:45:37 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Baltaxe", "Michael", ""], ["Meer", "Peter", ""], ["Lindenbaum", "Michael", ""]]}, {"id": "1504.06567", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Amaia Salvador, Matthias Zeppelzauer, Daniel Manchon-Vizuete, Andrea\n  Calafell and Xavier Giro-i-Nieto", "title": "Cultural Event Recognition with Visual ConvNets and Temporal Models", "comments": "Initial version of the paper accepted at the CVPR Workshop ChaLearn\n  Looking at People 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our contribution to the ChaLearn Challenge 2015 on\nCultural Event Classification. The challenge in this task is to automatically\nclassify images from 50 different cultural events. Our solution is based on the\ncombination of visual features extracted from convolutional neural networks\nwith temporal information using a hierarchical classifier scheme. We extract\nvisual features from the last three fully connected layers of both CaffeNet\n(pretrained with ImageNet) and our fine tuned version for the ChaLearn\nchallenge. We propose a late fusion strategy that trains a separate low-level\nSVM on each of the extracted neural codes. The class predictions of the\nlow-level SVMs form the input to a higher level SVM, which gives the final\nevent scores. We achieve our best result by adding a temporal refinement step\ninto our classification scheme, which is applied directly to the output of each\nlow-level SVM. Our approach penalizes high classification scores based on\nvisual features when their time stamp does not match well an event-specific\ntemporal distribution learned from the training and validation data. Our system\nachieved the second best result in the ChaLearn Challenge 2015 on Cultural\nEvent Classification with a mean average precision of 0.767 on the test set.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 17:00:44 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Salvador", "Amaia", ""], ["Zeppelzauer", "Matthias", ""], ["Manchon-Vizuete", "Daniel", ""], ["Calafell", "Andrea", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1504.06587", "submitter": "Dinesh Reddy Narapureddy", "authors": "N. Dinesh Reddy, Prateek Singhal, K. Madhava Krishna", "title": "Semantic Motion Segmentation Using Dense CRF Formulation", "comments": null, "journal-ref": null, "doi": "10.1145/2683483.2683539", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the literature has been fairly dense in the areas of scene\nunderstanding and semantic labeling there have been few works that make use of\nmotion cues to embellish semantic performance and vice versa. In this paper, we\naddress the problem of semantic motion segmentation, and show how semantic and\nmotion priors augments performance. We pro- pose an algorithm that jointly\ninfers the semantic class and motion labels of an object. Integrating semantic,\ngeometric and optical ow based constraints into a dense CRF-model we infer both\nthe object class as well as motion class, for each pixel. We found improvement\nin performance using a fully connected CRF as compared to a standard\nclique-based CRFs. For inference, we use a Mean Field approximation based\nalgorithm. Our method outperforms recently pro- posed motion detection\nalgorithms and also improves the semantic labeling compared to the\nstate-of-the-art Automatic Labeling Environment algorithm on the challenging\nKITTI dataset especially for object classes such as pedestrians and cars that\nare critical to an outdoor robotic navigation scenario.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 18:06:50 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Reddy", "N. Dinesh", ""], ["Singhal", "Prateek", ""], ["Krishna", "K. Madhava", ""]]}, {"id": "1504.06591", "submitter": "Konda Reddy Mopuri", "authors": "Konda Reddy Mopuri and R. Venkatesh Babu", "title": "Object Level Deep Feature Pooling for Compact Image Representation", "comments": "Deep Vision 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Network (CNN) features have been successfully employed\nin recent works as an image descriptor for various vision tasks. But the\ninability of the deep CNN features to exhibit invariance to geometric\ntransformations and object compositions poses a great challenge for image\nsearch. In this work, we demonstrate the effectiveness of the objectness prior\nover the deep CNN features of image regions for obtaining an invariant image\nrepresentation. The proposed approach represents the image as a vector of\npooled CNN features describing the underlying objects. This representation\nprovides robustness to spatial layout of the objects in the scene and achieves\ninvariance to general geometric transformations, such as translation, rotation\nand scaling. The proposed approach also leads to a compact representation of\nthe scene, making each image occupy a smaller memory footprint. Experiments\nshow that the proposed representation achieves state of the art retrieval\nresults on a set of challenging benchmark image datasets, while maintaining a\ncompact representation.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 18:27:25 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Mopuri", "Konda Reddy", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1504.06603", "submitter": "Dmytro Mishkin", "authors": "Dmytro Mishkin and Jiri Matas and Michal Perdoch and Karel Lenc", "title": "WxBS: Wide Baseline Stereo Generalizations", "comments": "Descriptor and detector evaluation expanded", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have presented a new problem -- the wide multiple baseline stereo (WxBS)\n-- which considers matching of images that simultaneously differ in more than\none image acquisition factor such as viewpoint, illumination, sensor type or\nwhere object appearance changes significantly, e.g. over time. A new dataset\nwith the ground truth for evaluation of matching algorithms has been introduced\nand will be made public.\n  We have extensively tested a large set of popular and recent detectors and\ndescriptors and show than the combination of RootSIFT and HalfRootSIFT as\ndescriptors with MSER and Hessian-Affine detectors works best for many\ndifferent nuisance factors. We show that simple adaptive thresholding improves\nHessian-Affine, DoG, MSER (and possibly other) detectors and allows to use them\non infrared and low contrast images.\n  A novel matching algorithm for addressing the WxBS problem has been\nintroduced. We have shown experimentally that the WxBS-M matcher dominantes the\nstate-of-the-art methods both on both the new and existing datasets.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 19:19:04 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 14:42:53 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Mishkin", "Dmytro", ""], ["Matas", "Jiri", ""], ["Perdoch", "Michal", ""], ["Lenc", "Karel", ""]]}, {"id": "1504.06678", "submitter": "Guo-Jun Qi", "authors": "Vivek Veeriah and Naifan Zhuang and Guo-Jun Qi", "title": "Differential Recurrent Neural Networks for Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The long short-term memory (LSTM) neural network is capable of processing\ncomplex sequential information since it utilizes special gating schemes for\nlearning representations from long input sequences. It has the potential to\nmodel any sequential time-series data, where the current hidden state has to be\nconsidered in the context of the past hidden states. This property makes LSTM\nan ideal choice to learn the complex dynamics of various actions.\nUnfortunately, the conventional LSTMs do not consider the impact of\nspatio-temporal dynamics corresponding to the given salient motion patterns,\nwhen they gate the information that ought to be memorized through time. To\naddress this problem, we propose a differential gating scheme for the LSTM\nneural network, which emphasizes on the change in information gain caused by\nthe salient motions between the successive frames. This change in information\ngain is quantified by Derivative of States (DoS), and thus the proposed LSTM\nmodel is termed as differential Recurrent Neural Network (dRNN). We demonstrate\nthe effectiveness of the proposed model by automatically recognizing actions\nfrom the real-world 2D and 3D human action datasets. Our study is one of the\nfirst works towards demonstrating the potential of learning complex time-series\nrepresentations via high-order derivatives of states.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2015 03:59:14 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Veeriah", "Vivek", ""], ["Zhuang", "Naifan", ""], ["Qi", "Guo-Jun", ""]]}, {"id": "1504.06692", "submitter": "Junhua Mao", "authors": "Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, Alan Yuille", "title": "Learning like a Child: Fast Novel Visual Concept Learning from Sentence\n  Descriptions of Images", "comments": "ICCV 2015 camera ready version. We add much more novel visual\n  concepts in the NVC dataset and have released it, see\n  http://www.stat.ucla.edu/~junhua.mao/projects/child_learning.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the task of learning novel visual concepts, and\ntheir interactions with other concepts, from a few images with sentence\ndescriptions. Using linguistic context and visual features, our method is able\nto efficiently hypothesize the semantic meaning of new words and add them to\nits word dictionary so that they can be used to describe images which contain\nthese novel concepts. Our method has an image captioning module based on m-RNN\nwith several improvements. In particular, we propose a transposed weight\nsharing scheme, which not only improves performance on image captioning, but\nalso makes the model more suitable for the novel concept learning task. We\npropose methods to prevent overfitting the new concepts. In addition, three\nnovel concept datasets are constructed for this new task. In the experiments,\nwe show that our method effectively learns novel visual concepts from a few\nexamples without disturbing the previously learned concepts. The project page\nis http://www.stat.ucla.edu/~junhua.mao/projects/child_learning.html\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2015 06:45:35 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2015 02:36:05 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Mao", "Junhua", ""], ["Xu", "Wei", ""], ["Yang", "Yi", ""], ["Wang", "Jiang", ""], ["Huang", "Zhiheng", ""], ["Yuille", "Alan", ""]]}, {"id": "1504.06719", "submitter": "Anurag Mittal", "authors": "Smit Marvaniya, Raj Gupta and Anurag Mittal", "title": "Adaptive Locally Affine-Invariant Shape Matching", "comments": "submitted to Image and Vision Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching deformable objects using their shapes is an important problem in\ncomputer vision since shape is perhaps the most distinguishable characteristic\nof an object. The problem is difficult due to many factors such as intra-class\nvariations, local deformations, articulations, viewpoint changes and missed and\nextraneous contour portions due to errors in shape extraction. While small\nlocal deformations has been handled in the literature by allowing some leeway\nin the matching of individual contour points via methods such as Chamfer\ndistance and Hausdorff distance, handling more severe deformations and\narticulations has been done by applying local geometric corrections such as\nsimilarity or affine. However, determining which portions of the shape should\nbe used for the geometric corrections is very hard, although some methods have\nbeen tried. In this paper, we address this problem by an efficient search for\nthe group of contour segments to be clustered together for a geometric\ncorrection using Dynamic Programming by essentially searching for the\nsegmentations of two shapes that lead to the best matching between them. At the\nsame time, we allow portions of the contours to remain unmatched to handle\nmissing and extraneous contour portions. Experiments indicate that our method\noutperforms other algorithms, especially when the shapes to be matched are more\ncomplex.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2015 12:01:30 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Marvaniya", "Smit", ""], ["Gupta", "Raj", ""], ["Mittal", "Anurag", ""]]}, {"id": "1504.06740", "submitter": "Siddharth Srivastava", "authors": "Siddharth Srivastava", "title": "SIFT Vs SURF: Quantifying the Variation in Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": "v01.0", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the robustness of SIFT and SURF against different image\ntransforms (rigid body, similarity, affine and projective) by quantitatively\nanalyzing the variations in the extent of transformations. Previous studies\nhave been comparing the two techniques on absolute transformations rather than\nthe specific amount of deformation caused by the transformation. The paper\nestablishes an exhaustive empirical analysis of such deformations and matching\ncapability of SIFT and SURF with variations in matching parameters and the\namount of tolerance. This is helpful in choosing the specific use case for\napplying these techniques.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2015 15:26:38 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Srivastava", "Siddharth", ""]]}, {"id": "1504.06755", "submitter": "Pingmei Xu", "authors": "Pingmei Xu, Krista A Ehinger, Yinda Zhang, Adam Finkelstein, Sanjeev\n  R. Kulkarni, Jianxiong Xiao", "title": "TurkerGaze: Crowdsourcing Saliency with Webcam based Eye Tracking", "comments": "9 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional eye tracking requires specialized hardware, which means\ncollecting gaze data from many observers is expensive, tedious and slow.\nTherefore, existing saliency prediction datasets are order-of-magnitudes\nsmaller than typical datasets for other vision recognition tasks. The small\nsize of these datasets limits the potential for training data intensive\nalgorithms, and causes overfitting in benchmark evaluation. To address this\ndeficiency, this paper introduces a webcam-based gaze tracking system that\nsupports large-scale, crowdsourced eye tracking deployed on Amazon Mechanical\nTurk (AMTurk). By a combination of careful algorithm and gaming protocol\ndesign, our system obtains eye tracking data for saliency prediction comparable\nto data gathered in a traditional lab setting, with relatively lower cost and\nless effort on the part of the researchers. Using this tool, we build a\nsaliency dataset for a large number of natural images. We will open-source our\ntool and provide a web server where researchers can upload their images to get\neye tracking results from AMTurk.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2015 19:26:47 GMT"}, {"version": "v2", "created": "Wed, 20 May 2015 18:51:23 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Xu", "Pingmei", ""], ["Ehinger", "Krista A", ""], ["Zhang", "Yinda", ""], ["Finkelstein", "Adam", ""], ["Kulkarni", "Sanjeev R.", ""], ["Xiao", "Jianxiong", ""]]}, {"id": "1504.06779", "submitter": "Emerson Machado", "authors": "Emerson Lopes Machado, Cristiano Jacques Miosso, Ricardo von Borries,\n  Murilo Coutinho, Pedro de Azevedo Berger, Thiago Marques, Ricardo Pezzuol\n  Jacobi", "title": "Computational Cost Reduction in Learned Transform Classifications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a theoretical analysis and empirical evaluations of a novel set of\ntechniques for computational cost reduction of classifiers that are based on\nlearned transform and soft-threshold. By modifying optimization procedures for\ndictionary and classifier training, as well as the resulting dictionary\nentries, our techniques allow to reduce the bit precision and to replace each\nfloating-point multiplication by a single integer bit shift. We also show how\nthe optimization algorithms in some dictionary training methods can be modified\nto penalize higher-energy dictionaries. We applied our techniques with the\nclassifier Learning Algorithm for Soft-Thresholding, testing on the datasets\nused in its original paper. Our results indicate it is feasible to use solely\nsums and bit shifts of integers to classify at test time with a limited\nreduction of the classification accuracy. These low power operations are a\nvaluable trade off in FPGA implementations as they increase the classification\nthroughput while decrease both energy consumption and manufacturing cost.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 01:16:44 GMT"}, {"version": "v2", "created": "Sat, 30 Apr 2016 15:03:29 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Machado", "Emerson Lopes", ""], ["Miosso", "Cristiano Jacques", ""], ["von Borries", "Ricardo", ""], ["Coutinho", "Murilo", ""], ["Berger", "Pedro de Azevedo", ""], ["Marques", "Thiago", ""], ["Jacobi", "Ricardo Pezzuol", ""]]}, {"id": "1504.06785", "submitter": "Ju Sun", "authors": "Ju Sun, Qing Qu, John Wright", "title": "Complete Dictionary Recovery over the Sphere", "comments": "104 pages, 5 figures. Due to length constraint of publication, this\n  long paper are subsequently divided into two papers (arXiv:1511.03607 and\n  arXiv:1511.04777). Further updates will be made only to the two papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.LG math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering a complete (i.e., square and\ninvertible) matrix $\\mathbf A_0$, from $\\mathbf Y \\in \\mathbb R^{n \\times p}$\nwith $\\mathbf Y = \\mathbf A_0 \\mathbf X_0$, provided $\\mathbf X_0$ is\nsufficiently sparse. This recovery problem is central to the theoretical\nunderstanding of dictionary learning, which seeks a sparse representation for a\ncollection of input signals, and finds numerous applications in modern signal\nprocessing and machine learning. We give the first efficient algorithm that\nprovably recovers $\\mathbf A_0$ when $\\mathbf X_0$ has $O(n)$ nonzeros per\ncolumn, under suitable probability model for $\\mathbf X_0$. In contrast, prior\nresults based on efficient algorithms provide recovery guarantees when $\\mathbf\nX_0$ has only $O(n^{1-\\delta})$ nonzeros per column for any constant $\\delta\n\\in (0, 1)$.\n  Our algorithmic pipeline centers around solving a certain nonconvex\noptimization problem with a spherical constraint, and hence is naturally\nphrased in the language of manifold optimization. To show this apparently hard\nproblem is tractable, we first provide a geometric characterization of the\nhigh-dimensional objective landscape, which shows that with high probability\nthere are no \"spurious\" local minima. This particular geometric structure\nallows us to design a Riemannian trust region algorithm over the sphere that\nprovably converges to one local minimizer with an arbitrary initialization,\ndespite the presence of saddle points. The geometric approach we develop here\nmay also shed light on other problems arising from nonconvex recovery of\nstructured signals.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 04:57:19 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2015 06:06:04 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2015 21:56:30 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Sun", "Ju", ""], ["Qu", "Qing", ""], ["Wright", "John", ""]]}, {"id": "1504.06786", "submitter": "Hossein Ziaei Nafchi", "authors": "Hossein Ziaei Nafchi, Rachid Hedjam, Atena Shahkolaei, Mohamed Cheriet", "title": "Deviation Based Pooling Strategies For Full Reference Image Quality\n  Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art pooling strategies for perceptual image quality\nassessment (IQA) are based on the mean and the weighted mean. They are robust\npooling strategies which usually provide a moderate to high performance for\ndifferent IQAs. Recently, standard deviation (SD) pooling was also proposed.\nAlthough, this deviation pooling provides a very high performance for a few\nIQAs, its performance is lower than mean poolings for many other IQAs. In this\npaper, we propose to use the mean absolute deviation (MAD) and show that it is\na more robust and accurate pooling strategy for a wider range of IQAs. In fact,\nMAD pooling has the advantages of both mean pooling and SD pooling. The joint\ncomputation and use of the MAD and SD pooling strategies is also considered in\nthis paper. Experimental results provide useful information on the choice of\nthe proper deviation pooling strategy for different IQA models.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 05:06:33 GMT"}, {"version": "v2", "created": "Wed, 6 May 2015 19:37:36 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Nafchi", "Hossein Ziaei", ""], ["Hedjam", "Rachid", ""], ["Shahkolaei", "Atena", ""], ["Cheriet", "Mohamed", ""]]}, {"id": "1504.06787", "submitter": "Chongxuan Li", "authors": "Chongxuan Li and Jun Zhu and Tianlin Shi and Bo Zhang", "title": "Max-margin Deep Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models (DGMs) are effective on learning multilayered\nrepresentations of complex data and performing inference of input data by\nexploring the generative ability. However, little work has been done on\nexamining or empowering the discriminative ability of DGMs on making accurate\npredictions. This paper presents max-margin deep generative models (mmDGMs),\nwhich explore the strongly discriminative principle of max-margin learning to\nimprove the discriminative power of DGMs, while retaining the generative\ncapability. We develop an efficient doubly stochastic subgradient algorithm for\nthe piecewise linear objective. Empirical results on MNIST and SVHN datasets\ndemonstrate that (1) max-margin learning can significantly improve the\nprediction performance of DGMs and meanwhile retain the generative ability; and\n(2) mmDGMs are competitive to the state-of-the-art fully discriminative\nnetworks by employing deep convolutional neural networks (CNNs) as both\nrecognition and generative models.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 06:01:19 GMT"}, {"version": "v2", "created": "Fri, 1 May 2015 01:58:31 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2015 08:40:09 GMT"}, {"version": "v4", "created": "Tue, 15 Dec 2015 03:01:06 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Li", "Chongxuan", ""], ["Zhu", "Jun", ""], ["Shi", "Tianlin", ""], ["Zhang", "Bo", ""]]}, {"id": "1504.06852", "submitter": "Philipp Fischer", "authors": "Philipp Fischer, Alexey Dosovitskiy, Eddy Ilg, Philip H\\\"ausser, Caner\n  Haz{\\i}rba\\c{s}, Vladimir Golkov, Patrick van der Smagt, Daniel Cremers,\n  Thomas Brox", "title": "FlowNet: Learning Optical Flow with Convolutional Networks", "comments": "Added supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have recently been very successful in a\nvariety of computer vision tasks, especially on those linked to recognition.\nOptical flow estimation has not been among the tasks where CNNs were\nsuccessful. In this paper we construct appropriate CNNs which are capable of\nsolving the optical flow estimation problem as a supervised learning task. We\npropose and compare two architectures: a generic architecture and another one\nincluding a layer that correlates feature vectors at different image locations.\n  Since existing ground truth data sets are not sufficiently large to train a\nCNN, we generate a synthetic Flying Chairs dataset. We show that networks\ntrained on this unrealistic data still generalize very well to existing\ndatasets such as Sintel and KITTI, achieving competitive accuracy at frame\nrates of 5 to 10 fps.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 17:30:32 GMT"}, {"version": "v2", "created": "Mon, 4 May 2015 08:50:57 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Fischer", "Philipp", ""], ["Dosovitskiy", "Alexey", ""], ["Ilg", "Eddy", ""], ["H\u00e4usser", "Philip", ""], ["Haz\u0131rba\u015f", "Caner", ""], ["Golkov", "Vladimir", ""], ["van der Smagt", "Patrick", ""], ["Cremers", "Daniel", ""], ["Brox", "Thomas", ""]]}, {"id": "1504.06864", "submitter": "Rafal Scherer", "authors": "Patryk Najgebauer, Janusz Rygal, Tomasz Nowak, Jakub Romanowski,\n  Leszek Rutkowski, Sviatoslav Voloshynovskiy, Rafal Scherer", "title": "Fast Dictionary Matching for Content-based Image Retrieval", "comments": "Accepted for the 14th International Conference on Artificial\n  Intelligence and Soft Computing, ICAISC, June 14-18, 2015, Zakopane, Poland,\n  http://www.icaisc.eu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a method for searching for common sets of descriptors\nbetween collections of images. The presented method operates on local interest\nkeypoints, which are generated using the SURF algorithm. The use of a\ndictionary of descriptors allowed achieving good performance of the\ncontent-based image retrieval. The method can be used to initially determine a\nset of similar pairs of keypoints between images. For this purpose, we use a\ncertain level of tolerance between values of descriptors, as values of feature\ndescriptors are almost never equal but similar between different images. After\nthat, the method compares the structure of rotation and location of interest\npoints in one image with the point structure in other images. Thus, we were\nable to find similar areas in images and determine the level of similarity\nbetween them, even when images contain different scenes.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 18:41:51 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Najgebauer", "Patryk", ""], ["Rygal", "Janusz", ""], ["Nowak", "Tomasz", ""], ["Romanowski", "Jakub", ""], ["Rutkowski", "Leszek", ""], ["Voloshynovskiy", "Sviatoslav", ""], ["Scherer", "Rafal", ""]]}, {"id": "1504.06897", "submitter": "Yilun Wang", "authors": "Chengqiang Bao and Liangtian He and Yilun Wang", "title": "Linear Spatial Pyramid Matching Using Non-convex and non-negative Sparse\n  Coding for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently sparse coding have been highly successful in image classification\nmainly due to its capability of incorporating the sparsity of image\nrepresentation. In this paper, we propose an improved sparse coding model based\non linear spatial pyramid matching(SPM) and Scale Invariant Feature Transform\n(SIFT ) descriptors. The novelty is the simultaneous non-convex and\nnon-negative characters added to the sparse coding model. Our numerical\nexperiments show that the improved approach using non-convex and non-negative\nsparse coding is superior than the original ScSPM[1] on several typical\ndatabases.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 00:46:54 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Bao", "Chengqiang", ""], ["He", "Liangtian", ""], ["Wang", "Yilun", ""]]}, {"id": "1504.06921", "submitter": "Hooi Sin Ng", "authors": "Hooi Sin Ng, Yong Haur Tay, Kim Meng Liang, Hamam Mokayed and Hock\n  Woon Hon", "title": "Detection and Recognition of Malaysian Special License Plate Based On\n  SIFT Features", "comments": "seven pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated car license plate recognition systems are developed and applied for\npurpose of facilitating the surveillance, law enforcement, access control and\nintelligent transportation monitoring with least human intervention. In this\npaper, an algorithm based on SIFT feature points clustering and matching is\nproposed to address the issue of recognizing Malaysian special plates. These\nspecial plates do not follow the format of standard car plates as they may\ncontain italic, cursive, connected and small letters. The algorithm is tested\nwith 150 Malaysian special plate images under different environment and the\npromising experimental results demonstrate that the proposed algorithm is\nrelatively robust.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 03:49:33 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Ng", "Hooi Sin", ""], ["Tay", "Yong Haur", ""], ["Liang", "Kim Meng", ""], ["Mokayed", "Hamam", ""], ["Hon", "Hock Woon", ""]]}, {"id": "1504.06993", "submitter": "Chao Dong", "authors": "Chao Dong and Yubin Deng and Chen Change Loy and Xiaoou Tang", "title": "Compression Artifacts Reduction by a Deep Convolutional Network", "comments": "9 pages, 12 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lossy compression introduces complex compression artifacts, particularly the\nblocking artifacts, ringing effects and blurring. Existing algorithms either\nfocus on removing blocking artifacts and produce blurred output, or restores\nsharpened images that are accompanied with ringing effects. Inspired by the\ndeep convolutional networks (DCN) on super-resolution, we formulate a compact\nand efficient network for seamless attenuation of different compression\nartifacts. We also demonstrate that a deeper model can be effectively trained\nwith the features learned in a shallow network. Following a similar \"easy to\nhard\" idea, we systematically investigate several practical transfer settings\nand show the effectiveness of transfer learning in low-level vision problems.\nOur method shows superior performance than the state-of-the-arts both on the\nbenchmark datasets and the real-world use case (i.e. Twitter). In addition, we\nshow that our method can be applied as pre-processing to facilitate other\nlow-level vision routines when they take compressed images as input.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 09:30:30 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Dong", "Chao", ""], ["Deng", "Yubin", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1504.07021", "submitter": "Shoaib Ehsan", "authors": "Shoaib Ehsan and Klaus D. McDonald-Maier", "title": "On-Board Vision Processing For Small UAVs: Time to Rethink Strategy", "comments": "2009 NASA/ESA Conference on Adaptive Hardware and Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ultimate research goal for unmanned aerial vehicles (UAVs) is to\nfacilitate autonomy of operation. Research in the last decade has highlighted\nthe potential of vision sensing in this regard. Although vital for\naccomplishment of missions assigned to any type of unmanned aerial vehicles,\nvision sensing is more critical for small aerial vehicles due to lack of high\nprecision inertial sensors. In addition, uncertainty of GPS signal in indoor\nand urban environments calls for more reliance on vision sensing for such small\nvehicles. With off-line processing does not offer an attractive option in terms\nof autonomy, these vehicles have been challenging platforms to implement vision\nprocessing onboard due to their strict payload capacity and power budget. The\nstrict constraints drive the need for new vision processing architectures for\nsmall unmanned aerial vehicles. Recent research has shown encouraging results\nwith FPGA based hardware architectures. This paper reviews the bottle necks\ninvolved in implementing vision processing on-board, advocates the potential of\nhardware based solutions to tackle strict constraints of small unmanned aerial\nvehicles and finally analyzes feasibility of ASICs, Structured ASICs and FPGAs\nfor use on future systems.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 10:52:27 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Ehsan", "Shoaib", ""], ["McDonald-Maier", "Klaus D.", ""]]}, {"id": "1504.07028", "submitter": "Filipe Condessa", "authors": "Filipe Condessa, Jose Bioucas-Dias, Jelena Kovacevic", "title": "SegSALSA-STR: A convex formulation to supervised hyperspectral image\n  segmentation using hidden fields and structure tensor regularization", "comments": "This paper was submitted to IEEE WHISPERS 2015: 7th Workshop on\n  Hyperspectral Image and Signal Processing: Evolution on Remote Sensing. 5\n  pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a supervised hyperspectral image segmentation algorithm based on a\nconvex formulation of a marginal maximum a posteriori segmentation with hidden\nfields and structure tensor regularization: Segmentation via the Constraint\nSplit Augmented Lagrangian Shrinkage by Structure Tensor Regularization\n(SegSALSA-STR). This formulation avoids the generally discrete nature of\nsegmentation problems and the inherent NP-hardness of the integer optimization\nassociated.\n  We extend the Segmentation via the Constraint Split Augmented Lagrangian\nShrinkage (SegSALSA) algorithm by generalizing the vectorial total variation\nprior using a structure tensor prior constructed from a patch-based Jacobian.\nThe resulting algorithm is convex, time-efficient and highly parallelizable.\nThis shows the potential of combining hidden fields with convex optimization\nthrough the inclusion of different regularizers. The SegSALSA-STR algorithm is\nvalidated in the segmentation of real hyperspectral images.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 11:08:53 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Condessa", "Filipe", ""], ["Bioucas-Dias", "Jose", ""], ["Kovacevic", "Jelena", ""]]}, {"id": "1504.07029", "submitter": "David Novotn\\'y", "authors": "David Novotny, Jiri Matas", "title": "Cascaded Sparse Spatial Bins for Efficient and Effective Generic Object\n  Detection", "comments": "Accepted to ICCV15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel efficient method for extraction of object proposals is introduced.\nIts \"objectness\" function exploits deep spatial pyramid features, a novel\nfast-to-compute HoG-based edge statistic and the EdgeBoxes score. The\nefficiency is achieved by the use of spatial bins in a novel combination with\nsparsity-inducing group normalized SVM. State-of-the-art recall performance is\nachieved on Pascal VOC07, significantly outperforming methods with comparable\nspeed. Interestingly, when only 100 proposals per image are considered the\nmethod attains 78% recall on VOC07. The method improves mAP of the RCNN\nstate-of-the-art class-specific detector, increasing it by 10 points when only\n50 proposals are used in each image. The system trained on twenty classes\nperforms well on the two hundred class ILSVRC2013 set confirming generalization\ncapability.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 11:14:27 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2015 10:30:22 GMT"}], "update_date": "2015-10-14", "authors_parsed": [["Novotny", "David", ""], ["Matas", "Jiri", ""]]}, {"id": "1504.07082", "submitter": "Bharathi Pilar", "authors": "B.H.Shekar, Bharathi Pilar", "title": "Shape Representation and Classification through Pattern Spectrum and\n  Local Binary Pattern - A Decision Level Fusion Approach", "comments": "Fifth International Conference on Signals and Image Processing\n  (ICSIP) 2014", "journal-ref": null, "doi": "10.1109/ICSIP.2014.41", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a decision level fused local Morphological Pattern\nSpectrum(PS) and Local Binary Pattern (LBP) approach for an efficient shape\nrepresentation and classification. This method makes use of Earth Movers\nDistance(EMD) as the measure in feature matching and shape retrieval process.\nThe proposed approach has three major phases : Feature Extraction, Construction\nof hybrid spectrum knowledge base and Classification. In the first phase,\nfeature extraction of the shape is done using pattern spectrum and local binary\npattern method. In the second phase, the histograms of both pattern spectrum\nand local binary pattern are fused and stored in the knowledge base. In the\nthird phase, the comparison and matching of the features, which are represented\nin the form of histograms, is done using Earth Movers Distance(EMD) as metric.\nThe top-n shapes are retrieved for each query shape. The accuracy is tested by\nmeans of standard Bulls eye score method. The experiments are conducted on\npublicly available shape datasets like Kimia-99, Kimia-216 and MPEG-7. The\ncomparative study is also provided with the well known approaches to exhibit\nthe retrieval accuracy of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 13:38:20 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Shekar", "B. H.", ""], ["Pilar", "Bharathi", ""]]}, {"id": "1504.07116", "submitter": "Kevin Moon", "authors": "Kevin R. Moon, Veronique Delouille, Alfred O. Hero III", "title": "Meta learning of bounds on the Bayes classifier error", "comments": "6 pages, 3 figures, to appear in proceedings of 2015 IEEE Signal\n  Processing and SP Education Workshop", "journal-ref": "IEEE Signal Processing and SP Education Workshop, pp. 13-18, Aug.\n  2015", "doi": "10.1109/DSP-SPE.2015.7369520", "report-no": null, "categories": "cs.LG astro-ph.SR cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta learning uses information from base learners (e.g. classifiers or\nestimators) as well as information about the learning problem to improve upon\nthe performance of a single base learner. For example, the Bayes error rate of\na given feature space, if known, can be used to aid in choosing a classifier,\nas well as in feature selection and model selection for the base classifiers\nand the meta classifier. Recent work in the field of f-divergence functional\nestimation has led to the development of simple and rapidly converging\nestimators that can be used to estimate various bounds on the Bayes error. We\nestimate multiple bounds on the Bayes error using an estimator that applies\nmeta learning to slowly converging plug-in estimators to obtain the parametric\nconvergence rate. We compare the estimated bounds empirically on simulated data\nand then estimate the tighter bounds on features extracted from an image patch\nanalysis of sunspot continuum and magnetogram images.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 14:49:24 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2015 17:34:13 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Moon", "Kevin R.", ""], ["Delouille", "Veronique", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1504.07159", "submitter": "Xiaochuan Fan", "authors": "Xiaochuan Fan, Kang Zheng, Yuewei Lin, Song Wang", "title": "Combining Local Appearance and Holistic View: Dual-Source Deep Neural\n  Networks for Human Pose Estimation", "comments": "CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new learning-based method for estimating 2D human pose from a\nsingle image, using Dual-Source Deep Convolutional Neural Networks (DS-CNN).\nRecently, many methods have been developed to estimate human pose by using pose\npriors that are estimated from physiologically inspired graphical models or\nlearned from a holistic perspective. In this paper, we propose to integrate\nboth the local (body) part appearance and the holistic view of each local part\nfor more accurate human pose estimation. Specifically, the proposed DS-CNN\ntakes a set of image patches (category-independent object proposals for\ntraining and multi-scale sliding windows for testing) as the input and then\nlearns the appearance of each local part by considering their holistic views in\nthe full body. Using DS-CNN, we achieve both joint detection, which determines\nwhether an image patch contains a body joint, and joint localization, which\nfinds the exact location of the joint in the image patch. Finally, we develop\nan algorithm to combine these joint detection/localization results from all the\nimage patches for estimating the human pose. The experimental results show the\neffectiveness of the proposed method by comparing to the state-of-the-art\nhuman-pose estimation methods based on pose priors that are estimated from\nphysiologically inspired graphical models or learned from a holistic\nperspective.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 17:00:30 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Fan", "Xiaochuan", ""], ["Zheng", "Kang", ""], ["Lin", "Yuewei", ""], ["Wang", "Song", ""]]}, {"id": "1504.07259", "submitter": "Heike Benninghoff", "authors": "Heike Benninghoff and Harald Garcke", "title": "Image Segmentation and Restoration Using Parametric Contours With Free\n  Endpoints", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2016.2529180", "report-no": null, "categories": "cs.CV math.AP math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel approach for active contours with free\nendpoints. A scheme is presented for image segmentation and restoration based\non a discrete version of the Mumford-Shah functional where the contours can be\nboth closed and open curves. Additional to a flow of the curves in normal\ndirection, evolution laws for the tangential flow of the endpoints are derived.\nUsing a parametric approach to describe the evolving contours together with an\nedge-preserving denoising, we obtain a fast method for image segmentation and\nrestoration. The analytical and numerical schemes are presented followed by\nnumerical experiments with artificial test images and with a real medical\nimage.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 20:08:37 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Benninghoff", "Heike", ""], ["Garcke", "Harald", ""]]}, {"id": "1504.07269", "submitter": "Narapureddy Dinesh Reddy", "authors": "N. Dinesh Reddy, Prateek Singhal, Visesh Chari and K. Madhava Krishna", "title": "Dynamic Body VSLAM with Semantic Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image based reconstruction of urban environments is a challenging problem\nthat deals with optimization of large number of variables, and has several\nsources of errors like the presence of dynamic objects. Since most large scale\napproaches make the assumption of observing static scenes, dynamic objects are\nrelegated to the noise modeling section of such systems. This is an approach of\nconvenience since the RANSAC based framework used to compute most multiview\ngeometric quantities for static scenes naturally confine dynamic objects to the\nclass of outlier measurements. However, reconstructing dynamic objects along\nwith the static environment helps us get a complete picture of an urban\nenvironment. Such understanding can then be used for important robotic tasks\nlike path planning for autonomous navigation, obstacle tracking and avoidance,\nand other areas. In this paper, we propose a system for robust SLAM that works\nin both static and dynamic environments. To overcome the challenge of dynamic\nobjects in the scene, we propose a new model to incorporate semantic\nconstraints into the reconstruction algorithm. While some of these constraints\nare based on multi-layered dense CRFs trained over appearance as well as motion\ncues, other proposed constraints can be expressed as additional terms in the\nbundle adjustment optimization process that does iterative refinement of 3D\nstructure and camera / object motion trajectories. We show results on the\nchallenging KITTI urban dataset for accuracy of motion segmentation and\nreconstruction of the trajectory and shape of moving objects relative to ground\ntruth. We are able to show average relative error reduction by a significant\namount for moving object trajectory reconstruction relative to state-of-the-art\nmethods like VISO 2, as well as standard bundle adjustment algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 20:30:04 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Reddy", "N. Dinesh", ""], ["Singhal", "Prateek", ""], ["Chari", "Visesh", ""], ["Krishna", "K. Madhava", ""]]}, {"id": "1504.07284", "submitter": "Aayush Bansal", "authors": "Aayush Bansal, Abhinav Shrivastava, Carl Doersch, Abhinav Gupta", "title": "Mid-level Elements for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building on the success of recent discriminative mid-level elements, we\npropose a surprisingly simple approach for object detection which performs\ncomparable to the current state-of-the-art approaches on PASCAL VOC comp-3\ndetection challenge (no external data). Through extensive experiments and\nablation analysis, we show how our approach effectively improves upon the\nHOG-based pipelines by adding an intermediate mid-level representation for the\ntask of object detection. This representation is easily interpretable and\nallows us to visualize what our object detector \"sees\". We also discuss the\ninsights our approach shares with CNN-based methods, such as sharing\nrepresentation between categories helps.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 21:41:01 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Bansal", "Aayush", ""], ["Shrivastava", "Abhinav", ""], ["Doersch", "Carl", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1504.07339", "submitter": "Bin Yang", "authors": "Bin Yang, Junjie Yan, Zhen Lei, Stan Z. Li", "title": "Convolutional Channel Features", "comments": "9 pages, 5 figures, 6 tables; ICCV 2015 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning methods are powerful tools but often suffer from expensive\ncomputation and limited flexibility. An alternative is to combine light-weight\nmodels with deep representations. As successful cases exist in several visual\nproblems, a unified framework is absent. In this paper, we revisit two widely\nused approaches in computer vision, namely filtered channel features and\nConvolutional Neural Networks (CNN), and absorb merits from both by proposing\nan integrated method called Convolutional Channel Features (CCF). CCF transfers\nlow-level features from pre-trained CNN models to feed the boosting forest\nmodel. With the combination of CNN features and boosting forest, CCF benefits\nfrom the richer capacity in feature representation compared with channel\nfeatures, as well as lower cost in computation and storage compared with\nend-to-end CNN methods. We show that CCF serves as a good way of tailoring\npre-trained CNN models to diverse tasks without fine-tuning the whole network\nto each task by achieving state-of-the-art performances in pedestrian\ndetection, face detection, edge detection and object proposal generation.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 03:44:39 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2015 16:57:07 GMT"}, {"version": "v3", "created": "Thu, 24 Sep 2015 17:22:41 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Yang", "Bin", ""], ["Yan", "Junjie", ""], ["Lei", "Zhen", ""], ["Li", "Stan Z.", ""]]}, {"id": "1504.07442", "submitter": "Gorka Velez Ph.D.", "authors": "Gorka Velez and Oihana Otaegui", "title": "Embedded Platforms for Computer Vision-based Advanced Driver Assistance\n  Systems: a Survey", "comments": "10 pages. To be published in ITS World Congress 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer Vision, either alone or combined with other technologies such as\nradar or Lidar, is one of the key technologies used in Advanced Driver\nAssistance Systems (ADAS). Its role understanding and analysing the driving\nscene is of great importance as it can be noted by the number of ADAS\napplications that use this technology. However, porting a vision algorithm to\nan embedded automotive system is still very challenging, as there must be a\ntrade-off between several design requisites. Furthermore, there is not a\nstandard implementation platform, so different alternatives have been proposed\nby both the scientific community and the industry. This paper aims to review\nthe requisites and the different embedded implementation platforms that can be\nused for Computer Vision-based ADAS, with a critical analysis and an outlook to\nfuture trends.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 12:19:45 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Velez", "Gorka", ""], ["Otaegui", "Oihana", ""]]}, {"id": "1504.07460", "submitter": "Alexander Kolesnikov", "authors": "Alexander Kolesnikov and Christoph H. Lampert", "title": "Identifying Reliable Annotations for Large Scale Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Challenging computer vision tasks, in particular semantic image segmentation,\nrequire large training sets of annotated images. While obtaining the actual\nimages is often unproblematic, creating the necessary annotation is a tedious\nand costly process. Therefore, one often has to work with unreliable annotation\nsources, such as Amazon Mechanical Turk or (semi-)automatic algorithmic\ntechniques. In this work, we present a Gaussian process (GP) based technique\nfor simultaneously identifying which images of a training set have unreliable\nannotation and learning a segmentation model in which the negative effect of\nthese images is suppressed. Alternatively, the model can also just be used to\nidentify the most reliably annotated images from the training set, which can\nthen be used for training any other segmentation method. By relying on \"deep\nfeatures\" in combination with a linear covariance function, our GP can be\nlearned and its hyperparameter determined efficiently using only matrix\noperations and gradient-based optimization. This makes our method scalable even\nto large datasets with several million training instances.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 13:19:21 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Kolesnikov", "Alexander", ""], ["Lampert", "Christoph H.", ""]]}, {"id": "1504.07469", "submitter": "Ariel Ephrat", "authors": "Yair Poleg, Ariel Ephrat, Shmuel Peleg, Chetan Arora", "title": "Compact CNN for Indexing Egocentric Videos", "comments": null, "journal-ref": "IEEE WACV'16, March 2016, pp. 1-9", "doi": "10.1109/WACV.2016.7477708", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While egocentric video is becoming increasingly popular, browsing it is very\ndifficult. In this paper we present a compact 3D Convolutional Neural Network\n(CNN) architecture for long-term activity recognition in egocentric videos.\nRecognizing long-term activities enables us to temporally segment (index) long\nand unstructured egocentric videos. Existing methods for this task are based on\nhand tuned features derived from visible objects, location of hands, as well as\noptical flow.\n  Given a sparse optical flow volume as input, our CNN classifies the camera\nwearer's activity. We obtain classification accuracy of 89%, which outperforms\nthe current state-of-the-art by 19%. Additional evaluation is performed on an\nextended egocentric video dataset, classifying twice the amount of categories\nthan current state-of-the-art. Furthermore, our CNN is able to recognize\nwhether a video is egocentric or not with 99.2% accuracy, up by 24% from\ncurrent state-of-the-art. To better understand what the network actually\nlearns, we propose a novel visualization of CNN kernels as flow fields.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 13:41:16 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 21:13:18 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Poleg", "Yair", ""], ["Ephrat", "Ariel", ""], ["Peleg", "Shmuel", ""], ["Arora", "Chetan", ""]]}, {"id": "1504.07488", "submitter": "Amir Hossein Bakhtiary Davijani", "authors": "Amir H. Bakhtiary (1), Agata Lapedriza (1), David Masip (1) ((1)\n  Universitat Oberta de Catalunya)", "title": "Speeding Up Neural Networks for Large Scale Classification using WTA\n  Hashing", "comments": "9 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose to use the Winner Takes All hashing technique to\nspeed up forward propagation and backward propagation in fully connected layers\nin convolutional neural networks. The proposed technique reduces significantly\nthe computational complexity, which in turn, allows us to train layers with a\nlarge number of kernels with out the associated time penalty.\n  As a consequence we are able to train convolutional neural network on a very\nlarge number of output classes with only a small increase in the computational\ncost. To show the effectiveness of the technique we train a new output layer on\na pretrained network using both the regular multiplicative approach and our\nproposed hashing methodology. Our results showed no drop in performance and\ndemonstrate, with our implementation, a 7 fold speed up during the training.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 14:17:24 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Bakhtiary", "Amir H.", ""], ["Lapedriza", "Agata", ""], ["Masip", "David", ""]]}, {"id": "1504.07575", "submitter": "Oisin Mac Aodha", "authors": "Edward Johns and Oisin Mac Aodha and Gabriel J. Brostow", "title": "Becoming the Expert - Interactive Multi-Class Machine Teaching", "comments": "CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to machines, humans are extremely good at classifying images into\ncategories, especially when they possess prior knowledge of the categories at\nhand. If this prior information is not available, supervision in the form of\nteaching images is required. To learn categories more quickly, people should\nsee important and representative images first, followed by less important\nimages later - or not at all. However, image-importance is individual-specific,\ni.e. a teaching image is important to a student if it changes their overall\nability to discriminate between classes. Further, students keep learning, so\nwhile image-importance depends on their current knowledge, it also varies with\ntime.\n  In this work we propose an Interactive Machine Teaching algorithm that\nenables a computer to teach challenging visual concepts to a human. Our\nadaptive algorithm chooses, online, which labeled images from a teaching set\nshould be shown to the student as they learn. We show that a teaching strategy\nthat probabilistically models the student's ability and progress, based on\ntheir correct and incorrect answers, produces better 'experts'. We present\nresults using real human participants across several varied and challenging\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 17:22:29 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Johns", "Edward", ""], ["Mac Aodha", "Oisin", ""], ["Brostow", "Gabriel J.", ""]]}, {"id": "1504.07590", "submitter": "Mrinal Haloi", "authors": "Mrinal Haloi, Dinesh Babu Jayagopi", "title": "A Robust Lane Detection and Departure Warning System", "comments": "The Intelligent Vehicles Symposium (IV2015). arXiv admin note: text\n  overlap with arXiv:1503.06648", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we have developed a robust lane detection and departure warning\ntechnique. Our system is based on single camera sensor. For lane detection a\nmodified Inverse Perspective Mapping using only a few extrinsic camera\nparameters and illuminant Invariant techniques is used. Lane markings are\nrepresented using a combination of 2nd and 4th order steerable filters, robust\nto shadowing. Effect of shadowing and extra sun light are removed using Lab\ncolor space, and illuminant invariant representation. Lanes are assumed to be\ncubic curves and fitted using robust RANSAC. This method can reliably detect\nlanes of the road and its boundary. This method has been experimented in Indian\nroad conditions under different challenging situations and the result obtained\nwere very good. For lane departure angle an optical flow based method were\nused.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 18:14:02 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Haloi", "Mrinal", ""], ["Jayagopi", "Dinesh Babu", ""]]}, {"id": "1504.07643", "submitter": "Ke Chen", "authors": "Mazlinda Ibrahim, Ke Chen and Carlos Brito-Loeza", "title": "A novel variational model for image registration using Gaussian\n  curvature", "comments": "23 pages, 5 figures. Key words: Image registration, Non-parametric\n  image registration, Regularisation, Gaussian curvature, surface mapping", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image registration is one important task in many image processing\napplications. It aims to align two or more images so that useful information\ncan be extracted through comparison, combination or superposition. This is\nachieved by constructing an optimal trans- formation which ensures that the\ntemplate image becomes similar to a given reference image. Although many models\nexist, designing a model capable of modelling large and smooth deformation\nfield continues to pose a challenge. This paper proposes a novel variational\nmodel for image registration using the Gaussian curvature as a regulariser. The\nmodel is motivated by the surface restoration work in geometric processing\n[Elsey and Esedoglu, Multiscale Model. Simul., (2009), pp. 1549-1573]. An\neffective numerical solver is provided for the model using an augmented\nLagrangian method. Numerical experiments can show that the new model\noutperforms three competing models based on, respectively, a linear curvature\n[Fischer and Modersitzki, J. Math. Imaging Vis., (2003), pp. 81- 85], the mean\ncurvature [Chumchob, Chen and Brito, Multiscale Model. Simul., (2011), pp.\n89-128] and the diffeomorphic demon model [Vercauteren at al., NeuroImage,\n(2009), pp. 61-72] in terms of robustness and accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 20:06:35 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Ibrahim", "Mazlinda", ""], ["Chen", "Ke", ""], ["Brito-Loeza", "Carlos", ""]]}, {"id": "1504.07786", "submitter": "Xiaobo Qu", "authors": "Yunsong Liu, Zhifang Zhan, Jian-Feng Cai, Di Guo, Zhong Chen, Xiaobo\n  Qu", "title": "Projected Iterative Soft-thresholding Algorithm for Tight Frames in\n  Compressed Sensing Magnetic Resonance Imaging", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing has shown great potentials in accelerating magnetic\nresonance imaging. Fast image reconstruction and high image quality are two\nmain issues faced by this new technology. It has been shown that, redundant\nimage representations, e.g. tight frames, can significantly improve the image\nquality. But how to efficiently solve the reconstruction problem with these\nredundant representation systems is still challenging. This paper attempts to\naddress the problem of applying iterative soft-thresholding algorithm (ISTA) to\ntight frames based magnetic resonance image reconstruction. By introducing the\ncanonical dual frame to construct the orthogonal projection operator on the\nrange of the analysis sparsity operator, we propose a projected iterative\nsoft-thresholding algorithm (pISTA) and further accelerate it by incorporating\nthe strategy proposed by Beck and Teboulle in 2009. We theoretically prove that\npISTA converges to the minimum of a function with a balanced tight frame\nsparsity. Experimental results demonstrate that the proposed algorithm achieves\nbetter reconstruction than the widely used synthesis sparse model and the\naccelerated pISTA converges faster or comparable to the state-of-art smoothing\nFISTA. One major advantage of pISTA is that only one extra parameter, the step\nsize, is introduced and the numerical solution is stable to it in terms of\nimage reconstruction errors, thus allowing easily setting in many fast magnetic\nresonance imaging applications.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 09:35:31 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2015 14:21:59 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Liu", "Yunsong", ""], ["Zhan", "Zhifang", ""], ["Cai", "Jian-Feng", ""], ["Guo", "Di", ""], ["Chen", "Zhong", ""], ["Qu", "Xiaobo", ""]]}, {"id": "1504.07857", "submitter": "Manuel W\\\"uthrich", "authors": "Manuel W\\\"uthrich, Peter Pastor, Ludovic Righetti, Aude Billard and\n  Stefan Schaal", "title": "Probabilistic Depth Image Registration incorporating Nonvisual\n  Information", "comments": null, "journal-ref": null, "doi": "10.1109/ICRA.2012.6225179", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we derive a probabilistic registration algorithm for object\nmodeling and tracking. In many robotics applications, such as manipulation\ntasks, nonvisual information about the movement of the object is available,\nwhich we will combine with the visual information. Furthermore we do not only\nconsider observations of the object, but we also take space into account which\nhas been observed to not be part of the object. Furthermore we are computing a\nposterior distribution over the relative alignment and not a point estimate as\ntypically done in for example Iterative Closest Point (ICP). To our knowledge\nno existing algorithm meets these three conditions and we thus derive a novel\nregistration algorithm in a Bayesian framework. Experimental results suggest\nthat the proposed methods perform favorably in comparison to PCL\nimplementations of feature mapping and ICP, especially if nonvisual information\nis available.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 13:45:20 GMT"}, {"version": "v2", "created": "Fri, 1 May 2015 19:02:13 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["W\u00fcthrich", "Manuel", ""], ["Pastor", "Peter", ""], ["Righetti", "Ludovic", ""], ["Billard", "Aude", ""], ["Schaal", "Stefan", ""]]}, {"id": "1504.07858", "submitter": "Qi Guo", "authors": "Qi Guo, Zixuan Wang, Ming Li and Hamid Aghajan", "title": "Intelligent Health Recommendation System for Computer Users", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The time people spend in front of computers has been increasing steadily due\nto the role computers play in modern society. Individuals who sit in front of\ncomputers for an extended period of time, specifically with improper postures\nmay incur various health issues. In this work, individuals' behaviors in front\nof computers are studied using web cameras. By means of non-rigid face tracking\nsystem, data are analyzed to determine the 3D head pose, blink rate and yawn\nfrequency of computer users. When combining these visual cues, a system of\nintelligent personal assistants for computer users is proposed.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 13:51:04 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Guo", "Qi", ""], ["Wang", "Zixuan", ""], ["Li", "Ming", ""], ["Aghajan", "Hamid", ""]]}, {"id": "1504.07874", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Jennifer Roldan-Carlos, Mathias Lux, Xavier Gir\\'o-i-Nieto, Pia\n  Mu\\~noz and Nektarios Anagnostopoulos", "title": "Visual Information Retrieval in Endoscopic Video Archives", "comments": "Paper accepted at the IEEE/ACM 13th International Workshop on\n  Content-Based Multimedia Indexing (CBMI) in Prague (Czech Republic) between\n  10 and 12 June 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In endoscopic procedures, surgeons work with live video streams from the\ninside of their subjects. A main source for documentation of procedures are\nstill frames from the video, identified and taken during the surgery. However,\nwith growing demands and technical means, the streams are saved to storage\nservers and the surgeons need to retrieve parts of the videos on demand. In\nthis submission we present a demo application allowing for video retrieval\nbased on visual features and late fusion, which allows surgeons to re-find\nshots taken during the procedure.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 14:35:35 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Roldan-Carlos", "Jennifer", ""], ["Lux", "Mathias", ""], ["Gir\u00f3-i-Nieto", "Xavier", ""], ["Mu\u00f1oz", "Pia", ""], ["Anagnostopoulos", "Nektarios", ""]]}, {"id": "1504.07889", "submitter": "Tsung-Yu Lin", "authors": "Tsung-Yu Lin, Aruni RoyChowdhury, Subhransu Maji", "title": "Bilinear CNNs for Fine-grained Visual Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple and effective architecture for fine-grained visual\nrecognition called Bilinear Convolutional Neural Networks (B-CNNs). These\nnetworks represent an image as a pooled outer product of features derived from\ntwo CNNs and capture localized feature interactions in a translationally\ninvariant manner. B-CNNs belong to the class of orderless texture\nrepresentations but unlike prior work they can be trained in an end-to-end\nmanner. Our most accurate model obtains 84.1%, 79.4%, 86.9% and 91.3% per-image\naccuracy on the Caltech-UCSD birds [67], NABirds [64], FGVC aircraft [42], and\nStanford cars [33] dataset respectively and runs at 30 frames-per-second on a\nNVIDIA Titan X GPU. We then present a systematic analysis of these networks and\nshow that (1) the bilinear features are highly redundant and can be reduced by\nan order of magnitude in size without significant loss in accuracy, (2) are\nalso effective for other image classification tasks such as texture and scene\nrecognition, and (3) can be trained from scratch on the ImageNet dataset\noffering consistent improvements over the baseline architecture. Finally, we\npresent visualizations of these models on various datasets using top\nactivations of neural units and gradient-based inversion techniques. The source\ncode for the complete system is available at http://vis-www.cs.umass.edu/bcnn.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 15:23:58 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2015 15:28:20 GMT"}, {"version": "v3", "created": "Tue, 29 Sep 2015 19:02:36 GMT"}, {"version": "v4", "created": "Mon, 28 Nov 2016 21:43:06 GMT"}, {"version": "v5", "created": "Wed, 31 May 2017 03:35:09 GMT"}, {"version": "v6", "created": "Thu, 1 Jun 2017 04:24:01 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Lin", "Tsung-Yu", ""], ["RoyChowdhury", "Aruni", ""], ["Maji", "Subhransu", ""]]}, {"id": "1504.07901", "submitter": "Achraf Ben-Hamadou", "authors": "Achraf Ben-Hamadou and Charles Soussen and Walter Blondel and\n  Christian Daul and Didier Wolf", "title": "Comparative study of image registration techniques for bladder\n  video-endoscopy", "comments": "7 pages, 5 figures", "journal-ref": "Novel Optical Instrumentation for Biomedical Applications, 737118\n  (10 July 2009)", "doi": "10.1117/12.831772", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bladder cancer is widely spread in the world. Many adequate diagnosis\ntechniques exist. Video-endoscopy remains the standard clinical procedure for\nvisual exploration of the bladder internal surface. However, video-endoscopy\npresents the limit that the imaged area for each image is about nearly 1cm2.\nAnd, lesions are, typically, spread over several images. The aim of this\ncontribution is to assess the performance of two mosaicing algorithms leading\nto the construction of panoramic maps (one unique image) of bladder walls. The\nquantitative comparison study is performed on a set of real endoscopic exam\ndata and on simulated data relative to bladder phantom.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 15:48:22 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Ben-Hamadou", "Achraf", ""], ["Soussen", "Charles", ""], ["Blondel", "Walter", ""], ["Daul", "Christian", ""], ["Wolf", "Didier", ""]]}, {"id": "1504.07907", "submitter": "Quynh Nguyen", "authors": "Quynh Nguyen (1 and 2), Antoine Gautier (2), Matthias Hein (2) ((1)\n  Max Planck Institute for Informatics, (2) Saarland University)", "title": "A Flexible Tensor Block Coordinate Ascent Scheme for Hypergraph Matching", "comments": "CVPR 2015 (Long version - All proofs included)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of correspondences between two images resp. point sets is a\ncore problem in computer vision. One way to formulate the problem is graph\nmatching leading to the quadratic assignment problem which is NP-hard. Several\nso called second order methods have been proposed to solve this problem. In\nrecent years hypergraph matching leading to a third order problem became\npopular as it allows for better integration of geometric information. For most\nof these third order algorithms no theoretical guarantees are known. In this\npaper we propose a general framework for tensor block coordinate ascent methods\nfor hypergraph matching. We propose two algorithms which both come along with\nthe guarantee of monotonic ascent in the matching score on the set of discrete\nassignment matrices. In the experiments we show that our new algorithms\noutperform previous work both in terms of achieving better matching scores and\nmatching accuracy. This holds in particular for very challenging settings where\none has a high number of outliers and other forms of noise.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 16:02:56 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2015 19:01:11 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Nguyen", "Quynh", "", "1 and 2"], ["Gautier", "Antoine", ""], ["Hein", "Matthias", ""]]}, {"id": "1504.07918", "submitter": "Filipe Condessa", "authors": "Filipe Condessa, Jose Bioucas-Dias, Jelena Kovacevic", "title": "Robust hyperspectral image classification with rejection fields", "comments": "This paper was submitted to IEEE WHISPERS 2015: 7th Workshop on\n  Hyperspectral Image and Signal Processing: Evolution on Remote Sensing. 5\n  pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel method for robust hyperspectral image\nclassification using context and rejection. Hyperspectral image classification\nis generally an ill-posed image problem where pixels may belong to unknown\nclasses, and obtaining representative and complete training sets is costly.\nFurthermore, the need for high classification accuracies is frequently greater\nthan the need to classify the entire image.\n  We approach this problem with a robust classification method that combines\nclassification with context with classification with rejection. A rejection\nfield that will guide the rejection is derived from the classification with\ncontextual information obtained by using the SegSALSA algorithm. We validate\nour method in real hyperspectral data and show that the performance gains\nobtained from the rejection fields are equivalent to an increase the dimension\nof the training sets.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 16:30:45 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Condessa", "Filipe", ""], ["Bioucas-Dias", "Jose", ""], ["Kovacevic", "Jelena", ""]]}, {"id": "1504.07947", "submitter": "Le Hou", "authors": "Le Hou, Dimitris Samaras, Tahsin M. Kurc, Yi Gao, James E. Davis, Joel\n  H. Saltz", "title": "Patch-based Convolutional Neural Network for Whole Slide Tissue Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) are state-of-the-art models for many\nimage classification tasks. However, to recognize cancer subtypes\nautomatically, training a CNN on gigapixel resolution Whole Slide Tissue Images\n(WSI) is currently computationally impossible. The differentiation of cancer\nsubtypes is based on cellular-level visual features observed on image patch\nscale. Therefore, we argue that in this situation, training a patch-level\nclassifier on image patches will perform better than or similar to an\nimage-level classifier. The challenge becomes how to intelligently combine\npatch-level classification results and model the fact that not all patches will\nbe discriminative. We propose to train a decision fusion model to aggregate\npatch-level predictions given by patch-level CNNs, which to the best of our\nknowledge has not been shown before. Furthermore, we formulate a novel\nExpectation-Maximization (EM) based method that automatically locates\ndiscriminative patches robustly by utilizing the spatial relationships of\npatches. We apply our method to the classification of glioma and non-small-cell\nlung carcinoma cases into subtypes. The classification accuracy of our method\nis similar to the inter-observer agreement between pathologists. Although it is\nimpossible to train CNNs on WSIs, we experimentally demonstrate using a\ncomparable non-cancer dataset of smaller images that a patch-based CNN can\noutperform an image-based CNN.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 18:15:22 GMT"}, {"version": "v2", "created": "Mon, 11 May 2015 01:55:55 GMT"}, {"version": "v3", "created": "Tue, 19 May 2015 21:01:11 GMT"}, {"version": "v4", "created": "Tue, 8 Mar 2016 18:07:03 GMT"}, {"version": "v5", "created": "Wed, 9 Mar 2016 14:26:16 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Hou", "Le", ""], ["Samaras", "Dimitris", ""], ["Kurc", "Tahsin M.", ""], ["Gao", "Yi", ""], ["Davis", "James E.", ""], ["Saltz", "Joel H.", ""]]}, {"id": "1504.07958", "submitter": "Shoaib Ehsan", "authors": "Shoaib Ehsan and Klaus D. McDonald-Maier", "title": "Exploring Integral Image Word Length Reduction Techniques for SURF\n  Detector", "comments": "ICCEE 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speeded Up Robust Features (SURF) is a state of the art computer vision\nalgorithm that relies on integral image representation for performing fast\ndetection and description of image features that are scale and rotation\ninvariant. Integral image representation, however, has major draw back of large\nbinary word length that leads to substantial increase in memory size. When\ndesigning a dedicated hardware to achieve real-time performance for the SURF\nalgorithm, it is imperative to consider the adverse effects of integral image\non memory size, bus width and computational resources. With the objective of\nminimizing hardware resources, this paper presents a novel implementation\nconcept of a reduced word length integral image based SURF detector. It\nevaluates two existing word length reduction techniques for the particular case\nof SURF detector and extends one of these to achieve more reduction in word\nlength. This paper also introduces a novel method to achieve integral image\nword length reduction for SURF detector.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 18:43:06 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Ehsan", "Shoaib", ""], ["McDonald-Maier", "Klaus D.", ""]]}, {"id": "1504.07962", "submitter": "Shoaib Ehsan", "authors": "Shoaib Ehsan, Adrian F. Clark and Klaus D. McDonald-Maier", "title": "Hardware based Scale- and Rotation-Invariant Feature Extraction: A\n  Retrospective Analysis and Future Directions", "comments": "ICCEE 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer Vision techniques represent a class of algorithms that are highly\ncomputation and data intensive in nature. Generally, performance of these\nalgorithms in terms of execution speed on desktop computers is far from\nreal-time. Since real-time performance is desirable in many applications,\nspecial-purpose hardware is required in most cases to achieve this goal. Scale-\nand rotation-invariant local feature extraction is a low level computer vision\ntask with very high computational complexity. The state-of-the-art algorithms\nthat currently exist in this domain, like SIFT and SURF, suffer from slow\nexecution speeds and at best can only achieve rates of 2-3 Hz on modern desktop\ncomputers. Hardware-based scale- and rotation-invariant local feature\nextraction is an emerging trend enabling real-time performance for these\ncomputationally complex algorithms. This paper takes a retrospective look at\nthe advances made so far in this field, discusses the hardware design\nstrategies employed and results achieved, identifies current research gaps and\nsuggests future research directions.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 18:52:37 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Ehsan", "Shoaib", ""], ["Clark", "Adrian F.", ""], ["McDonald-Maier", "Klaus D.", ""]]}, {"id": "1504.07967", "submitter": "Shoaib Ehsan", "authors": "Shoaib Ehsan, Nadia Kanwal, Adrian F. Clark and Klaus D.\n  McDonald-Maier", "title": "Improved repeatability measures for evaluating performance of feature\n  detectors", "comments": null, "journal-ref": "Electronics Letters 8th July 2010 Vol. 46 No. 14", "doi": null, "report-no": null, "categories": "cs.CV cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most frequently employed measure for performance characterisation of\nlocal feature detectors is repeatability, but it has been observed that this\ndoes not necessarily mirror actual performance. Presented are improved\nrepeatability formulations which correlate much better with the true\nperformance of feature detectors. Comparative results for several\nstate-of-the-art feature detectors are presented using these measures; it is\nfound that Hessian-based detectors are generally superior at identifying\nfeatures when images are subject to various geometric and photometric\ntransformations.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 19:01:30 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Ehsan", "Shoaib", ""], ["Kanwal", "Nadia", ""], ["Clark", "Adrian F.", ""], ["McDonald-Maier", "Klaus D.", ""]]}, {"id": "1504.08023", "submitter": "Carl Vondrick", "authors": "Carl Vondrick, Hamed Pirsiavash, Antonio Torralba", "title": "Anticipating Visual Representations from Unlabeled Video", "comments": "CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anticipating actions and objects before they start or appear is a difficult\nproblem in computer vision with several real-world applications. This task is\nchallenging partly because it requires leveraging extensive knowledge of the\nworld that is difficult to write down. We believe that a promising resource for\nefficiently learning this knowledge is through readily available unlabeled\nvideo. We present a framework that capitalizes on temporal structure in\nunlabeled video to learn to anticipate human actions and objects. The key idea\nbehind our approach is that we can train deep networks to predict the visual\nrepresentation of images in the future. Visual representations are a promising\nprediction target because they encode images at a higher semantic level than\npixels yet are automatic to compute. We then apply recognition algorithms on\nour predicted representation to anticipate objects and actions. We\nexperimentally validate this idea on two datasets, anticipating actions one\nsecond in the future and objects five seconds in the future.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 21:01:51 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 03:49:34 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Vondrick", "Carl", ""], ["Pirsiavash", "Hamed", ""], ["Torralba", "Antonio", ""]]}, {"id": "1504.08083", "submitter": "Ross Girshick", "authors": "Ross Girshick", "title": "Fast R-CNN", "comments": "To appear in ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a Fast Region-based Convolutional Network method (Fast\nR-CNN) for object detection. Fast R-CNN builds on previous work to efficiently\nclassify object proposals using deep convolutional networks. Compared to\nprevious work, Fast R-CNN employs several innovations to improve training and\ntesting speed while also increasing detection accuracy. Fast R-CNN trains the\nvery deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and\nachieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains\nVGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is\nimplemented in Python and C++ (using Caffe) and is available under the\nopen-source MIT License at https://github.com/rbgirshick/fast-rcnn.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 05:13:08 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2015 15:10:14 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Girshick", "Ross", ""]]}, {"id": "1504.08142", "submitter": "Haiping Lu", "authors": "Qiquan Shi and Haiping Lu", "title": "Semi-Orthogonal Multilinear PCA with Relaxed Start", "comments": "8 pages, 2 figures, to appear in Proceedings of the 24th\n  International Joint Conference on Artificial Intelligence (IJCAI 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is an unsupervised method for learning\nlow-dimensional features with orthogonal projections. Multilinear PCA methods\nextend PCA to deal with multidimensional data (tensors) directly via\ntensor-to-tensor projection or tensor-to-vector projection (TVP). However,\nunder the TVP setting, it is difficult to develop an effective multilinear PCA\nmethod with the orthogonality constraint. This paper tackles this problem by\nproposing a novel Semi-Orthogonal Multilinear PCA (SO-MPCA) approach. SO-MPCA\nlearns low-dimensional features directly from tensors via TVP by imposing the\northogonality constraint in only one mode. This formulation results in more\ncaptured variance and more learned features than full orthogonality. For better\ngeneralization, we further introduce a relaxed start (RS) strategy to get\nSO-MPCA-RS by fixing the starting projection vectors, which increases the bias\nand reduces the variance of the learning model. Experiments on both face (2D)\nand gait (3D) data demonstrate that SO-MPCA-RS outperforms other competing\nalgorithms on the whole, and the relaxed start strategy is also effective for\nother TVP-based PCA methods.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 09:40:09 GMT"}, {"version": "v2", "created": "Thu, 7 May 2015 01:40:27 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Shi", "Qiquan", ""], ["Lu", "Haiping", ""]]}, {"id": "1504.08200", "submitter": "Bugra Tekin", "authors": "Bugra Tekin, Xiaolu Sun, Xinchao Wang, Vincent Lepetit, Pascal Fua", "title": "Predicting People's 3D Poses from Short Sequences", "comments": "superseded by arXiv:1511.06692", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient approach to exploiting motion information from\nconsecutive frames of a video sequence to recover the 3D pose of people.\nInstead of computing candidate poses in individual frames and then linking\nthem, as is often done, we regress directly from a spatio-temporal block of\nframes to a 3D pose in the central one. We will demonstrate that this approach\nallows us to effectively overcome ambiguities and to improve upon the\nstate-of-the-art on challenging sequences.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 12:54:39 GMT"}, {"version": "v2", "created": "Fri, 1 May 2015 11:59:56 GMT"}, {"version": "v3", "created": "Mon, 4 May 2015 11:24:56 GMT"}, {"version": "v4", "created": "Mon, 23 Nov 2015 21:48:15 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Tekin", "Bugra", ""], ["Sun", "Xiaolu", ""], ["Wang", "Xinchao", ""], ["Lepetit", "Vincent", ""], ["Fua", "Pascal", ""]]}, {"id": "1504.08219", "submitter": "Oisin Mac Aodha", "authors": "Oisin Mac Aodha and Neill D.F. Campbell and Jan Kautz and Gabriel J.\n  Brostow", "title": "Hierarchical Subquery Evaluation for Active Learning on a Graph", "comments": "CVPR 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To train good supervised and semi-supervised object classifiers, it is\ncritical that we not waste the time of the human experts who are providing the\ntraining labels. Existing active learning strategies can have uneven\nperformance, being efficient on some datasets but wasteful on others, or\ninconsistent just between runs on the same dataset. We propose perplexity based\ngraph construction and a new hierarchical subquery evaluation algorithm to\ncombat this variability, and to release the potential of Expected Error\nReduction.\n  Under some specific circumstances, Expected Error Reduction has been one of\nthe strongest-performing informativeness criteria for active learning. Until\nnow, it has also been prohibitively costly to compute for sizeable datasets. We\ndemonstrate our highly practical algorithm, comparing it to other active\nlearning measures on classification datasets that vary in sparsity,\ndimensionality, and size. Our algorithm is consistent over multiple runs and\nachieves high accuracy, while querying the human expert for labels at a\nfrequency that matches their desired time budget.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 13:35:59 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Mac Aodha", "Oisin", ""], ["Campbell", "Neill D. F.", ""], ["Kautz", "Jan", ""], ["Brostow", "Gabriel J.", ""]]}, {"id": "1504.08289", "submitter": "Marcel Simon", "authors": "Marcel Simon and Erik Rodner", "title": "Neural Activation Constellations: Unsupervised Part Model Discovery with\n  Convolutional Networks", "comments": "Published at IEEE International Conference on Computer Vision (ICCV)\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Part models of object categories are essential for challenging recognition\ntasks, where differences in categories are subtle and only reflected in\nappearances of small parts of the object. We present an approach that is able\nto learn part models in a completely unsupervised manner, without part\nannotations and even without given bounding boxes during learning. The key idea\nis to find constellations of neural activation patterns computed using\nconvolutional neural networks. In our experiments, we outperform existing\napproaches for fine-grained recognition on the CUB200-2011, NA birds, Oxford\nPETS, and Oxford Flowers dataset in case no part or bounding box annotations\nare available and achieve state-of-the-art performance for the Stanford Dog\ndataset. We also show the benefits of neural constellation models as a data\naugmentation technique for fine-tuning. Furthermore, our paper unites the areas\nof generic and fine-grained classification, since our approach is suitable for\nboth scenarios. The source code of our method is available online at\nhttp://www.inf-cv.uni-jena.de/part_discovery\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 16:06:50 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2015 11:43:03 GMT"}, {"version": "v3", "created": "Sat, 5 Dec 2015 15:53:09 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Simon", "Marcel", ""], ["Rodner", "Erik", ""]]}, {"id": "1504.08308", "submitter": "Matthias Zeppelzauer", "authors": "Matthias Zeppelzauer and Markus Seidl", "title": "Efficient Image-Space Extraction and Representation of 3D Surface\n  Topography", "comments": "Initial version of the paper accepted at the IEEE ICIP Conference\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface topography refers to the geometric micro-structure of a surface and\ndefines its tactile characteristics (typically in the sub-millimeter range).\nHigh-resolution 3D scanning techniques developed recently enable the 3D\nreconstruction of surfaces including their surface topography. In his paper, we\npresent an efficient image-space technique for the extraction of surface\ntopography from high-resolution 3D reconstructions. Additionally, we filter\nnoise and enhance topographic attributes to obtain an improved representation\nfor subsequent topography classification. Comprehensive experiments show that\nthe our representation captures well topographic attributes and significantly\nimproves classification performance compared to alternative 2D and 3D\nrepresentations.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 17:02:59 GMT"}, {"version": "v2", "created": "Mon, 4 May 2015 11:35:27 GMT"}, {"version": "v3", "created": "Wed, 6 May 2015 16:40:43 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Zeppelzauer", "Matthias", ""], ["Seidl", "Markus", ""]]}, {"id": "1504.08362", "submitter": "Michael Figurnov", "authors": "Michael Figurnov, Aijan Ibraimova, Dmitry Vetrov, Pushmeet Kohli", "title": "PerforatedCNNs: Acceleration through Elimination of Redundant\n  Convolutions", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to reduce the computational cost of evaluation of\nconvolutional neural networks, a factor that has hindered their deployment in\nlow-power devices such as mobile phones. Inspired by the loop perforation\ntechnique from source code optimization, we speed up the bottleneck\nconvolutional layers by skipping their evaluation in some of the spatial\npositions. We propose and analyze several strategies of choosing these\npositions. We demonstrate that perforation can accelerate modern convolutional\nnetworks such as AlexNet and VGG-16 by a factor of 2x - 4x. Additionally, we\nshow that perforation is complementary to the recently proposed acceleration\nmethod of Zhang et al.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 19:48:48 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 18:36:06 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 20:10:22 GMT"}, {"version": "v4", "created": "Sun, 16 Oct 2016 02:00:47 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Figurnov", "Michael", ""], ["Ibraimova", "Aijan", ""], ["Vetrov", "Dmitry", ""], ["Kohli", "Pushmeet", ""]]}]