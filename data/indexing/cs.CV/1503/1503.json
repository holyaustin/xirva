[{"id": "1503.00040", "submitter": "Oncel Tuzel", "authors": "Chinmay Hegde, Oncel Tuzel, Fatih Porikli", "title": "Efficient Upsampling of Natural Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method of efficient upsampling of a single natural image.\nCurrent methods for image upsampling tend to produce high-resolution images\nwith either blurry salient edges, or loss of fine textural detail, or spurious\nnoise artifacts.\n  In our method, we mitigate these effects by modeling the input image as a sum\nof edge and detail layers, operating upon these layers separately, and merging\nthe upscaled results in an automatic fashion. We formulate the upsampled output\nimage as the solution to a non-convex energy minimization problem, and propose\nan algorithm to obtain a tractable approximate solution. Our algorithm\ncomprises two main stages. 1) For the edge layer, we use a nonparametric\napproach by constructing a dictionary of patches from a given image, and\nsynthesize edge regions in a higher-resolution version of the image. 2) For the\ndetail layer, we use a global parametric texture enhancement approach to\nsynthesize detail regions across the image.\n  We demonstrate that our method is able to accurately reproduce sharp edges as\nwell as synthesize photorealistic textures, while avoiding common artifacts\nsuch as ringing and haloing. In addition, our method involves no training phase\nor estimation of model parameters, and is easily parallelizable. We demonstrate\nthe utility of our method on a number of challenging standard test photos.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 00:18:39 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Hegde", "Chinmay", ""], ["Tuzel", "Oncel", ""], ["Porikli", "Fatih", ""]]}, {"id": "1503.00064", "submitter": "Sanja Fidler", "authors": "Dahua Lin, Chen Kong, Sanja Fidler, Raquel Urtasun", "title": "Generating Multi-Sentence Lingual Descriptions of Indoor Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel framework for generating lingual descriptions of\nindoor scenes. Whereas substantial efforts have been made to tackle this\nproblem, previous approaches focusing primarily on generating a single sentence\nfor each image, which is not sufficient for describing complex scenes. We\nattempt to go beyond this, by generating coherent descriptions with multiple\nsentences. Our approach is distinguished from conventional ones in several\naspects: (1) a 3D visual parsing system that jointly infers objects,\nattributes, and relations; (2) a generative grammar learned automatically from\ntraining text; and (3) a text generation algorithm that takes into account the\ncoherence among sentences. Experiments on the augmented NYU-v2 dataset show\nthat our framework can generate natural descriptions with substantially higher\nROGUE scores compared to those produced by the baseline.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 04:26:21 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Lin", "Dahua", ""], ["Kong", "Chen", ""], ["Fidler", "Sanja", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1503.00072", "submitter": "Hanxi Li", "authors": "Hanxi Li, Yi Li, Fatih Porikli", "title": "DeepTrack: Learning Discriminative Feature Representations Online for\n  Robust Visual Tracking", "comments": "12 pages", "journal-ref": null, "doi": "10.1109/TIP.2015.2510583", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Deep neural networks, albeit their great success on feature learning in\nvarious computer vision tasks, are usually considered as impractical for online\nvisual tracking because they require very long training time and a large number\nof training samples. In this work, we present an efficient and very robust\ntracking algorithm using a single Convolutional Neural Network (CNN) for\nlearning effective feature representations of the target object, in a purely\nonline manner. Our contributions are multifold: First, we introduce a novel\ntruncated structural loss function that maintains as many training samples as\npossible and reduces the risk of tracking error accumulation. Second, we\nenhance the ordinary Stochastic Gradient Descent approach in CNN training with\na robust sample selection mechanism. The sampling mechanism randomly generates\npositive and negative samples from different temporal distributions, which are\ngenerated by taking the temporal relations and label noise into account.\nFinally, a lazy yet effective updating scheme is designed for CNN training.\nEquipped with this novel updating algorithm, the CNN model is robust to some\nlong-existing difficulties in visual tracking such as occlusion or incorrect\ndetections, without loss of the effective adaption for significant appearance\nchanges. In the experiment, our CNN tracker outperforms all compared\nstate-of-the-art methods on two recently proposed benchmarks which in total\ninvolve over 60 video sequences. The remarkable performance improvement over\nthe existing trackers illustrates the superiority of the feature\nrepresentations which are learned\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 06:09:17 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Li", "Hanxi", ""], ["Li", "Yi", ""], ["Porikli", "Fatih", ""]]}, {"id": "1503.00081", "submitter": "Weiyao Lin", "authors": "Weiyao Lin, Ming-Ting Sun, Radha Poovendran, Zhengyou Zhang", "title": "Activity Recognition Using A Combination of Category Components And\n  Local Models for Video Surveillance", "comments": "This manuscript is the accepted version for TCSVT (IEEE Transactions\n  on Circuits and Systems for Video Technology)", "journal-ref": "IEEE Trans. Circuits and Systems for Video Technology, vol. 18,\n  pp. 1128-1139, 2008", "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for automatic recognition of human\nactivities for video surveillance applications. We propose to represent an\nactivity by a combination of category components, and demonstrate that this\napproach offers flexibility to add new activities to the system and an ability\nto deal with the problem of building models for activities lacking training\ndata. For improving the recognition accuracy, a Confident-Frame- based\nRecognition algorithm is also proposed, where the video frames with high\nconfidence for recognizing an activity are used as a specialized local model to\nhelp classify the remainder of the video frames. Experimental results show the\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 06:49:33 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Lin", "Weiyao", ""], ["Sun", "Ming-Ting", ""], ["Poovendran", "Radha", ""], ["Zhang", "Zhengyou", ""]]}, {"id": "1503.00082", "submitter": "Weiyao Lin", "authors": "Weiyao Lin, Ming-Ting Sun, Radha Poovendran, Zhengyou Zhang", "title": "Group Event Detection with a Varying Number of Group Members for Video\n  Surveillance", "comments": "This manuscript is the accepted version for TCSVT (IEEE Transactions\n  on Circuits and Systems for Video Technology)", "journal-ref": "IEEE Trans. Circuits and Systems for Video Technology, vol. 20,\n  no. 8, pp. 1057-1067, 2010", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for automatic recognition of group\nactivities for video surveillance applications. We propose to use a group\nrepresentative to handle the recognition with a varying number of group\nmembers, and use an Asynchronous Hidden Markov Model (AHMM) to model the\nrelationship between people. Furthermore, we propose a group activity detection\nalgorithm which can handle both symmetric and asymmetric group activities, and\ndemonstrate that this approach enables the detection of hierarchical\ninteractions between people. Experimental results show the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 06:51:39 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Lin", "Weiyao", ""], ["Sun", "Ming-Ting", ""], ["Poovendran", "Radha", ""], ["Zhang", "Zhengyou", ""]]}, {"id": "1503.00087", "submitter": "Weiyao Lin", "authors": "Weiyao Lin, Ming-Ting Sun, Hongxiang Li, Zhenzhong Chen, Wei Li, Bing\n  Zhou", "title": "Macroblock Classification Method for Video Applications Involving\n  Motions", "comments": "This manuscript is the accepted version for TB (IEEE Transactions on\n  Broadcasting)", "journal-ref": "IEEE Trans. Broadcasting, vol. 58, no. 1, pp. 34-46, 2012", "doi": "10.1109/TBC.2011.2170611", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a macroblock classification method is proposed for various\nvideo processing applications involving motions. Based on the analysis of the\nMotion Vector field in the compressed video, we propose to classify Macroblocks\nof each video frame into different classes and use this class information to\ndescribe the frame content. We demonstrate that this low-computation-complexity\nmethod can efficiently catch the characteristics of the frame. Based on the\nproposed macroblock classification, we further propose algorithms for different\nvideo processing applications, including shot change detection, motion\ndiscontinuity detection, and outlier rejection for global motion estimation.\nExperimental results demonstrate that the methods based on the proposed\napproach can work effectively on these applications.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 07:14:01 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Lin", "Weiyao", ""], ["Sun", "Ming-Ting", ""], ["Li", "Hongxiang", ""], ["Chen", "Zhenzhong", ""], ["Li", "Wei", ""], ["Zhou", "Bing", ""]]}, {"id": "1503.00090", "submitter": "Weiyao Lin", "authors": "Chongyang Zhang, Weiyao Lin, Wei Li, Bing Zhou, Jun Xie, Jijia Li", "title": "Improved Image Deblurring based on Salient-region Segmentation", "comments": "This manuscript is the accepted version for Image Comm (Signal\n  Processing: Image Communication)", "journal-ref": "Signal Processing: Image Communication, vol. 28, no. 9, pp.\n  1171-1186, 2013", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image deblurring techniques play important roles in many image processing\napplications. As the blur varies spatially across the image plane, it calls for\nrobust and effective methods to deal with the spatially-variant blur problem.\nIn this paper, a Saliency-based Deblurring (SD) approach is proposed based on\nthe saliency detection for salient-region segmentation and a corresponding\ncompensate method for image deblurring. We also propose a PDE-based deblurring\nmethod which introduces an anisotropic Partial Differential Equation (PDE)\nmodel for latent image prediction and employs an adaptive optimization model in\nthe kernel estimation and deconvolution steps. Experimental results demonstrate\nthe effectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 07:24:08 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Zhang", "Chongyang", ""], ["Lin", "Weiyao", ""], ["Li", "Wei", ""], ["Zhou", "Bing", ""], ["Xie", "Jun", ""], ["Li", "Jijia", ""]]}, {"id": "1503.00488", "submitter": "Chunlei Peng", "authors": "Chunlei Peng, Xinbo Gao, Nannan Wang, Jie Li", "title": "Graphical Representation for Heterogeneous Face Recognition", "comments": "13 pages, 10 figures, TPAMI 2016 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous face recognition (HFR) refers to matching face images acquired\nfrom different sources (i.e., different sensors or different wavelengths) for\nidentification. HFR plays an important role in both biometrics research and\nindustry. In spite of promising progresses achieved in recent years, HFR is\nstill a challenging problem due to the difficulty to represent two\nheterogeneous images in a homogeneous manner. Existing HFR methods either\nrepresent an image ignoring the spatial information, or rely on a\ntransformation procedure which complicates the recognition task. Considering\nthese problems, we propose a novel graphical representation based HFR method\n(G-HFR) in this paper. Markov networks are employed to represent heterogeneous\nimage patches separately, which takes the spatial compatibility between\nneighboring image patches into consideration. A coupled representation\nsimilarity metric (CRSM) is designed to measure the similarity between obtained\ngraphical representations. Extensive experiments conducted on multiple HFR\nscenarios (viewed sketch, forensic sketch, near infrared image, and thermal\ninfrared image) show that the proposed method outperforms state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 11:50:42 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 04:33:49 GMT"}, {"version": "v3", "created": "Mon, 14 Mar 2016 07:39:14 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Peng", "Chunlei", ""], ["Gao", "Xinbo", ""], ["Wang", "Nannan", ""], ["Li", "Jie", ""]]}, {"id": "1503.00516", "submitter": "Johann Bengua", "authors": "Johann A. Bengua, Ho N. Phien, Hoang D. Tuan and Minh N. Do", "title": "Matrix Product State for Feature Extraction of Higher-Order Tensors", "comments": "10 pages, 3 figures, updated introduction, submitted to IEEE\n  Transactions on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces matrix product state (MPS) decomposition as a\ncomputational tool for extracting features of multidimensional data represented\nby higher-order tensors. Regardless of tensor order, MPS extracts its relevant\nfeatures to the so-called core tensor of maximum order three which can be used\nfor classification. Mainly based on a successive sequence of singular value\ndecompositions (SVD), MPS is quite simple to implement without any recursive\nprocedure needed for optimizing local tensors. Thus, it leads to substantial\ncomputational savings compared to other tensor feature extraction methods such\nas higher-order orthogonal iteration (HOOI) underlying the Tucker decomposition\n(TD). Benchmark results show that MPS can reduce significantly the feature\nspace of data while achieving better classification performance compared to\nHOOI.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 13:20:25 GMT"}, {"version": "v2", "created": "Mon, 25 May 2015 22:45:24 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2016 21:29:47 GMT"}, {"version": "v4", "created": "Wed, 20 Jan 2016 22:11:39 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Bengua", "Johann A.", ""], ["Phien", "Ho N.", ""], ["Tuan", "Hoang D.", ""], ["Do", "Minh N.", ""]]}, {"id": "1503.00591", "submitter": "Xu Zhang", "authors": "Xu Zhang, Felix Xinnan Yu, Shih-Fu Chang, Shengjin Wang", "title": "Deep Transfer Network: Unsupervised Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation aims at training a classifier in one dataset and applying\nit to a related but not identical dataset. One successfully used framework of\ndomain adaptation is to learn a transformation to match both the distribution\nof the features (marginal distribution), and the distribution of the labels\ngiven features (conditional distribution). In this paper, we propose a new\ndomain adaptation framework named Deep Transfer Network (DTN), where the highly\nflexible deep neural networks are used to implement such a distribution\nmatching process.\n  This is achieved by two types of layers in DTN: the shared feature extraction\nlayers which learn a shared feature subspace in which the marginal\ndistributions of the source and the target samples are drawn close, and the\ndiscrimination layers which match conditional distributions by classifier\ntransduction. We also show that DTN has a computation complexity linear to the\nnumber of training samples, making it suitable to large-scale problems. By\ncombining the best paradigms in both worlds (deep neural networks in\nrecognition, and matching marginal and conditional distributions in domain\nadaptation), we demonstrate by extensive experiments that DTN improves\nsignificantly over former methods in both execution time and classification\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 16:17:06 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Zhang", "Xu", ""], ["Yu", "Felix Xinnan", ""], ["Chang", "Shih-Fu", ""], ["Wang", "Shengjin", ""]]}, {"id": "1503.00593", "submitter": "Jian Sun", "authors": "Jian Sun, Wenfei Cao, Zongben Xu, Jean Ponce", "title": "Learning a Convolutional Neural Network for Non-uniform Motion Blur\n  Removal", "comments": "This is a final version accepted by CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of estimating and removing non-uniform\nmotion blur from a single blurry image. We propose a deep learning approach to\npredicting the probabilistic distribution of motion blur at the patch level\nusing a convolutional neural network (CNN). We further extend the candidate set\nof motion kernels predicted by the CNN using carefully designed image\nrotations. A Markov random field model is then used to infer a dense\nnon-uniform motion blur field enforcing motion smoothness. Finally, motion blur\nis removed by a non-uniform deblurring model using patch-level image prior.\nExperimental evaluations show that our approach can effectively estimate and\nremove complex non-uniform motion blur that is not handled well by previous\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 16:22:51 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2015 09:04:09 GMT"}, {"version": "v3", "created": "Sun, 12 Apr 2015 10:05:53 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Sun", "Jian", ""], ["Cao", "Wenfei", ""], ["Xu", "Zongben", ""], ["Ponce", "Jean", ""]]}, {"id": "1503.00687", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "title": "A review of mean-shift algorithms for clustering", "comments": "28 pages, 9 figures. Invited book chapter to appear in the CRC\n  Handbook of Cluster Analysis (eds. Roberto Rocci, Fionn Murtagh, Marina Meila\n  and Christian Hennig)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural way to characterize the cluster structure of a dataset is by\nfinding regions containing a high density of data. This can be done in a\nnonparametric way with a kernel density estimate, whose modes and hence\nclusters can be found using mean-shift algorithms. We describe the theory and\npractice behind clustering based on kernel density estimates and mean-shift\nalgorithms. We discuss the blurring and non-blurring versions of mean-shift;\ntheoretical results about mean-shift algorithms and Gaussian mixtures;\nrelations with scale-space theory, spectral clustering and other algorithms;\nextensions to tracking, to manifold and graph data, and to manifold denoising;\nK-modes and Laplacian K-modes algorithms; acceleration strategies for large\ndatasets; and applications to image segmentation, manifold denoising and\nmultivalued regression.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 20:09:14 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""]]}, {"id": "1503.00757", "submitter": "Andreas Mang", "authors": "Andreas Mang and George Biros", "title": "Constrained $H^1$-regularization schemes for diffeomorphic image\n  registration", "comments": null, "journal-ref": "SIAM J Imaging Sci, 9(3):1154-1194, 2016", "doi": "10.1137/15M1010919", "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose regularization schemes for deformable registration and efficient\nalgorithms for their numerical approximation. We treat image registration as a\nvariational optimal control problem. The deformation map is parametrized by its\nvelocity. Tikhonov regularization ensures well-posedness. Our scheme augments\nstandard smoothness regularization operators based on $H^1$- and\n$H^2$-seminorms with a constraint on the divergence of the velocity field,\nwhich resembles variational formulations for Stokes incompressible flows. In\nour formulation, we invert for a stationary velocity field and a mass source\nmap. This allows us to explicitly control the compressibility of the\ndeformation map and by that the determinant of the deformation gradient. We\nalso introduce a new regularization scheme that allows us to control shear. We\nuse a globalized, preconditioned, matrix-free, reduced space\n(Gauss--)Newton--Krylov scheme for numerical optimization. We exploit variable\nelimination techniques to reduce the number of unknowns of our system; we only\niterate on the reduced space of the velocity field. Our current implementation\nis limited to the two-dimensional case. The numerical experiments demonstrate\nthat we can control the determinant of the deformation gradient without\ncompromising registration quality. This additional control allows us to avoid\noversmoothing of the deformation map. We also demonstrate that we can promote\nor penalize shear while controlling the determinant of the deformation\ngradient.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 21:33:54 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2015 16:28:18 GMT"}, {"version": "v3", "created": "Wed, 7 Sep 2016 20:08:32 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Mang", "Andreas", ""], ["Biros", "George", ""]]}, {"id": "1503.00769", "submitter": "Toshiro Kubota", "authors": "Toshiro Kubota", "title": "Grouping and Recognition of Dot Patterns with Straight Offset Polygons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the boundary of a familiar object is shown by a series of isolated dots,\nhumans can often recognize the object with ease. This ability can be sustained\nwith addition of distracting dots around the object. However, such capability\nhas not been reproduced algorithmically on computers. We introduce a new\nalgorithm that groups a set of dots into multiple non-disjoint subsets. It\nconnects the dots into a spanning tree using the proximity cue. It then applies\nthe straight polygon transformation to an initial polygon derived from the\nspanning tree. The straight polygon divides the space into polygons recursively\nand each polygon can be viewed as grouping of a subset of the dots. The number\nof polygons generated is O($n$). We also introduce simple shape selection and\nrecognition algorithms that can be applied to the grouping result. We used both\nnatural and synthetic images to show effectiveness of these algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 21:58:53 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Kubota", "Toshiro", ""]]}, {"id": "1503.00783", "submitter": "Davide Modolo", "authors": "Davide Modolo, Alexander Vezhnevets, Olga Russakovsky, Vittorio\n  Ferrari", "title": "Joint calibration of Ensemble of Exemplar SVMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for calibrating the Ensemble of Exemplar SVMs model.\nUnlike the standard approach, which calibrates each SVM independently, our\nmethod optimizes their joint performance as an ensemble. We formulate joint\ncalibration as a constrained optimization problem and devise an efficient\noptimization algorithm to find its global optimum. The algorithm dynamically\ndiscards parts of the solution space that cannot contain the optimum early on,\nmaking the optimization computationally feasible. We experiment with EE-SVM\ntrained on state-of-the-art CNN descriptors. Results on the ILSVRC 2014 and\nPASCAL VOC 2007 datasets show that (i) our joint calibration procedure\noutperforms independent calibration on the task of classifying windows as\nbelonging to an object class or not; and (ii) this improved window classifier\nleads to better performance on the object detection task.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 23:59:50 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2015 16:42:51 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Modolo", "Davide", ""], ["Vezhnevets", "Alexander", ""], ["Russakovsky", "Olga", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1503.00787", "submitter": "Davide Modolo", "authors": "Davide Modolo, Alexander Vezhnevets, Vittorio Ferrari", "title": "Context Forest for efficient object detection with large mixture models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Context Forest (ConF), a technique for predicting properties of\nthe objects in an image based on its global appearance. Compared to standard\nnearest-neighbour techniques, ConF is more accurate, fast and memory efficient.\nWe train ConF to predict which aspects of an object class are likely to appear\nin a given image (e.g. which viewpoint). This enables to speed-up\nmulti-component object detectors, by automatically selecting the most relevant\ncomponents to run on that image. This is particularly useful for detectors\ntrained from large datasets, which typically need many components to fully\nabsorb the data and reach their peak performance. ConF provides a speed-up of\n2x for the DPM detector [1] and of 10x for the EE-SVM detector [2]. To show\nConF's generality, we also train it to predict at which locations objects are\nlikely to appear in an image. Incorporating this information in the detector\nscore improves mAP performance by about 2% by removing false positive\ndetections in unlikely locations.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 00:20:58 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Modolo", "Davide", ""], ["Vezhnevets", "Alexander", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1503.00843", "submitter": "H.R.  Chennamma", "authors": "Sowmya K.N. and H.R. Chennamma", "title": "A Survey On Video Forgery Detection", "comments": "11 pages, 3 figures, International Journal of Computer Engineering\n  and Applications, Volume IX, Issue II, February 2015", "journal-ref": "International Journal of Computer Engineering and Applications,\n  Volume IX, Issue II, pp. 17-27, February 2015", "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Digital Forgeries though not visibly identifiable to human perception it\nmay alter or meddle with underlying natural statistics of digital content.\nTampering involves fiddling with video content in order to cause damage or make\nunauthorized alteration/modification. Tampering detection in video is\ncumbersome compared to image when considering the properties of the video.\nTampering impacts need to be studied and the applied technique/method is used\nto establish the factual information for legal course in judiciary. In this\npaper we give an overview of the prior literature and challenges involved in\nvideo forgery detection where passive approach is found.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 07:17:46 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["N.", "Sowmya K.", ""], ["Chennamma", "H. R.", ""]]}, {"id": "1503.00848", "submitter": "Jordi Pont-Tuset", "authors": "Jordi Pont-Tuset, Pablo Arbelaez, Jonathan T. Barron, Ferran Marques,\n  Jitendra Malik", "title": "Multiscale Combinatorial Grouping for Image Segmentation and Object\n  Proposal Generation", "comments": null, "journal-ref": null, "doi": "10.1109/TPAMI.2016.2537320", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a unified approach for bottom-up hierarchical image segmentation\nand object proposal generation for recognition, called Multiscale Combinatorial\nGrouping (MCG). For this purpose, we first develop a fast normalized cuts\nalgorithm. We then propose a high-performance hierarchical segmenter that makes\neffective use of multiscale information. Finally, we propose a grouping\nstrategy that combines our multiscale regions into highly-accurate object\nproposals by exploring efficiently their combinatorial space. We also present\nSingle-scale Combinatorial Grouping (SCG), a faster version of MCG that\nproduces competitive proposals in under five second per image. We conduct an\nextensive and comprehensive empirical validation on the BSDS500, SegVOC12, SBD,\nand COCO datasets, showing that MCG produces state-of-the-art contours,\nhierarchical regions, and object proposals.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 07:58:22 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2015 12:00:33 GMT"}, {"version": "v3", "created": "Mon, 29 Jun 2015 17:57:06 GMT"}, {"version": "v4", "created": "Tue, 1 Mar 2016 09:00:09 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Pont-Tuset", "Jordi", ""], ["Arbelaez", "Pablo", ""], ["Barron", "Jonathan T.", ""], ["Marques", "Ferran", ""], ["Malik", "Jitendra", ""]]}, {"id": "1503.00949", "submitter": "Ramazan Gokberk Cinbis", "authors": "Ramazan Gokberk Cinbis, Jakob Verbeek, Cordelia Schmid", "title": "Weakly Supervised Object Localization with Multi-fold Multiple Instance\n  Learning", "comments": "To appear in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)", "journal-ref": null, "doi": "10.1109/TPAMI.2016.2535231", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object category localization is a challenging problem in computer vision.\nStandard supervised training requires bounding box annotations of object\ninstances. This time-consuming annotation process is sidestepped in weakly\nsupervised learning. In this case, the supervised information is restricted to\nbinary labels that indicate the absence/presence of object instances in the\nimage, without their locations. We follow a multiple-instance learning approach\nthat iteratively trains the detector and infers the object locations in the\npositive training images. Our main contribution is a multi-fold multiple\ninstance learning procedure, which prevents training from prematurely locking\nonto erroneous object locations. This procedure is particularly important when\nusing high-dimensional representations, such as Fisher vectors and\nconvolutional neural network features. We also propose a window refinement\nmethod, which improves the localization accuracy by incorporating an objectness\nprior. We present a detailed experimental evaluation using the PASCAL VOC 2007\ndataset, which verifies the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 14:06:02 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2015 09:58:39 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2016 20:26:43 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Cinbis", "Ramazan Gokberk", ""], ["Verbeek", "Jakob", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1503.00992", "submitter": "Jean-Marie Mirebeau", "authors": "Jean-Marie Mirebeau (CEREMADE), J\\'er\\^ome Fehrenbach (IMT), Laurent\n  Risser (IMT), Shaza Tobji (IMT)", "title": "Anisotropic Diffusion in ITK", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anisotropic Non-Linear Diffusion is a powerful image processing technique,\nwhich allows to simultaneously remove the noise and enhance sharp features in\ntwo or three dimensional images. Anisotropic Diffusion is understood here in\nthe sense of Weickert, meaning that diffusion tensors are anisotropic and\nreflect the local orientation of image features. This is in contrast with the\nnon-linear diffusion filter of Perona and Malik, which only involves scalar\ndiffusion coefficients, in other words isotropic diffusion tensors. In this\npaper, we present an anisotropic non-linear diffusion technique we implemented\nin ITK. This technique is based on a recent adaptive scheme making the\ndiffusion stable and requiring limited numerical resources. (See supplementary\ndata.)\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 16:17:16 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Mirebeau", "Jean-Marie", "", "CEREMADE"], ["Fehrenbach", "J\u00e9r\u00f4me", "", "IMT"], ["Risser", "Laurent", "", "IMT"], ["Tobji", "Shaza", "", "IMT"]]}, {"id": "1503.01070", "submitter": "Atousa Torabi", "authors": "Atousa Torabi, Christopher Pal, Hugo Larochelle, Aaron Courville", "title": "Using Descriptive Video Services to Create a Large Data Source for Video\n  Annotation Research", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a dataset of video annotated with high quality\nnatural language phrases describing the visual content in a given segment of\ntime. Our dataset is based on the Descriptive Video Service (DVS) that is now\nencoded on many digital media products such as DVDs. DVS is an audio narration\ndescribing the visual elements and actions in a movie for the visually\nimpaired. It is temporally aligned with the movie and mixed with the original\nmovie soundtrack. We describe an automatic DVS segmentation and alignment\nmethod for movies, that enables us to scale up the collection of a DVS-derived\ndataset with minimal human intervention. Using this method, we have collected\nthe largest DVS-derived dataset for video description of which we are aware.\nOur dataset currently includes over 84.6 hours of paired video/sentences from\n92 DVDs and is growing.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 19:22:01 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Torabi", "Atousa", ""], ["Pal", "Christopher", ""], ["Larochelle", "Hugo", ""], ["Courville", "Aaron", ""]]}, {"id": "1503.01138", "submitter": "Zhangyang Wang", "authors": "Zhangyang Wang, Yingzhen Yang, Zhaowen Wang, Shiyu Chang, Jianchao\n  Yang, Thomas S. Huang", "title": "Learning Super-Resolution Jointly from External and Internal Examples", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2015.2462113", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image super-resolution (SR) aims to estimate a high-resolution (HR)\nimage from a lowresolution (LR) input. Image priors are commonly learned to\nregularize the otherwise seriously ill-posed SR problem, either using external\nLR-HR pairs or internal similar patterns. We propose joint SR to adaptively\ncombine the advantages of both external and internal SR methods. We define two\nloss functions using sparse coding based external examples, and epitomic\nmatching based on internal examples, as well as a corresponding adaptive weight\nto automatically balance their contributions according to their reconstruction\nerrors. Extensive SR results demonstrate the effectiveness of the proposed\nmethod over the existing state-of-the-art methods, and is also verified by our\nsubjective evaluation study.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 21:48:48 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2015 00:36:46 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2015 07:19:19 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Wang", "Zhangyang", ""], ["Yang", "Yingzhen", ""], ["Wang", "Zhaowen", ""], ["Chang", "Shiyu", ""], ["Yang", "Jianchao", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1503.01224", "submitter": "Chunhua Shen", "authors": "Peng Wang, Yuanzhouhan Cao, Chunhua Shen, Lingqiao Liu, Heng Tao Shen", "title": "Temporal Pyramid Pooling Based Convolutional Neural Networks for Action\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encouraged by the success of Convolutional Neural Networks (CNNs) in image\nclassification, recently much effort is spent on applying CNNs to video based\naction recognition problems. One challenge is that video contains a varying\nnumber of frames which is incompatible to the standard input format of CNNs.\nExisting methods handle this issue either by directly sampling a fixed number\nof frames or bypassing this issue by introducing a 3D convolutional layer which\nconducts convolution in spatial-temporal domain.\n  To solve this issue, here we propose a novel network structure which allows\nan arbitrary number of frames as the network input. The key of our solution is\nto introduce a module consisting of an encoding layer and a temporal pyramid\npooling layer. The encoding layer maps the activation from previous layers to a\nfeature vector suitable for pooling while the temporal pyramid pooling layer\nconverts multiple frame-level activations into a fixed-length video-level\nrepresentation. In addition, we adopt a feature concatenation layer which\ncombines appearance information and motion information. Compared with the frame\nsampling strategy, our method avoids the risk of missing any important frames.\nCompared with the 3D convolutional method which requires a huge video dataset\nfor network training, our model can be learned on a small target dataset\nbecause we can leverage the off-the-shelf image-level CNN for model parameter\ninitialization. Experiments on two challenging datasets, Hollywood2 and HMDB51,\ndemonstrate that our method achieves superior performance over state-of-the-art\nmethods while requiring much fewer training data.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 05:18:18 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2015 12:18:46 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Wang", "Peng", ""], ["Cao", "Yuanzhouhan", ""], ["Shen", "Chunhua", ""], ["Liu", "Lingqiao", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1503.01228", "submitter": "Kui Tang", "authors": "Kui Tang, Nicholas Ruozzi, David Belanger, Tony Jebara", "title": "Bethe Learning of Conditional Random Fields via MAP Decoding", "comments": "19 pages (9 supplementary), 10 figures (3 supplementary)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning tasks can be formulated in terms of predicting\nstructured outputs. In frameworks such as the structured support vector machine\n(SVM-Struct) and the structured perceptron, discriminative functions are\nlearned by iteratively applying efficient maximum a posteriori (MAP) decoding.\nHowever, maximum likelihood estimation (MLE) of probabilistic models over these\nsame structured spaces requires computing partition functions, which is\ngenerally intractable. This paper presents a method for learning discrete\nexponential family models using the Bethe approximation to the MLE. Remarkably,\nthis problem also reduces to iterative (MAP) decoding. This connection emerges\nby combining the Bethe approximation with a Frank-Wolfe (FW) algorithm on a\nconvex dual objective which circumvents the intractable partition function. The\nresult is a new single loop algorithm MLE-Struct, which is substantially more\nefficient than previous double-loop methods for approximate maximum likelihood\nestimation. Our algorithm outperforms existing methods in experiments involving\nimage segmentation, matching problems from vision, and a new dataset of\nuniversity roommate assignments.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 05:41:29 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Tang", "Kui", ""], ["Ruozzi", "Nicholas", ""], ["Belanger", "David", ""], ["Jebara", "Tony", ""]]}, {"id": "1503.01313", "submitter": "Matej Kristan", "authors": "Matej Kristan, Jiri Matas, Ales Leonardis, Tomas Vojir, Roman\n  Pflugfelder, Gustavo Fernandez, Georg Nebehay, Fatih Porikli and Luka Cehovin", "title": "A Novel Performance Evaluation Methodology for Single-Target Trackers", "comments": "Final version (Accepted), IEEE Pattern Analysis and Machine\n  Intelligence, 2016", "journal-ref": null, "doi": "10.1109/TPAMI.2016.2516982", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of single-target tracker performance\nevaluation. We consider the performance measures, the dataset and the\nevaluation system to be the most important components of tracker evaluation and\npropose requirements for each of them. The requirements are the basis of a new\nevaluation methodology that aims at a simple and easily interpretable tracker\ncomparison. The ranking-based methodology addresses tracker equivalence in\nterms of statistical significance and practical differences. A fully-annotated\ndataset with per-frame annotations with several visual attributes is\nintroduced. The diversity of its visual properties is maximized in a novel way\nby clustering a large number of videos according to their visual attributes.\nThis makes it the most sophistically constructed and annotated dataset to date.\nA multi-platform evaluation system allowing easy integration of third-party\ntrackers is presented as well. The proposed evaluation methodology was tested\non the VOT2014 challenge on the new dataset and 38 trackers, making it the\nlargest benchmark to date. Most of the tested trackers are indeed\nstate-of-the-art since they outperform the standard baselines, resulting in a\nhighly-challenging benchmark. An exhaustive analysis of the dataset from the\nperspective of tracking difficulty is carried out. To facilitate tracker\ncomparison a new performance visualization technique is proposed.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 14:12:17 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2015 14:00:23 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2016 15:27:11 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Kristan", "Matej", ""], ["Matas", "Jiri", ""], ["Leonardis", "Ales", ""], ["Vojir", "Tomas", ""], ["Pflugfelder", "Roman", ""], ["Fernandez", "Gustavo", ""], ["Nebehay", "Georg", ""], ["Porikli", "Fatih", ""], ["Cehovin", "Luka", ""]]}, {"id": "1503.01393", "submitter": "Mete Ozay", "authors": "Mete Ozay, Krzysztof Walas, Ales Leonardis", "title": "A Hierarchical Approach for Joint Multi-view Object Pose Estimation and\n  Categorization", "comments": "7 Figures", "journal-ref": "Proceedings of IEEE International Conference on Robotics and\n  Automation (ICRA), pp. 5480 - 5487, Hong Kong, 2014", "doi": "10.1109/ICRA.2014.6907665", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a joint object pose estimation and categorization approach which\nextracts information about object poses and categories from the object parts\nand compositions constructed at different layers of a hierarchical object\nrepresentation algorithm, namely Learned Hierarchy of Parts (LHOP). In the\nproposed approach, we first employ the LHOP to learn hierarchical part\nlibraries which represent entity parts and compositions across different object\ncategories and views. Then, we extract statistical and geometric features from\nthe part realizations of the objects in the images in order to represent the\ninformation about object pose and category at each different layer of the\nhierarchy. Unlike the traditional approaches which consider specific layers of\nthe hierarchies in order to extract information to perform specific tasks, we\ncombine the information extracted at different layers to solve a joint object\npose estimation and categorization problem using distributed optimization\nalgorithms. We examine the proposed generative-discriminative learning approach\nand the algorithms on two benchmark 2-D multi-view image datasets. The proposed\napproach and the algorithms outperform state-of-the-art classification,\nregression and feature extraction algorithms. In addition, the experimental\nresults shed light on the relationship between object categorization, pose\nestimation and the part realizations observed at different layers of the\nhierarchy.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 17:17:48 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Ozay", "Mete", ""], ["Walas", "Krzysztof", ""], ["Leonardis", "Ales", ""]]}, {"id": "1503.01444", "submitter": "Tae-Hyun Oh", "authors": "Tae-Hyun Oh, Yu-Wing Tai, Jean-Charles Bazin, Hyeongwoo Kim, In So\n  Kweon", "title": "Partial Sum Minimization of Singular Values in Robust PCA: Algorithm and\n  Applications", "comments": "Accepted in Transactions on Pattern Analysis and Machine Intelligence\n  (TPAMI). To appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust Principal Component Analysis (RPCA) via rank minimization is a\npowerful tool for recovering underlying low-rank structure of clean data\ncorrupted with sparse noise/outliers. In many low-level vision problems, not\nonly it is known that the underlying structure of clean data is low-rank, but\nthe exact rank of clean data is also known. Yet, when applying conventional\nrank minimization for those problems, the objective function is formulated in a\nway that does not fully utilize a priori target rank information about the\nproblems. This observation motivates us to investigate whether there is a\nbetter alternative solution when using rank minimization. In this paper,\ninstead of minimizing the nuclear norm, we propose to minimize the partial sum\nof singular values, which implicitly encourages the target rank constraint. Our\nexperimental analyses show that, when the number of samples is deficient, our\napproach leads to a higher success rate than conventional rank minimization,\nwhile the solutions obtained by the two approaches are almost identical when\nthe number of samples is more than sufficient. We apply our approach to various\nlow-level vision problems, e.g. high dynamic range imaging, motion edge\ndetection, photometric stereo, image alignment and recovery, and show that our\nresults outperform those obtained by the conventional nuclear norm rank\nminimization method.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 20:14:35 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2015 12:51:08 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Oh", "Tae-Hyun", ""], ["Tai", "Yu-Wing", ""], ["Bazin", "Jean-Charles", ""], ["Kim", "Hyeongwoo", ""], ["Kweon", "In So", ""]]}, {"id": "1503.01508", "submitter": "Xiangxin Zhu", "authors": "Xiangxin Zhu, Carl Vondrick, Charless Fowlkes, Deva Ramanan", "title": "Do We Need More Training Data?", "comments": null, "journal-ref": null, "doi": "10.1007/s11263-015-0812-2", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets for training object recognition systems are steadily increasing in\nsize. This paper investigates the question of whether existing detectors will\ncontinue to improve as data grows, or saturate in performance due to limited\nmodel complexity and the Bayes risk associated with the feature spaces in which\nthey operate. We focus on the popular paradigm of discriminatively trained\ntemplates defined on oriented gradient features. We investigate the performance\nof mixtures of templates as the number of mixture components and the amount of\ntraining data grows. Surprisingly, even with proper treatment of regularization\nand \"outliers\", the performance of classic mixture models appears to saturate\nquickly ($\\sim$10 templates and $\\sim$100 positive training examples per\ntemplate). This is not a limitation of the feature space as compositional\nmixtures that share template parameters via parts and that can synthesize new\ntemplates not encountered during training yield significantly better\nperformance. Based on our analysis, we conjecture that the greatest gains in\ndetection performance will continue to derive from improved representations and\nlearning algorithms that can make efficient use of large datasets.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 01:51:12 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Zhu", "Xiangxin", ""], ["Vondrick", "Carl", ""], ["Fowlkes", "Charless", ""], ["Ramanan", "Deva", ""]]}, {"id": "1503.01521", "submitter": "Liwen Zhang", "authors": "Liwen Zhang, Subhransu Maji, Ryota Tomioka", "title": "Jointly Learning Multiple Measures of Similarities from Triplet\n  Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity between objects is multi-faceted and it can be easier for human\nannotators to measure it when the focus is on a specific aspect. We consider\nthe problem of mapping objects into view-specific embeddings where the distance\nbetween them is consistent with the similarity comparisons of the form \"from\nthe t-th view, object A is more similar to B than to C\". Our framework jointly\nlearns view-specific embeddings exploiting correlations between views.\nExperiments on a number of datasets, including one of multi-view crowdsourced\ncomparison on bird images, show the proposed method achieves lower triplet\ngeneralization error when compared to both learning embeddings independently\nfor each view and all views pooled into one view. Our method can also be used\nto learn multiple measures of similarity over input features taking class\nlabels into account and compares favorably to existing approaches for\nmulti-task metric learning on the ISOLET dataset.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 02:57:19 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2015 20:09:09 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2015 21:42:55 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Zhang", "Liwen", ""], ["Maji", "Subhransu", ""], ["Tomioka", "Ryota", ""]]}, {"id": "1503.01531", "submitter": "Tomohiko Mizutani", "authors": "Tomohiko Mizutani", "title": "Spectral Clustering by Ellipsoid and Its Connection to Separable\n  Nonnegative Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a variant of the normalized cut algorithm for spectral\nclustering. Although the normalized cut algorithm applies the K-means algorithm\nto the eigenvectors of a normalized graph Laplacian for finding clusters, our\nalgorithm instead uses a minimum volume enclosing ellipsoid for them. We show\nthat the algorithm shares similarity with the ellipsoidal rounding algorithm\nfor separable nonnegative matrix factorization. Our theoretical insight implies\nthat the algorithm can serve as a bridge between spectral clustering and\nseparable NMF. The K-means algorithm has the issues in that the choice of\ninitial points affects the construction of clusters and certain choices result\nin poor clustering performance. The normalized cut algorithm inherits these\nissues since K-means is incorporated in it, whereas the algorithm proposed here\ndoes not. An empirical study is presented to examine the performance of the\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 04:07:46 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Mizutani", "Tomohiko", ""]]}, {"id": "1503.01532", "submitter": "Heechul Jung", "authors": "Heechul Jung, Sihaeng Lee, Sunjeong Park, Injae Lee, Chunghyun Ahn,\n  Junmo Kim", "title": "Deep Temporal Appearance-Geometry Network for Facial Expression\n  Recognition", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal information can provide useful features for recognizing facial\nexpressions. However, to manually design useful features requires a lot of\neffort. In this paper, to reduce this effort, a deep learning technique which\nis regarded as a tool to automatically extract useful features from raw data,\nis adopted. Our deep network is based on two different models. The first deep\nnetwork extracts temporal geometry features from temporal facial landmark\npoints, while the other deep network extracts temporal appearance features from\nimage sequences . These two models are combined in order to boost the\nperformance of the facial expression recognition. Through several experiments,\nwe showed that the two models cooperate with each other. As a result, we\nachieved superior performance to other state-of-the-art methods in CK+ and\nOulu-CASIA databases. Furthermore, one of the main contributions of this paper\nis that our deep network catches the facial action points automatically.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 04:07:56 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Jung", "Heechul", ""], ["Lee", "Sihaeng", ""], ["Park", "Sunjeong", ""], ["Lee", "Injae", ""], ["Ahn", "Chunghyun", ""], ["Kim", "Junmo", ""]]}, {"id": "1503.01538", "submitter": "Natalia Bilenko", "authors": "Natalia Y. Bilenko and Jack L. Gallant", "title": "Pyrcca: regularized kernel canonical correlation analysis in Python and\n  its applications to neuroimaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical correlation analysis (CCA) is a valuable method for interpreting\ncross-covariance across related datasets of different dimensionality. There are\nmany potential applications of CCA to neuroimaging data analysis. For instance,\nCCA can be used for finding functional similarities across fMRI datasets\ncollected from multiple subjects without resampling individual datasets to a\ntemplate anatomy. In this paper, we introduce Pyrcca, an open-source Python\nmodule for executing CCA between two or more datasets. Pyrcca can be used to\nimplement CCA with or without regularization, and with or without linear or a\nGaussian kernelization of the datasets. We demonstrate an application of CCA\nimplemented with Pyrcca to neuroimaging data analysis. We use CCA to find a\ndata-driven set of functional response patterns that are similar across\nindividual subjects in a natural movie experiment. We then demonstrate how this\nset of response patterns discovered by CCA can be used to accurately predict\nsubject responses to novel natural movie stimuli.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 04:57:22 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Bilenko", "Natalia Y.", ""], ["Gallant", "Jack L.", ""]]}, {"id": "1503.01543", "submitter": "Chunhua Shen", "authors": "Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel", "title": "Learning to rank in person re-identification with metric ensembles", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an effective structured learning based approach to the problem of\nperson re-identification which outperforms the current state-of-the-art on most\nbenchmark data sets evaluated. Our framework is built on the basis of multiple\nlow-level hand-crafted and high-level visual features. We then formulate two\noptimization algorithms, which directly optimize evaluation measures commonly\nused in person re-identification, also known as the Cumulative Matching\nCharacteristic (CMC) curve. Our new approach is practical to many real-world\nsurveillance applications as the re-identification performance can be\nconcentrated in the range of most practical importance. The combination of\nthese factors leads to a person re-identification system which outperforms most\nexisting algorithms. More importantly, we advance state-of-the-art results on\nperson re-identification by improving the rank-$1$ recognition rates from\n$40\\%$ to $50\\%$ on the iLIDS benchmark, $16\\%$ to $18\\%$ on the PRID2011\nbenchmark, $43\\%$ to $46\\%$ on the VIPeR benchmark, $34\\%$ to $53\\%$ on the\nCUHK01 benchmark and $21\\%$ to $62\\%$ on the CUHK03 benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 05:25:57 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Paisitkriangkrai", "Sakrapee", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1503.01557", "submitter": "Fumin Shen Dr.", "authors": "Fumin Shen, Chunhua Shen, Wei Liu and Heng Tao Shen", "title": "Supervised Discrete Hashing", "comments": "This paper has been withdrawn by the authour since the algorithm is\n  being used for patent application", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper has been withdrawn by the authour.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 07:06:02 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2015 08:34:52 GMT"}, {"version": "v3", "created": "Sun, 19 Apr 2015 02:42:16 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Shen", "Fumin", ""], ["Shen", "Chunhua", ""], ["Liu", "Wei", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1503.01558", "submitter": "Jonathan Malmaud", "authors": "Jonathan Malmaud, Jonathan Huang, Vivek Rathod, Nick Johnston, Andrew\n  Rabinovich, and Kevin Murphy", "title": "What's Cookin'? Interpreting Cooking Videos using Text, Speech and\n  Vision", "comments": "To appear in NAACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for aligning a sequence of instructions to a video\nof someone carrying out a task. In particular, we focus on the cooking domain,\nwhere the instructions correspond to the recipe. Our technique relies on an HMM\nto align the recipe steps to the (automatically generated) speech transcript.\nWe then refine this alignment using a state-of-the-art visual food detector,\nbased on a deep convolutional neural network. We show that our technique\noutperforms simpler techniques based on keyword spotting. It also enables\ninteresting applications, such as automatically illustrating recipes with\nkeyframes, and searching within a video for events of interest.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 07:07:48 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2015 04:11:49 GMT"}, {"version": "v3", "created": "Fri, 13 Mar 2015 18:55:22 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Malmaud", "Jonathan", ""], ["Huang", "Jonathan", ""], ["Rathod", "Vivek", ""], ["Johnston", "Nick", ""], ["Rabinovich", "Andrew", ""], ["Murphy", "Kevin", ""]]}, {"id": "1503.01563", "submitter": "Karri Sesh-Kumar", "authors": "K. S. Sesh Kumar (LIENS,INRIA Paris - Rocquencourt), Alvaro Barbero,\n  Stefanie Jegelka (MIT), Suvrit Sra (MIT), Francis Bach (LIENS,INRIA Paris -\n  Rocquencourt)", "title": "Convex Optimization for Parallel Energy Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy minimization has been an intensely studied core problem in computer\nvision. With growing image sizes (2D and 3D), it is now highly desirable to run\nenergy minimization algorithms in parallel. But many existing algorithms, in\nparticular, some efficient combinatorial algorithms, are difficult to\npar-allelize. By exploiting results from convex and submodular theory, we\nreformulate the quadratic energy minimization problem as a total variation\ndenoising problem, which, when viewed geometrically, enables the use of\nprojection and reflection based convex methods. The resulting min-cut algorithm\n(and code) is conceptually very simple, and solves a sequence of TV denoising\nproblems. We perform an extensive empirical evaluation comparing\nstate-of-the-art combinatorial algorithms and convex optimization techniques.\nOn small problems the iterative convex methods match the combinatorial max-flow\nalgorithms, while on larger problems they offer other flexibility and important\ngains: (a) their memory footprint is small; (b) their straightforward\nparallelizability fits multi-core platforms; (c) they can easily be\nwarm-started; and (d) they quickly reach approximately good solutions, thereby\nenabling faster \"inexact\" solutions. A key consequence of our approach based on\nsubmodularity and convexity is that it is allows to combine any arbitrary\ncombinatorial or convex methods as subroutines, which allows one to obtain\nhybrid combinatorial and convex optimization algorithms that benefit from the\nstrengths of both.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 07:40:56 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Kumar", "K. S. Sesh", "", "LIENS,INRIA Paris - Rocquencourt"], ["Barbero", "Alvaro", "", "MIT"], ["Jegelka", "Stefanie", "", "MIT"], ["Sra", "Suvrit", "", "MIT"], ["Bach", "Francis", "", "LIENS,INRIA Paris -\n  Rocquencourt"]]}, {"id": "1503.01626", "submitter": "Zohar Nussinov", "authors": "Z. Nussinov, P. Ronhovde, Dandan Hu, S. Chakrabarty, M. Sahu, Bo Sun,\n  N. A. Mauro, K. K. Sahu", "title": "Inference of hidden structures in complex physical systems by\n  multi-scale clustering", "comments": "25 pages, 16 Figures; a review of earlier works", "journal-ref": null, "doi": "10.1007/978-3-319-23871-5_6", "report-no": null, "categories": "cond-mat.mtrl-sci cond-mat.stat-mech cs.CV physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey the application of a relatively new branch of statistical\nphysics--\"community detection\"-- to data mining. In particular, we focus on the\ndiagnosis of materials and automated image segmentation. Community detection\ndescribes the quest of partitioning a complex system involving many elements\ninto optimally decoupled subsets or communities of such elements. We review a\nmultiresolution variant which is used to ascertain structures at different\nspatial and temporal scales. Significant patterns are obtained by examining the\ncorrelations between different independent solvers. Similar to other\ncombinatorial optimization problems in the NP complexity class, community\ndetection exhibits several phases. Typically, illuminating orders are revealed\nby choosing parameters that lead to extremal information theory correlations.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 12:40:38 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2016 18:19:21 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Nussinov", "Z.", ""], ["Ronhovde", "P.", ""], ["Hu", "Dandan", ""], ["Chakrabarty", "S.", ""], ["Sahu", "M.", ""], ["Sun", "Bo", ""], ["Mauro", "N. A.", ""], ["Sahu", "K. K.", ""]]}, {"id": "1503.01640", "submitter": "Jifeng Dai", "authors": "Jifeng Dai, Kaiming He, Jian Sun", "title": "BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks\n  for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent leading approaches to semantic segmentation rely on deep convolutional\nnetworks trained with human-annotated, pixel-level segmentation masks. Such\npixel-accurate supervision demands expensive labeling effort and limits the\nperformance of deep networks that usually benefit from more training data. In\nthis paper, we propose a method that achieves competitive accuracy but only\nrequires easily obtained bounding box annotations. The basic idea is to iterate\nbetween automatically generating region proposals and training convolutional\nnetworks. These two steps gradually recover segmentation masks for improving\nthe networks, and vise versa. Our method, called BoxSup, produces competitive\nresults supervised by boxes only, on par with strong baselines fully supervised\nby masks under the same setting. By leveraging a large amount of bounding\nboxes, BoxSup further unleashes the power of deep convolutional networks and\nyields state-of-the-art results on PASCAL VOC 2012 and PASCAL-CONTEXT.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 14:06:53 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 09:00:40 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Dai", "Jifeng", ""], ["He", "Kaiming", ""], ["Sun", "Jian", ""]]}, {"id": "1503.01646", "submitter": "Sahar Hooshmand", "authors": "Sahar Hooshmand, Ali Jamali Avilaq, Amir Hossein Rezaie", "title": "Video-Based Facial Expression Recognition Using Local Directional Binary\n  Pattern", "comments": "9 pages, 8 figures, 3 tables in IJASCSE 2015 volume 4 issue 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic facial expression analysis is a challenging issue and influenced so\nmany areas such as human computer interaction. Due to the uncertainties of the\nlight intensity and light direction, the face gray shades are uneven and the\nexpression recognition rate under simple Local Binary Pattern is not ideal and\npromising. In this paper we propose two state-of-the-art descriptors for\nperson-independent facial expression recognition. First the face regions of the\nwhole images in a video sequence are modeled with Volume Local Directional\nBinary pattern (VLDBP), which is an extended version of the LDBP operator,\nincorporating movement and appearance together. To make the survey\ncomputationally simple and easy to expand, only the co-occurrences of the Local\nDirectional Binary Pattern on three orthogonal planes (LDBP-TOP) are debated.\nAfter extracting the feature vectors the K-Nearest Neighbor classifier was used\nto recognize the expressions. The proposed methods are applied to the videos of\nthe Extended Cohn-Kanade database (CK+) and the experimental outcomes\ndemonstrate that the offered techniques achieve more accuracy in comparison\nwith the classic and traditional algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 14:33:58 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Hooshmand", "Sahar", ""], ["Avilaq", "Ali Jamali", ""], ["Rezaie", "Amir Hossein", ""]]}, {"id": "1503.01657", "submitter": "Rui  Zeng", "authors": "Rui Zeng, Jiasong Wu, Zhuhong Shao, Yang Chen, Lotfi Senhadji,\n  Huazhong Shu", "title": "Color Image Classification via Quaternion Principal Component Analysis\n  Network", "comments": "9 figures,5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Principal Component Analysis Network (PCANet), which is one of the\nrecently proposed deep learning architectures, achieves the state-of-the-art\nclassification accuracy in various databases. However, the performance of\nPCANet may be degraded when dealing with color images. In this paper, a\nQuaternion Principal Component Analysis Network (QPCANet), which is an\nextension of PCANet, is proposed for color images classification. Compared to\nPCANet, the proposed QPCANet takes into account the spatial distribution\ninformation of color images and ensures larger amount of intra-class invariance\nof color images. Experiments conducted on different color image datasets such\nas Caltech-101, UC Merced Land Use, Georgia Tech face and CURet have revealed\nthat the proposed QPCANet achieves higher classification accuracy than PCANet.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 15:12:28 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Zeng", "Rui", ""], ["Wu", "Jiasong", ""], ["Shao", "Zhuhong", ""], ["Chen", "Yang", ""], ["Senhadji", "Lotfi", ""], ["Shu", "Huazhong", ""]]}, {"id": "1503.01800", "submitter": "Samira Ebrahimi Kahou", "authors": "Samira Ebrahimi Kahou, Xavier Bouthillier, Pascal Lamblin, Caglar\n  Gulcehre, Vincent Michalski, Kishore Konda, S\\'ebastien Jean, Pierre\n  Froumenty, Yann Dauphin, Nicolas Boulanger-Lewandowski, Raul Chandias\n  Ferrari, Mehdi Mirza, David Warde-Farley, Aaron Courville, Pascal Vincent,\n  Roland Memisevic, Christopher Pal, Yoshua Bengio", "title": "EmoNets: Multimodal deep learning approaches for emotion recognition in\n  video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of the emotion recognition in the wild (EmotiW) Challenge is to\nassign one of seven emotions to short video clips extracted from Hollywood\nstyle movies. The videos depict acted-out emotions under realistic conditions\nwith a large degree of variation in attributes such as pose and illumination,\nmaking it worthwhile to explore approaches which consider combinations of\nfeatures from multiple modalities for label assignment. In this paper we\npresent our approach to learning several specialist models using deep learning\ntechniques, each focusing on one modality. Among these are a convolutional\nneural network, focusing on capturing visual information in detected faces, a\ndeep belief net focusing on the representation of the audio stream, a K-Means\nbased \"bag-of-mouths\" model, which extracts visual features around the mouth\nregion and a relational autoencoder, which addresses spatio-temporal aspects of\nvideos. We explore multiple methods for the combination of cues from these\nmodalities into one common classifier. This achieves a considerably greater\naccuracy than predictions from our strongest single-modality classifier. Our\nmethod was the winning submission in the 2013 EmotiW challenge and achieved a\ntest set accuracy of 47.67% on the 2014 dataset.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 22:03:26 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2015 00:55:02 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Kahou", "Samira Ebrahimi", ""], ["Bouthillier", "Xavier", ""], ["Lamblin", "Pascal", ""], ["Gulcehre", "Caglar", ""], ["Michalski", "Vincent", ""], ["Konda", "Kishore", ""], ["Jean", "S\u00e9bastien", ""], ["Froumenty", "Pierre", ""], ["Dauphin", "Yann", ""], ["Boulanger-Lewandowski", "Nicolas", ""], ["Ferrari", "Raul Chandias", ""], ["Mirza", "Mehdi", ""], ["Warde-Farley", "David", ""], ["Courville", "Aaron", ""], ["Vincent", "Pascal", ""], ["Memisevic", "Roland", ""], ["Pal", "Christopher", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1503.01804", "submitter": "Achuta Kadambi", "authors": "Achuta Kadambi, Vage Taamazyan, Suren Jayasuriya, Ramesh Raskar", "title": "Frequency Domain TOF: Encoding Object Depth in Modulation Frequency", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time of flight cameras may emerge as the 3-D sensor of choice. Today, time of\nflight sensors use phase-based sampling, where the phase delay between emitted\nand received, high-frequency signals encodes distance. In this paper, we\npresent a new time of flight architecture that relies only on frequency---we\nrefer to this technique as frequency-domain time of flight (FD-TOF). Inspired\nby optical coherence tomography (OCT), FD-TOF excels when frequency bandwidth\nis high. With the increasing frequency of TOF sensors, new challenges to time\nof flight sensing continue to emerge. At high frequencies, FD-TOF offers\nseveral potential benefits over phase-based time of flight methods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 22:15:33 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Kadambi", "Achuta", ""], ["Taamazyan", "Vage", ""], ["Jayasuriya", "Suren", ""], ["Raskar", "Ramesh", ""]]}, {"id": "1503.01820", "submitter": "Ninghang Hu", "authors": "Ninghang Hu, Gwenn Englebienne, Zhongyu Lou, and Ben Kr\\\"ose", "title": "Latent Hierarchical Model for Activity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel hierarchical model for human activity recognition. In\ncontrast to approaches that successively recognize actions and activities, our\napproach jointly models actions and activities in a unified framework, and\ntheir labels are simultaneously predicted. The model is embedded with a latent\nlayer that is able to capture a richer class of contextual information in both\nstate-state and observation-state pairs. Although loops are present in the\nmodel, the model has an overall linear-chain structure, where the exact\ninference is tractable. Therefore, the model is very efficient in both\ninference and learning. The parameters of the graphical model are learned with\na Structured Support Vector Machine (Structured-SVM). A data-driven approach is\nused to initialize the latent variables; therefore, no manual labeling for the\nlatent states is required. The experimental results from using two benchmark\ndatasets show that our model outperforms the state-of-the-art approach, and our\nmodel is computationally more efficient.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 00:05:12 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Hu", "Ninghang", ""], ["Englebienne", "Gwenn", ""], ["Lou", "Zhongyu", ""], ["Kr\u00f6se", "Ben", ""]]}, {"id": "1503.01832", "submitter": "Zhaopeng Cui", "authors": "Zhaopeng Cui, Nianjuan Jiang, Chengzhou Tang and Ping Tan", "title": "Linear Global Translation Estimation with Feature Tracks", "comments": "Changes: 1. Adopt BMVC2015 style; 2. Combine sections 3 and 5; 3.\n  Move \"Evaluation on synthetic data\" out to supplementary file; 4. Divide\n  subsection \"Evaluation on general data\" to subsections \"Experiment on\n  sequential data\" and \"Experiment on unordered Internet data\"; 5. Change Fig.\n  1 and Fig.8; 6. Move Fig. 6 and Fig. 7 to supplementary file; 7 Change some\n  symbols; 8. Correct some typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper derives a novel linear position constraint for cameras seeing a\ncommon scene point, which leads to a direct linear method for global camera\ntranslation estimation. Unlike previous solutions, this method deals with\ncollinear camera motion and weak image association at the same time. The final\nlinear formulation does not involve the coordinates of scene points, which\nmakes it efficient even for large scale data. We solve the linear equation\nbased on $L_1$ norm, which makes our system more robust to outliers in\nessential matrices and feature correspondences. We experiment this method on\nboth sequentially captured images and unordered Internet images. The\nexperiments demonstrate its strength in robustness, accuracy, and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 02:14:14 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2015 01:36:59 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Cui", "Zhaopeng", ""], ["Jiang", "Nianjuan", ""], ["Tang", "Chengzhou", ""], ["Tan", "Ping", ""]]}, {"id": "1503.01868", "submitter": "Yao Wang", "authors": "Wenfei Cao, Yao Wang, Jian Sun, Deyu Meng, Can Yang, Andrzej Cichocki,\n  Zongben Xu", "title": "Total Variation Regularized Tensor RPCA for Background Subtraction from\n  Compressive Measurements", "comments": "To appear in IEEE TIP", "journal-ref": null, "doi": "10.1109/TIP.2016.2579262", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background subtraction has been a fundamental and widely studied task in\nvideo analysis, with a wide range of applications in video surveillance,\nteleconferencing and 3D modeling. Recently, motivated by compressive imaging,\nbackground subtraction from compressive measurements (BSCM) is becoming an\nactive research task in video surveillance. In this paper, we propose a novel\ntensor-based robust PCA (TenRPCA) approach for BSCM by decomposing video frames\ninto backgrounds with spatial-temporal correlations and foregrounds with\nspatio-temporal continuity in a tensor framework. In this approach, we use 3D\ntotal variation (TV) to enhance the spatio-temporal continuity of foregrounds,\nand Tucker decomposition to model the spatio-temporal correlations of video\nbackground. Based on this idea, we design a basic tensor RPCA model over the\nvideo frames, dubbed as the holistic TenRPCA model (H-TenRPCA). To characterize\nthe correlations among the groups of similar 3D patches of video background, we\nfurther design a patch-group-based tensor RPCA model (PG-TenRPCA) by joint\ntensor Tucker decompositions of 3D patch groups for modeling the video\nbackground. Efficient algorithms using alternating direction method of\nmultipliers (ADMM) are developed to solve the proposed models. Extensive\nexperiments on simulated and real-world videos demonstrate the superiority of\nthe proposed approaches over the existing state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 08:00:43 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2015 03:20:34 GMT"}, {"version": "v3", "created": "Mon, 10 Aug 2015 14:50:48 GMT"}, {"version": "v4", "created": "Sun, 5 Jun 2016 17:46:14 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Cao", "Wenfei", ""], ["Wang", "Yao", ""], ["Sun", "Jian", ""], ["Meng", "Deyu", ""], ["Yang", "Can", ""], ["Cichocki", "Andrzej", ""], ["Xu", "Zongben", ""]]}, {"id": "1503.01903", "submitter": "Christine Guillemot Dr", "authors": "A. Mousnier, E. Vural, C. Guillemot", "title": "Partial light field tomographic reconstruction from a fixed-camera focal\n  stack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel approach to partially reconstruct\nhigh-resolution 4D light fields from a stack of differently focused photographs\ntaken with a fixed camera. First, a focus map is calculated from this stack\nusing a simple approach combining gradient detection and region expansion with\ngraph-cut. Then, this focus map is converted into a depth map thanks to the\ncalibration of the camera. We proceed after this with the tomographic\nreconstruction of the epipolar images by back-projecting the focused regions of\nthe scene only. We call it masked back-projection. The angles of\nback-projection are calculated from the depth map. Thanks to the high angular\nresolution we achieve by suitably exploiting the image content captured over a\nlarge interval of focus distances, we are able to render puzzling perspective\nshifts although the original photographs were taken from a single fixed camera\nat a fixed position.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 10:50:40 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Mousnier", "A.", ""], ["Vural", "E.", ""], ["Guillemot", "C.", ""]]}, {"id": "1503.01918", "submitter": "Matej Kristan", "authors": "Matej Kristan, Vildana Sulic, Stanislav Kovacic, Janez Pers", "title": "Fast image-based obstacle detection from unmanned surface vehicles", "comments": "This is an extended version of the ACCV2014 paper [Kristan et al.,\n  2014] submitted to a journal. [Kristan et al., 2014] M. Kristan, J. Pers, V.\n  Sulic, S. Kovacic, A graphical model for rapid obstacle image-map estimation\n  from unmanned surface vehicles, in Proc. Asian Conf. Computer Vision, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obstacle detection plays an important role in unmanned surface vehicles\n(USV). The USVs operate in highly diverse environments in which an obstacle may\nbe a floating piece of wood, a scuba diver, a pier, or a part of a shoreline,\nwhich presents a significant challenge to continuous detection from images\ntaken onboard. This paper addresses the problem of online detection by\nconstrained unsupervised segmentation. To this end, a new graphical model is\nproposed that affords a fast and continuous obstacle image-map estimation from\na single video stream captured onboard a USV. The model accounts for the\nsemantic structure of marine environment as observed from USV by imposing weak\nstructural constraints. A Markov random field framework is adopted and a highly\nefficient algorithm for simultaneous optimization of model parameters and\nsegmentation mask estimation is derived. Our approach does not require\ncomputationally intensive extraction of texture features and comfortably runs\nin real-time. The algorithm is tested on a new, challenging, dataset for\nsegmentation and obstacle detection in marine environments, which is the\nlargest annotated dataset of its kind. Results on this dataset show that our\nmodel outperforms the related approaches, while requiring a fraction of\ncomputational effort.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 11:21:07 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Kristan", "Matej", ""], ["Sulic", "Vildana", ""], ["Kovacic", "Stanislav", ""], ["Pers", "Janez", ""]]}, {"id": "1503.01986", "submitter": "Julien Rabin", "authors": "Julien Rabin and Nicolas Papadakis", "title": "Convex Color Image Segmentation with Optimal Transport Distances", "comments": "A short version of this report has been submitted to the Fifth\n  International Conference on Scale Space and Variational Methods in Computer\n  Vision (SSVM) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This work is about the use of regularized optimal-transport distances for\nconvex, histogram-based image segmentation. In the considered framework, fixed\nexemplar histograms define a prior on the statistical features of the two\nregions in competition. In this paper, we investigate the use of various\ntransport-based cost functions as discrepancy measures and rely on a\nprimal-dual algorithm to solve the obtained convex optimization problem.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 15:19:19 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2015 09:23:19 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Rabin", "Julien", ""], ["Papadakis", "Nicolas", ""]]}, {"id": "1503.01993", "submitter": "Sara Soltani", "authors": "Sara Soltani, Martin S. Andersen and Per Christian Hansen", "title": "Tomographic Image Reconstruction using Training images", "comments": "25 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe and examine an algorithm for tomographic image reconstruction\nwhere prior knowledge about the solution is available in the form of training\nimages. We first construct a nonnegative dictionary based on prototype elements\nfrom the training images; this problem is formulated as a regularized\nnon-negative matrix factorization. Incorporating the dictionary as a prior in a\nconvex reconstruction problem, we then find an approximate solution with a\nsparse representation in the dictionary. The dictionary is applied to\nnon-overlapping patches of the image, which reduces the computational\ncomplexity compared to other algorithms. Computational experiments clarify the\nchoice and interplay of the model parameters and the regularization parameters,\nand we show that in few-projection low-dose settings our algorithm is\ncompetitive with total variation regularization and tends to include more\ntexture and more correct edges.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 15:47:45 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2015 09:34:37 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Soltani", "Sara", ""], ["Andersen", "Martin S.", ""], ["Hansen", "Per Christian", ""]]}, {"id": "1503.02041", "submitter": "Mehrdad Gangeh", "authors": "Mehrdad J. Gangeh and Ali Ghodsi", "title": "On the Invariance of Dictionary Learning and Sparse Representation to\n  Projecting Data to a Discriminative Space", "comments": "We would like to withdraw this paper as it seems that the proof\n  provided in the paper is not including all the cases mentioned", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, it is proved that dictionary learning and sparse\nrepresentation is invariant to a linear transformation. It subsumes the special\ncase of transforming/projecting the data into a discriminative space. This is\nimportant because recently, supervised dictionary learning algorithms have been\nproposed, which suggest to include the category information into the learning\nof dictionary to improve its discriminative power. Among them, there are some\napproaches that propose to learn the dictionary in a discriminative projected\nspace. To this end, two approaches have been proposed: first, assigning the\ndiscriminative basis as the dictionary and second, perform dictionary learning\nin the projected space. Based on the invariance of dictionary learning to any\ntransformation in general, and to a discriminative space in particular, we\nadvocate the first approach.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 19:41:09 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2015 15:25:49 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Gangeh", "Mehrdad J.", ""], ["Ghodsi", "Ali", ""]]}, {"id": "1503.02090", "submitter": "Tales Cesar de Oliveira Imbiriba", "authors": "T. Imbiriba (1), J. C. M. Bermudez (1), C. Richard (2), J.-Y.\n  Tourneret (3) ((1) Federal University of Santa Catarina, Florian\\'opolis, SC,\n  Brazil, (2) Universit\\'e de Nice Sophia-Antipolis, CNRS, Nice, France, (3)\n  University of Toulouse, IRIT-ENSEEIHT, CNRS, Toulouse, France)", "title": "Band selection in RKHS for fast nonlinear unmixing of hyperspectral\n  images", "comments": null, "journal-ref": null, "doi": "10.1109/EUSIPCO.2015.7362664", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The profusion of spectral bands generated by the acquisition process of\nhyperspectral images generally leads to high computational costs. Such\ndifficulties arise in particular with nonlinear unmixing methods, which are\nnaturally more complex than linear ones. This complexity, associated with the\nhigh redundancy of information within the complete set of bands, make the\nsearch of band selection algorithms relevant. With this work, we propose a band\nselection strategy in reproducing kernel Hilbert spaces that allows to\ndrastically reduce the processing time required by nonlinear unmixing\ntechniques. Simulation results show a complexity reduction of two orders of\nmagnitude without compromising unmixing performance.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 21:20:15 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Imbiriba", "T.", ""], ["Bermudez", "J. C. M.", ""], ["Richard", "C.", ""], ["Tourneret", "J. -Y.", ""]]}, {"id": "1503.02136", "submitter": "Khan Munifa Tauqir", "authors": "Waheeda Dhokley, Khan Munifa, Shaikh Nazia and Shaikh Saiqua", "title": "An Improved Image Mosaicing Algorithm for Damaged Documents", "comments": "6 pages, 10 figures", "journal-ref": "IJASCSE, Volume 4, Issue 2, 2015", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a common phenomenon in day to day life; where in some of the document\ngets damaged. Out of several reasons, the main reason for documents getting\ndamaged is shredding by hands. Recovery of such documents is essential. Manual\nrecovery of such damaged document is tedious and time consuming task. In this\npaper, we are describing an algorithm which recovers the original document from\nsuch shredded pieces of the same. In order to implement this, we are using a\nsimple technique called Image Mosaicing. In this technique a complete new image\nis developed using two or more torn fragments. For simplicity of\nimplementation, we are considering only two torn pieces of a document that will\nbe mosaiced together. The successful implementation of this algorithm would\nlead to recovery of important information which in turn would be beneficial in\nvarious fields such as forensic sciences, archival study, etc\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2015 07:38:17 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Dhokley", "Waheeda", ""], ["Munifa", "Khan", ""], ["Nazia", "Shaikh", ""], ["Saiqua", "Shaikh", ""]]}, {"id": "1503.02291", "submitter": "Jan Funke", "authors": "Jan Funke, Francesc Moreno-Noguer, Albert Cardona, Matthew Cook", "title": "TED: A Tolerant Edit Distance for Segmentation Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel error measure to compare a segmentation\nagainst ground truth. This measure, which we call Tolerant Edit Distance (TED),\nis motivated by two observations: (1) Some errors, like small boundary shifts,\nare tolerable in practice. Which errors are tolerable is application dependent\nand should be a parameter of the measure. (2) Non-tolerable errors have to be\ncorrected manually. The time needed to do so should be reflected by the error\nmeasure. Using integer linear programming, the TED finds the minimal weighted\nsum of split and merge errors exceeding a given tolerance criterion, and thus\nprovides a time-to-fix estimate. In contrast to commonly used measures like\nRand index or variation of information, the TED (1) does not count small, but\ntolerable, differences, (2) provides intuitive numbers, (3) gives a time-to-fix\nestimate, and (4) can localize and classify the type of errors. By supporting\nboth isotropic and anisotropic volumes and having a flexible tolerance\ncriterion, the TED can be adapted to different requirements. On example\nsegmentations for 3D neuron segmentation, we demonstrate that the TED is\ncapable of counting topological errors, while ignoring small boundary shifts.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2015 16:11:50 GMT"}, {"version": "v2", "created": "Thu, 28 May 2015 00:06:57 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2016 14:26:10 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Funke", "Jan", ""], ["Moreno-Noguer", "Francesc", ""], ["Cardona", "Albert", ""], ["Cook", "Matthew", ""]]}, {"id": "1503.02302", "submitter": "Michele Piana", "authors": "Richard A Schwartz, Gabriele Torre, Anna Maria Massone, Michele Piana", "title": "DESAT: an SSW tool for SDO/AIA image de-saturation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saturation affects a significant rate of images recorded by the Atmospheric\nImaging Assembly on the Solar Dynamics Observatory. This paper describes a\ncomputational method and a technological pipeline for the de-saturation of such\nimages, based on several mathematical ingredients like Expectation\nMaximization, image correlation and interpolation. An analysis of the\ncomputational properties and demands of the pipeline, together with an\nassessment of its reliability are performed against a set of data recorded from\nthe Feburary 25 2014 flaring event.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2015 17:56:33 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Schwartz", "Richard A", ""], ["Torre", "Gabriele", ""], ["Massone", "Anna Maria", ""], ["Piana", "Michele", ""]]}, {"id": "1503.02318", "submitter": "Arturo Deza", "authors": "Arturo Deza, Devi Parikh", "title": "Understanding Image Virality", "comments": "Pre-print, IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2015", "journal-ref": null, "doi": "10.1109/CVPR.2015.7298791", "report-no": null, "categories": "cs.SI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virality of online content on social networking websites is an important but\nesoteric phenomenon often studied in fields like marketing, psychology and data\nmining. In this paper we study viral images from a computer vision perspective.\nWe introduce three new image datasets from Reddit, and define a virality score\nusing Reddit metadata. We train classifiers with state-of-the-art image\nfeatures to predict virality of individual images, relative virality in pairs\nof images, and the dominant topic of a viral image. We also compare machine\nperformance to human performance on these tasks. We find that computers perform\npoorly with low level features, and high level information is critical for\npredicting virality. We encode semantic information through relative\nattributes. We identify the 5 key visual attributes that correlate with\nvirality. We create an attribute-based characterization of images that can\npredict relative virality with 68.10% accuracy (SVM+Deep Relative Attributes)\n-- better than humans at 60.12%. Finally, we study how human prediction of\nimage virality varies with different `contexts' in which the images are viewed,\nsuch as the influence of neighbouring images, images recently viewed, as well\nas the image title or caption. This work is a first step in understanding the\ncomplex but important phenomenon of image virality. Our datasets and\nannotations will be made publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2015 20:29:28 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2015 18:04:29 GMT"}, {"version": "v3", "created": "Tue, 26 May 2015 16:57:18 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Deza", "Arturo", ""], ["Parikh", "Devi", ""]]}, {"id": "1503.02330", "submitter": "Patrik Huber", "authors": "Patrik Huber, Zhen-Hua Feng, William Christmas, Josef Kittler,\n  Matthias R\\\"atsch", "title": "Fitting 3D Morphable Models using Local Features", "comments": "Submitted to ICIP 2015; 4 pages, 4 figures", "journal-ref": "Proceedings of the IEEE International Conference on Image\n  Processing (ICIP) 2015, pages 1195-1199", "doi": "10.1109/ICIP.2015.7350989", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel fitting method that uses local image\nfeatures to fit a 3D Morphable Model to 2D images. To overcome the obstacle of\noptimising a cost function that contains a non-differentiable feature\nextraction operator, we use a learning-based cascaded regression method that\nlearns the gradient direction from data. The method allows to simultaneously\nsolve for shape and pose parameters. Our method is thoroughly evaluated on\nMorphable Model generated data and first results on real data are presented.\nCompared to traditional fitting methods, which use simple raw features like\npixel colour or edge maps, local features have been shown to be much more\nrobust against variations in imaging conditions. Our approach is unique in that\nwe are the first to use local features to fit a Morphable Model.\n  Because of the speed of our method, it is applicable for realtime\napplications. Our cascaded regression framework is available as an open source\nlibrary (https://github.com/patrikhuber).\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2015 21:57:49 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Huber", "Patrik", ""], ["Feng", "Zhen-Hua", ""], ["Christmas", "William", ""], ["Kittler", "Josef", ""], ["R\u00e4tsch", "Matthias", ""]]}, {"id": "1503.02351", "submitter": "Alexander Schwing", "authors": "Alexander G. Schwing and Raquel Urtasun", "title": "Fully Connected Deep Structured Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks with many layers have recently been shown to\nachieve excellent results on many high-level tasks such as image\nclassification, object detection and more recently also semantic segmentation.\nParticularly for semantic segmentation, a two-stage procedure is often\nemployed. Hereby, convolutional networks are trained to provide good local\npixel-wise features for the second step being traditionally a more global\ngraphical model. In this work we unify this two-stage process into a single\njoint training algorithm. We demonstrate our method on the semantic image\nsegmentation task and show encouraging results on the challenging PASCAL VOC\n2012 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 01:08:00 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Schwing", "Alexander G.", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1503.02391", "submitter": "Xiaodan Liang", "authors": "Xiaodan Liang, Si Liu, Xiaohui Shen, Jianchao Yang, Luoqi Liu, Jian\n  Dong, Liang Lin, Shuicheng Yan", "title": "Deep Human Parsing with Active Template Regression", "comments": "This manuscript is the accepted version for IEEE Transactions on\n  Pattern Analysis and Machine Intelligence (TPAMI) 2015", "journal-ref": null, "doi": "10.1109/TPAMI.2015.2408360", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, the human parsing task, namely decomposing a human image into\nsemantic fashion/body regions, is formulated as an Active Template Regression\n(ATR) problem, where the normalized mask of each fashion/body item is expressed\nas the linear combination of the learned mask templates, and then morphed to a\nmore precise mask with the active shape parameters, including position, scale\nand visibility of each semantic region. The mask template coefficients and the\nactive shape parameters together can generate the human parsing results, and\nare thus called the structure outputs for human parsing. The deep Convolutional\nNeural Network (CNN) is utilized to build the end-to-end relation between the\ninput human image and the structure outputs for human parsing. More\nspecifically, the structure outputs are predicted by two separate networks. The\nfirst CNN network is with max-pooling, and designed to predict the template\ncoefficients for each label mask, while the second CNN network is without\nmax-pooling to preserve sensitivity to label mask position and accurately\npredict the active shape parameters. For a new image, the structure outputs of\nthe two networks are fused to generate the probability of each label for each\npixel, and super-pixel smoothing is finally used to refine the human parsing\nresult. Comprehensive evaluations on a large dataset well demonstrate the\nsignificant superiority of the ATR framework over other state-of-the-arts for\nhuman parsing. In particular, the F1-score reaches $64.38\\%$ by our ATR\nframework, significantly higher than $44.76\\%$ based on the state-of-the-art\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 08:14:12 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Liang", "Xiaodan", ""], ["Liu", "Si", ""], ["Shen", "Xiaohui", ""], ["Yang", "Jianchao", ""], ["Liu", "Luoqi", ""], ["Dong", "Jian", ""], ["Lin", "Liang", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1503.02445", "submitter": "Muhammad  Uzair", "authors": "Muhammad Uzair, Faisal Shafait, Bernard Ghanem, Ajmal Mian", "title": "Representation Learning with Deep Extreme Learning Machines for\n  Efficient Image Set Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient and accurate joint representation of a collection of images, that\nbelong to the same class, is a major research challenge for practical image set\nclassification. Existing methods either make prior assumptions about the data\nstructure, or perform heavy computations to learn structure from the data\nitself. In this paper, we propose an efficient image set representation that\ndoes not make any prior assumptions about the structure of the underlying data.\nWe learn the non-linear structure of image sets with Deep Extreme Learning\nMachines (DELM) that are very efficient and generalize well even on a limited\nnumber of training samples. Extensive experiments on a broad range of public\ndatasets for image set classification (Honda/UCSD, CMU Mobo, YouTube\nCelebrities, Celebrity-1000, ETH-80) show that the proposed algorithm\nconsistently outperforms state-of-the-art image set classification methods both\nin terms of speed and accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 12:14:42 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2015 05:29:31 GMT"}, {"version": "v3", "created": "Wed, 1 Apr 2015 10:29:09 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Uzair", "Muhammad", ""], ["Shafait", "Faisal", ""], ["Ghanem", "Bernard", ""], ["Mian", "Ajmal", ""]]}, {"id": "1503.02466", "submitter": "Muhammad  Ali Qadar", "authors": "Muhammad Ali Qadar, Yan Zhaowen", "title": "Brain Tumor Segmentation: A Comparative Analysis", "comments": "8 Pages, 8 Figues, International Journal of Computer Science (IJCSI)", "journal-ref": "Muhammad Ali Qadar, Yan Zhaowen, Brain Tumor Segmentation: A\n  Comparative Analysis, IJCSI International Journal of Computer Science Issues,\n  Volume 11, Issue 6, No 1, November 2014,1694-0784", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Five different threshold segmentation based approaches have been reviewed and\ncompared over here to extract the tumor from set of brain images. This research\nfocuses on the analysis of image segmentation methods, a comparison of five\nsemi-automated methods have been undertaken for evaluating their relative\nperformance in the segmentation of tumor. Consequently, results are compared on\nthe basis of quantitative and qualitative analysis of respective methods. The\npurpose of this study was to analytically identify the methods, most suitable\nfor application for a particular genre of problems. The results show that of\nthe region growing segmentation performed better than rest in most cases.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 13:15:05 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Qadar", "Muhammad Ali", ""], ["Zhaowen", "Yan", ""]]}, {"id": "1503.02619", "submitter": "Dmytro Mishkin", "authors": "Dmytro Mishkin, Jiri Matas, Michal Perdoch", "title": "MODS: Fast and Robust Method for Two-View Matching", "comments": "Version accepted to CVIU. arXiv admin note: text overlap with\n  arXiv:1306.3855", "journal-ref": null, "doi": "10.1016/j.cviu.2015.08.005", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel algorithm for wide-baseline matching called MODS - Matching On Demand\nwith view Synthesis - is presented. The MODS algorithm is experimentally shown\nto solve a broader range of wide-baseline problems than the state of the art\nwhile being nearly as fast as standard matchers on simple problems. The\napparent robustness vs. speed trade-off is finessed by the use of progressively\nmore time-consuming feature detectors and by on-demand generation of\nsynthesized images that is performed until a reliable estimate of geometry is\nobtained.\n  We introduce an improved method for tentative correspondence selection,\napplicable both with and without view synthesis. A modification of the standard\nfirst to second nearest distance rule increases the number of correct matches\nby 5-20% at no additional computational cost.\n  Performance of the MODS algorithm is evaluated on several standard publicly\navailable datasets, and on a new set of geometrically challenging wide baseline\nproblems that is made public together with the ground truth. Experiments show\nthat the MODS outperforms the state-of-the-art in robustness and speed.\nMoreover, MODS performs well on other classes of difficult two-view problems\nlike matching of images from different modalities, with wide temporal baseline\nor with significant lighting changes.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 18:59:18 GMT"}, {"version": "v2", "created": "Sun, 1 May 2016 14:44:35 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Mishkin", "Dmytro", ""], ["Matas", "Jiri", ""], ["Perdoch", "Michal", ""]]}, {"id": "1503.02675", "submitter": "Clemens Arth", "authors": "Clemens Arth, Christian Pirchheim, Jonathan Ventura, Vincent Lepetit", "title": "Global 6DOF Pose Estimation from Untextured 2D City Models", "comments": "9 pages excluding supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We propose a method for estimating the 3D pose for the camera of a mobile\ndevice in outdoor conditions, using only an untextured 2D model. Previous\nmethods compute only a relative pose using a SLAM algorithm, or require many\nregistered images, which are cumbersome to acquire. By contrast, our method\nreturns an accurate, absolute camera pose in an absolute referential using\nsimple 2D+height maps, which are broadly available, to refine a first estimate\nof the pose provided by the device's sensors. We show how to first estimate the\ncamera absolute orientation from straight line segments, and then how to\nestimate the translation by aligning the 2D map with a semantic segmentation of\nthe input image. We demonstrate the robustness and accuracy of our approach on\na challenging dataset.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 20:18:19 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 12:11:35 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Arth", "Clemens", ""], ["Pirchheim", "Christian", ""], ["Ventura", "Jonathan", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1503.02725", "submitter": "Abhishek Sharma", "authors": "Abhishek Sharma and Oncel Tuzel and David W. Jacobs", "title": "Deep Hierarchical Parsing for Semantic Segmentation", "comments": "IEEE CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a learning-based approach to scene parsing inspired by\nthe deep Recursive Context Propagation Network (RCPN). RCPN is a deep\nfeed-forward neural network that utilizes the contextual information from the\nentire image, through bottom-up followed by top-down context propagation via\nrandom binary parse trees. This improves the feature representation of every\nsuper-pixel in the image for better classification into semantic categories. We\nanalyze RCPN and propose two novel contributions to further improve the model.\nWe first analyze the learning of RCPN parameters and discover the presence of\nbypass error paths in the computation graph of RCPN that can hinder contextual\npropagation. We propose to tackle this problem by including the classification\nloss of the internal nodes of the random parse trees in the original RCPN loss\nfunction. Secondly, we use an MRF on the parse tree nodes to model the\nhierarchical dependency present in the output. Both modifications provide\nperformance boosts over the original RCPN and the new system achieves\nstate-of-the-art performance on Stanford Background, SIFT-Flow and Daimler\nurban datasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 23:05:26 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2015 20:03:01 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Sharma", "Abhishek", ""], ["Tuzel", "Oncel", ""], ["Jacobs", "David W.", ""]]}, {"id": "1503.02727", "submitter": "Aswin Sankaranarayanan", "authors": "Aswin C. Sankaranarayanan, Lina Xu, Christoph Studer, Yun Li, Kevin\n  Kelly and Richard G. Baraniuk", "title": "Video Compressive Sensing for Spatial Multiplexing Cameras using\n  Motion-Flow Models", "comments": "in SIAM Journal on Imaging Sciences, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial multiplexing cameras (SMCs) acquire a (typically static) scene\nthrough a series of coded projections using a spatial light modulator (e.g., a\ndigital micro-mirror device) and a few optical sensors. This approach finds use\nin imaging applications where full-frame sensors are either too expensive\n(e.g., for short-wave infrared wavelengths) or unavailable. Existing SMC\nsystems reconstruct static scenes using techniques from compressive sensing\n(CS). For videos, however, existing acquisition and recovery methods deliver\npoor quality. In this paper, we propose the CS multi-scale video (CS-MUVI)\nsensing and recovery framework for high-quality video acquisition and recovery\nusing SMCs. Our framework features novel sensing matrices that enable the\nefficient computation of a low-resolution video preview, while enabling\nhigh-resolution video recovery using convex optimization. To further improve\nthe quality of the reconstructed videos, we extract optical-flow estimates from\nthe low-resolution previews and impose them as constraints in the recovery\nprocedure. We demonstrate the efficacy of our CS-MUVI framework for a host of\nsynthetic and real measured SMC video data, and we show that high-quality\nvideos can be recovered at roughly $60\\times$ compression.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 23:08:44 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2015 17:00:42 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Sankaranarayanan", "Aswin C.", ""], ["Xu", "Lina", ""], ["Studer", "Christoph", ""], ["Li", "Yun", ""], ["Kelly", "Kevin", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1503.02945", "submitter": "Xiaobo Qu", "authors": "Zhifang Zhan, Jian-Feng Cai, Di Guo, Yunsong Liu, Zhong Chen, Xiaobo\n  Qu", "title": "Fast Multi-class Dictionaries Learning with Geometrical Directions in\n  MRI Reconstruction", "comments": "13 pages, 15 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Improve the reconstructed image with fast and multi-class\ndictionaries learning when magnetic resonance imaging is accelerated by\nundersampling the k-space data. Methods: A fast orthogonal dictionary learning\nmethod is introduced into magnetic resonance image reconstruction to providing\nadaptive sparse representation of images. To enhance the sparsity, image is\ndivided into classified patches according to the same geometrical direction and\ndictionary is trained within each class. A new sparse reconstruction model with\nthe multi-class dictionaries is proposed and solved using a fast alternating\ndirection method of multipliers. Results: Experiments on phantom and brain\nimaging data with acceleration factor up to 10 and various undersampling\npatterns are conducted. The proposed method is compared with state-of-the-art\nmagnetic resonance image reconstruction methods. Conclusion: Artifacts are\nbetter suppressed and image edges are better preserved than the compared\nmethods. Besides, the computation of the proposed approach is much faster than\nthe typical K-SVD dictionary learning method in magnetic resonance image\nreconstruction. Significance: The proposed method can be exploited in\nundersapmled magnetic resonance imaging to reduce data acquisition time and\nreconstruct images with better image quality.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 15:08:21 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 03:18:01 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Zhan", "Zhifang", ""], ["Cai", "Jian-Feng", ""], ["Guo", "Di", ""], ["Liu", "Yunsong", ""], ["Chen", "Zhong", ""], ["Qu", "Xiaobo", ""]]}, {"id": "1503.03004", "submitter": "German Ros", "authors": "German Ros and Julio Guerrero", "title": "Fast and Robust Fixed-Rank Matrix Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of efficient sparse fixed-rank (S-FR) matrix\ndecomposition, i.e., splitting a corrupted matrix $M$ into an uncorrupted\nmatrix $L$ of rank $r$ and a sparse matrix of outliers $S$. Fixed-rank\nconstraints are usually imposed by the physical restrictions of the system\nunder study. Here we propose a method to perform accurate and very efficient\nS-FR decomposition that is more suitable for large-scale problems than existing\napproaches. Our method is a grateful combination of geometrical and algebraical\ntechniques, which avoids the bottleneck caused by the Truncated SVD (TSVD).\nInstead, a polar factorization is used to exploit the manifold structure of\nfixed-rank problems as the product of two Stiefel and an SPD manifold, leading\nto a better convergence and stability. Then, closed-form projectors help to\nspeed up each iteration of the method. We introduce a novel and fast projector\nfor the $\\text{SPD}$ manifold and a proof of its validity. Further acceleration\nis achieved using a Nystrom scheme. Extensive experiments with synthetic and\nreal data in the context of robust photometric stereo and spectral clustering\nshow that our proposals outperform the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 17:35:46 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2015 21:43:00 GMT"}, {"version": "v3", "created": "Wed, 25 Mar 2015 09:26:04 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Ros", "German", ""], ["Guerrero", "Julio", ""]]}, {"id": "1503.03016", "submitter": "P. Christopher Staecker", "authors": "Laurence Boxer and P. Christopher Staecker", "title": "Remarks on pointed digital homotopy", "comments": "major new section, some errors corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CV math.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and explore in detail a pair of digital images with\n$c_u$-adjacencies that are homotopic but not pointed homotopic. For two digital\nloops $f,g: [0,m]_Z \\rightarrow X$ with the same basepoint, we introduce the\nnotion of {\\em tight at the basepoint (TAB)} pointed homotopy, which is more\nrestrictive than ordinary pointed homotopy and yields some different results.\n  We present a variant form of the digital fundamental group. Based on what we\ncall {\\em eventually constant} loops, this version of the fundamental group is\nequivalent to that of Boxer (1999), but offers the advantage that eventually\nconstant maps are often easier to work with than the trivial extensions that\nare key to the development of the fundamental group in Boxer (1999) and many\nsubsequent papers.\n  We show that homotopy equivalent digital images have isomorphic fundamental\ngroups, even when the homotopy equivalence does not preserve the basepoint.\nThis assertion appeared in Boxer (2005), but there was an error in the proof;\nhere, we correct the error.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 17:52:28 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 14:10:56 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Boxer", "Laurence", ""], ["Staecker", "P. Christopher", ""]]}, {"id": "1503.03163", "submitter": "Yanwei  Fu", "authors": "Xi Zhang, Yanwei Fu, Andi Zang, Leonid Sigal, Gady Agam", "title": "Learning Classifiers from Synthetic Data Using a Multichannel\n  Autoencoder", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for using synthetic data to help learning classifiers.\nSynthetic data, even is generated based on real data, normally results in a\nshift from the distribution of real data in feature space. To bridge the gap\nbetween the real and synthetic data, and jointly learn from synthetic and real\ndata, this paper proposes a Multichannel Autoencoder(MCAE). We show that by\nsuing MCAE, it is possible to learn a better feature representation for\nclassification. To evaluate the proposed approach, we conduct experiments on\ntwo types of datasets. Experimental results on two datasets validate the\nefficiency of our MCAE model and our methodology of generating synthetic data.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 03:31:53 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Zhang", "Xi", ""], ["Fu", "Yanwei", ""], ["Zang", "Andi", ""], ["Sigal", "Leonid", ""], ["Agam", "Gady", ""]]}, {"id": "1503.03167", "submitter": "Tejas Kulkarni", "authors": "Tejas D. Kulkarni, Will Whitney, Pushmeet Kohli, Joshua B. Tenenbaum", "title": "Deep Convolutional Inverse Graphics Network", "comments": "First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a\nmodel that learns an interpretable representation of images. This\nrepresentation is disentangled with respect to transformations such as\nout-of-plane rotations and lighting variations. The DC-IGN model is composed of\nmultiple layers of convolution and de-convolution operators and is trained\nusing the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose a\ntraining procedure to encourage neurons in the graphics code layer to represent\na specific transformation (e.g. pose or light). Given a single input image, our\nmodel can generate new images of the same object with variations in pose and\nlighting. We present qualitative and quantitative results of the model's\nefficacy at learning a 3D rendering engine.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 04:08:42 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2015 04:57:24 GMT"}, {"version": "v3", "created": "Tue, 17 Mar 2015 02:22:07 GMT"}, {"version": "v4", "created": "Mon, 22 Jun 2015 02:10:00 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Kulkarni", "Tejas D.", ""], ["Whitney", "Will", ""], ["Kohli", "Pushmeet", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1503.03187", "submitter": "Wen-Ze Shao", "authors": "Wen-Ze Shao, Michael Elad", "title": "Simple, Accurate, and Robust Nonparametric Blind Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a simple, accurate, and robust approach to single image\nnonparametric blind Super-Resolution (SR). This task is formulated as a\nfunctional to be minimized with respect to both an intermediate super-resolved\nimage and a nonparametric blur-kernel. The proposed approach includes a\nconvolution consistency constraint which uses a non-blind learning-based SR\nresult to better guide the estimation process. Another key component is the\nunnatural bi-l0-l2-norm regularization imposed on the super-resolved, sharp\nimage and the blur-kernel, which is shown to be quite beneficial for estimating\nthe blur-kernel accurately. The numerical optimization is implemented by\ncoupling the splitting augmented Lagrangian and the conjugate gradient (CG).\nUsing the pre-estimated blur-kernel, we finally reconstruct the SR image by a\nvery simple non-blind SR method that uses a natural image prior. The proposed\napproach is demonstrated to achieve better performance than the recent method\nby Michaeli and Irani [2] in both terms of the kernel estimation accuracy and\nimage SR quality.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 06:21:39 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2015 10:12:07 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Shao", "Wen-Ze", ""], ["Elad", "Michael", ""]]}, {"id": "1503.03191", "submitter": "Ben Ward Dr", "authors": "Ben Ward, John Bastian, Anton van den Hengel, Daniel Pooley, Rajendra\n  Bari, Bettina Berger, Mark Tester", "title": "A model-based approach to recovering the structure of a plant from\n  images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for recovering the structure of a plant directly from a\nsmall set of widely-spaced images. Structure recovery is more complex than\nshape estimation, but the resulting structure estimate is more closely related\nto phenotype than is a 3D geometric model. The method we propose is applicable\nto a wide variety of plants, but is demonstrated on wheat. Wheat is made up of\nthin elements with few identifiable features, making it difficult to analyse\nusing standard feature matching techniques. Our method instead analyses the\nstructure of plants using only their silhouettes. We employ a generate-and-test\nmethod, using a database of manually modelled leaves and a model for their\ncomposition to synthesise plausible plant structures which are evaluated\nagainst the images. The method is capable of efficiently recovering accurate\nestimates of plant structure in a wide variety of imaging scenarios, with no\nmanual intervention.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 06:37:40 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2015 00:28:52 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Ward", "Ben", ""], ["Bastian", "John", ""], ["Hengel", "Anton van den", ""], ["Pooley", "Daniel", ""], ["Bari", "Rajendra", ""], ["Berger", "Bettina", ""], ["Tester", "Mark", ""]]}, {"id": "1503.03231", "submitter": "Joao Mota", "authors": "Joao F. C. Mota, Nikos Deligiannis, Aswin C. Sankaranarayanan, Volkan\n  Cevher, Miguel R. D. Rodrigues", "title": "Adaptive-Rate Sparse Signal Reconstruction With Application in\n  Compressive Background Subtraction", "comments": "submitted to IEEE Trans. Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze an online algorithm for reconstructing a sequence of\nsignals from a limited number of linear measurements. The signals are assumed\nsparse, with unknown support, and evolve over time according to a generic\nnonlinear dynamical model. Our algorithm, based on recent theoretical results\nfor $\\ell_1$-$\\ell_1$ minimization, is recursive and computes the number of\nmeasurements to be taken at each time on-the-fly. As an example, we apply the\nalgorithm to compressive video background subtraction, a problem that can be\nstated as follows: given a set of measurements of a sequence of images with a\nstatic background, simultaneously reconstruct each image while separating its\nforeground from the background. The performance of our method is illustrated on\nsequences of real images: we observe that it allows a dramatic reduction in the\nnumber of measurements with respect to state-of-the-art compressive background\nsubtraction schemes.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 09:16:39 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Mota", "Joao F. C.", ""], ["Deligiannis", "Nikos", ""], ["Sankaranarayanan", "Aswin C.", ""], ["Cevher", "Volkan", ""], ["Rodrigues", "Miguel R. D.", ""]]}, {"id": "1503.03270", "submitter": "Vandna Bhalla Ms", "authors": "Vandna Bhalla, Santanu Chaudhury, Arihant Jain", "title": "A Novel Hybrid CNN-AIS Visual Pattern Recognition Engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Machine learning methods are used today for most recognition problems.\nConvolutional Neural Networks (CNN) have time and again proved successful for\nmany image processing tasks primarily for their architecture. In this paper we\npropose to apply CNN to small data sets like for example, personal albums or\nother similar environs where the size of training dataset is a limitation,\nwithin the framework of a proposed hybrid CNN-AIS model. We use Artificial\nImmune System Principles to enhance small size of training data set. A layer of\nClonal Selection is added to the local filtering and max pooling of CNN\nArchitecture. The proposed Architecture is evaluated using the standard MNIST\ndataset by limiting the data size and also with a small personal data sample\nbelonging to two different classes. Experimental results show that the proposed\nhybrid CNN-AIS based recognition engine works well when the size of training\ndata is limited in size\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 10:58:25 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Bhalla", "Vandna", ""], ["Chaudhury", "Santanu", ""], ["Jain", "Arihant", ""]]}, {"id": "1503.03278", "submitter": "Nicolas Brodu", "authors": "Nicolas Brodu, Hussein Yahia", "title": "Stochastic Texture Difference for Scale-Dependent Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces the Stochastic Texture Difference method for\nanalyzing data at prescribed spatial and value scales. This method relies on\nconstrained random walks around each pixel, describing how nearby image values\ntypically evolve on each side of this pixel. Textures are represented as\nprobability distributions of such random walks, so a texture difference\noperator is statistically defined as a distance between these distributions in\na suitable reproducing kernel Hilbert space. The method is thus not limited to\nscalar pixel values: any data type for which a kernel is available may be\nconsidered, from color triplets and multispectral vector data to strings,\ngraphs, and more. By adjusting the size of the neighborhoods that are compared,\nthe method is implicitly scale-dependent. It is also able to focus on either\nsmall changes or large gradients. We demonstrate how it can be used to infer\nspatial and data value characteristic scales in measured signals and natural\nimages.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 11:30:04 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2015 08:40:08 GMT"}, {"version": "v3", "created": "Mon, 5 Oct 2015 08:26:28 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Brodu", "Nicolas", ""], ["Yahia", "Hussein", ""]]}, {"id": "1503.03429", "submitter": "Dat Tien Ngo", "authors": "Dat Tien Ngo, Sanghuyk Park, Anne Jorstad, Alberto Crivellaro, Chang\n  Yoo, Pascal Fua", "title": "Dense image registration and deformable surface reconstruction in\n  presence of occlusions and minimal texture", "comments": "In Proceedings of International Conference on Computer Vision, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformable surface tracking from monocular images is well-known to be\nunder-constrained. Occlusions often make the task even more challenging, and\ncan result in failure if the surface is not sufficiently textured. In this\nwork, we explicitly address the problem of 3D reconstruction of poorly\ntextured, occluded surfaces, proposing a framework based on a template-matching\napproach that scales dense robust features by a relevancy score. Our approach\nis extensively compared to current methods employing both local feature\nmatching and dense template alignment. We test on standard datasets as well as\non a new dataset (that will be made publicly available) of a sparsely textured,\noccluded surface. Our framework achieves state-of-the-art results for both well\nand poorly textured, occluded surfaces.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 17:37:22 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2015 10:30:02 GMT"}, {"version": "v3", "created": "Fri, 25 Sep 2015 09:07:09 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Ngo", "Dat Tien", ""], ["Park", "Sanghuyk", ""], ["Jorstad", "Anne", ""], ["Crivellaro", "Alberto", ""], ["Yoo", "Chang", ""], ["Fua", "Pascal", ""]]}, {"id": "1503.03491", "submitter": "Alexander Evako V", "authors": "Alexander V. Evako", "title": "Properties of simple sets in digital spaces. Contractions of simple sets\n  preserving the homotopy type of a digital space", "comments": "7 pages, 5 figures. arXiv admin note: text overlap with\n  arXiv:1412.0218", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DM math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A point of a digital space is called simple if it can be deleted from the\nspace without altering topology. This paper introduces the notion simple set of\npoints of a digital space. The definition is based on contractible spaces and\ncontractible transformations. A set of points in a digital space is called\nsimple if it can be contracted to a point without changing topology of the\nspace. It is shown that contracting a simple set of points does not change the\nhomotopy type of a digital space, and the number of points in a digital space\nwithout simple points can be reduces by contracting simple sets. Using the\nprocess of contracting, we can substantially compress a digital space while\npreserving the topology. The paper proposes a method for thinning a digital\nspace which shows that this approach can contribute to computer science such as\nmedical imaging, computer graphics and pattern analysis.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 20:17:58 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Evako", "Alexander V.", ""]]}, {"id": "1503.03492", "submitter": "Stephan Kramer", "authors": "Jan Lebert, Lutz K\\\"unneke, Johannes Hagemann and Stephan C. Kramer", "title": "Parallel Statistical Multi-resolution Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss several strategies to implement Dykstra's projection algorithm on\nNVIDIA's compute unified device architecture (CUDA). Dykstra's algorithm is the\ncentral step in and the computationally most expensive part of statistical\nmulti-resolution methods. It projects a given vector onto the intersection of\nconvex sets. Compared with a CPU implementation our CUDA implementation is one\norder of magnitude faster. For a further speed up and to reduce memory\nconsumption we have developed a new variant, which we call incomplete Dykstra's\nalgorithm. Implemented in CUDA it is one order of magnitude faster than the\nCUDA implementation of the standard Dykstra algorithm. As sample application we\ndiscuss using the incomplete Dykstra's algorithm as preprocessor for the\nrecently developed super-resolution optical fluctuation imaging (SOFI) method\n(Dertinger et al. 2009). We show that statistical multi-resolution estimation\ncan enhance the resolution improvement of the plain SOFI algorithm just as the\nFourier-reweighting of SOFI. The results are compared in terms of their power\nspectrum and their Fourier ring correlation (Saxton and Baumeister 1982). The\nFourier ring correlation indicates that the resolution for typical second order\nSOFI images can be improved by about 30 per cent. Our results show that a\ncareful parallelization of Dykstra's algorithm enables its use in large-scale\nstatistical multi-resolution analyses.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 09:54:40 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Lebert", "Jan", ""], ["K\u00fcnneke", "Lutz", ""], ["Hagemann", "Johannes", ""], ["Kramer", "Stephan C.", ""]]}, {"id": "1503.03506", "submitter": "Christian Wachinger", "authors": "Christian Wachinger and Polina Golland", "title": "Diverse Landmark Sampling from Determinantal Point Processes for\n  Scalable Manifold Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High computational costs of manifold learning prohibit its application for\nlarge point sets. A common strategy to overcome this problem is to perform\ndimensionality reduction on selected landmarks and to successively embed the\nentire dataset with the Nystr\\\"om method. The two main challenges that arise\nare: (i) the landmarks selected in non-Euclidean geometries must result in a\nlow reconstruction error, (ii) the graph constructed from sparsely sampled\nlandmarks must approximate the manifold well. We propose the sampling of\nlandmarks from determinantal distributions on non-Euclidean spaces. Since\ncurrent determinantal sampling algorithms have the same complexity as those for\nmanifold learning, we present an efficient approximation running in linear\ntime. Further, we recover the local geometry after the sparsification by\nassigning each landmark a local covariance matrix, estimated from the original\npoint set. The resulting neighborhood selection based on the Bhattacharyya\ndistance improves the embedding of sparsely sampled manifolds. Our experiments\nshow a significant performance improvement compared to state-of-the-art\nlandmark selection techniques.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 21:09:28 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Wachinger", "Christian", ""], ["Golland", "Polina", ""]]}, {"id": "1503.03514", "submitter": "Jose Rivera", "authors": "Jose Rivera-Rubio, Ioannis Alexiou and Anil A. Bharath", "title": "Appearance-based indoor localization: A comparison of patch descriptor\n  performance", "comments": "Accepted for publication on Pattern Recognition Letters", "journal-ref": null, "doi": "10.1016/j.patrec.2015.03.003", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision is one of the most important of the senses, and humans use it\nextensively during navigation. We evaluated different types of image and video\nframe descriptors that could be used to determine distinctive visual landmarks\nfor localizing a person based on what is seen by a camera that they carry. To\ndo this, we created a database containing over 3 km of video-sequences with\nground-truth in the form of distance travelled along different corridors. Using\nthis database, the accuracy of localization - both in terms of knowing which\nroute a user is on - and in terms of position along a certain route, can be\nevaluated. For each type of descriptor, we also tested different techniques to\nencode visual structure and to search between journeys to estimate a user's\nposition. The techniques include single-frame descriptors, those using\nsequences of frames, and both colour and achromatic descriptors. We found that\nsingle-frame indexing worked better within this particular dataset. This might\nbe because the motion of the person holding the camera makes the video too\ndependent on individual steps and motions of one particular journey. Our\nresults suggest that appearance-based information could be an additional source\nof navigational data indoors, augmenting that provided by, say, radio signal\nstrength indicators (RSSIs). Such visual information could be collected by\ncrowdsourcing low-resolution video feeds, allowing journeys made by different\nusers to be associated with each other, and location to be inferred without\nrequiring explicit mapping. This offers a complementary approach to methods\nbased on simultaneous localization and mapping (SLAM) algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 21:43:46 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Rivera-Rubio", "Jose", ""], ["Alexiou", "Ioannis", ""], ["Bharath", "Anil A.", ""]]}, {"id": "1503.03562", "submitter": "Zhiyong Cheng", "authors": "Zhiyong Cheng, Daniel Soudry, Zexi Mao, Zhenzhong Lan", "title": "Training Binary Multilayer Neural Networks for Image Classification\n  using Expectation Backpropagation", "comments": "8 pages with 1 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to Multilayer Neural Networks with real weights, Binary Multilayer\nNeural Networks (BMNNs) can be implemented more efficiently on dedicated\nhardware. BMNNs have been demonstrated to be effective on binary classification\ntasks with Expectation BackPropagation (EBP) algorithm on high dimensional text\ndatasets. In this paper, we investigate the capability of BMNNs using the EBP\nalgorithm on multiclass image classification tasks. The performances of binary\nneural networks with multiple hidden layers and different numbers of hidden\nunits are examined on MNIST. We also explore the effectiveness of image spatial\nfilters and the dropout technique in BMNNs. Experimental results on MNIST\ndataset show that EBP can obtain 2.12% test error with binary weights and 1.66%\ntest error with real weights, which is comparable to the results of standard\nBackPropagation algorithm on fully connected MNNs.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 02:24:31 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2015 01:32:15 GMT"}, {"version": "v3", "created": "Sun, 22 Mar 2015 21:47:56 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Cheng", "Zhiyong", ""], ["Soudry", "Daniel", ""], ["Mao", "Zexi", ""], ["Lan", "Zhenzhong", ""]]}, {"id": "1503.03606", "submitter": "Nagaraja S", "authors": "Nagaraja S. and Prabhakar C.J.", "title": "Low-Level Features for Image Retrieval Based on Extraction of\n  Directional Binary Patterns and Its Oriented Gradients Histogram", "comments": "7 Figures, 5 Tables 16 Pages in Computer Applications: An\n  International Journal (CAIJ), Vol.2, No.1, February 2015", "journal-ref": null, "doi": "10.5121/caij.2015.2102", "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel approach for image retrieval based on\nextraction of low level features using techniques such as Directional Binary\nCode, Haar Wavelet transform and Histogram of Oriented Gradients. The DBC\ntexture descriptor captures the spatial relationship between any pair of\nneighbourhood pixels in a local region along a given direction, while Local\nBinary Patterns descriptor considers the relationship between a given pixel and\nits surrounding neighbours. Therefore, DBC captures more spatial information\nthan LBP and its variants, also it can extract more edge information than LBP.\nHence, we employ DBC technique in order to extract grey level texture feature\nfrom each RGB channels individually and computed texture maps are further\ncombined which represents colour texture features of an image. Then, we\ndecomposed the extracted colour texture map and original image using Haar\nwavelet transform. Finally, we encode the shape and local features of wavelet\ntransformed images using Histogram of Oriented Gradients for content based\nimage retrieval. The performance of proposed method is compared with existing\nmethods on two databases such as Wang's corel image and Caltech 256. The\nevaluation results show that our approach outperforms the existing methods for\nimage retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 06:45:01 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["S.", "Nagaraja", ""], ["J.", "Prabhakar C.", ""]]}, {"id": "1503.03621", "submitter": "Zhangyang Wang", "authors": "Zhangyang Wang, Yingzhen Yang, Jianchao Yang, Thomas S. Huang", "title": "Designing A Composite Dictionary Adaptively From Joint Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complementary behaviors of external and internal examples in\nimage restoration, and are motivated to formulate a composite dictionary design\nframework. The composite dictionary consists of the global part learned from\nexternal examples, and the sample-specific part learned from internal examples.\nThe dictionary atoms in both parts are further adaptively weighted to emphasize\ntheir model statistics. Experiments demonstrate that the joint utilization of\nexternal and internal examples leads to substantial improvements, with\nsuccessful applications in image denoising and super resolution.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 08:09:13 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2015 19:04:30 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Wang", "Zhangyang", ""], ["Yang", "Yingzhen", ""], ["Yang", "Jianchao", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1503.03630", "submitter": "Liang-Jian Deng", "authors": "Liang-Jian Deng and Weihong Guo and Ting-Zhu Huang", "title": "Single image super-resolution by approximated Heaviside functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image super-resolution is a process to enhance image resolution. It is widely\nused in medical imaging, satellite imaging, target recognition, etc. In this\npaper, we conduct continuous modeling and assume that the unknown image\nintensity function is defined on a continuous domain and belongs to a space\nwith a redundant basis. We propose a new iterative model for single image\nsuper-resolution based on an observation: an image is consisted of smooth\ncomponents and non-smooth components, and we use two classes of approximated\nHeaviside functions (AHFs) to represent them respectively. Due to sparsity of\nthe non-smooth components, a $L_{1}$ model is employed. In addition, we apply\nthe proposed iterative model to image patches to reduce computation and\nstorage. Comparisons with some existing competitive methods show the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 08:54:54 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Deng", "Liang-Jian", ""], ["Guo", "Weihong", ""], ["Huang", "Ting-Zhu", ""]]}, {"id": "1503.03637", "submitter": "Federica Arrigoni", "authors": "Federica Arrigoni, Beatrice Rossi, Andrea Fusiello", "title": "On Computing the Translations Norm in the Epipolar Graph", "comments": "Accepted at 3DV 2015", "journal-ref": "Proceedings of the 2015 International Conference on 3D Vision, pp.\n  300-308", "doi": "10.1109/3DV.2015.41", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the problem of recovering the unknown norm of relative\ntranslations between cameras based on the knowledge of relative rotations and\ntranslation directions. We provide theoretical conditions for the solvability\nof such a problem, and we propose a two-stage method to solve it. First, a\ncycle basis for the epipolar graph is computed, then all the scaling factors\nare recovered simultaneously by solving a homogeneous linear system. We\ndemonstrate the accuracy of our solution by means of synthetic and real\nexperiments.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 09:16:11 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2015 13:05:18 GMT"}, {"version": "v3", "created": "Fri, 4 Sep 2015 11:33:39 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Arrigoni", "Federica", ""], ["Rossi", "Beatrice", ""], ["Fusiello", "Andrea", ""]]}, {"id": "1503.03732", "submitter": "Dominique Vaufreydaz", "authors": "Dominique Vaufreydaz (INRIA Grenoble Rh\\^one-Alpes / LIG Laboratoire\n  d'Informatique de Grenoble), Wafa Johal (LIG), Claudine Combe (INRIA Grenoble\n  Rh\\^one-Alpes / LIG Laboratoire d'Informatique de Grenoble)", "title": "Starting engagement detection towards a companion robot using multimodal\n  features", "comments": null, "journal-ref": "Robotics and Autonomous Systems, Elsevier, 2015, Robotics and\n  Autonomous Systems, pp.25", "doi": "10.1016/j.robot.2015.01.004", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of intentions is a subconscious cognitive process vital to human\ncommunication. This skill enables anticipation and increases the quality of\ninteractions between humans. Within the context of engagement, non-verbal\nsignals are used to communicate the intention of starting the interaction with\na partner. In this paper, we investigated methods to detect these signals in\norder to allow a robot to know when it is about to be addressed. Originality of\nour approach resides in taking inspiration from social and cognitive sciences\nto perform our perception task. We investigate meaningful features, i.e. human\nreadable features, and elicit which of these are important for recognizing\nsomeone's intention of starting an interaction. Classically, spatial\ninformation like the human position and speed, the human-robot distance are\nused to detect the engagement. Our approach integrates multimodal features\ngathered using a companion robot equipped with a Kinect. The evaluation on our\ncorpus collected in spontaneous conditions highlights its robustness and\nvalidates the use of such a technique in a real environment. Experimental\nvalidation shows that multimodal features set gives better precision and recall\nthan using only spatial and speed features. We also demonstrate that 7 selected\nfeatures are sufficient to provide a good starting engagement detection score.\nIn our last investigation, we show that among our full 99 features set, the\nspace reduction is not a solved task. This result opens new researches\nperspectives on multimodal engagement detection.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 14:19:40 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Vaufreydaz", "Dominique", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LIG Laboratoire\n  d'Informatique de Grenoble"], ["Johal", "Wafa", "", "LIG"], ["Combe", "Claudine", "", "INRIA Grenoble\n  Rh\u00f4ne-Alpes / LIG Laboratoire d'Informatique de Grenoble"]]}, {"id": "1503.03741", "submitter": "Samir Hafez", "authors": "Samir F. Hafez, Mazen M. Selim and Hala H. Zayed", "title": "2D Face Recognition System Based on Selected Gabor Filters and Linear\n  Discriminant Analysis LDA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach for face recognition system. The method is based on\n2D face image features using subset of non-correlated and Orthogonal Gabor\nFilters instead of using the whole Gabor Filter Bank, then compressing the\noutput feature vector using Linear Discriminant Analysis (LDA). The face image\nhas been enhanced using multi stage image processing technique to normalize it\nand compensate for illumination variation. Experimental results show that the\nproposed system is effective for both dimension reduction and good recognition\nperformance when compared to the complete Gabor filter bank. The system has\nbeen tested using CASIA, ORL and Cropped YaleB 2D face images Databases and\nachieved average recognition rate of 98.9 %.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 14:30:51 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Hafez", "Samir F.", ""], ["Selim", "Mazen M.", ""], ["Zayed", "Hala H.", ""]]}, {"id": "1503.03771", "submitter": "Eshed Ohn-Bar", "authors": "Eshed Ohn-Bar and Mohan M. Trivedi", "title": "Learning to Detect Vehicles by Clustering Appearance Patterns", "comments": "Preprint version of our T-ITS 2015 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies efficient means for dealing with intra-category diversity\nin object detection. Strategies for occlusion and orientation handling are\nexplored by learning an ensemble of detection models from visual and\ngeometrical clusters of object instances. An AdaBoost detection scheme is\nemployed with pixel lookup features for fast detection. The analysis provides\ninsight into the design of a robust vehicle detection system, showing promise\nin terms of detection performance and orientation estimation accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 15:33:09 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Ohn-Bar", "Eshed", ""], ["Trivedi", "Mohan M.", ""]]}, {"id": "1503.03832", "submitter": "Florian Schroff", "authors": "Florian Schroff, Dmitry Kalenichenko, James Philbin", "title": "FaceNet: A Unified Embedding for Face Recognition and Clustering", "comments": "Also published, in Proceedings of the IEEE Computer Society\n  Conference on Computer Vision and Pattern Recognition 2015", "journal-ref": null, "doi": "10.1109/CVPR.2015.7298682", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant recent advances in the field of face recognition,\nimplementing face verification and recognition efficiently at scale presents\nserious challenges to current approaches. In this paper we present a system,\ncalled FaceNet, that directly learns a mapping from face images to a compact\nEuclidean space where distances directly correspond to a measure of face\nsimilarity. Once this space has been produced, tasks such as face recognition,\nverification and clustering can be easily implemented using standard techniques\nwith FaceNet embeddings as feature vectors.\n  Our method uses a deep convolutional network trained to directly optimize the\nembedding itself, rather than an intermediate bottleneck layer as in previous\ndeep learning approaches. To train, we use triplets of roughly aligned matching\n/ non-matching face patches generated using a novel online triplet mining\nmethod. The benefit of our approach is much greater representational\nefficiency: we achieve state-of-the-art face recognition performance using only\n128-bytes per face.\n  On the widely used Labeled Faces in the Wild (LFW) dataset, our system\nachieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves\n95.12%. Our system cuts the error rate in comparison to the best published\nresult by 30% on both datasets.\n  We also introduce the concept of harmonic embeddings, and a harmonic triplet\nloss, which describe different versions of face embeddings (produced by\ndifferent networks) that are compatible to each other and allow for direct\ncomparison between each other.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 18:10:53 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 19:17:30 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2015 23:35:47 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Schroff", "Florian", ""], ["Kalenichenko", "Dmitry", ""], ["Philbin", "James", ""]]}, {"id": "1503.03913", "submitter": "Sabyasachi  Mukhopadhyay", "authors": "Sabyasachi Mukhopadhyay, Soham Mandal, Nandan K Das, Subhadip Dey,\n  Asish Mitra, Nirmalya Ghosh, Prasanta K Panigrahi", "title": "Diagnosing Heterogeneous Dynamics for CT Scan Images of Human Brain in\n  Wavelet and MFDFA domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CT scan images of human brain of a particular patient in different cross\nsections are taken, on which wavelet transform and multi-fractal analysis are\napplied. The vertical and horizontal unfolding of images are done before\nanalyzing these images. A systematic investigation of de-noised CT scan images\nof human brain in different cross-sections are carried out through wavelet\nnormalized energy and wavelet semi-log plots, which clearly points out the\nmismatch between results of vertical and horizontal unfolding. The mismatch of\nresults confirms the heterogeneity in spatial domain. Using the multi-fractal\nde-trended fluctuation analysis (MFDFA), the mismatch between the values of\nHurst exponent and width of singularity spectrum by vertical and horizontal\nunfolding confirms the same.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 23:20:30 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Mukhopadhyay", "Sabyasachi", ""], ["Mandal", "Soham", ""], ["Das", "Nandan K", ""], ["Dey", "Subhadip", ""], ["Mitra", "Asish", ""], ["Ghosh", "Nirmalya", ""], ["Panigrahi", "Prasanta K", ""]]}, {"id": "1503.04036", "submitter": "Mrinal Haloi", "authors": "Mrinal Haloi and Dinesh Babu Jayagopi", "title": "Characterizing driving behavior using automatic visual analysis", "comments": "4 pages,7 figures, IBM-ICARE2014", "journal-ref": null, "doi": "10.1145/2662117.2662126", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present the problem of rash driving detection algorithm\nusing a single wide angle camera sensor, particularly useful in the Indian\ncontext. To our knowledge this rash driving problem has not been addressed\nusing Image processing techniques (existing works use other sensors such as\naccelerometer). Car Image processing literature, though rich and mature, does\nnot address the rash driving problem. In this work-in-progress paper, we\npresent the need to address this problem, our approach and our future plans to\nbuild a rash driving detector.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 12:30:34 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Haloi", "Mrinal", ""], ["Jayagopi", "Dinesh Babu", ""]]}, {"id": "1503.04065", "submitter": "Praveen Kulkarni", "authors": "Praveen Kulkarni, Joaquin Zepeda, Frederic Jurie, Patrick Perez and\n  Louis Chevallier", "title": "Hybrid multi-layer Deep CNN/Aggregator feature for image classification", "comments": "Accepted in ICASSP 2015 conference, 5 pages including reference, 4\n  figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (DCNN) have established a remarkable\nperformance benchmark in the field of image classification, displacing\nclassical approaches based on hand-tailored aggregations of local descriptors.\nYet DCNNs impose high computational burdens both at training and at testing\ntime, and training them requires collecting and annotating large amounts of\ntraining data. Supervised adaptation methods have been proposed in the\nliterature that partially re-learn a transferred DCNN structure from a new\ntarget dataset. Yet these require expensive bounding-box annotations and are\nstill computationally expensive to learn. In this paper, we address these\nshortcomings of DCNN adaptation schemes by proposing a hybrid approach that\ncombines conventional, unsupervised aggregators such as Bag-of-Words (BoW),\nwith the DCNN pipeline by treating the output of intermediate layers as densely\nextracted local descriptors.\n  We test a variant of our approach that uses only intermediate DCNN layers on\nthe standard PASCAL VOC 2007 dataset and show performance significantly higher\nthan the standard BoW model and comparable to Fisher vector aggregation but\nwith a feature that is 150 times smaller. A second variant of our approach that\nincludes the fully connected DCNN layers significantly outperforms Fisher\nvector schemes and performs comparably to DCNN approaches adapted to Pascal VOC\n2007, yet at only a small fraction of the training and testing cost.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 13:49:26 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Kulkarni", "Praveen", ""], ["Zepeda", "Joaquin", ""], ["Jurie", "Frederic", ""], ["Perez", "Patrick", ""], ["Chevallier", "Louis", ""]]}, {"id": "1503.04115", "submitter": "Nam Le", "authors": "Nam Do-Hoang Le", "title": "Sparse Code Formation with Linear Inhibition", "comments": "Technical report, 4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse code formation in the primary visual cortex (V1) has been inspiration\nfor many state-of-the-art visual recognition systems. To stimulate this\nbehavior, networks are trained networks under mathematical constraint of\nsparsity or selectivity. In this paper, the authors exploit another approach\nwhich uses lateral interconnections in feature learning networks. However,\ninstead of adding direct lateral interconnections among neurons, we introduce\nan inhibitory layer placed right after normal encoding layer. This idea\novercomes the challenge of computational cost and complexity on lateral\nnetworks while preserving crucial objective of sparse code formation. To\ndemonstrate this idea, we use sparse autoencoder as normal encoding layer and\napply inhibitory layer. Early experiments in visual recognition show relative\nimprovements over traditional approach on CIFAR-10 dataset. Moreover, simple\ninstallment and training process using Hebbian rule allow inhibitory layer to\nbe integrated into existing networks, which enables further analysis in the\nfuture.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 15:45:11 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Le", "Nam Do-Hoang", ""]]}, {"id": "1503.04127", "submitter": "Kevin Moon", "authors": "Kevin R. Moon, Jimmy J. Li, Veronique Delouille, Ruben De Visscher,\n  Fraser Watson, Alfred O. Hero III", "title": "Image patch analysis of sunspots and active regions. I. Intrinsic\n  dimension and correlation analysis", "comments": "Accepted for publication in the Journal of Space Weather and Space\n  Climate (SWSC). 23 pages, 11 figures", "journal-ref": "Journal of Space Weather and Space Climate, Vol. 6, A2 (2016)", "doi": "10.1051/swsc/2015044", "report-no": null, "categories": "astro-ph.SR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The flare-productivity of an active region is observed to be related to its\nspatial complexity. Mount Wilson or McIntosh sunspot classifications measure\nsuch complexity but in a categorical way, and may therefore not use all the\ninformation present in the observations. Moreover, such categorical schemes\nhinder a systematic study of an active region's evolution for example. We\npropose fine-scale quantitative descriptors for an active region's complexity\nand relate them to the Mount Wilson classification. We analyze the local\ncorrelation structure within continuum and magnetogram data, as well as the\ncross-correlation between continuum and magnetogram data. We compute the\nintrinsic dimension, partial correlation, and canonical correlation analysis\n(CCA) of image patches of continuum and magnetogram active region images taken\nfrom the SOHO-MDI instrument. We use masks of sunspots derived from continuum\nas well as larger masks of magnetic active regions derived from the magnetogram\nto analyze separately the core part of an active region from its surrounding\npart. We find the relationship between complexity of an active region as\nmeasured by Mount Wilson and the intrinsic dimension of its image patches.\nPartial correlation patterns exhibit approximately a third-order Markov\nstructure. CCA reveals different patterns of correlation between continuum and\nmagnetogram within the sunspots and in the region surrounding the sunspots.\nThese results also pave the way for patch-based dictionary learning with a view\ntowards automatic clustering of active regions.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 16:09:44 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2015 17:09:27 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Moon", "Kevin R.", ""], ["Li", "Jimmy J.", ""], ["Delouille", "Veronique", ""], ["De Visscher", "Ruben", ""], ["Watson", "Fraser", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1503.04144", "submitter": "Shengxin Zha", "authors": "Shengxin Zha, Florian Luisier, Walter Andrews, Nitish Srivastava,\n  Ruslan Salakhutdinov", "title": "Exploiting Image-trained CNN Architectures for Unconstrained Video\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct an in-depth exploration of different strategies for doing event\ndetection in videos using convolutional neural networks (CNNs) trained for\nimage classification. We study different ways of performing spatial and\ntemporal pooling, feature normalization, choice of CNN layers as well as choice\nof classifiers. Making judicious choices along these dimensions led to a very\nsignificant increase in performance over more naive approaches that have been\nused till now. We evaluate our approach on the challenging TRECVID MED'14\ndataset with two popular CNN architectures pretrained on ImageNet. On this\nMED'14 dataset, our methods, based entirely on image-trained CNN features, can\noutperform several state-of-the-art non-CNN models. Our proposed late fusion of\nCNN- and motion-based features can further increase the mean average precision\n(mAP) on MED'14 from 34.95% to 38.74%. The fusion approach achieves the\nstate-of-the-art classification performance on the challenging UCF-101 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 17:00:53 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2015 00:53:49 GMT"}, {"version": "v3", "created": "Fri, 8 May 2015 01:54:08 GMT"}], "update_date": "2015-05-11", "authors_parsed": [["Zha", "Shengxin", ""], ["Luisier", "Florian", ""], ["Andrews", "Walter", ""], ["Srivastava", "Nitish", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1503.04253", "submitter": "Yong-Rim Kang", "authors": "Kang Yong-Rim and Kim Yong-Jin", "title": "Novel Super-Resolution Method Based on High Order Nonlocal-Means", "comments": "1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution without explicit sub-pixel motion estimation is a very\nactive subject of image reconstruction containing general motion. The Non-Local\nMeans (NLM) method is a simple image reconstruction method without explicit\nmotion estimation. In this paper we generalize NLM method to higher orders\nusing kernel regression can apply to super-resolution reconstruction. The\nperformance of the generalized method is compared with other methods.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2015 00:16:06 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2015 20:54:56 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2015 08:24:52 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Yong-Rim", "Kang", ""], ["Yong-Jin", "Kim", ""]]}, {"id": "1503.04265", "submitter": "Aswin Sankaranarayanan", "authors": "Zhuo Hui and Aswin C. Sankaranarayanan", "title": "A Dictionary-based Approach for Estimating Shape and Spatially-Varying\n  Reflectance", "comments": "IEEE Intl. Conf. Computational Photography, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique for estimating the shape and reflectance of an object\nin terms of its surface normals and spatially-varying BRDF. We assume that\nmultiple images of the object are obtained under fixed view-point and varying\nillumination, i.e, the setting of photometric stereo. Assuming that the BRDF at\neach pixel lies in the non-negative span of a known BRDF dictionary, we derive\na per-pixel surface normal and BRDF estimation framework that requires neither\niterative optimization techniques nor careful initialization, both of which are\nendemic to most state-of-the-art techniques. We showcase the performance of our\ntechnique on a wide range of simulated and real scenes where we outperform\ncompeting methods.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2015 04:24:23 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Hui", "Zhuo", ""], ["Sankaranarayanan", "Aswin C.", ""]]}, {"id": "1503.04267", "submitter": "Aswin Sankaranarayanan", "authors": "Jian Wang and Mohit Gupta and Aswin C. Sankaranarayanan", "title": "LiSens --- A Scalable Architecture for Video Compressive Sensing", "comments": "IEEE Intl. Conf. Computational Photography, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The measurement rate of cameras that take spatially multiplexed measurements\nby using spatial light modulators (SLM) is often limited by the switching speed\nof the SLMs. This is especially true for single-pixel cameras where the\nphotodetector operates at a rate that is many orders-of-magnitude greater than\nthe SLM. We study the factors that determine the measurement rate for such\nspatial multiplexing cameras (SMC) and show that increasing the number of\npixels in the device improves the measurement rate, but there is an optimum\nnumber of pixels (typically, few thousands) beyond which the measurement rate\ndoes not increase. This motivates the design of LiSens, a novel imaging\narchitecture, that replaces the photodetector in the single-pixel camera with a\n1D linear array or a line-sensor. We illustrate the optical architecture\nunderlying LiSens, build a prototype, and demonstrate results of a range of\nindoor and outdoor scenes. LiSens delivers on the promise of SMCs: imaging at a\nmegapixel resolution, at video rate, using an inexpensive low-resolution\nsensor.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2015 04:36:07 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Wang", "Jian", ""], ["Gupta", "Mohit", ""], ["Sankaranarayanan", "Aswin C.", ""]]}, {"id": "1503.04287", "submitter": "Pratik Agarwal", "authors": "Pratik Agarwal, Wolfram Burgard, Luciano Spinello", "title": "Metric Localization using Google Street View", "comments": "8 pages, 11 figures. 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate metrical localization is one of the central challenges in mobile\nrobotics. Many existing methods aim at localizing after building a map with the\nrobot. In this paper, we present a novel approach that instead uses geotagged\npanoramas from the Google Street View as a source of global positioning. We\nmodel the problem of localization as a non-linear least squares estimation in\ntwo phases. The first estimates the 3D position of tracked feature points from\nshort monocular camera sequences. The second computes the rigid body\ntransformation between the Street View panoramas and the estimated points. The\nonly input of this approach is a stream of monocular camera images and odometry\nestimates. We quantified the accuracy of the method by running the approach on\na robotic platform in a parking lot by using visual fiducials as ground truth.\nAdditionally, we applied the approach in the context of personal localization\nin a real urban scenario by using data from a Google Tango tablet.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2015 10:22:39 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2015 16:15:55 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Agarwal", "Pratik", ""], ["Burgard", "Wolfram", ""], ["Spinello", "Luciano", ""]]}, {"id": "1503.04338", "submitter": "Matthias Petschow", "authors": "Matthias Petschow", "title": "Towards radio astronomical imaging using an arbitrary basis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new generation of radio telescopes, such as the Square Kilometer Array\n(SKA), requires dramatic advances in computer hardware and software, in order\nto process the large amounts of produced data efficiently. In this document, we\nexplore a new approach to wide-field imaging. By generalizing the image\nreconstruction, which is performed by an inverse Fourier transform, to\narbitrary transformations, we gain enormous new possibilities. In particular,\nwe outline an approach that might allow to obtain a sky image of size P times Q\nin (optimal) O(PQ) time. This could be a step in the direction of real-time,\nwide-field sky imaging for future telescopes.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2015 19:56:00 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2015 14:03:46 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Petschow", "Matthias", ""]]}, {"id": "1503.04400", "submitter": "Jaros{\\l}aw Miszczak", "authors": "Jaros{\\l}aw Adam Miszczak", "title": "Separable and non-separable data representation for pattern\n  discrimination", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a complete work-flow, based on the language of quantum information\ntheory, suitable for processing data for the purpose of pattern recognition.\nThe main advantage of the introduced scheme is that it can be easily\nimplemented and applied to process real-world data using modest computation\nresources. At the same time it can be used to investigate the difference in the\npattern recognition resulting from the utilization of the tensor product\nstructure of the space of quantum states. We illustrate this difference by\nproviding a simple example based on the classification of 2D data.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2015 09:04:22 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Miszczak", "Jaros\u0142aw Adam", ""]]}, {"id": "1503.04444", "submitter": "Muhammad Masood Tahir Mr.", "authors": "Muhammad Masood Tahir, Ayyaz Hussain", "title": "Pattern Recognition of Bearing Faults using Smoother Statistical\n  Features", "comments": "This paper has been withdrawn by the author due to a crucial errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A pattern recognition (PR) based diagnostic scheme is presented to identify\nbearing faults, using time domain features. Vibration data is acquired from\nfaulty bearings using a test rig. The features are extracted from the data, and\nprocessed prior to utilize in the PR process. The processing involves smoothing\nof feature distributions. This reduces the undesired impact of vibration\nrandomness on the PR process, and thus enhances the diagnostic accuracy of the\nmodel.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2015 16:05:24 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 15:04:15 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Tahir", "Muhammad Masood", ""], ["Hussain", "Ayyaz", ""]]}, {"id": "1503.04585", "submitter": "Muneki Yasuda", "authors": "Muneki Yasuda, Shun Kataoka, Kazuyuki Tanaka", "title": "Statistical Analysis of Loopy Belief Propagation in Random Fields", "comments": null, "journal-ref": "Phys. Rev. E 92, 042120 (2015)", "doi": "10.1103/PhysRevE.92.042120", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loopy belief propagation (LBP), which is equivalent to the Bethe\napproximation in statistical mechanics, is a message-passing-type inference\nmethod that is widely used to analyze systems based on Markov random fields\n(MRFs). In this paper, we propose a message-passing-type method to analytically\nevaluate the quenched average of LBP in random fields by using the replica\ncluster variation method. The proposed analytical method is applicable to\ngeneral pair-wise MRFs with random fields whose distributions differ from each\nother and can give the quenched averages of the Bethe free energies over random\nfields, which are consistent with numerical results. The order of its\ncomputational cost is equivalent to that of standard LBP. In the latter part of\nthis paper, we describe the application of the proposed method to Bayesian\nimage restoration, in which we observed that our theoretical results are in\ngood agreement with the numerical results for natural images.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 10:08:01 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2015 10:22:41 GMT"}, {"version": "v3", "created": "Sun, 13 Sep 2015 09:17:43 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Yasuda", "Muneki", ""], ["Kataoka", "Shun", ""], ["Tanaka", "Kazuyuki", ""]]}, {"id": "1503.04596", "submitter": "Mark McDonnell", "authors": "Mark D. McDonnell and Tony Vladusich", "title": "Enhanced Image Classification With a Fast-Learning Shallow Convolutional\n  Neural Network", "comments": "7 pages, 2 figures, Paper at IJCNN 2015 (International Joint\n  Conference on Neural Networks, 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural network architecture and training method designed to\nenable very rapid training and low implementation complexity. Due to its\ntraining speed and very few tunable parameters, the method has strong potential\nfor applications requiring frequent retraining or online training. The approach\nis characterized by (a) convolutional filters based on biologically inspired\nvisual processing filters, (b) randomly-valued classifier-stage input weights,\n(c) use of least squares regression to train the classifier output weights in a\nsingle batch, and (d) linear classifier-stage output units. We demonstrate the\nefficacy of the method by applying it to image classification. Our results\nmatch existing state-of-the-art results on the MNIST (0.37% error) and\nNORB-small (2.2% error) image classification databases, but with very fast\ntraining times compared to standard deep network approaches. The network's\nperformance on the Google Street View House Number (SVHN) (4% error) database\nis also competitive with state-of-the art methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 10:41:30 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2015 06:26:40 GMT"}, {"version": "v3", "created": "Sat, 15 Aug 2015 13:02:08 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["McDonnell", "Mark D.", ""], ["Vladusich", "Tony", ""]]}, {"id": "1503.04598", "submitter": "Reza Sabzevari", "authors": "Reza Sabzevari, Vittori Murino, and Alessio Del Bue", "title": "PiMPeR: Piecewise Dense 3D Reconstruction from Multi-View and\n  Multi-Illumination Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of dense 3D reconstruction from\nmultiple view images subject to strong lighting variations. In this regard, a\nnew piecewise framework is proposed to explicitly take into account the change\nof illumination across several wide-baseline images. Unlike multi-view stereo\nand multi-view photometric stereo methods, this pipeline deals with\nwide-baseline images that are uncalibrated, in terms of both camera parameters\nand lighting conditions. Such a scenario is meant to avoid use of any specific\nimaging setup and provide a tool for normal users without any expertise. To the\nbest of our knowledge, this paper presents the first work that deals with such\nunconstrained setting. We propose a coarse-to-fine approach, in which a coarse\nmesh is first created using a set of geometric constraints and, then, fine\ndetails are recovered by exploiting photometric properties of the scene.\nAugmenting the fine details on the coarse mesh is done via a final optimization\nstep. Note that the method does not provide a generic solution for multi-view\nphotometric stereo problem but it relaxes several common assumptions of this\nproblem. The approach scales very well in size given its piecewise nature,\ndealing with large scale optimization and with severe missing data. Experiments\non a benchmark dataset Robot data-set show the method performance against 3D\nground truth.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 10:51:08 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2015 12:59:24 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Sabzevari", "Reza", ""], ["Murino", "Vittori", ""], ["Del Bue", "Alessio", ""]]}, {"id": "1503.04643", "submitter": "Dat Tien Ngo", "authors": "Dat Tien Ngo, Jonas Ostlund, Pascal Fua", "title": "Template-based Monocular 3D Shape Recovery using Laplacian Meshes", "comments": "Article", "journal-ref": null, "doi": "10.1109/TPAMI.2015.2435739", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that by extending the Laplacian formalism, which was first introduced\nin the Graphics community to regularize 3D meshes, we can turn the monocular 3D\nshape reconstruction of a deformable surface given correspondences with a\nreference image into a much better-posed problem. This allows us to quickly and\nreliably eliminate outliers by simply solving a linear least squares problem.\nThis yields an initial 3D shape estimate, which is not necessarily accurate,\nbut whose 2D projections are. The initial shape is then refined by a\nconstrained optimization problem to output the final surface reconstruction.\n  Our approach allows us to reduce the dimensionality of the surface\nreconstruction problem without sacrificing accuracy, thus allowing for\nreal-time implementations.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 13:42:09 GMT"}, {"version": "v2", "created": "Wed, 6 May 2015 11:49:04 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Ngo", "Dat Tien", ""], ["Ostlund", "Jonas", ""], ["Fua", "Pascal", ""]]}, {"id": "1503.04729", "submitter": "Carsten Gottschlich", "authors": "Carsten Gottschlich", "title": "Skilled Impostor Attacks Against Fingerprint Verification Systems And\n  Its Remedy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprint verification systems are becoming ubiquitous in everyday life.\nThis trend is propelled especially by the proliferation of mobile devices with\nfingerprint sensors such as smartphones and tablet computers, and fingerprint\nverification is increasingly applied for authenticating financial transactions.\nIn this study we describe a novel attack vector against fingerprint\nverification systems which we coin skilled impostor attack. We show that\nexisting protocols for performance evaluation of fingerprint verification\nsystems are flawed and as a consequence of this, the system's real\nvulnerability is systematically underestimated. We examine a scenario in which\na fingerprint verification system is tuned to operate at false acceptance rate\nof 0.1% using the traditional verification protocols with random impostors\n(zero-effort attacks). We demonstrate that an active and intelligent attacker\ncan achieve a chance of success in the area of 89% or more against this system\nby performing skilled impostor attacks. We describe a new protocol for\nevaluating fingerprint verification performance in order to improve the\nassessment of potential and limitations of fingerprint recognition systems.\nThis new evaluation protocol enables a more informed decision concerning the\noperating threshold in practical applications and the respective trade-off\nbetween security (low false acceptance rates) and usability (low false\nrejection rates). The skilled impostor attack is a general attack concept which\nis independent of specific databases or comparison algorithms. The proposed\nprotocol relying on skilled impostor attacks can directly be applied for\nevaluating the verification performance of other biometric modalities such as\ne.g. iris, face, ear, finger vein, gait or speaker recognition.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 17:03:30 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Gottschlich", "Carsten", ""]]}, {"id": "1503.04776", "submitter": "Mohammad Tofighi", "authors": "Mohammad Tofighi, Onur Yorulmaz and A. Enis Cetin", "title": "Phase and TV Based Convex Sets for Blind Deconvolution of Microscopic\n  Images", "comments": "Submitted to IEEE Selected Topics in Signal Processing", "journal-ref": null, "doi": "10.1109/JSTSP.2015.2502541", "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, two closed and convex sets for blind deconvolution problem\nare proposed. Most blurring functions in microscopy are symmetric with respect\nto the origin. Therefore, they do not modify the phase of the Fourier transform\n(FT) of the original image. As a result blurred image and the original image\nhave the same FT phase. Therefore, the set of images with a prescribed FT phase\ncan be used as a constraint set in blind deconvolution problems. Another convex\nset that can be used during the image reconstruction process is the epigraph\nset of Total Variation (TV) function. This set does not need a prescribed upper\nbound on the total variation of the image. The upper bound is automatically\nadjusted according to the current image of the restoration process. Both of\nthese two closed and convex sets can be used as a part of any blind\ndeconvolution algorithm. Simulation examples are presented.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 19:18:10 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Tofighi", "Mohammad", ""], ["Yorulmaz", "Onur", ""], ["Cetin", "A. Enis", ""]]}, {"id": "1503.04949", "submitter": "Martin Kiefel", "authors": "Varun Jampani and Martin Kiefel and Peter V. Gehler", "title": "Learning Sparse High Dimensional Filters: Image Filtering, Dense CRFs\n  and Bilateral Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bilateral filters have wide spread use due to their edge-preserving\nproperties. The common use case is to manually choose a parametric filter type,\nusually a Gaussian filter. In this paper, we will generalize the\nparametrization and in particular derive a gradient descent algorithm so the\nfilter parameters can be learned from data. This derivation allows to learn\nhigh dimensional linear filters that operate in sparsely populated feature\nspaces. We build on the permutohedral lattice construction for efficient\nfiltering. The ability to learn more general forms of high-dimensional filters\ncan be used in several diverse applications. First, we demonstrate the use in\napplications where single filter applications are desired for runtime reasons.\nFurther, we show how this algorithm can be used to learn the pairwise\npotentials in densely connected conditional random fields and apply these to\ndifferent image segmentation tasks. Finally, we introduce layers of bilateral\nfilters in CNNs and propose bilateral neural networks for the use of\nhigh-dimensional sparse data. This view provides new ways to encode model\nstructure into network architectures. A diverse set of experiments empirically\nvalidates the usage of general forms of filters.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 08:43:57 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2015 08:45:05 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2015 23:16:15 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Jampani", "Varun", ""], ["Kiefel", "Martin", ""], ["Gehler", "Peter V.", ""]]}, {"id": "1503.05038", "submitter": "Bojan Pepikj", "authors": "Bojan Pepik, Michael Stark, Peter Gehler, Tobias Ritschel, Bernt\n  Schiele", "title": "3D Object Class Detection in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object class detection has been a synonym for 2D bounding box localization\nfor the longest time, fueled by the success of powerful statistical learning\ntechniques, combined with robust image representations. Only recently, there\nhas been a growing interest in revisiting the promise of computer vision from\nthe early days: to precisely delineate the contents of a visual scene, object\nby object, in 3D. In this paper, we draw from recent advances in object\ndetection and 2D-3D object lifting in order to design an object class detector\nthat is particularly tailored towards 3D object class detection. Our 3D object\nclass detection method consists of several stages gradually enriching the\nobject detection output with object viewpoint, keypoints and 3D shape\nestimates. Following careful design, in each stage it constantly improves the\nperformance and achieves state-ofthe-art performance in simultaneous 2D\nbounding box and viewpoint estimation on the challenging Pascal3D+ dataset.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 13:34:22 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Pepik", "Bojan", ""], ["Stark", "Michael", ""], ["Gehler", "Peter", ""], ["Ritschel", "Tobias", ""], ["Schiele", "Bernt", ""]]}, {"id": "1503.05430", "submitter": "Toufiq Parag", "authors": "Toufiq Parag", "title": "What Properties are Desirable from an Electron Microscopy Segmentation\n  Algorithm", "comments": "Extended version of the ICCV 2015 paper: Efficient Classifier\n  Training to Minimize False Merges in Electron Microscopy Segmentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prospect of neural reconstruction from Electron Microscopy (EM) images\nhas been elucidated by the automatic segmentation algorithms. Although\nsegmentation algorithms eliminate the necessity of tracing the neurons by hand,\nsignificant manual effort is still essential for correcting the mistakes they\nmake. A considerable amount of human labor is also required for annotating\ngroundtruth volumes for training the classifiers of a segmentation framework.\nIt is critically important to diminish the dependence on human interaction in\nthe overall reconstruction system. This study proposes a novel classifier\ntraining algorithm for EM segmentation aimed to reduce the amount of manual\neffort demanded by the groundtruth annotation and error refinement tasks.\nInstead of using an exhaustive pixel level groundtruth, an active learning\nalgorithm is proposed for sparse labeling of pixel and boundaries of\nsuperpixels. Because over-segmentation errors are in general more tolerable and\neasier to correct than the under-segmentation errors, our algorithm is designed\nto prioritize minimization of false-merges over false-split mistakes. Our\nexperiments on both 2D and 3D data suggest that the proposed method yields\nsegmentation outputs that are more amenable to neural reconstruction than those\nof existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 14:38:00 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2015 16:06:22 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2015 14:05:32 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Parag", "Toufiq", ""]]}, {"id": "1503.05521", "submitter": "Tales Cesar de Oliveira Imbiriba", "authors": "Tales Imbiriba (1), Jos\\'e Carlos Moreira Bermudez (1), C\\'edric\n  Richard (2), Jean-Yves Tourneret (3) ((1) Federal University of Santa\n  Catarina, Florian\\'opolis, SC, Brazil, (2) Universit\\'e de Nice\n  Sophia-Antipolis, CNRS, Nice, France, (3) Universit\\'e de Toulouse,\n  IRIT-ENSEEIHT, CNRS, Toulouse, France)", "title": "Nonparametric Detection of Nonlinearly Mixed Pixels and Endmember\n  Estimation in Hyperspectral Images", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2015.2509258", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixing phenomena in hyperspectral images depend on a variety of factors such\nas the resolution of observation devices, the properties of materials, and how\nthese materials interact with incident light in the scene. Different parametric\nand nonparametric models have been considered to address hyperspectral unmixing\nproblems. The simplest one is the linear mixing model. Nevertheless, it has\nbeen recognized that mixing phenomena can also be nonlinear. The corresponding\nnonlinear analysis techniques are necessarily more challenging and complex than\nthose employed for linear unmixing. Within this context, it makes sense to\ndetect the nonlinearly mixed pixels in an image prior to its analysis, and then\nemploy the simplest possible unmixing technique to analyze each pixel. In this\npaper, we propose a technique for detecting nonlinearly mixed pixels. The\ndetection approach is based on the comparison of the reconstruction errors\nusing both a Gaussian process regression model and a linear regression model.\nThe two errors are combined into a detection statistics for which a probability\ndensity function can be reasonably approximated. We also propose an iterative\nendmember extraction algorithm to be employed in combination with the detection\nalgorithm. The proposed Detect-then-Unmix strategy, which consists of\nextracting endmembers, detecting nonlinearly mixed pixels and unmixing, is\ntested with synthetic and real images.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 18:20:24 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Imbiriba", "Tales", ""], ["Bermudez", "Jos\u00e9 Carlos Moreira", ""], ["Richard", "C\u00e9dric", ""], ["Tourneret", "Jean-Yves", ""]]}, {"id": "1503.05528", "submitter": "Andres Almansa", "authors": "Alasdair Newson, Andr\\'es Almansa (LTCI), Matthieu Fradet, Yann\n  Gousseau, Patrick P\\'erez", "title": "Video Inpainting of Complex Scenes", "comments": null, "journal-ref": "SIAM Journal on Imaging Sciences, Society for Industrial and\n  Applied Mathematics, 2014, 7 (4), pp.1993-2019", "doi": "10.1137/140954933", "report-no": null, "categories": "cs.CV cs.MM eess.IV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an automatic video inpainting algorithm which relies on the\noptimisation of a global, patch-based functional. Our algorithm is able to deal\nwith a variety of challenging situations which naturally arise in video\ninpainting, such as the correct reconstruction of dynamic textures, multiple\nmoving objects and moving background. Furthermore, we achieve this in an order\nof magnitude less execution time with respect to the state-of-the-art. We are\nalso able to achieve good quality results on high definition videos. Finally,\nwe provide specific algorithmic details to make implementation of our algorithm\nas easy as possible. The resulting algorithm requires no segmentation or manual\ninput other than the definition of the inpainting mask, and can deal with a\nwider variety of situations than is handled by previous work. 1. Introduction.\nAdvanced image and video editing techniques are increasingly common in the\nimage processing and computer vision world, and are also starting to be used in\nmedia entertainment. One common and difficult task closely linked to the world\nof video editing is image and video \" inpainting \". Generally speaking, this is\nthe task of replacing the content of an image or video with some other content\nwhich is visually pleasing. This subject has been extensively studied in the\ncase of images, to such an extent that commercial image inpainting products\ndestined for the general public are available, such as Photoshop's \" Content\nAware fill \" [1]. However, while some impressive results have been obtained in\nthe case of videos, the subject has been studied far less extensively than\nimage inpainting. This relative lack of research can largely be attributed to\nhigh time complexity due to the added temporal dimension. Indeed, it has only\nvery recently become possible to produce good quality inpainting results on\nhigh definition videos, and this only in a semi-automatic manner. Nevertheless,\nhigh-quality video inpainting has many important and useful applications such\nas film restoration, professional post-production in cinema and video editing\nfor personal use. For this reason, we believe that an automatic, generic video\ninpainting algorithm would be extremely useful for both academic and\nprofessional communities.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 18:35:40 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2015 06:43:01 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Newson", "Alasdair", "", "LTCI"], ["Almansa", "Andr\u00e9s", "", "LTCI"], ["Fradet", "Matthieu", ""], ["Gousseau", "Yann", ""], ["P\u00e9rez", "Patrick", ""]]}, {"id": "1503.05689", "submitter": "Bashir Sadiq Mr", "authors": "B.O Sadiq, S.M Sani, S Garba", "title": "Edge Detection: A Collection of Pixel based Approach for Colored Images", "comments": "5 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing traditional edge detection algorithms process a single pixel on\nan image at a time, thereby calculating a value which shows the edge magnitude\nof the pixel and the edge orientation. Most of these existing algorithms\nconvert the coloured images into gray scale before detection of edges. However,\nthis process leads to inaccurate precision of recognized edges, thus producing\nfalse and broken edges in the image. This paper presents a profile modelling\nscheme for collection of pixels based on the step and ramp edges, with a view\nto reducing the false and broken edges present in the image. The collection of\npixel scheme generated is used with the Vector Order Statistics to reduce the\nimprecision of recognized edges when converting from coloured to gray scale\nimages. The Pratt Figure of Merit (PFOM) is used as a quantitative comparison\nbetween the existing traditional edge detection algorithm and the developed\nalgorithm as a means of validation. The PFOM value obtained for the developed\nalgorithm is 0.8480, which showed an improvement over the existing traditional\nedge detection algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 10:08:37 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Sadiq", "B. O", ""], ["Sani", "S. M", ""], ["Garba", "S", ""]]}, {"id": "1503.05692", "submitter": "Bashir Sadiq Mr", "authors": "B O. Sadiq, S.M. Sani and S. Garba", "title": "An approach to improving edge detection for facial and remotely sensed\n  images using vector order statistics", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an improved edge detection algorithm for facial and\nremotely sensed images using vector order statistics. The developed algorithm\nprocesses colored images directly without been converted to gray scale. A\nnumber of the existing algorithms converts the colored images into gray scale\nbefore detection of edges. But this process leads to inaccurate precision of\nrecognized edges, thus producing false and broken edges in the output edge map.\nFacial and remotely sensed images consist of curved edge lines which have to be\ndetected continuously to prevent broken edges. In order to deal with this, a\ncollection of pixel approach is introduced with a view to minimizing the false\nand broken edges that exists in the generated output edge map of facial and\nremotely sensed images.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 10:18:05 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Sadiq", "B O.", ""], ["Sani", "S. M.", ""], ["Garba", "S.", ""]]}, {"id": "1503.05767", "submitter": "Francois Chung", "authors": "Fran\\c{c}ois Chung and Tom\\'as Rodr\\'iguez", "title": "Automatic Pollen Grain and Exine Segmentation from Microscope Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose an automatic method for the segmentation of\npollen grains from microscope images, followed by the automatic segmentation of\ntheir exine. The objective of exine segmentation is to separate the pollen\ngrain in two regions of interest: exine and inner part. A coarse-to-fine\napproach ensures a smooth and accurate segmentation of both structures. As a\nrough stage, grain segmentation is performed by a procedure involving\nclustering and morphological operations, while the exine is approximated by an\niterative procedure consisting in consecutive cropping steps of the pollen\ngrain. A snake-based segmentation is performed to refine the segmentation of\nboth structures. Results have shown that our segmentation method is able to\ndeal with different pollen types, as well as with different types of exine and\ninner part appearance. The proposed segmentation method aims to be generic and\nhas been designed as one of the core steps of an automatic pollen\nclassification framework.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 13:58:15 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Chung", "Fran\u00e7ois", ""], ["Rodr\u00edguez", "Tom\u00e1s", ""]]}, {"id": "1503.05768", "submitter": "Yunjin Chen", "authors": "Yunjin Chen, Wei Yu, Thomas Pock", "title": "On learning optimized reaction diffusion processes for effective image\n  restoration", "comments": "9 pages, 3 figures, 3 tables. CVPR2015 oral presentation together\n  with the supplemental material of 13 pages, 8 pages (Notes on diffusion\n  networks)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For several decades, image restoration remains an active research topic in\nlow-level computer vision and hence new approaches are constantly emerging.\nHowever, many recently proposed algorithms achieve state-of-the-art performance\nonly at the expense of very high computation time, which clearly limits their\npractical relevance. In this work, we propose a simple but effective approach\nwith both high computational efficiency and high restoration quality. We extend\nconventional nonlinear reaction diffusion models by several parametrized linear\nfilters as well as several parametrized influence functions. We propose to\ntrain the parameters of the filters and the influence functions through a loss\nbased approach. Experiments show that our trained nonlinear reaction diffusion\nmodels largely benefit from the training of the parameters and finally lead to\nthe best reported performance on common test datasets for image restoration.\nDue to their structural simplicity, our trained models are highly efficient and\nare also well-suited for parallel computation on GPUs.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 14:01:42 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2015 19:59:44 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Chen", "Yunjin", ""], ["Yu", "Wei", ""], ["Pock", "Thomas", ""]]}, {"id": "1503.05782", "submitter": "Sheng Huang", "authors": "Sheng Huang and Mohamed Elhoseiny and Ahmed Elgammal and Dan Yang", "title": "Learning Hypergraph-regularized Attribute Predictors", "comments": "This is an attribute learning paper accepted by CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We present a novel attribute learning framework named Hypergraph-based\nAttribute Predictor (HAP). In HAP, a hypergraph is leveraged to depict the\nattribute relations in the data. Then the attribute prediction problem is\ncasted as a regularized hypergraph cut problem in which HAP jointly learns a\ncollection of attribute projections from the feature space to a hypergraph\nembedding space aligned with the attribute space. The learned projections\ndirectly act as attribute classifiers (linear and kernelized). This formulation\nleads to a very efficient approach. By considering our model as a multi-graph\ncut task, our framework can flexibly incorporate other available information,\nin particular class label. We apply our approach to attribute prediction,\nZero-shot and $N$-shot learning tasks. The results on AWA, USAA and CUB\ndatabases demonstrate the value of our methods in comparison with the\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 14:31:56 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Huang", "Sheng", ""], ["Elhoseiny", "Mohamed", ""], ["Elgammal", "Ahmed", ""], ["Yang", "Dan", ""]]}, {"id": "1503.05786", "submitter": "Fran\\c{c}ois Chung Ph.D.", "authors": "Fran\\c{c}ois Chung and Tom\\'as Rodr\\'iguez", "title": "A General Framework for Multi-focal Image Classification and\n  Authentication: Application to Microscope Pollen Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a general framework for multi-focal image\nclassification and authentication, the methodology being demonstrated on\nmicroscope pollen images. The framework is meant to be generic and based on a\nbrute force-like approach aimed to be efficient not only on any kind, and any\nnumber, of pollen images (regardless of the pollen type), but also on any kind\nof multi-focal images. All stages of the framework's pipeline are designed to\nbe used in an automatic fashion. First, the optimal focus is selected using the\nabsolute gradient method. Then, pollen grains are extracted using a\ncoarse-to-fine approach involving both clustering and morphological techniques\n(coarse stage), and a snake-based segmentation (fine stage). Finally, features\nare extracted and selected using a generalized approach, and their\nclassification is tested with four classifiers: Weighted Neighbor Distance,\nNeural Network, Decision Tree and Random Forest. The latter method, which has\nshown the best and more robust classification accuracy results (above 97\\% for\nany number of pollen types), is finally used for the authentication stage.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 14:44:29 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Chung", "Fran\u00e7ois", ""], ["Rodr\u00edguez", "Tom\u00e1s", ""]]}, {"id": "1503.05830", "submitter": "Lucas Rioux-Maldague", "authors": "Lucas Rioux-Maldague, Philippe Gigu\\`ere", "title": "Sign Language Fingerspelling Classification from Depth and Color Images\n  using a Deep Belief Network", "comments": "Published in 2014 Canadian Conference on Computer and Robot Vision", "journal-ref": null, "doi": "10.1109/CRV.2014.20", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic sign language recognition is an open problem that has received a\nlot of attention recently, not only because of its usefulness to signers, but\nalso due to the numerous applications a sign classifier can have. In this\narticle, we present a new feature extraction technique for hand pose\nrecognition using depth and intensity images captured from a Microsoft Kinect\nsensor. We applied our technique to American Sign Language fingerspelling\nclassification using a Deep Belief Network, for which our feature extraction\ntechnique is tailored. We evaluated our results on a multi-user data set with\ntwo scenarios: one with all known users and one with an unseen user. We\nachieved 99% recall and precision on the first, and 77% recall and 79%\nprecision on the second. Our method is also capable of real-time sign\nclassification and is adaptive to any environment or lightning intensity.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 16:30:10 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Rioux-Maldague", "Lucas", ""], ["Gigu\u00e8re", "Philippe", ""]]}, {"id": "1503.05860", "submitter": "Leonid Pishchulin", "authors": "Leonid Pishchulin, Stefanie Wuhrer, Thomas Helten, Christian Theobalt,\n  Bernt Schiele", "title": "Building Statistical Shape Spaces for 3D Human Modeling", "comments": "Published in Pattern Recognition 2017", "journal-ref": null, "doi": "10.1016/j.patcog.2017.02.018", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical models of 3D human shape and pose learned from scan databases\nhave developed into valuable tools to solve a variety of vision and graphics\nproblems. Unfortunately, most publicly available models are of limited\nexpressiveness as they were learned on very small databases that hardly reflect\nthe true variety in human body shapes. In this paper, we contribute by\nrebuilding a widely used statistical body representation from the largest\ncommercially available scan database, and making the resulting model available\nto the community (visit http://humanshape.mpi-inf.mpg.de). As preprocessing\nseveral thousand scans for learning the model is a challenge in itself, we\ncontribute by developing robust best practice solutions for scan alignment that\nquantitatively lead to the best learned models. We make implementations of\nthese preprocessing steps also publicly available. We extensively evaluate the\nimproved accuracy and generality of our new model, and show its improved\nperformance for human body reconstruction from sparse input data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 17:59:57 GMT"}, {"version": "v2", "created": "Fri, 3 Mar 2017 20:44:19 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Pishchulin", "Leonid", ""], ["Wuhrer", "Stefanie", ""], ["Helten", "Thomas", ""], ["Theobalt", "Christian", ""], ["Schiele", "Bernt", ""]]}, {"id": "1503.05947", "submitter": "Yanlai Chen", "authors": "Yanlai Chen", "title": "Reduced Basis Decomposition: a Certified and Fast Lossy Data Compression\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.AI cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimension reduction is often needed in the area of data mining. The goal of\nthese methods is to map the given high-dimensional data into a low-dimensional\nspace preserving certain properties of the initial data. There are two kinds of\ntechniques for this purpose. The first, projective methods, builds an explicit\nlinear projection from the high-dimensional space to the low-dimensional one.\nOn the other hand, the nonlinear methods utilizes nonlinear and implicit\nmapping between the two spaces. In both cases, the methods considered in\nliterature have usually relied on computationally very intensive matrix\nfactorizations, frequently the Singular Value Decomposition (SVD). The\ncomputational burden of SVD quickly renders these dimension reduction methods\ninfeasible thanks to the ever-increasing sizes of the practical datasets.\n  In this paper, we present a new decomposition strategy, Reduced Basis\nDecomposition (RBD), which is inspired by the Reduced Basis Method (RBM). Given\n$X$ the high-dimensional data, the method approximates it by $Y \\, T (\\approx\nX)$ with $Y$ being the low-dimensional surrogate and $T$ the transformation\nmatrix. $Y$ is obtained through a greedy algorithm thus extremely efficient. In\nfact, it is significantly faster than SVD with comparable accuracy. $T$ can be\ncomputed on the fly. Moreover, unlike many compression algorithms, it easily\nfinds the mapping for an arbitrary ``out-of-sample'' vector and it comes with\nan ``error indicator'' certifying the accuracy of the compression. Numerical\nresults are shown validating these claims.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 21:10:57 GMT"}], "update_date": "2015-03-23", "authors_parsed": [["Chen", "Yanlai", ""]]}, {"id": "1503.06275", "submitter": "Kazi Tanvir Ahmed Siddiqui", "authors": "Kazi Tanvir Ahmed Siddiqui and Abu Wasif", "title": "Skin Detection of Animation Characters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing popularity of animes makes it vulnerable to unwanted usages\nlike copyright violations and pornography. That is why, we need to develop a\nmethod to detect and recognize animation characters. Skin detection is one of\nthe most important steps in this way. Though there are some methods to detect\nhuman skin color, but those methods do not work properly for anime characters.\nAnime skin varies greatly from human skin in color, texture, tone and in\ndifferent kinds of lighting. They also vary greatly among themselves. Moreover,\nmany other things (for example leather, shirt, hair etc.), which are not skin,\ncan have color similar to skin. In this paper, we have proposed three methods\nthat can identify an anime character skin more successfully as compared with\nKovac, Swift, Saleh and Osman methods, which are primarily designed for human\nskin detection. Our methods are based on RGB values and their comparative\nrelations.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2015 07:36:47 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Siddiqui", "Kazi Tanvir Ahmed", ""], ["Wasif", "Abu", ""]]}, {"id": "1503.06323", "submitter": "Sabyasachi  Mukhopadhyay", "authors": "Sabyasachi Mukhopadhyay, Nandan K. Das, Soham Mandal, Sawon Pratiher,\n  Asish Mitra, Asima Pradhan, Nirmalya Ghosh, Prasanta K. Panigrahi", "title": "Wavelet based approach for tissue fractal parameter measurement: Pre\n  cancer detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we have carried out the detail studies of pre-cancer by\nwavelet coherency and multifractal based detrended fluctuation analysis (MFDFA)\non differential interference contrast (DIC) images of stromal region among\ndifferent grades of pre-cancer tissues. Discrete wavelet transform (DWT)\nthrough Daubechies basis has been performed for identifying fluctuations over\npolynomial trends for clear characterization and differentiation of tissues.\nWavelet coherence plots are performed for identifying the level of correlation\nin time scale plane between normal and various grades of DIC samples. Applying\nMFDFA on refractive index variations of cervical tissues, we have observed that\nthe values of Hurst exponent (correlation) decreases from healthy (normal) to\npre-cancer tissues. The width of singularity spectrum has a sudden degradation\nat grade-I in comparison of healthy (normal) tissue but later on it increases\nas cancer progresses from grade-II to grade-III.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2015 17:09:40 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Mukhopadhyay", "Sabyasachi", ""], ["Das", "Nandan K.", ""], ["Mandal", "Soham", ""], ["Pratiher", "Sawon", ""], ["Mitra", "Asish", ""], ["Pradhan", "Asima", ""], ["Ghosh", "Nirmalya", ""], ["Panigrahi", "Prasanta K.", ""]]}, {"id": "1503.06350", "submitter": "Nikolaos Karianakis", "authors": "Nikolaos Karianakis, Thomas J. Fuchs and Stefano Soatto", "title": "Boosting Convolutional Features for Robust Object Proposals", "comments": "9 pages, 4 figures, 2 tables, 42 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) have demonstrated excellent\nperformance in image classification, but still show room for improvement in\nobject-detection tasks with many categories, in particular for cluttered scenes\nand occlusion. Modern detection algorithms like Regions with CNNs (Girshick et\nal., 2014) rely on Selective Search (Uijlings et al., 2013) to propose regions\nwhich with high probability represent objects, where in turn CNNs are deployed\nfor classification. Selective Search represents a family of sophisticated\nalgorithms that are engineered with multiple segmentation, appearance and\nsaliency cues, typically coming with a significant run-time overhead.\nFurthermore, (Hosang et al., 2014) have shown that most methods suffer from low\nreproducibility due to unstable superpixels, even for slight image\nperturbations. Although CNNs are subsequently used for classification in\ntop-performing object-detection pipelines, current proposal methods are\nagnostic to how these models parse objects and their rich learned\nrepresentations. As a result they may propose regions which may not resemble\nhigh-level objects or totally miss some of them. To overcome these drawbacks we\npropose a boosting approach which directly takes advantage of hierarchical CNN\nfeatures for detecting regions of interest fast. We demonstrate its performance\non ImageNet 2013 detection benchmark and compare it with state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2015 20:54:39 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Karianakis", "Nikolaos", ""], ["Fuchs", "Thomas J.", ""], ["Soatto", "Stefano", ""]]}, {"id": "1503.06383", "submitter": "Angshul Majumdar Dr.", "authors": "Angshul Majumdar", "title": "Real-time Dynamic MRI Reconstruction using Stacked Denoising Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we address the problem of real-time dynamic MRI reconstruction.\nThere are a handful of studies on this topic; these techniques are either based\non compressed sensing or employ Kalman Filtering. These techniques cannot\nachieve the reconstruction speed necessary for real-time reconstruction. In\nthis work, we propose a new approach to MRI reconstruction. We learn a\nnon-linear mapping from the unstructured aliased images to the corresponding\nclean images using a stacked denoising autoencoder (SDAE). The training for\nSDAE is slow, but the reconstruction is very fast - only requiring a few matrix\nvector multiplications. In this work, we have shown that using SDAE one can\nreconstruct the MRI frame faster than the data acquisition rate, thereby\nachieving real-time reconstruction. The quality of reconstruction is of the\nsame order as a previous compressed sensing based online reconstruction\ntechnique.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 04:09:31 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Majumdar", "Angshul", ""]]}, {"id": "1503.06465", "submitter": "Joao Carreira", "authors": "Joao Carreira, Sara Vicente, Lourdes Agapito and Jorge Batista", "title": "Lifting Object Detection Datasets into 3D", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While data has certainly taken the center stage in computer vision in recent\nyears, it can still be difficult to obtain in certain scenarios. In particular,\nacquiring ground truth 3D shapes of objects pictured in 2D images remains a\nchallenging feat and this has hampered progress in recognition-based object\nreconstruction from a single image. Here we propose to bypass previous\nsolutions such as 3D scanning or manual design, that scale poorly, and instead\npopulate object category detection datasets semi-automatically with dense,\nper-object 3D reconstructions, bootstrapped from:(i) class labels, (ii) ground\ntruth figure-ground segmentations and (iii) a small set of keypoint\nannotations. Our proposed algorithm first estimates camera viewpoint using\nrigid structure-from-motion and then reconstructs object shapes by optimizing\nover visual hull proposals guided by loose within-class shape similarity\nassumptions. The visual hull sampling process attempts to intersect an object's\nprojection cone with the cones of minimal subsets of other similar objects\namong those pictured from certain vantage points. We show that our method is\nable to produce convincing per-object 3D reconstructions and to accurately\nestimate cameras viewpoints on one of the most challenging existing\nobject-category detection datasets, PASCAL VOC. We hope that our results will\nre-stimulate interest on joint object recognition and 3D reconstruction from a\nsingle image.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 19:26:57 GMT"}, {"version": "v2", "created": "Sun, 31 Jul 2016 09:49:19 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Carreira", "Joao", ""], ["Vicente", "Sara", ""], ["Agapito", "Lourdes", ""], ["Batista", "Jorge", ""]]}, {"id": "1503.06561", "submitter": "Ankit Gupta", "authors": "Ankit Gupta, Ashish Oberoi", "title": "A Comparative Analysis of Tensor Decomposition Models Using Hyper\n  Spectral Image", "comments": "7 pages, 3 figures,1 table", "journal-ref": "International Journal of Computer Science Trends and Technology\n  (IJCST) V3(2): Page(5-11) Mar-Apr 2015. ISSN: 2347-8578", "doi": null, "report-no": null, "categories": "cs.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyper spectral imaging is a remote sensing technology, providing variety of\napplications such as material identification, space object identification,\nplanetary exploitation etc. It deals with capturing continuum of images of the\nearth surface from different angles. Due to the multidimensional nature of the\nimage, multi-way arrays are one of the possible solutions for analyzing hyper\nspectral data. This multi-way array is called tensor. Our approach deals with\nimplementing three decomposition models LMLRA, BTD and CPD to the sample data\nfor choosing the best decomposition of the data set. The results have proved\nthat Block Term Decomposition (BTD) is the best tensor model for decomposing\nthe hyper spectral image in to resultant factor matrices.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 09:08:50 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Gupta", "Ankit", ""], ["Oberoi", "Ashish", ""]]}, {"id": "1503.06642", "submitter": "Junyan Wang", "authors": "Junyan Wang and Sai-Kit Yeung", "title": "Superpixelizing Binary MRF for Image Labeling Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superpixels have become prevalent in computer vision. They have been used to\nachieve satisfactory performance at a significantly smaller computational cost\nfor various tasks. People have also combined superpixels with Markov random\nfield (MRF) models. However, it often takes additional effort to formulate MRF\non superpixel-level, and to the best of our knowledge there exists no\nprincipled approach to obtain this formulation. In this paper, we show how\ngeneric pixel-level binary MRF model can be solved in the superpixel space. As\nthe main contribution of this paper, we show that a superpixel-level MRF can be\nderived from the pixel-level MRF by substituting the superpixel representation\nof the pixelwise label into the original pixel-level MRF energy. The resultant\nsuperpixel-level MRF energy also remains submodular for a submodular\npixel-level MRF. The derived formula hence gives us a handy way to formulate\nMRF energy in superpixel-level. In the experiments, we demonstrate the efficacy\nof our approach on several computer vision problems.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 14:09:09 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Wang", "Junyan", ""], ["Yeung", "Sai-Kit", ""]]}, {"id": "1503.06643", "submitter": "Mrinal Haloi", "authors": "Mrinal Haloi", "title": "A novel pLSA based Traffic Signs Classification System", "comments": "APMediaCast-2015, Bali, Indonesia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we developed a novel and fast traffic sign recognition system, a\nvery important part for advanced driver assistance system and for autonomous\ndriving. Traffic signs play a very vital role in safe driving and avoiding\naccident. We have used image processing and topic discovery model pLSA to\ntackle this challenging multiclass classification problem. Our algorithm is\nconsist of two parts, shape classification and sign classification for improved\naccuracy. For processing and representation of image we have used bag of\nfeatures model with SIFT local descriptor. Where a visual vocabulary of size\n300 words are formed using k-means codebook formation algorithm. We exploited\nthe concept that every image is a collection of visual topics and images having\nsame topics will belong to same category. Our algorithm is tested on German\ntraffic sign recognition benchmark (GTSRB) and gives very promising result near\nto existing state of the art techniques.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 14:10:44 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Haloi", "Mrinal", ""]]}, {"id": "1503.06648", "submitter": "Mrinal Haloi", "authors": "Mrinal Haloi and Dinesh Babu Jayagopi", "title": "Vehicle Local Position Estimation System", "comments": "Accepted in ICVES-2014, Hyderabad, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a robust vehicle local position estimation with the help of\nsingle camera sensor and GPS is presented. A modified Inverse Perspective\nMapping, illuminant Invariant techniques and object detection based approach is\nused to localize the vehicle in the road. Vehicles current lane, its position\nfrom road boundary and other cars are used to define its local position. For\nthis purpose Lane markings are detected using a Laplacian edge feature, robust\nto shadowing. Effect of shadowing and extra sun light are removed using Lab\ncolor space and illuminant invariant techniques. Lanes are assumed to be as\nparabolic model and fitted using robust RANSAC. This method can reliably detect\nall lanes of the road, estimate lane departure angle and local position of\nvehicle relative to lanes, road boundary and other cars. Different type of\nobstacle like pedestrians, vehicles are detected using HOG feature based\ndeformable part model.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 14:14:10 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Haloi", "Mrinal", ""], ["Jayagopi", "Dinesh Babu", ""]]}, {"id": "1503.06680", "submitter": "Kieran Larkin", "authors": "Kieran Gerard Larkin", "title": "Structural Similarity Index SSIMplified: Is there really a simpler\n  concept at the heart of image quality measurement?", "comments": "Updated abstract and references. 4 pages total, main analysis 2\n  pages, notes and minimal references 1 page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Structural Similarity Index (SSIM) is generally considered to be a\nmilestone in the recent history of Image Quality Assessment (IQA). Alas, SSIM's\naccepted development from the product of three heuristic factors continues to\nobscure it's real underlying simplicity. Starting instead from a\nsymmetric-antisymmetric reformulation we first show SSIM to be a contrast or\nvisibility function in the classic sense. Furthermore, the previously enigmatic\nstructural covariance is revealed to be the difference of variances. The second\nstep, eliminating the intrinsic quadratic nature of SSIM, allows a near linear\ncorrelation with human observer scores, and without invoking the usual, but\narbitrary, sigmoid model fitting. We conclude that SSIM can be re-interpreted\nin terms of perceptual masking: it is essentially equivalent to a normalised\nerror or noise visibility function (NVF), and, furthermore, the NVF alone\nexplains it success in modelling perceptual image quality. We use the term\nDissimilarity Quotient (DQ) for the specifically anti/symmetric SSIM derived\nNVF. It seems that IQA researchers may now have two choices: 1) Continue to use\nthe complex SSIM formula, but noting that SSIM only works coincidentally since\nthe covariance term is actually the mean square error (MSE) in disguise. 2) Use\nthe simplest of all perceptually-masked image quality metrics, namely NVF or\nDQ. On this choice Occam is clear: in the absence of differences in predictive\nability, the fewer assumptions that are made, the better.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 21:27:49 GMT"}, {"version": "v2", "created": "Mon, 25 May 2015 01:53:07 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Larkin", "Kieran Gerard", ""]]}, {"id": "1503.06699", "submitter": "Zhengwu Zhang", "authors": "Zhengwu Zhang, Jingyong Su, Eric Klassen, Huiling Le, and Anuj\n  Srivastava", "title": "Video-Based Action Recognition Using Rate-Invariant Analysis of\n  Covariance Trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical classification of actions in videos is mostly performed by\nextracting relevant features, particularly covariance features, from image\nframes and studying time series associated with temporal evolutions of these\nfeatures. A natural mathematical representation of activity videos is in form\nof parameterized trajectories on the covariance manifold, i.e. the set of\nsymmetric, positive-definite matrices (SPDMs). The variable execution-rates of\nactions implies variable parameterizations of the resulting trajectories, and\ncomplicates their classification. Since action classes are invariant to\nexecution rates, one requires rate-invariant metrics for comparing\ntrajectories. A recent paper represented trajectories using their transported\nsquare-root vector fields (TSRVFs), defined by parallel translating\nscaled-velocity vectors of trajectories to a reference tangent space on the\nmanifold. To avoid arbitrariness of selecting the reference and to reduce\ndistortion introduced during this mapping, we develop a purely intrinsic\napproach where SPDM trajectories are represented by redefining their TSRVFs at\nthe starting points of the trajectories, and analyzed as elements of a vector\nbundle on the manifold. Using a natural Riemannain metric on vector bundles of\nSPDMs, we compute geodesic paths and geodesic distances between trajectories in\nthe quotient space of this vector bundle, with respect to the\nre-parameterization group. This makes the resulting comparison of trajectories\ninvariant to their re-parameterization. We demonstrate this framework on two\napplications involving video classification: visual speech recognition or\nlip-reading and hand-gesture recognition. In both cases we achieve results\neither comparable to or better than the current literature.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 16:08:08 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2015 20:18:46 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Zhang", "Zhengwu", ""], ["Su", "Jingyong", ""], ["Klassen", "Eric", ""], ["Le", "Huiling", ""], ["Srivastava", "Anuj", ""]]}, {"id": "1503.06775", "submitter": "Robert Amelard", "authors": "Robert Amelard, Christian Scharfenberger, Farnoud Kazemzadeh, Kaylen\n  J. Pfisterer, Bill S. Lin, Alexander Wong, David A. Clausi", "title": "Non-contact transmittance photoplethysmographic imaging (PPGI) for\n  long-distance cardiovascular monitoring", "comments": "13 pages, 6 figures, submitted to Nature Scientific Reports, for\n  associated video files see\n  http://vip.uwaterloo.ca/publications/non-contact-transmittance-photoplethysmographic-imaging-ppgi-long-distance", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.optics cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photoplethysmography (PPG) devices are widely used for monitoring\ncardiovascular function. However, these devices require skin contact, which\nrestrict their use to at-rest short-term monitoring using single-point\nmeasurements. Photoplethysmographic imaging (PPGI) has been recently proposed\nas a non-contact monitoring alternative by measuring blood pulse signals across\na spatial region of interest. Existing systems operate in reflectance mode, of\nwhich many are limited to short-distance monitoring and are prone to temporal\nchanges in ambient illumination. This paper is the first study to investigate\nthe feasibility of long-distance non-contact cardiovascular monitoring at the\nsupermeter level using transmittance PPGI. For this purpose, a novel PPGI\nsystem was designed at the hardware and software level using ambient correction\nvia temporally coded illumination (TCI) and signal processing for PPGI signal\nextraction. Experimental results show that the processing steps yield a\nsubstantially more pulsatile PPGI signal than the raw acquired signal,\nresulting in statistically significant increases in correlation to ground-truth\nPPG in both short- ($p \\in [<0.0001, 0.040]$) and long-distance ($p \\in\n[<0.0001, 0.056]$) monitoring. The results support the hypothesis that\nlong-distance heart rate monitoring is feasible using transmittance PPGI,\nallowing for new possibilities of monitoring cardiovascular function in a\nnon-contact manner.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 19:19:40 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Amelard", "Robert", ""], ["Scharfenberger", "Christian", ""], ["Kazemzadeh", "Farnoud", ""], ["Pfisterer", "Kaylen J.", ""], ["Lin", "Bill S.", ""], ["Wong", "Alexander", ""], ["Clausi", "David A.", ""]]}, {"id": "1503.06813", "submitter": "Tarek El-Gaaly", "authors": "Haopeng Zhang, Tarek El-Gaaly, Ahmed Elgammal, Zhiguo Jiang", "title": "Factorization of View-Object Manifolds for Joint Object Recognition and\n  Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to large variations in shape, appearance, and viewing conditions, object\nrecognition is a key precursory challenge in the fields of object manipulation\nand robotic/AI visual reasoning in general. Recognizing object categories,\nparticular instances of objects and viewpoints/poses of objects are three\ncritical subproblems robots must solve in order to accurately grasp/manipulate\nobjects and reason about their environments. Multi-view images of the same\nobject lie on intrinsic low-dimensional manifolds in descriptor spaces (e.g.\nvisual/depth descriptor spaces). These object manifolds share the same topology\ndespite being geometrically different. Each object manifold can be represented\nas a deformed version of a unified manifold. The object manifolds can thus be\nparameterized by its homeomorphic mapping/reconstruction from the unified\nmanifold. In this work, we develop a novel framework to jointly solve the three\nchallenging recognition sub-problems, by explicitly modeling the deformations\nof object manifolds and factorizing it in a view-invariant space for\nrecognition. We perform extensive experiments on several challenging datasets\nand achieve state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 20:05:36 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2015 02:59:41 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Zhang", "Haopeng", ""], ["El-Gaaly", "Tarek", ""], ["Elgammal", "Ahmed", ""], ["Jiang", "Zhiguo", ""]]}, {"id": "1503.06917", "submitter": "Yilin Wang", "authors": "Qiang Zhang, Yilin Wang, Baoxin Li", "title": "Unsupervised Video Analysis Based on a Spatiotemporal Saliency Detector", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual saliency, which predicts regions in the field of view that draw the\nmost visual attention, has attracted a lot of interest from researchers. It has\nalready been used in several vision tasks, e.g., image classification, object\ndetection, foreground segmentation. Recently, the spectrum analysis based\nvisual saliency approach has attracted a lot of interest due to its simplicity\nand good performance, where the phase information of the image is used to\nconstruct the saliency map. In this paper, we propose a new approach for\ndetecting spatiotemporal visual saliency based on the phase spectrum of the\nvideos, which is easy to implement and computationally efficient. With the\nproposed algorithm, we also study how the spatiotemporal saliency can be used\nin two important vision task, abnormality detection and spatiotemporal interest\npoint detection. The proposed algorithm is evaluated on several commonly used\ndatasets with comparison to the state-of-art methods from the literature. The\nexperiments demonstrate the effectiveness of the proposed approach to\nspatiotemporal visual saliency detection and its application to the above\nvision tasks\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 05:25:45 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Zhang", "Qiang", ""], ["Wang", "Yilin", ""], ["Li", "Baoxin", ""]]}, {"id": "1503.06959", "submitter": "Luca Baroffio", "authors": "Luca Baroffio, Matteo Cesana, Alessandro Redondi, Marco Tagliasacchi", "title": "Fast keypoint detection in video sequences", "comments": "submitted to IEEE International Conference on Image Processing 2015", "journal-ref": null, "doi": "10.1109/ICASSP.2016.7471895", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of computer vision tasks exploit a succinct representation of the\nvisual content in the form of sets of local features. Given an input image,\nfeature extraction algorithms identify a set of keypoints and assign to each of\nthem a description vector, based on the characteristics of the visual content\nsurrounding the interest point. Several tasks might require local features to\nbe extracted from a video sequence, on a frame-by-frame basis. Although\ntemporal downsampling has been proven to be an effective solution for mobile\naugmented reality and visual search, high temporal resolution is a key\nrequirement for time-critical applications such as object tracking, event\nrecognition, pedestrian detection, surveillance. In recent years, more and more\ncomputationally efficient visual feature detectors and decriptors have been\nproposed. Nonetheless, such approaches are tailored to still images. In this\npaper we propose a fast keypoint detection algorithm for video sequences, that\nexploits the temporal coherence of the sequence of keypoints. According to the\nproposed method, each frame is preprocessed so as to identify the parts of the\ninput frame for which keypoint detection and description need to be performed.\nOur experiments show that it is possible to achieve a reduction in\ncomputational time of up to 40%, without significantly affecting the task\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 09:28:28 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Baroffio", "Luca", ""], ["Cesana", "Matteo", ""], ["Redondi", "Alessandro", ""], ["Tagliasacchi", "Marco", ""]]}, {"id": "1503.07077", "submitter": "Sander Dieleman", "authors": "Sander Dieleman, Kyle W. Willett, Joni Dambre", "title": "Rotation-invariant convolutional neural networks for galaxy morphology\n  prediction", "comments": "Accepted for publication in MNRAS. 20 pages, 14 figures", "journal-ref": null, "doi": "10.1093/mnras/stv632", "report-no": null, "categories": "astro-ph.IM astro-ph.GA cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the morphological parameters of galaxies is a key requirement for\nstudying their formation and evolution. Surveys such as the Sloan Digital Sky\nSurvey (SDSS) have resulted in the availability of very large collections of\nimages, which have permitted population-wide analyses of galaxy morphology.\nMorphological analysis has traditionally been carried out mostly via visual\ninspection by trained experts, which is time-consuming and does not scale to\nlarge ($\\gtrsim10^4$) numbers of images.\n  Although attempts have been made to build automated classification systems,\nthese have not been able to achieve the desired level of accuracy. The Galaxy\nZoo project successfully applied a crowdsourcing strategy, inviting online\nusers to classify images by answering a series of questions. Unfortunately,\neven this approach does not scale well enough to keep up with the increasing\navailability of galaxy images.\n  We present a deep neural network model for galaxy morphology classification\nwhich exploits translational and rotational symmetry. It was developed in the\ncontext of the Galaxy Challenge, an international competition to build the best\nmodel for morphology classification based on annotated images from the Galaxy\nZoo project.\n  For images with high agreement among the Galaxy Zoo participants, our model\nis able to reproduce their consensus with near-perfect accuracy ($> 99\\%$) for\nmost questions. Confident model predictions are highly accurate, which makes\nthe model suitable for filtering large collections of images and forwarding\nchallenging images to experts for manual annotation. This approach greatly\nreduces the experts' workload without affecting accuracy. The application of\nthese algorithms to larger sets of training data will be critical for analysing\nresults from future surveys such as the LSST.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 15:34:06 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Dieleman", "Sander", ""], ["Willett", "Kyle W.", ""], ["Dambre", "Joni", ""]]}, {"id": "1503.07274", "submitter": "Elman Mansimov", "authors": "Elman Mansimov, Nitish Srivastava, Ruslan Salakhutdinov", "title": "Initialization Strategies of Spatio-Temporal Convolutional Neural\n  Networks", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new way of incorporating temporal information present in videos\ninto Spatial Convolutional Neural Networks (ConvNets) trained on images, that\navoids training Spatio-Temporal ConvNets from scratch. We describe several\ninitializations of weights in 3D Convolutional Layers of Spatio-Temporal\nConvNet using 2D Convolutional Weights learned from ImageNet. We show that it\nis important to initialize 3D Convolutional Weights judiciously in order to\nlearn temporal representations of videos. We evaluate our methods on the\nUCF-101 dataset and demonstrate improvement over Spatial ConvNets.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 03:41:47 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Mansimov", "Elman", ""], ["Srivastava", "Nitish", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1503.07297", "submitter": "Chandrajit Pal", "authors": "Chandrajit Pal, Amlan Chakrabarti, and Ranjan Ghosh", "title": "A Brief Survey of Recent Edge-Preserving Smoothing Algorithms on Digital\n  Images", "comments": "Manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge preserving filters preserve the edges and its information while blurring\nan image. In other words they are used to smooth an image, while reducing the\nedge blurring effects across the edge like halos, phantom etc. They are\nnonlinear in nature. Examples are bilateral filter, anisotropic diffusion\nfilter, guided filter, trilateral filter etc. Hence these family of filters are\nvery useful in reducing the noise in an image making it very demanding in\ncomputer vision and computational photography applications like denoising,\nvideo abstraction, demosaicing, optical-flow estimation, stereo matching, tone\nmapping, style transfer, relighting etc. This paper provides a concrete\nintroduction to edge preserving filters starting from the heat diffusion\nequation in olden to recent eras, an overview of its numerous applications, as\nwell as mathematical analysis, various efficient and optimized ways of\nimplementation and their interrelationships, keeping focus on preserving the\nboundaries, spikes and canyons in presence of noise. Furthermore it provides a\nrealistic notion for efficient implementation with a research scope for\nhardware realization for further acceleration.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 07:16:35 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Pal", "Chandrajit", ""], ["Chakrabarti", "Amlan", ""], ["Ghosh", "Ranjan", ""]]}, {"id": "1503.07384", "submitter": "Elma Hot", "authors": "Elma Hot, Petar Sekuli\\'c", "title": "Compressed sensing MRI using masked DCT and DFT measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents modification of the TwIST algorithm for Compressive\nSensing MRI images reconstruction. Compressive Sensing is new approach in\nsignal processing whose basic idea is recovering signal form small set of\navailable samples. The application of the Compressive Sensing in biomedical\nimaging has found great importance. It allows significant lowering of the\nacquisition time, and therefore, save the patient from the negative impact of\nthe MR apparatus. TwIST is commonly used algorithm for 2D signals\nreconstruction using Compressive Sensing principle. It is based on the Total\nVariation minimization. Standard version of the TwIST uses masked 2D Discrete\nFourier Transform coefficients as Compressive Sensing measurements. In this\npaper, different masks and different transformation domains for coefficients\nselection are tested. Certain percent of the measurements is used from the\nmask, as well as small number of coefficients outside the mask. Comparative\nanalysis using 2D DFT and 2D DCT coefficients, with different mask shapes is\nperformed. The theory is proved with experimental results.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 14:16:21 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Hot", "Elma", ""], ["Sekuli\u0107", "Petar", ""]]}, {"id": "1503.07460", "submitter": "Sheng hui Xu", "authors": "Shenghui Xu", "title": "RANSAC based three points algorithm for ellipse fitting of spherical\n  object's projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the spherical object can be seen everywhere, we should extract the ellipse\nimage accurately and fit it by implicit algebraic curve in order to finish the\n3D reconstruction. In this paper, we propose a new ellipse fitting algorithm\nwhich only needs three points to fit the projection of spherical object and is\ndifferent from the traditional algorithms that need at least five point. The\nfitting procedure is just similar as the estimation of Fundamental Matrix\nestimation by seven points, and the RANSAC algorithm has also been used to\nexclude the interference of noise and scattered points.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2015 15:05:12 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Xu", "Shenghui", ""]]}, {"id": "1503.07697", "submitter": "Corneliu Florea", "authors": "Laura Florea and Corneliu Florea and Constantin Vertan", "title": "Robust Eye Centers Localization with Zero--Crossing Encoded Image\n  Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new framework for the eye centers localization by the\njoint use of encoding of normalized image projections and a Multi Layer\nPerceptron (MLP) classifier. The encoding is novel and it consists in\nidentifying the zero-crossings and extracting the relevant parameters from the\nresulting modes. The compressed normalized projections produce feature\ndescriptors that are inputs to a properly-trained MLP, for discriminating among\nvarious categories of image regions. The proposed framework forms a fast and\nreliable system for the eye centers localization, especially in the context of\nface expression analysis in unconstrained environments. We successfully test\nthe proposed method on a wide variety of databases including BioID,\nCohn-Kanade, Extended Yale B and Labelled Faces in the Wild (LFW) databases.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 11:51:14 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Florea", "Laura", ""], ["Florea", "Corneliu", ""], ["Vertan", "Constantin", ""]]}, {"id": "1503.07706", "submitter": "Corneliu Florea", "authors": "Corneliu Florea, Laura Florea, Raluca Boia, Alessandra Bandrabur,\n  Constantin Vertan", "title": "Pain Intensity Estimation by a Self--Taught Selection of Histograms of\n  Topographical Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pain assessment through observational pain scales is necessary for special\ncategories of patients such as neonates, patients with dementia, critically ill\npatients, etc. The recently introduced Prkachin-Solomon score allows pain\nassessment directly from facial images opening the path for multiple assistive\napplications. In this paper, we introduce the Histograms of Topographical (HoT)\nfeatures, which are a generalization of the topographical primal sketch, for\nthe description of the face parts contributing to the mentioned score. We\npropose a semi-supervised, clustering oriented self--taught learning procedure\ndeveloped on the emotion oriented Cohn-Kanade database. We use this procedure\nto improve the discrimination between different pain intensity levels and the\ngeneralization with respect to the monitored persons, while testing on the UNBC\nMcMaster Shoulder Pain database.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 12:32:20 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Florea", "Corneliu", ""], ["Florea", "Laura", ""], ["Boia", "Raluca", ""], ["Bandrabur", "Alessandra", ""], ["Vertan", "Constantin", ""]]}, {"id": "1503.07783", "submitter": "Faraz Saeedan", "authors": "Faraz Saeedan, Barbara Caputo", "title": "Towards Learning free Naive Bayes Nearest Neighbor-based Domain\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As of today, object categorization algorithms are not able to achieve the\nlevel of robustness and generality necessary to work reliably in the real\nworld. Even the most powerful convolutional neural network we can train fails\nto perform satisfactorily when trained and tested on data from different\ndatabases. This issue, known as domain adaptation and/or dataset bias in the\nliterature, is due to a distribution mismatch between data collections. Methods\naddressing it go from max-margin classifiers to learning how to modify the\nfeatures and obtain a more robust representation. Recent work showed that by\ncasting the problem into the image-to-class recognition framework, the domain\nadaptation problem is significantly alleviated \\cite{danbnn}. Here we follow\nthis approach, and show how a very simple, learning free Naive Bayes Nearest\nNeighbor (NBNN)-based domain adaptation algorithm can significantly alleviate\nthe distribution mismatch among source and target data, especially when the\nnumber of classes and the number of sources grow. Experiments on standard\nbenchmarks used in the literature show that our approach (a) is competitive\nwith the current state of the art on small scale problems, and (b) achieves the\ncurrent state of the art as the number of classes and sources grows, with\nminimal computational requirements.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 16:55:19 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Saeedan", "Faraz", ""], ["Caputo", "Barbara", ""]]}, {"id": "1503.07790", "submitter": "Yongxin Yang", "authors": "Yanwei Fu, Yongxin Yang, Tim Hospedales, Tao Xiang and Shaogang Gong", "title": "Transductive Multi-label Zero-shot Learning", "comments": "12 pages, 6 figures, Accepted to BMVC 2014 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning has received increasing interest as a means to alleviate\nthe often prohibitive expense of annotating training data for large scale\nrecognition problems. These methods have achieved great success via learning\nintermediate semantic representations in the form of attributes and more\nrecently, semantic word vectors. However, they have thus far been constrained\nto the single-label case, in contrast to the growing popularity and importance\nof more realistic multi-label data. In this paper, for the first time, we\ninvestigate and formalise a general framework for multi-label zero-shot\nlearning, addressing the unique challenge therein: how to exploit multi-label\ncorrelation at test time with no training data for those classes? In\nparticular, we propose (1) a multi-output deep regression model to project an\nimage into a semantic word space, which explicitly exploits the correlations in\nthe intermediate semantic layer of word vectors; (2) a novel zero-shot learning\nalgorithm for multi-label data that exploits the unique compositionality\nproperty of semantic word vector representations; and (3) a transductive\nlearning strategy to enable the regression model learned from seen classes to\ngeneralise well to unseen classes. Our zero-shot learning experiments on a\nnumber of standard multi-label datasets demonstrate that our method outperforms\na variety of baselines.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 17:12:34 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Fu", "Yanwei", ""], ["Yang", "Yongxin", ""], ["Hospedales", "Tim", ""], ["Xiang", "Tao", ""], ["Gong", "Shaogang", ""]]}, {"id": "1503.07816", "submitter": "Abdelkhalak Bahri bahriinfo", "authors": "Bahri Abdelkhalak and Hamid Zouaki", "title": "Content-Based Bird Retrieval using Shape context, Color moments and Bag\n  of Features", "comments": "5 pages, 2 figures, IJCSI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new descriptor for birds search. First, our work\nwas carried on the choice of a descriptor. This choice is usually driven by the\napplication requirements such as robustness to noise, stability with respect to\nbias, the invariance to geometrical transformations or tolerance to occlusions.\nIn this context, we introduce a descriptor which combines the shape and color\ndescriptors to have an effectiveness description of birds. The proposed\ndescriptor is an adaptation of a descriptor based on the contours defined in\narticle Belongie et al. [5] combined with color moments [19]. Specifically,\npoints of interest are extracted from each image and information's in the\nregion in the vicinity of these points are represented by descriptors of shape\ncontext concatenated with color moments. Thus, the approach bag of visual words\nis applied to the latter. The experimental results show the effectiveness of\nour descriptor for the bird search by content.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2015 11:02:14 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Abdelkhalak", "Bahri", ""], ["Zouaki", "Hamid", ""]]}, {"id": "1503.07884", "submitter": "Yongxin Yang", "authors": "Yanwei Fu, Yongxin Yang, Timothy M. Hospedales, Tao Xiang and Shaogang\n  Gong", "title": "Transductive Multi-class and Multi-label Zero-shot Learning", "comments": "4 pages, 4 figures, ECCV 2014 Workshop on Parts and Attributes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, zero-shot learning (ZSL) has received increasing interest. The key\nidea underpinning existing ZSL approaches is to exploit knowledge transfer via\nan intermediate-level semantic representation which is assumed to be shared\nbetween the auxiliary and target datasets, and is used to bridge between these\ndomains for knowledge transfer. The semantic representation used in existing\napproaches varies from visual attributes to semantic word vectors and semantic\nrelatedness. However, the overall pipeline is similar: a projection mapping\nlow-level features to the semantic representation is learned from the auxiliary\ndataset by either classification or regression models and applied directly to\nmap each instance into the same semantic representation space where a zero-shot\nclassifier is used to recognise the unseen target class instances with a single\nknown 'prototype' of each target class. In this paper we discuss two related\nlines of work improving the conventional approach: exploiting transductive\nlearning ZSL, and generalising ZSL to the multi-label case.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 20:07:37 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Fu", "Yanwei", ""], ["Yang", "Yongxin", ""], ["Hospedales", "Timothy M.", ""], ["Xiang", "Tao", ""], ["Gong", "Shaogang", ""]]}, {"id": "1503.07989", "submitter": "Naveed Akhtar Mr.", "authors": "Naveed Akhtar, Faisal Shafait, Ajmal Mian", "title": "Discriminative Bayesian Dictionary Learning for Classification", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian approach to learn discriminative dictionaries for\nsparse representation of data. The proposed approach infers probability\ndistributions over the atoms of a discriminative dictionary using a Beta\nProcess. It also computes sets of Bernoulli distributions that associate class\nlabels to the learned dictionary atoms. This association signifies the\nselection probabilities of the dictionary atoms in the expansion of\nclass-specific data. Furthermore, the non-parametric character of the proposed\napproach allows it to infer the correct size of the dictionary. We exploit the\naforementioned Bernoulli distributions in separately learning a linear\nclassifier. The classifier uses the same hierarchical Bayesian model as the\ndictionary, which we present along the analytical inference solution for Gibbs\nsampling. For classification, a test instance is first sparsely encoded over\nthe learned dictionary and the codes are fed to the classifier. We performed\nexperiments for face and action recognition; and object and scene-category\nclassification using five public datasets and compared the results with\nstate-of-the-art discriminative sparse representation approaches. Experiments\nshow that the proposed Bayesian approach consistently outperforms the existing\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 08:36:15 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Akhtar", "Naveed", ""], ["Shafait", "Faisal", ""], ["Mian", "Ajmal", ""]]}, {"id": "1503.07998", "submitter": "Benjamin Schmid", "authors": "Benjamin Schmid and Jan Huisken", "title": "Real-time multi-view deconvolution", "comments": "8 pages, 5 figures, submitted to Bioinformatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In light-sheet microscopy, overall image content and resolution are improved\nby acquiring and fusing multiple views of the sample from different directions.\nState-of-the-art multi-view (MV) deconvolution employs the point spread\nfunctions (PSF) of the different views to simultaneously fuse and deconvolve\nthe images in 3D, but processing takes a multiple of the acquisition time and\nconstitutes the bottleneck in the imaging pipeline. Here we show that MV\ndeconvolution in 3D can finally be achieved in real-time by reslicing the\nacquired data and processing cross-sectional planes individually on the\nmassively parallel architecture of a graphics processing unit (GPU).\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 09:17:27 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Schmid", "Benjamin", ""], ["Huisken", "Jan", ""]]}, {"id": "1503.08223", "submitter": "David Arathorn", "authors": "David W. Arathorn", "title": "A System View of the Recognition and Interpretation of Observed Human\n  Shape, Pose and Action", "comments": "41 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is physiological evidence that our ability to interpret human pose and\naction from 2D visual imagery (binocular or monocular) engages the circuitry of\nthe motor cortices as well as the visual areas of the brain. This implies that\nthe capability of the motor cortices to solve inverse kinematics is flexible\nenough to apply to both motion planning as well as serving as a generative\nmodel for the visual processing of human figures, despite the differing\nfunctional requirements of the two tasks. This paper provides a computational\nmodel of the cooperation between visual and motor areas: in other words, a\nsystem view of an important class of brain computations. The model unifies the\nsolution of the separate inverse problems involved in the task, visual\ntransformation discovery, inverse kinematics, and adaptation to morphology\nvariations, using several instances of the Map-seeking Circuit algorithm. While\nthe paper is weighted toward the exposition of a neurobiological hypothesis,\nfrom mathematical formalization of the problem to neuronal circuitry, the\nalgorithmic expression of the solution is also a functional machine vision\nsystem for human figure recognition, and 3D pose and body morphology\nreconstruction from monocular, perspective-less input imagery. With an inverse\nkinematic generative model capable of imposing a variety of endogenous and\nexogenous constraints the machine vision implementation acquires\ncharacteristics currently unique among such systems.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 20:29:50 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Arathorn", "David W.", ""]]}, {"id": "1503.08248", "submitter": "Xirong Li", "authors": "Xirong Li and Tiberio Uricchio and Lamberto Ballan and Marco Bertini\n  and Cees G. M. Snoek and Alberto Del Bimbo", "title": "Socializing the Semantic Gap: A Comparative Survey on Image Tag\n  Assignment, Refinement and Retrieval", "comments": "to appear in ACM Computing Surveys", "journal-ref": "ACM Computing Surveys, Volume 49 Issue 1, 14:1-14:39, June 2016", "doi": "10.1145/2906152", "report-no": null, "categories": "cs.IR cs.CV cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Where previous reviews on content-based image retrieval emphasize on what can\nbe seen in an image to bridge the semantic gap, this survey considers what\npeople tag about an image. A comprehensive treatise of three closely linked\nproblems, i.e., image tag assignment, refinement, and tag-based image retrieval\nis presented. While existing works vary in terms of their targeted tasks and\nmethodology, they rely on the key functionality of tag relevance, i.e.\nestimating the relevance of a specific tag with respect to the visual content\nof a given image and its social context. By analyzing what information a\nspecific method exploits to construct its tag relevance function and how such\ninformation is exploited, this paper introduces a taxonomy to structure the\ngrowing literature, understand the ingredients of the main works, clarify their\nconnections and difference, and recognize their merits and limitations. For a\nhead-to-head comparison between the state-of-the-art, a new experimental\nprotocol is presented, with training sets containing 10k, 100k and 1m images\nand an evaluation on three test sets, contributed by various research groups.\nEleven representative works are implemented and evaluated. Putting all this\ntogether, the survey aims to provide an overview of the past and foster\nprogress for the near future.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 00:10:16 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2015 05:33:21 GMT"}, {"version": "v3", "created": "Wed, 23 Mar 2016 05:45:31 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Li", "Xirong", ""], ["Uricchio", "Tiberio", ""], ["Ballan", "Lamberto", ""], ["Bertini", "Marco", ""], ["Snoek", "Cees G. M.", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "1503.08263", "submitter": "Chunhua Shen", "authors": "Fayao Liu, Guosheng Lin, Chunhua Shen", "title": "CRF Learning with CNN Features for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional Random Rields (CRF) have been widely applied in image\nsegmentations. While most studies rely on hand-crafted features, we here\npropose to exploit a pre-trained large convolutional neural network (CNN) to\ngenerate deep features for CRF learning. The deep CNN is trained on the\nImageNet dataset and transferred to image segmentations here for constructing\npotentials of superpixels. Then the CRF parameters are learnt using a\nstructured support vector machine (SSVM). To fully exploit context information\nin inference, we construct spatially related co-occurrence pairwise potentials\nand incorporate them into the energy function. This prefers labelling of object\npairs that frequently co-occur in a certain spatial layout and at the same time\navoids implausible labellings during the inference. Extensive experiments on\nbinary and multi-class segmentation benchmarks demonstrate the promise of the\nproposed method. We thus provide new baselines for the segmentation performance\non the Weizmann horse, Graz-02, MSRC-21, Stanford Background and PASCAL VOC\n2011 datasets.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 04:05:09 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Liu", "Fayao", ""], ["Lin", "Guosheng", ""], ["Shen", "Chunhua", ""]]}, {"id": "1503.08596", "submitter": "Marco Cuturi", "authors": "Alexandre Gramfort, Gabriel Peyr\\'e, Marco Cuturi", "title": "Fast Optimal Transport Averaging of Neuroimaging Data", "comments": "Information Processing in Medical Imaging (IPMI), Jun 2015, Isle of\n  Skye, United Kingdom. Springer, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowing how the Human brain is anatomically and functionally organized at the\nlevel of a group of healthy individuals or patients is the primary goal of\nneuroimaging research. Yet computing an average of brain imaging data defined\nover a voxel grid or a triangulation remains a challenge. Data are large, the\ngeometry of the brain is complex and the between subjects variability leads to\nspatially or temporally non-overlapping effects of interest. To address the\nproblem of variability, data are commonly smoothed before group linear\naveraging. In this work we build on ideas originally introduced by Kantorovich\nto propose a new algorithm that can average efficiently non-normalized data\ndefined over arbitrary discrete domains using transportation metrics. We show\nhow Kantorovich means can be linked to Wasserstein barycenters in order to take\nadvantage of an entropic smoothing approach. It leads to a smooth convex\noptimization problem and an algorithm with strong convergence guarantees. We\nillustrate the versatility of this tool and its empirical behavior on\nfunctional neuroimaging data, functional MRI and magnetoencephalography (MEG)\nsource estimates, defined on voxel grids and triangulations of the folded\ncortical surface.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 08:48:08 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2015 01:47:02 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Gramfort", "Alexandre", ""], ["Peyr\u00e9", "Gabriel", ""], ["Cuturi", "Marco", ""]]}, {"id": "1503.08663", "submitter": "Guanbin Li", "authors": "Guanbin Li and Yizhou Yu", "title": "Visual Saliency Based on Multiscale Deep Features", "comments": "To appear in CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual saliency is a fundamental problem in both cognitive and computational\nsciences, including computer vision. In this CVPR 2015 paper, we discover that\na high-quality visual saliency model can be trained with multiscale features\nextracted using a popular deep learning architecture, convolutional neural\nnetworks (CNNs), which have had many successes in visual recognition tasks. For\nlearning such saliency models, we introduce a neural network architecture,\nwhich has fully connected layers on top of CNNs responsible for extracting\nfeatures at three different scales. We then propose a refinement method to\nenhance the spatial coherence of our saliency results. Finally, aggregating\nmultiple saliency maps computed for different levels of image segmentation can\nfurther boost the performance, yielding saliency maps better than those\ngenerated from a single segmentation. To promote further research and\nevaluation of visual saliency models, we also construct a new large database of\n4447 challenging images and their pixelwise saliency annotation. Experimental\nresults demonstrate that our proposed method is capable of achieving\nstate-of-the-art performance on all public benchmarks, improving the F-Measure\nby 5.0% and 13.2% respectively on the MSRA-B dataset and our new dataset\n(HKU-IS), and lowering the mean absolute error by 5.7% and 35.1% respectively\non these two datasets.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 13:21:09 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2015 05:21:02 GMT"}, {"version": "v3", "created": "Fri, 10 Apr 2015 06:40:46 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Li", "Guanbin", ""], ["Yu", "Yizhou", ""]]}, {"id": "1503.08677", "submitter": "Zeynep Akata PhD", "authors": "Zeynep Akata, Florent Perronnin, Zaid Harchaoui, Cordelia Schmid", "title": "Label-Embedding for Image Classification", "comments": "IEEE TPAMI preprint", "journal-ref": null, "doi": "10.1109/TPAMI.2015.2487986", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attributes act as intermediate representations that enable parameter sharing\nbetween classes, a must when training data is scarce. We propose to view\nattribute-based image classification as a label-embedding problem: each class\nis embedded in the space of attribute vectors. We introduce a function that\nmeasures the compatibility between an image and a label embedding. The\nparameters of this function are learned on a training set of labeled samples to\nensure that, given an image, the correct classes rank higher than the incorrect\nones. Results on the Animals With Attributes and Caltech-UCSD-Birds datasets\nshow that the proposed framework outperforms the standard Direct Attribute\nPrediction baseline in a zero-shot learning scenario. Label embedding enjoys a\nbuilt-in ability to leverage alternative sources of information instead of or\nin addition to attributes, such as e.g. class hierarchies or textual\ndescriptions. Moreover, label embedding encompasses the whole range of learning\nsettings from zero-shot learning to regular learning with a large number of\nlabeled examples.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 14:04:34 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2015 10:48:38 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Akata", "Zeynep", ""], ["Perronnin", "Florent", ""], ["Harchaoui", "Zaid", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1503.08843", "submitter": "Peng Sun", "authors": "Peng Sun, James K. Min, Guanglei Xiong", "title": "Globally Tuned Cascade Pose Regression via Back Propagation with\n  Application in 2D Face Pose Estimation and Heart Segmentation in 3D CT Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a successful pose estimation algorithm, called Cascade Pose\nRegression (CPR), was proposed in the literature. Trained over Pose Index\nFeature, CPR is a regressor ensemble that is similar to Boosting. In this paper\nwe show how CPR can be represented as a Neural Network. Specifically, we adopt\na Graph Transformer Network (GTN) representation and accordingly train CPR with\nBack Propagation (BP) that permits globally tuning. In contrast, previous CPR\nliterature only took a layer wise training without any post fine tuning. We\nempirically show that global training with BP outperforms layer-wise\n(pre-)training. Our CPR-GTN adopts a Multi Layer Percetron as the regressor,\nwhich utilized sparse connection to learn local image feature representation.\nWe tested the proposed CPR-GTN on 2D face pose estimation problem as in\nprevious CPR literature. Besides, we also investigated the possibility of\nextending CPR-GTN to 3D pose estimation by doing experiments using 3D Computed\nTomography dataset for heart segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 20:17:23 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Sun", "Peng", ""], ["Min", "James K.", ""], ["Xiong", "Guanglei", ""]]}, {"id": "1503.08853", "submitter": "Ali Borji", "authors": "Ali Borji and James Tanner", "title": "Reconciling saliency and object center-bias hypotheses in explaining\n  free-viewing fixations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting where people look in natural scenes has attracted a lot of\ninterest in computer vision and computational neuroscience over the past two\ndecades. Two seemingly contrasting categories of cues have been proposed to\ninfluence where people look: \\textit{low-level image saliency} and\n\\textit{high-level semantic information}. Our first contribution is to take a\ndetailed look at these cues to confirm the hypothesis proposed by\nHenderson~\\cite{henderson1993eye} and Nuthmann \\&\nHenderson~\\cite{nuthmann2010object} that observers tend to look at the center\nof objects. We analyzed fixation data for scene free-viewing over 17 observers\non 60 fully annotated images with various types of objects. Images contained\ndifferent types of scenes, such as natural scenes, line drawings, and 3D\nrendered scenes. Our second contribution is to propose a simple combined model\nof low-level saliency and object center-bias that outperforms each individual\ncomponent significantly over our data, as well as on the OSIE dataset by Xu et\nal.~\\cite{xu2014predicting}. The results reconcile saliency with object\ncenter-bias hypotheses and highlight that both types of cues are important in\nguiding fixations. Our work opens new directions to understand strategies that\nhumans use in observing scenes and objects, and demonstrates the construction\nof combined models of low-level saliency and high-level object-based\ninformation.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 21:07:53 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Borji", "Ali", ""], ["Tanner", "James", ""]]}, {"id": "1503.08909", "submitter": "George Toderici", "authors": "Joe Yue-Hei Ng and Matthew Hausknecht and Sudheendra Vijayanarasimhan\n  and Oriol Vinyals and Rajat Monga and George Toderici", "title": "Beyond Short Snippets: Deep Networks for Video Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have been extensively applied for image\nrecognition problems giving state-of-the-art results on recognition, detection,\nsegmentation and retrieval. In this work we propose and evaluate several deep\nneural network architectures to combine image information across a video over\nlonger time periods than previously attempted. We propose two methods capable\nof handling full length videos. The first method explores various convolutional\ntemporal feature pooling architectures, examining the various design choices\nwhich need to be made when adapting a CNN for this task. The second proposed\nmethod explicitly models the video as an ordered sequence of frames. For this\npurpose we employ a recurrent neural network that uses Long Short-Term Memory\n(LSTM) cells which are connected to the output of the underlying CNN. Our best\nnetworks exhibit significant performance improvements over previously published\nresults on the Sports 1 million dataset (73.1% vs. 60.9%) and the UCF-101\ndatasets with (88.6% vs. 88.0%) and without additional optical flow information\n(82.6% vs. 72.8%).\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 04:34:12 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2015 19:44:25 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Ng", "Joe Yue-Hei", ""], ["Hausknecht", "Matthew", ""], ["Vijayanarasimhan", "Sudheendra", ""], ["Vinyals", "Oriol", ""], ["Monga", "Rajat", ""], ["Toderici", "George", ""]]}]