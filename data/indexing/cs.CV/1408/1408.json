[{"id": "1408.0173", "submitter": "Michael Moeller", "authors": "Michael Moeller, Martin Benning, Carola Sch\\\"onlieb, Daniel Cremers", "title": "Variational Depth from Focus Reconstruction", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2015.2479469", "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the problem of reconstructing a depth map from a\nsequence of differently focused images, also known as depth from focus or shape\nfrom focus. We propose to state the depth from focus problem as a variational\nproblem including a smooth but nonconvex data fidelity term, and a convex\nnonsmooth regularization, which makes the method robust to noise and leads to\nmore realistic depth maps. Additionally, we propose to solve the nonconvex\nminimization problem with a linearized alternating directions method of\nmultipliers (ADMM), allowing to minimize the energy very efficiently. A\nnumerical comparison to classical methods on simulated as well as on real data\nis presented.\n", "versions": [{"version": "v1", "created": "Fri, 1 Aug 2014 13:26:41 GMT"}, {"version": "v2", "created": "Wed, 5 Nov 2014 11:03:55 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Moeller", "Michael", ""], ["Benning", "Martin", ""], ["Sch\u00f6nlieb", "Carola", ""], ["Cremers", "Daniel", ""]]}, {"id": "1408.0204", "submitter": "Momiao Xiong", "authors": "Nan Lin, Junhai Jiang, Shicheng Guo and Momiao Xiong", "title": "Functional Principal Component Analysis and Randomized Sparse Clustering\n  Algorithm for Medical Image Analysis", "comments": "35 pages, 2 figures, 6 tables", "journal-ref": null, "doi": "10.1371/journal.pone.0132945", "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to advances in sensors, growing large and complex medical image data have\nthe ability to visualize the pathological change in the cellular or even the\nmolecular level or anatomical changes in tissues and organs. As a consequence,\nthe medical images have the potential to enhance diagnosis of disease,\nprediction of clinical outcomes, characterization of disease progression,\nmanagement of health care and development of treatments, but also pose great\nmethodological and computational challenges for representation and selection of\nfeatures in image cluster analysis. To address these challenges, we first\nextend one dimensional functional principal component analysis to the two\ndimensional functional principle component analyses (2DFPCA) to fully capture\nspace variation of image signals. Image signals contain a large number of\nredundant and irrelevant features which provide no additional or no useful\ninformation for cluster analysis. Widely used methods for removing redundant\nand irrelevant features are sparse clustering algorithms using a lasso-type\npenalty to select the features. However, the accuracy of clustering using a\nlasso-type penalty depends on how to select penalty parameters and a threshold\nfor selecting features. In practice, they are difficult to determine. Recently,\nrandomized algorithms have received a great deal of attention in big data\nanalysis. This paper presents a randomized algorithm for accurate feature\nselection in image cluster analysis. The proposed method is applied to ovarian\nand kidney cancer histology image data from the TCGA database. The results\ndemonstrate that the randomized feature selection method coupled with\nfunctional principal component analysis substantially outperforms the current\nsparse clustering algorithms in image cluster analysis.\n", "versions": [{"version": "v1", "created": "Fri, 1 Aug 2014 15:15:48 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Lin", "Nan", ""], ["Jiang", "Junhai", ""], ["Guo", "Shicheng", ""], ["Xiong", "Momiao", ""]]}, {"id": "1408.0452", "submitter": "T.R. Gopalakrishnan Nair", "authors": "T.R. Gopalakrishnan Nair, A.P. Geetha, Asharani", "title": "Methodology For Detection of QRS Pattern Using Secondary Wavelets", "comments": "3 pages, 8 figures,Systemics, Cybernetics and Informatics\n  (ICSCI)2012, International Conference on pp.327,330, Dr.MCR HRD Institute,\n  Hyderabad, India, 15-18Feb 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications of wavelet transform to the field of health care signals have\npaved the way for implementing revolutionary approaches in detecting the\npresence of certain abnormalities in human health patterns. There were\nextensive studies carried out using primary wavelets in various signals like\nElectrocardiogram (ECG), sonogram etc. with a certain amount of success. On the\nother hand analysis using secondary wavelets which inherits the characteristics\nof a set of variations available in signals like ECG can be a promise to detect\ndiseases with ease. Here a method to create a generalized adapted wavelet is\npresented which contains the information of QRS pattern collected from an\nanomaly sample space. The method has been tested and found to be successful in\nlocating the position of R peak in noise embedded ECG signal.\n", "versions": [{"version": "v1", "created": "Sun, 3 Aug 2014 03:43:28 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Nair", "T. R. Gopalakrishnan", ""], ["Geetha", "A. P.", ""], ["Asharani", "", ""]]}, {"id": "1408.0453", "submitter": "T.R. Gopalakrishnan Nair", "authors": "T.R. Gopalakrishnan Nair, A.P. Geetha, M. Asharani", "title": "Adaptive Wavelet Based Identification and Extraction of PQRST\n  Combination in Randomly Stretching ECG Sequence", "comments": "Signal and Information Processing (SIP), 2013 IEEE China Summit &\n  International Conference on, pp.278,282,Beijing, China, 6-10 July. 2013", "journal-ref": null, "doi": "10.1109/ChinaSIP.2013.6625344", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiovascular system study using ECG signals have evolved tremendously in\nthe domain of electronics and signal processing. However, there are certain\nfloating challenges unresolved in the analysis and detection of abnormal\nperformances of cardiovascular system. As the medical field is moving towards\nmore automated and intelligent systems, wrong detection or wrong\ninterpretations of ECG waveform of abnormal conditions can be quite fatal.\nSince the PQRST signals vary their positions randomly, the process of locating,\nidentifying and classifying each feature can be cumbersome and it is prone to\nerrors. Here we present an automated scheme using adaptive wavelet to detect\nprominent R-peak with extreme accuracy and algorithmically tag and mark the\ncoexisting peaks P, Q, S, and T with almost same accuracy. The adaptive wavelet\napproach used in this scheme is capable of detecting R-peak in ECG with 99.99%\naccuracy along with the rest of the waveforms.\n", "versions": [{"version": "v1", "created": "Sun, 3 Aug 2014 03:49:36 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Nair", "T. R. Gopalakrishnan", ""], ["Geetha", "A. P.", ""], ["Asharani", "M.", ""]]}, {"id": "1408.0680", "submitter": "Rafael Berri A", "authors": "Rafael A. Berri, Alexandre G. Silva, Rafael S. Parpinelli, Elaine\n  Girardi, Rangel Arthur", "title": "A Pattern Recognition System for Detecting Use of Mobile Phones While\n  Driving", "comments": "8 pages, 9th International Conference on Computer Vision Theory and\n  Applications", "journal-ref": null, "doi": "10.5220/0004684504110418", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is estimated that 80% of crashes and 65% of near collisions involved\ndrivers inattentive to traffic for three seconds before the event. This paper\ndevelops an algorithm for extracting characteristics allowing the cell phones\nidentification used during driving a vehicle. Experiments were performed on\nsets of images with 100 positive images (with phone) and the other 100 negative\nimages (no phone), containing frontal images of the driver. Support Vector\nMachine (SVM) with Polynomial kernel is the most advantageous classification\nsystem to the features provided by the algorithm, obtaining a success rate of\n91.57% for the vision system. Tests done on videos show that it is possible to\nuse the image datasets for training classifiers in real situations. Periods of\n3 seconds were correctly classified at 87.43% of cases.\n", "versions": [{"version": "v1", "created": "Mon, 4 Aug 2014 13:35:24 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Berri", "Rafael A.", ""], ["Silva", "Alexandre G.", ""], ["Parpinelli", "Rafael S.", ""], ["Girardi", "Elaine", ""], ["Arthur", "Rangel", ""]]}, {"id": "1408.0765", "submitter": "Yu Liu", "authors": "Yu Liu, Osvaldo Simeone, Alexander M. Haimovich and Wei Su", "title": "Modulation Classification via Gibbs Sampling Based on a Latent Dirichlet\n  Bayesian Network", "comments": "Contains corrections with respect to the version to appear on IEEE\n  Signal Processing Letters (see Fig. 2)", "journal-ref": null, "doi": "10.1109/LSP.2014.2327193", "report-no": null, "categories": "cs.IT cs.CV math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel Bayesian modulation classification scheme is proposed for a\nsingle-antenna system over frequency-selective fading channels. The method is\nbased on Gibbs sampling as applied to a latent Dirichlet Bayesian network (BN).\nThe use of the proposed latent Dirichlet BN provides a systematic solution to\nthe convergence problem encountered by the conventional Gibbs sampling approach\nfor modulation classification. The method generalizes, and is shown to improve\nupon, the state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 4 Aug 2014 18:47:24 GMT"}, {"version": "v2", "created": "Wed, 20 Aug 2014 02:04:25 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Liu", "Yu", ""], ["Simeone", "Osvaldo", ""], ["Haimovich", "Alexander M.", ""], ["Su", "Wei", ""]]}, {"id": "1408.0814", "submitter": "Emre Akbas", "authors": "Emre Akbas, Miguel P. Eckstein", "title": "Object Detection Through Exploration With A Foveated Visual Field", "comments": "An extended version of this manuscript was published in PLOS\n  Computational Biology (October 2017) at\n  https://doi.org/10.1371/journal.pcbi.1005743", "journal-ref": "PLOS Computational Biology, 2017, 13(10), e1005743", "doi": "10.1371/journal.pcbi.1005743", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a foveated object detector (FOD) as a biologically-inspired\nalternative to the sliding window (SW) approach which is the dominant method of\nsearch in computer vision object detection. Similar to the human visual system,\nthe FOD has higher resolution at the fovea and lower resolution at the visual\nperiphery. Consequently, more computational resources are allocated at the\nfovea and relatively fewer at the periphery. The FOD processes the entire\nscene, uses retino-specific object detection classifiers to guide eye\nmovements, aligns its fovea with regions of interest in the input image and\nintegrates observations across multiple fixations. Our approach combines modern\nobject detectors from computer vision with a recent model of peripheral pooling\nregions found at the V1 layer of the human visual system. We assessed various\neye movement strategies on the PASCAL VOC 2007 dataset and show that the FOD\nperforms on par with the SW detector while bringing significant computational\ncost savings.\n", "versions": [{"version": "v1", "created": "Mon, 4 Aug 2014 20:49:26 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 07:17:44 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Akbas", "Emre", ""], ["Eckstein", "Miguel P.", ""]]}, {"id": "1408.0872", "submitter": "Shengcai Liao", "authors": "Shengcai Liao, Zhipeng Mo, Jianqing Zhu, Yang Hu, and Stan Z. Li", "title": "Open-set Person Re-identification", "comments": "The OPeRID v1.0 dataset and evaluation toolkit is now available to\n  download at http://www.cbsr.ia.ac.cn/users/scliao/projects/operidv1/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is becoming a hot research for developing both\nmachine learning algorithms and video surveillance applications. The task of\nperson re-identification is to determine which person in a gallery has the same\nidentity to a probe image. This task basically assumes that the subject of the\nprobe image belongs to the gallery, that is, the gallery contains this person.\nHowever, in practical applications such as searching a suspect in a video, this\nassumption is usually not true. In this paper, we consider the open-set person\nre-identification problem, which includes two sub-tasks, detection and\nidentification. The detection sub-task is to determine the presence of the\nprobe subject in the gallery, and the identification sub-task is to determine\nwhich person in the gallery has the same identity as the accepted probe. We\npresent a database collected from a video surveillance setting of 6 cameras,\nwith 200 persons and 7,413 images segmented. Based on this database, we develop\na benchmark protocol for evaluating the performance under the open-set person\nre-identification scenario. Several popular metric learning algorithms for\nperson re-identification have been evaluated as baselines. From the baseline\nperformance, we observe that the open-set person re-identification problem is\nstill largely unresolved, thus further attention and effort is needed.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 05:55:16 GMT"}, {"version": "v2", "created": "Wed, 15 Oct 2014 14:28:08 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Liao", "Shengcai", ""], ["Mo", "Zhipeng", ""], ["Zhu", "Jianqing", ""], ["Hu", "Yang", ""], ["Li", "Stan Z.", ""]]}, {"id": "1408.0889", "submitter": "Vahid Moosavi", "authors": "Vahid Moosavi", "title": "Computing With Contextual Numbers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self Organizing Map (SOM) has been applied into several classical modeling\ntasks including clustering, classification, function approximation and\nvisualization of high dimensional spaces. The final products of a trained SOM\nare a set of ordered (low dimensional) indices and their associated high\ndimensional weight vectors. While in the above-mentioned applications, the\nfinal high dimensional weight vectors play the primary role in the\ncomputational steps, from a certain perspective, one can interpret SOM as a\nnonparametric encoder, in which the final low dimensional indices of the\ntrained SOM are pointer to the high dimensional space. We showed how using a\none-dimensional SOM, which is not common in usual applications of SOM, one can\ndevelop a nonparametric mapping from a high dimensional space to a continuous\none-dimensional numerical field. These numerical values, called contextual\nnumbers, are ordered in a way that in a given context, similar numbers refer to\nsimilar high dimensional states. Further, as these numbers can be treated\nsimilarly to usual continuous numbers, they can be replaced with their\ncorresponding high dimensional states within any data driven modeling problem.\nAs a potential application, we showed how using contextual numbers could be\nused for the problem of high dimensional spatiotemporal dynamics.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 08:37:11 GMT"}, {"version": "v2", "created": "Wed, 6 Aug 2014 04:22:26 GMT"}], "update_date": "2014-08-07", "authors_parsed": [["Moosavi", "Vahid", ""]]}, {"id": "1408.0967", "submitter": "Shaina Race", "authors": "Shaina Race, Carl Meyer, Kevin Valakuzhy", "title": "Determining the Number of Clusters via Iterative Consensus Clustering", "comments": "Proceedings of the 2013 SIAM International Conference on Data Mining", "journal-ref": null, "doi": "10.1137/1.9781611972832.11", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use a cluster ensemble to determine the number of clusters, k, in a group\nof data. A consensus similarity matrix is formed from the ensemble using\nmultiple algorithms and several values for k. A random walk is induced on the\ngraph defined by the consensus matrix and the eigenvalues of the associated\ntransition probability matrix are used to determine the number of clusters. For\nnoisy or high-dimensional data, an iterative technique is presented to refine\nthis consensus matrix in way that encourages a block-diagonal form. It is shown\nthat the resulting consensus matrix is generally superior to existing\nsimilarity matrices for this type of spectral analysis.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 13:40:03 GMT"}], "update_date": "2014-08-06", "authors_parsed": [["Race", "Shaina", ""], ["Meyer", "Carl", ""], ["Valakuzhy", "Kevin", ""]]}, {"id": "1408.0972", "submitter": "Shaina Race Ph.D", "authors": "Shaina Race and Carl Meyer", "title": "A Flexible Iterative Framework for Consensus Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel framework for consensus clustering is presented which has the ability\nto determine both the number of clusters and a final solution using multiple\nalgorithms. A consensus similarity matrix is formed from an ensemble using\nmultiple algorithms and several values for k. A variety of dimension reduction\ntechniques and clustering algorithms are considered for analysis. For noisy or\nhigh-dimensional data, an iterative technique is presented to refine this\nconsensus matrix in way that encourages algorithms to agree upon a common\nsolution. We utilize the theory of nearly uncoupled Markov chains to determine\nthe number, k , of clusters in a dataset by considering a random walk on the\ngraph defined by the consensus matrix. The eigenvalues of the associated\ntransition probability matrix are used to determine the number of clusters.\nThis method succeeds at determining the number of clusters in many datasets\nwhere previous methods fail. On every considered dataset, our consensus method\nprovides a final result with accuracy well above the average of the individual\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 13:54:01 GMT"}], "update_date": "2014-08-06", "authors_parsed": [["Race", "Shaina", ""], ["Meyer", "Carl", ""]]}, {"id": "1408.1135", "submitter": "Ali Avanaki", "authors": "Ali R. N. Avanaki, Kathryn S. Espig, Albert Xthona, Tom R. L. Kimpe,\n  Predrag R. Bakic, Andrew D. A. Maidment", "title": "It is hard to see a needle in a haystack: Modeling contrast masking\n  effect in a numerical observer", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-07887-8_100", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the framework of a virtual clinical trial for breast imaging, we aim\nto develop numerical observers that follow the same detection performance\ntrends as those of a typical human observer. In our prior work, we showed that\nby including spatiotemporal contrast sensitivity function (stCSF) of human\nvisual system (HVS) in a multi-slice channelized Hotelling observer (msCHO), we\ncan correctly predict trends of a typical human observer performance with the\nviewing parameters of browsing speed, viewing distance and contrast. In this\nwork we further improve our numerical observer by modeling contrast masking.\nAfter stCSF, contrast masking is the second most prominent property of HVS and\nit refers to the fact that the presence of one signal affects the visibility\nthreshold for another signal. Our results indicate that the improved numerical\nobserver better predicts changes in detection performance with background\ncomplexity.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 22:23:40 GMT"}], "update_date": "2014-08-07", "authors_parsed": [["Avanaki", "Ali R. N.", ""], ["Espig", "Kathryn S.", ""], ["Xthona", "Albert", ""], ["Kimpe", "Tom R. L.", ""], ["Bakic", "Predrag R.", ""], ["Maidment", "Andrew D. A.", ""]]}, {"id": "1408.1167", "submitter": "Truyen Tran", "authors": "Truyen Tran, Hung Bui, Svetha Venkatesh", "title": "Boosted Markov Networks for Activity Recognition", "comments": "International Conference on Intelligent Sensors, Sensor Networks and\n  Information Processing (ISSNIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore a framework called boosted Markov networks to combine the learning\ncapacity of boosting and the rich modeling semantics of Markov networks and\napplying the framework for video-based activity recognition. Importantly, we\nextend the framework to incorporate hidden variables. We show how the framework\ncan be applied for both model learning and feature selection. We demonstrate\nthat boosted Markov networks with hidden variables perform comparably with the\nstandard maximum likelihood estimation. However, our framework is able to learn\nsparse models, and therefore can provide computational savings when the learned\nmodels are used for classification.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 02:45:51 GMT"}], "update_date": "2014-08-07", "authors_parsed": [["Tran", "Truyen", ""], ["Bui", "Hung", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1408.1292", "submitter": "Ilja Kuzborskij", "authors": "Ilja Kuzborskij, Francesco Orabona, Barbara Caputo", "title": "Scalable Greedy Algorithms for Transfer Learning", "comments": null, "journal-ref": null, "doi": "10.1016/j.cviu.2016.09.003", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the binary transfer learning problem, focusing on\nhow to select and combine sources from a large pool to yield a good performance\non a target task. Constraining our scenario to real world, we do not assume the\ndirect access to the source data, but rather we employ the source hypotheses\ntrained from them. We propose an efficient algorithm that selects relevant\nsource hypotheses and feature dimensions simultaneously, building on the\nliterature on the best subset selection problem. Our algorithm achieves\nstate-of-the-art results on three computer vision datasets, substantially\noutperforming both transfer learning and popular feature selection baselines in\na small-sample setting. We also present a randomized variant that achieves the\nsame results with the computational cost independent from the number of source\nhypotheses and feature dimensions. Also, we theoretically prove that, under\nreasonable assumptions on the source hypotheses, our algorithm can learn\neffectively from few examples.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 14:27:57 GMT"}, {"version": "v2", "created": "Thu, 4 Dec 2014 15:56:53 GMT"}, {"version": "v3", "created": "Thu, 8 Oct 2015 10:27:39 GMT"}, {"version": "v4", "created": "Sat, 18 Jun 2016 00:17:50 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Kuzborskij", "Ilja", ""], ["Orabona", "Francesco", ""], ["Caputo", "Barbara", ""]]}, {"id": "1408.1549", "submitter": "Reza Azad", "authors": "Reza Azad, Babak Azad, Nabil Belhaj Khalifa, Shahram Jamali", "title": "Real-Time Human-Computer Interaction Based on Face and Hand Gesture\n  Recognition", "comments": null, "journal-ref": "International Journal in Foundations of Computer Science &\n  Technology 07/2014; 4(4):37-48", "doi": "10.5121/ijfcst.2014.4403", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the present time, hand gestures recognition system could be used as a more\nexpected and useable approach for human computer interaction. Automatic hand\ngesture recognition system provides us a new tactic for interactive with the\nvirtual environment. In this paper, a face and hand gesture recognition system\nwhich is able to control computer media player is offered. Hand gesture and\nhuman face are the key element to interact with the smart system. We used the\nface recognition scheme for viewer verification and the hand gesture\nrecognition in mechanism of computer media player, for instance, volume\ndown/up, next music and etc. In the proposed technique, first, the hand gesture\nand face location is extracted from the main image by combination of skin and\ncascade detector and then is sent to recognition stage. In recognition stage,\nfirst, the threshold condition is inspected then the extracted face and gesture\nwill be recognized. In the result stage, the proposed technique is applied on\nthe video dataset and the high precision ratio acquired. Additional the\nrecommended hand gesture recognition method is applied on static American Sign\nLanguage (ASL) database and the correctness rate achieved nearby 99.40%. also\nthe planned method could be used in gesture based computer games and virtual\nreality.\n", "versions": [{"version": "v1", "created": "Thu, 7 Aug 2014 11:38:20 GMT"}], "update_date": "2014-08-08", "authors_parsed": [["Azad", "Reza", ""], ["Azad", "Babak", ""], ["Khalifa", "Nabil Belhaj", ""], ["Jamali", "Shahram", ""]]}, {"id": "1408.1656", "submitter": "Shengcai Liao", "authors": "Shengcai Liao, Anil K. Jain, and Stan Z. Li", "title": "A Fast and Accurate Unconstrained Face Detector", "comments": "This paper has been accepted by TPAMI. The source code is available\n  on the project page\n  http://www.cbsr.ia.ac.cn/users/scliao/projects/npdface/index.html", "journal-ref": null, "doi": "10.1109/TPAMI.2015.2448075", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to address challenges in unconstrained face detection,\nsuch as arbitrary pose variations and occlusions. First, a new image feature\ncalled Normalized Pixel Difference (NPD) is proposed. NPD feature is computed\nas the difference to sum ratio between two pixel values, inspired by the Weber\nFraction in experimental psychology. The new feature is scale invariant,\nbounded, and is able to reconstruct the original image. Second, we propose a\ndeep quadratic tree to learn the optimal subset of NPD features and their\ncombinations, so that complex face manifolds can be partitioned by the learned\nrules. This way, only a single soft-cascade classifier is needed to handle\nunconstrained face detection. Furthermore, we show that the NPD features can be\nefficiently obtained from a look up table, and the detection template can be\neasily scaled, making the proposed face detector very fast. Experimental\nresults on three public face datasets (FDDB, GENKI, and CMU-MIT) show that the\nproposed method achieves state-of-the-art performance in detecting\nunconstrained faces with arbitrary pose variations and occlusions in cluttered\nscenes.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 15:17:33 GMT"}, {"version": "v2", "created": "Tue, 12 Aug 2014 14:24:52 GMT"}, {"version": "v3", "created": "Mon, 7 Sep 2015 08:17:34 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Liao", "Shengcai", ""], ["Jain", "Anil K.", ""], ["Li", "Stan Z.", ""]]}, {"id": "1408.1688", "submitter": "Chao Yang Mr.", "authors": "Chao Yang, Shengnan Caih, Jingdong Wang and Long Quan", "title": "Low-rank SIFT: An Affine Invariant Feature for Place Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper, we present a novel affine-invariant feature based on SIFT,\nleveraging the regular appearance of man-made objects. The feature achieves\nfull affine invariance without needing to simulate over affine parameter space.\nLow-rank SIFT, as we name the feature, is based on our observation that local\ntilt, which are caused by changes of camera axis orientation, could be\nnormalized by converting local patches to standard low-rank forms. Rotation,\ntranslation and scaling invariance could be achieved in ways similar to SIFT.\nAs an extension of SIFT, our method seeks to add prior to solve the ill-posed\naffine parameter estimation problem and normalizes them directly, and is\napplicable to objects with regular structures. Furthermore, owing to recent\nbreakthrough in convex optimization, such parameter could be computed\nefficiently. We will demonstrate its effectiveness in place recognition as our\nmajor application. As extra contributions, we also describe our pipeline of\nconstructing geotagged building database from the ground up, as well as an\nefficient scheme for automatic feature selection.\n", "versions": [{"version": "v1", "created": "Thu, 7 Aug 2014 19:49:13 GMT"}], "update_date": "2014-08-08", "authors_parsed": [["Yang", "Chao", ""], ["Caih", "Shengnan", ""], ["Wang", "Jingdong", ""], ["Quan", "Long", ""]]}, {"id": "1408.1759", "submitter": "Reza Azad", "authors": "Reza Azad, Babak Azad, Iman Tavakoli Kazerooni", "title": "Real-Time and Robust Method for Hand Gesture Recognition System Based on\n  Cross-Correlation Coefficient", "comments": "arXiv admin note: substantial text overlap with\n  http://dx.doi.org/10.1109/ICCCA.2012.6179213 by other author", "journal-ref": "Advances in Computer Science: an International Journal, Vol. 2,\n  Issue 5, No.6, pp. 121-125, 2013", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand gesture recognition possesses extensive applications in virtual reality,\nsign language recognition, and computer games. The direct interface of hand\ngestures provides us a new way for communicating with the virtual environment.\nIn this paper a novel and real-time approach for hand gesture recognition\nsystem is presented. In the suggested method, first, the hand gesture is\nextracted from the main image by the image segmentation and morphological\noperation and then is sent to feature extraction stage. In feature extraction\nstage the Cross-correlation coefficient is applied on the gesture to recognize\nit. In the result part, the proposed approach is applied on American Sign\nLanguage (ASL) database and the accuracy rate obtained 98.34%.\n", "versions": [{"version": "v1", "created": "Fri, 8 Aug 2014 04:52:59 GMT"}], "update_date": "2014-08-11", "authors_parsed": [["Azad", "Reza", ""], ["Azad", "Babak", ""], ["Kazerooni", "Iman Tavakoli", ""]]}, {"id": "1408.1984", "submitter": "Christian Mayr", "authors": "C. Mayr, R. Sch\\\"uffny", "title": "Neighborhood Rank Order Coding for Robust Texture Analysis and Feature\n  Extraction", "comments": null, "journal-ref": "Proc. 7th International Conference on Hybrid Intelligent Systems\n  HIS 07, pages 290-295, 2007", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research into the visual cortex and general neural information processing has\nled to various attempts to integrate pulse computation schemes in image\nanalysis systems. Of interest is especially the robustness of representing an\nanalogue signal in the phase or duration of a pulsed, quasi-digital signal, as\nwell as the possibility of direct digital interaction, i.e. computation, among\nthese signals. Such a computation can also achieve information compaction for\nsubsequent processing stages. By using a pulse order encoding scheme motivated\nby dendritic pulse interaction, we will show that a powerful low-level feature\nand texture extraction operator, called Pulsed Local Orientation Coding (PLOC),\ncan be implemented. Feature extraction results are being presented, and a\npossible VLSI implementation is detailed.\n", "versions": [{"version": "v1", "created": "Fri, 8 Aug 2014 21:23:46 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Mayr", "C.", ""], ["Sch\u00fcffny", "R.", ""]]}, {"id": "1408.1986", "submitter": "Christian Mayr", "authors": "C. Mayr, A. Heittmann, R. Sch\\\"uffny", "title": "Gabor-like Image Filtering using a Neural Microcircuit", "comments": null, "journal-ref": "IEEE Transactions on Neural Networks, vol. 18, pages 955-959, 2007", "doi": "10.1109/TNN.2007.891687", "report-no": null, "categories": "cs.CV cs.ET q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we present an implementation of a neural microcircuit for\nimage processing employing Hebbian-adaptive learning. The neuronal circuit\nutilizes only excitatory synapses to correlate action potentials, extracting\nthe uncorrelated ones, which contain significant image information. This\ncircuit is capable of approximating Gabor-like image filtering and other image\nprocessing functions\n", "versions": [{"version": "v1", "created": "Fri, 8 Aug 2014 21:24:59 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Mayr", "C.", ""], ["Heittmann", "A.", ""], ["Sch\u00fcffny", "R.", ""]]}, {"id": "1408.2015", "submitter": "Mohammed  Javed", "authors": "Abdessamad Elboushaki, Rachida Hannane, P. Nagabhushan, Mohammed Javed", "title": "Automatic Removal of Marginal Annotations in Printed Text Document", "comments": "Original Article Published by Elsevier at ERCICA-2014, Pages 123-131,\n  August 2014", "journal-ref": "Proceedings of Second International Conference on Emerging\n  Research in Computing, Information,Communication and Applications\n  (ERCICA-14), pages 123-131, August 2014, Bangalore", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering the original printed texts from a document with added handwritten\nannotations in the marginal area is one of the challenging problems, especially\nwhen the original document is not available. Therefore, this paper aims at\nsalvaging automatically the original document from the annotated document by\ndetecting and removing any handwritten annotations that appear in the marginal\narea of the document without any loss of information. Here a two stage\nalgorithm is proposed, where in the first stage due to approximate marginal\nboundary detection with horizontal and vertical projection profiles, all of the\nmarginal annotations along with some part of the original printed text that may\nappear very close to the marginal boundary are removed. Therefore as a second\nstage, using the connected components, a strategy is applied to bring back the\nprinted text components cropped during the first stage. The proposed method is\nvalidated using a dataset of 50 documents having complex handwritten\nannotations, which gives an overall accuracy of 89.01% in removing the marginal\nannotations and 97.74% in case of retrieving the original printed text\ndocument.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 03:56:16 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Elboushaki", "Abdessamad", ""], ["Hannane", "Rachida", ""], ["Nagabhushan", "P.", ""], ["Javed", "Mohammed", ""]]}, {"id": "1408.2289", "submitter": "Yi Li", "authors": "Yi Li, Qi Wei, Fei Qiao, Huazhong Yang", "title": "Physical Computing With No Clock to Implement the Gaussian Pyramid of\n  SIFT Algorithm", "comments": "6", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Physical computing is a technology utilizing the nature of electronic devices\nand circuit topology to cope with computing tasks. In this paper, we propose an\nactive circuit network to implement multi-scale Gaussian filter, which is also\ncalled Gaussian Pyramid in image preprocessing. Various kinds of methods have\nbeen tried to accelerate the key stage in image feature extracting algorithm\nthese years. Compared with existing technologies, GPU parallel computing and\nFPGA accelerating technology, physical computing has great advantage on\nprocessing speed as well as power consumption. We have verified that processing\ntime to implement the Gaussian pyramid of the SIFT algorithm stands on\nnanosecond level through the physical computing technology, while other\nexisting methods all need at least hundreds of millisecond. With an estimate on\nthe stray capacitance of the circuit, the power consumption is around 670pJ to\nfilter a 256x256 image. To the best of our knowledge, this is the most fast\nprocessing technology to accelerate the SIFT algorithm, and it is also a rather\nenergy-efficient method, thanks to the proposed physical computing technology.\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2014 01:08:53 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Li", "Yi", ""], ["Wei", "Qi", ""], ["Qiao", "Fei", ""], ["Yang", "Huazhong", ""]]}, {"id": "1408.2313", "submitter": "Conrad Sanderson", "authors": "Sareh Shirazi, Conrad Sanderson, Chris McCool, Mehrtash T. Harandi", "title": "Bags of Affine Subspaces for Robust Object Tracking", "comments": "in International Conference on Digital Image Computing: Techniques\n  and Applications, 2015", "journal-ref": null, "doi": "10.1109/DICTA.2015.7371239", "report-no": null, "categories": "cs.CV cs.MM cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an adaptive tracking algorithm where the object is modelled as a\ncontinuously updated bag of affine subspaces, with each subspace constructed\nfrom the object's appearance over several consecutive frames. In contrast to\nlinear subspaces, affine subspaces explicitly model the origin of subspaces.\nFurthermore, instead of using a brittle point-to-subspace distance during the\nsearch for the object in a new frame, we propose to use a subspace-to-subspace\ndistance by representing candidate image areas also as affine subspaces.\nDistances between subspaces are then obtained by exploiting the non-Euclidean\ngeometry of Grassmann manifolds. Experiments on challenging videos (containing\nobject occlusions, deformations, as well as variations in pose and\nillumination) indicate that the proposed method achieves higher tracking\naccuracy than several recent discriminative trackers.\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2014 05:13:15 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2015 02:58:39 GMT"}, {"version": "v3", "created": "Fri, 5 Feb 2016 07:35:54 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Shirazi", "Sareh", ""], ["Sanderson", "Conrad", ""], ["McCool", "Chris", ""], ["Harandi", "Mehrtash T.", ""]]}, {"id": "1408.2380", "submitter": "Dacheng Tao", "authors": "Xiaoyan Li and Dacheng Tao", "title": "Video Face Editing Using Temporal-Spatial-Smooth Warping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Editing faces in videos is a popular yet challenging aspect of computer\nvision and graphics, which encompasses several applications including facial\nattractiveness enhancement, makeup transfer, face replacement, and expression\nmanipulation. Simply applying image-based warping algorithms to video-based\nface editing produces temporal incoherence in the synthesized videos because it\nis impossible to consistently localize facial features in two frames\nrepresenting two different faces in two different videos (or even two\nconsecutive frames representing the same face in one video). Therefore, high\nperformance face editing usually requires significant manual manipulation. In\nthis paper we propose a novel temporal-spatial-smooth warping (TSSW) algorithm\nto effectively exploit the temporal information in two consecutive frames, as\nwell as the spatial smoothness within each frame. TSSW precisely estimates two\ncontrol lattices in the horizontal and vertical directions respectively from\nthe corresponding control lattices in the previous frame, by minimizing a novel\nenergy function that unifies a data-driven term, a smoothness term, and feature\npoint constraints. Corresponding warping surfaces then precisely map source\nframes to the target frames. Experimental testing on facial attractiveness\nenhancement, makeup transfer, face replacement, and expression manipulation\ndemonstrates that the proposed approaches can effectively preserve spatial\nsmoothness and temporal coherence in editing facial geometry, skin detail,\nidentity, and expression, which outperform the existing face editing methods.\nIn particular, TSSW is robust to subtly inaccurate localization of feature\npoints and is a vast improvement over image-based warping methods.\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2014 11:48:46 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Li", "Xiaoyan", ""], ["Tao", "Dacheng", ""]]}, {"id": "1408.2478", "submitter": "Stefano Melacci", "authors": "Marco Gori, Marco Lippi, Marco Maggini, Stefano Melacci", "title": "Learning to see like children: proof of concept", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years we have seen a growing interest in machine learning\napproaches to computer vision and, especially, to semantic labeling. Nowadays\nstate of the art systems use deep learning on millions of labeled images with\nvery successful results on benchmarks, though it is unlikely to expect similar\nresults in unrestricted visual environments. Most learning schemes essentially\nignore the inherent sequential structure of videos: this might be a critical\nissue, since any visual recognition process is remarkably more complex when\nshuffling video frames. Based on this remark, we propose a re-foundation of the\ncommunication protocol between visual agents and the environment, which is\nreferred to as learning to see like children. Like for human interaction,\nvisual concepts are acquired by the agents solely by processing their own\nvisual stream along with human supervisions on selected pixels. We give a proof\nof concept that remarkable semantic labeling can emerge within this protocol by\nusing only a few supervised examples. This is made possible by exploiting a\nconstraint of motion coherent labeling that virtually offers tons of\nsupervisions. Additional visual constraints, including those associated with\nobject supervisions, are used within the context of learning from constraints.\nThe framework is extended in the direction of lifelong learning, so as our\nvisual agents live in their own visual environment without distinguishing\nlearning and test set. Learning takes place in deep architectures under a\nprogressive developmental scheme. In order to evaluate our Developmental Visual\nAgents (DVAs), in addition to classic benchmarks, we open the doors of our lab,\nallowing people to evaluate DVAs by crowd-sourcing. Such assessment mechanism\nmight result in a paradigm shift in methodologies and algorithms for computer\nvision, encouraging truly novel solutions within the proposed framework.\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2014 17:38:19 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Gori", "Marco", ""], ["Lippi", "Marco", ""], ["Maggini", "Marco", ""], ["Melacci", "Stefano", ""]]}, {"id": "1408.2584", "submitter": "P. Christopher Staecker", "authors": "Jason Haarmann, Meg P. Murphy, Casey S. Peters, P. Christopher\n  Staecker", "title": "Homotopy equivalence of finite digital images", "comments": "major fixes and removal of errors, terminology changes", "journal-ref": "Journal of Mathematical Imaging and Vision 53, Issue 3, p 288-302,\n  2015", "doi": "10.1007/s10851-015-0578-8", "report-no": null, "categories": "math.GN cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For digital images, there is an established homotopy equivalence relation\nwhich parallels that of classical topology. Many classical homotopy equivalence\ninvariants, such as the Euler characteristic and the homology groups, do not\nremain invariants in the digital setting. This paper develops a numerical\ndigital homotopy invariant and begins to catalog all possible connected digital\nimages on a small number of points, up to homotopy equivalence.\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2014 23:57:39 GMT"}, {"version": "v2", "created": "Mon, 13 Oct 2014 17:14:00 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Haarmann", "Jason", ""], ["Murphy", "Meg P.", ""], ["Peters", "Casey S.", ""], ["Staecker", "P. Christopher", ""]]}, {"id": "1408.2590", "submitter": "Hugh Kennedy Dr.", "authors": "Hugh L. Kennedy", "title": "Multidimensional Digital Filters for Point-Target Detection in Cluttered\n  Infrared Scenes", "comments": "Accepted version", "journal-ref": "J. Electron. Imaging. 23 (6), 063019 (December 17, 2014)", "doi": "10.1117/1.JEI.23.6.063019", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A 3-D spatiotemporal prediction-error filter (PEF), is used to enhance\nforeground/background contrast in (real and simulated) sensor image sequences.\nRelative velocity is utilized to extract point-targets that would otherwise be\nindistinguishable on spatial frequency alone. An optical-flow field is\ngenerated using local estimates of the 3-D autocorrelation function via the\napplication of the fast Fourier transform (FFT) and inverse FFT. Velocity\nestimates are then used to tune in a background-whitening PEF that is matched\nto the motion and texture of the local background. Finite-impulse-response\n(FIR) filters are designed and implemented in the frequency domain. An\nanalytical expression for the frequency response of velocity-tuned FIR filters,\nof odd or even dimension, with an arbitrary delay in each dimension, is\nderived.\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2014 00:30:04 GMT"}, {"version": "v2", "created": "Tue, 4 Nov 2014 23:45:41 GMT"}, {"version": "v3", "created": "Fri, 16 Jan 2015 23:01:33 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Kennedy", "Hugh L.", ""]]}, {"id": "1408.2810", "submitter": "Roozbeh Rajabi", "authors": "Roozbeh Rajabi, Hassan Ghassemian", "title": "Spectral Unmixing of Hyperspectral Imagery using Multilayer NMF", "comments": "5 pages, Journal", "journal-ref": null, "doi": "10.1109/LGRS.2014.2325874", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral images contain mixed pixels due to low spatial resolution of\nhyperspectral sensors. Spectral unmixing problem refers to decomposing mixed\npixels into a set of endmembers and abundance fractions. Due to nonnegativity\nconstraint on abundance fractions, nonnegative matrix factorization (NMF)\nmethods have been widely used for solving spectral unmixing problem. In this\nletter we proposed using multilayer NMF (MLNMF) for the purpose of\nhyperspectral unmixing. In this approach, spectral signature matrix can be\nmodeled as a product of sparse matrices. In fact MLNMF decomposes the\nobservation matrix iteratively in a number of layers. In each layer, we applied\nsparseness constraint on spectral signature matrix as well as on abundance\nfractions matrix. In this way signatures matrix can be sparsely decomposed\ndespite the fact that it is not generally a sparse matrix. The proposed\nalgorithm is applied on synthetic and real datasets. Synthetic data is\ngenerated based on endmembers from USGS spectral library. AVIRIS Cuprite\ndataset has been used as a real dataset for evaluation of proposed method.\nResults of experiments are quantified based on SAD and AAD measures. Results in\ncomparison with previously proposed methods show that the multilayer approach\ncan unmix data more effectively.\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2014 19:07:23 GMT"}], "update_date": "2014-08-13", "authors_parsed": [["Rajabi", "Roozbeh", ""], ["Ghassemian", "Hassan", ""]]}, {"id": "1408.2927", "submitter": "Jingdong Wang", "authors": "Jingdong Wang, Heng Tao Shen, Jingkuan Song, and Jianqiu Ji", "title": "Hashing for Similarity Search: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search (nearest neighbor search) is a problem of pursuing the data\nitems whose distances to a query item are the smallest from a large database.\nVarious methods have been developed to address this problem, and recently a lot\nof efforts have been devoted to approximate search. In this paper, we present a\nsurvey on one of the main solutions, hashing, which has been widely studied\nsince the pioneering work locality sensitive hashing. We divide the hashing\nalgorithms two main categories: locality sensitive hashing, which designs hash\nfunctions without exploring the data distribution and learning to hash, which\nlearns hash functions according the data distribution, and review them from\nvarious aspects, including hash function design and distance measure and search\nscheme in the hash coding space.\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2014 07:29:12 GMT"}], "update_date": "2014-08-14", "authors_parsed": [["Wang", "Jingdong", ""], ["Shen", "Heng Tao", ""], ["Song", "Jingkuan", ""], ["Ji", "Jianqiu", ""]]}, {"id": "1408.2938", "submitter": "Wenbin Li", "authors": "Wenbin Li, Mario Fritz", "title": "Learning Multi-Scale Representations for Material Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent progress in sparse coding and deep learning has made unsupervised\nfeature learning methods a strong competitor to hand-crafted descriptors. In\ncomputer vision, success stories of learned features have been predominantly\nreported for object recognition tasks. In this paper, we investigate if and how\nfeature learning can be used for material recognition. We propose two\nstrategies to incorporate scale information into the learning procedure\nresulting in a novel multi-scale coding procedure. Our results show that our\nlearned features for material recognition outperform hand-crafted descriptors\non the FMD and the KTH-TIPS2 material classification benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2014 08:27:12 GMT"}], "update_date": "2014-08-14", "authors_parsed": [["Li", "Wenbin", ""], ["Fritz", "Mario", ""]]}, {"id": "1408.2997", "submitter": "M. C Hanumantharaju Raju", "authors": "Sreenivasa Setty, N. K Srinath, and M. C Hanumantharaju", "title": "An Improved Approach for Contrast Enhancement of Spinal Cord Images\n  based on Multiscale Retinex Algorithm", "comments": "13 pages, 6 figures, International Journal of Imaging and Robotics.\n  arXiv admin note: text overlap with arXiv:1406.5710", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach for contrast enhancement of spinal cord\nmedical images based on multirate scheme incorporated into multiscale retinex\nalgorithm. The proposed work here uses HSV color space, since HSV color space\nseparates color details from intensity. The enhancement of medical image is\nachieved by down sampling the original image into five versions, namely, tiny,\nsmall, medium, fine, and normal scale. This is due to the fact that the each\nversions of the image when independently enhanced and reconstructed results in\nenormous improvement in the visual quality. Further, the contrast stretching\nand MultiScale Retinex (MSR) techniques are exploited in order to enhance each\nof the scaled version of the image. Finally, the enhanced image is obtained by\ncombining each of these scales in an efficient way to obtain the composite\nenhanced image. The efficiency of the proposed algorithm is validated by using\na wavelet energy metric in the wavelet domain. Reconstructed image using\nproposed method highlights the details (edges and tissues), reduces image noise\n(Gaussian and Speckle) and improves the overall contrast. The proposed\nalgorithm also enhances sharp edges of the tissue surrounding the spinal cord\nregions which is useful for diagnosis of spinal cord lesions. Elaborated\nexperiments are conducted on several medical images and results presented show\nthat the enhanced medical pictures are of good quality and is found to be\nbetter compared with other researcher methods.\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2014 12:50:54 GMT"}], "update_date": "2014-08-14", "authors_parsed": [["Setty", "Sreenivasa", ""], ["Srinath", "N. K", ""], ["Hanumantharaju", "M. C", ""]]}, {"id": "1408.3081", "submitter": "Truyen Tran", "authors": "Truyen Tran, Hung Bui, Svetha Venkatesh", "title": "Human Activity Learning and Segmentation using Partially Hidden\n  Discriminative Models", "comments": "HAREM 2005: Proceedings of the International Workshop on Human\n  Activity Recognition and Modelling", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning and understanding the typical patterns in the daily activities and\nroutines of people from low-level sensory data is an important problem in many\napplication domains such as building smart environments, or providing\nintelligent assistance. Traditional approaches to this problem typically rely\non supervised learning and generative models such as the hidden Markov models\nand its extensions. While activity data can be readily acquired from pervasive\nsensors, e.g. in smart environments, providing manual labels to support\nsupervised training is often extremely expensive. In this paper, we propose a\nnew approach based on semi-supervised training of partially hidden\ndiscriminative models such as the conditional random field (CRF) and the\nmaximum entropy Markov model (MEMM). We show that these models allow us to\nincorporate both labeled and unlabeled data for learning, and at the same time,\nprovide us with the flexibility and accuracy of the discriminative framework.\nOur experimental results in the video surveillance domain illustrate that these\nmodels can perform better than their generative counterpart, the partially\nhidden Markov model, even when a substantial amount of labels are unavailable.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 02:35:49 GMT"}], "update_date": "2014-08-14", "authors_parsed": [["Tran", "Truyen", ""], ["Bui", "Hung", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1408.3139", "submitter": "Farzad Hessar", "authors": "Hossein Hosseini, Farzad Hessar and Farokh Marvasti", "title": "Real-Time Impulse Noise Suppression from Images Using an Efficient\n  Weighted-Average Filtering", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2014.2381649", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method for real-time high density impulse noise\nsuppression from images. In our method, we first apply an impulse detector to\nidentify the corrupted pixels and then employ an innovative weighted-average\nfilter to restore them. The filter takes the nearest neighboring interpolated\nimage as the initial image and computes the weights according to the relative\npositions of the corrupted and uncorrupted pixels. Experimental results show\nthat the proposed method outperforms the best existing methods in both PSNR\nmeasure and visual quality and is quite suitable for real-time applications.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 21:06:02 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Hosseini", "Hossein", ""], ["Hessar", "Farzad", ""], ["Marvasti", "Farokh", ""]]}, {"id": "1408.3218", "submitter": "Babak Saleh", "authors": "Babak Saleh, Kanako Abe, Ravneet Singh Arora, Ahmed Elgammal", "title": "Toward Automated Discovery of Artistic Influence", "comments": "29 pages, 14 figures and 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering the huge amount of art pieces that exist, there is valuable\ninformation to be discovered. Examining a painting, an expert can determine its\nstyle, genre, and the time period that the painting belongs. One important task\nfor art historians is to find influences and connections between artists. Is\ninfluence a task that a computer can measure? The contribution of this paper is\nin exploring the problem of computer-automated suggestion of influences between\nartists, a problem that was not addressed before in a general setting. We first\npresent a comparative study of different classification methodologies for the\ntask of fine-art style classification. A two-level comparative study is\nperformed for this classification problem. The first level reviews the\nperformance of discriminative vs. generative models, while the second level\ntouches the features aspect of the paintings and compares semantic-level\nfeatures vs. low-level and intermediate-level features present in the painting.\nThen, we investigate the question \"Who influenced this artist?\" by looking at\nhis masterpieces and comparing them to others. We pose this interesting\nquestion as a knowledge discovery problem. For this purpose, we investigated\nseveral painting-similarity and artist-similarity measures. As a result, we\nprovide a visualization of artists (Map of Artists) based on the similarity\nbetween their works\n", "versions": [{"version": "v1", "created": "Thu, 14 Aug 2014 08:42:22 GMT"}], "update_date": "2014-08-15", "authors_parsed": [["Saleh", "Babak", ""], ["Abe", "Kanako", ""], ["Arora", "Ravneet Singh", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1408.3264", "submitter": "Mohammad Ali Keyvanrad", "authors": "Mohammad Ali Keyvanrad, Mohammad Mehdi Homayounpour", "title": "A brief survey on deep belief networks and introducing a new object\n  oriented toolbox (DeeBNet)", "comments": "Technical Report 27 pages, Ver3.0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, this is very popular to use the deep architectures in machine\nlearning. Deep Belief Networks (DBNs) are deep architectures that use stack of\nRestricted Boltzmann Machines (RBM) to create a powerful generative model using\ntraining data. DBNs have many ability like feature extraction and\nclassification that are used in many applications like image processing, speech\nprocessing and etc. This paper introduces a new object oriented MATLAB toolbox\nwith most of abilities needed for the implementation of DBNs. In the new\nversion, the toolbox can be used in Octave. According to the results of the\nexperiments conducted on MNIST (image), ISOLET (speech), and 20 Newsgroups\n(text) datasets, it was shown that the toolbox can learn automatically a good\nrepresentation of the input from unlabeled data with better discrimination\nbetween different classes. Also on all datasets, the obtained classification\nerrors are comparable to those of state of the art classifiers. In addition,\nthe toolbox supports different sampling methods (e.g. Gibbs, CD, PCD and our\nnew FEPCD method), different sparsity methods (quadratic, rate distortion and\nour new normal method), different RBM types (generative and discriminative),\nusing GPU, etc. The toolbox is a user-friendly open source software and is\nfreely available on the website\nhttp://ceit.aut.ac.ir/~keyvanrad/DeeBNet%20Toolbox.html .\n", "versions": [{"version": "v1", "created": "Thu, 14 Aug 2014 12:37:57 GMT"}, {"version": "v2", "created": "Mon, 8 Dec 2014 14:44:02 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2015 12:44:01 GMT"}, {"version": "v4", "created": "Fri, 10 Jul 2015 13:21:02 GMT"}, {"version": "v5", "created": "Wed, 22 Jul 2015 14:25:13 GMT"}, {"version": "v6", "created": "Mon, 7 Sep 2015 14:44:47 GMT"}, {"version": "v7", "created": "Wed, 6 Jan 2016 13:20:11 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Keyvanrad", "Mohammad Ali", ""], ["Homayounpour", "Mohammad Mehdi", ""]]}, {"id": "1408.3300", "submitter": "Yuanhao Gong", "authors": "Yuanhao Gong, Ivo F. Sbalzarini", "title": "Gradient Distribution Priors for Biomedical Image Processing", "comments": "submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ill-posed inverse problems are commonplace in biomedical image processing.\nTheir solution typically requires imposing prior knowledge about the latent\nground truth. While this regularizes the problem to an extent where it can be\nsolved, it also biases the result toward the expected. With inappropriate\npriors harming more than they use, it remains unclear what prior to use for a\ngiven practical problem. Priors are hence mostly chosen in an {\\em ad hoc} or\nempirical fashion. We argue here that the gradient distribution of\nnatural-scene images may provide a versatile and well-founded prior for\nbiomedical images. We provide motivation for this choice from different points\nof view, and we fully validate the resulting prior for use on biomedical images\nby showing its stability and correlation with image quality. We then provide a\nset of simple parametric models for the resulting prior, leading to\nstraightforward (quasi-)convex optimization problems for which we provide\nefficient solver algorithms. We illustrate the use of the present models and\nsolvers in a variety of common image-processing tasks, including contrast\nenhancement, noise level estimation, denoising, blind deconvolution,\nzooming/up-sampling, and dehazing. In all cases we show that the present method\nleads to results that are comparable to or better than the state of the art;\nalways using the same, simple prior. We conclude by discussing the limitations\nand possible interpretations of the prior.\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2014 14:37:59 GMT"}, {"version": "v2", "created": "Wed, 3 Sep 2014 15:24:29 GMT"}], "update_date": "2014-09-05", "authors_parsed": [["Gong", "Yuanhao", ""], ["Sbalzarini", "Ivo F.", ""]]}, {"id": "1408.3304", "submitter": "Simon Lacoste-Julien", "authors": "Visesh Chari and Simon Lacoste-Julien and Ivan Laptev and Josef Sivic", "title": "On Pairwise Costs for Network Flow Multi-Object Tracking", "comments": null, "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2015, pp. 5537-5545", "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-object tracking has been recently approached with the min-cost network\nflow optimization techniques. Such methods simultaneously resolve multiple\nobject tracks in a video and enable modeling of dependencies among tracks.\nMin-cost network flow methods also fit well within the \"tracking-by-detection\"\nparadigm where object trajectories are obtained by connecting per-frame outputs\nof an object detector. Object detectors, however, often fail due to occlusions\nand clutter in the video. To cope with such situations, we propose to add\npairwise costs to the min-cost network flow framework. While integer solutions\nto such a problem become NP-hard, we design a convex relaxation solution with\nan efficient rounding heuristic which empirically gives certificates of small\nsuboptimality. We evaluate two particular types of pairwise costs and\ndemonstrate improvements over recent tracking methods in real-world video\nsequences.\n", "versions": [{"version": "v1", "created": "Thu, 14 Aug 2014 14:47:01 GMT"}, {"version": "v2", "created": "Tue, 5 May 2015 23:57:25 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Chari", "Visesh", ""], ["Lacoste-Julien", "Simon", ""], ["Laptev", "Ivan", ""], ["Sivic", "Josef", ""]]}, {"id": "1408.3337", "submitter": "Ari Seff", "authors": "Ari Seff, Le Lu, Kevin M. Cherry, Holger Roth, Jiamin Liu, Shijun\n  Wang, Joanne Hoffman, Evrim B. Turkbey, and Ronald M. Summers", "title": "2D View Aggregation for Lymph Node Detection Using a Shallow Hierarchy\n  of Linear Classifiers", "comments": "This article will be presented at MICCAI (Medical Image Computing and\n  Computer-Assisted Intervention) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Enlarged lymph nodes (LNs) can provide important information for cancer\ndiagnosis, staging, and measuring treatment reactions, making automated\ndetection a highly sought goal. In this paper, we propose a new algorithm\nrepresentation of decomposing the LN detection problem into a set of 2D object\ndetection subtasks on sampled CT slices, largely alleviating the curse of\ndimensionality issue. Our 2D detection can be effectively formulated as linear\nclassification on a single image feature type of Histogram of Oriented\nGradients (HOG), covering a moderate field-of-view of 45 by 45 voxels. We\nexploit both simple pooling and sparse linear fusion schemes to aggregate these\n2D detection scores for the final 3D LN detection. In this manner, detection is\nmore tractable and does not need to perform perfectly at instance level (as\nweak hypotheses) since our aggregation process will robustly harness collective\ninformation for LN detection. Two datasets (90 patients with 389 mediastinal\nLNs and 86 patients with 595 abdominal LNs) are used for validation.\nCross-validation demonstrates 78.0% sensitivity at 6 false positives/volume\n(FP/vol.) (86.1% at 10 FP/vol.) and 73.1% sensitivity at 6 FP/vol. (87.2% at 10\nFP/vol.), for the mediastinal and abdominal datasets respectively. Our results\ncompare favorably to previous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Aug 2014 16:47:34 GMT"}], "update_date": "2014-08-15", "authors_parsed": [["Seff", "Ari", ""], ["Lu", "Le", ""], ["Cherry", "Kevin M.", ""], ["Roth", "Holger", ""], ["Liu", "Jiamin", ""], ["Wang", "Shijun", ""], ["Hoffman", "Joanne", ""], ["Turkbey", "Evrim B.", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1408.3526", "submitter": "Hugh Kennedy Dr.", "authors": "Hugh L. Kennedy", "title": "Parallel software implementation of recursive multidimensional digital\n  filters for point-target detection in cluttered infrared scenes", "comments": "To appear in Proc. 2015 IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP). Added header and DOI", "journal-ref": null, "doi": "10.1109/ICASSP.2015.7178137", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A technique for the enhancement of point targets in clutter is described. The\nlocal 3-D spectrum at each pixel is estimated recursively. An optical\nflow-field for the textured background is then generated using the 3-D\nautocorrelation function and the local velocity estimates are used to apply\nhigh-pass velocity-selective spatiotemporal filters, with finite impulse\nresponses (FIRs), to subtract the background clutter signal, leaving the\nforeground target signal, plus noise. Parallel software implementations using a\nmulticore central processing unit (CPU) and a graphical processing unit (GPU)\nare investigated.\n", "versions": [{"version": "v1", "created": "Fri, 15 Aug 2014 13:06:35 GMT"}, {"version": "v2", "created": "Wed, 3 Sep 2014 00:34:18 GMT"}, {"version": "v3", "created": "Fri, 16 Jan 2015 23:19:59 GMT"}, {"version": "v4", "created": "Fri, 21 Aug 2015 23:57:03 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Kennedy", "Hugh L.", ""]]}, {"id": "1408.3573", "submitter": "Emrah Tasli Dr", "authors": "H. Emrah Tasli and Paul Ivan", "title": "Turkish Presidential Elections TRT Publicity Speech Facial Expression\n  Analysis", "comments": "2 pages 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, facial expressions of the three Turkish presidential\ncandidates Demirtas, Erdogan and Ihsanoglu (in alphabetical order) are analyzed\nduring the publicity speeches featured at TRT (Turkish Radio and Television) on\n03.08.2014. FaceReader is used for the analysis where 3D modeling of the face\nis achieved using the active appearance models (AAM). Over 500 landmark points\nare tracked and analyzed for obtaining the facial expressions during the whole\nspeech. All source videos and the data are publicly available for research\npurposes.\n", "versions": [{"version": "v1", "created": "Fri, 15 Aug 2014 15:51:29 GMT"}], "update_date": "2014-08-18", "authors_parsed": [["Tasli", "H. Emrah", ""], ["Ivan", "Paul", ""]]}, {"id": "1408.3686", "submitter": "Paramanand Chandramouli", "authors": "Paramanand Chandramouli, Paolo Favaro and Daniele Perrone", "title": "Motion Deblurring for Plenoptic Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address for the first time the issue of motion blur in light field images\ncaptured from plenoptic cameras. We propose a solution to the estimation of a\nsharp high resolution scene radiance given a blurry light field image, when the\nmotion blur point spread function is unknown, i.e., the so-called blind\ndeconvolution problem. In a plenoptic camera, the spatial sampling in each view\nis not only decimated but also defocused. Consequently, current blind\ndeconvolution approaches for traditional cameras are not applicable. Due to the\ncomplexity of the imaging model, we investigate first the case of uniform\n(shift-invariant) blur of Lambertian objects, i.e., when objects are\nsufficiently far away from the camera to be approximately invariant to depth\nchanges and their reflectance does not vary with the viewing direction. We\nintroduce a highly parallelizable model for light field motion blur that is\ncomputationally and memory efficient. We then adapt a regularized blind\ndeconvolution approach to our model and demonstrate its performance on both\nsynthetic and real light field data. Our method handles practical issues in\nreal cameras such as radial distortion correction and alignment within an\nenergy minimization framework.\n", "versions": [{"version": "v1", "created": "Sat, 16 Aug 2014 00:18:50 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2016 23:42:34 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Chandramouli", "Paramanand", ""], ["Favaro", "Paolo", ""], ["Perrone", "Daniele", ""]]}, {"id": "1408.3709", "submitter": "Parama Bagchi", "authors": "Parama Bagchi, Debotosh Bhattacharjee and Mita Nasipuri", "title": "Robust 3D face recognition in presence of pose and partial occlusions or\n  missing parts", "comments": "the paper is of 15 pages, International Journal in Foundations of\n  Computer Science & Technology (IJFCST), Vol.4, No.4, July 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a robust 3D face recognition system which can\nhandle pose as well as occlusions in real world. The system at first takes as\ninput, a 3D range image, simultaneously registers it using ICP(Iterative\nClosest Point) algorithm. ICP used in this work, registers facial surfaces to a\ncommon model by minimizing distances between a probe model and a gallery model.\nHowever the performance of ICP relies heavily on the initial conditions. Hence,\nit is necessary to provide an initial registration, which will be improved\niteratively and finally converge to the best alignment possible. Once the faces\nare registered, the occlusions are automatically extracted by thresholding the\ndepth map values of the 3D image. After the occluded regions are detected,\nrestoration is done by Principal Component Analysis (PCA). The restored images,\nafter the removal of occlusions, are then fed to the recognition system for\nclassification purpose. Features are extracted from the reconstructed\nnon-occluded face images in the form of face normals. The experimental results\nwhich were obtained on the occluded facial images from the Bosphorus 3D face\ndatabase, illustrate that our occlusion compensation scheme has attained a\nrecognition accuracy of 91.30%.\n", "versions": [{"version": "v1", "created": "Sat, 16 Aug 2014 06:43:30 GMT"}], "update_date": "2014-08-19", "authors_parsed": [["Bagchi", "Parama", ""], ["Bhattacharjee", "Debotosh", ""], ["Nasipuri", "Mita", ""]]}, {"id": "1408.3740", "submitter": "Yangyang Xu", "authors": "Yangyang Xu and Wotao Yin", "title": "A fast patch-dictionary method for whole image recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various algorithms have been proposed for dictionary learning. Among those\nfor image processing, many use image patches to form dictionaries. This paper\nfocuses on whole-image recovery from corrupted linear measurements. We address\nthe open issue of representing an image by overlapping patches: the overlapping\nleads to an excessive number of dictionary coefficients to determine. With very\nfew exceptions, this issue has limited the applications of image-patch methods\nto the local kind of tasks such as denoising, inpainting, cartoon-texture\ndecomposition, super-resolution, and image deblurring, for which one can\nprocess a few patches at a time. Our focus is global imaging tasks such as\ncompressive sensing and medical image recovery, where the whole image is\nencoded together, making it either impossible or very ineffective to update a\nfew patches at a time.\n  Our strategy is to divide the sparse recovery into multiple subproblems, each\nof which handles a subset of non-overlapping patches, and then the results of\nthe subproblems are averaged to yield the final recovery. This simple strategy\nis surprisingly effective in terms of both quality and speed. In addition, we\naccelerate computation of the learned dictionary by applying a recent block\nproximal-gradient method, which not only has a lower per-iteration complexity\nbut also takes fewer iterations to converge, compared to the current\nstate-of-the-art. We also establish that our algorithm globally converges to a\nstationary point. Numerical results on synthetic data demonstrate that our\nalgorithm can recover a more faithful dictionary than two state-of-the-art\nmethods.\n  Combining our whole-image recovery and dictionary-learning methods, we\nnumerically simulate image inpainting, compressive sensing recovery, and\ndeblurring. Our recovery is more faithful than those of a total variation\nmethod and a method based on overlapping patches.\n", "versions": [{"version": "v1", "created": "Sat, 16 Aug 2014 15:09:32 GMT"}], "update_date": "2014-08-19", "authors_parsed": [["Xu", "Yangyang", ""], ["Yin", "Wotao", ""]]}, {"id": "1408.3750", "submitter": "S\\'ebastien Ouellet", "authors": "S\\'ebastien Ouellet", "title": "Real-time emotion recognition for gaming using deep convolutional\n  network features", "comments": "6 pages, 8 figures, IEEE style", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of the present study is to explore the application of deep\nconvolutional network features to emotion recognition. Results indicate that\nthey perform similarly to other published models at a best recognition rate of\n94.4%, and do so with a single still image rather than a video stream. An\nimplementation of an affective feedback game is also described, where a\nclassifier using these features tracks the facial expressions of a player in\nreal-time.\n", "versions": [{"version": "v1", "created": "Sat, 16 Aug 2014 17:11:44 GMT"}], "update_date": "2014-08-19", "authors_parsed": [["Ouellet", "S\u00e9bastien", ""]]}, {"id": "1408.3772", "submitter": "Shervin Minaee", "authors": "Shervin Minaee and AmirAli Abdolrashidi", "title": "Highly Accurate Multispectral Palmprint Recognition Using Statistical\n  and Wavelet Features", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Palmprint is one of the most useful physiological biometrics that can be used\nas a powerful means in personal recognition systems. The major features of the\npalmprints are palm lines, wrinkles and ridges, and many approaches use them in\ndifferent ways towards solving the palmprint recognition problem. Here we have\nproposed to use a set of statistical and wavelet-based features; statistical to\ncapture the general characteristics of palmprints; and wavelet-based to find\nthose information not evident in the spatial domain. Also we use two different\nclassification approaches, minimum distance classifier scheme and weighted\nmajority voting algorithm, to perform palmprint matching. The proposed method\nis tested on a well-known palmprint dataset of 6000 samples and has shown an\nimpressive accuracy rate of 99.65\\%-100\\% for most scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 16 Aug 2014 21:02:44 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2015 16:31:26 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Minaee", "Shervin", ""], ["Abdolrashidi", "AmirAli", ""]]}, {"id": "1408.3809", "submitter": "Hossein Rahmani", "authors": "Hossein Rahmani, Arif Mahmood, Du Q. Huynh, Ajmal Mian", "title": "HOPC: Histogram of Oriented Principal Components of 3D Pointclouds for\n  Action Recognition", "comments": "ECCV 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing techniques for 3D action recognition are sensitive to viewpoint\nvariations because they extract features from depth images which change\nsignificantly with viewpoint. In contrast, we directly process the pointclouds\nand propose a new technique for action recognition which is more robust to\nnoise, action speed and viewpoint variations. Our technique consists of a novel\ndescriptor and keypoint detection algorithm. The proposed descriptor is\nextracted at a point by encoding the Histogram of Oriented Principal Components\n(HOPC) within an adaptive spatio-temporal support volume around that point.\nBased on this descriptor, we present a novel method to detect Spatio-Temporal\nKey-Points (STKPs) in 3D pointcloud sequences. Experimental results show that\nthe proposed descriptor and STKP detector outperform state-of-the-art\nalgorithms on three benchmark human activity datasets. We also introduce a new\nmultiview public dataset and show the robustness of our proposed method to\nviewpoint variations.\n", "versions": [{"version": "v1", "created": "Sun, 17 Aug 2014 10:34:47 GMT"}, {"version": "v2", "created": "Mon, 1 Sep 2014 02:49:32 GMT"}, {"version": "v3", "created": "Tue, 2 Sep 2014 01:46:55 GMT"}, {"version": "v4", "created": "Mon, 22 Sep 2014 06:50:28 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Rahmani", "Hossein", ""], ["Mahmood", "Arif", ""], ["Huynh", "Du Q.", ""], ["Mian", "Ajmal", ""]]}, {"id": "1408.3810", "submitter": "Hossein Rahmani", "authors": "Hossein Rahmani, Arif Mahmood, Du Huynh, Ajmal Mian", "title": "Action Classification with Locality-constrained Linear Coding", "comments": "ICPR 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an action classification algorithm which uses Locality-constrained\nLinear Coding (LLC) to capture discriminative information of human body\nvariations in each spatiotemporal subsequence of a video sequence. Our proposed\nmethod divides the input video into equally spaced overlapping spatiotemporal\nsubsequences, each of which is decomposed into blocks and then cells. We use\nthe Histogram of Oriented Gradient (HOG3D) feature to encode the information in\neach cell. We justify the use of LLC for encoding the block descriptor by\ndemonstrating its superiority over Sparse Coding (SC). Our sequence descriptor\nis obtained via a logistic regression classifier with L2 regularization. We\nevaluate and compare our algorithm with ten state-of-the-art algorithms on five\nbenchmark datasets. Experimental results show that, on average, our algorithm\ngives better accuracy than these ten algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 17 Aug 2014 10:46:45 GMT"}, {"version": "v2", "created": "Mon, 22 Sep 2014 06:54:34 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Rahmani", "Hossein", ""], ["Mahmood", "Arif", ""], ["Huynh", "Du", ""], ["Mian", "Ajmal", ""]]}, {"id": "1408.3814", "submitter": "Binarani Devi Oinam", "authors": "Oinam Binarani Devi, Nissi S. Paul, Y. Jayanta Singh", "title": "Robust Statistical Approach for Extraction of Moving Human Silhouettes\n  from Videos", "comments": "10 pages, 5 figures", "journal-ref": "International Journal on Information Theory (IJIT), Vol.3, No.3,\n  July 2014, Pg.55-64", "doi": "10.5121/ijit.2014.3306", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose estimation is one of the key problems in computer vision that has\nbeen studied in the recent years. The significance of human pose estimation is\nin the higher level tasks of understanding human actions applications such as\nrecognition of anomalous actions present in videos and many other related\napplications. The human poses can be estimated by extracting silhouettes of\nhumans as silhouettes are robust to variations and it gives the shape\ninformation of the human body. Some common challenges include illumination\nchanges, variation in environments, and variation in human appearances. Thus\nthere is a need for a robust method for human pose estimation. This paper\npresents a study and analysis of approaches existing for silhouette extraction\nand proposes a robust technique for extracting human silhouettes in video\nsequences. Gaussian Mixture Model (GMM) A statistical approach is combined with\nHSV (Hue, Saturation and Value) color space model for a robust background model\nthat is used for background subtraction to produce foreground blobs, called\nhuman silhouettes. Morphological operations are then performed on foreground\nblobs from background subtraction. The silhouettes obtained from this work can\nbe used in further tasks associated with human action interpretation and\nactivity processes like human action classification, human pose estimation and\naction recognition or action interpretation.\n", "versions": [{"version": "v1", "created": "Sun, 17 Aug 2014 11:25:34 GMT"}], "update_date": "2014-08-26", "authors_parsed": [["Devi", "Oinam Binarani", ""], ["Paul", "Nissi S.", ""], ["Singh", "Y. Jayanta", ""]]}, {"id": "1408.3818", "submitter": "h\\'ector Rabal", "authors": "Lucia I. Passoni, Ana I. Dai Pra, Gustavo J. Meschino, MArcelo Guzman,\n  Chistian Weber, H\\'ector Rabal, Marcelo Trivi", "title": "Unsupervised learning segmentation for dynamic speckle activity images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.optics cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the design of decision models based on Computational\nIntelligence techniques applied to image sequences of dynamic laser speckle.\nThese models aim to identify image regions of biological specimens illuminated\nby a coherent beam coming from a laser. The field image is pseudo colored using\na Self Organizing Map projection. This process is carried out using a set of\ndescriptors applied to the intensity variations along time in every pixel of an\nimage sequence. The models use descriptors selected to improve effectiveness,\ndepending on the specific application. We present two examples of the\napplication of the proposed techniques to assess biological tissues. The\nresults obtained are encouraging and significantly improve those obtained using\na single descriptor.\n", "versions": [{"version": "v1", "created": "Sun, 17 Aug 2014 12:46:20 GMT"}], "update_date": "2014-08-19", "authors_parsed": [["Passoni", "Lucia I.", ""], ["Pra", "Ana I. Dai", ""], ["Meschino", "Gustavo J.", ""], ["Guzman", "MArcelo", ""], ["Weber", "Chistian", ""], ["Rabal", "H\u00e9ctor", ""], ["Trivi", "Marcelo", ""]]}, {"id": "1408.3873", "submitter": "Lorenzo Livi", "authors": "Lorenzo Livi, Antonello Rizzi, Alireza Sadeghian", "title": "Classifying sequences by the optimized dissimilarity space embedding\n  approach: a case study on the solubility analysis of the E. coli proteome", "comments": "10 pages, 49 references", "journal-ref": null, "doi": "10.3233/IFS-151550", "report-no": null, "categories": "cs.CV cs.AI physics.bio-ph q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evaluate a version of the recently-proposed classification system named\nOptimized Dissimilarity Space Embedding (ODSE) that operates in the input space\nof sequences of generic objects. The ODSE system has been originally presented\nas a classification system for patterns represented as labeled graphs. However,\nsince ODSE is founded on the dissimilarity space representation of the input\ndata, the classifier can be easily adapted to any input domain where it is\npossible to define a meaningful dissimilarity measure. Here we demonstrate the\neffectiveness of the ODSE classifier for sequences by considering an\napplication dealing with the recognition of the solubility degree of the\nEscherichia coli proteome. Solubility, or analogously aggregation propensity,\nis an important property of protein molecules, which is intimately related to\nthe mechanisms underlying the chemico-physical process of folding. Each protein\nof our dataset is initially associated with a solubility degree and it is\nrepresented as a sequence of symbols, denoting the 20 amino acid residues. The\nherein obtained computational results, which we stress that have been achieved\nwith no context-dependent tuning of the ODSE system, confirm the validity and\ngenerality of the ODSE-based approach for structured data classification.\n", "versions": [{"version": "v1", "created": "Sun, 17 Aug 2014 23:46:55 GMT"}, {"version": "v2", "created": "Wed, 14 Jan 2015 21:20:19 GMT"}], "update_date": "2015-02-16", "authors_parsed": [["Livi", "Lorenzo", ""], ["Rizzi", "Antonello", ""], ["Sadeghian", "Alireza", ""]]}, {"id": "1408.3967", "submitter": "Zhanpeng Zhang", "authors": "Zhanpeng Zhang, Ping Luo, Chen Change Loy, Xiaoou Tang", "title": "Learning Deep Representation for Face Alignment with Auxiliary\n  Attributes", "comments": "to be published in the IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI)", "journal-ref": null, "doi": "10.1109/TPAMI.2015.2469286", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we show that landmark detection or face alignment task is not\na single and independent problem. Instead, its robustness can be greatly\nimproved with auxiliary information. Specifically, we jointly optimize landmark\ndetection together with the recognition of heterogeneous but subtly correlated\nfacial attributes, such as gender, expression, and appearance attributes. This\nis non-trivial since different attribute inference tasks have different\nlearning difficulties and convergence rates. To address this problem, we\nformulate a novel tasks-constrained deep model, which not only learns the\ninter-task correlation but also employs dynamic task coefficients to facilitate\nthe optimization convergence when learning multiple complex tasks. Extensive\nevaluations show that the proposed task-constrained learning (i) outperforms\nexisting face alignment methods, especially in dealing with faces with severe\nocclusion and pose variation, and (ii) reduces model complexity drastically\ncompared to the state-of-the-art methods based on cascaded deep model.\n", "versions": [{"version": "v1", "created": "Mon, 18 Aug 2014 10:34:29 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2015 08:02:41 GMT"}, {"version": "v3", "created": "Sun, 9 Aug 2015 00:39:18 GMT"}, {"version": "v4", "created": "Tue, 11 Aug 2015 10:08:40 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Zhang", "Zhanpeng", ""], ["Luo", "Ping", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1408.3985", "submitter": "George Eskander PhD", "authors": "George S. Eskander, Robert Sabourin and Eric Granger", "title": "Offline Signature-Based Fuzzy Vault (OSFV: Review and New Results", "comments": "This paper has been submitted to The 2014 IEEE Symposium on\n  Computational Intelligence in Biometrics and Identity Management (CIBIM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An offline signature-based fuzzy vault (OSFV) is a bio-cryptographic\nimplementation that uses handwritten signature images as biometrics instead of\ntraditional passwords to secure private cryptographic keys. Having a reliable\nOSFV implementation is the first step towards automating financial and legal\nauthentication processes, as it provides greater security of confidential\ndocuments by means of the embedded handwritten signatures. The authors have\nrecently proposed the first OSFV implementation which is reviewed in this\npaper. In this system, a machine learning approach based on the dissimilarity\nrepresentation concept is employed to select a reliable feature representation\nadapted for the fuzzy vault scheme. Some variants of this system are proposed\nfor enhanced accuracy and security. In particular, a new method that adapts\nuser key size is presented. Performance of proposed methods are compared using\nthe Brazilian PUCPR and GPDS signature databases and results indicate that the\nkey-size adaptation method achieves a good compromise between security and\naccuracy. While average system entropy is increased from 45-bits to about\n51-bits, the AER (average error rate) is decreased by about 21%.\n", "versions": [{"version": "v1", "created": "Mon, 18 Aug 2014 11:54:48 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Eskander", "George S.", ""], ["Sabourin", "Robert", ""], ["Granger", "Eric", ""]]}, {"id": "1408.4002", "submitter": "Benjamin Eltzner", "authors": "Benjamin Eltzner, Carina Wollnik, Carsten Gottschlich, Stephan\n  Huckemann, Florian Rehfeldt", "title": "The Filament Sensor for Near Real-Time Detection of Cytoskeletal Fiber\n  Structures", "comments": "32 pages, 21 figures", "journal-ref": "PLoS ONE 10(5): e0126346, May 2015", "doi": "10.1371/journal.pone.0126346", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reliable extraction of filament data from microscopic images is of high\ninterest in the analysis of acto-myosin structures as early morphological\nmarkers in mechanically guided differentiation of human mesenchymal stem cells\nand the understanding of the underlying fiber arrangement processes. In this\npaper, we propose the filament sensor (FS), a fast and robust processing\nsequence which detects and records location, orientation, length and width for\neach single filament of an image, and thus allows for the above described\nanalysis. The extraction of these features has previously not been possible\nwith existing methods. We evaluate the performance of the proposed FS in terms\nof accuracy and speed in comparison to three existing methods with respect to\ntheir limited output. Further, we provide a benchmark dataset of real cell\nimages along with filaments manually marked by a human expert as well as\nsimulated benchmark images. The FS clearly outperforms existing methods in\nterms of computational runtime and filament extraction accuracy. The\nimplementation of the FS and the benchmark database are available as open\nsource.\n", "versions": [{"version": "v1", "created": "Mon, 18 Aug 2014 13:06:03 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2015 13:19:42 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2015 08:40:32 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Eltzner", "Benjamin", ""], ["Wollnik", "Carina", ""], ["Gottschlich", "Carsten", ""], ["Huckemann", "Stephan", ""], ["Rehfeldt", "Florian", ""]]}, {"id": "1408.4143", "submitter": "Mohammed Abdelsamea", "authors": "Marghny H. Mohamed and Mohammed M. Abdelsamea", "title": "Self Organization Map based Texture Feature Extraction for Efficient\n  Medical Image Categorization", "comments": "In Proceedings of the 4th ACM International Conference on Intelligent\n  Computing and Information Systems, ICICIS 2009, Cairo, Egypt 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture is one of the most important properties of visual surface that helps\nin discriminating one object from another or an object from background. The\nself-organizing map (SOM) is an excellent tool in exploratory phase of data\nmining. It projects its input space on prototypes of a low-dimensional regular\ngrid that can be effectively utilized to visualize and explore properties of\nthe data. This paper proposes an enhancement extraction method for accurate\nextracting features for efficient image representation it based on SOM neural\nnetwork. In this approach, we apply three different partitioning approaches as\na region of interested (ROI) selection methods for extracting different\naccurate textural features from medical image as a primary step of our\nextraction method. Fisherfaces feature selection is used, for selecting\ndiscriminated features form extracted textural features. Experimental result\nshowed the high accuracy of medical image categorization with our proposed\nextraction method. Experiments held on Mammographic Image Analysis Society\n(MIAS) dataset.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jul 2014 13:43:19 GMT"}], "update_date": "2014-08-20", "authors_parsed": [["Mohamed", "Marghny H.", ""], ["Abdelsamea", "Mohammed M.", ""]]}, {"id": "1408.4325", "submitter": "Diane Larlus", "authors": "Yangmuzi Zhang, Diane Larlus, Florent Perronnin", "title": "What makes an Image Iconic? A Fine-Grained Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural approach to teaching a visual concept, e.g. a bird species, is to\nshow relevant images. However, not all relevant images represent a concept\nequally well. In other words, they are not necessarily iconic. This observation\nraises three questions. Is iconicity a subjective property? If not, can we\npredict iconicity? And what exactly makes an image iconic? We provide answers\nto these questions through an extensive experimental study on a challenging\nfine-grained dataset of birds. We first show that iconicity ratings are\nconsistent across individuals, even when they are not domain experts, thus\ndemonstrating that iconicity is not purely subjective. We then consider an\nexhaustive list of properties that are intuitively related to iconicity and\nmeasure their correlation with these iconicity ratings. We combine them to\npredict iconicity of new unseen images. We also propose a direct iconicity\npredictor that is discriminatively trained with iconicity ratings. By combining\nboth systems, we get an iconicity prediction that approaches human performance.\n", "versions": [{"version": "v1", "created": "Tue, 19 Aug 2014 13:26:01 GMT"}], "update_date": "2014-08-20", "authors_parsed": [["Zhang", "Yangmuzi", ""], ["Larlus", "Diane", ""], ["Perronnin", "Florent", ""]]}, {"id": "1408.4363", "submitter": "Xavier Giro-i-Nieto", "authors": "Eva Mohedano (1), Graham Healy (1), Kevin McGuinness (1), Xavier\n  Giro-i-Nieto (2), Noel E. O'Connor (1) and Alan F. Smeaton (1) ((1) Dublin\n  City University, (2) Universitat Politecnica de Catalunya)", "title": "Object Segmentation in Images using EEG Signals", "comments": "This is a preprint version prior to submission for peer-review of the\n  paper accepted to the 22nd ACM International Conference on Multimedia\n  (November 3-7, 2014, Orlando, Florida, USA) for the High Risk High Reward\n  session. 10 pages", "journal-ref": null, "doi": "10.1145/2647868.2654896", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the potential of brain-computer interfaces in segmenting\nobjects from images. Our approach is centered around designing an effective\nmethod for displaying the image parts to the users such that they generate\nmeasurable brain reactions. When an image region, specifically a block of\npixels, is displayed we estimate the probability of the block containing the\nobject of interest using a score based on EEG activity. After several such\nblocks are displayed, the resulting probability map is binarized and combined\nwith the GrabCut algorithm to segment the image into object and background\nregions. This study shows that BCI and simple EEG analysis are useful in\nlocating object boundaries in images.\n", "versions": [{"version": "v1", "created": "Tue, 19 Aug 2014 15:24:44 GMT"}], "update_date": "2014-08-20", "authors_parsed": [["Mohedano", "Eva", ""], ["Healy", "Graham", ""], ["McGuinness", "Kevin", ""], ["Giro-i-Nieto", "Xavier", ""], ["O'Connor", "Noel E.", ""], ["Smeaton", "Alan F.", ""]]}, {"id": "1408.4504", "submitter": "Mohammed Abdelsamea", "authors": "Mohammed M. Abdelsamea", "title": "Unsupervised Parallel Extraction based Texture for Efficient Image\n  Representation", "comments": "arXiv admin note: substantial text overlap with arXiv:1408.4143", "journal-ref": "2011 International Conference on Signal, Image Processing and\n  Applications With workshop of ICEEA 2011, IPCSIT vol.21 (2011), IACSIT Press,\n  Singapore", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SOM is a type of unsupervised learning where the goal is to discover some\nunderlying structure of the data. In this paper, a new extraction method based\non the main idea of Concurrent Self-Organizing Maps (CSOM), representing a\nwinner-takes-all collection of small SOM networks is proposed. Each SOM of the\nsystem is trained individually to provide best results for one class only. The\nexperiments confirm that the proposed features based CSOM is capable to\nrepresent image content better than extracted features based on a single big\nSOM and these proposed features improve the final decision of the CAD.\nExperiments held on Mammographic Image Analysis Society (MIAS) dataset.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 01:10:44 GMT"}], "update_date": "2014-08-21", "authors_parsed": [["Abdelsamea", "Mohammed M.", ""]]}, {"id": "1408.4576", "submitter": "Bingchen Gong", "authors": "Sibei Yang and Liangde Tao and Bingchen Gong", "title": "Introduction to Clustering Algorithms and Applications", "comments": "This paper has been withdrawn by the author due to unsuitable content", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Data clustering is the process of identifying natural groupings or clusters\nwithin multidimensional data based on some similarity measure. Clustering is a\nfundamental process in many different disciplines. Hence, researchers from\ndifferent fields are actively working on the clustering problem. This paper\nprovides an overview of the different representative clustering methods. In\naddition, application of clustering in different field is briefly introduced.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 09:25:18 GMT"}, {"version": "v2", "created": "Mon, 25 Aug 2014 14:49:56 GMT"}], "update_date": "2014-08-26", "authors_parsed": [["Yang", "Sibei", ""], ["Tao", "Liangde", ""], ["Gong", "Bingchen", ""]]}, {"id": "1408.4692", "submitter": "Erik Rodner", "authors": "Alexander Freytag, Johannes R\\\"uhle, Paul Bodesheim, Erik Rodner, and\n  Joachim Denzler", "title": "Seeing through bag-of-visual-word glasses: towards understanding\n  quantization effects in feature extraction methods", "comments": "An abstract version of this paper was accepted for the ICPR FEAST\n  Workshop", "journal-ref": null, "doi": null, "report-no": "TR-FSU-INF-CV-2014-01", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector-quantized local features frequently used in bag-of-visual-words\napproaches are the backbone of popular visual recognition systems due to both\ntheir simplicity and their performance. Despite their success,\nbag-of-words-histograms basically contain low-level image statistics (e.g.,\nnumber of edges of different orientations). The question remains how much\nvisual information is \"lost in quantization\" when mapping visual features to\ncode words? To answer this question, we present an in-depth analysis of the\neffect of local feature quantization on human recognition performance. Our\nanalysis is based on recovering the visual information by inverting quantized\nlocal features and presenting these visualizations with different codebook\nsizes to human observers. Although feature inversion techniques are around for\nquite a while, to the best of our knowledge, our technique is the first\nvisualizing especially the effect of feature quantization. Thereby, we are now\nable to compare single steps in common image classification pipelines to human\ncounterparts.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 15:16:10 GMT"}], "update_date": "2014-08-21", "authors_parsed": [["Freytag", "Alexander", ""], ["R\u00fchle", "Johannes", ""], ["Bodesheim", "Paul", ""], ["Rodner", "Erik", ""], ["Denzler", "Joachim", ""]]}, {"id": "1408.4703", "submitter": "Amelia Carolina Sparavigna", "authors": "Amelia Carolina Sparavigna", "title": "GIMP and Wavelets for Medical Image Processing: Enhancing Images of the\n  Fundus of the Eye", "comments": "Keywords: Image processing, Retina, Retina Vessels, GIMP,\n  AstroFracTool, Iris, Wavelets", "journal-ref": "ijSciences, 2014, Volume 3, Issue 8, pages 35-47", "doi": "10.18483/ijSci.556", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visual analysis of retina and of its vascular characteristics is\nimportant in the diagnosis and monitoring of diseases of visual perception. In\nthe related medical diagnoses, the digital processing of the fundus images is\nused to obtain the segmentation of retinal vessels. However, an image\nsegmentation is often requiring methods based on peculiar or complex\nalgorithms: in this paper we will show some alternative approaches obtained by\napplying freely available tools to enhance, without a specific segmentation,\nthe images of the fundus of the eye. We will see in particular, that combining\nthe use of GIMP, the GNU Image Manipulation Program, with the wavelet filter of\nIris, a program well-known for processing astronomical images, the result is\ngiving images which can be alternative of those obtained from segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 15:49:17 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Sparavigna", "Amelia Carolina", ""]]}, {"id": "1408.4712", "submitter": "Wen-Ze Shao", "authors": "Wen-Ze Shao, Hai-Bo Li, Michael Elad", "title": "Bi-l0-l2-Norm Regularization for Blind Motion Deblurring", "comments": "32 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In blind motion deblurring, leading methods today tend towards highly\nnon-convex approximations of the l0-norm, especially in the image\nregularization term. In this paper, we propose a simple, effective and fast\napproach for the estimation of the motion blur-kernel, through a bi-l0-l2-norm\nregularization imposed on both the intermediate sharp image and the\nblur-kernel. Compared with existing methods, the proposed regularization is\nshown to be more effective and robust, leading to a more accurate motion\nblur-kernel and a better final restored image. A fast numerical scheme is\ndeployed for alternatingly computing the sharp image and the blur-kernel, by\ncoupling the operator splitting and augmented Lagrangian methods. Experimental\nresults on both a benchmark image dataset and real-world motion blurred images\nshow that the proposed approach is highly competitive with state-of-the- art\nmethods in both deblurring effectiveness and computational efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 16:18:13 GMT"}, {"version": "v2", "created": "Tue, 20 Jan 2015 08:06:47 GMT"}, {"version": "v3", "created": "Thu, 22 Jan 2015 14:02:38 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Shao", "Wen-Ze", ""], ["Li", "Hai-Bo", ""], ["Elad", "Michael", ""]]}, {"id": "1408.4721", "submitter": "Frank Hannig", "authors": "Moritz Schmid, Oliver Reiche, Christian Schmitt, Frank Hannig,\n  J\\\"urgen Teich", "title": "Code Generation for High-Level Synthesis of Multiresolution Applications\n  on FPGAs", "comments": "Presented at First International Workshop on FPGAs for Software\n  Programmers (FSP 2014) (arXiv:1408.4423)", "journal-ref": null, "doi": null, "report-no": "FSP/2014/04", "categories": "cs.CV cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiresolution Analysis (MRA) is a mathematical method that is based on\nworking on a problem at different scales. One of its applications is medical\nimaging where processing at multiple scales, based on the concept of Gaussian\nand Laplacian image pyramids, is a well-known technique. It is often applied to\nreduce noise while preserving image detail on different levels of granularity\nwithout modifying the filter kernel. In scientific computing, multigrid methods\nare a popular choice, as they are asymptotically optimal solvers for elliptic\nPartial Differential Equations (PDEs). As such algorithms have a very high\ncomputational complexity that would overwhelm CPUs in the presence of real-time\nconstraints, application-specific processors come into consideration for\nimplementation. Despite of huge advancements in leveraging productivity in the\nrespective fields, designers are still required to have detailed knowledge\nabout coding techniques and the targeted architecture to achieve efficient\nsolutions. Recently, the HIPAcc framework was proposed as a means for automatic\ncode generation of image processing algorithms, based on a Domain-Specific\nLanguage (DSL). From the same code base, it is possible to generate code for\nefficient implementations on several accelerator technologies including\ndifferent types of Graphics Processing Units (GPUs) as well as reconfigurable\nlogic (FPGAs). In this work, we demonstrate the ability of HIPAcc to generate\ncode for the implementation of multiresolution applications on FPGAs and\nembedded GPUs.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 16:56:42 GMT"}], "update_date": "2014-08-21", "authors_parsed": [["Schmid", "Moritz", ""], ["Reiche", "Oliver", ""], ["Schmitt", "Christian", ""], ["Hannig", "Frank", ""], ["Teich", "J\u00fcrgen", ""]]}, {"id": "1408.5093", "submitter": "Yangqing Jia", "authors": "Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan\n  Long, Ross Girshick, Sergio Guadarrama, Trevor Darrell", "title": "Caffe: Convolutional Architecture for Fast Feature Embedding", "comments": "Tech report for the Caffe software at http://github.com/BVLC/Caffe/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caffe provides multimedia scientists and practitioners with a clean and\nmodifiable framework for state-of-the-art deep learning algorithms and a\ncollection of reference models. The framework is a BSD-licensed C++ library\nwith Python and MATLAB bindings for training and deploying general-purpose\nconvolutional neural networks and other deep models efficiently on commodity\narchitectures. Caffe fits industry and internet-scale media needs by CUDA GPU\ncomputation, processing over 40 million images a day on a single K40 or Titan\nGPU ($\\approx$ 2.5 ms per image). By separating model representation from\nactual implementation, Caffe allows experimentation and seamless switching\namong platforms for ease of development and deployment from prototyping\nmachines to cloud environments. Caffe is maintained and developed by the\nBerkeley Vision and Learning Center (BVLC) with the help of an active community\nof contributors on GitHub. It powers ongoing research projects, large-scale\nindustrial applications, and startup prototypes in vision, speech, and\nmultimedia.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 23:00:32 GMT"}], "update_date": "2014-08-22", "authors_parsed": [["Jia", "Yangqing", ""], ["Shelhamer", "Evan", ""], ["Donahue", "Jeff", ""], ["Karayev", "Sergey", ""], ["Long", "Jonathan", ""], ["Girshick", "Ross", ""], ["Guadarrama", "Sergio", ""], ["Darrell", "Trevor", ""]]}, {"id": "1408.5275", "submitter": "Mohammad Reza Keshtkaran", "authors": "Mohammad Reza Keshtkaran and Zhi Yang", "title": "Unsupervised Spike Sorting Based on Discriminative Subspace Learning", "comments": "EMBC14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spike sorting is a fundamental preprocessing step for many neuroscience\nstudies which rely on the analysis of spike trains. In this paper, we present\ntwo unsupervised spike sorting algorithms based on discriminative subspace\nlearning. The first algorithm simultaneously learns the discriminative feature\nsubspace and performs clustering. It uses histogram of features in the most\ndiscriminative projection to detect the number of neurons. The second algorithm\nperforms hierarchical divisive clustering that learns a discriminative\n1-dimensional subspace for clustering in each level of the hierarchy until\nachieving almost unimodal distribution in the subspace. The algorithms are\ntested on synthetic and in-vivo data, and are compared against two widely used\nspike sorting methods. The comparative results demonstrate that our spike\nsorting methods can achieve substantially higher accuracy in lower dimensional\nfeature space, and they are highly robust to noise. Moreover, they provide\nsignificantly better cluster separability in the learned subspace than in the\nsubspace obtained by principal component analysis or wavelet transform.\n", "versions": [{"version": "v1", "created": "Fri, 22 Aug 2014 12:12:35 GMT"}], "update_date": "2014-08-25", "authors_parsed": [["Keshtkaran", "Mohammad Reza", ""], ["Yang", "Zhi", ""]]}, {"id": "1408.5286", "submitter": "Lorenzo Livi", "authors": "Lorenzo Livi", "title": "Designing labeled graph classifiers by exploiting the R\\'enyi entropy of\n  the dissimilarity representation", "comments": "Revised version", "journal-ref": null, "doi": "10.3390/e19050216", "report-no": null, "categories": "cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing patterns as labeled graphs is becoming increasingly common in\nthe broad field of computational intelligence. Accordingly, a wide repertoire\nof pattern recognition tools, such as classifiers and knowledge discovery\nprocedures, are nowadays available and tested for various datasets of labeled\ngraphs. However, the design of effective learning procedures operating in the\nspace of labeled graphs is still a challenging problem, especially from the\ncomputational complexity viewpoint. In this paper, we present a major\nimprovement of a general-purpose classifier for graphs, which is conceived on\nan interplay between dissimilarity representation, clustering,\ninformation-theoretic techniques, and evolutionary optimization algorithms. The\nimprovement focuses on a specific key subroutine devised to compress the input\ndata. We prove different theorems which are fundamental to the setting of the\nparameters controlling such a compression operation. We demonstrate the\neffectiveness of the resulting classifier by benchmarking the developed\nvariants on well-known datasets of labeled graphs, considering as distinct\nperformance indicators the classification accuracy, computing time, and\nparsimony in terms of structural complexity of the synthesized classification\nmodels. The results show state-of-the-art standards in terms of test set\naccuracy and a considerable speed-up for what concerns the computing time.\n", "versions": [{"version": "v1", "created": "Fri, 22 Aug 2014 13:03:00 GMT"}, {"version": "v2", "created": "Sun, 11 Jan 2015 15:29:31 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2016 23:44:25 GMT"}, {"version": "v4", "created": "Fri, 11 Mar 2016 13:18:17 GMT"}, {"version": "v5", "created": "Fri, 31 Mar 2017 19:26:16 GMT"}, {"version": "v6", "created": "Tue, 4 Apr 2017 20:48:07 GMT"}, {"version": "v7", "created": "Thu, 20 Apr 2017 14:40:11 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Livi", "Lorenzo", ""]]}, {"id": "1408.5400", "submitter": "Sebastian Ramos", "authors": "Jiaolong Xu, Sebastian Ramos, David Vazquez, Antonio M. Lopez", "title": "Hierarchical Adaptive Structural SVM for Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key topic in classification is the accuracy loss produced when the data\ndistribution in the training (source) domain differs from that in the testing\n(target) domain. This is being recognized as a very relevant problem for many\ncomputer vision tasks such as image classification, object detection, and\nobject category recognition. In this paper, we present a novel domain\nadaptation method that leverages multiple target domains (or sub-domains) in a\nhierarchical adaptation tree. The core idea is to exploit the commonalities and\ndifferences of the jointly considered target domains.\n  Given the relevance of structural SVM (SSVM) classifiers, we apply our idea\nto the adaptive SSVM (A-SSVM), which only requires the target domain samples\ntogether with the existing source-domain classifier for performing the desired\nadaptation. Altogether, we term our proposal as hierarchical A-SSVM (HA-SSVM).\n  As proof of concept we use HA-SSVM for pedestrian detection and object\ncategory recognition. In the former we apply HA-SSVM to the deformable\npart-based model (DPM) while in the latter HA-SSVM is applied to multi-category\nclassifiers. In both cases, we show how HA-SSVM is effective in increasing the\ndetection/recognition accuracy with respect to adaptation strategies that\nignore the structure of the target data. Since, the sub-domains of the target\ndata are not always known a priori, we shown how HA-SSVM can incorporate\nsub-domain structure discovery for object category recognition.\n", "versions": [{"version": "v1", "created": "Fri, 22 Aug 2014 19:56:14 GMT"}], "update_date": "2014-08-26", "authors_parsed": [["Xu", "Jiaolong", ""], ["Ramos", "Sebastian", ""], ["Vazquez", "David", ""], ["Lopez", "Antonio M.", ""]]}, {"id": "1408.5418", "submitter": "Jianping Shi", "authors": "Jianping Shi, Qiong Yan, Li Xu, Jiaya Jia", "title": "Hierarchical Saliency Detection on Extended CSSD", "comments": "14 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": "CUHK-CSE-201408", "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Complex structures commonly exist in natural images. When an image contains\nsmall-scale high-contrast patterns either in the background or foreground,\nsaliency detection could be adversely affected, resulting erroneous and\nnon-uniform saliency assignment. The issue forms a fundamental challenge for\nprior methods. We tackle it from a scale point of view and propose a\nmulti-layer approach to analyze saliency cues. Different from varying patch\nsizes or downsizing images, we measure region-based scales. The final saliency\nvalues are inferred optimally combining all the saliency cues in different\nscales using hierarchical inference. Through our inference model, single-scale\ninformation is selected to obtain a saliency map. Our method improves detection\nquality on many images that cannot be handled well traditionally. We also\nconstruct an extended Complex Scene Saliency Dataset (ECSSD) to include complex\nbut general natural images.\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2014 15:18:47 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2015 07:49:43 GMT"}], "update_date": "2015-08-05", "authors_parsed": [["Shi", "Jianping", ""], ["Yan", "Qiong", ""], ["Xu", "Li", ""], ["Jia", "Jiaya", ""]]}, {"id": "1408.5516", "submitter": "Sanja Fidler", "authors": "Sanja Fidler, Marko Boben, Ales Leonardis", "title": "Learning a Hierarchical Compositional Shape Vocabulary for Multi-class\n  Object Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchies allow feature sharing between objects at multiple levels of\nrepresentation, can code exponential variability in a very compact way and\nenable fast inference. This makes them potentially suitable for learning and\nrecognizing a higher number of object classes. However, the success of the\nhierarchical approaches so far has been hindered by the use of hand-crafted\nfeatures or predetermined grouping rules. This paper presents a novel framework\nfor learning a hierarchical compositional shape vocabulary for representing\nmultiple object classes. The approach takes simple contour fragments and learns\ntheir frequent spatial configurations. These are recursively combined into\nincreasingly more complex and class-specific shape compositions, each exerting\na high degree of shape variability. At the top-level of the vocabulary, the\ncompositions are sufficiently large and complex to represent the whole shapes\nof the objects. We learn the vocabulary layer after layer, by gradually\nincreasing the size of the window of analysis and reducing the spatial\nresolution at which the shape configurations are learned. The lower layers are\nlearned jointly on images of all classes, whereas the higher layers of the\nvocabulary are learned incrementally, by presenting the algorithm with one\nobject class after another. The experimental results show that the learned\nmulti-class object representation scales favorably with the number of object\nclasses and achieves a state-of-the-art detection performance at both, faster\ninference as well as shorter training times.\n", "versions": [{"version": "v1", "created": "Sat, 23 Aug 2014 17:25:33 GMT"}], "update_date": "2014-08-26", "authors_parsed": [["Fidler", "Sanja", ""], ["Boben", "Marko", ""], ["Leonardis", "Ales", ""]]}, {"id": "1408.5552", "submitter": "Jae Jun Lee", "authors": "Jaejun Lee and Taeseon Yun", "title": "Fuzzy and entropy facial recognition", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper suggests an effective method for facial recognition using fuzzy\ntheory and Shannon entropy. Combination of fuzzy theory and Shannon entropy\neliminates the complication of other methods. Shannon entropy calculates the\nratio of an element between faces, and fuzzy theory calculates the member ship\nof the entropy with 1. More details will be mentioned in Section 3. The\nlearning performance is better than others as it is very simple, and only need\ntwo data per learning. By using factors that don't usually change during the\nlife, the method will have a high accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 24 Aug 2014 04:46:58 GMT"}], "update_date": "2014-08-26", "authors_parsed": [["Lee", "Jaejun", ""], ["Yun", "Taeseon", ""]]}, {"id": "1408.5574", "submitter": "Chunhua Shen", "authors": "Guosheng Lin, Chunhua Shen, Anton van den Hengel", "title": "Supervised Hashing Using Graph Cuts and Boosted Decision Trees", "comments": "15 pages. Appearing in IEEE T. Pattern Analysis & Machine\n  Intelligence. arXiv admin note: text overlap with arXiv:1404.1561", "journal-ref": null, "doi": "10.1109/TPAMI.2015.2404776", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding image features into a binary Hamming space can improve both the\nspeed and accuracy of large-scale query-by-example image retrieval systems.\nSupervised hashing aims to map the original features to compact binary codes in\na manner which preserves the label-based similarities of the original data.\nMost existing approaches apply a single form of hash function, and an\noptimization process which is typically deeply coupled to this specific form.\nThis tight coupling restricts the flexibility of those methods, and can result\nin complex optimization problems that are difficult to solve. In this work we\nproffer a flexible yet simple framework that is able to accommodate different\ntypes of loss functions and hash functions. The proposed framework allows a\nnumber of existing approaches to hashing to be placed in context, and\nsimplifies the development of new problem-specific hashing methods. Our\nframework decomposes the into two steps: binary code (hash bits) learning, and\nhash function learning. The first step can typically be formulated as a binary\nquadratic problem, and the second step can be accomplished by training standard\nbinary classifiers. For solving large-scale binary code inference, we show how\nto ensure that the binary quadratic problems are submodular such that an\nefficient graph cut approach can be used. To achieve efficiency as well as\nefficacy on large-scale high-dimensional data, we propose to use boosted\ndecision trees as the hash functions, which are nonlinear, highly descriptive,\nand very fast to train and evaluate. Experiments demonstrate that our proposed\nmethod significantly outperforms most state-of-the-art methods, especially on\nhigh-dimensional data.\n", "versions": [{"version": "v1", "created": "Sun, 24 Aug 2014 07:40:19 GMT"}, {"version": "v2", "created": "Sun, 8 Feb 2015 23:52:38 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Lin", "Guosheng", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1408.5601", "submitter": "Jianwei Yang", "authors": "Jianwei Yang, Zhen Lei, Stan Z. Li", "title": "Learn Convolutional Neural Network for Face Anti-Spoofing", "comments": "8 pages, 9 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though having achieved some progresses, the hand-crafted texture features,\ne.g., LBP [23], LBP-TOP [11] are still unable to capture the most\ndiscriminative cues between genuine and fake faces. In this paper, instead of\ndesigning feature by ourselves, we rely on the deep convolutional neural\nnetwork (CNN) to learn features of high discriminative ability in a supervised\nmanner. Combined with some data pre-processing, the face anti-spoofing\nperformance improves drastically. In the experiments, over 70% relative\ndecrease of Half Total Error Rate (HTER) is achieved on two challenging\ndatasets, CASIA [36] and REPLAY-ATTACK [7] compared with the state-of-the-art.\nMeanwhile, the experimental results from inter-tests between two datasets\nindicates CNN can obtain features with better generalization ability. Moreover,\nthe nets trained using combined data from two datasets have less biases between\ntwo datasets.\n", "versions": [{"version": "v1", "created": "Sun, 24 Aug 2014 13:08:19 GMT"}, {"version": "v2", "created": "Tue, 26 Aug 2014 02:45:55 GMT"}], "update_date": "2014-08-27", "authors_parsed": [["Yang", "Jianwei", ""], ["Lei", "Zhen", ""], ["Li", "Stan Z.", ""]]}, {"id": "1408.5667", "submitter": "Dornoosh  Zonoobi", "authors": "Dornoosh Zonoobi, Shahrooz Faghih Roohi, Ashraf A. Kassim", "title": "Dependent Nonparametric Bayesian Group Dictionary Learning for online\n  reconstruction of Dynamic MR images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a dictionary learning based approach applied to\nthe problem of real-time reconstruction of MR image sequences that are highly\nundersampled in k-space. Unlike traditional dictionary learning, our method\nintegrates both global and patch-wise (local) sparsity information and\nincorporates some priori information into the reconstruction process. Moreover,\nwe use a Dependent Hierarchical Beta-process as the prior for the group-based\ndictionary learning, which adaptively infers the dictionary size and the\nsparsity of each patch; and also ensures that similar patches are manifested in\nterms of similar dictionary atoms. An efficient numerical algorithm based on\nthe alternating direction method of multipliers (ADMM) is also presented.\nThrough extensive experimental results we show that our proposed method\nachieves superior reconstruction quality, compared to the other state-of-the-\nart DL-based methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Aug 2014 06:28:11 GMT"}, {"version": "v2", "created": "Mon, 17 Nov 2014 09:25:38 GMT"}, {"version": "v3", "created": "Thu, 12 Feb 2015 04:21:00 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Zonoobi", "Dornoosh", ""], ["Roohi", "Shahrooz Faghih", ""], ["Kassim", "Ashraf A.", ""]]}, {"id": "1408.6257", "submitter": "Sheng Huang", "authors": "Sheng Huang, Dan Yang, Jia Zhou, Luwen Huangfu, Xiaohong Zhang", "title": "Sparse Graph-based Transduction for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the remarkable successes of Graph-based Transduction (GT) and\nSparse Representation (SR), we present a novel Classifier named Sparse\nGraph-based Classifier (SGC) for image classification. In SGC, SR is leveraged\nto measure the correlation (similarity) of each two samples and a graph is\nconstructed for encoding these correlations. Then the Laplacian eigenmapping is\nadopted for deriving the graph Laplacian of the graph. Finally, SGC can be\nobtained by plugging the graph Laplacian into the conventional GT framework. In\nthe image classification procedure, SGC utilizes the correlations, which are\nencoded in the learned graph Laplacian, to infer the labels of unlabeled\nimages. SGC inherits the merits of both GT and SR. Compared to SR, SGC improves\nthe robustness and the discriminating power of GT. Compared to GT, SGC\nsufficiently exploits the whole data. Therefore it alleviates the undercomplete\ndictionary issue suffered by SR. Four popular image databases are employed for\nevaluation. The results demonstrate that SGC can achieve a promising\nperformance in comparison with the state-of-the-art classifiers, particularly\nin the small training sample size case and the noisy sample case.\n", "versions": [{"version": "v1", "created": "Tue, 26 Aug 2014 20:53:44 GMT"}, {"version": "v2", "created": "Fri, 12 Dec 2014 09:53:36 GMT"}], "update_date": "2014-12-15", "authors_parsed": [["Huang", "Sheng", ""], ["Yang", "Dan", ""], ["Zhou", "Jia", ""], ["Huangfu", "Luwen", ""], ["Zhang", "Xiaohong", ""]]}, {"id": "1408.6299", "submitter": "Andreas Mang", "authors": "Andreas Mang and George Biros", "title": "An inexact Newton-Krylov algorithm for constrained diffeomorphic image\n  registration", "comments": "32 pages; 10 figures; 9 tables", "journal-ref": "SIAM J. Imaging Sci., 8(2):1030-1069, 2015", "doi": "10.1137/140984002", "report-no": null, "categories": "math.NA cs.CV cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose numerical algorithms for solving large deformation diffeomorphic\nimage registration problems. We formulate the nonrigid image registration\nproblem as a problem of optimal control. This leads to an infinite-dimensional\npartial differential equation (PDE) constrained optimization problem.\n  The PDE constraint consists, in its simplest form, of a hyperbolic transport\nequation for the evolution of the image intensity. The control variable is the\nvelocity field. Tikhonov regularization on the control ensures well-posedness.\nWe consider standard smoothness regularization based on $H^1$- or\n$H^2$-seminorms. We augment this regularization scheme with a constraint on the\ndivergence of the velocity field rendering the deformation incompressible and\nthus ensuring that the determinant of the deformation gradient is equal to one,\nup to the numerical error.\n  We use a Fourier pseudospectral discretization in space and a Chebyshev\npseudospectral discretization in time. We use a preconditioned, globalized,\nmatrix-free, inexact Newton-Krylov method for numerical optimization. A\nparameter continuation is designed to estimate an optimal regularization\nparameter. Regularity is ensured by controlling the geometric properties of the\ndeformation field. Overall, we arrive at a black-box solver. We study spectral\nproperties of the Hessian, grid convergence, numerical accuracy, computational\nefficiency, and deformation regularity of our scheme. We compare the designed\nNewton-Krylov methods with a globalized preconditioned gradient descent. We\nstudy the influence of a varying number of unknowns in time.\n  The reported results demonstrate excellent numerical accuracy, guaranteed\nlocal deformation regularity, and computational efficiency with an optional\ncontrol on local mass conservation. The Newton-Krylov methods clearly\noutperform the Picard method if high accuracy of the inversion is required.\n", "versions": [{"version": "v1", "created": "Wed, 27 Aug 2014 02:36:11 GMT"}, {"version": "v2", "created": "Mon, 26 Jan 2015 23:05:07 GMT"}, {"version": "v3", "created": "Thu, 7 May 2015 13:37:06 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Mang", "Andreas", ""], ["Biros", "George", ""]]}, {"id": "1408.6335", "submitter": "Leonid Yaroslavsky", "authors": "Leonid Yaroslavsky", "title": "Compression, Restoration, Re-sampling, Compressive Sensing: Fast\n  Transforms in Digital Imaging", "comments": "41 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transform image processing methods are methods that work in domains of image\ntransforms, such as Discrete Fourier, Discrete Cosine, Wavelet and alike. They\nare the basic tool in image compression, in image restoration, in image\nre-sampling and geometrical transformations and can be traced back to early\n1970-ths. The paper presents a review of these methods with emphasis on their\ncomparison and relationships, from the very first steps of transform image\ncompression methods to adaptive and local adaptive transform domain filters for\nimage restoration, to methods of precise image re-sampling and image\nreconstruction from sparse samples and up to \"compressive sensing\" approach\nthat has gained popularity in last few years. The review has a tutorial\ncharacter and purpose.\n", "versions": [{"version": "v1", "created": "Wed, 27 Aug 2014 07:43:15 GMT"}], "update_date": "2014-08-28", "authors_parsed": [["Yaroslavsky", "Leonid", ""]]}, {"id": "1408.6418", "submitter": "Andrei Barbu", "authors": "Andrei Barbu, Alexander Bridge, Zachary Burchill, Dan Coroian, Sven\n  Dickinson, Sanja Fidler, Aaron Michaux, Sam Mussman, Siddharth Narayanaswamy,\n  Dhaval Salvi, Lara Schmidt, Jiangnan Shangguan, Jeffrey Mark Siskind, Jarrell\n  Waggoner, Song Wang, Jinlian Wei, Yifan Yin, Zhiqi Zhang", "title": "Video In Sentences Out", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-102-112", "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system that produces sentential descriptions of video: who did\nwhat to whom, and where and how they did it. Action class is rendered as a\nverb, participant objects as noun phrases, properties of those objects as\nadjectival modifiers in those noun phrases, spatial relations between those\nparticipants as prepositional phrases, and characteristics of the event as\nprepositional-phrase adjuncts and adverbial modifiers. Extracting the\ninformation needed to render these linguistic entities requires an approach to\nevent recognition that recovers object tracks, the trackto-role assignments,\nand changing body posture.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:43:12 GMT"}], "update_date": "2014-08-28", "authors_parsed": [["Barbu", "Andrei", ""], ["Bridge", "Alexander", ""], ["Burchill", "Zachary", ""], ["Coroian", "Dan", ""], ["Dickinson", "Sven", ""], ["Fidler", "Sanja", ""], ["Michaux", "Aaron", ""], ["Mussman", "Sam", ""], ["Narayanaswamy", "Siddharth", ""], ["Salvi", "Dhaval", ""], ["Schmidt", "Lara", ""], ["Shangguan", "Jiangnan", ""], ["Siskind", "Jeffrey Mark", ""], ["Waggoner", "Jarrell", ""], ["Wang", "Song", ""], ["Wei", "Jinlian", ""], ["Yin", "Yifan", ""], ["Zhang", "Zhiqi", ""]]}, {"id": "1408.6615", "submitter": "Shervin Minaee", "authors": "Shervin Minaee and AmirAli Abdolrashidi", "title": "Multispectral Palmprint Recognition Using Textural Features", "comments": "5 pages, Published in IEEE Signal Processing in Medicine and Biology\n  Symposium 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to utilize identification to the best extent, we need robust and\nfast algorithms and systems to process the data. Having palmprint as a reliable\nand unique characteristic of every person, we extract and use its features\nbased on its geometry, lines and angles. There are countless ways to define\nmeasures for the recognition task. To analyze a new point of view, we extracted\ntextural features and used them for palmprint recognition. Co-occurrence matrix\ncan be used for textural feature extraction. As classifiers, we have used the\nminimum distance classifier (MDC) and the weighted majority voting system\n(WMV). The proposed method is tested on a well-known multispectral palmprint\ndataset of 6000 samples and an accuracy rate of 99.96-100% is obtained for most\nscenarios which outperforms all previous works in multispectral palmprint\nrecognition.\n", "versions": [{"version": "v1", "created": "Thu, 28 Aug 2014 03:20:38 GMT"}, {"version": "v2", "created": "Mon, 1 Sep 2014 04:49:30 GMT"}, {"version": "v3", "created": "Thu, 12 Feb 2015 03:03:02 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Minaee", "Shervin", ""], ["Abdolrashidi", "AmirAli", ""]]}, {"id": "1408.6911", "submitter": "Chandranath Adak", "authors": "Chandranath Adak, Bidyut B. Chaudhuri", "title": "Text Line Identification in Tagore's Manuscript", "comments": null, "journal-ref": "Proc. IEEE TechSym-2014, IEEE Conf. #32812, pp. 210-213,\n  Kharagpur, India, 28 Feb.-2 Mar., 2014", "doi": "10.1109/TechSym.2014.6808048", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a text line identification method is proposed. The text lines\nof printed document are easy to segment due to uniform straightness of the\nlines and sufficient gap between the lines. But in handwritten documents, the\nline is non-uniform and interline gaps are variable. We take Rabindranath\nTagore's manuscript as it is one of the most difficult manuscripts that contain\ndoodles. Our method consists of a pre-processing stage to clean the document\nimage. Then we separate doodles from the manuscript to get the textual region.\nAfter that we identify the text lines on the manuscript. For text line\nidentification, we use window examination, black run-length smearing,\nhorizontal histogram and connected component analysis.\n", "versions": [{"version": "v1", "created": "Fri, 29 Aug 2014 02:53:46 GMT"}, {"version": "v2", "created": "Thu, 18 Aug 2016 01:26:05 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Adak", "Chandranath", ""], ["Chaudhuri", "Bidyut B.", ""]]}, {"id": "1408.6915", "submitter": "Ling Lu", "authors": "Scott A. Skirlo, Ling Lu and Marin Solja\\v{c}i\\'c", "title": "Binary matrices of optimal autocorrelations as alignment marks", "comments": "8 pages, 6 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a new class of binary matrices by maximizing the peak-sidelobe\ndistances in the aperiodic autocorrelations. These matrices can be used as\nrobust position marks for in-plane spatial alignment. The optimal square\nmatrices of dimensions up to 7 by 7 and optimal diagonally-symmetric matrices\nof 8 by 8 and 9 by 9 were found by exhaustive searches.\n", "versions": [{"version": "v1", "created": "Fri, 29 Aug 2014 03:34:52 GMT"}], "update_date": "2014-09-01", "authors_parsed": [["Skirlo", "Scott A.", ""], ["Lu", "Ling", ""], ["Solja\u010di\u0107", "Marin", ""]]}, {"id": "1408.6963", "submitter": "Xavier Boix", "authors": "Xavier Boix, Gemma Roig, Luc Van Gool", "title": "Comment on \"Ensemble Projection for Semi-supervised Image\n  Classification\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a series of papers by Dai and colleagues [1,2], a feature map (or kernel)\nwas introduced for semi- and unsupervised learning. This feature map is build\nfrom the output of an ensemble of classifiers trained without using the\nground-truth class labels. In this critique, we analyze the latest version of\nthis series of papers, which is called Ensemble Projections [2]. We show that\nthe results reported in [2] were not well conducted, and that Ensemble\nProjections performs poorly for semi-supervised learning.\n", "versions": [{"version": "v1", "created": "Fri, 29 Aug 2014 09:37:56 GMT"}], "update_date": "2014-09-01", "authors_parsed": [["Boix", "Xavier", ""], ["Roig", "Gemma", ""], ["Van Gool", "Luc", ""]]}, {"id": "1408.6974", "submitter": "Pui Tung Choi", "authors": "Pui Tung Choi, Lok Ming Lui", "title": "Fast Disk Conformal Parameterization of Simply-connected Open Surfaces", "comments": null, "journal-ref": "Journal of Scientific Computing 65, 1065-1090 (2015)", "doi": "10.1007/s10915-015-9998-2", "report-no": null, "categories": "cs.CG cs.CV cs.GR cs.MM math.DG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Surface parameterizations have been widely used in computer graphics and\ngeometry processing. In particular, as simply-connected open surfaces are\nconformally equivalent to the unit disk, it is desirable to compute the disk\nconformal parameterizations of the surfaces. In this paper, we propose a novel\nalgorithm for the conformal parameterization of a simply-connected open surface\nonto the unit disk, which significantly speeds up the computation, enhances the\nconformality and stability, and guarantees the bijectivity. The conformality\ndistortions at the inner region and on the boundary are corrected by two steps,\nwith the aid of an iterative scheme using quasi-conformal theories.\nExperimental results demonstrate the effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 29 Aug 2014 10:31:56 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Choi", "Pui Tung", ""], ["Lui", "Lok Ming", ""]]}, {"id": "1408.7071", "submitter": "Zhenzhong Lan", "authors": "Zhenzhong Lan, Xuanchong Li, Alexandar G. Hauptmann", "title": "Temporal Extension of Scale Pyramid and Spatial Pyramid Matching for\n  Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historically, researchers in the field have spent a great deal of effort to\ncreate image representations that have scale invariance and retain spatial\nlocation information. This paper proposes to encode equivalent temporal\ncharacteristics in video representations for action recognition. To achieve\ntemporal scale invariance, we develop a method called temporal scale pyramid\n(TSP). To encode temporal information, we present and compare two methods\ncalled temporal extension descriptor (TED) and temporal division pyramid (TDP)\n. Our purpose is to suggest solutions for matching complex actions that have\nlarge variation in velocity and appearance, which is missing from most current\naction representations. The experimental results on four benchmark datasets,\nUCF50, HMDB51, Hollywood2 and Olympic Sports, support our approach and\nsignificantly outperform state-of-the-art methods. Most noticeably, we achieve\n65.0% mean accuracy and 68.2% mean average precision on the challenging HMDB51\nand Hollywood2 datasets which constitutes an absolute improvement over the\nstate-of-the-art by 7.8% and 3.9%, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 29 Aug 2014 17:05:29 GMT"}], "update_date": "2014-09-01", "authors_parsed": [["Lan", "Zhenzhong", ""], ["Li", "Xuanchong", ""], ["Hauptmann", "Alexandar G.", ""]]}]